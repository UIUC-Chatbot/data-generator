[
    {
        "question": "What is the design process for a digital FSM?\\n",
        "answers": [
            "\nThe design process for a digital FSM includes six steps: developing an abstract model, specifying I/O behavior, completing the specification, choosing a state representation, calculating logic expressions, and implementing with flip-flops and gates.",
            "The purpose of using a Gray code is to create a sequence of patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.",
            "What is the difference between SOP and POS?\n\nSOP and POS are two forms of two-level logic. In SOP, the output is true if any of the inputs are true. In POS, the output is true if all of the inputs are true."
        ],
        "score": [
            8.828121185302734,
            -11.017366409301758,
            -11.31814193725586
        ],
        "text": "{Finite State Machine Design Examples, Part I}  This set of notes uses a series of examples to illustrate design principles  for the implementation of finite state machines (FSMs) using digital logic. We begin with an overview of the design process for a digital FSM, from the development of an abstract model through the implementation of functions for the next-state variables and output signals. Our first few examples cover only the concrete aspects: we implement several counters, which illustrate the basic  process of translating a concrete and complete state transition diagram into an implementation based on flip-flops and logic gates. We next consider a counter with a number of states that is not a power of two, with which we illustrate the need for FSM initialization.  As part of solving the initialization problem, we also introduce   a general form of selection logic called a multiplexer.  We then consider the design process as a whole through a more general example of a counter with multiple inputs to control its behavior.  We work from an abstract model down to an implementation, illustrating how semantic knowledge from the abstract model can be used to simplify the  implementation.  Finally, we illustrate how the choice of representation for the FSM's internal state affects the complexity of the implementation. Fortunately, designs that are more intuitive and easier for humans to understand also typically make the best designs in terms of  other metrics, such as logic complexity.   {Steps in the Design Process}  Before we begin exploring designs, let's talk briefly about the general approach that we take when designing an FSM.  We follow a six-step process:{-8pt}  {{}{} {}{}{} {develop an abstract model}{step-abs} {specify I/O behavior}{step-io} {complete the specification}{step-complete} {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates} } {-8pt}  In Step {step-abs}, we translate our description in human language into a model with states and desired behavior.  At this stage, we  simply try to capture the intent of the description and are not particularly thorough nor exact.  Step {step-io} begins to formalize the model, starting with its input and output behavior.  If we eventually plan to develop an implementation of our FSM as a digital system (which is not the  only choice, of course!), all input and output must consist of bits.  Often, input and/or output specifications may need to match other digital systems to which we plan to connect our FSM.  In fact, { most problems in developing large digital systems today arise because of incompatibilities when composing two or more separately designed pieces} (or { modules}) into an integrated system.  Once we know the I/O behavior for our FSM, in Step {step-complete} we start to make any implicit assumptions clear and to make any other decisions necessary to the design.  Occasionally, we may choose to leave something undecided in the hope of simplifying the design with ``don't care'' entries in the logic formulation.  In Step {step-repn}, we select an internal representation for the bits necessary to encode the state of our FSM.  In practice, for small designs, this representation can be selected by a computer  in such a way as to optimize the implementation.  However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.  In the later examples in this set of notes, we show how even a  small design can leverage meaningful information from the design when selecting the representation, leading to an implementation that is simpler and is easier to build correctly.  We also show how one can use abstraction to simplify an implementation.  By Step {step-logic}, our design is a complete specification in terms of bits, and we need merely derive logic expressions for the next-state variables and the output signals."
    },
    {
        "question": "What are the concrete aspects of our first few examples?\\n",
        "answers": [
            "\nThe first few examples in the passage are of two-level logic, DeMorgan's law, and SOP form. Two-level logic is a popular way of expressing logic functions, and DeMorgan's law is a way to simplify expressions. SOP form is a way of representing a logic function.",
            "The vending machine keeps track of how much money has been inserted as a multiple of 5 cents (one nickel). It has six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).",
            "One reason to try to leverage the fact that the next-state patterns from these two states are not relevant is that it may simplify the design of the system. If the next-state patterns are not needed, then the system can be designed without them, which may make the design process simpler and faster. Additionally, eliminating the next-state patterns may also reduce the cost of the system."
        ],
        "score": [
            0.3679552972316742,
            -11.32126235961914,
            -11.28647518157959
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the need for FSM initialization?\\n",
        "answers": [
            "\nAn FSM must be initialized to ensure that it starts in a known state and to avoid undefined behavior. Without initialization, an FSM may produce incorrect results or may not work at all.",
            "A Gray code counter is designed by first creating a transition diagram that shows the states and outputs of the counter. Next, the next-state logic is developed by creating K-maps for the next-state values and deriving algebraic expressions. Finally, the next-state logic is implemented in a diagram.",
            "The FSM ensures that the lights on one or both roads are red at all times by keeping track of the state of the light. If the light is green, it will turn yellow and then red. If the light is red, it will stay red."
        ],
        "score": [
            6.33916711807251,
            -11.165115356445312,
            -5.356406211853027
        ],
        "text": "level state, to simplify the implementation.} {Use counters to insert time-based transitions between states (such  as timeouts).} {Implement an FSM using logic components such as registers,  counters, comparators, and adders as building blocks.} {Explain the basic organization of a computer's microarchitecture as well as the role played by elements of a von Neumann design in the processing of instructions.} {Identify the stages of processing an instruction (such as fetch, decode, getting operands, execution, and writing back results) in a  processor control unit state machine diagram.}   And, at the highest level, we expect that you will be able to do the following:  {}{{}{} {}{}{}  {Explain the difference between the Moore and Mealy machine models,  as well as why you might find each of them useful when designing an FSM.} {Understand the need for initialization of an FSM, be able to analyze  and identify potential problems arising from lack of initialization, and  be able to extend an implementation to include initialization to an  appropriate state when necessary.} {Understand how the choice of internal state bits for an FSM can  affect the complexity of the implementation of next-state and output  logic, and be able to select a reasonable state assignment.} {Identify and fix design flaws in simple FSMs by analyzing an existing  implementation, comparing it with the specification, and removing any  differences by making any necessary changes to the implementation.}      {   }  empty 3rd page      {Instruction Set Architecture*}  This set of notes discusses  tradeoffs and design elements of instruction set architectures (ISAs). { The material is beyond the scope of our class, and is provided purely for your interest.}  Those who find these topics interesting may also want to read the ECE391 notes, which describe similar material with a focus on the x86 ISA.  As you know, the ISA defines the interface between software and hardware, abstracting the capabilities of a computer's datapath and standardizing the format of instructions to utilize those capabilities.  Successful ISAs are rarely discarded, as success implies the existence of large amounts of software built to use the ISA.  Rather, they are extended, and their original forms must be supported for decades (consider, for example, the IBM 360 and the Intel x86).  Employing sound design principles is thus imperative in an ISA.  {Formats and Fields*}  The LC-3 ISA  employs fixed-length instructions and a load-store architecture, two aspects that help to reduce the design space to a manageable set of choices.  In a general ISA design, many other options exist for instruction formats.  Recall the idea of separating the bits of an instruction into (possibly non-contiguous) fields.  One of the fields must contain an opcode, which specifies the type of operation to be performed by the instruction.  In the {LC-3} ISA, most opcodes specify both the type of operation and the types of arguments to the operation.   More generally, many addressing modes are possible for each operand, and we can think of the bits that specify the addressing mode as a separate field, known as the { mode} field.    As a simple example, the {LC-3's} ADD and AND instructions  contain a {1-bit} mode field that specifies whether the second operand of the ADD/AND comes from a register or is an immediate value.  Several questions must be answered in order to define the possible instruction formats for an ISA.  First, are instructions fixed-length or variable-length?  Second, how many addresses are needed for each instruction, and how many of the addresses can be memory addresses? Finally, what forms of addresses are possible for each operand? For example, can one use full memory addresses or only limited offsets relative to a register?  The answer to the first question depends on many factors, but several clear advantages exist for both answers.  { Fixed-length instructions} are easy to fetch and decode.  A"
    },
    {
        "question": "What is a multiplexer?\\n",
        "answers": [
            "\nA multiplexer is a digital logic block that allows one digital signal to select which of several others is forwarded to an output. The simplest form of the multiplexer is the 2-to-1 multiplexer, which has two inputs from the left and one from the top. The top input allows us to choose which of the left inputs is forwarded to the output.",
            "The choice of an FSM's input and output behavior affects the design process by determining the set of possible states and the set of possible outputs. The set of possible states is determined by the set of possible inputs, and the set of possible outputs is determined by the set of possible outputs.",
            "What is the reason for the popularity of two-level logic?\n\nThe popularity of two-level logic is due to the fact that it is a simple way to express logic functions. Additionally, two-level logic is easy to map into CMOS gates."
        ],
        "score": [
            7.5719895362854,
            -11.279451370239258,
            -10.736122131347656
        ],
        "text": "K-maps, we must start again with new K-maps for the lower-case checker.  If instead we have taken the approach of designing logic for the upper and  lower bits of the ASCII character, we can reuse most of that logic, since the functions T_4 and T_5 are identical when checking for a lower-case character.  Recalling the algebraic form of U(C), we can then write a function L(C) (a lower-case checker) as shown on the left below.   {eqnarray*} U&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5  L&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5  &=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+ &&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0}) {eqnarray*}   {file=part2/figs/ascii-cmp-lower.eps,width=3.6in}   Finally, if we have used a design based on comparators or adders, the design of a lower-case checker becomes trivial: simply change the numbers that we input to these components, as shown in the figure on the right above for the comparator-based design.  The only changes from the upper-case checker design are the inputs to the comparators and the output produced,  highlighted with blue text in the figure.    {The Multiplexer}  Using the more abstract designs for checking ranges of ASCII characters, we can go a step further and create a checker for both upper- and lower-case letters.  To do so, we add another input S that allows us to select the  function that we want---either the upper-case checker U(C) or the  lower-case checker L(C).  For this purpose, we make use of a logic block called a { multiplexer}, or { mux}. Multiplexers are an important abstraction for digital logic.  In  general, a multiplexer allows us to use one digital signal to  select which of several others is forwarded to an output.   The simplest form of the multiplexer is the 2-to-1 multiplexer shown to  the right.   The logic diagram illustrates how the mux works.  The block  has two inputs from the left and one from the top.  The top input allows  us to choose which of the left inputs is forwarded to the output.   When the input S=0, the upper AND gate outputs 0, and the lower AND gate outputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0. Similarly, when input S=1, the upper AND gate outputs D_1, and the lower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1.      The symbolic form of the mux is a trapezoid with data inputs on the  larger side, an output on the smaller side, and a select input on the angled part of the trapezoid.  The labels inside the trapezoid indicate  the value of the select input S for which the adjacent data signal,  D_1 or D_0, is copied to the output Q.  We can generalize multiplexers in two ways.  First, we can extend the  single select input to a group of select inputs.  An {N-bit} select input allows selection from amongst 2^N inputs.  A {4-to-1}  multiplexer is shown below, for example.  The logic diagram on the left shows how the {4-to-1 mux} operates.  For any combination of S_1S_0, three of the AND gates produce 0, and the fourth outputs the D input corresponding to the interpretation of S as an unsigned number. Given three zeroes and one D input, the OR gate thus reproduces one of  the D's.  When S_1S_0=10, for example, the third AND gate copies D_2, and Q=D_2.  {{file=part2/figs/mux4-to-1.eps,width=5.60in}}  As shown in the middle figure, a {4-to-1} mux can also be built from three {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux  appears on the right in the figure.    The second way in which we can generalize multiplexers is by using several multiplexers of the same type and using the same signals for  selection.  For example, we might use a single select bit T to choose  between any number of paired inputs.  Denote input pair by i D_1^i  and D_0^i.  For each pair, we have an output Q_i.    When T=0, Q_i=D_0^i for each value of i.  And, when T=1, Q_i=D_1^i for each value of i.  Each value of i requires a {2-to-1} mux with its select input driven by the global select signal T.  Returning to the example of the upper-"
    },
    {
        "question": "How does the choice of representation for the FSM's internal state affect the complexity of the implementation?\", '\\n",
        "answers": [
            "\nThe choice of representation for the FSM's internal state affects the complexity of the implementation in terms of the number of states, the number of inputs, the number of outputs, and the number of transition rules.",
            "The timer is set to 0, and the state of the FSM is changed to the next state in the sequence. The new state of the FSM determines the duration of the next state, and the timer is set accordingly.",
            "The second step in the design process is to think carefully about metrics of possible interest. Once you have a set of metrics that you feel is complete, your next step is to get rid of as many as you can."
        ],
        "score": [
            9.088595390319824,
            -2.1944117546081543,
            -11.120656967163086
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": "What is the design process for a digital FSM?\\n",
        "answers": [
            "\nThe design process for a digital FSM typically consists of six steps: (1) developing an abstract model, (2) specifying I/O behavior, (3) completing the specification, (4) choosing a state representation, (5) calculating logic expressions, and (6) implementing with flip-flops and gates.",
            "The value of register N corresponds to the number of nickels that have been inserted into the machine. If the user inserts a quarter, the machine will transition to a state with N+5, indicating that 5 more nickels have been inserted. If the user attempts to purchase an item without inserting enough money first, the machine will remain in the same state and reject the request.",
            "The REJECTD and REJECTQ states are used to handle user mistakes. If a user enters a second dime in the DIME state, our FSM should reject the coin. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0). The REJECTQ state is used to handle a user who inserts more than one quarter."
        ],
        "score": [
            8.915242195129395,
            -9.834997177124023,
            -6.159486770629883
        ],
        "text": "{Finite State Machine Design Examples, Part I}  This set of notes uses a series of examples to illustrate design principles  for the implementation of finite state machines (FSMs) using digital logic. We begin with an overview of the design process for a digital FSM, from the development of an abstract model through the implementation of functions for the next-state variables and output signals. Our first few examples cover only the concrete aspects: we implement several counters, which illustrate the basic  process of translating a concrete and complete state transition diagram into an implementation based on flip-flops and logic gates. We next consider a counter with a number of states that is not a power of two, with which we illustrate the need for FSM initialization.  As part of solving the initialization problem, we also introduce   a general form of selection logic called a multiplexer.  We then consider the design process as a whole through a more general example of a counter with multiple inputs to control its behavior.  We work from an abstract model down to an implementation, illustrating how semantic knowledge from the abstract model can be used to simplify the  implementation.  Finally, we illustrate how the choice of representation for the FSM's internal state affects the complexity of the implementation. Fortunately, designs that are more intuitive and easier for humans to understand also typically make the best designs in terms of  other metrics, such as logic complexity.   {Steps in the Design Process}  Before we begin exploring designs, let's talk briefly about the general approach that we take when designing an FSM.  We follow a six-step process:{-8pt}  {{}{} {}{}{} {develop an abstract model}{step-abs} {specify I/O behavior}{step-io} {complete the specification}{step-complete} {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates} } {-8pt}  In Step {step-abs}, we translate our description in human language into a model with states and desired behavior.  At this stage, we  simply try to capture the intent of the description and are not particularly thorough nor exact.  Step {step-io} begins to formalize the model, starting with its input and output behavior.  If we eventually plan to develop an implementation of our FSM as a digital system (which is not the  only choice, of course!), all input and output must consist of bits.  Often, input and/or output specifications may need to match other digital systems to which we plan to connect our FSM.  In fact, { most problems in developing large digital systems today arise because of incompatibilities when composing two or more separately designed pieces} (or { modules}) into an integrated system.  Once we know the I/O behavior for our FSM, in Step {step-complete} we start to make any implicit assumptions clear and to make any other decisions necessary to the design.  Occasionally, we may choose to leave something undecided in the hope of simplifying the design with ``don't care'' entries in the logic formulation.  In Step {step-repn}, we select an internal representation for the bits necessary to encode the state of our FSM.  In practice, for small designs, this representation can be selected by a computer  in such a way as to optimize the implementation.  However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.  In the later examples in this set of notes, we show how even a  small design can leverage meaningful information from the design when selecting the representation, leading to an implementation that is simpler and is easier to build correctly.  We also show how one can use abstraction to simplify an implementation.  By Step {step-logic}, our design is a complete specification in terms of bits, and we need merely derive logic expressions for the next-state variables and the output signals."
    },
    {
        "question": "What are the concrete aspects of the design process?\\n",
        "answers": [
            "\nThe design process involves the manipulation of logic expressions in order to optimize them for specific metrics, such as cost, power, or performance. Today, this manipulation is performed almost entirely by computers, with human engineers supplying the logic functions of interest and imposing constraints on the process.",
            "An FSM is a finite state machine, which is a model of computation used to design both computer programs and hardware. An FSM is defined by a set of states, a set of inputs, a set of outputs, and a set of transitions between states.",
            "A priority encoder is a type of encoder that generates a signal whenever any of its input lines are active, and encodes the index of the highest active input as a two-bit unsigned number. A priority encoder is used to determine which of its input lines has the highest priority."
        ],
        "score": [
            2.4070401191711426,
            -9.735620498657227,
            -11.132038116455078
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": "What is an abstract model?\\n",
        "answers": [
            "\nAn abstract model is a mathematical model that is used to describe a system. It is usually simplified and does not include all the details of the system.",
            "SDRAM is a type of DRAM that is used for main memory in computers. DDR4 SDRAM is a newer type of DRAM that is faster than SDRAM.",
            "DRAM is a type of memory that uses capacitors to store data. When a row of DRAM is selected, all of the cells in that row are read and then written back, which refreshes the data in those cells. DRAM is said to have destructive reads because each read operation brings the voltage on the capacitor closer to the intermediate voltage level, which effectively destroys the data in the cell."
        ],
        "score": [
            8.713319778442383,
            -11.225364685058594,
            -11.046796798706055
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the need for FSM initialization?\\n",
        "answers": [
            "\nAn FSM must be initialized to ensure that it starts in a known state. This allows the FSM to function correctly and avoid any potential problems that could arise from an unknown starting state.",
            "Prof. Doug Jones designed the vending machine application and the FSM by first identifying the different states that the machine could be in. He then created a state diagram that showed how the machine would transition from one state to another. Finally, he implemented the FSM in hardware using the Altera Quartus software.",
            "The state table represents the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The lights are all red in both of these states, which is necessary for safety."
        ],
        "score": [
            6.235898971557617,
            -3.2665913105010986,
            -11.27925968170166
        ],
        "text": "level state, to simplify the implementation.} {Use counters to insert time-based transitions between states (such  as timeouts).} {Implement an FSM using logic components such as registers,  counters, comparators, and adders as building blocks.} {Explain the basic organization of a computer's microarchitecture as well as the role played by elements of a von Neumann design in the processing of instructions.} {Identify the stages of processing an instruction (such as fetch, decode, getting operands, execution, and writing back results) in a  processor control unit state machine diagram.}   And, at the highest level, we expect that you will be able to do the following:  {}{{}{} {}{}{}  {Explain the difference between the Moore and Mealy machine models,  as well as why you might find each of them useful when designing an FSM.} {Understand the need for initialization of an FSM, be able to analyze  and identify potential problems arising from lack of initialization, and  be able to extend an implementation to include initialization to an  appropriate state when necessary.} {Understand how the choice of internal state bits for an FSM can  affect the complexity of the implementation of next-state and output  logic, and be able to select a reasonable state assignment.} {Identify and fix design flaws in simple FSMs by analyzing an existing  implementation, comparing it with the specification, and removing any  differences by making any necessary changes to the implementation.}      {   }  empty 3rd page      {Instruction Set Architecture*}  This set of notes discusses  tradeoffs and design elements of instruction set architectures (ISAs). { The material is beyond the scope of our class, and is provided purely for your interest.}  Those who find these topics interesting may also want to read the ECE391 notes, which describe similar material with a focus on the x86 ISA.  As you know, the ISA defines the interface between software and hardware, abstracting the capabilities of a computer's datapath and standardizing the format of instructions to utilize those capabilities.  Successful ISAs are rarely discarded, as success implies the existence of large amounts of software built to use the ISA.  Rather, they are extended, and their original forms must be supported for decades (consider, for example, the IBM 360 and the Intel x86).  Employing sound design principles is thus imperative in an ISA.  {Formats and Fields*}  The LC-3 ISA  employs fixed-length instructions and a load-store architecture, two aspects that help to reduce the design space to a manageable set of choices.  In a general ISA design, many other options exist for instruction formats.  Recall the idea of separating the bits of an instruction into (possibly non-contiguous) fields.  One of the fields must contain an opcode, which specifies the type of operation to be performed by the instruction.  In the {LC-3} ISA, most opcodes specify both the type of operation and the types of arguments to the operation.   More generally, many addressing modes are possible for each operand, and we can think of the bits that specify the addressing mode as a separate field, known as the { mode} field.    As a simple example, the {LC-3's} ADD and AND instructions  contain a {1-bit} mode field that specifies whether the second operand of the ADD/AND comes from a register or is an immediate value.  Several questions must be answered in order to define the possible instruction formats for an ISA.  First, are instructions fixed-length or variable-length?  Second, how many addresses are needed for each instruction, and how many of the addresses can be memory addresses? Finally, what forms of addresses are possible for each operand? For example, can one use full memory addresses or only limited offsets relative to a register?  The answer to the first question depends on many factors, but several clear advantages exist for both answers.  { Fixed-length instructions} are easy to fetch and decode.  A"
    },
    {
        "question": "What is a multiplexer?', \"\\n",
        "answers": [
            "\nA multiplexer is a device that allows multiple signals to be routed onto a single channel. This is done by selecting one of several input signals and connecting it to the output. Multiplexers are used in a variety of applications, such as telecommunications and computer networking.",
            "A decoder is a logic circuit that converts a binary code into a set of signals that can be used to operate a device. A decoder takes an input signal, typically one coded as a binary number, and produces one output for each possible value of the signal. When the decoder is enabled (EN=1), each of the AND gates produces one minterm of the input signal S. When the decoder is not enabled (EN=0), the decoder produces all 0 bits.",
            "Tri-state buffers are used to provide logical disconnection between the tri-state buffer and the output. This logical disconnection is achieved by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table."
        ],
        "score": [
            7.5047831535339355,
            -8.39901351928711,
            -10.712653160095215
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How is a digital FSM designed from an abstract model?\\n",
        "answers": [
            "\nFrom the passage, it seems that a digital FSM is designed from an abstract model by first identifying a finite set of states, a set of possible inputs, a set of possible outputs, and a set of transition rules. Then, the FSM is implemented as a digital system by representing all states as patterns using a fixed number of bits, translating all inputs into bits, and translating all outputs into bits. Finally, transition rules are defined so that given any state of the FSM and any pattern of input bits, a transition is defined from that state to another state.",
            "Abstraction can be used to build a three-bit Gray code counter from the two-bit design by using copies of the bit-sliced adder circuit. This approach is less efficient than designing a circuit specifically for a three-bit Gray code counter, but it is simpler.",
            "A fully-specified transition diagram is a diagram that specifies all of the states that a system can be in, as well as all of the transitions between those states. This allows for a clear understanding of the behavior of the system."
        ],
        "score": [
            9.327714920043945,
            -4.5645246505737305,
            -11.200422286987305
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " How are next-state variables and output signals implemented in a FSM?\\n",
        "answers": [
            "\nA FSM is implemented as a digital system, with all states represented as patterns using a fixed number of bits, all inputs translated into bits, and all outputs translated into bits. Transition rules must be complete, meaning that given any state of the FSM and any pattern of input bits, a transition must be defined from that state to another state. Outputs for a digital FSM are calculated using Boolean logic expressions.",
            "The final state table for the FSM is given in the passage. The table shows the states of the FSM, the outputs for each state, and the duration of each state.",
            "The next-state values for S_2, S_1, and S_0 are all 1."
        ],
        "score": [
            3.3289501667022705,
            -0.9794670343399048,
            -3.3108482360839844
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " What is the design process for a counter with a number of states that is not a power of two?\\n",
        "answers": [
            "\nThe design process for a counter with a number of states that is not a power of two is to choose a representation for the states that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning.",
            "The counter's implementation diagram simplifies understanding the diagram by breaking it down into smaller, more manageable pieces. By doing this, it is easier to see how the various parts of the diagram interact with each other. Additionally, it makes it easier to identify potential problems or areas for improvement.",
            "A Gray code counter is a type of digital counter that is used to count the number of clock pulses. It is named after Frank Gray, who invented the code in 1953. The main advantage of a Gray code counter over other types of counters is that it is less likely to produce errors when counting."
        ],
        "score": [
            9.158401489257812,
            -7.475677490234375,
            -8.05925178527832
        ],
        "text": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in"
    },
    {
        "question": " How is FSM initialization solved?\\n",
        "answers": [
            "\nThe FSM is initialized to one of the six states in the desired loop. This ensures that the lights on both roads are not red when cars are waiting on one of the roads.",
            "A RAM is a type of memory that is used to store data. It is a volatile memory, which means that the data stored in the memory is erased when the power is turned off or fails. RAM is a random access memory, which means that the time required to access an element in the memory is independent of the element being accessed.",
            "The choice of representation for the FSM's internal state affects the complexity of the implementation in terms of the number of states, the number of inputs, the number of outputs, and the number of transition rules."
        ],
        "score": [
            3.693730592727661,
            -11.21430778503418,
            -3.4534082412719727
        ],
        "text": "produce it gives  F=S_1S_0{V^+{S_1}S_0{V^.  If we  ignore the two states outside of the main loop for S, the first term  is 1 only when the lights are green on the East and West roads and the  detector for the North and South roads indicates that no vehicles are  approaching.  Similarly, the second term is 1 only when the lights are  green on the North and South roads and the detector for the East and  West roads indicates that no vehicles are approaching.  What happens when F=1?  First, the OR gate feeding into the timer's LD input produces a 1, meaning that the timer loads a new value instead of counting down.  Second, the OR gate controlling the lower mux selects the A input.  In other words, the timer is reset to A cycles, corresponding to the initial value for the green light states. In other words, the light stays green until vehicles approach on  the other road, plus A more cycles.  Unfortunately, the signal F may also be 1 in the unused states of S, in which case the lights on both roads may remain red even though cars are waiting on one of the roads.  To avoid this behavior, we must be  sure to initialize the state S to one of the six states in the desired loop.    {From FSM to Computer}  The FSM designs we have explored so far have started with a human-based design process in which someone writes down the desired behavior in terms of states, inputs, outputs, and transitions.  Such an approach makes it easier to build a digital FSM, since the abstraction used corresponds almost directly to the implementation.  As an alternative, one can start by mapping the desired task into a high-level programming language, then using components such as registers, counters, and memories to implement the variables needed.  In this approach, the control structure of the code maps into a high-level FSM design. Of course, in order to implement our FSM with digital logic, we eventually still need to map down to bits and gates.  In this set of notes, we show how one can transform a piece of code written in a high-level language into an FSM.  This process is meant to help you understand how we can design an FSM that executes simple pieces of a flow chart such as assignments, { if} statements, and  loops.  Later, we generalize this concept and build an FSM that allows the pieces to be executed to be specified after the FSM is built---in  other words, the FSM executes a program specified by bits stored in  memory.  This more general model, as you might have already guessed,  is a computer.    {Specifying the Problem}  Let's begin by specifying the problem that we want to solve. Say that we want to find the minimum value in a set of 10 integers. Using the C programming language, we can write the following fragment of  code:   {  aaaa=aaaa= int >values[10];    /* 10 integers--filled in by other code */ int >idx; int >min  min = values[0]; for (idx = 1; 10 > idx; idx = idx + 1) { >  if (min > values[idx]) { >  >  min = values[idx]; >  } } /* The minimum value from array is now in min. */  }   The code uses array notation, which we have not used previously in our  class, so let's first discuss the meaning of the code.  The code uses three variables.  The variable { values} represents the 10 values in our set. The suffix ``[10]'' after the variable name tells the compiler that we want an array of 10 integers ({ int}) indexed from 0 to 9. These integers can be treated as 10 separate variables, but can be accessed using the single name ``{ values}'' along with an index (again, from 0 to 9 in this case).  The variable { idx} holds a loop index that we use to examine each of the values one by one in order to find the minimum value in the set.  Finally, the variable { min} holds the smallest known value as  the program examines each of the values in the set.  The program body consists of two statements.    We assume that some other piece of code---one not shown here---has  initialized the 10 values in our set before the code above executes.  The first"
    },
    {
        "question": " How does the choice of representation for the FSM's internal state affect the complexity of the implementation?\"]",
        "answers": [
            "\nThe choice of representation for the FSM's internal state affects the complexity of the implementation. If the states are represented as bit patterns, the transition rules must be complete. If the states are represented as abstract states, the outputs for each state must be specified.",
            "There are four states in the second version of the table. The states are A, B, C, and D. A and B are both in the ON state, while C and D are both in the OFF state.",
            "The register PRICE1 has six bits. This allows the machine to keep track of up to 3.15 (63 nickels)."
        ],
        "score": [
            9.062749862670898,
            -11.110885620117188,
            -11.287193298339844
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": "What is the first step in the design process?\\n",
        "answers": [
            "\nThe first step in the design process is to develop an abstract model. This model is a translation of the human language description into a model with states and desired behavior. The model is not particularly thorough or exact at this stage.",
            "An FSM is a finite state machine, which is a mathematical model of computation used to design both computer programs and digital logic circuits. It is an abstract machine that can be in one of a finite number of states.",
            "The main difference between a mux and a decoder is that a mux has multiple inputs and one output, while a decoder has one input and multiple outputs. A mux is used to select one of several input signals and send it to the output, while a decoder is used to interpret an input signal and produce several output signals."
        ],
        "score": [
            8.606502532958984,
            -10.482109069824219,
            -11.164901733398438
        ],
        "text": "{Finite State Machine Design Examples, Part I}  This set of notes uses a series of examples to illustrate design principles  for the implementation of finite state machines (FSMs) using digital logic. We begin with an overview of the design process for a digital FSM, from the development of an abstract model through the implementation of functions for the next-state variables and output signals. Our first few examples cover only the concrete aspects: we implement several counters, which illustrate the basic  process of translating a concrete and complete state transition diagram into an implementation based on flip-flops and logic gates. We next consider a counter with a number of states that is not a power of two, with which we illustrate the need for FSM initialization.  As part of solving the initialization problem, we also introduce   a general form of selection logic called a multiplexer.  We then consider the design process as a whole through a more general example of a counter with multiple inputs to control its behavior.  We work from an abstract model down to an implementation, illustrating how semantic knowledge from the abstract model can be used to simplify the  implementation.  Finally, we illustrate how the choice of representation for the FSM's internal state affects the complexity of the implementation. Fortunately, designs that are more intuitive and easier for humans to understand also typically make the best designs in terms of  other metrics, such as logic complexity.   {Steps in the Design Process}  Before we begin exploring designs, let's talk briefly about the general approach that we take when designing an FSM.  We follow a six-step process:{-8pt}  {{}{} {}{}{} {develop an abstract model}{step-abs} {specify I/O behavior}{step-io} {complete the specification}{step-complete} {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates} } {-8pt}  In Step {step-abs}, we translate our description in human language into a model with states and desired behavior.  At this stage, we  simply try to capture the intent of the description and are not particularly thorough nor exact.  Step {step-io} begins to formalize the model, starting with its input and output behavior.  If we eventually plan to develop an implementation of our FSM as a digital system (which is not the  only choice, of course!), all input and output must consist of bits.  Often, input and/or output specifications may need to match other digital systems to which we plan to connect our FSM.  In fact, { most problems in developing large digital systems today arise because of incompatibilities when composing two or more separately designed pieces} (or { modules}) into an integrated system.  Once we know the I/O behavior for our FSM, in Step {step-complete} we start to make any implicit assumptions clear and to make any other decisions necessary to the design.  Occasionally, we may choose to leave something undecided in the hope of simplifying the design with ``don't care'' entries in the logic formulation.  In Step {step-repn}, we select an internal representation for the bits necessary to encode the state of our FSM.  In practice, for small designs, this representation can be selected by a computer  in such a way as to optimize the implementation.  However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.  In the later examples in this set of notes, we show how even a  small design can leverage meaningful information from the design when selecting the representation, leading to an implementation that is simpler and is easier to build correctly.  We also show how one can use abstraction to simplify an implementation.  By Step {step-logic}, our design is a complete specification in terms of bits, and we need merely derive logic expressions for the next-state variables and the output signals."
    },
    {
        "question": "What is the second step in the design process?\\n",
        "answers": [
            "\nThe second step in the design process is to think carefully about metrics of possible interest. Once you have a set of metrics that you feel is complete, your next step is to get rid of as many as you can.",
            "A priority encoder is a type of encoder that assigns priority to certain input signals. It is used to determine which input signal should be given priority when multiple input signals are present.",
            "A Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all patterns in the second copy."
        ],
        "score": [
            8.662867546081543,
            -11.00396728515625,
            -8.126550674438477
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the third step in the design process?\\n",
        "answers": [
            "\nThe third step in the design process is to identify metrics that have no practical impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.",
            "A Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all patterns in the second copy.",
            "The lab simulates two-level logic and how it can be used to map an expression into CMOS gates. It also shows how to graphically manipulate expressions to understand gate diagrams."
        ],
        "score": [
            8.796653747558594,
            -10.21224308013916,
            -11.287813186645508
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the fourth step in the design process?\\n",
        "answers": [
            "\nThe fourth step in the design process is to choose a particular design from the set of possible designs. In making this choice, engineers typically consider a variety of factors, including cost, performance, manufacturability, and reliability.",
            "What is the reason for the popularity of two-level logic?\n\nThe reason for the popularity of two-level logic is that it is a simple way to express logic functions. Two-level logic is a popular way of expressing logic functions because it is a simple way to express logic functions.",
            "A decoder is a device that converts coded signals into usable information. Decoders are used in a variety of applications, such as error detection and correction."
        ],
        "score": [
            8.927321434020996,
            -11.320550918579102,
            -11.120762825012207
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the fifth step in the design process?', '\\n",
        "answers": [
            "\nThe fifth step in the design process is to select an internal representation for the bits necessary to encode the state of our FSM. In practice, for small designs, this representation can be selected by a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.",
            "A priority encoder is a type of encoder that generates a signal whenever any of its input lines are active, and encodes the index of the highest active input as a two-bit unsigned number. A priority encoder is used to determine which of its input lines has the highest priority.",
            "Logical completeness does not necessarily help with efficient design of logic functions. A single Boolean logic function can be expressed in many different ways. Learning how to develop an efficient implementation of a function as well as how to determine whether two logic expressions are identical without actually writing out truth tables are important engineering skills that students will learn in the coming months."
        ],
        "score": [
            8.44192123413086,
            -11.006202697753906,
            -10.952107429504395
        ],
        "text": "{Finite State Machine Design Examples, Part I}  This set of notes uses a series of examples to illustrate design principles  for the implementation of finite state machines (FSMs) using digital logic. We begin with an overview of the design process for a digital FSM, from the development of an abstract model through the implementation of functions for the next-state variables and output signals. Our first few examples cover only the concrete aspects: we implement several counters, which illustrate the basic  process of translating a concrete and complete state transition diagram into an implementation based on flip-flops and logic gates. We next consider a counter with a number of states that is not a power of two, with which we illustrate the need for FSM initialization.  As part of solving the initialization problem, we also introduce   a general form of selection logic called a multiplexer.  We then consider the design process as a whole through a more general example of a counter with multiple inputs to control its behavior.  We work from an abstract model down to an implementation, illustrating how semantic knowledge from the abstract model can be used to simplify the  implementation.  Finally, we illustrate how the choice of representation for the FSM's internal state affects the complexity of the implementation. Fortunately, designs that are more intuitive and easier for humans to understand also typically make the best designs in terms of  other metrics, such as logic complexity.   {Steps in the Design Process}  Before we begin exploring designs, let's talk briefly about the general approach that we take when designing an FSM.  We follow a six-step process:{-8pt}  {{}{} {}{}{} {develop an abstract model}{step-abs} {specify I/O behavior}{step-io} {complete the specification}{step-complete} {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates} } {-8pt}  In Step {step-abs}, we translate our description in human language into a model with states and desired behavior.  At this stage, we  simply try to capture the intent of the description and are not particularly thorough nor exact.  Step {step-io} begins to formalize the model, starting with its input and output behavior.  If we eventually plan to develop an implementation of our FSM as a digital system (which is not the  only choice, of course!), all input and output must consist of bits.  Often, input and/or output specifications may need to match other digital systems to which we plan to connect our FSM.  In fact, { most problems in developing large digital systems today arise because of incompatibilities when composing two or more separately designed pieces} (or { modules}) into an integrated system.  Once we know the I/O behavior for our FSM, in Step {step-complete} we start to make any implicit assumptions clear and to make any other decisions necessary to the design.  Occasionally, we may choose to leave something undecided in the hope of simplifying the design with ``don't care'' entries in the logic formulation.  In Step {step-repn}, we select an internal representation for the bits necessary to encode the state of our FSM.  In practice, for small designs, this representation can be selected by a computer  in such a way as to optimize the implementation.  However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.  In the later examples in this set of notes, we show how even a  small design can leverage meaningful information from the design when selecting the representation, leading to an implementation that is simpler and is easier to build correctly.  We also show how one can use abstraction to simplify an implementation.  By Step {step-logic}, our design is a complete specification in terms of bits, and we need merely derive logic expressions for the next-state variables and the output signals."
    },
    {
        "question": "What is the first step in the design process?\\n",
        "answers": [
            "\nThe first step in the design process is to develop an abstract model. This model captures the intent of the description and is not particularly thorough nor exact.",
            "A bad state representation can be fixed by using a code with a larger Hamming distance. This will enable the system to detect and correct more errors.",
            "The value of register PRICE3 is the price of the third item in the machine, which is determined by the owner of the machine."
        ],
        "score": [
            8.81238079071045,
            -11.32394027709961,
            -11.263495445251465
        ],
        "text": "{Finite State Machine Design Examples, Part I}  This set of notes uses a series of examples to illustrate design principles  for the implementation of finite state machines (FSMs) using digital logic. We begin with an overview of the design process for a digital FSM, from the development of an abstract model through the implementation of functions for the next-state variables and output signals. Our first few examples cover only the concrete aspects: we implement several counters, which illustrate the basic  process of translating a concrete and complete state transition diagram into an implementation based on flip-flops and logic gates. We next consider a counter with a number of states that is not a power of two, with which we illustrate the need for FSM initialization.  As part of solving the initialization problem, we also introduce   a general form of selection logic called a multiplexer.  We then consider the design process as a whole through a more general example of a counter with multiple inputs to control its behavior.  We work from an abstract model down to an implementation, illustrating how semantic knowledge from the abstract model can be used to simplify the  implementation.  Finally, we illustrate how the choice of representation for the FSM's internal state affects the complexity of the implementation. Fortunately, designs that are more intuitive and easier for humans to understand also typically make the best designs in terms of  other metrics, such as logic complexity.   {Steps in the Design Process}  Before we begin exploring designs, let's talk briefly about the general approach that we take when designing an FSM.  We follow a six-step process:{-8pt}  {{}{} {}{}{} {develop an abstract model}{step-abs} {specify I/O behavior}{step-io} {complete the specification}{step-complete} {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates} } {-8pt}  In Step {step-abs}, we translate our description in human language into a model with states and desired behavior.  At this stage, we  simply try to capture the intent of the description and are not particularly thorough nor exact.  Step {step-io} begins to formalize the model, starting with its input and output behavior.  If we eventually plan to develop an implementation of our FSM as a digital system (which is not the  only choice, of course!), all input and output must consist of bits.  Often, input and/or output specifications may need to match other digital systems to which we plan to connect our FSM.  In fact, { most problems in developing large digital systems today arise because of incompatibilities when composing two or more separately designed pieces} (or { modules}) into an integrated system.  Once we know the I/O behavior for our FSM, in Step {step-complete} we start to make any implicit assumptions clear and to make any other decisions necessary to the design.  Occasionally, we may choose to leave something undecided in the hope of simplifying the design with ``don't care'' entries in the logic formulation.  In Step {step-repn}, we select an internal representation for the bits necessary to encode the state of our FSM.  In practice, for small designs, this representation can be selected by a computer  in such a way as to optimize the implementation.  However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.  In the later examples in this set of notes, we show how even a  small design can leverage meaningful information from the design when selecting the representation, leading to an implementation that is simpler and is easier to build correctly.  We also show how one can use abstraction to simplify an implementation.  By Step {step-logic}, our design is a complete specification in terms of bits, and we need merely derive logic expressions for the next-state variables and the output signals."
    },
    {
        "question": "What is the second step in the design process?\\n",
        "answers": [
            "\nThe second step in the design process is to think carefully about metrics of possible interest. Once you have a set of metrics that you feel is complete, your next step is to get rid of as many as you can.",
            "The vending machine FSM is designed to control the release of items from the vending machine based on the user's purchase request. The FSM uses a priority encoder to determine which item the user has requested, and a decoder to generate the individual item release signals.",
            "The next-state values for S_2, S_1, and S_0 are given in the table to the right. The states of S act like a counter, and by examining the connections, we can derive equations for the next state and draw a transition diagram. There are six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle."
        ],
        "score": [
            8.662867546081543,
            -11.057762145996094,
            -11.044590950012207
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the third step in the design process?\\n",
        "answers": [
            "\nThe third step in the design process is to evaluate the design using multiple metrics. This includes identifying metrics that have no practical impact in current technology, setting threshold values for other metrics to simplify reasoning about them, eliminating redundant metrics, calculating linear sums to reduce the count of metrics, and making use of the notion of Pareto optimality.",
            "DRAM is a type of memory that is slower than SRAM but denser (has more bits per chip area). DRAM cells use only a single transistor, while SRAM cells use six transistors. DRAM designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor count.",
            "The purpose of the FSM is to control the stoplights at a busy intersection. The FSM ensures that the lights on one or both roads are red at all times, that a road has a green light, and that the controller should show a yellow light before showing a red light."
        ],
        "score": [
            8.889129638671875,
            -11.175655364990234,
            -11.164475440979004
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the fourth step in the design process?\\n",
        "answers": [
            "\nThe fourth step in the design process is to choose a particular design from the set of possible designs. This choice is made based on a number of factors, including the objectives of the design, the constraints of the problem, and the preferences of the designer.",
            "The FSM needs to be initialized by external logic filling the memory (the array of integer values), executing the FSM ``code,'' and then checking the answer.  To support this use model, we create a FSM state called { WAIT} for cycles in which the FSM has no work to do. Later, we also make use of an external input signal { START} to start the FSM execution.",
            "The purpose of the FSM is to control the stoplights at a busy intersection. The FSM ensures that the lights are always red on one or both roads, that a yellow light is shown before a green light, and that the green lights are alternated between the two roads."
        ],
        "score": [
            9.170330047607422,
            -11.196967124938965,
            -11.163407325744629
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the fifth step in the design process?', \"\\n",
        "answers": [
            "\nThe fifth step in the design process is to select an internal representation for the bits necessary to encode the state of the FSM. In practice, for small designs, this representation can be selected by a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.",
            "A bad state representation can lead to a number of problems, including wasted engineering effort, poor performance, and even product cancellations.",
            "Adding a third bit to a Gray code counter increases design complexity because it requires an additional carry bit. This carry bit must be propagated through the adder bit slice logic, which increases the number of logic gates required and the overall complexity of the design."
        ],
        "score": [
            8.278159141540527,
            -11.223803520202637,
            -9.653181076049805
        ],
        "text": "{Finite State Machine Design Examples, Part I}  This set of notes uses a series of examples to illustrate design principles  for the implementation of finite state machines (FSMs) using digital logic. We begin with an overview of the design process for a digital FSM, from the development of an abstract model through the implementation of functions for the next-state variables and output signals. Our first few examples cover only the concrete aspects: we implement several counters, which illustrate the basic  process of translating a concrete and complete state transition diagram into an implementation based on flip-flops and logic gates. We next consider a counter with a number of states that is not a power of two, with which we illustrate the need for FSM initialization.  As part of solving the initialization problem, we also introduce   a general form of selection logic called a multiplexer.  We then consider the design process as a whole through a more general example of a counter with multiple inputs to control its behavior.  We work from an abstract model down to an implementation, illustrating how semantic knowledge from the abstract model can be used to simplify the  implementation.  Finally, we illustrate how the choice of representation for the FSM's internal state affects the complexity of the implementation. Fortunately, designs that are more intuitive and easier for humans to understand also typically make the best designs in terms of  other metrics, such as logic complexity.   {Steps in the Design Process}  Before we begin exploring designs, let's talk briefly about the general approach that we take when designing an FSM.  We follow a six-step process:{-8pt}  {{}{} {}{}{} {develop an abstract model}{step-abs} {specify I/O behavior}{step-io} {complete the specification}{step-complete} {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates} } {-8pt}  In Step {step-abs}, we translate our description in human language into a model with states and desired behavior.  At this stage, we  simply try to capture the intent of the description and are not particularly thorough nor exact.  Step {step-io} begins to formalize the model, starting with its input and output behavior.  If we eventually plan to develop an implementation of our FSM as a digital system (which is not the  only choice, of course!), all input and output must consist of bits.  Often, input and/or output specifications may need to match other digital systems to which we plan to connect our FSM.  In fact, { most problems in developing large digital systems today arise because of incompatibilities when composing two or more separately designed pieces} (or { modules}) into an integrated system.  Once we know the I/O behavior for our FSM, in Step {step-complete} we start to make any implicit assumptions clear and to make any other decisions necessary to the design.  Occasionally, we may choose to leave something undecided in the hope of simplifying the design with ``don't care'' entries in the logic formulation.  In Step {step-repn}, we select an internal representation for the bits necessary to encode the state of our FSM.  In practice, for small designs, this representation can be selected by a computer  in such a way as to optimize the implementation.  However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.  In the later examples in this set of notes, we show how even a  small design can leverage meaningful information from the design when selecting the representation, leading to an implementation that is simpler and is easier to build correctly.  We also show how one can use abstraction to simplify an implementation.  By Step {step-logic}, our design is a complete specification in terms of bits, and we need merely derive logic expressions for the next-state variables and the output signals."
    },
    {
        "question": " How does one use abstraction to simplify an implementation?\\n",
        "answers": [
            "\nAbstraction is the process of simplifying a complex system by identifying the most important aspects of that system and ignoring the rest. In the context of implementation, abstraction can be used to simplify a design by identifying the most important features of the design and ignoring the rest. This can make the design more manageable and easier to understand.",
            "A tri-state buffer is a type of digital logic device that is used to either connect or disconnect a signal line from a power source or ground. This is achieved by using two transistors, one to connect the signal line to the power source, and the other to connect the signal line to ground. When the buffer is enabled, both transistors are turned on, and the signal line is connected to either the power source or ground, depending on the value of the input signal. When the buffer is disabled, both transistors are turned off, effectively disconnecting the signal line from the power source or ground.",
            "PASSAGE - compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column"
        ],
        "score": [
            7.1267924308776855,
            -10.966313362121582,
            -9.812973976135254
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " Why is it important to make implicit assumptions clear in the design process?\\n",
        "answers": [
            "\nThere are a few reasons why it is important to make implicit assumptions clear in the design process. First, if assumptions are not made clear, it can lead to confusion and misunderstanding among the team members working on the design. Second, making assumptions explicit can help to ensure that everyone is on the same page and working towards the same goal. Finally, explicit assumptions can help to prevent errors and mistakes in the design process.",
            "From the passage, it seems that a digital FSM is designed from an abstract model by first identifying a finite set of states, a set of possible inputs, a set of possible outputs, and a set of transition rules. Then, the FSM is implemented as a digital system by representing all states as patterns using a fixed number of bits, translating all inputs into bits, and translating all outputs into bits. Finally, transition rules are defined so that given any state of the FSM and any pattern of input bits, a transition is defined from that state to another state.",
            "The input behavior of the machine is based on the type of coin inserted and the selection of an item for purchase. If the user inserts a coin that is not a nickel, the machine will reject the coin and remain in the same state. If the user inserts a nickel, the machine will transition to the next state and accept the coin. If the user selects an item for purchase, the machine will release the item if the user has inserted enough money. Otherwise, the machine will remain in the same state."
        ],
        "score": [
            9.865188598632812,
            -10.652261734008789,
            -11.193721771240234
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": " How does the choice of state representation affect the implementation of an FSM?\\n",
        "answers": [
            "\nThe choice of state representation affects the implementation of an FSM in terms of the number of gates and inverters required, as well as the delay of the overall design. A serial comparator design based on the 2-bit slice variant, for example, requires 24 gates and 10 inverters, which is not much larger than the earlier serial design. In terms of gate delays, however, the new design is identical, meaning that comparisons are finished in half the time. More realistic area and timing metrics show slightly more difference between the two designs. These differences can dominate the results if the design is not carefully considered.",
            "One way to verify that an implementation is correct is to test it against a known set of inputs and compare the results to the expected results. If the results match, then the implementation is likely correct. Another way to verify an implementation is to examine the code and look for errors. This can be done manually or with the help of a software tool.",
            "The input sequence that is used to initialize the FSM is a sequence of 0s and 1s that are input into the FSM in order to set the state of the FSM to the desired state."
        ],
        "score": [
            8.410383224487305,
            -9.803227424621582,
            -0.4375646710395813
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " How does the choice of an FSM's input and output behavior affect the design process?\\n",
        "answers": [
            "\nThe choice of an FSM's input and output behavior affects the design process by determining the set of possible states and the set of possible outputs. The set of possible states is determined by the set of possible inputs, and the set of possible outputs is determined by the set of possible outputs.",
            "The light should be red on one or both roads at all times. If a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. Finally, for fairness, the controller should alternate green lights between the two roads.",
            "Prof. Doug Jones designed the vending machine application and the FSM by first identifying the different states that the machine could be in. He then created a state diagram that showed how the machine would transition from one state to another. Finally, he implemented the FSM in hardware using the Altera Quartus software."
        ],
        "score": [
            9.441426277160645,
            -11.255949974060059,
            -4.449169158935547
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " Why is it important for the components of a digital system to be compatible?\"]",
        "answers": [
            "\nOne important reason for the compatibility of components in a digital system is that it helps to ensure that the system will function correctly. If the components are not compatible, the system may not work correctly, or may not work at all. Additionally, compatibility among components can help to reduce the cost of manufacturing the system, as well as the cost of maintaining and repairing the system.",
            "A tri-state buffer is a type of digital logic device that is used to either connect or disconnect a signal line from a power source or ground. This is achieved by using two transistors, one to connect the signal line to the power source, and the other to connect the signal line to ground. When the buffer is enabled, both transistors are turned on, and the signal line is connected to either the power source or ground, depending on the value of the input signal. When the buffer is disabled, both transistors are turned off, effectively disconnecting the signal line from the power source or ground.",
            "The FSM handles multiple input events in a single clock cycle by first storing the input events in a queue. When the FSM is ready to process an input event, it dequeues the event from the queue and processes it."
        ],
        "score": [
            9.901329040527344,
            -8.813638687133789,
            -10.639896392822266
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What is a Gray code?\\n",
        "answers": [
            "\nA Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all patterns in the second copy.",
            "The AND gate on the left is used to determine if all of the input values are 1. If all of the input values are 1, then the output of the AND gate will be 1. If any of the input values are 0, then the output of the AND gate will be 0.",
            "The final state table and state transition diagram appear as follows: \n\nThe final state table lists the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The table also includes the states outside of the desired loop. The lights are all red in both of these states, which is necessary for safety.\n\nThe state transition diagram shows the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The diagram also shows the duration of each state."
        ],
        "score": [
            8.471837997436523,
            -11.163915634155273,
            -11.248102188110352
        ],
        "text": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more"
    },
    {
        "question": " What is the purpose of a Gray code counter?\\n",
        "answers": [
            "\nA Gray code counter is a device that is used to count the number of operations in a logic expression. The device works by counting the number of variables in the expression and adding the number of operations. The device is used to optimize logic expressions by reducing the number of operations needed to implement the logic function.",
            "The output of the FSM looks like a green light followed by a yellow light followed by a red light.",
            "One builds memories (static RAMs) using logic by connecting a dual-inverter loop to opposing BIT lines through transistors controlled by a SELECT line. When SELECT is high, the transistors connect the inverter loop to the bit lines. When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input. When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs."
        ],
        "score": [
            7.733799457550049,
            -11.280604362487793,
            -11.172005653381348
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": " What are the inputs to a Gray code counter?\\n",
        "answers": [
            "\nThe inputs to a Gray code counter are the current state of the counter and the clock signal. The clock signal triggers the state change of the counter.",
            "Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created both the wiki documentation and the Altera Quartus portions of the lab. Prof. Kindratenko also helped to scale the design in a way that made it possible to deliver to the over 400 students entering ECE every semester.",
            "The state table represents the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The lights are all red in both of these states, which is necessary for safety."
        ],
        "score": [
            8.859431266784668,
            -11.312015533447266,
            -11.306199073791504
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the transition diagram for a two-bit Gray code counter?\\n",
        "answers": [
            "\nThe transition diagram for a two-bit Gray code counter is a loop with the counter moving from one state to the next each cycle. Each state in the diagram is marked with the internal state value S_1S_0 (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are always equal for this counter.",
            "What is the difference between SOP and POS?\n\nSOP and POS are two forms of two-level logic. In SOP, the output is true if any of the inputs are true. In POS, the output is true if all of the inputs are true.",
            "If A were to rise with T and fall with the rising clock edge, then the processor would not be able to recognize interrupts between instructions. This would cause the program to terminate unexpectedly."
        ],
        "score": [
            9.81413459777832,
            -11.313013076782227,
            -11.333064079284668
        ],
        "text": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems"
    },
    {
        "question": " How do you develop the implementation for a Gray code counter?', '",
        "answers": [
            "\nThe implementation for a Gray code counter is developed by first filling in the K-maps for the next-state values, then deriving algebraic expressions. The next-state logic is then used to develop the implementation.",
            "The first step in the design process is to develop an abstract model. This model captures the intent of the description and is not particularly thorough nor exact.",
            "DRAM is a type of capacitive storage, where a bit is stored by charging or not charging a capacitor. The capacitor is attached to a bit line through a transistor controlled by a select line. When the select line is high, the capacitor is isolated and holds its charge. However, the transistor's resistance is finite, and some charge leaks out onto the bit line. Charge also leaks into the substrate on which the transistor is constructed."
        ],
        "score": [
            9.84968376159668,
            -10.715994834899902,
            -11.25987434387207
        ],
        "text": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems"
    },
    {
        "question": " What are the inputs to the counter?\\n",
        "answers": [
            "\nThe inputs to the counter are the coins that are inserted into the machine. The machine will track the value of the coins in terms of nickels and will reject any coins that are inserted that would cause the value to exceed the maximum value that the machine can track.",
            "The second optical sensor is used to detect when a chip finishes its work so that the chip can turn itself off and save energy. This metric is used to guide the process of optimizing logic expressions so that a balance can be struck between the different metrics involved.",
            "The purpose of adding a few AND gates and a input is to create a logic function that is more efficient in terms of area and power. This is done by reducing the number of variables in the expression."
        ],
        "score": [
            6.884878158569336,
            -11.29013442993164,
            -8.73259162902832
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What is the purpose of a Gray code counter?\\n",
        "answers": [
            "\nA Gray code counter is a type of digital counter that is used to count the number of clock pulses. It is named after Frank Gray, who invented the code in 1953. The main advantage of a Gray code counter over other types of counters is that it is less likely to produce errors when counting.",
            "The input sequence that is used to initialize the FSM is a sequence of 0s and 1s that are input into the FSM in order to set the state of the FSM to the desired state.",
            "What is the difference between SOP and POS?\n\nSOP (Sum of Products) is a Boolean algebra expression that is the OR of ANDed terms, while POS (Product of Sums) is an expression that is the AND of ORed terms."
        ],
        "score": [
            7.573708534240723,
            -10.92538833618164,
            -11.30513858795166
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": " What is the difference between a Gray code counter and a regular counter?\\n",
        "answers": [
            "\nA Gray code counter is a type of counter that uses a Gray code, which is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit. A regular counter is a type of counter that does not use a Gray code.",
            "Pressing the ``halt'' button does nothing when the counter is halted. The counter remains in the same state until the ``go'' button is pressed.",
            "The user purchases an item by inserting either a dime followed by a quarter, or a quarter followed by a dime. If a user inserts a second dime, the machine will reject the coin. If a user inserts a second quarter, the machine will accept the coin and the user will have paid the full price of 35 cents."
        ],
        "score": [
            8.718578338623047,
            -9.575849533081055,
            -11.195174217224121
        ],
        "text": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems"
    },
    {
        "question": " How does the counter move from one state to the next?\\n",
        "answers": [
            "\nThe timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle. The states of S thus act like a counter.",
            "Adding a third bit to a Gray code counter increases design complexity because it requires an additional carry bit. This carry bit must be propagated through the adder bit slice logic, which increases the number of logic gates required and the overall complexity of the design.",
            "The timer is set to 0, and the state of the FSM is changed to the next state in the sequence. The new state of the FSM determines the duration of the next state, and the timer is set accordingly."
        ],
        "score": [
            0.33571308851242065,
            -10.410524368286133,
            -2.7160089015960693
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the output of the counter?', '\\n",
        "answers": [
            "\nThe output of the counter is the number of successful conversions made by the scanf function.",
            "Abstraction can be used to build a three-bit Gray code counter from the two-bit design by using copies of the bit-sliced adder circuit. This approach is less efficient than designing a circuit specifically for a three-bit Gray code counter, but it is simpler.",
            "There are two input bits."
        ],
        "score": [
            7.433904647827148,
            -7.534348487854004,
            -8.867444038391113
        ],
        "text": "&a, &b);   /* These have the */} > { scanf (\"d d\", &a, &b);  /* same effect.   */} effect: > try to convert two integers typed in decimal to > 2's complement and store the results in { a} and { b}  > { scanf (\"cx lf\", &c, &u, &d);} effect: > try to read an ASCII character into { c}, a value > typed in hexadecimal into { u}, and a double- > precision > floating-point number into { d}  > { scanf (\"lf f\", &d, &f);} effect: > try to read two real numbers typed as decimal, > convert the first to double-precision and store it  > in { d}, and convert the second to single-precision  > and store it in { f}     ple.  The spaces are ignored by { scanf}.  However, { any non-space characters in the format string must be typed exactly by the user!}  The remaining arguments to { scanf} specify memory addresses where the function can store the converted values.    The ampersand (``&'') in front of each variable name in the examples is an operator that returns the address of a variable in memory.  For each con-   {|c|l|} escape  &                          sequence& { scanf} function's conversion to bits  { c}& store one ASCII character (as { char}) { d}& convert decimal integer to 2's complement { f}& convert decimal real number to float { lf}& convert decimal real number to double { u}& convert decimal integer to unsigned int { x}& convert hexadecimal integer to unsigned int { X}& (as above)     version in the format string, the { scanf} function tries to convert input from the user into the appropriate result, then stores the result in memory at the address given by the next argument.  The programmer is responsible for ensuring that the number of  conversions in the format string matches the number of arguments provided (not counting the format string itself).  The programmer must also ensure that the type of information produced by each conversion can be stored at the address passed for that conversion---in other words, the address of a variable with the correct type must be provided.  Modern compilers often detect missing { &} operators and incorrect variable types, but many only give warnings to the programmer.  The { scanf} function itself cannot tell whether the arguments given to it are valid or not.  If a conversion fails---for example, if a user types ``hello'' when { scanf} expects an integer---{ scanf} does not overwrite the corresponding variable and immediately stops trying to convert input.  The { scanf} function returns the number of successful  conversions, allowing a programmer to check for bad input from the user.  {Types of Statements in C}  Each statement in a C program specifies a complete operation.  There are three types of statements, but two of these types can be constructed from additional statements, which can in turn be constructed from additional statements.  The C language specifies no bound on this type of recursive construction, but code  readability does impose a practical limit.   The three types are shown to the right. They are the { null statement},  { simple statements},  and { compound statements}.  A null statement is just a semicolon, and a compound statement  is just a sequence of statements surrounded by braces.  Simple statements can take several forms.  All of the examples shown to the right, including the call to { printf}, are simple state-   {  aaaa=aaaaaaaaaaaaaa=/* =aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa= ;   > > /* >a null statement (does nothing) >*/  A = B; > > /*  >examples of simple statements >*/ printf (\"Hello, world!n\");  {    > > /* > a compound statement >*/  >  C = D; > /* > (a sequence of statements >*/ >  N = 4; > /* > between braces) >*/  >  L = D - N; }  } {-2pt}   ments consisting of a C expression followed by a  semicolon.  Simple statements can also consist of conditionals or iterations, which we introduce next.  Remember that after variable declarations, the { main} function contains a sequence of statements.  These statements are executed one at a time in the order given in the"
    },
    {
        "question": " How does a Gray code counter work?\\n",
        "answers": [
            "\nA Gray code counter is a type of counter that cycles through all of the possible bit patterns of a certain length. The states in the transition diagram for a Gray code counter form a loop, with the counter moving from one state to the next each cycle. Each state in the diagram is marked with the internal state value and the output value, which are always equal for this counter.",
            "There are two possible outcomes when the user presses the ``go'' button while in state { HALT A}. The first outcome is that the FSM moves to state { COUNT B}, which is the next state in the sequence. The second outcome is that the FSM resets to state { COUNT A}, which is the initial state.",
            "The final state table for the FSM is given in the passage. The table shows the states of the FSM, the outputs for each state, and the duration of each state."
        ],
        "score": [
            7.530831813812256,
            -10.858003616333008,
            -11.331605911254883
        ],
        "text": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems"
    },
    {
        "question": " What are the benefits of using a Gray code counter?\\n",
        "answers": [
            "\nThere are a few benefits of using a Gray code counter. First, it is easy to design and implement. Second, it is easy to check the correctness of the logic. Third, it is easy to read and understand the logic.",
            "A write operation on a SRAM cell is performed by bringing the bit line to an intermediate voltage level, then raising the SELECT line, which allows the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line. The resulting change in voltage is then detected by a sense amplifier at the end of the bit line.",
            "If A were to rise with T and fall with the rising clock edge, then the processor would not be able to recognize interrupts between instructions. This would cause the program to terminate unexpectedly."
        ],
        "score": [
            8.764683723449707,
            -11.235624313354492,
            -11.287607192993164
        ],
        "text": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems"
    },
    {
        "question": " What are some applications of a Gray code counter?\\n",
        "answers": [
            "\nA Gray code counter is used in applications where a digital value must be changed incrementally with the least number of steps. This is because a Gray code counter only changes one bit at a time, which minimizes the number of digital values that must be changed.",
            "One way to make use of the internal reset lines on the latches is to force each flip-flop into the 0 state. This can be done by connecting the reset lines of the latches to a 0 voltage source.",
            "The HOLD variable is used to save energy. When a chip finishes its work, it can turn itself off, saving energy. This variable allows the chip to turn itself off when it is finished, saving energy."
        ],
        "score": [
            7.063650131225586,
            -11.30463981628418,
            -11.217514991760254
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": " What are the inputs and outputs of a Gray code counter?\\n",
        "answers": [
            "\nA Gray code counter is a type of digital counter that uses a Gray code to generate its output. The input to a Gray code counter is a digital signal that represents the number of clock pulses that have been applied to the counter. The output of a Gray code counter is a digital signal that represents the number of clock pulses that have been applied to the counter, plus one.",
            "A Karnaugh map is a graphical representation of a Boolean function that helps to simplify the function. The map consists of a grid of squares, each of which corresponds to a possible combination of input values. The map is used to find groups of product terms that are implicants of the function.",
            "A row is a natural basis for the refresh cycle because the DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An output enable input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present."
        ],
        "score": [
            7.726411819458008,
            -10.963542938232422,
            -9.271041870117188
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How do you design a Gray code counter?']",
        "answers": [
            "\nA Gray code counter is designed by first creating a transition diagram that shows the states and outputs of the counter. Next, the next-state logic is developed by creating K-maps for the next-state values and deriving algebraic expressions. Finally, the next-state logic is implemented in a diagram.",
            "In the mid-1980's, the VAX architecture dominated the workstation and minicomputer markets, which included most universities.  Digital Equipment Corporation, the creator of the VAX, was second only to IBM in terms of computer sales.  VAXen, as the machines were called, used microprogrammed control units and supported numerous addressing modes as well as complex instructions ranging from ``square root'' to ``find roots of polynomial equation.''  The impact of increasingly dense integrated circuit technology had begun to have its effect, however, and in view of increasing processor clock speeds, more and more programmers were using high-level languages rather than writing assembly code.  Although assembly programmers often made use of the complex VAX instructions, compilers were usually unable to recognize the corresponding high-level language constructs and thus were unable to make use of the instructions.\n\nThe VAX architecture was developed by Digital Equipment Corporation in the mid-1980's. The architecture dominated the workstation and minicomputer markets at the time and was second only to IBM in terms of computer sales. The VAX architecture used microprogrammed control units and supported numerous addressing modes as well as complex instructions.",
            "The next state value for S0 is 0. This is because the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes. The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1. The top mux selects B when S_0=1, and selects C when S_0=0."
        ],
        "score": [
            9.573844909667969,
            -11.233945846557617,
            -11.150016784667969
        ],
        "text": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems"
    },
    {
        "question": " What is the internal state value for the third bit gray code counter?\\n",
        "answers": [
            "\nThe internal state value for the third bit gray code counter is S_2S_1S_0=001. This is because when HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired.",
            "The benefits of using DRAM are that it is denser than SRAM and uses fewer transistors. This makes DRAM ideal for applications where size is more important than speed.",
            "The output of the counter is the number of successful conversions made by the scanf function."
        ],
        "score": [
            9.75517463684082,
            -11.285446166992188,
            -10.134869575500488
        ],
        "text": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below"
    },
    {
        "question": " What is the output for the third bit gray code counter?\\n",
        "answers": [
            "\nThe output for the third bit gray code counter would be either 0 or 1. If the input is 0, the output is 1. If the input is 1, the output is 0.",
            "There are 63 possible states for the PAID state. This is because the machine can only track up to 3.15 dollars worth of nickels, which is 63 nickels.",
            "There are two possible ways that the user could press both buttons while in state { HALT A}. The first way is that the user could press the \"halt\" button and then the \"go\" button. In this case, the FSM would move to state { HALT A} and then to state { COUNT B}. The second way is that the user could press the \"go\" button and then the \"halt\" button. In this case, the FSM would move to state { COUNT B} and then to state { HALT A}."
        ],
        "score": [
            9.414352416992188,
            -11.330533981323242,
            -11.243547439575195
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": " What is the next state value for S2?\\n",
        "answers": [
            "\nThe next state value for S2 is 0. This is because the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes. The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.",
            "Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created both the wiki documentation and the Altera Quartus portions of the lab. Prof. Kindratenko also helped to scale the design in a way that made it possible to deliver to the over 400 students entering ECE every semester.",
            "What are the three ranges for a given metric?\n\nThe first range is all measurement values are equivalently useful. The second range is possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice."
        ],
        "score": [
            8.723152160644531,
            -11.295075416564941,
            -10.948366165161133
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the next state value for S1?\\n",
        "answers": [
            "\nThe next state value for S1 is 1. This is because the value of S1 is determined by the value of S0, and the value of S0 is 1.",
            "The output behavior is that the add3 procedure takes three integers as arguments, adds them together, and returns the sum. The procedure is called with the constants 10, 20, and 30, and the result is printed. By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.",
            "The state representation for the lab FSM is a 3-bit binary number, with the following meaning:\n\nS_2: type of last coin inserted (0 for dime, 1 for quarter)\nS_1: more than one quarter inserted? (1 for yes, 0 for no)\nS_0: more than one dime inserted? (1 for yes, 0 for no)"
        ],
        "score": [
            8.660791397094727,
            -9.561483383178711,
            -3.846830368041992
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the next state value for S0?', '\\n",
        "answers": [
            "\nThe next state value for S0 is 0. This is because the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes. The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1. The top mux selects B when S_0=1, and selects C when S_0=0.",
            "The value of register PRICE3 is the price of the third item in the machine, which is determined by the owner of the machine.",
            "Each bit pattern represents a column in an addition problem. The carry in bit (C_) is added to the A and B bits to produce a sum bit (S_) and a carry out bit (C_)."
        ],
        "score": [
            7.992840766906738,
            -9.963630676269531,
            -11.170088768005371
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the fully-specified transition diagram for a three-bit Gray code counter?\\n",
        "answers": [
            "\nThe fully-specified transition diagram for a three-bit Gray code counter is as follows:\n\nThe states of the counter form a loop, with the counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_2S_1S_0 (before \" / \") and the output Z_2Z_1Z_0 (after \" / \").\n\nBased on the transition diagram, the next-state values S_2^+, S_1^+, and S_0^+ can be filled in as shown.",
            "The fully-specified transition diagram for a three-bit Gray code counter is as follows:\n\nThe states of the counter form a loop, with the counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_2S_1S_0 (before \" / \") and the output Z_2Z_1Z_0 (after \" / \").\n\nBased on the transition diagram, the next-state values S_2^+, S_1^+, and S_0^+ can be filled in as shown.",
            "There are two input bits."
        ],
        "score": [
            9.505402565002441,
            9.505402565002441,
            -10.111248970031738
        ],
        "text": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems"
    },
    {
        "question": " What are the next-state values for a three-bit Gray code counter?\\n",
        "answers": [
            "\nThe next-state values for a three-bit Gray code counter are as follows: S_2^+=0, S_1^+=S_0, and S_0^+={{S_1}}.",
            "The state of the FSM has been split into two pieces: a {3-bit} register S and a {6-bit} timer. The timer is simply a binary counter that counts downward and produces an output of Z=1 when it reaches 0. Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.\n\nWe can thus think of transitions in the FSM on a cycle by cycle basis as consisting of two types. The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S for some number of cycles.",
            "The FSM is implemented using registers, counters, and a comparator. The comparator is used to implement the if statement in the program. The registers and counters are used to implement the variables idx and min in the program. The memory is used to implement the array values in the program."
        ],
        "score": [
            9.650566101074219,
            0.5764226913452148,
            -10.350320816040039
        ],
        "text": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below"
    },
    {
        "question": " What is the equation for the next-state logic of a three-bit Gray code counter?\\n",
        "answers": [
            "\nThe next-state logic for a three-bit Gray code counter is as follows:\n\nS_2^+ = S_2 + S_0\nS_1^+ = S_2\nS_0^+ = S_2 S_1",
            "There are two input bits.",
            "The addressability of a computer memory is the number of bits that can be stored in each element of the memory. The size of the elements in a memory can vary from a single binary digit, or bit, to a byte (8 bits) or more. Typically, data elements larger than a byte are referred to as words, but the size of a word depends on context."
        ],
        "score": [
            9.406353950500488,
            -9.608009338378906,
            -11.058740615844727
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the implementation diagram for a three-bit Gray code counter?\\n",
        "answers": [
            "\nThe implementation diagram for a three-bit Gray code counter is a logic diagram that shows the inputs, outputs, and next-state logic for the counter. The diagram is ordered from left to right as {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2, which makes it easy to read and check for correctness.",
            "One can design an FSM that executes simple pieces of a flow chart by breaking down the program into smaller steps and mapping each step to a state in the FSM. One can also use registers and counters to implement variables in the program, and use a comparator to implement the test for the if statement.",
            "Prof. Doug Jones designed the vending machine application and the FSM by first identifying the different states that the machine could be in. He then created a state diagram that showed how the machine would transition from one state to another. Finally, he implemented the FSM in hardware using the Altera Quartus software."
        ],
        "score": [
            9.831977844238281,
            -8.966301918029785,
            -10.82048511505127
        ],
        "text": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems"
    },
    {
        "question": " How are the vertical lines carrying the current state values and their inverses back to the next state logic inputs ordered in a three-bit Gray code counter?', \"\\n",
        "answers": [
            "\nThe vertical lines carrying the current state values and their inverses back to the next state logic inputs are ordered from left to right (on the left side of the figure) as {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.",
            "The output of the OR gate is 1 when the signal F is 1. F is 1 when the state of the FSM is S_0S_1S_2=001 or S_0S_1S_2=011.",
            "The states in the transition diagram represent the different colors of the stoplight. The first two states are red, followed by yellow, and then green. The last two states are again red."
        ],
        "score": [
            8.277690887451172,
            -10.599602699279785,
            -10.450603485107422
        ],
        "text": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems"
    },
    {
        "question": " How does adding a third bit to a Gray code counter increase design complexity?\\n\\n",
        "answers": [
            "\nAdding a third bit to a Gray code counter increases design complexity because it requires an additional carry bit. This carry bit must be propagated through the adder bit slice logic, which increases the number of logic gates required and the overall complexity of the design.",
            "If the ``halt'' button is pressed (HG=10), the counter remains in the halted state. If both buttons are pressed (HG=11), the counter also remains in the halted state.",
            "Multiple chips can be used to construct wider memories by connecting the address, write enable, and chip select inputs to both chips. The data inputs and outputs are separate, with the left chip handling the high N bits of input on writes and producing the high N bits of output on reads, while the right chip handles the low N bits of input and produces the low N bits of output."
        ],
        "score": [
            9.154359817504883,
            -10.390222549438477,
            -9.339301109313965
        ],
        "text": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is"
    },
    {
        "question": " How can you reduce design complexity by making use of common algebraic terms and sub-expressions?\\n\\n",
        "answers": [
            "\nOne way to reduce design complexity is to make use of common algebraic terms and sub-expressions. This can help to simplify the design and make it easier to understand. Another way to reduce complexity is to use DeMorgan's laws to simplify the expression. This can help to reduce the number of gates needed and make the design more efficient.",
            "The FSM shows a yellow light before showing a red light by setting the timer to a specific value when the state of the FSM changes. This value is set by the signal F, which is calculated at the bottom of the FSM logic diagram.",
            "The FSM ensures that the lights on one or both roads are red at all times by keeping track of the state of the light. If the light is green, it will turn yellow and then red. If the light is red, it will stay red."
        ],
        "score": [
            8.863962173461914,
            -11.106683731079102,
            -11.140506744384766
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " Why is it important to have a reasonable order when designing any logic diagram?\\n\\n",
        "answers": [
            "\nIt is important to have a reasonable order when designing any logic diagram because it can help simplify the design process and make it easier to understand the function of the logic diagram. Additionally, it can help to ensure that the logic diagram is compatible with other products that may be used in the same system.",
            "The input of the FSM is a 3-bit register S and a 6-bit timer. The timer is used to count down and produce an output of Z=1 when it reaches 0. The register S is used to store the state of the FSM.",
            "Starting with a human-based design process typically results in a design that is more efficient and effective than one that is mapped from a high-level programming language. This is because human designers can more easily optimize a design for specific metrics, such as power consumption or performance. Additionally, human designers can more easily impose constraints on a design, such as limits on chip area or minimum performance, in order to guide the design process."
        ],
        "score": [
            9.568435668945312,
            -11.036890029907227,
            -9.46333122253418
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How does the counter's implementation diagram simplify understanding the diagram?\\n\\n",
        "answers": [
            "\nThe counter's implementation diagram simplifies understanding the diagram by breaking it down into smaller, more manageable pieces. By doing this, it is easier to see how the various parts of the diagram interact with each other. Additionally, it makes it easier to identify potential problems or areas for improvement.",
            "There are a few reasons why it is important to make implicit assumptions clear in the design process. First, if assumptions are not made clear, it can lead to confusion and misunderstanding among the team members working on the design. Second, making assumptions explicit can help to ensure that everyone is on the same page and working towards the same goal. Finally, explicit assumptions can help to prevent errors and mistakes in the design process.",
            "The desired pattern is the one that represents the number in the 2's complement representation. This is because the 2's complement representation is defined by choosing a set of integers balanced around zero from the groups. So, if we want to represent a number in the 2's complement representation, we need to choose the pattern that represents the number in that representation."
        ],
        "score": [
            8.743881225585938,
            -11.161101341247559,
            -10.779842376708984
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What is the purpose of the vertical lines carrying the current state values and their inverses back to the next state logic inputs?\"]",
        "answers": [
            "\nThe purpose of the vertical lines carrying the current state values and their inverses back to the next state logic inputs is to ensure that the state of the FSM is always consistent. This is necessary for safety reasons, as the FSM must always ensure that the lights on one or both roads are red at all times.",
            "The FSM has a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.",
            "The stoplight controller has a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle."
        ],
        "score": [
            10.478456497192383,
            -9.85687255859375,
            -10.236305236816406
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the color mapping for a three-bit RGB encoding system?\\n\\n",
        "answers": [
            "\nThe color mapping for a three-bit RGB encoding system is a system where colors are represented by a combination of red, green, and blue light. The color mapping for such a system is shown to the right. Imagine that you are charged with creating a counter to drive a light through a sequence of colors.",
            "The two states in the first column of the table are the states of the FSM when F=0. In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.",
            "A tri-state buffer is a type of digital logic circuit that is used to either connect or disconnect a signal line from a power supply or ground. This is done by using two transistors, one to connect the signal line to the power supply, and the other to connect the signal line to ground. When the buffer is enabled, both transistors are turned on, and the signal line is connected to either the power supply or ground, depending on the value of the input signal. When the buffer is disabled, both transistors are turned off, and the signal line is effectively disconnected from both the power supply and ground."
        ],
        "score": [
            9.192232131958008,
            -11.056755065917969,
            -10.97750473022461
        ],
        "text": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM"
    },
    {
        "question": " What is the desired pattern for the light?\\n\\n",
        "answers": [
            "\nThe light should be red on one or both roads at all times. If a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. Finally, for fairness, the controller should alternate green lights between the two roads.",
            "A tri-state buffer is a type of digital logic circuit that is used to either connect or disconnect a signal line from a power supply or ground. This is done by using two transistors, one to connect the signal line to the power supply, and the other to connect the signal line to ground. When the buffer is enabled, both transistors are turned on, and the signal line is connected to either the power supply or ground, depending on the value of the input signal. When the buffer is disabled, both transistors are turned off, and the signal line is effectively disconnected from both the power supply and ground.",
            "One reason it is often best to start by selecting a representation that makes sense to a human is that it can be easier to understand and work with. Additionally, starting with a representation that makes sense to a human can help ensure that the final product is also easy to understand and use."
        ],
        "score": [
            -6.644586086273193,
            -10.981399536132812,
            -11.195754051208496
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How many flip-flops will be needed?\\n\\n",
        "answers": [
            "\nFrom the passage, it can be inferred that logical completeness does not necessarily help with efficient design of logic functions. In other words, a single Boolean logic function can be expressed in many different ways. Therefore, it is important for engineers to learn how to develop an efficient implementation of a function, as well as how to determine whether two logic expressions are identical without actually writing out truth tables.",
            "The benefit of using a tri-state buffer is that it allows for logical disconnection between the tri-state buffer and the output. This means that the output can be shared by more than one memory without the need for additional multiplexers. This can be useful for creating larger memories by combining multiple chips.",
            "One way to combine memory chips is to use them to create a larger memory. This can be done by using one address bit to drive a decoder that determines which of the two chips is active. The other address bits, as well as the external data inputs and write enable signal, are then delivered to both memories. The external data outputs are also connected to both memories.\n\nAnother way to combine memory chips is to use them to create a wider memory. In this case, both chips are either active or inactive at the same time, and the external address, write enable, and chip select inputs are routed to both chips. However, the data inputs and outputs are separate, with each chip handling a different part of the data."
        ],
        "score": [
            -11.167673110961914,
            -11.074594497680664,
            -11.162881851196289
        ],
        "text": "Lemma 2:}  Given enough {2-input} OR functions, one can express an OR function on any finite number of variables.  { Proof:}  The proof of Lemma 2 is identical in structure to that of Lemma 1, but uses OR functions instead of AND functions.   Let's now consider a small subset of functions on N variables.   For any such function, you can write out the truth table for the function.  The output of a logic function is just a bit,  either a 0 or a 1.  Let's consider the set of functions on N variables that produce a 1 for exactly one combination of the N variables.  In other words, if you were to write out the truth table for such a function, exactly one row in the truth table would have output value 1, while all other rows had output value 0.  { Lemma 3:}  Given enough AND functions and {1-input} NOT functions, one can express any Boolean logic function  that produces a 1 for exactly one combination of any finite number of variables.  { Proof:}  The proof of Lemma 3 is by construction. Let N be the number of variables on which the function operates. We construct a { minterm} on these N variables, which is an AND operation on each variable or its complement. The minterm is specified by looking at the unique combination of  variable values that produces a 1 result for the function. Each variable that must be a 1 is included as itself, while each variable that must be a 0 is included as the variable's complement (using a NOT function).  The resulting minterm produces the desired function exactly.  When the variables all match the values for which the function should produce 1, the inputs to the AND function are all 1, and the function produces 1. When any variable does not match the value for which the function should produce 1, that variable (or its complement) acts as a 0 input to the AND function, and the function produces a 0, as desired.  The table below shows all eight minterms for three variables.  { {ccc|cccccccc} {c|}&  A& B& C&    &   C&  B &  B C& A  & A  C& A B & A B C   0& 0& 0& 1& 0& 0& 0& 0& 0& 0& 0 0& 0& 1& 0& 1& 0& 0& 0& 0& 0& 0 0& 1& 0& 0& 0& 1& 0& 0& 0& 0& 0 0& 1& 1& 0& 0& 0& 1& 0& 0& 0& 0 1& 0& 0& 0& 0& 0& 0& 1& 0& 0& 0 1& 0& 1& 0& 0& 0& 0& 0& 1& 0& 0 1& 1& 0& 0& 0& 0& 0& 0& 0& 1& 0 1& 1& 1& 0& 0& 0& 0& 0& 0& 0& 1  }  We are now ready to prove our theorem.  { Proof (of Theorem):}  Any given function on N variables produces the value 1 for some set of combinations of inputs.  Let's say that M such combinations produce 1.   Note that M{2^N}. For each combination that produces 1, we can use Lemma 1 to construct an {N-input} AND function. Then, using Lemma 3, we can use as many as M NOT functions and the {N-input} AND function to construct a minterm for that input combination. Finally, using Lemma 2, we can construct an {M-input} OR function and OR together all of the minterms. The result of the OR is the desired function. If the function should produce a 1 for some combination of inputs, that combination's  minterm provides a 1 input to the OR, which in turn produces a 1. If a combination should produce a 0, its minterm does not appear in the OR; all other minterms produce 0 for that combination, and thus all inputs to the OR are 0 in such cases, and the OR produces 0, as desired.  The construction that we used to prove logical completeness does not necessarily help with efficient design of logic functions.  Think about some of the expressions that we discussed earlier in these notes for overflow conditions.  How many minterms do you need  for {N-bit} unsigned overflow?  A single Boolean logic function can be expressed in many different ways, and learning how to develop an efficient implementation of a function as well as how to determine whether two logic expressions are identical without actually writing out truth tables are important engineering skills that you will start to  learn in the coming months.    {Implications of Logical Completeness}  If logical completeness doesn't really help us to engineer logic"
    },
    {
        "question": " What are the next-state values for S_2, S_1, and S_0?\\n\\n",
        "answers": [
            "\nThe next-state values for S_2, S_1, and S_0 are given in the table to the right. The states of S act like a counter, and by examining the connections, we can derive equations for the next state and draw a transition diagram. There are six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle.",
            "SRAM is a type of memory that is used in computers. It is made up of a number of transistors that store data in a way that is easy for the computer to access. SRAM is faster than other types of memory, but it is also more expensive.",
            "The state representation is used to determine the next state of the FSM. This is done by examining the inputs to the FSM and deriving equations for the next state. The states of the FSM act like a counter, and by examining the connections, the next state can be determined."
        ],
        "score": [
            7.889678001403809,
            -10.78822135925293,
            -5.427431583404541
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the equation for S_2^+?', \"\\n\\n",
        "answers": [
            "\nThe equation for S_2^+ is as follows: S_2^+ = A + B + C, where A is the carry bit, B is the first bit being added, and C is the second bit being added. If an overflow occurs, then S_2^+ = C.",
            "The state table form of a specification can make the missing parts of the specification more obvious by showing the different states that the system can be in and the transitions between those states. This can help to identify what parts of the specification are missing and what needs to be added in order to complete the specification.",
            "A decoder is used in this context to simplify the expression of a logic function. By using a decoder, the function can be expressed as a two-level logic function, which is easier to understand and manipulate."
        ],
        "score": [
            8.227288246154785,
            -10.720985412597656,
            -10.54268741607666
        ],
        "text": "unit.  We define S as the number represented by the bit pattern produced by the add unit.  When overflow  occurs, S=C, but we always have (S=C)  2^N.  { Theorem:}   Addition of two {N-bit} 2's complement numbers A and B overflows if and only if one of the following conditions holds:  {A<0 and B<0 and S} {A and B and S<0}   { Proof:}  We once again start with the ``if'' direction.  That is, if condition 1  or condition 2 holds, we have an overflow.  The proofs are straightforward. Given condition 1, we can add the two inequalities A<0 and B<0 to  obtain C=A+B<0.  But S, so clearly S=C, thus overflow  has occurred.  Similarly, if condition 2 holds, we can add the inequalities A and B to obtain C=A+B.  Here we have S<0, so again S=C, and we have an overflow.  We must now prove the ``only if'' direction, showing that any overflow implies either condition 1 or condition 2.  By the  { contrapositive}{If we have a statement of the form (p implies q), its contrapositive is the  statement (not q implies not p). Both statements have the same truth value.  In this case, we can turn our Lemma around as stated.} of our Lemma, we know that if an overflow occurs, either both operands are  negative, or they are both positive.      Let's start with the case in which both operands are negative, so A<0 and B<0, and thus the real sum C<0 as well.  Given that A and B are represented as {N-bit} 2's complement, they must fall in the representable range, so we can write  {eqnarray*} -2^{N-1}  & A & < 0 -2^{N-1}  & B & < 0 {eqnarray*}  We add these two inequalities and replace A+B with C to obtain  {eqnarray*} -2^N  & C & < 0 {eqnarray*}  Given that an overflow has occurred, C must fall outside of the  representable range.  Given that C<0, it cannot be larger than the largest possible number representable using {N-bit} 2's complement, so we can write  {eqnarray*} -2^N  & C & < -2^{N-1} {eqnarray*}  We now add 2^N to each part to obtain  {eqnarray*} 0  & C + 2^N & < 2^{N-1} {eqnarray*}  This range of integers falls within the representable range  for {N-bit} 2's complement, so we can replace the middle expression with S (equal to C modulo 2^N) to find that  {eqnarray*} 0  & S & < 2^{N-1} {eqnarray*}  Thus, if we have an overflow and both A<0 and B<0, the resulting sum S, and condition 1 holds.  The proof for the case in which we observe an overflow when  both operands are non-negative (A and B) is similar, and leads to condition 2.  We again begin with inequalities for A and B:  {eqnarray*} 0  & A & < 2^{N-1} 0  & B & < 2^{N-1} {eqnarray*}  We add these two inequalities and replace A+B with C to obtain  {eqnarray*} 0  & C < & 2^N {eqnarray*}  Given that an overflow has occurred, C must fall outside of the  representable range.  Given that C{}0, it cannot be smaller than the smallest possible number representable using {N-bit} 2's complement, so we can write  {eqnarray*} 2^{N-1}  & C & < 2^N {eqnarray*}  We now subtract 2^N to each part to obtain  {eqnarray*} -2^{N-1}  & C - 2^N & < 0 {eqnarray*}  This range of integers falls within the representable range  for {N-bit} 2's complement, so we can replace the middle expression with S (equal to C modulo 2^N) to find that  {eqnarray*} -2^{N-1}  & S & < 0 {eqnarray*}  Thus, if we have an overflow and both A and B, the resulting sum S<0, and condition 2 holds.  Thus overflow implies either condition 1 or condition 2, completing our proof."
    },
    {
        "question": " How many flip-flops will we need?\\n",
        "answers": [
            "\nLogical completeness does not necessarily help with efficient design of logic functions. A single Boolean logic function can be expressed in many different ways. Learning how to develop an efficient implementation of a function as well as how to determine whether two logic expressions are identical without actually writing out truth tables are important engineering skills that students will learn in the coming months.",
            "PASSAGE - compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column",
            "The RAS signal indicates when the row component of an address is available. Raising the RAS signal for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh. The row is thus a natural basis for the refresh cycle."
        ],
        "score": [
            -11.286130905151367,
            -11.293807029724121,
            -11.263998031616211
        ],
        "text": "Lemma 2:}  Given enough {2-input} OR functions, one can express an OR function on any finite number of variables.  { Proof:}  The proof of Lemma 2 is identical in structure to that of Lemma 1, but uses OR functions instead of AND functions.   Let's now consider a small subset of functions on N variables.   For any such function, you can write out the truth table for the function.  The output of a logic function is just a bit,  either a 0 or a 1.  Let's consider the set of functions on N variables that produce a 1 for exactly one combination of the N variables.  In other words, if you were to write out the truth table for such a function, exactly one row in the truth table would have output value 1, while all other rows had output value 0.  { Lemma 3:}  Given enough AND functions and {1-input} NOT functions, one can express any Boolean logic function  that produces a 1 for exactly one combination of any finite number of variables.  { Proof:}  The proof of Lemma 3 is by construction. Let N be the number of variables on which the function operates. We construct a { minterm} on these N variables, which is an AND operation on each variable or its complement. The minterm is specified by looking at the unique combination of  variable values that produces a 1 result for the function. Each variable that must be a 1 is included as itself, while each variable that must be a 0 is included as the variable's complement (using a NOT function).  The resulting minterm produces the desired function exactly.  When the variables all match the values for which the function should produce 1, the inputs to the AND function are all 1, and the function produces 1. When any variable does not match the value for which the function should produce 1, that variable (or its complement) acts as a 0 input to the AND function, and the function produces a 0, as desired.  The table below shows all eight minterms for three variables.  { {ccc|cccccccc} {c|}&  A& B& C&    &   C&  B &  B C& A  & A  C& A B & A B C   0& 0& 0& 1& 0& 0& 0& 0& 0& 0& 0 0& 0& 1& 0& 1& 0& 0& 0& 0& 0& 0 0& 1& 0& 0& 0& 1& 0& 0& 0& 0& 0 0& 1& 1& 0& 0& 0& 1& 0& 0& 0& 0 1& 0& 0& 0& 0& 0& 0& 1& 0& 0& 0 1& 0& 1& 0& 0& 0& 0& 0& 1& 0& 0 1& 1& 0& 0& 0& 0& 0& 0& 0& 1& 0 1& 1& 1& 0& 0& 0& 0& 0& 0& 0& 1  }  We are now ready to prove our theorem.  { Proof (of Theorem):}  Any given function on N variables produces the value 1 for some set of combinations of inputs.  Let's say that M such combinations produce 1.   Note that M{2^N}. For each combination that produces 1, we can use Lemma 1 to construct an {N-input} AND function. Then, using Lemma 3, we can use as many as M NOT functions and the {N-input} AND function to construct a minterm for that input combination. Finally, using Lemma 2, we can construct an {M-input} OR function and OR together all of the minterms. The result of the OR is the desired function. If the function should produce a 1 for some combination of inputs, that combination's  minterm provides a 1 input to the OR, which in turn produces a 1. If a combination should produce a 0, its minterm does not appear in the OR; all other minterms produce 0 for that combination, and thus all inputs to the OR are 0 in such cases, and the OR produces 0, as desired.  The construction that we used to prove logical completeness does not necessarily help with efficient design of logic functions.  Think about some of the expressions that we discussed earlier in these notes for overflow conditions.  How many minterms do you need  for {N-bit} unsigned overflow?  A single Boolean logic function can be expressed in many different ways, and learning how to develop an efficient implementation of a function as well as how to determine whether two logic expressions are identical without actually writing out truth tables are important engineering skills that you will start to  learn in the coming months.    {Implications of Logical Completeness}  If logical completeness doesn't really help us to engineer logic"
    },
    {
        "question": " Given that we need three flip-flops, can we choose to use the counter's internal state directly as our output values?\\n",
        "answers": [
            "\nYes, we can choose to use the counter's internal state directly as our output values. This is because the colors we need to produce as outputs are all unique bit patterns. By using the counter's internal state directly as our output values, we can simplify our design.",
            "The idx variable is used to keep track of the number of operations in the logic expression. This metric is used to estimate the number of transistors needed for the design, which in turn is an estimate for the area required for the design.",
            "A row is a natural basis for the refresh cycle because the DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An output enable input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present."
        ],
        "score": [
            6.530567169189453,
            -11.194677352905273,
            -10.8421630859375
        ],
        "text": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM"
    },
    {
        "question": " What is a fully-specified transition diagram?\\n",
        "answers": [
            "\nA fully-specified transition diagram is a diagram that specifies all of the states that a system can be in, as well as all of the transitions between those states. This allows for a clear understanding of the behavior of the system.",
            "The S_2 value specifies the number of inputs that must be satisfied in order for the output to be 1. This cuts the number of possible choices from four to two in each case because there are only two possible combinations of inputs that can satisfy this condition.",
            "The counter's implementation diagram simplifies understanding the diagram by breaking it down into smaller, more manageable pieces. By doing this, it is easier to see how the various parts of the diagram interact with each other. Additionally, it makes it easier to identify potential problems or areas for improvement."
        ],
        "score": [
            9.659065246582031,
            -10.962040901184082,
            -10.51675033569336
        ],
        "text": "least a controllable number of cycles. If we cannot specify clearly when transitions occur from an FSM state, we may not be able to implement the state.    Now let's go through the flow chart and identify states.  Initialization of { min} and { idx} need not occur serially, and the result of the first comparison between { idx} and the constant 10 is known in advance, so we can merge all three operations into a single state, which we  call { INIT}.  We can also merge the updates of { min} and { idx} into a second FSM state, which we call { COPY}.  However, the update to { min}  occurs only when the comparison ({ min > value[idx]}) is true.   We can use logic to predicate execution of the update.  In other words, we  can use the output of the comparator, which is available after the comparator  has finished comparing the two values (in a high-level FSM state that we  have yet to define), to determine whether or not the register holding  { min} loads a new value in the { COPY} state.  Our model of use for this FSM involves external logic filling the memory (the array of integer values), executing the FSM ``code,'' and then checking the answer.  To support this use model, we create a FSM state  called { WAIT} for cycles in which the FSM has no work to do. Later, we also make use of an external input signal { START}  to start the FSM execution.  The { WAIT} state logically corresponds to the ``START''  bubble in the flow chart.   Only the test for the { if} statement remains.  Using a serial comparator to compare two {32-bit} values requires 32 cycles. However, we need an additional cycle to move values into our shift  registers so that the comparator can see the first bit.  Thus our single comparison operation breaks into two high-level states.  In the first state, which we call { PREP}, we copy { min} to one of the shift registers, copy { values[idx]} to the other shift register, and reset the counter that measures the cycles needed for our serial comparator.  We then move to a second high-level state, which we call { COMPARE}, in which we feed one bit per cycle from each shift register to the serial comparator.  The { COMPARE}  state   {{file=part3/figs/part3-min-flow-chart-states.eps,width=3.96in}}   executes for 32 cycles, after which the comparator produces the one-bit answer that we need, and we can move to the { COPY} state.  The association between the flow chart and the high-level FSM states is illustrated in the figure shown to the right above.   We can now also draw an abstract state diagram for our FSM, as shown to the right.  The FSM begins in the { WAIT} state.  After external logic fills the { values} array, it signals the FSM to begin by raising the { START} signal.  The FSM transitions into the  { INIT} state, and in the next cycle into the { PREP} state. From { PREP}, the FSM always moves to { COMPARE}, where it remains for 32 cycles while the serial comparator executes a comparison. After { COMPARE}, the FSM moves to the { COPY}   {{file=part3/figs/part3-min-state-diag.eps,width=3in}}   state, where it remains for one cycle.  The transition from { COPY} depends on how many loop iterations have executed.  If more loop iterations remain, the FSM moves to { PREP} to execute the next iteration.  If the loop is done, the FSM returns to { WAIT} to allow external logic to read the result of the computation.    {Laying Out Components}   Our high-level FSM design tells us what our components need to be able to do in any given cycle.  For example, when we load new values into the shift registers that provide bits to the serial comparator, we always copy  { min} into one shift register and { values[idx]} into the second. Using this information, we can put together our components and simplify our design by fixing the way in which bits flow between them.  The figure at the right shows how we can organize our components. Again, in practice, one goes back and forth thinking about states, components, and flow from state to"
    },
    {
        "question": " What is a K-map?\\n",
        "answers": [
            "\nA Karnaugh map is a graphical representation of a Boolean function that helps to simplify the function. The map consists of a grid of squares, each of which corresponds to a possible combination of input values. The map is used to find groups of product terms that are implicants of the function.",
            "A decoder is a type of logic circuit that takes an input signal and produces one output for each possible value of the signal. A decoder is similar to a multiplexer in that it has a number of inputs and outputs. However, a decoder only has one input signal, while a multiplexer has multiple input signals.",
            "The REJECTD and REJECTQ states are used to handle user mistakes. If a user enters a second dime in the DIME state, our FSM should reject the coin. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0). The REJECTQ state is used to handle a user who inserts more than one quarter."
        ],
        "score": [
            2.060546875,
            -10.788097381591797,
            -10.914531707763672
        ],
        "text": "have other names for the smaller values: a point for N=0, a line segment for N=1, a square for N=2, and a cube for N=3.  The diagrams above and to the right illustrate the cases that  are easily drawn on paper.  The black dots represent specific input combinations, and the blue edges connect input combinations that differ in exactly one input value (one bit).    By viewing a function's domain in this way, we can make a connection between a product of literals and the structure of the domain.  Let's use the {3-dimensional} version as an example.  We call the  variables A, B, and C, and note that the cube has 2^3=8 corners corresponding to the 2^3 possible combinations of A, B, and C. The simplest product of literals in this case is 1, which is the product of 0 literals.  Obviously, the product 1  evaluates to 1 for any variable values.  We can thus think of it as covering the entire domain of the function.  In the case of our example, the product 1 covers the whole cube.  In order for the product 1 to  be an implicant of a function, the function itself must be the function 1.  What about a product consisting of a single literal, such as A  or ?  The dividing lines in the diagram illustrate the answer: any such product term evaluates to 1 on a face of the cube, which includes 2^2=4 of the corners.  If a function evaluates to 1 on any of the six faces of the cube, the corresponding product term (consisting of a single literal) is an implicant of the function.  Continuing with products of two literals, we see that any product of  two literals, such as A or C, corresponds to an edge of our {3-dimensional} cube.  The edge includes 2^1=2 corners.  And, if a function evaluates to 1 on any of the 12 edges of the cube, the corresponding product term (consisting of two literals) is an implicant of the function.  Finally, any product of three literals, such as B, corresponds to a corner of the cube.  But for a function on three variables, these are just the minterms.  As you know, if a function evaluates to 1 on any of the 8 corners of the cube, that minterm is an implicant of the function (we used this idea to construct the function to prove logical completeness).  How do these connections help us to simplify functions?  If we're careful, we can map cubes onto paper in such a way that product terms (the possible implicants of the function) usually form contiguous  groups of 1s, allowing us to spot them easily.  Let's work upwards starting from one variable to see how this idea works.  The end result is called a Karnaugh map.   The first drawing shown to the right replicates our view of  the {1-dimensional} hypercube, corresponding to the domain of a function on one variable, in this case the variable A.   To the right of the hypercube (line segment) are two variants of a Karnaugh map on one variable.  The middle variant clearly  indicates the column corresponding to the product A (the other  column corresponds to ).  The right variant simply labels the column with values for A.       The three drawings shown to the right illustrate the three possible product terms on one variable.  { The functions shown in  these Karnaugh maps are arbitrary, except that we have chosen them such that each implicant shown is a prime implicant for the illustrated function.}        Let's now look at two-variable functions.  We have replicated our drawing of the {2-dimensional} hypercube (square) to the right  along with two variants of Karnaugh maps on two variables. With only two variables (A and B), the extension is fairly  straightforward, since we can use the second dimension of the  paper (vertical) to express the second variable (B).       The number of possible products of literals grows rapidly with the  number of variables. For two variables, nine are possible, as shown to the right. Notice that all implicants have two properties.  First, they occupy contiguous regions of the grid.  And, second, their height and width are always powers of two."
    },
    {
        "question": " What are the next-state logic equations?\", '\\n\\n",
        "answers": [
            "\nThe next-state logic equations are the functions that describe the relationship between the current state of the system and the next state of the system. These functions can be represented in either SOP or POS form. DeMorgan's law can be used to simplify these functions by replacing OR gates with NAND gates, or AND gates with NOR gates.",
            "The next-state logic equations are the functions that describe the relationship between the current state of the system and the next state of the system. These functions can be represented in either SOP or POS form. DeMorgan's law can be used to simplify these functions by replacing OR gates with NAND gates, or AND gates with NOR gates.",
            "Random access memories (RAMs) are memories that support two operations: writing data into an element, and reading data from an element. The memories are also volatile, which means that the data held by a memory are erased when electrical power is turned off or fails. Static random access memory (SRAM) is used for high-speed applications such as processor caches and some embedded designs."
        ],
        "score": [
            8.228653907775879,
            8.228653907775879,
            -10.96949291229248
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How many flip-flops will we need?\\n",
        "answers": [
            "\nLogical completeness does not necessarily help with efficient design of logic functions. A single Boolean logic function can be expressed in many different ways, and learning how to develop an efficient implementation of a function as well as how to determine whether two logic expressions are identical without actually writing out truth tables are important engineering skills that you will start to learn in the coming months.",
            "The next-state logic for a three-bit Gray code counter is as follows:\n\nS_2^+ = S_2 + S_0\nS_1^+ = S_2\nS_0^+ = S_2 S_1",
            "A write operation on an SRAM cell is performed by charging or not charging a capacitor. The capacitor is attached to a BIT line through a transistor controlled by a SELECT line. When SELECT is low, the capacitor is isolated and holds its charge. However, when SELECT is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it."
        ],
        "score": [
            -11.278675079345703,
            -11.152843475341797,
            -11.253361701965332
        ],
        "text": "Lemma 2:}  Given enough {2-input} OR functions, one can express an OR function on any finite number of variables.  { Proof:}  The proof of Lemma 2 is identical in structure to that of Lemma 1, but uses OR functions instead of AND functions.   Let's now consider a small subset of functions on N variables.   For any such function, you can write out the truth table for the function.  The output of a logic function is just a bit,  either a 0 or a 1.  Let's consider the set of functions on N variables that produce a 1 for exactly one combination of the N variables.  In other words, if you were to write out the truth table for such a function, exactly one row in the truth table would have output value 1, while all other rows had output value 0.  { Lemma 3:}  Given enough AND functions and {1-input} NOT functions, one can express any Boolean logic function  that produces a 1 for exactly one combination of any finite number of variables.  { Proof:}  The proof of Lemma 3 is by construction. Let N be the number of variables on which the function operates. We construct a { minterm} on these N variables, which is an AND operation on each variable or its complement. The minterm is specified by looking at the unique combination of  variable values that produces a 1 result for the function. Each variable that must be a 1 is included as itself, while each variable that must be a 0 is included as the variable's complement (using a NOT function).  The resulting minterm produces the desired function exactly.  When the variables all match the values for which the function should produce 1, the inputs to the AND function are all 1, and the function produces 1. When any variable does not match the value for which the function should produce 1, that variable (or its complement) acts as a 0 input to the AND function, and the function produces a 0, as desired.  The table below shows all eight minterms for three variables.  { {ccc|cccccccc} {c|}&  A& B& C&    &   C&  B &  B C& A  & A  C& A B & A B C   0& 0& 0& 1& 0& 0& 0& 0& 0& 0& 0 0& 0& 1& 0& 1& 0& 0& 0& 0& 0& 0 0& 1& 0& 0& 0& 1& 0& 0& 0& 0& 0 0& 1& 1& 0& 0& 0& 1& 0& 0& 0& 0 1& 0& 0& 0& 0& 0& 0& 1& 0& 0& 0 1& 0& 1& 0& 0& 0& 0& 0& 1& 0& 0 1& 1& 0& 0& 0& 0& 0& 0& 0& 1& 0 1& 1& 1& 0& 0& 0& 0& 0& 0& 0& 1  }  We are now ready to prove our theorem.  { Proof (of Theorem):}  Any given function on N variables produces the value 1 for some set of combinations of inputs.  Let's say that M such combinations produce 1.   Note that M{2^N}. For each combination that produces 1, we can use Lemma 1 to construct an {N-input} AND function. Then, using Lemma 3, we can use as many as M NOT functions and the {N-input} AND function to construct a minterm for that input combination. Finally, using Lemma 2, we can construct an {M-input} OR function and OR together all of the minterms. The result of the OR is the desired function. If the function should produce a 1 for some combination of inputs, that combination's  minterm provides a 1 input to the OR, which in turn produces a 1. If a combination should produce a 0, its minterm does not appear in the OR; all other minterms produce 0 for that combination, and thus all inputs to the OR are 0 in such cases, and the OR produces 0, as desired.  The construction that we used to prove logical completeness does not necessarily help with efficient design of logic functions.  Think about some of the expressions that we discussed earlier in these notes for overflow conditions.  How many minterms do you need  for {N-bit} unsigned overflow?  A single Boolean logic function can be expressed in many different ways, and learning how to develop an efficient implementation of a function as well as how to determine whether two logic expressions are identical without actually writing out truth tables are important engineering skills that you will start to  learn in the coming months.    {Implications of Logical Completeness}  If logical completeness doesn't really help us to engineer logic"
    },
    {
        "question": " What is the color mapping for a three-bit RGB encoding system?\\n",
        "answers": [
            "\nThe color mapping for a three-bit RGB encoding system is that the colors are produced by unique bit patterns. The states form a loop, and are marked with the internal state value S_2S_1S_0 and the output RGB.",
            "The addressability of a computer memory is the number of bits that can be stored in each element of the memory. The size of the elements in a memory can vary from a single bit to a byte or more. Typically, data elements larger than a byte are referred to as words, but the size of a word depends on context.",
            "SRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors, while a DRAM cell uses only one transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor"
        ],
        "score": [
            9.425226211547852,
            -10.878286361694336,
            -11.136796951293945
        ],
        "text": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM"
    },
    {
        "question": " What is the desired pattern?\\n",
        "answers": [
            "\nThe desired pattern is the one that represents the number in the 2's complement representation. This is because the 2's complement representation is defined by choosing a set of integers balanced around zero from the groups. So, if we want to represent a number in the 2's complement representation, we need to choose the pattern that represents the number in that representation.",
            "SRAM is a type of memory that is used in computers. It is made up of a number of transistors that store data in a way that is easy for the computer to access. SRAM is faster than other types of memory, but it is also more expensive.",
            "A Gray code counter is a type of counter that cycles through all of the possible bit patterns of a certain length. The states in the transition diagram for a Gray code counter form a loop, with the counter moving from one state to the next each cycle. Each state in the diagram is marked with the internal state value and the output value, which are always equal for this counter."
        ],
        "score": [
            6.3065714836120605,
            -11.186524391174316,
            -8.943695068359375
        ],
        "text": "representation, we can add by counting clockwise around the circle, and we can subtract by counting in a counterclockwise direction around the circle.  With an unsigned representation, we choose to use the group from [0,15] (the middle group in the diagram markings to the right) as the number represented by each of the patterns.  Overflow occurs with unsigned addition (or subtraction) because we can only choose one value for each binary pattern.      In fact, we can choose any single value for each pattern to create a  representation, and our add unit will always produce results that are correct modulo 16.  Look back at our overflow example, where we added 12 and 6 to obtain 2, and notice that (2=18)  16. Normally, only a contiguous sequence of integers makes a useful representation, but we do not have to restrict ourselves to  non-negative numbers.  The 2's complement representation can then be defined by choosing a  set of integers balanced around zero from the groups.  In the circle  diagram, for example, we might choose to represent numbers in the range [-7,7] when using 4 bits.  What about the last pattern, 1000? We could choose to represent either -8 or 8.  The number of arithmetic operations that overflow is the same with both choices (the choices are symmetric around 0, as are the combinations of input operands that  overflow), so we gain nothing in that sense from either choice. If we choose to represent -8, however, notice that all patterns starting with a 1 bit then represent negative numbers.  No such simple check arises with the opposite choice, and thus an {N-bit} 2's complement  representation is defined to represent the range [-2^{N-1},2^{N-1}-1], with patterns chosen as shown in the circle.  {An Algebraic Approach}  Some people prefer an algebraic approach to understanding the definition of 2's complement, so we present such an approach next. Let's start by writing f(A,B) for the result of our add unit:  {eqnarray*} f(A,B) = (A + B)  2^N {eqnarray*}  We assume that we want to represent a set of integers balanced around 0 using our signed representation, and that we will use the same binary patterns as we do with an unsigned representation to represent non-negative numbers.  Thus, with an {N-bit} representation, the patterns in the range [0,2^{N-1}-1] are the same as those used with an unsigned representation.  In this case, we are left with all patterns beginning with a 1 bit.  The question then is this: given an integer k, 2^{N-1}>k>0, for which we  want to find a pattern to represent -k, and any integer m that we might want to add to -k,  can we find another integer p>0 such that    (-k + m = p + m)  2^N   ?   If we can, we can use p's representation to represent -k and our unsigned addition unit f(A,B) will work correctly.  To find the value p, start by subtracting m from both sides of Equation () to obtain:   (-k = p)  2^N   Note that (2^N=0)  2^N, and add this equation to  Equation () to obtain  {eqnarray*} (2^N-k = p)  2^N {eqnarray*}  Let p=2^N-k.    For example, if N=4, k=3 gives p=16-3=13, which is the pattern 1101. With N=4 and k=5, we obtain p=16-5=11, which is the pattern 1011. In general, since 2^{N-1}>k>0,  we have 2^{N-1}<p<2^N.  But these patterns are all unused---they all start with a 1 bit!---so the patterns that we have defined for negative numbers are disjoint from those that we used for positive numbers, and the meaning of each pattern is unambiguous.  The algebraic definition of bit patterns for negative numbers also matches our circle diagram from the last section exactly, of course.    {Negating 2's Complement Numbers}  The algebraic approach makes understanding negation of an integer represented using 2's complement fairly straightforward, and gives  us an easy procedure for doing so. Recall that given an integer k in an {N-bit} 2's complement representation, the {N-bit} pattern for -k is given by 2^N-k  (also true for k=0 if we keep only the low N bits of the result).   But"
    },
    {
        "question": " What is the transition diagram for our color sequencer?\\n",
        "answers": [
            "\nThe transition diagram for the color sequencer is a loop of six states, with the two remaining states converging into the loop after a single cycle. The lights are all red in both of these states, which is necessary for safety. The timer is set only when the state S changes, and the duration of each state is calculated by analyzing the muxes.",
            "The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle. The states of S thus act like a counter.",
            "SRAM is a type of memory that is used in computers. It is made up of a number of transistors that store data in a way that is easy for the computer to access. SRAM is faster than other types of memory, but it is also more expensive."
        ],
        "score": [
            8.211446762084961,
            -11.103948593139648,
            -11.234569549560547
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What are the next-state values for S_2, S_1, and S_0?']",
        "answers": [
            "\nThe next-state values for S_2, S_1, and S_0 are all 1.",
            "The FSM can get into unexpected states if the inputs are not set properly. For example, if the input is set to 111, the FSM will stay in the WHITE state indefinitely.",
            "The design for the counter is completed by applying the same decisions that were made for the { COUNT A} state to all of the other counting states, and the decisions that were made for the { HALT A} state to all of the other halted states. This results in a design in which states are labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0)."
        ],
        "score": [
            8.105598449707031,
            -9.49563217163086,
            1.1940531730651855
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the initial state of the three flip-flops in our implementation?\\n",
        "answers": [
            "\nThe initial state of the three flip-flops in our implementation is 0s and 1s. We can use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).",
            "The design process involves the manipulation of logic expressions in order to optimize them for specific metrics, such as cost, power, or performance. Today, this manipulation is performed almost entirely by computers, with human engineers supplying the logic functions of interest and imposing constraints on the process.",
            "The FSM needs to keep track of the loop index, the current minimum value, and the progress of the serial comparator."
        ],
        "score": [
            9.503521919250488,
            -11.308845520019531,
            -11.313854217529297
        ],
        "text": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset"
    },
    {
        "question": "What happens in the next cycle?\\n",
        "answers": [
            "\nThe timer is set to 0, and the state of the FSM is changed to the next state in the sequence. The new state of the FSM determines the duration of the next state, and the timer is set accordingly.",
            "A decoder is a logic device that converts binary information from the form of one code to another, usually a natural binary code. A multiplexer is a device that selects one of several analog or digital input signals and forwards the selected input into a single output line.",
            "If a user presses the \"go\" button (HG=01), the machine should dispense one and a half servings of lychee and a half serving of mango."
        ],
        "score": [
            -4.6702141761779785,
            -11.077615737915039,
            -11.071755409240723
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "What could be going wrong?\\n",
        "answers": [
            "\nThe passage states that power consumption is an important metric that engineers must take into account in their designs. However, in some cases, such as desktop and high-end server processors, other metrics (such as performance) may be so important that we always want to operate at the edge of the interesting range. This could be what is going wrong, as engineers are not taking power consumption into account as much as they should be.",
            "The register PRICE1 has six bits. This allows the machine to keep track of up to 3.15 (63 nickels).",
            "PASSAGE - \n\nTwo-level logic is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. In this section, we illustrate one of the reasons for this popularity and show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.\n\nQUESTION -\n\nWhat are the two levels of two-level logic?"
        ],
        "score": [
            1.162821888923645,
            -11.32899284362793,
            -11.19404411315918
        ],
        "text": "range corresponds to systems in which when a processor's power consumption in a digital  system is extremely low relative to the power consumption of the system. For example, the processor in a computer might use less than 1  of the total used by  the system including the disk drive, the monitor, the power  supply, and so forth.  One power consumption value in this range  is just as good as any another, and no one cares about the power consumption of the processor  in such cases.  In the second range, power consumption of the processor makes a difference.  Cell phones use most of their energy in radio operation, for example, but if you own a phone with a powerful processor, you may have noticed that you can turn off the phone and  drain the battery fairly quickly by playing a game.  Designing a processor that uses half as much power lengthens the battery life in such cases.  Finally, the third region of power consumption measurements is impossible: if you use so much power, your chip will overheat or even burst into flames.  Consumers get unhappy when such things happen.  As a first step, you can remove any metrics for which all solutions are effectively equivalent.  Until a little less than a decade ago, for example, the power  consumption of a desktop processor actually was in the first range that we discussed.  Power was simply not a concern to engineers: all designs of  interest consumed so little power that no one cared.  Unfortunately, at that point, power consumption jumped into the third range rather quickly.  Processors hit a wall, and  products had to be cancelled.  Given that the time spent designing a processor has historically been about five years, a lot of engineering effort was wasted because people had not thought carefully enough about power (since it had never mattered in the past).  Today, power is an important metric that engineers must take into account in their designs.   However, in some areas, such as desktop and high-end server processors, other metrics (such as performance) may be so  important that we always want to operate at the edge of the interesting range.  In such cases, we might choose to treat  a metric such as power consumption as a { threshold}: stay below 150 Watts for a desktop processor, for example.  One still has to make a coordinated effort to ensure that the system as a whole does not exceed the threshold, but reasoning about  threshold values, a form of constraint, is easier than trying to think about multiple metrics at once.  Some metrics may only allow discrete quantification.  For example, one could choose to define compatibility with previous processor generations as binary: either an existing piece of software (or operating system) runs out of the box on your new processor, or it does not.  If you  want people who own that software to make use of your new processor, you must ensure that the value of this binary metric is 1, which can also be viewed as a threshold.  In some cases, two metrics may be strongly { correlated}, meaning that a design that is good for one of the metrics is frequently  good for the other metric as well.  Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider them separately.  A design that requires a larger chip is probably more complex, and thus takes more engineering time to get right (engineering time costs money).    Each silicon wafer costs money to fabricate, and fewer copies of a  large design fit on one wafer, so large chips mean more fabrication cost.  Physical defects in silicon can cause some chips not to work.  A large chip uses more silicon than a small one, and is thus more likely to suffer from defects (and not work).  Cost thus goes up again for large chips relative to small ones.  Finally, large chips usually require more careful testing to ensure that they work properly (even ignoring the cost of getting the design right, we have to test for the presence of defects), which"
    },
    {
        "question": "How can the FSM ever get into the unexpected states?\\n",
        "answers": [
            "\nOne possibility is that the FSM is not resetting properly, so it starts in an unexpected state. Another possibility is that there is a race condition in the FSM, so that it can get into an unexpected state if the inputs change too quickly.",
            "The next-state values for S_2, S_1, and S_0 are all 1.",
            "The FSM for the lab is a coin-counting FSM that requires the use of a Moore machine model to avoid sending the servo controlling the coin's path an output pulse that was too short to enforce the FSM's decision about which way to send the coin. By adding more states to the FSM, the servo can be held in place, as desired."
        ],
        "score": [
            6.2939863204956055,
            -11.192394256591797,
            -3.991361141204834
        ],
        "text": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM"
    },
    {
        "question": "What is the purpose of adding a few AND gates and a  input?', '\\n",
        "answers": [
            "\nThe purpose of adding a few AND gates and a input is to create a logic function that is more efficient in terms of area and power. This is done by reducing the number of variables in the expression.",
            "A sense amplifier is a circuit that is used to read the data stored in a memory cell. The sense amplifier amplifies the small signal from the memory cell and converts it into a digital signal that can be read by the processor.",
            "The choice of representation for the FSM's internal state affects the complexity of the implementation in terms of the number of states, the number of inputs, the number of outputs, and the number of transition rules."
        ],
        "score": [
            9.44158935546875,
            -10.912891387939453,
            -10.492584228515625
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": "What is the issue with the FSM getting into unexpected states?\\n",
        "answers": [
            "\nThe issue with the FSM getting into unexpected states is that it can cause the output of the circuit to glitch. This can be problematic in some applications where a stable output is required.",
            "A priority encoder is a circuit that takes in a number of input signals, and outputs a code that represents the highest priority input signal. The output code is usually a binary number.",
            "What is the difference between SOP and POS?\n\nSOP is a form of two-level logic, while POS is a form of multi-level logic. SOP is more popular because it is simpler to manipulate expressions algebraically and graphically."
        ],
        "score": [
            9.791561126708984,
            -11.233838081359863,
            -11.300314903259277
        ],
        "text": "{3-input} NAND gates and allows set/reset of the flip-flop (using two extra inputs).    The { timing diagram} to the right illustrates the operation of our flip-flop.  In a timing diagram, the horizontal axis represents (continuous) increasing time, and the individual lines represent voltages for logic signals.  The relatively simple version shown here uses only binary values for each signal.  One can also draw  transitions more realistically (as taking finite time).  The dashed vertical lines here represent the times at which the clock rises. To make the      example interesting, we have varied D over two clock cycles.  Notice that even though D rises and falls during the second clock cycle, its value is not copied to the output of our flip-flop. One can build flip-flops that ``catch'' this kind of behavior (and change to output 1), but we leave such designs for later in your career.  Circuits such as latches and flip-flops are called { sequential feedback} circuits, and the process by which they are designed  is beyond the scope of our course.  The ``feedback'' part of the name refers to the fact that the outputs of some gates are fed back  into the inputs of others.  Each cycle in a sequential feedback  circuit can store one bit.  Circuits that merely use latches and flip-flops as building blocks are called { clocked synchronous sequential circuits}.  Such designs are still sequential: their behavior depends on the bits currently stored in the latches and flip-flops.  However, their behavior is substantially simplified by the use of a clock signal (the ``clocked'' part of the name) in a way that all elements change at the same time (``synchronously'').  The value of using flip-flops and assuming a square-wave clock signal with uniform timing may not be clear to you yet, but it bears emphasis.  With such assumptions, { we can treat time as having  discrete values.}  In other words, time ``ticks'' along discretely, like integers instead of real numbers.  We can look at the state of the system, calculate the inputs to our flip-flops through the combinational logic that drives their D inputs, and be confident that, when time moves to the next discrete value, we will know the new bit values stored in our flip-flops, allowing us to repeat the process for the next clock cycle without worrying about exactly when things change.  Values change only on the rising edge of the clock!  Real systems, of course, are not so simple, and we do not have one clock to drive the universe, so engineers must also design systems that interact even though each has its own private clock signal (usually with different periods).    {Static Hazards: Causes and Cures*}  Before we forget about the fact that real designs do not provide perfect clocks, let's explore some of the issues that engineers must sometimes face.    We discuss these primarily to ensure that you appreciate the power of the abstraction that we use in the rest of our course. In later classes (probably our 298, which will absorb material  from 385), you may be required to master this material. { For now, we provide it simply for your interest.}  Consider the circuit shown below, for which the output is given by  the equation S=AB+.   {{file=part2/figs/lec15-1.eps,width=4in}}  The timing diagram on the right shows a { glitch} in the output when the input shifts from ABC=110 to 100, that is, when B falls. The problem lies in the possibility that the upper AND gate, driven  by B, might go low before the lower AND gate, driven by , goes high.  In such a case, the OR gate output S falls until the second AND gate rises, and the output exhibits a glitch.  A circuit that might exhibit a glitch in an output that functionally remains stable at 1 is said to have a { static-1 hazard}.  The qualifier ``static'' here refers to the fact that we expect the output to remain static, while the ``1'' refers to the expected value of the output.    The presence of hazards in circuits can be problematic in"
    },
    {
        "question": "How can the FSM ever get into the unexpected states?\\n",
        "answers": [
            "\nThe FSM can get into unexpected states if the inputs are not set properly. For example, if the input is set to 111, the FSM will stay in the WHITE state indefinitely.",
            "Logical completeness does not necessarily help with efficient design of logic functions. A single Boolean logic function can be expressed in many different ways. Learning how to develop an efficient implementation of a function as well as how to determine whether two logic expressions are identical without actually writing out truth tables are important engineering skills that students will learn in the coming months.",
            "The light sometimes glows cyan or red briefly before appearing white because the controller must ensure that the lights on one or both roads are red at all times. Similarly, if a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down."
        ],
        "score": [
            8.224390029907227,
            -11.312376022338867,
            -11.294293403625488
        ],
        "text": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM"
    },
    {
        "question": "What is the initial state of the three flip-flops in our implementation?\\n",
        "answers": [
            "\nThe initial state of the three flip-flops in our implementation is the 0 state. This is because we have an explicit mechanism for initialization that forces each flip-flop into the 0 state. Alternatively, we could add some extra logic to our design that would allow us to force the FSM into any specific state in the next cycle.",
            "The use of S_2 separates out the logic for deciding whether to advance the counter by allowing the counter to be in a halted state (S_2=1) or a counting state (S_2=0). When HOLD=1, the counter retains its state, and when HOLD=0, it counts.",
            "The fourth step in the design process is to choose a particular design from the set of possible designs. In making this choice, engineers typically consider a variety of factors, including cost, performance, manufacturability, and reliability."
        ],
        "score": [
            9.709831237792969,
            -10.61826229095459,
            -11.325820922851562
        ],
        "text": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset"
    },
    {
        "question": "What is the first approach to making use of the flip-flop design?\\n",
        "answers": [
            "\nThe first approach is to use the flip-flop design to add a single bit and pass any necessary information into another copy of itself. By using copies of this bit-sliced adder circuit, we can mimic our approach as humans and build adders of any size.",
            "Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created the wiki documentation and the Altera Quartus portions of the lab. They based the lab on earlier Mentor Graphics work by Prof. Herman. Prof. Kindratenko also helped to scale the design in a way that made it possible to deliver to the over 400 students entering ECE every semester.",
            "The output A does not depend on the input T because the function of AND is to evaluate to 1 if all of the input values are 1. If there is an input with value 0, then the function will evaluate to 0."
        ],
        "score": [
            8.795085906982422,
            -10.722376823425293,
            -11.268289566040039
        ],
        "text": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is"
    },
    {
        "question": "What is a more powerful initialization mechanism?', '\\n",
        "answers": [
            "\nInterrupts are more powerful than exceptions because they can occur asynchronously with respect to the program. This means that the processor can check for interrupts between instructions, which allows for more flexibility in the design of the processor.",
            "The right strategy for choosing a representation is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. This will make the design process simpler and more efficient in the long run.",
            "What is the reason for the popularity of two-level logic?\n\nThe popularity of two-level logic is due to the fact that it is a simple way to express logic functions. Additionally, two-level logic is easy to map into CMOS gates."
        ],
        "score": [
            -3.3215246200561523,
            -11.255346298217773,
            -11.168248176574707
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": " Why does the light sometimes glow cyan or red briefly before appearing white?\\n",
        "answers": [
            "\nThe light sometimes glows cyan or red briefly before appearing white because the controller must ensure that the lights on one or both roads are red at all times. Similarly, if a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down.",
            "There are a few disadvantages of using an FSM. First, if the FSM is not designed carefully, it can be hard to understand and debug. Second, an FSM can use more resources than a simpler design, such as a state machine. Finally, an FSM can be more difficult to change or extend than a simpler design.",
            "From the passage, it seems that a digital FSM is designed from an abstract model by first identifying a finite set of states, a set of possible inputs, a set of possible outputs, and a set of transition rules. Then, the FSM is implemented as a digital system by representing all states as patterns using a fixed number of bits, translating all inputs into bits, and translating all outputs into bits. Finally, transition rules are defined so that given any state of the FSM and any pattern of input bits, a transition is defined from that state to another state."
        ],
        "score": [
            9.35457992553711,
            -11.27175235748291,
            -11.211112976074219
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How can the FSM ever get into the unexpected states?\\n",
        "answers": [
            "\nOne possibility is that the FSM is not resetting properly, so it does not start from the expected initial state. Another possibility is that there is a race condition in the FSM, so that it can reach an unexpected state if the inputs change too quickly.",
            "A read operation is performed on an SRAM cell by first bringing the bit line to an intermediate voltage level, then raising the SELECT line. This allows the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line. The resulting change in voltage is then detected by a sense amplifier.",
            "The lab simulates two-level logic and how it can be used to map an expression into CMOS gates. It also shows how to graphically manipulate expressions to understand gate diagrams."
        ],
        "score": [
            4.659221649169922,
            -11.299531936645508,
            -11.321462631225586
        ],
        "text": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM"
    },
    {
        "question": " What is the initial state of the three flip-flops in our implementation?\\n",
        "answers": [
            "\nThe initial state of the three flip-flops is unknown unless we have an explicit mechanism for initialization. Initialization can work in two ways. The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input. Alternatively, we can add some extra logic to our design.",
            "One possibility is that the FSM is not resetting properly, so it starts in an unexpected state. Another possibility is that there is a race condition in the FSM, so that it can get into an unexpected state if the inputs change too quickly.",
            "One way to transform code written in a high-level language into an FSM is to use a compiler. A compiler is a program that translates code written in a high-level language into code that can be run on a computer. Another way to transform code written in a high-level language into an FSM is to use an interpreter. An interpreter is a program that executes code written in a high-level language."
        ],
        "score": [
            6.916034698486328,
            -11.009864807128906,
            -11.245462417602539
        ],
        "text": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset"
    },
    {
        "question": " How can we make use of the internal reset lines on the latches to force each flip-flop into the 0 state?\\n",
        "answers": [
            "\nOne way to make use of the internal reset lines on the latches is to force each flip-flop into the 0 state. This can be done by connecting the reset lines of the latches to a 0 voltage source.",
            "The FSM uses the signal T to determine whether to accept or reject the coin by checking the rising edge of the signal from the second sensor. If the rising edge occurs, the signal T from the first sensor indicates whether the coin is a quarter (T=1) or a dime (T=0). A sample timing diagram for the lab appears to the right. The clock signal generated by the lab is not only not a square wave---in other words, the high and low portions are not equal---but is also unlikely to be periodic. Instead, the ``cycle'' is defined by the time between coin insertions.",
            "One way to combine memory chips is to use multiple chips to implement a larger memory. This can be done by using one address bit to drive a decoder that determines which of the two chips is active. Another way to combine memory chips is to use multiple chips to implement a wider memory. This can be done by routing the external address, write enable, and chip select inputs to both chips. The data inputs and outputs are separate, with the left chip handling the high N bits of input on writes and producing the high N bits of output on reads, while the right chip handles the low N bits of input and produces the low N bits of output."
        ],
        "score": [
            9.233997344970703,
            -11.11015510559082,
            -10.933541297912598
        ],
        "text": "enable input (we called it WE) to control when the latch copies its input into the stored bit. A symbol for a gated D latch appears to the right.  Note that we have dropped the name P in favor of , since P= in a gated D latch.   {The Clock Abstraction}  High-speed logic designs often use latches directly.  Engineers specify the number of latches as well as combinational logic  functions needed to connect one latch to the next, and the CAD tools optimize the combinational logic. The enable inputs of successive groups of latches are then driven by what we call a clock signal, a single bit line distributed across most of the chip that alternates between 0 and 1 with a regular period.  While the clock is 0, one set of latches holds its bit values fixed, and combinational logic uses those latches as inputs to produce  bits that are copied into a second set of latches.  When the clock switches to 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches. Of course, some of the latches in the first and third sets may be the same.  The timing of signals in such designs plays a critical role in their correct operation.  Fortunately, we have developed powerful abstractions  that allow engineers to ignore much of the complexity while thinking about the Boolean logic needed for a given design.  Towards that end, we make a simplifying assumption for the rest of our class, and for most of your career as an undergraduate: the clock signal is a { square wave} delivered uniformly across a chip.  For example, if the period of a clock is 0.5 nanoseconds (2 GHz), the clock signal is a 1 for 0.25 nanoseconds, then a 0 for 0.25 nanoseconds.  We assume that the clock signal changes instantaneously and at the same time across the chip.  Such a signal can never exist in the real world: voltages do not change instantaneously, and the  phrase ``at the same time'' may not even make sense at these scales. However, circuit designers can usually provide a clock signal that is close enough, allowing us to forget for now that no physical signal can meet our abstract definition.    SSL altered terminology on 3 Dec 21     The device shown to the right is a { master-slave} implementation of a   The device shown to the right is a { dual-latch} implementation of a  { positive edge-triggered} D flip-flop.  As you can see, we have  constructed it from two gated D latches with opposite senses of write enable.  The ``D'' part of the name has the same meaning as with a gated D latch: the bit stored is the same as the one delivered   {file=part2/figs/latch-step-8.eps,width=2.75in}   {file=part2/figs/latch-step-9.eps,width=0.95in}   to the input.  Other variants of flip-flops have also been built, but this  type dominates designs today.  Most are actually generated automatically from hardware ``design'' languages (that is, computer programming languages for hardware design).  When the clock is low (0), the first latch copies its value from the flip-flop's D input to the midpoint (marked X in our figure, but not usually given a name).  When the clock is high (1), the second latch copies its value from X to the flip-flop's output Q. Since X can not change when the clock is high, the result is that the output changes each time the clock changes from 0 to 1, which is called the { rising edge} or { positive edge} (the derivative) of the clock signal.  Hence the qualifier  ``positive edge-triggered,'' which describes the flip-flop's behavior.  The ``master-slave'' implementation refers to the use of two latches.  The ``dual-latch'' implementation refers to the use of two  latches.{Historically, this implementation was called ``master-slave,'' but ECE Illinois has decided to eliminate use of such terminology.} latches. In practice, flip-flops are almost never built this way.  To see a  commercial design, look up 74LS74, which uses six"
    },
    {
        "question": " How can we add multiplexers to each of our flip-flop inputs to choose between normal operation and forcing the FSM into a specific state?']",
        "answers": [
            "\nThe passage discusses the design of a serial comparator using a bit-sliced approach. This approach leads to a design with 24 gates and 10 inverters, which is not much larger than the earlier serial design. However, the new design is identical in terms of gate delays, meaning that a comparison can be finished in half the time.",
            "Abstraction is a powerful tool that can help simplify complex problems. When selecting a state representation, abstraction can help identify the most important aspects of the problem and ignore irrelevant details. This can make it easier to find a solution that is both effective and efficient.",
            "Dynamic RAMs are memories that use a charged capacitor to store a bit. The charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.'' DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs."
        ],
        "score": [
            -11.260144233703613,
            -11.055994033813477,
            -11.313097953796387
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " What is the purpose of an abstract model?\\n",
        "answers": [
            "\nAn abstract model is a simplified representation of a system, typically used to aid in understanding the system. In the context of logic optimization, an abstract model can be used to help understand the tradeoffs between different logic expressions.",
            "The stoplight controller alternates green lights between the two roads by changing the value of the register S. The value of S is changed when the timer reaches 0, and the timer is set when S enters a new state. The duration of each state is determined by the value of the register S.",
            "There are two possible ways that the user could press both buttons while in state { HALT A}. The first way is that the user could press the \"halt\" button and then the \"go\" button. In this case, the FSM would move to state { HALT A} and then to state { COUNT B}. The second way is that the user could press the \"go\" button and then the \"halt\" button. In this case, the FSM would move to state { COUNT B} and then to state { HALT A}."
        ],
        "score": [
            7.216141700744629,
            -11.261491775512695,
            -11.13435173034668
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": " What is the difference between a next-state table and a transition diagram?\\n",
        "answers": [
            "\nA next-state table lists all of the possible states that the FSM can be in, as well as the inputs and outputs for each state. A transition diagram shows the order in which the states will be visited, based on the inputs and outputs.",
            "It is important to have a reasonable order when designing any logic diagram because it can help simplify the design process and make it easier to understand the function of the logic diagram. Additionally, it can help to ensure that the logic diagram is compatible with other products that may be used in the same system.",
            "There are a few disadvantages of using an FSM. First, if the FSM is not designed carefully, it can be hard to understand and debug. Second, an FSM can use more resources than a simpler design, such as a state machine. Finally, an FSM can be more difficult to change or extend than a simpler design."
        ],
        "score": [
            7.282391548156738,
            -10.704116821289062,
            -10.740730285644531
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How many states are in the second version of the table?\\n",
        "answers": [
            "\nThere are four states in the second version of the table. The states are A, B, C, and D. A and B are both in the ON state, while C and D are both in the OFF state.",
            "If a user presses the \"go\" button (HG=01), the ice cream cup will overflow. This is because the outputs are C_L=11 and C_M=10, which means that the dispenser will release a full serving of each flavor, or two servings total.",
            "The similarity between the equations for S_1^+S_0^+ and the equations for a {2-to-1} mux is that both can be used to represent a logic function. The two-level logic is a popular way of expressing logic functions because it is simple to map our expression into CMOS gates. Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate."
        ],
        "score": [
            8.8163423538208,
            -11.119382858276367,
            -11.225553512573242
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What does the second version of the table imply about the buttons?\\n",
        "answers": [
            "\nThe second version of the table implies that the buttons are not placed in the most ergonomic way possible. The buttons are too close together and there is not enough space between the buttons and the edge of the table.",
            "The right strategy is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning.",
            "The inputs to the counter are the coins that are inserted into the machine. The machine will track the value of the coins in terms of nickels and will reject any coins that are inserted that would cause the value to exceed the maximum value that the machine can track."
        ],
        "score": [
            8.764388084411621,
            -11.256840705871582,
            -11.137176513671875
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What choices have been made about the counter design?', \"\\n\\n",
        "answers": [
            "\nThe design of the counter includes eight states, with three state bits used to keep track of the current state. The choice of representation for an FSM's states can dramatically affect the design complexity. The right strategy is thus often to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary.",
            "The output A does not depend on the input T because the function of AND is to evaluate to 1 if all of the input values are 1. If there is an input with value 0, then the function will evaluate to 0.",
            "DRAM is used for storing data in a capacitor. The capacitor is attached to a bit line through a transistor controlled by a select line. When the select line is low, the capacitor is isolated and holds its charge. However, the transistor's resistance is finite, and some charge leaks out onto the bit line."
        ],
        "score": [
            3.6345696449279785,
            -11.176218032836914,
            -11.224235534667969
        ],
        "text": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in"
    },
    {
        "question": " What are the two states in the first column of the table?\\n",
        "answers": [
            "\nThe two states in the first column of the table are the states of the FSM when F=0. In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.",
            "A tri-state buffer is a type of digital logic device that is used to either connect or disconnect a signal line from a power source or ground. This is achieved by using two transistors, one to connect the signal line to the power source, and the other to connect the signal line to ground. When the buffer is enabled, both transistors are turned on, and the signal line is connected to either the power source or ground, depending on the value of the input signal. When the buffer is disabled, both transistors are turned off, effectively disconnecting the signal line from the power source or ground.",
            "The state table form of a specification can make the missing parts of the specification more obvious by showing the different states that the system can be in and the transitions between those states. This can help to identify what parts of the specification are missing and what needs to be added in order to complete the specification."
        ],
        "score": [
            8.178496360778809,
            -9.841123580932617,
            -5.227304458618164
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What are the four counting states?\\n",
        "answers": [
            "\nThe four counting states are:\n\n1) State A: The timer is set to A when S_2=0. The duration of this state is A.\n\n2) State B: The timer is set to B when S_0=1. The duration of this state is B.\n\n3) State C: The timer is set to C when S_0=0. The duration of this state is C.\n\n4) State D: The timer is set to D when S_2=1. The duration of this state is D.",
            "Yes, we can choose to use the counter's internal state directly as our output values. This is because the colors we need to produce as outputs are all unique bit patterns. By using the counter's internal state directly as our output values, we can simplify our design.",
            "The fourth step in the design process is to choose a particular design from the set of possible designs. In making this choice, engineers typically consider a variety of factors, including cost, performance, manufacturability, and reliability."
        ],
        "score": [
            6.820558547973633,
            -11.175936698913574,
            -11.264824867248535
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What are the four halted states?\\n",
        "answers": [
            "\nThere are four halted states: Halt A, Halt B, Halt C, and Halt D. In each of these states, the counter retains its current value until the user presses the \"go\" button.",
            "The state table form of a specification can make the missing parts of the specification more obvious by showing the different states that the system can be in and the transitions between those states. This can help to identify what parts of the specification are missing and what needs to be added in order to complete the specification.",
            "The main difference between a mux and a decoder is that a mux has multiple inputs and one output, while a decoder has one input and multiple outputs. A mux is used to select one of several input signals and send it to the output, while a decoder is used to interpret an input signal and produce several output signals."
        ],
        "score": [
            8.258177757263184,
            -11.014448165893555,
            -11.195345878601074
        ],
        "text": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset"
    },
    {
        "question": " What does pressing the ``halt'' button do when the counter is halted?\\n",
        "answers": [
            "\nPressing the ``halt'' button does nothing when the counter is halted. The counter remains in the same state until the ``go'' button is pressed.",
            "The RAS signal indicates when the row component of an address is available. Raising the RAS signal for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh. The row is thus a natural basis for the refresh cycle.",
            "The FSM needs to be initialized by external logic filling the memory (the array of integer values), executing the FSM ``code,'' and then checking the answer.  To support this use model, we create a FSM state called { WAIT} for cycles in which the FSM has no work to do. Later, we also make use of an external input signal { START} to start the FSM execution."
        ],
        "score": [
            9.072290420532227,
            -10.788318634033203,
            -10.413640975952148
        ],
        "text": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux"
    },
    {
        "question": " What does pressing the ``go'' button do when the counter is counting?\", '\\n\\n",
        "answers": [
            "\nPressing the ``go'' button causes the counter to start counting.",
            "The state representation has a significant impact on the overall design of the system. It affects the way the system behaves and how it interacts with its environment. A well-chosen state representation can simplify the design of the system and make it more efficient.",
            "A Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all patterns in the second copy."
        ],
        "score": [
            7.898533344268799,
            -10.88242244720459,
            -10.576066970825195
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": " What is the purpose of an FSM?\\n",
        "answers": [
            "\nAn FSM is a finite state machine, which is a mathematical model of computation used to design both computer programs and digital logic circuits. It is an abstract machine that can be in one of a finite number of states.",
            "The final state table for the FSM is given in the passage. The table shows the states of the FSM, the outputs for each state, and the duration of each state.",
            "Data elements in a computer memory are stored either in static random access memory (SRAM) or in dynamic random access memory (DRAM). SRAM uses active logic in the form of a two-inverter loop to maintain stored values, while DRAM uses a charged capacitor to store a bit."
        ],
        "score": [
            5.902677059173584,
            -2.4251320362091064,
            -10.493576049804688
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What are the benefits of using an FSM?\\n",
        "answers": [
            "\nThere are several benefits of using an FSM. One benefit is that an FSM can be used to implement a wide range of logic functions. Another benefit is that an FSM can be used to construct larger memories from multiple smaller memories. Finally, an FSM can be used to reduce the number of pins needed on a chip.",
            "The second step in the design process is to think carefully about metrics of possible interest. Once you have a set of metrics that you feel is complete, your next step is to get rid of as many as you can.",
            "One could design a game controller for a logic puzzle that would have buttons corresponding to the different operations that could be performed on the logic puzzle. For example, there could be a button to add a new element to the puzzle, a button to remove an element from the puzzle, and a button to change the value of an element in the puzzle. The game controller could also have a display that would show the current state of the puzzle and the player's progress."
        ],
        "score": [
            8.62669563293457,
            -11.039651870727539,
            -11.20677661895752
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " What are the disadvantages of using an FSM?\\n",
        "answers": [
            "\nThere are a few disadvantages of using an FSM. First, if the FSM is not designed carefully, it can be hard to understand and debug. Second, an FSM can use more resources than a simpler design, such as a state machine. Finally, an FSM can be more difficult to change or extend than a simpler design.",
            "Starting with a human-based design process typically results in a design that is more efficient and effective than one that is mapped from a high-level programming language. This is because human designers can more easily optimize a design for specific metrics, such as power consumption or performance. Additionally, human designers can more easily impose constraints on a design, such as limits on chip area or minimum performance, in order to guide the design process.",
            "Prof. Jones assigns bit patterns in a way that makes it easy for humans to understand the meaning of the states. For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since the FSM starts in that state. However, for the other four states, the meanings are clear and easy to apply."
        ],
        "score": [
            8.465434074401855,
            -10.783607482910156,
            -5.481423377990723
        ],
        "text": "range corresponds to systems in which when a processor's power consumption in a digital  system is extremely low relative to the power consumption of the system. For example, the processor in a computer might use less than 1  of the total used by  the system including the disk drive, the monitor, the power  supply, and so forth.  One power consumption value in this range  is just as good as any another, and no one cares about the power consumption of the processor  in such cases.  In the second range, power consumption of the processor makes a difference.  Cell phones use most of their energy in radio operation, for example, but if you own a phone with a powerful processor, you may have noticed that you can turn off the phone and  drain the battery fairly quickly by playing a game.  Designing a processor that uses half as much power lengthens the battery life in such cases.  Finally, the third region of power consumption measurements is impossible: if you use so much power, your chip will overheat or even burst into flames.  Consumers get unhappy when such things happen.  As a first step, you can remove any metrics for which all solutions are effectively equivalent.  Until a little less than a decade ago, for example, the power  consumption of a desktop processor actually was in the first range that we discussed.  Power was simply not a concern to engineers: all designs of  interest consumed so little power that no one cared.  Unfortunately, at that point, power consumption jumped into the third range rather quickly.  Processors hit a wall, and  products had to be cancelled.  Given that the time spent designing a processor has historically been about five years, a lot of engineering effort was wasted because people had not thought carefully enough about power (since it had never mattered in the past).  Today, power is an important metric that engineers must take into account in their designs.   However, in some areas, such as desktop and high-end server processors, other metrics (such as performance) may be so  important that we always want to operate at the edge of the interesting range.  In such cases, we might choose to treat  a metric such as power consumption as a { threshold}: stay below 150 Watts for a desktop processor, for example.  One still has to make a coordinated effort to ensure that the system as a whole does not exceed the threshold, but reasoning about  threshold values, a form of constraint, is easier than trying to think about multiple metrics at once.  Some metrics may only allow discrete quantification.  For example, one could choose to define compatibility with previous processor generations as binary: either an existing piece of software (or operating system) runs out of the box on your new processor, or it does not.  If you  want people who own that software to make use of your new processor, you must ensure that the value of this binary metric is 1, which can also be viewed as a threshold.  In some cases, two metrics may be strongly { correlated}, meaning that a design that is good for one of the metrics is frequently  good for the other metric as well.  Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider them separately.  A design that requires a larger chip is probably more complex, and thus takes more engineering time to get right (engineering time costs money).    Each silicon wafer costs money to fabricate, and fewer copies of a  large design fit on one wafer, so large chips mean more fabrication cost.  Physical defects in silicon can cause some chips not to work.  A large chip uses more silicon than a small one, and is thus more likely to suffer from defects (and not work).  Cost thus goes up again for large chips relative to small ones.  Finally, large chips usually require more careful testing to ensure that they work properly (even ignoring the cost of getting the design right, we have to test for the presence of defects), which"
    },
    {
        "question": " How do you design an FSM?\\n",
        "answers": [
            "\nOne way to design an FSM is to start with a flow chart of the program to be implemented. This flow chart can be used to identify high-level states that can be implemented in a fixed number of clock cycles. Another way to design an FSM is to start with a list of components that are needed to implement the program. This list of components can be used to identify high-level states that can be implemented in a fixed number of clock cycles.",
            "Prof. Chris Schmitz prototyped and constructed the physical elements with some help from the ECE shop. He designed the vending machine application and the FSM, while Prof. Doug Jones created the wiki documentation and the Altera Quartus portions of the lab. Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other changes to improve robustness.",
            "SRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors, while a DRAM cell uses only one transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor"
        ],
        "score": [
            7.947826385498047,
            0.9704551100730896,
            -11.065873146057129
        ],
        "text": "statement initializes the minimum known value ({ min}) to the value stored at index 0 in the  array ({ values[0]}). The second statement is a loop in which the variable { index}  takes on values from 1 to 9.  For each value, an { if} statement compares the current known minimum with the value stored in the array at index given by the { idx} variable.  If the stored value is smaller, the current known  value (again, { min}) is updated to reflect the program's having found a smaller value.  When the loop finishes all nine iterations, the variable { min} holds the smallest value among the set of 10  integers stored in the { values} array.   As a first step towards designing an FSM to implement the code, we transform the code into a flow chart, as shown to the right.  The program again begins with initialization, which appears in the second column of the flow chart.   The loop in the program translates to the third column of the flow chart,  and the { if} statement to the middle comparison and update  of { min}.  Our goal is now to design an FSM to implement the flow chart.  In order to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless entry system with a timer.  Although the timer's value was technically also   {{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}   part of the FSM's state, we treated it as data and integrated it into our next-state decisions in only a couple of cases.  For our minimum value problem, we have two sources of data.  First, an external program supplies data in the form of a set of 10 integers.  If we assume {32-bit} integers, these data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value.  These are technically state bits.  For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it  using the components that we have developed earlier in our course.  {Choosing Components and Identifying States}  Now we are ready to design an FSM that implements the flow chart. What components do we need, other than our state logic? We use registers and counters to implement the variables { idx} and { min} in the program. For the array { values}, we use a {1632-bit}  memory.{We technically only need a {1032-bit}  memory, but we round up the size of the address space to reflect more realistic memory designs; one can always optimize later.} We need a comparator to implement the test for the { if} statement. We choose to use a serial comparator, which allows us to illustrate again how one logical high-level state can be subdivided into many actual states. To operate the serial comparator, we make use of two shift registers that  present the comparator with one bit per cycle on each input, and a counter to keep track of the comparator's progress.  How do we identify high-level states from our flow chart?  Although the flow chart attempts to break down the program into `simple' steps, one step of a flow chart may sometimes require more than one state in an FSM.  Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities.  How we map flow chart elements into FSM states also depends to some  degree on what components we use, which is why we began with some discussion of components.  In practice, one can go back and forth between the two,  adjusting components to better match the high-level states, and adjusting  states to better match the desired components.  Finally, note that we are only concerned with high-level states, so we do  not need to provide details (yet) down to the level of individual clock  cycles, but we do want to define high-level states that can be implemented in a fixed number of cycles, or at"
    },
    {
        "question": " How do you implement an FSM?']",
        "answers": [
            "\nThe FSM is implemented using registers, counters, and a comparator. The comparator is used to implement the if statement in the program. The registers and counters are used to implement the variables idx and min in the program. The memory is used to implement the array values in the program.",
            "The purpose of the FSM is to control the stoplights at a busy intersection. The FSM ensures that the lights on one or both roads are red at all times, that a road has a green light, and that the controller should show a yellow light before showing a red light.",
            "The state table form of a specification can make the missing parts of the specification more obvious by showing the different states that the system can be in and the transitions between those states. This can help to identify what parts of the specification are missing and what needs to be added in order to complete the specification."
        ],
        "score": [
            7.988395690917969,
            3.8394582271575928,
            -10.502201080322266
        ],
        "text": "statement initializes the minimum known value ({ min}) to the value stored at index 0 in the  array ({ values[0]}). The second statement is a loop in which the variable { index}  takes on values from 1 to 9.  For each value, an { if} statement compares the current known minimum with the value stored in the array at index given by the { idx} variable.  If the stored value is smaller, the current known  value (again, { min}) is updated to reflect the program's having found a smaller value.  When the loop finishes all nine iterations, the variable { min} holds the smallest value among the set of 10  integers stored in the { values} array.   As a first step towards designing an FSM to implement the code, we transform the code into a flow chart, as shown to the right.  The program again begins with initialization, which appears in the second column of the flow chart.   The loop in the program translates to the third column of the flow chart,  and the { if} statement to the middle comparison and update  of { min}.  Our goal is now to design an FSM to implement the flow chart.  In order to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless entry system with a timer.  Although the timer's value was technically also   {{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}   part of the FSM's state, we treated it as data and integrated it into our next-state decisions in only a couple of cases.  For our minimum value problem, we have two sources of data.  First, an external program supplies data in the form of a set of 10 integers.  If we assume {32-bit} integers, these data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value.  These are technically state bits.  For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it  using the components that we have developed earlier in our course.  {Choosing Components and Identifying States}  Now we are ready to design an FSM that implements the flow chart. What components do we need, other than our state logic? We use registers and counters to implement the variables { idx} and { min} in the program. For the array { values}, we use a {1632-bit}  memory.{We technically only need a {1032-bit}  memory, but we round up the size of the address space to reflect more realistic memory designs; one can always optimize later.} We need a comparator to implement the test for the { if} statement. We choose to use a serial comparator, which allows us to illustrate again how one logical high-level state can be subdivided into many actual states. To operate the serial comparator, we make use of two shift registers that  present the comparator with one bit per cycle on each input, and a counter to keep track of the comparator's progress.  How do we identify high-level states from our flow chart?  Although the flow chart attempts to break down the program into `simple' steps, one step of a flow chart may sometimes require more than one state in an FSM.  Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities.  How we map flow chart elements into FSM states also depends to some  degree on what components we use, which is why we began with some discussion of components.  In practice, one can go back and forth between the two,  adjusting components to better match the high-level states, and adjusting  states to better match the desired components.  Finally, note that we are only concerned with high-level states, so we do  not need to provide details (yet) down to the level of individual clock  cycles, but we do want to define high-level states that can be implemented in a fixed number of cycles, or at"
    },
    {
        "question": " What is the input behavior?\\n",
        "answers": [
            "\nThe input behavior of the machine is based on the type of coin inserted and the selection of an item for purchase. If the user inserts a coin that is not a nickel, the machine will reject the coin and remain in the same state. If the user inserts a nickel, the machine will transition to the next state and accept the coin. If the user selects an item for purchase, the machine will release the item if the user has inserted enough money. Otherwise, the machine will remain in the same state.",
            "The input sequence for initialization is: 1) the value in R0 is 10, 2) the value in R1 is 20, and 3) the value in R2 is 30.",
            "The two most common types of memory used in digital systems are static random access memory (SRAM) and dynamic random access memory (DRAM). SRAM is used for high-speed applications such as processor caches and some embedded designs, while DRAM is used for applications with less demanding speed requirements, such as main memory in most computers."
        ],
        "score": [
            3.7804887294769287,
            -7.190773963928223,
            -11.154412269592285
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What is the output behavior?\\n",
        "answers": [
            "\nThe output behavior is that the add3 procedure takes three integers as arguments, adds them together, and returns the sum. The procedure is called with the constants 10, 20, and 30, and the result is printed. By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.",
            "S_2 is 1 when the counter is in the HALT A state and HOLD=1. This is because when S_2=1, the counter is halted, and when HOLD=1, the counter should retain its current value.",
            "The fifth step in the design process is to select an internal representation for the bits necessary to encode the state of our FSM. In practice, for small designs, this representation can be selected by a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand."
        ],
        "score": [
            6.539098262786865,
            -10.568037986755371,
            -11.27149772644043
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": " What does the ``halt'' button do?\\n",
        "answers": [
            "\nThe ``halt'' button on the ice cream dispenser stops the machine from dispensing any more ice cream. This is useful if the user wants to stop the machine from dispensing more ice cream, or if the machine is malfunctioning and needs to be stopped.",
            "The outputs needed to observe in each state to check the output logic are the values of the state register S and the timer. These outputs can be used to determine the duration of each state and the order in which the states occur.",
            "The signal F allows the timer to be set under other conditions, in addition to when the state S changes. This allows for more flexibility in the duration of each state."
        ],
        "score": [
            7.702926158905029,
            -10.95316219329834,
            -8.16433334350586
        ],
        "text": "However, if the person who designed the dispenser assumed that no one would ever ask for more than one serving, something worse might happen.  In other words, giving an input of C_L=11 to the ice cream dispenser may lead to other unexpected behavior if its designer decided that that input  pattern was a ``don't care.''  The root of the problem is that { while we don't care about the value of any particular output marked ``x'' for any particular input combination, we do actually care about the relationship between the outputs}.    What can we do?  When in doubt, it is safest to make  choices and to add the new decisions to the specification rather than  leaving output values specified as ``don't care.''  For our ice cream dispenser logic, rather than leaving the outputs  unspecified whenever a user presses more than one button, we could  choose an acceptable outcome for each input combination and  replace the x's with 0s and 1s.  We might, for example, decide to produce lychee ice cream whenever the lychee button is pressed, regardless of other buttons (LBM=1xx, which means that we don't care about the inputs B and M, so LBM=100, LBM=101, LBM=110, or LBM=111).   That decision alone covers three of the four unspecified input patterns.  We might also decide that when the  blend and mango buttons are pushed together (but without the lychee button, LBM=011), our logic produces a blend.    The resulting K-maps are shown below, again with shading and green italics  identifying the combinations in which our original design specified ``don't care.''  {{file=part2/figs/CLhigh-priority.eps,width=1.00in}{file=part2/figs/CLlow-priority.eps,width=1.00in}{file=part2/figs/CMhigh-priority.eps,width=1.00in}{file=part2/figs/CMlow-priority.eps,width=1.00in}}   The logic in the dashed box to the right implements the set of choices just discussed, and matches the K-maps above.  Based on our additional choices, this implementation enforces a strict priority scheme on the user's button presses.  If a user requests lychee, they can also press either or both of the other buttons with no effect.  The lychee button has priority.  Similarly, if the user does not press lychee, but press-      es the blend button, pressing the mango button at the same time has no effect.  Choosing mango requires that no other buttons be pressed. We have thus chosen a prioritization order for the buttons and imposed  this order on the design.  We can view this same implementation in another way.  Note the one-to-one correspondence between inputs (on the left) and  outputs (on the right) for the dashed box.  This logic takes the user's button presses and chooses at most one of the buttons to pass along to our original controller implementation  (to the right of the dashed box).  In other words, rather than thinking of the logic in the dashed box as implementing a specific set of decisions, we can think of the logic as cleaning up the inputs to ensure that only valid combinations are passed to our original implementation.  Once the inputs are cleaned up, the original implementation is  acceptable, because input combinations containing more than a  single 1 are in fact impossible.  Strict prioritization is one useful way to clean up our inputs.  In general, we can design logic to map each of the four undesirable input patterns into one of the permissible  combinations (the four that we specified explicitly in our original design, with LBM  in the set ).  Selecting a prioritization scheme is just one approach for making these choices in a way that is easy for a user to understand and  is fairly easy to implement.   A second simple approach is to ignore illegal combinations by mapping them into the ``no buttons pressed''  input pattern.  Such an implementation appears to the right, laid out to show that one can again view the logic in the dashed box either as cleaning up  the inputs (by mentally grouping the logic with the inputs) or as a specific  set of choices for our ``don't"
    },
    {
        "question": " What does the ``go'' button do?\\n",
        "answers": [
            "\nThe \"go\" button initiates a system call that causes the processor to enter kernel mode and execute the instructions associated with the system call. These instructions are part of the operating system and are used to perform the desired action, such as printing a character to the console.",
            "The next-state logic for S_1^+ and S_0^+ when HOLD=0 is as follows:\n\nS_1^+ = S_0 + HOLD\n\nS_0^+ = S_1 + HOLD",
            "The value of register A is the equivalence class of the sum of the two 4-bit numbers. In this case, the sum is 18, which is equivalent to 2 (mod 16)."
        ],
        "score": [
            6.153947353363037,
            -10.91103458404541,
            -10.884904861450195
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": " What is the two-bit Gray code?\", '\\n\\n",
        "answers": [
            "\nThe two-bit Gray code is a sequence of two-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.",
            "A bad state representation can lead to wasted engineering effort, as well as products that have to be cancelled due to poor performance. In some cases, a bad state representation can also lead to increased costs for fabrication and testing.",
            "What is the significance of DeMorgan's law?\n\nDeMorgan's law is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. In this section, we illustrate one of the reasons for this popularity and show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams."
        ],
        "score": [
            8.912349700927734,
            -11.188886642456055,
            -9.877685546875
        ],
        "text": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more"
    },
    {
        "question": " What are the two control buttons?\\n",
        "answers": [
            "\nThere are two control buttons: one for the east-west road and one for the north-south road.",
            "The value of register A is the equivalence class of the sum of the two 4-bit numbers. In this case, the sum is 18, which is equivalent to 2 (mod 16).",
            "One way to improve a bad state representation is to make it more compact. This can be done by using two-level logic instead of three-level logic. Another way to improve a bad state representation is to make it more readable. This can be done by using clear and consistent naming conventions."
        ],
        "score": [
            6.519120216369629,
            -10.898998260498047,
            -11.185567855834961
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the purpose of a Gray code?\\n",
        "answers": [
            "\nA Gray code is a sequence of patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit. You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns.",
            "The \"go\" button initiates a system call that causes the processor to enter kernel mode and execute the instructions associated with the system call. These instructions are part of the operating system and are used to perform the desired action, such as printing a character to the console.",
            "The second step in the design process is to think carefully about metrics of possible interest. Once you have a set of metrics that you feel is complete, your next step is to get rid of as many as you can."
        ],
        "score": [
            5.565742015838623,
            -10.885624885559082,
            -11.22281265258789
        ],
        "text": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more"
    },
    {
        "question": " What do the states represent in the transition diagram?\\n",
        "answers": [
            "\nThe states in the transition diagram represent the different colors of the stoplight. The first two states are red, followed by yellow, and then green. The last two states are again red.",
            "The value of register PRICE3 is the price of the third item in the machine, which is determined by the owner of the machine.",
            "SRAM cells are combined into a bit slice by sharing bit lines and analog read and write logic. The bit slice in the figure can be thought of as a 16-address, 1-bit-addressable memory (2^4b). The cells in a bit slice share bit lines and analog read and write logic, which appears to the right in the figure."
        ],
        "score": [
            7.944432258605957,
            -11.193672180175781,
            -11.140960693359375
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What does the uninterrupted counting cycle represent?\\n",
        "answers": [
            "\nThe uninterrupted counting cycle represents the duration of each state in the FSM. This is used to calculate the duration of each state in the FSM when F=0.",
            "The player wins the game by getting all four entities (farmer, fox, goose, and corn) to the right bank of the river. If the player makes a mistake, the game is over and they lose.",
            "Jaramillo later identified common failure modes, including variability caused by sunshine through the windows in ECEB, and made some changes to improve robustness. He also created the PowerPoint slides that are typically used to describe the lab in lecture."
        ],
        "score": [
            8.719135284423828,
            -11.294684410095215,
            -11.309208869934082
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How many input bits are there?', \" ",
        "answers": [
            "\nThere are two input bits.",
            "A sense amplifier is a device that is used to amplify the signal from a sensor. It is typically used to amplify the signal from a small sensor, such as a pressure sensor, to a level that can be read by a larger device, such as a computer.",
            "A sense amplifier is a device that is used to amplify the signal from a sensor. It is typically used to amplify the signal from a small sensor, such as a pressure sensor, to a level that can be read by a larger device, such as a computer."
        ],
        "score": [
            7.7141876220703125,
            -11.095952033996582,
            -11.095952033996582
        ],
        "text": "and lower-case checker, we can make use of two groups of seven {2-to-1} muxes, all controlled by a single bit select signal S, to choose between the inputs needed for an upper-case checker and those needed for a lower-case checker.  Specific configurations of multiplexers are often referred to as { N-to-M multiplexers}.  Here the value N refers to the number of inputs, and M refers to the number of outputs.  The number of select bits can then be calculated as _2(N/M)---N/M  is generally a power of two---and one way to build such a  multiplexer is to use M copies of an  (N/M)-to-1 multiplexer.  Let's extend our upper- and lower-case checker to check for four different ranges of ASCII characters, as shown below. This design uses two {28-to-7} muxes to create a single checker for the four ranges.  Each of the muxes in the figure logically  represents seven {4-to-1} muxes.  {{file=part2/figs/ascii-four-range.eps,width=3.75in}}   The table to the right describes the behavior of the checker.  When the select input S is set to 00, the left mux selects the value 0x00, and the right mux selects the value 0x1F, which checks whether the ASCII character represented by C is a control character.  When the select input S=01, the muxes produce the values needed to check whether C is an upper-case letter.  Similarly, when the select input S=10,   {c|c|c|c} & left& right&  & comparator& comparator&  S_1S_0& input& input& R(C) produced   00& 0x00& 0x1F& control character? 01& 0x41& 0x5A& upper-case letter? 10& 0x61& 0x7A& lower-case letter? 11& 0x30& 0x39& numeric digit?     the muxes produce the values  needed to check whether C is a lower-case letter.  Finally, when the select input S=11, the left mux selects the value 0x30, and the right mux selects the value 0x39, which checks whether the ASCII character represented by C is a digit (0 to 9).     {Example: Bit-Sliced Comparison}  This set of notes develops comparators for unsigned and 2's complement  numbers using the bit-sliced approach that we introduced in Notes Set 2.3.   We then use algebraic manipulation and variation of the internal  representation to illustrate design tradeoffs.   {Comparing Two Numbers}   Let's begin by thinking about how we as humans compare two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation, so we can just think of them as binary numbers with leading 0s.  We handle 2's complement values later in these notes.  As humans, we typically start comparing at the most significant bit. After all, if we find a difference in that bit, we are done, saving ourselves some time.  In the example to the right, we know that A<B  as soon as we reach bit 4 and observe that A_4<B_4.  If we instead start from the least significant bit, we must always look at all of the bits.  When building hardware to compare all of the bits at once, however, hardware for comparing each bit must exist, and the final result  must be able to consider      all of the bits.  Our choice of direction should thus instead depend on  how effectively we can build the  corresponding functions.  For a single bit slice, the two directions  are almost identical.  Let's develop a bit slice for comparing from least to most significant.  { NOTE TO SELF: We should either do the bit-by-bit comparator  state machine in the notes or as an FSM homework problem.  Probably the lattter.}  {An Abstract Model}  Comparison of two numbers, A and B, can produce three possible answers: A<B, A=B, or A>B (one can also build an equality comparator that combines the A<B and A>B cases into a single  answer).  As we move from bit to bit in our design, how much information needs  to pass from one bit to the next?  Here you may want to think about how you perform the task yourself. And perhaps to focus on the calculation for the most significant bit. You need to know the values of the two bits that you are comparing. If those two"
    },
    {
        "question": "What is the input behavior?\\n ",
        "answers": [
            "\nSystem calls are initiated with a trap instruction, and system calls are also known as traps. With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.",
            "The output behavior is that the add3 procedure takes three integers as arguments, adds them together, and returns the sum. The procedure is called with the constants 10, 20, and 30, and the result is printed. By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.",
            "The third step in the design process is to identify metrics that have no practical impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality."
        ],
        "score": [
            -10.760251998901367,
            3.954176902770996,
            -11.263141632080078
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": "What is the output behavior?\\n ",
        "answers": [
            "\nThe output behavior is that the add3 procedure takes three integers as arguments, adds them together, and returns the sum. The procedure is called with the constants 10, 20, and 30, and the result is printed. By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.",
            "The FSM should be designed to handle purchases, with the current number of nickels stored in a register. Each cycle, the register is fed into an adder, which subtracts the price of any purchase requested in that cycle. The priority encoder's S signal is then used to select the value of one of three registers as the second input to the adder. The adder calculates a carry out signal C_, which is used to indicate a successful purchase. The signal R is then used to select the next value of N using a mux, and is also used to enable a decoder that generates the three individual item release outputs.",
            "A SRAM cell is organized as a flip-flop, with one transistor and one capacitor per bit. The capacitor stores the data bit, and the transistor controls access to the bit."
        ],
        "score": [
            6.539098262786865,
            -8.491403579711914,
            -11.278366088867188
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": "What is the purpose of using a Gray code?\\n ",
        "answers": [
            "\nThe purpose of using a Gray code is to create a sequence of patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.",
            "The next-state logic for S_1^+ and S_0^+ when HOLD=0 is as follows:\n\nS_1^+ = S_0 + HOLD\n\nS_0^+ = S_1 + HOLD",
            "The choice of representation for the FSM's internal state affects the complexity of the implementation in terms of the number of states, the number of inputs, the number of outputs, and the number of transition rules."
        ],
        "score": [
            8.917501449584961,
            -11.02839183807373,
            -11.10520076751709
        ],
        "text": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more"
    },
    {
        "question": "How does the design continue until we press H?\\n ",
        "answers": [
            "\nThe design continues by choosing a state representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning.",
            "DRAM is a type of memory that uses capacitors to store data. When a row of DRAM is selected, all of the cells in that row are read and then written back, which refreshes the data in those cells. DRAM is said to have destructive reads because each read operation brings the voltage on the capacitor closer to the intermediate voltage level, which effectively destroys the data in the cell.",
            "A SRAM cell is organized as a flip-flop, with one transistor and one capacitor per bit. The capacitor stores the data bit, and the transistor controls access to the bit."
        ],
        "score": [
            -0.15353409945964813,
            -11.277114868164062,
            -11.301668167114258
        ],
        "text": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in"
    },
    {
        "question": "What would happen if we didn't use a Gray code?\"]",
        "answers": [
            "\nIf we didn't use a Gray code, we would have to compare two expressions to determine whether they represent the same logic function. This would be a test of logical equivalence, and is an important part of hardware design. However, tools today provide help with this problem, so it is not necessary to use a Gray code.",
            "The FSM is a stoplight controller that ensures that the lights on one or both roads are red at all times. The controller also alternates green lights between the two roads.",
            "The FSM is responsible for controlling the stoplights at a busy intersection. It ensures that the lights on one or both roads are red at all times, that a road has a green light, and that the controller should show a yellow light before showing a red light."
        ],
        "score": [
            8.760442733764648,
            -11.155268669128418,
            -11.17529296875
        ],
        "text": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more"
    },
    {
        "question": " What should happen if the user presses the ``go'' button while in state { COUNT A}?\\n",
        "answers": [
            "\nIf the user presses the \"go\" button while in state {COUNT A}, then the outputs are C_L=11 and C_M=01. This means that the user will get one and a half servings of lychee and a half serving of mango.",
            "The output of the FSM is a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.",
            "Mapping down to bits and gates means to design a circuit that performs an operation on a single bit and passes any necessary information into another copy of itself. By using copies of this bit-sliced adder circuit, we can mimic our approach as humans and build adders of any size."
        ],
        "score": [
            6.720355033874512,
            -10.097630500793457,
            -10.904190063476562
        ],
        "text": "implementation of  a specification  implies decisions} about these details, so decisions should only be left  unspecified if any of the possible answers is indeed acceptable.  As a concrete example, let's design logic to control an ice cream  dispenser.  The dispenser has two flavors, lychee and mango, but also allows us to create a blend of the two flavors. For each of the two flavors, our logic must output two bits to control the amount of ice cream that comes out of the dispenser. The two-bit C_L[1:0] output of our logic must specify the number  of half-servings of lychee ice cream as a binary number, and the two-bit C_M[1:0] output must specify the number of  half-servings of mango ice cream.  Thus, for either flavor, 00 indicates none of that flavor, 01 indicates one-half of a serving, and  10 indicates a full serving.  Inputs to our logic will consist of three buttons: an L button to request a serving of lychee ice cream, a B button to request a blend---half a serving of each flavor, and an M button to request a serving of mango ice cream.  Each button produces a 1  when pressed and a 0 when not pressed.    Let's start with the assumption that the user only presses one button at a time.  In this case, we can treat input combinations in which more than one button is pressed as ``don't care'' values in the truth tables for the outputs.  K-maps for all four output bits appear below. The x's indicate ``don't care'' values.    When we calculate the logic function for an output, each ``don't care'' value can be treated as either 0 or 1, whichever is more convenient in terms of creating the logic.  In the case of C_M[1], for  example, we can treat the three x's in the ellipse as 1s, treat the x outside of the ellipse as a 0, and simply use M (the implicant represented by the ellipse) for C_M[1].  The other three output bits are left as an exercise, although the result  appears momentarily.   The implementation at right takes full advantage of the ``don't care'' parts of our specification.  In this case, we require no logic at all; we need merely connect the inputs to the correct outputs.  Let's verify the operation.  We have four cases to consider.  First, if none of the  buttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00  and C_L=00).  Second, if we request lychee ice cream (LBM=100), the outputs are C_L=10 and C_M=00, so we get a full serving of lychee and no mango.  Third, if we request a blend (LBM=010), the outputs are C_L=01 and C_M=01, giving us half a serving of each flavor. Finally, if we request mango ice cream (LBM=001), we get no lychee but a full serving of mango.      The K-maps for this implementation appear below.  Each of the ``don't care'' x's from the original design has been replaced with either a 0 or a 1 and highlighted with shading and green italics. Any implementation produces  either 0 or 1 for every output bit for every possible input combination.  {{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}  As you can see, leveraging ``don't care'' output bits can sometimes significantly simplify our logic.  In the case of this example, we were able to completely eliminate any need for gates!  Unfortunately,  the resulting implementation may sometimes produce unexpected results.   Based on the implementation, what happens if a user presses more than one button?  The ice cream cup overflows!  Let's see why.  Consider the case LBM=101, in which we've pressed both the lychee and mango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a  full serving of each flavor, or two servings total.  Pressing other  combinations may have other repercussions as well. Consider pressing lychee and blend (LBM=110).  The outputs are then C_L=11 and C_M=01.  Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango."
    },
    {
        "question": " What should happen if the user presses the ``halt'' button while in state { COUNT A}?\\n",
        "answers": [
            "\nThere is no explicit answer to this question in the passage. However, based on the information given, it seems that if the user presses the ``halt'' button while in state { COUNT A}, the system should reset to state { HALT A}.",
            "The color mapping for a three-bit RGB encoding system is a system where colors are represented by a combination of red, green, and blue light. The color mapping for such a system is shown to the right. Imagine that you are charged with creating a counter to drive a light through a sequence of colors.",
            "The FSM is responsible for controlling the stoplights at a busy intersection. It ensures that the lights are always red on one or both roads, that a green light is always followed by a yellow light, and that the green lights are alternated between the two roads."
        ],
        "score": [
            9.045526504516602,
            -10.980896949768066,
            -10.611939430236816
        ],
        "text": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset"
    },
    {
        "question": " What should happen if the user presses the ``go'' button while in state { HALT A}?\\n",
        "answers": [
            "\nThere are two possible outcomes when the user presses the ``go'' button while in state { HALT A}. The first outcome is that the FSM moves to state { COUNT B}, which is the next state in the sequence. The second outcome is that the FSM resets to state { COUNT A}, which is the initial state.",
            "The priority encoder is used to determine which item the user wants to purchase, and to generate a signal indicating that the user is trying to make a purchase. The output of the priority encoder is used to select the value of one of the price registers, which is then subtracted from the current number of nickels stored in a register. If the purchase is successful, the price is subtracted from the number of nickels and the item is released.",
            "An FSM is a finite state machine, which is a mathematical model of computation used to design both computer programs and digital logic circuits. It is an abstract machine that can be in one of a finite number of states."
        ],
        "score": [
            9.262462615966797,
            -10.246829986572266,
            -10.216352462768555
        ],
        "text": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset"
    },
    {
        "question": " What should happen if the user presses the ``halt'' button while in state { HALT A}?\\n",
        "answers": [
            "\nThere are a few possible ways to answer this question. One way would be to say that if the user presses the ``halt'' button while in state { HALT A}, the system should reset to state { COUNT A}. Another way to answer this question would be to say that if the user presses the ``halt'' button while in state { HALT A}, the system should remain in state { HALT A}.",
            "A Gray code is a sequence of patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit. You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns.",
            "The issue with the FSM getting into unexpected states is that it can cause the output of the circuit to glitch. This can be problematic in some applications where a stable output is required."
        ],
        "score": [
            9.287548065185547,
            -10.829480171203613,
            -8.844315528869629
        ],
        "text": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset"
    },
    {
        "question": " What should happen if the user presses both buttons while in state { HALT A}?\", \"\\n",
        "answers": [
            "\nThere are two possible ways that the user could press both buttons while in state { HALT A}. The first way is that the user could press the \"halt\" button and then the \"go\" button. In this case, the FSM would move to state { HALT A} and then to state { COUNT B}. The second way is that the user could press the \"go\" button and then the \"halt\" button. In this case, the FSM would move to state { COUNT B} and then to state { HALT A}.",
            "A priority encoder is a type of encoder that assigns priority to certain input signals. The priority encoder takes in a signal and produces an output signal that indicates which input signal has the highest priority. The output signal is typically a binary number that is used to select one of the input signals.",
            "There are 64 bit patterns available for the PAID state."
        ],
        "score": [
            7.855250358581543,
            -10.961692810058594,
            -10.501178741455078
        ],
        "text": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset"
    },
    {
        "question": " How do you determine what should happen if a user presses the ``go'' button (HG=01)?\\n",
        "answers": [
            "\nIf a user presses the \"go\" button (HG=01), the ice cream cup will overflow. This is because the outputs are C_L=11 and C_M=10, which means that the dispenser will release a full serving of each flavor, or two servings total.",
            "The HOLD variable simplifies the design by allowing the designer to separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.",
            "The FSM has five parts: a finite set of states, a set of possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs. In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops."
        ],
        "score": [
            6.620805740356445,
            -8.979323387145996,
            -10.562533378601074
        ],
        "text": "implementation of  a specification  implies decisions} about these details, so decisions should only be left  unspecified if any of the possible answers is indeed acceptable.  As a concrete example, let's design logic to control an ice cream  dispenser.  The dispenser has two flavors, lychee and mango, but also allows us to create a blend of the two flavors. For each of the two flavors, our logic must output two bits to control the amount of ice cream that comes out of the dispenser. The two-bit C_L[1:0] output of our logic must specify the number  of half-servings of lychee ice cream as a binary number, and the two-bit C_M[1:0] output must specify the number of  half-servings of mango ice cream.  Thus, for either flavor, 00 indicates none of that flavor, 01 indicates one-half of a serving, and  10 indicates a full serving.  Inputs to our logic will consist of three buttons: an L button to request a serving of lychee ice cream, a B button to request a blend---half a serving of each flavor, and an M button to request a serving of mango ice cream.  Each button produces a 1  when pressed and a 0 when not pressed.    Let's start with the assumption that the user only presses one button at a time.  In this case, we can treat input combinations in which more than one button is pressed as ``don't care'' values in the truth tables for the outputs.  K-maps for all four output bits appear below. The x's indicate ``don't care'' values.    When we calculate the logic function for an output, each ``don't care'' value can be treated as either 0 or 1, whichever is more convenient in terms of creating the logic.  In the case of C_M[1], for  example, we can treat the three x's in the ellipse as 1s, treat the x outside of the ellipse as a 0, and simply use M (the implicant represented by the ellipse) for C_M[1].  The other three output bits are left as an exercise, although the result  appears momentarily.   The implementation at right takes full advantage of the ``don't care'' parts of our specification.  In this case, we require no logic at all; we need merely connect the inputs to the correct outputs.  Let's verify the operation.  We have four cases to consider.  First, if none of the  buttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00  and C_L=00).  Second, if we request lychee ice cream (LBM=100), the outputs are C_L=10 and C_M=00, so we get a full serving of lychee and no mango.  Third, if we request a blend (LBM=010), the outputs are C_L=01 and C_M=01, giving us half a serving of each flavor. Finally, if we request mango ice cream (LBM=001), we get no lychee but a full serving of mango.      The K-maps for this implementation appear below.  Each of the ``don't care'' x's from the original design has been replaced with either a 0 or a 1 and highlighted with shading and green italics. Any implementation produces  either 0 or 1 for every output bit for every possible input combination.  {{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}  As you can see, leveraging ``don't care'' output bits can sometimes significantly simplify our logic.  In the case of this example, we were able to completely eliminate any need for gates!  Unfortunately,  the resulting implementation may sometimes produce unexpected results.   Based on the implementation, what happens if a user presses more than one button?  The ice cream cup overflows!  Let's see why.  Consider the case LBM=101, in which we've pressed both the lychee and mango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a  full serving of each flavor, or two servings total.  Pressing other  combinations may have other repercussions as well. Consider pressing lychee and blend (LBM=110).  The outputs are then C_L=11 and C_M=01.  Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango."
    },
    {
        "question": " How do you determine what should happen if a user presses the ``halt'' button (HG=10)?\\n",
        "answers": [
            "\nIf the ``halt'' button is pressed (HG=10), the counter remains halted in state { HALT A}. If both buttons are pressed (HG=11), ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted. Thus, input patterns HG=10 and HG=11 also take state { HALT A} back to itself.",
            "A sense amplifier is a device that is used to amplify the signal from a sensor. It is typically used to amplify the signal from a small sensor, such as a pressure sensor, to a level that can be read by a larger device, such as a computer.",
            "SRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors, while a DRAM cell uses only one transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor"
        ],
        "score": [
            8.12118911743164,
            -10.450634002685547,
            -10.783525466918945
        ],
        "text": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in"
    },
    {
        "question": " How do you determine what should happen if a user presses both buttons (HG=11)?\\n",
        "answers": [
            "\nIf a user presses both buttons (HG=11), the ice cream cup overflows. This is because the outputs are C_L=11 and C_M=10, so our dispenser releases a full serving of each flavor, or two servings total.",
            "A decoder is a device that converts coded signals into usable information. Decoders are used in a variety of applications, such as error detection and correction.",
            "A tri-state buffer is a type of digital logic circuit that is used to either connect or disconnect a signal line from a power supply or ground. This is done by using two transistors, one to connect the signal line to the power supply, and the other to connect the signal line to ground. When the buffer is enabled, both transistors are turned on, and the signal line is connected to either the power supply or ground, depending on the value of the input signal. When the buffer is disabled, both transistors are turned off, and the signal line is effectively disconnected from both the power supply and ground."
        ],
        "score": [
            6.968435287475586,
            -10.643646240234375,
            -10.330379486083984
        ],
        "text": "implementation of  a specification  implies decisions} about these details, so decisions should only be left  unspecified if any of the possible answers is indeed acceptable.  As a concrete example, let's design logic to control an ice cream  dispenser.  The dispenser has two flavors, lychee and mango, but also allows us to create a blend of the two flavors. For each of the two flavors, our logic must output two bits to control the amount of ice cream that comes out of the dispenser. The two-bit C_L[1:0] output of our logic must specify the number  of half-servings of lychee ice cream as a binary number, and the two-bit C_M[1:0] output must specify the number of  half-servings of mango ice cream.  Thus, for either flavor, 00 indicates none of that flavor, 01 indicates one-half of a serving, and  10 indicates a full serving.  Inputs to our logic will consist of three buttons: an L button to request a serving of lychee ice cream, a B button to request a blend---half a serving of each flavor, and an M button to request a serving of mango ice cream.  Each button produces a 1  when pressed and a 0 when not pressed.    Let's start with the assumption that the user only presses one button at a time.  In this case, we can treat input combinations in which more than one button is pressed as ``don't care'' values in the truth tables for the outputs.  K-maps for all four output bits appear below. The x's indicate ``don't care'' values.    When we calculate the logic function for an output, each ``don't care'' value can be treated as either 0 or 1, whichever is more convenient in terms of creating the logic.  In the case of C_M[1], for  example, we can treat the three x's in the ellipse as 1s, treat the x outside of the ellipse as a 0, and simply use M (the implicant represented by the ellipse) for C_M[1].  The other three output bits are left as an exercise, although the result  appears momentarily.   The implementation at right takes full advantage of the ``don't care'' parts of our specification.  In this case, we require no logic at all; we need merely connect the inputs to the correct outputs.  Let's verify the operation.  We have four cases to consider.  First, if none of the  buttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00  and C_L=00).  Second, if we request lychee ice cream (LBM=100), the outputs are C_L=10 and C_M=00, so we get a full serving of lychee and no mango.  Third, if we request a blend (LBM=010), the outputs are C_L=01 and C_M=01, giving us half a serving of each flavor. Finally, if we request mango ice cream (LBM=001), we get no lychee but a full serving of mango.      The K-maps for this implementation appear below.  Each of the ``don't care'' x's from the original design has been replaced with either a 0 or a 1 and highlighted with shading and green italics. Any implementation produces  either 0 or 1 for every output bit for every possible input combination.  {{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}  As you can see, leveraging ``don't care'' output bits can sometimes significantly simplify our logic.  In the case of this example, we were able to completely eliminate any need for gates!  Unfortunately,  the resulting implementation may sometimes produce unexpected results.   Based on the implementation, what happens if a user presses more than one button?  The ice cream cup overflows!  Let's see why.  Consider the case LBM=101, in which we've pressed both the lychee and mango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a  full serving of each flavor, or two servings total.  Pressing other  combinations may have other repercussions as well. Consider pressing lychee and blend (LBM=110).  The outputs are then C_L=11 and C_M=01.  Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango."
    },
    {
        "question": " What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?\\n",
        "answers": [
            "\nIf the \"halt\" button is pressed (HG=10), the logic outputs C_L=00 and C_M=00, so the dispenser gives no ice cream. If both buttons are pressed (HG=11), the outputs are C_L=11 and C_M=11, so the dispenser gives two servings of each flavor, or four servings total.",
            "What are some of the ideas that can be used to reduce the number of metrics?\n\nSome of the ideas that can be used to reduce the number of metrics include: identifying metrics that have no practical impact in current technology, setting threshold values for other metrics to simplify reasoning about them, eliminating redundant metrics, calculating linear sums to reduce the count of metrics, and making use of the notion of Pareto optimality.",
            "The third step in the design process is to identify metrics that have no practical impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality."
        ],
        "score": [
            8.208681106567383,
            -9.93566608428955,
            -9.47227668762207
        ],
        "text": "implementation of  a specification  implies decisions} about these details, so decisions should only be left  unspecified if any of the possible answers is indeed acceptable.  As a concrete example, let's design logic to control an ice cream  dispenser.  The dispenser has two flavors, lychee and mango, but also allows us to create a blend of the two flavors. For each of the two flavors, our logic must output two bits to control the amount of ice cream that comes out of the dispenser. The two-bit C_L[1:0] output of our logic must specify the number  of half-servings of lychee ice cream as a binary number, and the two-bit C_M[1:0] output must specify the number of  half-servings of mango ice cream.  Thus, for either flavor, 00 indicates none of that flavor, 01 indicates one-half of a serving, and  10 indicates a full serving.  Inputs to our logic will consist of three buttons: an L button to request a serving of lychee ice cream, a B button to request a blend---half a serving of each flavor, and an M button to request a serving of mango ice cream.  Each button produces a 1  when pressed and a 0 when not pressed.    Let's start with the assumption that the user only presses one button at a time.  In this case, we can treat input combinations in which more than one button is pressed as ``don't care'' values in the truth tables for the outputs.  K-maps for all four output bits appear below. The x's indicate ``don't care'' values.    When we calculate the logic function for an output, each ``don't care'' value can be treated as either 0 or 1, whichever is more convenient in terms of creating the logic.  In the case of C_M[1], for  example, we can treat the three x's in the ellipse as 1s, treat the x outside of the ellipse as a 0, and simply use M (the implicant represented by the ellipse) for C_M[1].  The other three output bits are left as an exercise, although the result  appears momentarily.   The implementation at right takes full advantage of the ``don't care'' parts of our specification.  In this case, we require no logic at all; we need merely connect the inputs to the correct outputs.  Let's verify the operation.  We have four cases to consider.  First, if none of the  buttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00  and C_L=00).  Second, if we request lychee ice cream (LBM=100), the outputs are C_L=10 and C_M=00, so we get a full serving of lychee and no mango.  Third, if we request a blend (LBM=010), the outputs are C_L=01 and C_M=01, giving us half a serving of each flavor. Finally, if we request mango ice cream (LBM=001), we get no lychee but a full serving of mango.      The K-maps for this implementation appear below.  Each of the ``don't care'' x's from the original design has been replaced with either a 0 or a 1 and highlighted with shading and green italics. Any implementation produces  either 0 or 1 for every output bit for every possible input combination.  {{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}  As you can see, leveraging ``don't care'' output bits can sometimes significantly simplify our logic.  In the case of this example, we were able to completely eliminate any need for gates!  Unfortunately,  the resulting implementation may sometimes produce unexpected results.   Based on the implementation, what happens if a user presses more than one button?  The ice cream cup overflows!  Let's see why.  Consider the case LBM=101, in which we've pressed both the lychee and mango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a  full serving of each flavor, or two servings total.  Pressing other  combinations may have other repercussions as well. Consider pressing lychee and blend (LBM=110).  The outputs are then C_L=11 and C_M=01.  Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango."
    },
    {
        "question": " How do you apply the same decisions that you made for the { COUNT A} state to all of the other counting states?\", \"\\n",
        "answers": [
            "\nThe design for the counter is completed by applying the same decisions that were made for the { COUNT A} state to all of the other counting states, and the decisions that were made for the { HALT A} state to all of the other halted states. This results in a design in which states are labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0).",
            "If a user presses both buttons (HG=11), the ice cream cup overflows. This is because the outputs are C_L=11 and C_M=10, so our dispenser releases a full serving of each flavor, or two servings total.",
            "The right strategy is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning."
        ],
        "score": [
            7.248688697814941,
            -10.86933708190918,
            -10.373443603515625
        ],
        "text": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in"
    },
    {
        "question": " Why is it important to consider all possible input conditions from all possible states?\\n",
        "answers": [
            "\nOne important reason to consider all possible input conditions from all possible states is that it can help ensure that a design is correct. For example, if a design only ever receives input values that are within a certain range, it is possible that the design is not actually correct for all input values. Considering all possible input conditions can help catch errors like this.\n\nAnother reason to consider all possible input conditions is that it can help optimize a design. For example, if a design only ever receives input values that are within a certain range, it is possible to simplify the design or make it more efficient. Considering all possible input conditions can help identify opportunities for optimization.",
            "The benefit of using a tri-state buffer is that it allows for logical disconnection between the tri-state buffer and the output. This means that the output can be shared by more than one memory without the need for additional multiplexers. This can be useful for creating larger memories by combining multiple chips.",
            "The more general model of a computer is a system that can execute a set of instructions, called a program, and store the resulting data. The system has a set of registers that hold data and instructions, and a set of memory locations that store data and programs. The system also has a set of input and output devices that allow it to interact with the outside world."
        ],
        "score": [
            9.671120643615723,
            -9.86632251739502,
            -11.069331169128418
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How can a transition diagram help with this process?\\n",
        "answers": [
            "\nA transition diagram can help with this process by helping to identify the most important metrics and by helping to balance the design effectively according to those metrics.",
            "The addressability of a computer memory is the number of bits that can be stored in each element of the memory. The size of the elements in a memory can vary from a single binary digit, or bit, to a byte (8 bits) or more. Typically, data elements larger than a byte are referred to as words, but the size of a word depends on context.",
            "Juan Jos'e Jaramillo identified common failure modes, including variability caused by sunshine through the windows in ECEB, and made some changes to improve robustness. He also created the PowerPoint slides that are typically used to describe the lab in lecture."
        ],
        "score": [
            8.73635196685791,
            -11.174650192260742,
            -11.30903434753418
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How can the state table form make the missing parts of the specification even more obvious?\\n",
        "answers": [
            "\nThe state table form of a specification can make the missing parts of the specification more obvious by showing the different states that the system can be in and the transitions between those states. This can help to identify what parts of the specification are missing and what needs to be added in order to complete the specification.",
            "The signal F allows the timer to be set under other conditions, in addition to when the state S changes. This allows for more flexibility in the duration of each state.",
            "The FSM has a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle."
        ],
        "score": [
            9.232417106628418,
            -10.83573055267334,
            -11.258652687072754
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What should happen if a user presses the ``go'' button (HG=01)?\\n",
        "answers": [
            "\nIf a user presses the \"go\" button (HG=01), the machine should dispense one and a half servings of lychee and a half serving of mango.",
            "What is the difference between SOP and POS?\n\nSOP and POS are two forms of two-level logic. In SOP, the output is true if any of the inputs are true. In POS, the output is true if all of the inputs are true.",
            "SRAM cells are combined into a bit slice by sharing bit lines and analog read and write logic. A decoder sets one cell's SELECT line high to enable a read or write operation to the cell. The chip select input CS drives the enable input of the decoder, so none of the memory cells is active when chip select is low (CS=0), and exactly one of the memory cells is active when chip select is high (CS=1)."
        ],
        "score": [
            7.212699890136719,
            -10.745648384094238,
            -10.534865379333496
        ],
        "text": "implementation of  a specification  implies decisions} about these details, so decisions should only be left  unspecified if any of the possible answers is indeed acceptable.  As a concrete example, let's design logic to control an ice cream  dispenser.  The dispenser has two flavors, lychee and mango, but also allows us to create a blend of the two flavors. For each of the two flavors, our logic must output two bits to control the amount of ice cream that comes out of the dispenser. The two-bit C_L[1:0] output of our logic must specify the number  of half-servings of lychee ice cream as a binary number, and the two-bit C_M[1:0] output must specify the number of  half-servings of mango ice cream.  Thus, for either flavor, 00 indicates none of that flavor, 01 indicates one-half of a serving, and  10 indicates a full serving.  Inputs to our logic will consist of three buttons: an L button to request a serving of lychee ice cream, a B button to request a blend---half a serving of each flavor, and an M button to request a serving of mango ice cream.  Each button produces a 1  when pressed and a 0 when not pressed.    Let's start with the assumption that the user only presses one button at a time.  In this case, we can treat input combinations in which more than one button is pressed as ``don't care'' values in the truth tables for the outputs.  K-maps for all four output bits appear below. The x's indicate ``don't care'' values.    When we calculate the logic function for an output, each ``don't care'' value can be treated as either 0 or 1, whichever is more convenient in terms of creating the logic.  In the case of C_M[1], for  example, we can treat the three x's in the ellipse as 1s, treat the x outside of the ellipse as a 0, and simply use M (the implicant represented by the ellipse) for C_M[1].  The other three output bits are left as an exercise, although the result  appears momentarily.   The implementation at right takes full advantage of the ``don't care'' parts of our specification.  In this case, we require no logic at all; we need merely connect the inputs to the correct outputs.  Let's verify the operation.  We have four cases to consider.  First, if none of the  buttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00  and C_L=00).  Second, if we request lychee ice cream (LBM=100), the outputs are C_L=10 and C_M=00, so we get a full serving of lychee and no mango.  Third, if we request a blend (LBM=010), the outputs are C_L=01 and C_M=01, giving us half a serving of each flavor. Finally, if we request mango ice cream (LBM=001), we get no lychee but a full serving of mango.      The K-maps for this implementation appear below.  Each of the ``don't care'' x's from the original design has been replaced with either a 0 or a 1 and highlighted with shading and green italics. Any implementation produces  either 0 or 1 for every output bit for every possible input combination.  {{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}  As you can see, leveraging ``don't care'' output bits can sometimes significantly simplify our logic.  In the case of this example, we were able to completely eliminate any need for gates!  Unfortunately,  the resulting implementation may sometimes produce unexpected results.   Based on the implementation, what happens if a user presses more than one button?  The ice cream cup overflows!  Let's see why.  Consider the case LBM=101, in which we've pressed both the lychee and mango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a  full serving of each flavor, or two servings total.  Pressing other  combinations may have other repercussions as well. Consider pressing lychee and blend (LBM=110).  The outputs are then C_L=11 and C_M=01.  Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango."
    },
    {
        "question": " What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?\"]",
        "answers": [
            "\nIf the ``halt'' button is pressed (HG=10), the counter remains in the halted state. If both buttons are pressed (HG=11), the counter also remains in the halted state.",
            "The final state table and state transition diagram appear as follows: \n\nThe final state table lists the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The table also includes the states outside of the desired loop. The lights are all red in both of these states, which is necessary for safety.\n\nThe state transition diagram shows the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The diagram also shows the duration of each state.",
            "If the ``halt'' button is pressed (HG=10), the counter remains halted in state { HALT A}. If both buttons are pressed (HG=11), ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted. Thus, input patterns HG=10 and HG=11 also take state { HALT A} back to itself."
        ],
        "score": [
            9.663874626159668,
            -9.874743461608887,
            10.087532043457031
        ],
        "text": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in"
    },
    {
        "question": " How many states does an FSM need in order to have eight states?\\nA: 3 \\n\\n",
        "answers": [
            "\nThe FSM needs at least three states in order to have eight states. This is because there are three possible items that can be purchased, and each item can be purchased in one of two ways (with or without enough money).",
            "The 'REJECTD' state is a state in which a processor interrupts a program due to an error or unexpected behavior. This state usually causes the program to terminate.",
            "The color mapping for a three-bit RGB encoding system is a system where colors are represented by a combination of red, green, and blue light. The color mapping for such a system is shown to the right. Imagine that you are charged with creating a counter to drive a light through a sequence of colors."
        ],
        "score": [
            8.492156982421875,
            -10.576995849609375,
            -10.943787574768066
        ],
        "text": "0& 0  EXTRA1& 011& 000& 100& x& x EXTRA2& 111& 000& 100& x& x    states, but left the output bits as don't cares.  A state transition diagram appears at the bottom of this page.  {Testing the Design}  Having a complete design on paper is a good step forward, but humans make mistakes at all stages.  How can we know that a circuit that we build in the lab correctly implements the FSM that we have outlined  in these notes?  For the lab design, we have two problems to solve.  First, we have not specified an initialization scheme for the FSM. We may want the FSM to start in one of the PAID states, but adding initialization logic to the design may mean requiring you to wire together significantly more chips.  Second, we need a sequence of inputs that manages to test that all of the next-state and output logic implementations are correct.  Testing sequential logic, including FSMs, is in general extremely difficult. In fact, large sequential systems today are generally converted into  combinational logic by using shift registers to fill the  flip-flops with a particular pattern,  executing the logic for one clock cycle, and checking that the resulting  pattern of bits in the flip-flops is correct.  This approach is called  { scan-based testing}, and is discussed in ECE 543.  You  will make use of a similar approach when you test your combinational logic in the second week of the lab, before wiring up the flip-flops.  We have designed our FSM to be easy to test (even small FSMs may be challenging) with a brute force approach.  In particular, we  identify two input sequences that together serve both to initialize and  to test a correctly implemented variant of our FSM.  Our initialization sequence forces the FSM into a specific state regardless of its initial state.  And our test sequence crosses every transition arc leaving the six valid states.    In terms of T, the coin type, we initialize the FSM with the input sequence 001.  Notice that such a sequence takes any initial  state into PAID2.  For testing, we use the input sequence 111010010001.  You should trace  this sequence, starting from PAID2, on the diagram below to see how the test sequence covers all of the possible arcs.  As we test, we need also to observe the A and P outputs in each state to check the output logic.  {{file=part3/figs/lab-diag-notes.eps,width=4.25in}}    {Finite State Machine Design Examples, Part II}  This set of notes provides several additional examples of FSM design. We first design an FSM to control a vending machine, introducing encoders and decoders as components that help us to implement our design.  We then design a game controller for a logic puzzle implemented as a children's game.  Finally, we analyze a digital FSM designed to control the stoplights at the intersection of two roads.   {Design of a Vending Machine}  For the next example, we design an FSM to control a simple vending machine.   The machine accepts {U.S. coins}{Most countries have small  bills or coins in demoninations suitable for vending machine prices, so think  about some other currency if you prefer.} as payment and offers a choice of three items for sale.  What states does such an FSM need?  The FSM needs to keep track of how much money has been inserted in order to decide whether a user can  purchase one of the items.  That information alone is enough for the simplest machine, but let's create a machine with adjustable item prices.  We can use registers to hold the item prices, which  we denote P_1, P_2, and P_3.  Technically, the item prices are also part of the internal state of the  FSM.  However,  we leave out discussion (and, indeed, methods) for setting the item prices, so no state with a given combination of prices has any  transition to a state with a different set of item prices. In other words, any given combination of item prices induces a subset  of states that operate independently of the subset induced by a distinct  combination of item prices.  By abstracting"
    },
    {
        "question": " What does it mean when a counter is \"halted?\"\\nA: When a counter is halted, it is no longer counting. \\n\\n",
        "answers": [
            "\nWhen a Turing machine is given an input, it either finishes in a finite number of steps, or it does not. If it does not finish in a finite number of steps, the machine is said to be \"halted.\"",
            "The design process involves the manipulation of logic expressions in order to optimize them for specific metrics, such as cost, power, or performance. Today, this manipulation is performed almost entirely by computers, with human engineers supplying the logic functions of interest and imposing constraints on the process.",
            "The stoplight controller alternates green lights between the two roads by changing the value of the register S. The value of S is changed when the timer reaches 0, and the timer is set when S enters a new state. The duration of each state is determined by the value of the register S."
        ],
        "score": [
            4.452431678771973,
            -7.155963897705078,
            -4.831286430358887
        ],
        "text": "systematic technique (or a machine) capable of solving a problem that cannot be solved by a Turing machine.  No one has been able to do so to date.   {The Halting Problem*}  You might reasonably ask whether any problems can be shown to be incomputable.  More common terms for such problems---those known to be insolvable by any computer---are { intractable} or  undecidable.  In the same 1936 paper in which he introduced the universal computing machine, Alan Turing also provided an answer to this question by introducing (and proving) that there are in fact problems that cannot be computed by a universal computing machine. The problem that he proved undecidable, using proof techniques almost identical to those developed for similar problems in the 1880s, is now known as { the halting problem}.    The halting problem is easy to state and easy to prove undecidable. The problem is this: given a Turing machine and an input to the Turing machine, does the Turing machine finish computing in a finite number of steps (a finite amount of time)?  In order to solve the problem, an answer, either yes or no, must be given in a finite amount of time regardless of the machine or input in question.  Clearly some machines never finish.  For example, we can write a Turing machine that counts upwards starting from one.  You may find the proof structure for undecidability of the halting problem easier to understand if you first think about a related problem with which you may already be familiar, the Liar's paradox (which is at least 2,300 years old).  In its stengthened form, it is the following sentence: ``This sentence is not true.''   To see that no Turing machine can solve the halting problem, we begin by assuming that such a machine exists, and then show that its existence is self-contradictory.  We call the machine the ``Halting Machine,'' or HM for short.  HM is a machine that operates on  another      Turing machine and its inputs to produce a yes or no answer in finite time: either the machine in question finishes in finite time (HM returns ``yes''), or it does not (HM returns ``no'').  The figure illustrates HM's operation.   From HM, we construct a second machine that we call the HM Inverter, or HMI.  This machine inverts the sense of the answer given by HM.  In particular, the inputs are fed directly into a copy of HM, and if HM answers ``yes,'' HMI enters an infinite loop.  If HM answers ``no,'' HMI halts.  A diagram appears to the right.  The inconsistency can now be seen by asking HM whether HMI halts when given itself as an input (repeatedly), as      shown below.  Two copies of HM are thus being asked the same question.  One copy is the rightmost in the figure below and the second is embedded in the HMI machine that we are using as the input to the rightmost HM.  As the two copies of HM operate on the same input (HMI operating on HMI), they should return the same answer: a Turing machine either halts on an input, or it does not; they are deterministic.    Let's assume that the rightmost HM tells us that HMI operating on itself halts. Then the copy of HM in HMI (when HMI executes on itself, with itself as an input) must also say ``yes.''  But this answer implies that HMI doesn't halt (see the figure above), so the answer should have been no!  Alternatively, we can assume that the rightmost HM says that HMI operating on itself does not halt.  Again, the copy of HM in HMI must give the same answer.  But in this case HMI halts, again contradicting our assumption.  Since neither answer is consistent, no consistent answer can be given, and the original assumption that HM exists is incorrect.  Thus, no Turing machine can solve the halting problem.      {Programming Concepts and the C Language}  This set of notes introduces the C programming language and explains some basic concepts in computer programming.  Our purpose in showing you a high-level language at this early stage of the course is to give you time to become familiar with the"
    },
    {
        "question": " What is the difference between a counter that is \"counting\" and a counter that is \"halted?\"\\nA: When a counter is counting, it is in the process of incrementing. When a counter is halted, it is not incrementing. \\n\\n",
        "answers": [
            "\nA counter that is \"counting\" is in the process of incrementing, while a counter that is \"halted\" is not incrementing.",
            "The choice of representation for an FSM's states can affect the design complexity because it determines how many bits are needed to represent each state. If more bits are needed to represent a state, then the design will be more complex.",
            "SRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor"
        ],
        "score": [
            8.823295593261719,
            4.644230365753174,
            2.166536569595337
        ],
        "text": "systematic technique (or a machine) capable of solving a problem that cannot be solved by a Turing machine.  No one has been able to do so to date.   {The Halting Problem*}  You might reasonably ask whether any problems can be shown to be incomputable.  More common terms for such problems---those known to be insolvable by any computer---are { intractable} or  undecidable.  In the same 1936 paper in which he introduced the universal computing machine, Alan Turing also provided an answer to this question by introducing (and proving) that there are in fact problems that cannot be computed by a universal computing machine. The problem that he proved undecidable, using proof techniques almost identical to those developed for similar problems in the 1880s, is now known as { the halting problem}.    The halting problem is easy to state and easy to prove undecidable. The problem is this: given a Turing machine and an input to the Turing machine, does the Turing machine finish computing in a finite number of steps (a finite amount of time)?  In order to solve the problem, an answer, either yes or no, must be given in a finite amount of time regardless of the machine or input in question.  Clearly some machines never finish.  For example, we can write a Turing machine that counts upwards starting from one.  You may find the proof structure for undecidability of the halting problem easier to understand if you first think about a related problem with which you may already be familiar, the Liar's paradox (which is at least 2,300 years old).  In its stengthened form, it is the following sentence: ``This sentence is not true.''   To see that no Turing machine can solve the halting problem, we begin by assuming that such a machine exists, and then show that its existence is self-contradictory.  We call the machine the ``Halting Machine,'' or HM for short.  HM is a machine that operates on  another      Turing machine and its inputs to produce a yes or no answer in finite time: either the machine in question finishes in finite time (HM returns ``yes''), or it does not (HM returns ``no'').  The figure illustrates HM's operation.   From HM, we construct a second machine that we call the HM Inverter, or HMI.  This machine inverts the sense of the answer given by HM.  In particular, the inputs are fed directly into a copy of HM, and if HM answers ``yes,'' HMI enters an infinite loop.  If HM answers ``no,'' HMI halts.  A diagram appears to the right.  The inconsistency can now be seen by asking HM whether HMI halts when given itself as an input (repeatedly), as      shown below.  Two copies of HM are thus being asked the same question.  One copy is the rightmost in the figure below and the second is embedded in the HMI machine that we are using as the input to the rightmost HM.  As the two copies of HM operate on the same input (HMI operating on HMI), they should return the same answer: a Turing machine either halts on an input, or it does not; they are deterministic.    Let's assume that the rightmost HM tells us that HMI operating on itself halts. Then the copy of HM in HMI (when HMI executes on itself, with itself as an input) must also say ``yes.''  But this answer implies that HMI doesn't halt (see the figure above), so the answer should have been no!  Alternatively, we can assume that the rightmost HM says that HMI operating on itself does not halt.  Again, the copy of HM in HMI must give the same answer.  But in this case HMI halts, again contradicting our assumption.  Since neither answer is consistent, no consistent answer can be given, and the original assumption that HM exists is incorrect.  Thus, no Turing machine can solve the halting problem.      {Programming Concepts and the C Language}  This set of notes introduces the C programming language and explains some basic concepts in computer programming.  Our purpose in showing you a high-level language at this early stage of the course is to give you time to become familiar with the"
    },
    {
        "question": " How does the choice of representation for an FSM\\'s states affect the design complexity?\\nA: The choice of representation can dramatically affect the design complexity. \\n\\n",
        "answers": [
            "\nThe choice of representation for an FSM's states can affect the design complexity because it determines how many bits are needed to represent each state. If more bits are needed to represent a state, then the design will be more complex.",
            "The transitions from all states are well-defined because the FSM has a timer that counts down and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.",
            "The main benefit of using a tri-state buffer is that it allows for logical disconnection between the buffer and the output. This means that multiple memories can share the same output line without the need for additional multiplexers."
        ],
        "score": [
            8.771692276000977,
            2.677640438079834,
            -2.4220051765441895
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " What is the right strategy for choosing a representation?\\nA: The right strategy is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary.', '\\n",
        "answers": [
            "\nThe right strategy for choosing a representation is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. This will make the design process simpler and more efficient in the long run.",
            "Mapping down to bits and gates means to design a circuit that performs an operation on a single bit and passes any necessary information into another copy of itself. By using copies of this bit-sliced adder circuit, we can mimic our approach as humans and build adders of any size.",
            "The fifth step in the design process is to select an internal representation for the bits necessary to encode the state of our FSM. In practice, for small designs, this representation can be selected by a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand."
        ],
        "score": [
            9.685295104980469,
            1.317231297492981,
            4.206628322601318
        ],
        "text": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is"
    },
    {
        "question": " How does the choice of state representation affect design complexity?\\n",
        "answers": [
            "\nThe choice of state representation affects design complexity in a few ways. First, it can affect the number of bits needed to represent the states. Second, it can affect the number of possible states that can be represented. Third, it can affect the number of transitions that need to be considered when designing the state machine.",
            "DRAM is a type of memory that uses capacitors to store data. When a row of DRAM is selected, all of the cells in that row are read and then written back, which refreshes the data in those cells. DRAM is said to have destructive reads because each read operation brings the voltage on the capacitor closer to the intermediate voltage level, which effectively destroys the data in the cell.",
            "One reason it is important to start with a representation that makes sense to a human is that humans must supply the logic functions of interest, and must program the acceptable transformations between equivalent forms. A second reason is that the complexity of the problem is a good way to introduce you to real engineering. Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today."
        ],
        "score": [
            9.743267059326172,
            -11.238014221191406,
            -2.9043047428131104
        ],
        "text": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in"
    },
    {
        "question": " What is the right strategy for selecting a state representation?\\n",
        "answers": [
            "\nThe right strategy is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning.",
            "The FSM has five parts: a finite set of states, a set of possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs. In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.",
            "A decoder is a logic circuit that converts coded inputs into coded outputs, usually one output code for each input code. Decoders are used in a variety of applications, including data storage and transmission, error detection and correction, and data compression."
        ],
        "score": [
            6.059752941131592,
            -10.560247421264648,
            -11.146231651306152
        ],
        "text": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in"
    },
    {
        "question": " Why is it often best to start by selecting a representation that makes sense to a human?\\n",
        "answers": [
            "\nOne reason it is often best to start by selecting a representation that makes sense to a human is that it can be easier to understand and work with. Additionally, starting with a representation that makes sense to a human can help ensure that the final product is also easy to understand and use.",
            "The decoder generates the release signals for the vending machine by connecting the signal S produced by the priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input. The outputs D_1, D_2, and D_3 then correspond to the individual item release signals R_1, R_2, and R_3 for our vending machine.",
            "The purpose of the FSM is to control the stoplights at a busy intersection. The FSM ensures that the lights are always red on one or both roads, that a yellow light is shown before a green light, and that the green lights are alternated between the two roads."
        ],
        "score": [
            9.884490966796875,
            -11.255411148071289,
            -11.250133514404297
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What are the benefits of using abstraction when selecting a state representation?\\n",
        "answers": [
            "\nAbstraction is a powerful tool that can help simplify complex problems. When selecting a state representation, abstraction can help identify the most important aspects of the problem and ignore irrelevant details. This can make it easier to find a solution that is both effective and efficient.",
            "The first approach is to use the flip-flop design to add a single bit and pass any necessary information into another copy of itself. By using copies of this bit-sliced adder circuit, we can mimic our approach as humans and build adders of any size.",
            "The FSM shows a yellow light before showing a red light by setting the timer to a specific value when the state of the FSM changes. This value is set by the signal F, which is calculated at the bottom of the FSM logic diagram."
        ],
        "score": [
            7.715181350708008,
            -11.210038185119629,
            -10.904295921325684
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How can the use of abstraction in selecting a state representation lead to a better design?', '\\n",
        "answers": [
            "\nAbstraction in selecting a state representation can lead to a better design by allowing the engineer to focus on the important aspects of the problem and ignoring the details that are not important. This can help to simplify the problem and make it easier to find a solution.",
            "The state representation is used to determine the next state of the FSM. This is done by examining the inputs to the FSM and deriving equations for the next state. The states of the FSM act like a counter, and by examining the connections, the next state can be determined.",
            "There are several benefits of a good state representation. First, it can help reduce the number of pins needed on a chip. Second, it can improve the density of a memory chip. Third, it can improve the speed of a memory chip."
        ],
        "score": [
            9.278197288513184,
            -3.1852736473083496,
            -2.5770111083984375
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How does the number of state bits affect the design complexity?\\n",
        "answers": [
            "\nThe number of state bits affects the design complexity because it determines the number of possible states that the FSM can be in. A larger number of state bits means that there are more possible states and the design must be able to handle all of them.",
            "Abstraction is important when selecting a state representation because it allows the designer to select a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning.",
            "The fifth step in the design process is to select an internal representation for the bits necessary to encode the state of our FSM. In practice, for small designs, this representation can be selected by a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand."
        ],
        "score": [
            9.501489639282227,
            -2.409792900085449,
            -2.5186309814453125
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " Why is it important to use abstraction when selecting a state representation?\\n",
        "answers": [
            "\nAbstraction is important when selecting a state representation because it allows the designer to select a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning.",
            "If the ``halt'' button is pressed (HG=10), the counter remains in the halted state. If both buttons are pressed (HG=11), the counter also remains in the halted state.",
            "The final state table and state transition diagram appear as follows: \n\nThe final state table lists the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The table also includes the states outside of the desired loop. The lights are all red in both of these states, which is necessary for safety.\n\nThe state transition diagram shows the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The diagram also shows the duration of each state."
        ],
        "score": [
            9.719866752624512,
            -10.781917572021484,
            -10.144615173339844
        ],
        "text": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in"
    },
    {
        "question": " How does the choice of state representation affect the design of the FSM?\\n",
        "answers": [
            "\nDifferent state representations can lead to different FSM designs. For example, if states are represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits. If an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.",
            "The purpose of the FSM is to control a stoplight. The stoplight must be red on one or both roads at all times, and the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. The controller should also alternate green lights between the two roads.",
            "There are a few possible ways to answer this question. One way would be to say that if the user presses the ``halt'' button while in state { HALT A}, the system should reset to state { COUNT A}. Another way to answer this question would be to say that if the user presses the ``halt'' button while in state { HALT A}, the system should remain in state { HALT A}."
        ],
        "score": [
            5.402968883514404,
            -4.240033149719238,
            -11.042276382446289
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " Why is it important to start with a representation that makes sense to a human?\\n",
        "answers": [
            "\nOne reason it is important to start with a representation that makes sense to a human is that humans must supply the logic functions of interest, and must program the acceptable transformations between equivalent forms. A second reason is that the complexity of the problem is a good way to introduce you to real engineering. Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.",
            "The state representation has a significant impact on the overall design of the system. It affects the way the system behaves and how it interacts with its environment. A well-chosen state representation can simplify the design of the system and make it more efficient.",
            "The state table form of a specification can make the missing parts of the specification more obvious by showing the different states that the system can be in and the transitions between those states. This can help to identify what parts of the specification are missing and what needs to be added in order to complete the specification."
        ],
        "score": [
            9.526113510131836,
            -3.0961265563964844,
            -11.083141326904297
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": " How does the use of abstraction in selecting a representation impact the design metrics?']",
        "answers": [
            "\nAbstraction in selecting a representation can impact the design metrics by reducing the number of metrics that need to be considered. This can simplify the process of designing a solution and make it easier to find an optimal solution.",
            "The next-state values for S_2, S_1, and S_0 are all 1.",
            "The decoder generates the release signals for the vending machine by connecting the signal S produced by the priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input. The outputs D_1, D_2, and D_3 then correspond to the individual item release signals R_1, R_2, and R_3 for our vending machine."
        ],
        "score": [
            9.871903419494629,
            -11.026792526245117,
            -11.18692398071289
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What is the value of HOLD when the counter is in the COUNT A state and S_2=0?\\n",
        "answers": [
            "\nHOLD is 1 when the counter is in the COUNT A state and S_2=0. This is because the counter is already halted and we didn't press the \"go\" button.",
            "The value of register PRICE2 is the number of nickels that have been inserted into the machine.",
            "The two approaches to FSM design are the state-transition table and the state diagram. The state-transition table lists all the possible states that the system can be in, and the state diagram shows the sequence of states that the system goes through. The state-transition table is more compact and easier to use when comparing functions, but the state diagram is more intuitive and easier to understand."
        ],
        "score": [
            8.233205795288086,
            -9.254939079284668,
            -10.864706039428711
        ],
        "text": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux"
    },
    {
        "question": " What is the value of HOLD when the counter is in the HALT A state and S_2=1?\\n",
        "answers": [
            "\nHOLD = H + S_2\n\nWhen the counter is in the HALT A state and S_2=1, HOLD = 1. This is because the counter should stop (move into a halted state without changing value) when H is true, and in the halted states, when S_2=1, the counter stops (stays in a halted state) when H+ is true.",
            "The FSM needs to track the state of the stoplight and the duration of each state.",
            "A fully-specified transition diagram is a diagram that specifies all of the states that a system can be in, as well as all of the transitions between those states. This allows for a clear understanding of the behavior of the system."
        ],
        "score": [
            8.953996658325195,
            -7.029552936553955,
            -10.995112419128418
        ],
        "text": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux"
    },
    {
        "question": " What is the value of S_2 when the counter is in the COUNT A state and HOLD=0?\\n",
        "answers": [
            "\nThe value of S_2 when the counter is in the COUNT A state and HOLD=0 is undefined. This is because, as the passage states, overflow occurs when either both operands are negative or both operands are positive. If both operands are negative, then the resulting sum S is equal to C. However, if both operands are positive, then the resulting sum S is less than C. Thus, it is impossible to determine the value of S_2 without knowing the values of both operands.",
            "The design process involves the manipulation of logic expressions in order to optimize them for specific metrics, such as cost, power, or performance. Today, this manipulation is performed almost entirely by computers, with human engineers supplying the logic functions of interest and imposing constraints on the process.",
            "The second optical sensor is used to detect when a chip finishes its work so that the chip can turn itself off and save energy. This metric is used to guide the process of optimizing logic expressions so that a balance can be struck between the different metrics involved."
        ],
        "score": [
            8.297812461853027,
            -11.201385498046875,
            -11.038192749023438
        ],
        "text": "unit.  We define S as the number represented by the bit pattern produced by the add unit.  When overflow  occurs, S=C, but we always have (S=C)  2^N.  { Theorem:}   Addition of two {N-bit} 2's complement numbers A and B overflows if and only if one of the following conditions holds:  {A<0 and B<0 and S} {A and B and S<0}   { Proof:}  We once again start with the ``if'' direction.  That is, if condition 1  or condition 2 holds, we have an overflow.  The proofs are straightforward. Given condition 1, we can add the two inequalities A<0 and B<0 to  obtain C=A+B<0.  But S, so clearly S=C, thus overflow  has occurred.  Similarly, if condition 2 holds, we can add the inequalities A and B to obtain C=A+B.  Here we have S<0, so again S=C, and we have an overflow.  We must now prove the ``only if'' direction, showing that any overflow implies either condition 1 or condition 2.  By the  { contrapositive}{If we have a statement of the form (p implies q), its contrapositive is the  statement (not q implies not p). Both statements have the same truth value.  In this case, we can turn our Lemma around as stated.} of our Lemma, we know that if an overflow occurs, either both operands are  negative, or they are both positive.      Let's start with the case in which both operands are negative, so A<0 and B<0, and thus the real sum C<0 as well.  Given that A and B are represented as {N-bit} 2's complement, they must fall in the representable range, so we can write  {eqnarray*} -2^{N-1}  & A & < 0 -2^{N-1}  & B & < 0 {eqnarray*}  We add these two inequalities and replace A+B with C to obtain  {eqnarray*} -2^N  & C & < 0 {eqnarray*}  Given that an overflow has occurred, C must fall outside of the  representable range.  Given that C<0, it cannot be larger than the largest possible number representable using {N-bit} 2's complement, so we can write  {eqnarray*} -2^N  & C & < -2^{N-1} {eqnarray*}  We now add 2^N to each part to obtain  {eqnarray*} 0  & C + 2^N & < 2^{N-1} {eqnarray*}  This range of integers falls within the representable range  for {N-bit} 2's complement, so we can replace the middle expression with S (equal to C modulo 2^N) to find that  {eqnarray*} 0  & S & < 2^{N-1} {eqnarray*}  Thus, if we have an overflow and both A<0 and B<0, the resulting sum S, and condition 1 holds.  The proof for the case in which we observe an overflow when  both operands are non-negative (A and B) is similar, and leads to condition 2.  We again begin with inequalities for A and B:  {eqnarray*} 0  & A & < 2^{N-1} 0  & B & < 2^{N-1} {eqnarray*}  We add these two inequalities and replace A+B with C to obtain  {eqnarray*} 0  & C < & 2^N {eqnarray*}  Given that an overflow has occurred, C must fall outside of the  representable range.  Given that C{}0, it cannot be smaller than the smallest possible number representable using {N-bit} 2's complement, so we can write  {eqnarray*} 2^{N-1}  & C & < 2^N {eqnarray*}  We now subtract 2^N to each part to obtain  {eqnarray*} -2^{N-1}  & C - 2^N & < 0 {eqnarray*}  This range of integers falls within the representable range  for {N-bit} 2's complement, so we can replace the middle expression with S (equal to C modulo 2^N) to find that  {eqnarray*} -2^{N-1}  & S & < 0 {eqnarray*}  Thus, if we have an overflow and both A and B, the resulting sum S<0, and condition 2 holds.  Thus overflow implies either condition 1 or condition 2, completing our proof."
    },
    {
        "question": " What is the value of S_2 when the counter is in the HALT A state and HOLD=1?\\n",
        "answers": [
            "\nS_2 is 1 when the counter is in the HALT A state and HOLD=1. This is because when S_2=1, the counter is halted, and when HOLD=1, the counter should retain its current value.",
            "What is the reason for the popularity of two-level logic?\n\nThe popularity of two-level logic is due to the fact that it is a simple way to express logic functions. Additionally, two-level logic is easy to map into CMOS gates.",
            "The game revolves around an old logic problem in which a farmer must cross a river in order to reach the market. The farmer is traveling to the market to sell a fox, a goose, and some corn. The farmer has a boat, but the boat is only large enough to carry the fox, the goose, or the corn at a time. The fox cannot be left alone with the goose, or the fox will eat the goose. The goose cannot be left alone with the corn, or the goose will eat the corn. How can the farmer get all three items to the market?"
        ],
        "score": [
            8.751683235168457,
            -11.115802764892578,
            -11.310529708862305
        ],
        "text": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux"
    },
    {
        "question": " What is the value of S_1S_0 when the counter is in the COUNT A state and HOLD=1?', '\\n\\n",
        "answers": [
            "\nThe value of S_1S_0 when the counter is in the COUNT A state and HOLD=1 is 01.",
            "One way to construct a wider memory is to use multiple chips and connect the data inputs and outputs of each chip separately. This way, the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of input and produces the low N bits of output.",
            "System calls are initiated with a trap instruction, and system calls are also known as traps. With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system."
        ],
        "score": [
            9.185112953186035,
            -10.747963905334473,
            -11.084763526916504
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the purpose of the HOLD variable?\\n",
        "answers": [
            "\nThe HOLD variable is used to save energy. When a chip finishes its work, it can turn itself off, saving energy. This variable allows the chip to turn itself off when it is finished, saving energy.",
            "An FSM must be initialized to ensure that it starts in a known state and to avoid undefined behavior. Without initialization, an FSM may produce incorrect results or may not work at all.",
            "The possible inputs for this FSM are a dime (T=0) or a quarter (T=1). The machine will reject any other input. If the user tries to purchase an item without inserting enough money first, the FSM will also reject the request."
        ],
        "score": [
            6.741426944732666,
            -10.31562614440918,
            -10.229833602905273
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": " What is the value of HOLD when the counter is counting?\\n",
        "answers": [
            "\nThe value of HOLD when the counter is counting is 0. The value of HOLD when the counter is halted is 1.",
            "Different state representations can lead to different FSM designs. For example, if states are represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits. If an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.",
            "The state table represents the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The lights are all red in both of these states, which is necessary for safety."
        ],
        "score": [
            8.840017318725586,
            -11.322129249572754,
            -11.255998611450195
        ],
        "text": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset"
    },
    {
        "question": " What is the value of HOLD when the counter is halted?\\n",
        "answers": [
            "\nThe value of HOLD when the counter is halted is the value of the last input given to the machine.",
            "The decoder in the memory chip is responsible for translating the memory address into the correct location in the memory chip. This is necessary to ensure that the correct data is read from or written to the correct location in the memory chip.",
            "A write operation on a SRAM cell is performed by bringing the bit line to an intermediate voltage level, then raising the SELECT line, which allows the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line. The resulting change in voltage is then detected by a sense amplifier at the end of the bit line."
        ],
        "score": [
            8.66940975189209,
            -11.313793182373047,
            -11.275230407714844
        ],
        "text": "systematic technique (or a machine) capable of solving a problem that cannot be solved by a Turing machine.  No one has been able to do so to date.   {The Halting Problem*}  You might reasonably ask whether any problems can be shown to be incomputable.  More common terms for such problems---those known to be insolvable by any computer---are { intractable} or  undecidable.  In the same 1936 paper in which he introduced the universal computing machine, Alan Turing also provided an answer to this question by introducing (and proving) that there are in fact problems that cannot be computed by a universal computing machine. The problem that he proved undecidable, using proof techniques almost identical to those developed for similar problems in the 1880s, is now known as { the halting problem}.    The halting problem is easy to state and easy to prove undecidable. The problem is this: given a Turing machine and an input to the Turing machine, does the Turing machine finish computing in a finite number of steps (a finite amount of time)?  In order to solve the problem, an answer, either yes or no, must be given in a finite amount of time regardless of the machine or input in question.  Clearly some machines never finish.  For example, we can write a Turing machine that counts upwards starting from one.  You may find the proof structure for undecidability of the halting problem easier to understand if you first think about a related problem with which you may already be familiar, the Liar's paradox (which is at least 2,300 years old).  In its stengthened form, it is the following sentence: ``This sentence is not true.''   To see that no Turing machine can solve the halting problem, we begin by assuming that such a machine exists, and then show that its existence is self-contradictory.  We call the machine the ``Halting Machine,'' or HM for short.  HM is a machine that operates on  another      Turing machine and its inputs to produce a yes or no answer in finite time: either the machine in question finishes in finite time (HM returns ``yes''), or it does not (HM returns ``no'').  The figure illustrates HM's operation.   From HM, we construct a second machine that we call the HM Inverter, or HMI.  This machine inverts the sense of the answer given by HM.  In particular, the inputs are fed directly into a copy of HM, and if HM answers ``yes,'' HMI enters an infinite loop.  If HM answers ``no,'' HMI halts.  A diagram appears to the right.  The inconsistency can now be seen by asking HM whether HMI halts when given itself as an input (repeatedly), as      shown below.  Two copies of HM are thus being asked the same question.  One copy is the rightmost in the figure below and the second is embedded in the HMI machine that we are using as the input to the rightmost HM.  As the two copies of HM operate on the same input (HMI operating on HMI), they should return the same answer: a Turing machine either halts on an input, or it does not; they are deterministic.    Let's assume that the rightmost HM tells us that HMI operating on itself halts. Then the copy of HM in HMI (when HMI executes on itself, with itself as an input) must also say ``yes.''  But this answer implies that HMI doesn't halt (see the figure above), so the answer should have been no!  Alternatively, we can assume that the rightmost HM says that HMI operating on itself does not halt.  Again, the copy of HM in HMI must give the same answer.  But in this case HMI halts, again contradicting our assumption.  Since neither answer is consistent, no consistent answer can be given, and the original assumption that HM exists is incorrect.  Thus, no Turing machine can solve the halting problem.      {Programming Concepts and the C Language}  This set of notes introduces the C programming language and explains some basic concepts in computer programming.  Our purpose in showing you a high-level language at this early stage of the course is to give you time to become familiar with the"
    },
    {
        "question": " How does the value of HOLD determine whether the counter counts or not?\\n",
        "answers": [
            "\nThe value of HOLD determines whether the counter counts or not by determining whether the counter should hold its current value (stop counting) if the ``halt'' button is pressed or if the counter was already halted and the ``go'' button wasn't pressed.",
            "The more general model of a computer is a system that can execute a set of instructions, called a program, and store the resulting data. The system has a set of registers that hold data and instructions, and a set of memory locations that store data and programs. The system also has a set of input and output devices that allow it to interact with the outside world.",
            "The most general model of a computer is the Turing machine. It is a mathematical model of a computer that can be used to simulate any other computer."
        ],
        "score": [
            9.440977096557617,
            -11.241045951843262,
            -11.30691146850586
        ],
        "text": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux"
    },
    {
        "question": " What is the next-state logic for S_1^+ and S_0^+ when HOLD=0?', '\\n\\n",
        "answers": [
            "\nThe next-state logic for S_1^+ and S_0^+ when HOLD=0 is as follows:\n\nS_1^+ = S_0 + HOLD\n\nS_0^+ = S_1 + HOLD",
            "Data elements in a computer memory are stored either in static random access memory (SRAM) or in dynamic random access memory (DRAM). SRAM uses active logic in the form of a two-inverter loop to maintain stored values, while DRAM uses a charged capacitor to store a bit.",
            "Yes, we can choose to use the counter's internal state directly as our output values. This is because the colors we need to produce as outputs are all unique bit patterns. By using the counter's internal state directly as our output values, we can simplify our design."
        ],
        "score": [
            9.445741653442383,
            -9.602712631225586,
            -8.27836799621582
        ],
        "text": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux"
    },
    {
        "question": " How does the HOLD variable simplify the design?\\n",
        "answers": [
            "\nThe HOLD variable simplifies the design by allowing the designer to separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.",
            "The lab simulates two-level logic and how it can be used to map an expression into CMOS gates. It also shows how to graphically manipulate expressions to understand gate diagrams.",
            "The vending machine application and FSM were designed by connecting the buttons B_1, B_2, and B_3 to the priority encoder (and feeding 0 into the fourth input), which produces a signal P indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants. The signal R is also used to enable a decoder that generates the three individual item release outputs. The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next cycle."
        ],
        "score": [
            8.302667617797852,
            -11.204395294189453,
            -10.836848258972168
        ],
        "text": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux"
    },
    {
        "question": " What is the significance of the similarity between the equations for S_1^+S_0^+ and the equations for a {2-to-1} mux?\\n",
        "answers": [
            "\nThe similarity between the equations for S_1^+S_0^+ and the equations for a {2-to-1} mux is that both can be used to represent a logic function. The two-level logic is a popular way of expressing logic functions because it is simple to map our expression into CMOS gates. Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.",
            "The second version of the table implies that the buttons are not placed in the most ergonomic way possible. The buttons are too close together and there is not enough space between the buttons and the edge of the table.",
            "An FSM must be initialized to ensure that it starts in a known state. This allows the FSM to function correctly and avoid any potential problems that could arise from an unknown starting state."
        ],
        "score": [
            8.156015396118164,
            -9.896651268005371,
            -9.799623489379883
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How does the use of S_2 separate out the logic for deciding whether to advance the counter?\\n",
        "answers": [
            "\nThe use of S_2 separates out the logic for deciding whether to advance the counter by allowing the counter to be in a halted state (S_2=1) or a counting state (S_2=0). When HOLD=1, the counter retains its state, and when HOLD=0, it counts.",
            "A computer is a machine that can be programmed to carry out a set of arithmetic or logical operations automatically. An FSM is a model of behavior composed of a finite set of states, transitions between those states, and actions.",
            "The output behavior is that the add3 procedure takes three integers as arguments, adds them together, and returns the sum. The procedure is called with the constants 10, 20, and 30, and the result is printed. By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30."
        ],
        "score": [
            9.713610649108887,
            -11.179864883422852,
            -10.986522674560547
        ],
        "text": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux"
    },
    {
        "question": " What is the impact of HOLD on the counting states and halted states?\\n",
        "answers": [
            "\nF has two effects on the operation of the FSM.  First, it prevents the timer from being reset when S enters a new state.  This has the effect of lengthening the duration of each state.  Second, it prevents the FSM from entering the loop defined by the next-state logic.  This has the effect of halting the FSM in the current state.",
            "One way to verify that an implementation is correct is to test it against a known set of inputs and compare the results to the expected results. If the results match, then the implementation is likely correct. Another way to verify an implementation is to examine the code and look for errors. This can be done manually or with the help of a software tool.",
            "The lab simulates two-level logic and how it can be used to map an expression into CMOS gates. It also shows how to graphically manipulate expressions to understand gate diagrams."
        ],
        "score": [
            -3.2432644367218018,
            -11.265950202941895,
            -11.320722579956055
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How can abstraction be used to build a three-bit Gray code counter from the two-bit design?']",
        "answers": [
            "\nAbstraction can be used to build a three-bit Gray code counter from the two-bit design by using copies of the bit-sliced adder circuit. This approach is less efficient than designing a circuit specifically for a three-bit Gray code counter, but it is simpler.",
            "A Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all patterns in the second copy.",
            "The addressability of a computer memory is the number of bits that can be stored in each element of the memory. The size of the elements in a memory can vary from a single bit to a byte or more. Typically, data elements larger than a byte are referred to as words, but the size of a word depends on context."
        ],
        "score": [
            10.365283012390137,
            1.1846305131912231,
            -11.036251068115234
        ],
        "text": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is"
    },
    {
        "question": " What is the purpose of the state representation?\\n\\n",
        "answers": [
            "\nThe state representation is used to determine the next state of the FSM. This is done by examining the inputs to the FSM and deriving equations for the next state. The states of the FSM act like a counter, and by examining the connections, the next state can be determined.",
            "Prof. Kindratenko helped to scale the design in a way that made it possible to deliver to the over 400 students entering ECE every semester by creating both the wiki documentation and the Altera Quartus portions of the lab. He also helped to improve the robustness of the lab by identifying common failure modes and making some changes to the design.",
            "Data elements in a computer memory are stored either in static random access memory (SRAM) or in dynamic random access memory (DRAM). SRAM uses active logic in the form of a two-inverter loop to maintain stored values, while DRAM uses a charged capacitor to store a bit."
        ],
        "score": [
            5.699043273925781,
            -11.162063598632812,
            -10.83079719543457
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What are the benefits of a good state representation?\\n\\n",
        "answers": [
            "\nThere are several benefits of a good state representation. First, it can help reduce the number of pins needed on a chip. Second, it can improve the density of a memory chip. Third, it can improve the speed of a memory chip.",
            "The inputs to the FSM are the states of the two roads, and the outputs are the colors of the lights for the two roads. The FSM ensures that the lights on one or both roads are red at all times, and that the lights on the other road are green. The FSM also alternates green lights between the two roads.",
            "The passage states that power consumption is an important metric that engineers must take into account in their designs. However, in some cases, such as desktop and high-end server processors, other metrics (such as performance) may be so important that we always want to operate at the edge of the interesting range. This could be what is going wrong, as engineers are not taking power consumption into account as much as they should be."
        ],
        "score": [
            8.258533477783203,
            -10.971616744995117,
            -11.1647310256958
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " What are the consequences of a bad state representation?\\n\\n",
        "answers": [
            "\nA bad state representation can lead to wasted engineering effort, as well as products that have to be cancelled due to poor performance. In some cases, a bad state representation can also lead to increased costs for fabrication and testing.",
            "The two approaches to FSM design are the state-transition table and the state diagram. The state-transition table lists all the possible states that the system can be in, and the state diagram shows the sequence of states that the system goes through. The state-transition table is more compact and easier to use when comparing functions, but the state diagram is more intuitive and easier to understand.",
            "An abstract model is a mathematical model that is used to describe a system. It is usually simplified and does not include all the details of the system."
        ],
        "score": [
            7.35062313079834,
            -10.91319751739502,
            -11.079015731811523
        ],
        "text": "range corresponds to systems in which when a processor's power consumption in a digital  system is extremely low relative to the power consumption of the system. For example, the processor in a computer might use less than 1  of the total used by  the system including the disk drive, the monitor, the power  supply, and so forth.  One power consumption value in this range  is just as good as any another, and no one cares about the power consumption of the processor  in such cases.  In the second range, power consumption of the processor makes a difference.  Cell phones use most of their energy in radio operation, for example, but if you own a phone with a powerful processor, you may have noticed that you can turn off the phone and  drain the battery fairly quickly by playing a game.  Designing a processor that uses half as much power lengthens the battery life in such cases.  Finally, the third region of power consumption measurements is impossible: if you use so much power, your chip will overheat or even burst into flames.  Consumers get unhappy when such things happen.  As a first step, you can remove any metrics for which all solutions are effectively equivalent.  Until a little less than a decade ago, for example, the power  consumption of a desktop processor actually was in the first range that we discussed.  Power was simply not a concern to engineers: all designs of  interest consumed so little power that no one cared.  Unfortunately, at that point, power consumption jumped into the third range rather quickly.  Processors hit a wall, and  products had to be cancelled.  Given that the time spent designing a processor has historically been about five years, a lot of engineering effort was wasted because people had not thought carefully enough about power (since it had never mattered in the past).  Today, power is an important metric that engineers must take into account in their designs.   However, in some areas, such as desktop and high-end server processors, other metrics (such as performance) may be so  important that we always want to operate at the edge of the interesting range.  In such cases, we might choose to treat  a metric such as power consumption as a { threshold}: stay below 150 Watts for a desktop processor, for example.  One still has to make a coordinated effort to ensure that the system as a whole does not exceed the threshold, but reasoning about  threshold values, a form of constraint, is easier than trying to think about multiple metrics at once.  Some metrics may only allow discrete quantification.  For example, one could choose to define compatibility with previous processor generations as binary: either an existing piece of software (or operating system) runs out of the box on your new processor, or it does not.  If you  want people who own that software to make use of your new processor, you must ensure that the value of this binary metric is 1, which can also be viewed as a threshold.  In some cases, two metrics may be strongly { correlated}, meaning that a design that is good for one of the metrics is frequently  good for the other metric as well.  Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider them separately.  A design that requires a larger chip is probably more complex, and thus takes more engineering time to get right (engineering time costs money).    Each silicon wafer costs money to fabricate, and fewer copies of a  large design fit on one wafer, so large chips mean more fabrication cost.  Physical defects in silicon can cause some chips not to work.  A large chip uses more silicon than a small one, and is thus more likely to suffer from defects (and not work).  Cost thus goes up again for large chips relative to small ones.  Finally, large chips usually require more careful testing to ensure that they work properly (even ignoring the cost of getting the design right, we have to test for the presence of defects), which"
    },
    {
        "question": " How can you verify that an implementation is correct?\\n\\n",
        "answers": [
            "\nOne way to verify that an implementation is correct is to test it against a known set of inputs and compare the results to the expected results. If the results match, then the implementation is likely correct. Another way to verify an implementation is to examine the code and look for errors. This can be done manually or with the help of a software tool.",
            "The purpose of this process is to find the best formulation of a specific logic function for a given metric. This is done by using heuristic approaches to find solutions that strike a balance between various metrics such as cost, power, or performance.",
            "The transition diagram for the color sequencer is a loop of six states, with the two remaining states converging into the loop after a single cycle. The lights are all red in both of these states, which is necessary for safety. The timer is set only when the state S changes, and the duration of each state is calculated by analyzing the muxes."
        ],
        "score": [
            8.703164100646973,
            -10.741002082824707,
            -11.101594924926758
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": " What is the difference between a good and a bad state representation?', '\\n",
        "answers": [
            "\nA good state representation should be easy to understand and should be able to capture all the relevant information about the problem. A bad state representation may be difficult to understand or may not be able to capture all the relevant information.",
            "One way to build an FSM that allows the pieces to be executed to be specified after the FSM is built is to use a state machine that has an output that can be routed to different inputs. This allows the FSM to be configured to execute different pieces of code based on the input. Another way to build an FSM that allows the pieces to be executed to be specified after the FSM is built is to use a state machine that has multiple states that can be executed in parallel. This allows the FSM to be configured to execute different pieces of code in parallel based on the input.",
            "The vending machine FSM is designed to control the release of items from the vending machine based on the user's purchase request. The FSM uses a priority encoder to determine which item the user has requested, and a decoder to generate the individual item release signals."
        ],
        "score": [
            7.378777503967285,
            -11.177680969238281,
            -11.259316444396973
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the impact of the state representation?\\n",
        "answers": [
            "\nThe state representation has a significant impact on the overall design of the system. It affects the way the system behaves and how it interacts with its environment. A well-chosen state representation can simplify the design of the system and make it more efficient.",
            "The vertical lines carrying the current state values and their inverses back to the next state logic inputs are ordered from left to right (on the left side of the figure) as {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.",
            "The lab simulates two-level logic and how it can be used to map an expression into CMOS gates. It also shows how to graphically manipulate expressions to understand gate diagrams."
        ],
        "score": [
            7.4298906326293945,
            -8.344701766967773,
            -11.331693649291992
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "How does the state representation impact the FSM?\\n",
        "answers": [
            "\nThe states of the FSM act like a counter, and the value of the FSM is set whenever the state changes. The timer is set only when the state changes, and the duration of each state is determined by the value of the timer. The FSM can also be set under other conditions, such as when the signal F is calculated.",
            "The states of the FSM act like a counter, and the value of the FSM is set whenever the state changes. The timer is set only when the state changes, and the duration of each state is determined by the value of the timer. The FSM can also be set under other conditions, such as when the signal F is calculated.",
            "We can test the circuit that we build in the lab to see if it correctly implements the FSM that we have outlined in these notes. To do this, we can input a known sequence of 0s and 1s and see if the circuit outputs the correct responses. If it does, then we can be confident that the circuit is functioning correctly."
        ],
        "score": [
            2.033355951309204,
            2.033355951309204,
            -8.416738510131836
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "Why is it important to have a good state representation?\\n",
        "answers": [
            "\nA good state representation is important because it can help reduce the number of logic gates needed for a design, and thus save on chip area. Additionally, a good state representation can help improve the performance of a design by reducing the number of logic operations needed.",
            "Adding a third bit to a Gray code counter increases design complexity because it requires an additional carry bit. This carry bit must be propagated through the adder bit slice logic, which increases the number of logic gates required and the overall complexity of the design.",
            "Tri-state buffers can be used to reduce the number of pins needed on a chip by combining inputs and outputs. For example, data inputs and outputs of memory can be combined into a single set of data wires, with bidirectional signals. This reduces the number of pins needed while still allowing for the same amount of data to be transferred."
        ],
        "score": [
            9.485465049743652,
            -11.042701721191406,
            -10.764730453491211
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": "What are the consequences of a bad state representation?\\n",
        "answers": [
            "\nA bad state representation can lead to wasted engineering effort, as well as products that are cancelled due to poor power consumption. In some cases, two metrics may be strongly correlated, meaning that a design that is good for one of the metrics is frequently also good for the other metric.",
            "A decoder is a logic circuit that converts coded inputs into coded outputs, usually one output code for each input code. Decoders are used in a variety of applications, including data storage and transmission, error detection and correction, and data compression.",
            "The fully-specified transition diagram for a three-bit Gray code counter is as follows:\n\nThe states of the counter form a loop, with the counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_2S_1S_0 (before \" / \") and the output Z_2Z_1Z_0 (after \" / \").\n\nBased on the transition diagram, the next-state values S_2^+, S_1^+, and S_0^+ can be filled in as shown."
        ],
        "score": [
            6.953990936279297,
            -11.203390121459961,
            -11.194188117980957
        ],
        "text": "range corresponds to systems in which when a processor's power consumption in a digital  system is extremely low relative to the power consumption of the system. For example, the processor in a computer might use less than 1  of the total used by  the system including the disk drive, the monitor, the power  supply, and so forth.  One power consumption value in this range  is just as good as any another, and no one cares about the power consumption of the processor  in such cases.  In the second range, power consumption of the processor makes a difference.  Cell phones use most of their energy in radio operation, for example, but if you own a phone with a powerful processor, you may have noticed that you can turn off the phone and  drain the battery fairly quickly by playing a game.  Designing a processor that uses half as much power lengthens the battery life in such cases.  Finally, the third region of power consumption measurements is impossible: if you use so much power, your chip will overheat or even burst into flames.  Consumers get unhappy when such things happen.  As a first step, you can remove any metrics for which all solutions are effectively equivalent.  Until a little less than a decade ago, for example, the power  consumption of a desktop processor actually was in the first range that we discussed.  Power was simply not a concern to engineers: all designs of  interest consumed so little power that no one cared.  Unfortunately, at that point, power consumption jumped into the third range rather quickly.  Processors hit a wall, and  products had to be cancelled.  Given that the time spent designing a processor has historically been about five years, a lot of engineering effort was wasted because people had not thought carefully enough about power (since it had never mattered in the past).  Today, power is an important metric that engineers must take into account in their designs.   However, in some areas, such as desktop and high-end server processors, other metrics (such as performance) may be so  important that we always want to operate at the edge of the interesting range.  In such cases, we might choose to treat  a metric such as power consumption as a { threshold}: stay below 150 Watts for a desktop processor, for example.  One still has to make a coordinated effort to ensure that the system as a whole does not exceed the threshold, but reasoning about  threshold values, a form of constraint, is easier than trying to think about multiple metrics at once.  Some metrics may only allow discrete quantification.  For example, one could choose to define compatibility with previous processor generations as binary: either an existing piece of software (or operating system) runs out of the box on your new processor, or it does not.  If you  want people who own that software to make use of your new processor, you must ensure that the value of this binary metric is 1, which can also be viewed as a threshold.  In some cases, two metrics may be strongly { correlated}, meaning that a design that is good for one of the metrics is frequently  good for the other metric as well.  Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider them separately.  A design that requires a larger chip is probably more complex, and thus takes more engineering time to get right (engineering time costs money).    Each silicon wafer costs money to fabricate, and fewer copies of a  large design fit on one wafer, so large chips mean more fabrication cost.  Physical defects in silicon can cause some chips not to work.  A large chip uses more silicon than a small one, and is thus more likely to suffer from defects (and not work).  Cost thus goes up again for large chips relative to small ones.  Finally, large chips usually require more careful testing to ensure that they work properly (even ignoring the cost of getting the design right, we have to test for the presence of defects), which"
    },
    {
        "question": "How can a bad state representation be improved?', '\\n",
        "answers": [
            "\nOne way to improve a bad state representation is to make it more compact. This can be done by using two-level logic instead of three-level logic. Another way to improve a bad state representation is to make it more readable. This can be done by using clear and consistent naming conventions.",
            "Logical completeness does not necessarily help with efficient design of logic functions. A single Boolean logic function can be expressed in many different ways. Learning how to develop an efficient implementation of a function as well as how to determine whether two logic expressions are identical without actually writing out truth tables are important engineering skills that students will learn in the coming months.",
            "The uninterrupted counting cycle represents the duration of each state in the FSM. This is used to calculate the duration of each state in the FSM when F=0."
        ],
        "score": [
            8.714927673339844,
            -11.252517700195312,
            -11.035775184631348
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How does the state representation impact the FSM?\\n",
        "answers": [
            "\nThe state representation impacts the FSM by determining the duration of each state. The timer is set whenever the state S changes, and the duration of each state is determined by the muxes. The bottom mux selects A when S_2=0, and selects the output of the top mux when S_2=1. The top mux selects B when S_0=1, and selects C when S_0=0.",
            "A decoder is used in this context to simplify the expression of a logic function. By using a decoder, the function can be expressed as a two-level logic function, which is easier to understand and manipulate.",
            "The output of the FSM is a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle."
        ],
        "score": [
            8.601655006408691,
            -11.304567337036133,
            -5.0882978439331055
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What are the consequences of a bad state representation?\\n",
        "answers": [
            "\nA bad state representation can lead to a number of problems, including wasted engineering effort, poor performance, and even product cancellations.",
            "A decoder is similar to a mux in that it allows one digital signal to select which of several others is forwarded to an output. A decoder has one input and multiple outputs, while a mux has multiple inputs and one output. A decoder can be thought of as a special case of a mux.",
            "SRAM cells are combined into a bit slice by sharing bit lines and analog read and write logic. The bit slice in the figure can be thought of as a 16-address, 1-bit-addressable memory (2^4b). The cells in a bit slice share bit lines and analog read and write logic, which appears to the right in the figure."
        ],
        "score": [
            7.776063919067383,
            -11.258938789367676,
            -11.290929794311523
        ],
        "text": "range corresponds to systems in which when a processor's power consumption in a digital  system is extremely low relative to the power consumption of the system. For example, the processor in a computer might use less than 1  of the total used by  the system including the disk drive, the monitor, the power  supply, and so forth.  One power consumption value in this range  is just as good as any another, and no one cares about the power consumption of the processor  in such cases.  In the second range, power consumption of the processor makes a difference.  Cell phones use most of their energy in radio operation, for example, but if you own a phone with a powerful processor, you may have noticed that you can turn off the phone and  drain the battery fairly quickly by playing a game.  Designing a processor that uses half as much power lengthens the battery life in such cases.  Finally, the third region of power consumption measurements is impossible: if you use so much power, your chip will overheat or even burst into flames.  Consumers get unhappy when such things happen.  As a first step, you can remove any metrics for which all solutions are effectively equivalent.  Until a little less than a decade ago, for example, the power  consumption of a desktop processor actually was in the first range that we discussed.  Power was simply not a concern to engineers: all designs of  interest consumed so little power that no one cared.  Unfortunately, at that point, power consumption jumped into the third range rather quickly.  Processors hit a wall, and  products had to be cancelled.  Given that the time spent designing a processor has historically been about five years, a lot of engineering effort was wasted because people had not thought carefully enough about power (since it had never mattered in the past).  Today, power is an important metric that engineers must take into account in their designs.   However, in some areas, such as desktop and high-end server processors, other metrics (such as performance) may be so  important that we always want to operate at the edge of the interesting range.  In such cases, we might choose to treat  a metric such as power consumption as a { threshold}: stay below 150 Watts for a desktop processor, for example.  One still has to make a coordinated effort to ensure that the system as a whole does not exceed the threshold, but reasoning about  threshold values, a form of constraint, is easier than trying to think about multiple metrics at once.  Some metrics may only allow discrete quantification.  For example, one could choose to define compatibility with previous processor generations as binary: either an existing piece of software (or operating system) runs out of the box on your new processor, or it does not.  If you  want people who own that software to make use of your new processor, you must ensure that the value of this binary metric is 1, which can also be viewed as a threshold.  In some cases, two metrics may be strongly { correlated}, meaning that a design that is good for one of the metrics is frequently  good for the other metric as well.  Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider them separately.  A design that requires a larger chip is probably more complex, and thus takes more engineering time to get right (engineering time costs money).    Each silicon wafer costs money to fabricate, and fewer copies of a  large design fit on one wafer, so large chips mean more fabrication cost.  Physical defects in silicon can cause some chips not to work.  A large chip uses more silicon than a small one, and is thus more likely to suffer from defects (and not work).  Cost thus goes up again for large chips relative to small ones.  Finally, large chips usually require more careful testing to ensure that they work properly (even ignoring the cost of getting the design right, we have to test for the presence of defects), which"
    },
    {
        "question": " How can a bad state representation be fixed?\\n",
        "answers": [
            "\nA bad state representation can be fixed by using a code with a larger Hamming distance. This will enable the system to detect and correct more errors.",
            "The game revolves around an old logic problem in which a farmer must cross a river in order to reach the market. The farmer is traveling to the market to sell a fox, a goose, and some corn. The farmer has a boat, but the boat is only large enough to carry the fox, the goose, or the corn at a time. The fox cannot be left alone with the goose, or the fox will eat the goose. The goose cannot be left alone with the corn, or the goose will eat the corn. How can the farmer get all three items to the market?",
            "A write operation on a SRAM cell is performed by bringing the bit line to an intermediate voltage level, then raising the SELECT line, which allows the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line. The resulting change in voltage is then detected by a sense amplifier at the end of the bit line."
        ],
        "score": [
            8.868773460388184,
            -11.302597999572754,
            -11.2861967086792
        ],
        "text": "In contrast, the Hamming distance of the {2-out-of-5} code that we discussed earlier is 2.  Similarly, the Hamming distance of any representation extended with a parity bit is at least 2.  Now let's think about the problem slightly differently.  Given a particular representation,   how many bit errors can we detect in values using that representation?  { A representation with Hamming distance d can detect up to d-1 bit errors.}  To understand this claim, start by selecting a code word from the representation and changing up to d-1 of the bits.  No matter how one chooses to change the bits, these changes cannot result in another code word, since we know that any other code word has to  require at least d changes from our original code word, by the definition of the representation's Hamming distance.  A digital system using the representation can thus detect up to d-1 errors.  However, if d or more errors occur, the system might sometimes fail to detect any error in the stored value.   {Error Correction}  Detection of errors is important, but may sometimes not be enough. What can a digital system do when it detects an error?  In some cases, the system may be able to find the original value elsewhere, or may be able to re-compute the value from other values.  In other  cases, the value is simply lost, and the digital system may need to reboot or even shut down until a human can attend to it.  Many real systems cannot afford such a luxury.  Life-critical systems such as medical equipment and airplanes should not turn themselves off and wait for a human's attention.  Space vehicles face a similar dilemma, since no human may be able to reach them.  Can we use a strategy similar to the one that we have developed for error detection in order to try to perform { error correction}, recovering the original value?  Yes, but the overhead---the extra bits that we need to provide such functionality---is higher.    Let's start by thinking about a code with Hamming distance 2, such as {4-bit} 2's complement with odd parity.  We know that such a  code can detect one bit error.  Can it correct such a bit error, too?  Imagine that a system has stored the decimal value 6 using the  pattern 0110{ 1}, where the last bit is the odd parity bit. A bit error occurs, changing the stored pattern to 0111{ 1}, which is not a valid pattern, since it has an even number of 1s.  But can the system know that the original value stored was 6?  No, it cannot. The original value may also have been 7, in which case the original pattern was 0111{ 0}, and the bit error occurred in the final bit.  The original value may also have been -1, 3, or 5.  The system has no way of resolving this ambiguity.  The same problem arises if a digital system uses a code with Hamming distance d to detect up to d-1 errors.   Error correction is possible, however, if we assume that fewer bit errors occur (or if we instead use a representation with a larger Hamming distance).  As a simple example, let's create a representation for the numbers 0 through 3 by making three copies of the {2-bit} unsigned  representation, as shown to the right.  The Hamming distance of the resulting code is 3, so any two bit errors can be detected.  However, this code also enables us to correct a single bit error.  Intuitively,  think of the three copies as voting on the right answer.   {c|c} value      & three-copy represented& code  0& 000000 1& 010101 2& 101010 3& 111111    Since a single bit error can only corrupt one copy, a majority vote always gives the right answer! Tripling the number of bits needed in a representation is not a good general strategy, however.  Notice also that ``correcting'' a pattern with two bit errors can produce the wrong result.  Let's think about the problem in terms of Hamming distance.  Assume that we use a code with Hamming distance d and imagine that up to k bit errors affect a stored value. The resulting pattern then falls within a neighborhood of distance k from the"
    },
    {
        "question": " What is the best state representation for an FSM?\\n",
        "answers": [
            "\nThere is no best state representation for an FSM. The most important factor is that the representation is complete, meaning that all states are represented and all inputs are accounted for. Other considerations include the size of the state space and the number of inputs and outputs.",
            "What is the difference between two-level logic and other forms of logic?\n\nTwo-level logic is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. In this section, we illustrate one of the reasons for this popularity and show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.",
            "The output of the counter is the number of successful conversions made by the scanf function."
        ],
        "score": [
            6.9446868896484375,
            -11.219252586364746,
            -11.164117813110352
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " Why is it important to have a good state representation?']",
        "answers": [
            "\nA good state representation is important because it can help reduce the number of transistors needed for a design, and thus save space and energy. Additionally, it can help improve the performance of a design by making it easier for the computer to manipulate the logic expressions.",
            "The internal state value for the third bit gray code counter is S_2S_1S_0=001. This is because when HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired.",
            "The fourth step in the design process is to choose a particular design from the set of possible designs. In making this choice, engineers typically consider a variety of factors, including cost, performance, manufacturability, and reliability."
        ],
        "score": [
            10.277412414550781,
            -9.814538955688477,
            -11.15162467956543
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": "What is the memory abstraction?\\n",
        "answers": [
            "\nA computer memory is a group of storage elements and the logic necessary to move data in and out of the elements. The size of the elements in a memory—called the addressability of the memory—varies from a single binary digit, or bit, to a byte (8 bits) or more. Typically, we refer to data elements larger than a byte as words, but the size of a word depends on context.",
            "The value of S_2 when the counter is in the COUNT A state and HOLD=0 is undefined. This is because, as the passage states, overflow occurs when either both operands are negative or both operands are positive. If both operands are negative, then the resulting sum S is equal to C. However, if both operands are positive, then the resulting sum S is less than C. Thus, it is impossible to determine the value of S_2 without knowing the values of both operands.",
            "A computer memory is a group of storage elements and the logic necessary to move data in and out of the elements. The size of the elements in a memory—called the addressability of the memory—varies from a single binary digit, or bit, to a byte (8 bits) or more. Typically, we refer to data elements larger than a byte as words, but the size of a word depends on context."
        ],
        "score": [
            -0.8544909954071045,
            -11.197060585021973,
            -0.8544909954071045
        ],
        "text": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below"
    },
    {
        "question": "What are the types of memory most commonly used in digital systems?\\n",
        "answers": [
            "\nThe most common types of memory used in digital systems are SRAM and DRAM. SRAM is used for high-speed applications such as processor caches and some embedded designs. DRAM is used for most applications with less demanding speed requirements.",
            "HOLD = H + S_2\n\nWhen the counter is in the HALT A state and S_2=1, HOLD = 1. This is because the counter should stop (move into a halted state without changing value) when H is true, and in the halted states, when S_2=1, the counter stops (stays in a halted state) when H+ is true.",
            "A computer memory is a storage device for data. There are two main types of memory: SRAM and DRAM. SRAM is faster but more expensive, while DRAM is slower but cheaper. DRAM is more commonly used for main memory in computers."
        ],
        "score": [
            9.575108528137207,
            -11.115988731384277,
            4.4195733070373535
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": "How can one build static RAMs using logic?\\n",
        "answers": [
            "\nOne can build static RAMs using logic by using tri-state buffers. Tri-state buffers can be used to create larger memories by using multiple chips. Wider memories can also be created by using multiple chips.",
            "The state of the FSM has been split into two pieces: a {3-bit} register S and a {6-bit} timer. The timer is simply a binary counter that counts downward and produces an output of Z=1 when it reaches 0. Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.\n\nWe can thus think of transitions in the FSM on a cycle by cycle basis as consisting of two types. The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S for some number of cycles.",
            "The next-state logic equations are the functions that describe the relationship between the current state of the system and the next state of the system. These functions can be represented in either SOP or POS form. DeMorgan's law can be used to simplify these functions by replacing OR gates with NAND gates, or AND gates with NOR gates."
        ],
        "score": [
            9.192243576049805,
            -11.060721397399902,
            -10.224589347839355
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": "What are tri-state buffers and how do they simplify output connections?\\n",
        "answers": [
            "\nTri-state buffers are used to provide logical disconnection between the tri-state buffer and the output. This logical disconnection is achieved by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.",
            "The possible outputs for this FSM are A=1 or P=1. F is a signal that is used to set the timer, and it can be either 0 or 1. If F=0, then the timer is only set when the state S changes. If F=1, then the timer is set when the state S changes or when the light on one of the roads is green.",
            "The vending machine application and FSM were designed by connecting the buttons B_1, B_2, and B_3 to the priority encoder (and feeding 0 into the fourth input), which produces a signal P indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants. The signal R is also used to enable a decoder that generates the three individual item release outputs. The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next cycle."
        ],
        "score": [
            5.478311538696289,
            -10.193862915039062,
            -9.998964309692383
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": "How can memory chips be combined to provide larger and wider memories?', '",
        "answers": [
            "\nMemory chips can be combined to provide larger and wider memories by using multiple chips. For example, two 2-bit memories can be used to implement a 4-bit memory. One of the address bits is used to drive a decoder that determines which of the two chips is active (CS=1). The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.",
            "The value of register PRICE1 is the price of the first item in the machine, which is determined by the owner of the machine.",
            "Abstraction in selecting a representation can impact the design metrics by reducing the number of metrics that need to be considered. This can simplify the process of designing a solution and make it easier to find an optimal solution."
        ],
        "score": [
            10.31557846069336,
            -11.265661239624023,
            -11.206079483032227
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": "What is the memory abstraction?\\n",
        "answers": [
            "\nA computer memory is a group of storage elements and the logic necessary to move data in and out of the elements. The size of the elements in a memory—called the addressability of the memory—varies from a single binary digit, or bit, to a byte (8 bits) or more. Typically, we refer to data elements larger than a byte as words, but the size of a word depends on context.",
            "If A were to rise with T and fall with the rising clock edge, then the processor would not be able to recognize interrupts between instructions. This would cause the program to terminate unexpectedly.",
            "One possibility is that the FSM is not resetting properly, so it does not start from the expected initial state. Another possibility is that there is a race condition in the FSM, so that it can reach an unexpected state if the inputs change too quickly."
        ],
        "score": [
            -0.8544909954071045,
            -11.32398796081543,
            -11.286787986755371
        ],
        "text": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below"
    },
    {
        "question": "What are the types of memory most commonly used in digital systems?\\n",
        "answers": [
            "\nThe two most common types of memory used in digital systems are static random access memory (SRAM) and dynamic random access memory (DRAM). SRAM is used for high-speed applications such as processor caches and some embedded designs, while DRAM is used for applications with less demanding speed requirements, such as main memory in most computers.",
            "The most common types of memory used in digital systems are SRAM and DRAM. SRAM is used for high-speed applications such as processor caches and some embedded designs. DRAM is used for most applications with less demanding speed requirements.",
            "The input sequence for testing is as follows: first, the user is prompted to input two integers in decimal form, which are then converted to 2's complement and stored in variables 'a' and 'b'. Next, the user is prompted to input an ASCII character, a value in hexadecimal, and a double-precision floating-point number, which are then stored in variables 'c', 'u', and 'd', respectively. Finally, the user is prompted to input two real numbers in decimal form, which are then converted to single- and double-precision floating-point numbers and stored in variables 'f' and 'd', respectively."
        ],
        "score": [
            9.587854385375977,
            9.575108528137207,
            -11.117599487304688
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": "How can one build memories (static RAMs) using logic?\\n",
        "answers": [
            "\nOne can build memories using logic by creating a tri-state buffer. This logical disconnection is achieved by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.",
            "The inputs to the FSM are the states of the two roads, and the outputs are the colors of the lights for the two roads. The FSM ensures that the lights on one or both roads are red at all times, and that the lights on the other road are green. The FSM also alternates green lights between the two roads.",
            "The second version of the table implies that the buttons are not placed in the most ergonomic way possible. The buttons are too close together and there is not enough space between the buttons and the edge of the table."
        ],
        "score": [
            6.699590682983398,
            -11.325695037841797,
            -11.30361557006836
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": "What is a tri-state buffer?\\n",
        "answers": [
            "\nA tri-state buffer is a type of digital logic device that is used to either connect or disconnect a signal line from a power source or ground. This is achieved by using two transistors, one to connect the signal line to the power source, and the other to connect the signal line to ground. When the buffer is enabled, both transistors are turned on, and the signal line is connected to either the power source or ground, depending on the value of the input signal. When the buffer is disabled, both transistors are turned off, effectively disconnecting the signal line from the power source or ground.",
            "The FSM is initialized to the state \"000\" in order to ensure that the lights on one or both roads are red at all times. The timer is set to \"A\" in order to maintain the new value of S for some number of cycles.",
            "The state table represents the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The lights are all red in both of these states, which is necessary for safety."
        ],
        "score": [
            8.645543098449707,
            -11.043415069580078,
            -8.377574920654297
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": "How can memory chips be combined to provide larger and wider memories?', '\\n\\n",
        "answers": [
            "\nOne way to combine memory chips is to use multiple chips to implement a larger memory. This can be done by using one address bit to drive a decoder that determines which of the two chips is active. Another way to combine memory chips is to use multiple chips to implement a wider memory. This can be done by routing the external address, write enable, and chip select inputs to both chips. The data inputs and outputs are separate, with the left chip handling the high N bits of input on writes and producing the high N bits of output on reads, while the right chip handles the low N bits of input and produces the low N bits of output.",
            "A good state representation is important because it can help reduce the number of logic gates needed for a design, and thus save on chip area. Additionally, a good state representation can help improve the performance of a design by reducing the number of logic operations needed.",
            "The fully-specified transition diagram for a three-bit Gray code counter is as follows:\n\nThe states of the counter form a loop, with the counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_2S_1S_0 (before \" / \") and the output Z_2Z_1Z_0 (after \" / \").\n\nBased on the transition diagram, the next-state values S_2^+, S_1^+, and S_0^+ can be filled in as shown."
        ],
        "score": [
            7.924561977386475,
            -10.78068733215332,
            -11.214936256408691
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How does one build memories (static RAMs) using logic?\\n",
        "answers": [
            "\nOne builds memories (static RAMs) using logic by connecting a dual-inverter loop to opposing BIT lines through transistors controlled by a SELECT line. When SELECT is high, the transistors connect the inverter loop to the bit lines. When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input. When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.",
            "One way to transform code written in a high-level language into an FSM is to use a compiler. A compiler is a program that translates code written in a high-level language into code that can be run on a computer. Another way to transform code written in a high-level language into an FSM is to use an interpreter. An interpreter is a program that executes code written in a high-level language.",
            "Prof. Doug Jones designed the vending machine application and the FSM by first identifying the different states that the machine could be in. He then created a state diagram that showed how the machine would transition from one state to another. Finally, he implemented the FSM in hardware using the Altera Quartus software."
        ],
        "score": [
            9.685003280639648,
            -11.165358543395996,
            -11.202383041381836
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": " What is a tri-state buffer and how does it simplify output connections?\\n",
        "answers": [
            "\nA tri-state buffer is a logic circuit that can be used to disconnect an output from a logic circuit. This is achieved by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT.",
            "SDRAM is a type of DRAM that is used for main memory in computers. DDR4 SDRAM is a newer type of DRAM that is faster than SDRAM.",
            "A multiplexer is a device that allows multiple signals to be routed onto a single channel. This is done by selecting one of several input signals and connecting it to the output. Multiplexers are used in a variety of applications, such as telecommunications and computer networking."
        ],
        "score": [
            6.722924709320068,
            -11.125228881835938,
            -7.118690013885498
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How can memory chips be combined to provide larger and wider memories?\\n",
        "answers": [
            "\nOne way to combine memory chips is to use them to create a larger memory. This can be done by using one address bit to drive a decoder that determines which of the two chips is active. The other address bits, as well as the external data inputs and write enable signal, are then delivered to both memories. The external data outputs are also connected to both memories.\n\nAnother way to combine memory chips is to use them to create a wider memory. In this case, both chips are either active or inactive at the same time, and the external address, write enable, and chip select inputs are routed to both chips. However, the data inputs and outputs are separate, with each chip handling a different part of the data.",
            "The next-state logic equations are the functions that describe the relationship between the current state of the system and the next state of the system. These functions can be represented in either SOP or POS form. DeMorgan's law can be used to simplify these functions by replacing OR gates with NAND gates, or AND gates with NOR gates.",
            "Scan-based testing is a method of testing where the input is scanned and converted into the appropriate result. The programmer is responsible for ensuring that the number of conversions in the format string matches the number of arguments provided. The programmer must also ensure that the type of information produced by each conversion can be stored at the address passed for that conversion."
        ],
        "score": [
            8.462427139282227,
            -11.269559860229492,
            -11.21841049194336
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " What is a more detailed description of dynamic RAMs?\\n",
        "answers": [
            "\nDynamic RAMs are memories that use a charged capacitor to store a bit. The charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.'' DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.",
            "One way to improve a bad state representation is to make it more compact. This can be done by using two-level logic instead of three-level logic. Another way to improve a bad state representation is to make it more readable. This can be done by using clear and consistent naming conventions.",
            "The next state is FXGC=0101. The farmer is not on the same bank as the fox, nor as the corn, so the B_X and B_C buttons are ignored, leading to the next-state table to the right. Notice that B_G is accepted even if B_C is pressed because the farmer is not on the same bank as the corn."
        ],
        "score": [
            4.1085205078125,
            -11.266328811645508,
            -11.318989753723145
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": " How do random access memories (RAMs) work?']",
        "answers": [
            "\nRandom access memories (RAMs) are memories that support two operations: writing data into an element, and reading data from an element. The memories are also volatile, which means that the data held by a memory are erased when electrical power is turned off or fails. Static random access memory (SRAM) is used for high-speed applications such as processor caches and some embedded designs.",
            "What is the difference between SOP and POS?\n\nSOP and POS are two forms of two-level logic. In SOP, the output is true if any of the inputs are true. In POS, the output is true if all of the inputs are true.",
            "The flow chart represents the steps that should be taken when designing a new product. The first step is to think carefully about metrics of possible interest. The second step is to get rid of as many metrics as possible. The third step is to identify metrics that have no practical impact in current technology. The fourth step is to set threshold values for other metrics to simplify reasoning about them. The fifth step is to eliminate redundant metrics. The sixth step is to calculate linear sums to reduce the count of metrics. The seventh step is to make use of the notion of Pareto optimality."
        ],
        "score": [
            8.752019882202148,
            -11.27370834350586,
            -11.157554626464844
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": " What is the addressability of a computer memory?\\n",
        "answers": [
            "\nThe addressability of a computer memory is the number of bits that can be stored in each element of the memory. The size of the elements in a memory can vary from a single binary digit, or bit, to a byte (8 bits) or more. Typically, data elements larger than a byte are referred to as words, but the size of a word depends on context.",
            "An FSM is a finite state machine, which is a model of computation used to design both computer programs and hardware. An FSM is defined by a set of states, a set of inputs, a set of outputs, and a set of transitions between states.",
            "Abstraction is a powerful tool that can help simplify complex problems. When selecting a state representation, abstraction can help identify the most important aspects of the problem and ignore irrelevant details. This can make it easier to find a solution that is both effective and efficient."
        ],
        "score": [
            8.18252182006836,
            -10.842914581298828,
            -11.189680099487305
        ],
        "text": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below"
    },
    {
        "question": " What is the difference between SRAM and DRAM?\\n",
        "answers": [
            "\nSRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors, while a DRAM cell uses only one transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor",
            "If the \"halt\" button is pressed (HG=10), the logic outputs C_L=00 and C_M=00, so the dispenser gives no ice cream. If both buttons are pressed (HG=11), the outputs are C_L=11 and C_M=11, so the dispenser gives two servings of each flavor, or four servings total.",
            "Pressing the ``halt'' button does nothing when the counter is halted. The counter remains in the same state until the ``go'' button is pressed."
        ],
        "score": [
            8.886859893798828,
            -10.915733337402344,
            -11.148513793945312
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How do you write data into a memory element?\\n",
        "answers": [
            "\nTo write data into a memory element, an external circuit sends a signal to the memory indicating the address of the element to be written. The data to be written is then sent to the memory, and the memory stores the data in the element at the specified address.",
            "The passage discusses the design of a serial comparator using a bit-sliced approach. This approach leads to a design with 24 gates and 10 inverters, which is not much larger than the earlier serial design. However, the new design is identical in terms of gate delays, meaning that a comparison can be finished in half the time.",
            "The purpose of the FSM is to control the stoplights at a busy intersection. The FSM ensures that the lights are always red on one or both roads, that a yellow light is shown before a green light, and that the green lights are alternated between the two roads."
        ],
        "score": [
            8.700210571289062,
            -11.309572219848633,
            -11.268518447875977
        ],
        "text": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below"
    },
    {
        "question": " How do you read data from a memory element?\\n",
        "answers": [
            "\nA computer memory is a group of storage elements and the logic necessary to move data in and out of the elements. The size of the elements in a memory—called the addressability of the memory—varies from a single binary digit, or bit, to a byte (8 bits) or more. Typically, we refer to data elements larger than a byte as words, but the size of a word depends on context. Each element in a memory is assigned a unique name, called an address, that allows an external circuit to identify the particular element of interest. These addresses are not unlike the street addresses that you use when you send a letter. Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.",
            "The player loses the game if they are not able to correctly answer the question. If the player gets the question wrong, they will have to start the game over from the beginning.",
            "One way to design an FSM is to start with a flow chart of the program that is to be implemented. This flow chart can then be used to identify high-level states that can be implemented in a fixed number of clock cycles. Another way to design an FSM is to start with a list of the components that are needed to implement the program. This list can then be used to identify high-level states that can be implemented using those components."
        ],
        "score": [
            2.7144079208374023,
            -11.30847454071045,
            -11.18111801147461
        ],
        "text": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below"
    },
    {
        "question": " What is a chip select input?', '\\n",
        "answers": [
            "\nA chip select input is an input to a chip that allows the chip to be turned on or off. This input is used to save energy by turning off the chip when it is not needed.",
            "There are a few possible ways to answer this question. One way would be to say that if the user presses the ``halt'' button while in state { HALT A}, the system should reset to state { COUNT A}. Another way to answer this question would be to say that if the user presses the ``halt'' button while in state { HALT A}, the system should remain in state { HALT A}.",
            "The inputs to the FSM are the states of the two roads, and the outputs are the colors of the lights for the two roads. The FSM ensures that the lights on one or both roads are red at all times, and that the lights on the other road are green. The FSM also alternates green lights between the two roads."
        ],
        "score": [
            8.48147964477539,
            -11.154749870300293,
            -10.152839660644531
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": "What is the addressability of a computer memory?\\n",
        "answers": [
            "\nA computer memory is a group of storage elements and the logic necessary to move data in and out of the elements. The size of the elements in a memory—called the addressability of the memory—varies from a single binary digit, or bit, to a byte (8 bits) or more. Typically, we refer to data elements larger than a byte as words, but the size of a word depends on context.",
            "What is the difference between SOP and POS?\n\nSOP and POS are two forms of two-level logic. In SOP, the output is true if any of the inputs are true. In POS, the output is true if all of the inputs are true.",
            "The purpose of adding a few AND gates and a input is to create a logic function that is more efficient in terms of area and power. This is done by reducing the number of variables in the expression."
        ],
        "score": [
            6.024157524108887,
            -11.32525634765625,
            -11.298999786376953
        ],
        "text": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below"
    },
    {
        "question": "What is a word in the context of computer memory?\\n",
        "answers": [
            "\nA word in the context of computer memory is a sequence of bits that represents a unit of data. Words can represent numbers, characters, or other data types. The size of a word is typically determined by the computer's word size, which is the number of bits the computer can process at a time.",
            "The problem that the code is trying to solve is the halting problem. The halting problem is a problem that cannot be solved by a Turing machine. The problem is that given a Turing machine and an input to the Turing machine, it is not possible to determine whether the machine will finish computing in a finite number of steps.",
            "DRAM is a type of memory that is slower than SRAM but denser (has more bits per chip area). DRAM cells use only a single transistor, while SRAM cells use six transistors. DRAM designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor count."
        ],
        "score": [
            8.644383430480957,
            -10.979316711425781,
            -10.224853515625
        ],
        "text": "to associate arbitrary { symbolic names} (sequences of English characters,  such as ``sum'' or ``product'' or ``highScore'') with specific types of data, such as a {16-bit} unsigned integer or a double-precision floating-point number.   In the example program above, the variable { answer} is declared to be a {32-bit} {2's} complement number.  Those with no programming experience may at first find the difference between variables in algebra and variables in programs slightly  confusing.  { As a program executes, the values of variables can  change from step to step of execution.}  The statements in the { main} function are executed one by one until the program terminates.    Programs are not limited to simple sequences of statements, however. Some types of statements allow a programmer to specify conditional behavior.  For example, a program might only print out secret information if the user's name is ``lUmeTTa.'' Other types of statements allow a programmer to repeat the execution of a group of statements until a condition is met.  For example, a program might print the numbers from 1 to 10, or ask for input until the user types a number between 1 and 10.  The order of statement execution is well-defined in C, but the statements in { main} do not necessarily make up an algorithm: { we can easily write a C program that never terminates}.  If a program terminates, the { main} function returns an integer to the operating system, usually by executing a { return} statement, as in the example program.  By convention, returning the value 0 indicates successful completion of the program, while any non-zero value indicates a program-specific error.  However, { main} is not necessarily a function in the mathematical  sense because { the value returned from { main} is not  necessarily unique for a given set of input values to the program}.    For example, we can write a program that selects a number from 1 to 10  at random and returns the number to the operating system.     {Data Types}  As you know, modern digital computers represent all information with binary digits (0s and 1s), or { bits}.  Whether you are representing  something as simple as an integer or as complex as an undergraduate  thesis, the data are simply a bunch of 0s and 1s inside a computer.    For any given type of information, a human selects a data type for the information.  A { data type} (often called just a { type}) consists of both a size in bits and a representation, such as the 2's complement representation for signed integers, or the ASCII representation for English text.  A { representation} is a way of encoding the things being represented as a set of bits, with each bit pattern corresponding to a unique object or thing.  A typical ISA supports a handful of data types in hardware in the sense that it provides hardware  support for operations on those data types.  The arithmetic logic units (ALUs) in most modern processors, for example, support addition and subtraction of both unsigned and 2's complement representations, with the specific data type (such as 16- or 64-bit 2's complement) depending on the ISA.  Data types and operations not supported by the ISA must be handled in software using a small set of primitive operations, which form the { instructions} available in the ISA.  Instructions usually include data movement instructions such as loads and stores and control instructions such as branches and subroutine calls in addition to arithmetic and logic operations.    The last quarter of our class covers these concepts in more detail and explores their meaning using an example ISA from the textbook.  In class, we emphasized the idea that digital systems such as computers do not interpret the meaning of bits.  Rather, they do exactly what they have been designed to do, even if that design is meaningless.  If, for example, you store a sequence of ASCII characters  in a computer's memory as  and then write computer instructions to add consecutive groups of"
    },
    {
        "question": "What is an example of a non-volatile memory?\\n",
        "answers": [
            "\nNon-volatile memories include magnetic and optical storage media such as DVDs, CD-ROMs, disks, and tapes, capacitive storage media such as Flash drives, and some programmable logic devices.",
            "The value of register PRICE2 is the number of nickels in the machine multiplied by the price of the item in question. In this way, the machine can keep track of how much money has been inserted and how much is owed for the item.",
            "The outputs needed to observe in each state to check the output logic are the values of the state register S and the timer. These outputs can be used to determine the duration of each state and the order in which the states occur."
        ],
        "score": [
            6.494564056396484,
            -11.328117370605469,
            -11.324954986572266
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": "What is a RAM?\\n",
        "answers": [
            "\nA RAM is a type of memory that is used to store data. It is a volatile memory, which means that the data stored in the memory is erased when the power is turned off or fails. RAM is a random access memory, which means that the time required to access an element in the memory is independent of the element being accessed.",
            "The fourth step in the design process is to choose a particular design from the set of possible designs. In making this choice, engineers typically consider a variety of factors, including cost, performance, manufacturability, and reliability.",
            "The device uses the sensors to generate a clock signal by alternating between 0 and 1 with a regular period. When the clock is 0, one set of latches holds its bit values fixed, and combinational logic uses those latches as inputs to produce  bits that are copied into a second set of latches. When the clock is 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches."
        ],
        "score": [
            5.940305709838867,
            -11.297304153442383,
            -10.952877044677734
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": "What is the difference between SRAM and DRAM?', '\\n",
        "answers": [
            "\nSRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor",
            "A decoder is a logic circuit that converts signals from one form to another. In binary form, a decoder can be used to convert a binary number into a decimal number.",
            "The next-state logic equations are the functions that describe the relationship between the current state of the system and the next state of the system. These functions can be represented in either SOP or POS form. DeMorgan's law can be used to simplify these functions by replacing OR gates with NAND gates, or AND gates with NOR gates."
        ],
        "score": [
            8.118745803833008,
            -10.245919227600098,
            -10.843611717224121
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How does a computer memory work?\\n",
        "answers": [
            "\nA computer memory is a storage device for data. There are two main types of memory: SRAM and DRAM. SRAM is faster but more expensive, while DRAM is slower but cheaper. DRAM is more commonly used for main memory in computers.",
            "Non-volatile memories include magnetic and optical storage media such as DVDs, CD-ROMs, disks, and tapes, capacitive storage media such as Flash drives, and some programmable logic devices.",
            "Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created the wiki documentation and the Altera Quartus portions of the lab."
        ],
        "score": [
            2.36780047416687,
            -9.024269104003906,
            -11.323513984680176
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How are data elements stored in a computer memory?\\n",
        "answers": [
            "\nData elements in a computer memory are stored either in static random access memory (SRAM) or in dynamic random access memory (DRAM). SRAM uses active logic in the form of a two-inverter loop to maintain stored values, while DRAM uses a charged capacitor to store a bit.",
            "The fifth step in the design process is to select an internal representation for the bits necessary to encode the state of our FSM. In practice, for small designs, this representation can be selected by a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.",
            "The purpose of the FSM is to control the stoplights at a busy intersection. The FSM ensures that the lights are always red on one or both roads, that a yellow light is shown before a green light, and that the green lights are alternated between the two roads."
        ],
        "score": [
            8.077404975891113,
            -11.144619941711426,
            -11.258857727050781
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": " What is the addressability of a computer memory?\\n",
        "answers": [
            "\nThe addressability of a computer memory is the number of bits that can be stored in each element of the memory. The size of the elements in a memory can vary from a single bit to a byte or more. Typically, data elements larger than a byte are referred to as words, but the size of a word depends on context.",
            "The benefits of using DRAM are that it is denser than SRAM and uses fewer transistors. This makes DRAM ideal for applications where size is more important than speed.",
            "A decoder is a logic circuit that converts a binary code into a set of signals that can be used to operate a device. A decoder takes an input signal, typically one coded as a binary number, and produces one output for each possible value of the signal. When the decoder is enabled (EN=1), each of the AND gates produces one minterm of the input signal S. When the decoder is not enabled (EN=0), the decoder produces all 0 bits."
        ],
        "score": [
            8.178445816040039,
            -11.31238842010498,
            -11.048131942749023
        ],
        "text": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below"
    },
    {
        "question": " How are words stored in a computer memory?\\n",
        "answers": [
            "\nWords in a computer memory are stored as bits, which are either 0 or 1. A group of bits makes up a byte, which is the smallest unit of data that can be stored in a computer memory. A group of bytes makes up a word, which is the smallest unit of data that can be read or written in a computer memory.",
            "The FSM for the lab is a coin-counting FSM that requires the use of a Moore machine model to avoid sending the servo controlling the coin's path an output pulse that was too short to enforce the FSM's decision about which way to send the coin. By adding more states to the FSM, the servo can be held in place, as desired.",
            "The game revolves around an old logic problem in which a farmer must cross a river in order to reach the market. The farmer is traveling to the market to sell a fox, a goose, and some corn. The farmer has a boat, but the boat is only large enough to carry the fox, the goose, or the corn at a time. The fox cannot be left alone with the goose, or the fox will eat the goose. The goose cannot be left alone with the corn, or the goose will eat the corn. How can the farmer get all three items to the market?"
        ],
        "score": [
            7.075474739074707,
            -11.242887496948242,
            -11.299867630004883
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": " What is a random access memory?']",
        "answers": [
            "\nA random access memory (RAM) is a type of memory that allows data to be stored and retrieved in any order. RAM is volatile, which means that it loses all data when the power supply is removed. There are two types of RAM: static RAM (SRAM) and dynamic RAM (DRAM).",
            "A chip select input is used to enable or disable a chip. This input is used to save power by turning off the chip when it is not needed.",
            "The min variable is a heuristic metric used to estimate the time needed to compute a function. This metric is based on the longest chain of operations performed on any instance of a variable. The rationale for this metric is that gate outputs do not change instantaneously when their inputs change."
        ],
        "score": [
            10.045910835266113,
            -11.151994705200195,
            -11.135734558105469
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": " What is static random access memory used for?\\n",
        "answers": [
            "\nStatic random access memory is used for high-speed applications such as processor caches and some embedded designs. SRAM bit density is significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.",
            "The 'REJECTD' state is a state in which a processor interrupts a program due to an error or unexpected behavior. This state usually causes the program to terminate.",
            "The output A does not depend on the input T because the function of AND is to evaluate to 1 if all of the input values are 1. If there is an input with value 0, then the function will evaluate to 0."
        ],
        "score": [
            8.853185653686523,
            -10.935970306396484,
            -11.291555404663086
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": " What is the difference between SRAM and DRAM?\\n",
        "answers": [
            "\nSRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors, while a DRAM cell uses only one transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor",
            "Tri-state buffers are used to provide logical disconnection between the tri-state buffer and the output. This logical disconnection is achieved by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.",
            "From the passage, it seems that a digital FSM is designed from an abstract model by first identifying a finite set of states, a set of possible inputs, a set of possible outputs, and a set of transition rules. Then, the FSM is implemented as a digital system by representing all states as patterns using a fixed number of bits, translating all inputs into bits, and translating all outputs into bits. Finally, transition rules are defined so that given any state of the FSM and any pattern of input bits, a transition is defined from that state to another state."
        ],
        "score": [
            8.886859893798828,
            -10.806191444396973,
            -10.990987777709961
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How does an SRAM cell work?\\n",
        "answers": [
            "\nAn SRAM cell is a flip-flop made up of two cross-coupled inverters. The output of one inverter is connected to the input of the other, and vice versa. When power is applied, the SRAM cell is in one of two stable states, which represent a 0 or a 1. The state of the cell can be changed by applying the correct sequence of voltages to the inputs.",
            "Yes, we can choose to use the counter's internal state directly as our output values. This is because the colors we need to produce as outputs are all unique bit patterns. By using the counter's internal state directly as our output values, we can simplify our design.",
            "Dynamic RAMs are memories that use a charged capacitor to store a bit. The charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.'' DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs."
        ],
        "score": [
            6.417896270751953,
            -11.193672180175781,
            -10.161754608154297
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " How are SRAM cells combined into a bit slice?\\n",
        "answers": [
            "\nSRAM cells are combined into a bit slice by sharing bit lines and analog read and write logic. The bit slice in the figure can be thought of as a 16-address, 1-bit-addressable memory (2^4b). The cells in a bit slice share bit lines and analog read and write logic, which appears to the right in the figure.",
            "PASSAGE - compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column",
            "The HOLD variable is used to save energy. When a chip finishes its work, it can turn itself off, saving energy. This variable allows the chip to turn itself off when it is finished, saving energy."
        ],
        "score": [
            9.448197364807129,
            -11.30346965789795,
            -11.296310424804688
        ],
        "text": "line.  When SELECT is low, the transistors disconnect the inverters from the bit lines, and the cell holds its current value until SELECT goes high again.  The actual operation of an SRAM cell is more complicated than we have described.  For example, when writing a bit, the BIT lines  can temporarily connect high voltage to ground (a short).  The  circuit must be designed carefully to minimize the power consumed during this process.  When reading a bit, the BIT lines are pre-charged halfway between high-voltage and ground, and  analog devices called sense amplifiers are used to detect the voltage changes on the BIT lines (driven by the inverter loop) as quickly as possible.  These analog design issues are outside of  the scope of our class.   A number of cells are combined into a { bit slice}, as shown to the right.  The labels along the bottom of the figure are external inputs to the  bit slice, and match the labels for the abstract   {file=part3/figs/lec18-3.eps,width=5in}   memory discussed earlier.  The  bit slice in the figure can be thought of as a {16-address}, {1-bit-addressable} memory (2^4b).  The cells in a bit slice share bit lines and analog read and write logic, which appears to the right in the figure.  Based on the ADDR input, a decoder sets one cell's SELECT line high to enable a read or write operation to the cell.    The chip select input CS drives the enable input of the decoder, so none of the memory cells is active when chip select is low (CS=0), and exactly one of the memory cells is active when chip select is high (CS=1).  Actual bit slices can contain many more cells than are shown in the  figure---more cells means less extra logic per cell, but slower memory, since longer wires have higher capacitance.  A read operation is performed as follows.  We set CS=1 and WE=0, and place the address of the cell to be read on the ADDR input. The decoder outputs a 1 on the appropriate cell's SELECT line, and the read logic reads the bit from the cell and delivers it to its Q output, which is then available on the bit  slice's {DATA-OUT} output.  For a write operation, we set CS=1 and WE=1.  We again place the address of the cell to be written on the ADDR input and set the value of the bit slice's {DATA-IN} input to the value to be written into the memory cell.  When the decoder activates the cell's SELECT line, the write logic writes the new value from its D input into the memory cell.  Later reads from that cell then produce the new value.  {{file=part3/figs/lec18-4.eps,width=6.15in}}  The outputs of the cell selection decoder can be used to control multiple bit slices, as shown in the figure above of a {2^6b} memory.  Selection between bit slices is then based on other bits from the address (ADDR).  In the figure above, a {2-to-4} decoder is used to deliver write requests to one of four bit slices, and a {4-to-1} mux is used to choose the appropriate output bit for read requests.  The {4-to-16} decoder now activates one cell in each of the four  bit slices.  For a read operation, WE=0, and the {2-to-4} decoder  is not enabled, so it outputs all 0s.  All four bit slices thus perform reads, and the desired result bit is forwarded to {DATA-OUT} by the  {4-to-1} mux.  The tri-state buffer between the mux  and {DATA-OUT} is explained in a later section.  For a write operation, exactly one of the bit slices has its WE input set to 1 by the {2-to-4} decoder. That bit slice writes the bit value delivered to all bit slices from {DATA-IN}.  The other three bit slices perform reads, but their  results are simply discarded.  The approach shown above, in which a cell is selected through a two-dimensional indexing scheme, is known as { coincident selection}.  The qualifier ``coincident'' arises from the notion that the desired cell coincides with the intersection of the active row and column outputs from the decoders.  The benefit of coincident selection is easily calculated in terms of the number of gates required for the"
    },
    {
        "question": " What is the purpose of a chip select input?', '",
        "answers": [
            "\nA chip select input is used to enable or disable a chip. This input is used to save power by turning off the chip when it is not needed.",
            "The HOLD variable simplifies the design by allowing the designer to separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.",
            "A counter that is \"counting\" is in the process of incrementing, while a counter that is \"halted\" is not incrementing."
        ],
        "score": [
            8.618498802185059,
            -10.609020233154297,
            -10.98543930053711
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": " How is SRAM different from DRAM?\\n",
        "answers": [
            "\nSRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor",
            "One important reason for the compatibility of components in a digital system is that it helps to ensure that the system will function correctly. If the components are not compatible, the system may not work correctly, or may not work at all. Additionally, compatibility among components can help to reduce the cost of manufacturing the system, as well as the cost of maintaining and repairing the system.",
            "The state of the FSM has been split into two pieces: a {3-bit} register S and a {6-bit} timer. The timer is simply a binary counter that counts downward and produces an output of Z=1 when it reaches 0. Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.\n\nWe can thus think of transitions in the FSM on a cycle by cycle basis as consisting of two types. The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S for some number of cycles."
        ],
        "score": [
            8.202800750732422,
            -11.20176887512207,
            -11.045758247375488
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " What is the main advantage of SRAM over DRAM?\\n",
        "answers": [
            "\nThe main advantage of SRAM over DRAM is its speed. SRAM is faster than DRAM because it does not need to refresh its memory cells as often. DRAM is denser than SRAM, so it can store more data in a given area.",
            "One way to build an FSM that allows the pieces to be executed to be specified after the FSM is built is to use a state machine that has an output that can be routed to different inputs. This allows the FSM to be configured to execute different pieces of code based on the input. Another way to build an FSM that allows the pieces to be executed to be specified after the FSM is built is to use a state machine that has multiple states that can be executed in parallel. This allows the FSM to be configured to execute different pieces of code in parallel based on the input.",
            "A chip select input is an input to a chip that allows the chip to be turned on or off. This input is used to save energy by turning off the chip when it is not needed."
        ],
        "score": [
            10.11935806274414,
            -11.17972183227539,
            -10.963035583496094
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How is data stored in an SRAM cell?\\n",
        "answers": [
            "\nSRAM cells store data using a dual-inverter loop. When writing to a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input. When reading from a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.",
            "Abstraction is important when selecting a state representation because it allows the designer to select a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning.",
            "HOLD = H + S_2\n\nWhen the counter is in the HALT A state and S_2=1, HOLD = 1. This is because the counter should stop (move into a halted state without changing value) when H is true, and in the halted states, when S_2=1, the counter stops (stays in a halted state) when H+ is true."
        ],
        "score": [
            7.320613861083984,
            -11.261556625366211,
            -11.057470321655273
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": " How is a read operation performed on an SRAM cell?\\n",
        "answers": [
            "\nA read operation is performed on an SRAM cell by first bringing the bit line to an intermediate voltage level, then raising the SELECT line. This allows the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line. The resulting change in voltage is then detected by a sense amplifier.",
            "The transitions from all states are well-defined because the FSM has a timer that counts down and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.",
            "To write data into a memory element, an external circuit sends a signal to the memory indicating the address of the element to be written. The data to be written is then sent to the memory, and the memory stores the data in the element at the specified address."
        ],
        "score": [
            9.753265380859375,
            -11.211198806762695,
            -10.52234172821045
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " How is a write operation performed on an SRAM cell?', '",
        "answers": [
            "\nA write operation on an SRAM cell is performed by charging or not charging a capacitor. The capacitor is attached to a BIT line through a transistor controlled by a SELECT line. When SELECT is low, the capacitor is isolated and holds its charge. However, when SELECT is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.",
            "A tri-state buffer achieves logical disconnection by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT.",
            "When the player loses, the output of the FSM looks like it is red in both roads. This is because the controller must ensure that the lights on one or both roads are red at all times."
        ],
        "score": [
            9.68631362915039,
            -10.999737739562988,
            -11.326569557189941
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " How does SRAM work?\\n",
        "answers": [
            "\nSRAM is a type of memory that is used in computers. It is made up of a number of transistors that store data in a way that is easy for the computer to access. SRAM is faster than other types of memory, but it is also more expensive.",
            "The FSM is initialized to one of the six states in the desired loop. This ensures that the lights on both roads are not red when cars are waiting on one of the roads.",
            "The main difference between a mux and a decoder is that a mux has multiple inputs and one output, while a decoder has one input and multiple outputs. A mux is used to select one of several input signals and send it to the output, while a decoder is used to interpret an input signal and produce several output signals."
        ],
        "score": [
            4.013123989105225,
            -10.721683502197266,
            -10.889102935791016
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How is SRAM different from DRAM?\\n",
        "answers": [
            "\nSRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors (two for each inverter, and two more to connect the inverters to the bit lines), while a DRAM cell uses only a single transistor.",
            "Tri-state buffers can be used to construct larger memories by using multiple chips and connecting the external address, write enable, and chip select inputs to both chips. The data inputs and outputs are separate, with the left chip handling the high N bits of input on writes and producing the high N bits of output on reads, while the right chip handles the low N bits of input and produces the low N bits of output.",
            "The FSM is implemented using registers, counters, and a comparator. The comparator is used to implement the if statement in the program. The registers and counters are used to implement the variables idx and min in the program. The memory is used to implement the array values in the program."
        ],
        "score": [
            9.055770874023438,
            -10.248967170715332,
            -11.145119667053223
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How is a SRAM cell organized?\\n",
        "answers": [
            "\nA SRAM cell is organized as a flip-flop, with one transistor and one capacitor per bit. The capacitor stores the data bit, and the transistor controls access to the bit.",
            "The more general model of a computer is a system that can execute a set of instructions, called a program, and store the resulting data. The system has a set of registers that hold data and instructions, and a set of memory locations that store data and programs. The system also has a set of input and output devices that allow it to interact with the outside world.",
            "The DIME state is the state in which the machine is waiting for a dime to be inserted. This is the first state in which the machine accepts a coin. The machine outputs A=1 and P=0 in this state, indicating that the user has not yet paid the full price of 35 cents."
        ],
        "score": [
            7.861846446990967,
            -11.23807144165039,
            -11.13222885131836
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " How are SRAM cells combined into a bit slice?\\n",
        "answers": [
            "\nSRAM cells are combined into a bit slice by sharing bit lines and analog read and write logic. A decoder sets one cell's SELECT line high to enable a read or write operation to the cell. The chip select input CS drives the enable input of the decoder, so none of the memory cells is active when chip select is low (CS=0), and exactly one of the memory cells is active when chip select is high (CS=1).",
            "Prof. Chris Schmitz prototyped and constructed the physical elements with some help from the ECE shop. He designed the vending machine application and the FSM, while Prof. Doug Jones created the wiki documentation and the Altera Quartus portions of the lab. Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other changes to improve robustness.",
            "There are a few benefits of using a Gray code counter. First, it is easy to design and implement. Second, it is easy to check the correctness of the logic. Third, it is easy to read and understand the logic."
        ],
        "score": [
            8.418900489807129,
            -11.292535781860352,
            -11.30057430267334
        ],
        "text": "line.  When SELECT is low, the transistors disconnect the inverters from the bit lines, and the cell holds its current value until SELECT goes high again.  The actual operation of an SRAM cell is more complicated than we have described.  For example, when writing a bit, the BIT lines  can temporarily connect high voltage to ground (a short).  The  circuit must be designed carefully to minimize the power consumed during this process.  When reading a bit, the BIT lines are pre-charged halfway between high-voltage and ground, and  analog devices called sense amplifiers are used to detect the voltage changes on the BIT lines (driven by the inverter loop) as quickly as possible.  These analog design issues are outside of  the scope of our class.   A number of cells are combined into a { bit slice}, as shown to the right.  The labels along the bottom of the figure are external inputs to the  bit slice, and match the labels for the abstract   {file=part3/figs/lec18-3.eps,width=5in}   memory discussed earlier.  The  bit slice in the figure can be thought of as a {16-address}, {1-bit-addressable} memory (2^4b).  The cells in a bit slice share bit lines and analog read and write logic, which appears to the right in the figure.  Based on the ADDR input, a decoder sets one cell's SELECT line high to enable a read or write operation to the cell.    The chip select input CS drives the enable input of the decoder, so none of the memory cells is active when chip select is low (CS=0), and exactly one of the memory cells is active when chip select is high (CS=1).  Actual bit slices can contain many more cells than are shown in the  figure---more cells means less extra logic per cell, but slower memory, since longer wires have higher capacitance.  A read operation is performed as follows.  We set CS=1 and WE=0, and place the address of the cell to be read on the ADDR input. The decoder outputs a 1 on the appropriate cell's SELECT line, and the read logic reads the bit from the cell and delivers it to its Q output, which is then available on the bit  slice's {DATA-OUT} output.  For a write operation, we set CS=1 and WE=1.  We again place the address of the cell to be written on the ADDR input and set the value of the bit slice's {DATA-IN} input to the value to be written into the memory cell.  When the decoder activates the cell's SELECT line, the write logic writes the new value from its D input into the memory cell.  Later reads from that cell then produce the new value.  {{file=part3/figs/lec18-4.eps,width=6.15in}}  The outputs of the cell selection decoder can be used to control multiple bit slices, as shown in the figure above of a {2^6b} memory.  Selection between bit slices is then based on other bits from the address (ADDR).  In the figure above, a {2-to-4} decoder is used to deliver write requests to one of four bit slices, and a {4-to-1} mux is used to choose the appropriate output bit for read requests.  The {4-to-16} decoder now activates one cell in each of the four  bit slices.  For a read operation, WE=0, and the {2-to-4} decoder  is not enabled, so it outputs all 0s.  All four bit slices thus perform reads, and the desired result bit is forwarded to {DATA-OUT} by the  {4-to-1} mux.  The tri-state buffer between the mux  and {DATA-OUT} is explained in a later section.  For a write operation, exactly one of the bit slices has its WE input set to 1 by the {2-to-4} decoder. That bit slice writes the bit value delivered to all bit slices from {DATA-IN}.  The other three bit slices perform reads, but their  results are simply discarded.  The approach shown above, in which a cell is selected through a two-dimensional indexing scheme, is known as { coincident selection}.  The qualifier ``coincident'' arises from the notion that the desired cell coincides with the intersection of the active row and column outputs from the decoders.  The benefit of coincident selection is easily calculated in terms of the number of gates required for the"
    },
    {
        "question": " How is a write operation performed on a SRAM cell?']",
        "answers": [
            "\nA write operation on a SRAM cell is performed by bringing the bit line to an intermediate voltage level, then raising the SELECT line, which allows the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line. The resulting change in voltage is then detected by a sense amplifier at the end of the bit line.",
            "The internal state value for the third bit gray code counter is S_2S_1S_0=001. This is because when HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired.",
            "The stoplight controller has a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle."
        ],
        "score": [
            10.194509506225586,
            -11.12944221496582,
            -11.121566772460938
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " What is the main benefit of using a tri-state buffer?\\n",
        "answers": [
            "\nThe main benefit of using a tri-state buffer is that it allows for logical disconnection between the buffer and the output. This means that multiple memories can share the same output line without the need for additional multiplexers.",
            "One way to verify that an implementation is correct is to test it against a known set of inputs and compare the results to the expected results. If the results match, then the implementation is likely correct. Another way to verify an implementation is to examine the code and look for errors. This can be done manually or with the help of a software tool.",
            "One reason it is often best to start by selecting a representation that makes sense to a human is that it can be easier to understand and work with. Additionally, starting with a representation that makes sense to a human can help ensure that the final product is also easy to understand and use."
        ],
        "score": [
            9.820645332336426,
            -11.254538536071777,
            -11.251919746398926
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How does a tri-state buffer achieve logical disconnection?\\n",
        "answers": [
            "\nA tri-state buffer achieves logical disconnection by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT.",
            "How is multi-metric optimization useful in engineering?\n\nMulti-metric optimization is useful in engineering because it allows engineers to identify the most important metrics and to balance their design effectively according to them. Multi-metric optimization also allows engineers to get rid of as many metrics as possible so that they can focus on the most important ones.",
            "The choice of representation for an FSM's states can affect the design complexity because it determines how many bits are needed to represent each state. If more bits are needed to represent a state, then the design will be more complex."
        ],
        "score": [
            9.892096519470215,
            -11.28462028503418,
            -11.315526962280273
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " What is the function of the decoder in the memory chip?\\n",
        "answers": [
            "\nThe decoder in the memory chip is responsible for translating the memory address into the correct location in the memory chip. This is necessary to ensure that the correct data is read from or written to the correct location in the memory chip.",
            "There are 63 possible states for the PAID state. This is because the machine can only track up to 3.15 dollars worth of nickels, which is 63 nickels.",
            "The two states in the first column of the table are the states of the FSM when F=0. In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes."
        ],
        "score": [
            7.803398132324219,
            -11.28902530670166,
            -11.246318817138672
        ],
        "text": "decoders.  Decoder complexity is roughly equal to the number of outputs, as each output is a minterm and requires a unique gate to calculate it.     Fanout trees for input terms and inverted terms add relatively few gates.    Consider a 1M8b RAM chip.  The number of addresses is 2^, and the total number of memory cells is 8,388,608 (2^). One option is to use eight bit slices and a {20-to-1,048,576} decoder, or about 2^ gates.  Alternatively, we can use 8,192 bit slices of 1,024 cells.  For the second implementation, we need  two {10-to-1024} decoders, or about 2^ gates.  As chip  area is roughly proportional to the number of gates, the savings are  substantial.  Other schemes are possible as well: if we want a more  square chip area, we might choose to use 4,096 bit slices of 2,048  cells along with one {11-to-2048} decoder and one {9-to-512} decoder.  This approach requires roughly 25 more decoder gates than our previous example, but is still far superior to the eight-bit-slice implementation.  Memories are typically unclocked devices.  However, as you have seen, the circuits are highly structured, which enables engineers to cope with the complexity of sequential feedback design.  Devices used to control memories are typically clocked, and the interaction between the two can be fairly complex.     Timing diagrams for reads and writes to SRAM are shown to the right.  A write operation appears on the left.  In the first cycle, the controller raises the chip select signal and places the memory address to be written on the address inputs.  Once the memory has had time to set up the  appropriate   {file=part3/figs/lec18-6.eps,width=4in}   select lines internally, the WE input is raised, and data are placed on the data inputs.  The delay, which is specified by the memory manufacturer, is necessary to avoid writing data to the incorrect element within the memory.  The timing shown in the figure rounds this delay up to a single clock cycle, but the actual delay needed depends on the clock speed and the memory's  specification.  At some point after new data have been delivered to the memory, the write operation completes within the memory.  The time from the application of the address until the (worst-case) completion of the write operation is called the { write cycle} of the memory, and is also specified by the memory  manufacturer.  Once the write cycle has passed, the controlling logic  lowers WE, waits for the change to settle within the memory, then removes the address and lowers the chip select signal.  The reason for the delay between these signal changes is the same: to  avoid mistakenly overwriting another memory location.  A read operation is quite similar.  As shown on the right, the controlling logic places the address on the input lines and raises the chip select signal.  No races need be considered, as read operations on SRAM do not affect the stored data.  After a delay called the { read cycle}, the data can be read from the data outputs.  The address can then be removed and the chip select signal lowered.  For both reads and writes, the number of cycles required for an operation depends on a combination of the clock cycle of the controller and the cycle time of the memory.  For example, with a 25 nanosecond write cycle and a 10 nanosecond clock cycle, a write requires three cycles.  In general, the number of cycles required is given by the formula {memory cycle time}/{clock cycle time}.    {Tri-State Buffers and Combining Chips}  Recall the buffer symbol---a triangle like an inverter, but with no inversion bubble---between the mux and the {DATA-OUT}  signal of the {2^6b} memory shown earlier.  This  { tri-state buffer} serves to disconnect the memory logic  from the output line when the memory is not performing a read.    An implementation diagram for a tri-state buffer appears to the right  along with the symbolic form and a truth table.  The ``Z'' in the truth table output means  high impedance (and is sometimes"
    },
    {
        "question": " How do multiple chips allow for the construction of wider memories?\\n",
        "answers": [
            "\nMultiple chips can be used to construct wider memories by connecting the address, write enable, and chip select inputs to both chips. The data inputs and outputs are separate, with the left chip handling the high N bits of input on writes and producing the high N bits of output on reads, while the right chip handles the low N bits of input and produces the low N bits of output.",
            "The light should be red on one or both roads at all times. If a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. Finally, for fairness, the controller should alternate green lights between the two roads.",
            "DRAM is a type of capacitive storage, where a bit is stored by charging or not charging a capacitor. The capacitor is attached to a bit line through a transistor controlled by a select line. When the select line is high, the capacitor is isolated and holds its charge. However, the transistor's resistance is finite, and some charge leaks out onto the bit line. Charge also leaks into the substrate on which the transistor is constructed."
        ],
        "score": [
            9.290671348571777,
            -11.302053451538086,
            -11.233663558959961
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " Why were tri-state buffers used to reduce the number of pins needed on chips?', '\\n",
        "answers": [
            "\nTri-state buffers were used to reduce the number of pins needed on chips by combining inputs and outputs. This allowed chip designers to halve the number of pins needed.",
            "The FSM shows a yellow light before showing a red light by setting the timer to a specific value when the state of the FSM changes. This value is set by the signal F, which is calculated at the bottom of the FSM logic diagram.",
            "The value of S_1S_0 when the counter is in the COUNT A state and HOLD=1 is 01."
        ],
        "score": [
            9.278084754943848,
            -11.212614059448242,
            -10.651002883911133
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": "What is the purpose of a tri-state buffer?\\n",
        "answers": [
            "\nA tri-state buffer is a type of digital logic circuit that is used to either connect or disconnect a signal line from a power supply or ground. This is done by using two transistors, one to connect the signal line to the power supply, and the other to connect the signal line to ground. When the buffer is enabled, both transistors are turned on, and the signal line is connected to either the power supply or ground, depending on the value of the input signal. When the buffer is disabled, both transistors are turned off, and the signal line is effectively disconnected from both the power supply and ground.",
            "The input behavior of the machine is based on the type of coin inserted and the selection of an item for purchase. If the user inserts a coin that is not a nickel, the machine will reject the coin and remain in the same state. If the user inserts a nickel, the machine will transition to the next state and accept the coin. If the user selects an item for purchase, the machine will release the item if the user has inserted enough money. Otherwise, the machine will remain in the same state.",
            "A Karnaugh map is a graphical representation of a Boolean function that helps to simplify the function. The map consists of a grid of squares, each of which corresponds to a possible combination of input values. The map is used to find groups of product terms that are implicants of the function."
        ],
        "score": [
            7.755418300628662,
            -10.929323196411133,
            -10.92132568359375
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": "How does a tri-state buffer work?\\n",
        "answers": [
            "\nA tri-state buffer is a digital circuit that can be used to either connect or disconnect an output from a signal line. The buffer has three states: high, low, and high impedance. When the buffer is in the high state, the output is connected to the signal line and copies the signal from the input. When the buffer is in the low state, the output is disconnected from the signal line. When the buffer is in the high impedance state, the output is effectively disconnected from the signal line.",
            "What is the difference between SOP and POS?\n\nSOP and POS are two forms of two-level logic. In SOP, the output is true if any of the inputs are true. In POS, the output is true if all of the inputs are true.",
            "How is multi-metric optimization useful in engineering?\n\nMulti-metric optimization is useful in engineering because it allows engineers to identify the most important metrics and to balance their design effectively according to them. Multi-metric optimization also allows engineers to get rid of as many metrics as possible so that they can focus on the most important ones."
        ],
        "score": [
            8.058428764343262,
            -11.322624206542969,
            -11.040569305419922
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": "What is the benefit of using a tri-state buffer?\\n",
        "answers": [
            "\nThe benefit of using a tri-state buffer is that it allows for logical disconnection between the tri-state buffer and the output. This means that the output can be shared by more than one memory without the need for additional multiplexers. This can be useful for creating larger memories by combining multiple chips.",
            "The stoplight controller has a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.",
            "The REJECTD and REJECTQ states are used to handle user mistakes. If a user enters a second dime in the DIME state, our FSM should reject the coin. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0). The REJECTQ state is used to handle a user who inserts more than one quarter."
        ],
        "score": [
            9.471181869506836,
            -11.28022575378418,
            -9.316967010498047
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": "How can tri-state buffers be used to construct larger memories?\\n",
        "answers": [
            "\nTri-state buffers can be used to construct larger memories by using multiple chips and connecting the external address, write enable, and chip select inputs to both chips. The data inputs and outputs are separate, with the left chip handling the high N bits of input on writes and producing the high N bits of output on reads, while the right chip handles the low N bits of input and produces the low N bits of output.",
            "There are a few reasons why it is important to make implicit assumptions clear in the design process. First, if assumptions are not made clear, it can lead to confusion and misunderstanding among the team members working on the design. Second, making assumptions explicit can help to ensure that everyone is on the same page and working towards the same goal. Finally, explicit assumptions can help to prevent errors and mistakes in the design process.",
            "There is no explicit answer to this question in the passage. However, based on the information given, it seems that if the user presses the ``halt'' button while in state { COUNT A}, the system should reset to state { HALT A}."
        ],
        "score": [
            9.927484512329102,
            -11.251469612121582,
            -11.250249862670898
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": "How can tri-state buffers be used to construct wider memories?', '\\n",
        "answers": [
            "\nTri-state buffers can be used to construct wider memories by connecting the data inputs and outputs of multiple memories together. This allows the same output line to be shared by more than one memory without the need for additional multiplexers.",
            "The value of register PRICE2 is the number of nickels that have been inserted into the machine.",
            "The possible inputs for this FSM are a dime (T=0) or a quarter (T=1). The machine will reject any other input. If the user tries to purchase an item without inserting enough money first, the FSM will also reject the request."
        ],
        "score": [
            9.588068962097168,
            -11.297830581665039,
            -11.311006546020508
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How does a tri-state buffer work?\\n",
        "answers": [
            "\nA tri-state buffer is a digital circuit that can be used to either connect or disconnect a signal from a circuit. The buffer has three states: high, low, and high impedance. The high impedance state effectively disconnects the signal from the circuit. This allows multiple memories to share the same output line without the need for additional multiplexers.",
            "The 'QUARTER' state refers to the fact that the addition unit for our 4-bit examples is not closed under addition. This means that for any value of N, we can easily find two 4-bit unsigned numbers that, when added together, cannot be represented as a 4-bit unsigned number. With N=4, for example, we can add 12 (1100) and 6 (0110) to obtain 18. Since 18 is outside of the range [0,2^4-1] representable using the 4-bit unsigned representation, our representation breaks if we try to represent the sum using this representation.",
            "There are two control buttons: one for the east-west road and one for the north-south road."
        ],
        "score": [
            7.600385665893555,
            -6.814465045928955,
            -11.197298049926758
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " What is the purpose of a tri-state buffer?\\n",
        "answers": [
            "\nA tri-state buffer is used to create an electrical connection between the tri-state buffer and the output. This logical disconnection is achieved by using the outer transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT.",
            "The fourth step in the design process is to choose a particular design from the set of possible designs. In making this choice, engineers typically consider a variety of factors, including cost, performance, manufacturability, and reliability.",
            "The significance of the rising edge of the signal from the second sensor is that it indicates the presence of an object in the field of view of the sensor. This is because the rising edge of the signal indicates that the sensor has detected a change in the intensity of the light that it is receiving."
        ],
        "score": [
            7.174020767211914,
            -11.273445129394531,
            -11.158628463745117
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How can tri-state buffers be used to reduce the number of pins needed on a chip?\\n",
        "answers": [
            "\nTri-state buffers can be used to reduce the number of pins needed on a chip by combining inputs and outputs. For example, data inputs and outputs of memory can be combined into a single set of data wires, with bidirectional signals. This reduces the number of pins needed while still allowing for the same amount of data to be transferred.",
            "The choice of representation for the FSM's internal state affects the complexity of the implementation. If the states are represented as bit patterns, the transition rules must be complete. If the states are represented as abstract states, the outputs for each state must be specified.",
            "The choice of state representation affects design complexity in a few ways. First, it can affect the number of bits needed to represent the states. Second, it can affect the number of possible states that can be represented. Third, it can affect the number of transitions that need to be considered when designing the state machine."
        ],
        "score": [
            9.915674209594727,
            -11.278852462768555,
            -10.961784362792969
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How can multiple chips be used to construct a wider memory?\\n",
        "answers": [
            "\nOne way to construct a wider memory is to use multiple chips and connect the data inputs and outputs of each chip separately. This way, the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of input and produces the low N bits of output.",
            "A good state representation is important because it can help reduce the number of logic gates needed for a design, and thus save on chip area. Additionally, a good state representation can help improve the performance of a design by reducing the number of logic operations needed.",
            "One way to make use of the internal reset lines on the latches is to force each flip-flop into the 0 state. This can be done by connecting the reset lines of the latches to a 0 voltage source."
        ],
        "score": [
            9.12938404083252,
            -10.49856948852539,
            -11.13235092163086
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How can multiple chips be used to construct a larger memory?']",
        "answers": [
            "\nOne way to construct a larger memory is to use multiple chips and have one of the address bits drive a decoder that determines which of the two chips is active. The decoder is enabled with the chip select signal for the larger memory. This way, neither chip is enabled when the external CS is low.",
            "Starting with a human-based design process typically results in a design that is more efficient and effective than one that is mapped from a high-level programming language. This is because human designers can more easily optimize a design for specific metrics, such as power consumption or performance. Additionally, human designers can more easily impose constraints on a design, such as limits on chip area or minimum performance, in order to guide the design process.",
            "The most general model of a computer is the Turing machine. It is a mathematical model of a computer that can be used to simulate any other computer."
        ],
        "score": [
            9.342666625976562,
            -11.145059585571289,
            -10.900484085083008
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " What is the main difference between SRAM and DRAM?\\n",
        "answers": [
            "\nThe main difference between SRAM and DRAM is that SRAM is faster but uses more transistors, while DRAM is slower but uses fewer transistors. DRAM is also denser than SRAM, meaning that more bits can be stored in a given area.",
            "There are a few disadvantages of using an FSM. First, if the FSM is not designed carefully, it can be hard to understand and debug. Second, an FSM can use more resources than a simpler design, such as a state machine. Finally, an FSM can be more difficult to change or extend than a simpler design.",
            "The first optical sensor is used to detect the presence of a chip. If a chip is present, the sensor turns on and the chip can then be used. If no chip is present, the sensor turns off and the chip cannot be used. This sensor is used to save energy by turning off the chip when it is not needed."
        ],
        "score": [
            10.461236000061035,
            -11.14968490600586,
            -10.611125946044922
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How does a DRAM cell store a bit?\\n",
        "answers": [
            "\nA DRAM cell stores a bit by using a charged capacitor. The charge on the capacitor drains over time and must be replaced in order to maintain the stored value.",
            "Tri-state buffers can be used to construct wider memories by connecting the data inputs and outputs of multiple memories together. This allows the same output line to be shared by more than one memory without the need for additional multiplexers.",
            "One can build static RAMs using logic by using tri-state buffers. Tri-state buffers can be used to create larger memories by using multiple chips. Wider memories can also be created by using multiple chips."
        ],
        "score": [
            8.742403030395508,
            -11.22897720336914,
            -11.273191452026367
        ],
        "text": "illustrates the concept.  Each house represents a storage element and  is associated with a unique address.  {{file=part3/figs/lec18-1.eps,width=4in}}  The memories that we consider in this class have several properties in common.  These memories support two operations: { write} places a word of data into an element, and { read} retrieves a copy of a word of data from an element.  The memories are also { volatile}, which means that the data held by a memory are erased when electrical power is turned off or fails.  { Non-volatile} forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks,  and tapes, capacitive storage media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this class are { random access memories (RAMs)}, which means that the time required to access an element in the memory is independent of the element being accessed. In contrast, { serial memories} such as magnetic tape require much less time to access data near the current location in the tape than data far away from the current location.  The figure on the left above shows a generic RAM structure.  The memory contains 2^k elements of N bits each.  A {k-bit} address input, ADDR, identifies the memory element of interest for any particular operation.  The write enable input, WE, selects the operation to be performed: if WE is high, the operation is a write; if it is low, the operation is a read.  Data to be written into an element are provided through N inputs at the top, and data read from an element appear on N outputs at the bottom.  Finally, a { chip select} input, CS, functions as an enable control for the memory; when CS is low, the memory neither reads nor writes any location.  Random access memory further divides into two important types: { static RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}. SRAM employs active logic in the form of a two-inverter loop to maintain stored values.  DRAM uses a charged capacitor to store a bit; the charge drains over time and must be replaced, giving rise to the qualifier ``dynamic.''  ``Static'' thus serves only to differentiate memories with active logic elements from those with capacitive elements.  Both types are volatile, that is, both lose all data when the power supply is removed.  We consider both SRAM and DRAM  in this course, but the details of DRAM operation are beyond our scope.    {Static Random Access Memory}  Static random access memory is used for high-speed applications such as processor caches and some embedded designs.  As SRAM bit {density---the} number of bits in a given chip {area---is} significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.  The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.{Chips combining both DRAM and processor logic are available, and are used by some processor manufacturers (such  as IBM).  Research is underway to couple such logic types more efficiently by building 3D stacks of chips.}  DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs.   A diagram of an SRAM { cell} (a single bit) appears to the right.  A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line.    The cell works as follows.  When SELECT is high, the transistors connect the inverter loop to the bit lines.  When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input.  When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.     {file=part3/figs/lec18-2.eps,width=2.20in}   The value stored previously is thus copied onto the BIT line as an output, and the opposite value is placed on the"
    },
    {
        "question": " What is the purpose of a sense amplifier?\\n",
        "answers": [
            "\nA sense amplifier is a circuit that is used to read the data stored in a memory cell. The sense amplifier amplifies the small signal from the memory cell and converts it into a digital signal that can be read by the processor.",
            "SRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor",
            "Pressing the ``go'' button causes the counter to start counting."
        ],
        "score": [
            7.313288688659668,
            -11.223854064941406,
            -11.284605979919434
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " Why is a row a natural basis for the refresh cycle?\\n",
        "answers": [
            "\nA row is a natural basis for the refresh cycle because the DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An output enable input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.",
            "SRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors (two for each inverter, and two more to connect the inverters to the bit lines), while a DRAM cell uses only a single transistor.",
            "The first optical sensor is used to detect the presence of a chip. If a chip is present, the sensor turns on and the chip can then be used. If no chip is present, the sensor turns off and the chip cannot be used. This sensor is used to save energy by turning off the chip when it is not needed."
        ],
        "score": [
            9.295170783996582,
            -11.233455657958984,
            -11.2918119430542
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " What is the difference between SDRAM and DDR4 SDRAM?', '",
        "answers": [
            "\nSDRAM is a type of DRAM that is used for main memory in computers. DDR4 SDRAM is a newer type of DRAM that is faster than SDRAM.",
            "What are some of the ideas that can be used to reduce the number of metrics?\n\nSome of the ideas that can be used to reduce the number of metrics include: identifying metrics that have no practical impact in current technology, setting threshold values for other metrics to simplify reasoning about them, eliminating redundant metrics, calculating linear sums to reduce the count of metrics, and making use of the notion of Pareto optimality.",
            "The significance of the rising edge of the signal from the second sensor is that it indicates the presence of an object in the field of view of the sensor. This is because the rising edge of the signal indicates that the sensor has detected a change in the intensity of the light that it is receiving."
        ],
        "score": [
            8.667876243591309,
            -11.069713592529297,
            -9.590898513793945
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " What is DRAM used for?\\n",
        "answers": [
            "\nDRAM is used for storing data in a capacitor. The capacitor is attached to a bit line through a transistor controlled by a select line. When the select line is low, the capacitor is isolated and holds its charge. However, the transistor's resistance is finite, and some charge leaks out onto the bit line.",
            "The REJECTD and REJECTQ states are used to handle user mistakes. If a user enters a second dime in the DIME state, our FSM should reject the coin. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0). The REJECTQ state is used to handle a user who inserts more than one quarter.",
            "The four counting states are:\n\n1) State A: The timer is set to A when S_2=0. The duration of this state is A.\n\n2) State B: The timer is set to B when S_0=1. The duration of this state is B.\n\n3) State C: The timer is set to C when S_0=0. The duration of this state is C.\n\n4) State D: The timer is set to D when S_2=1. The duration of this state is D."
        ],
        "score": [
            7.4437174797058105,
            -8.166162490844727,
            -11.16665267944336
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " How does DRAM work?\\n",
        "answers": [
            "\nDRAM is a type of memory that uses capacitors to store data. When a row of DRAM is selected, all of the cells in that row are read and then written back, which refreshes the data in those cells. DRAM is said to have destructive reads because each read operation brings the voltage on the capacitor closer to the intermediate voltage level, which effectively destroys the data in the cell.",
            "The input of the FSM is a reset button R, and four buttons B_F, B_X, B_G, B_C. The FSM produces a signal P indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants. The signal R is also used to enable a decoder that generates the three individual item release outputs.",
            "Abstraction in selecting a state representation can lead to a better design by allowing the engineer to focus on the important aspects of the problem and ignoring the details that are not important. This can help to simplify the problem and make it easier to find a solution."
        ],
        "score": [
            5.301085948944092,
            -10.229475021362305,
            -10.949122428894043
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " What are the benefits to using DRAM?\\n",
        "answers": [
            "\nThe benefits of using DRAM are that it is denser than SRAM and uses fewer transistors. This makes DRAM ideal for applications where size is more important than speed.",
            "The transition diagram for the color sequencer is a loop of six states, with the two remaining states converging into the loop after a single cycle. The lights are all red in both of these states, which is necessary for safety. The timer is set only when the state S changes, and the duration of each state is calculated by analyzing the muxes.",
            "There are two optical sensors positioned at different heights on the device. The first sensor is positioned high enough above the slope that a dime passes beneath the sensor, allowing the signal T produced by the sensor to tell us whether the coin is a dime or a quarter. The second sensor is positioned so that all coins pass in front of it. The signal from the second sensor can be used as a clock for our FSM."
        ],
        "score": [
            7.922463417053223,
            -11.307613372802734,
            -11.076035499572754
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " Are there any disadvantages to using DRAM?\\n",
        "answers": [
            "\nThere are several disadvantages to using DRAM. First, DRAM is slower than SRAM. Second, DRAM is less dense than SRAM, so it requires more chip area to store the same amount of data. Finally, DRAM cells are more complex than SRAM cells, so they are more difficult to design and manufacture.",
            "A priority encoder is a circuit that takes in a number of input signals, and outputs a code that represents the highest priority input signal. The output code is usually a binary number.",
            "The decoder in the memory chip is responsible for translating the memory address into the correct location in the memory chip. This is necessary to ensure that the correct data is read from or written to the correct location in the memory chip."
        ],
        "score": [
            8.29757308959961,
            -11.11152458190918,
            -11.271008491516113
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How is DRAM different from SRAM?', '\\n\\n",
        "answers": [
            "\nDRAM is a type of memory that is slower than SRAM but denser (has more bits per chip area). DRAM cells use only a single transistor, while SRAM cells use six transistors. DRAM designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor count.",
            "The most common types of memory used in digital systems are SRAM and DRAM. SRAM is used for high-speed applications such as processor caches and some embedded designs. DRAM is used for most applications with less demanding speed requirements.",
            "The counter's implementation diagram simplifies understanding the diagram by breaking it down into smaller, more manageable pieces. By doing this, it is easier to see how the various parts of the diagram interact with each other. Additionally, it makes it easier to identify potential problems or areas for improvement."
        ],
        "score": [
            8.37027359008789,
            5.949632167816162,
            -11.07889175415039
        ],
        "text": "written ``hi-Z'').  In other words, there is effectively no electrical connection between the tri-state  buffer and the output OUT.  This logical disconnection is achieved by using the outer   {file=part3/figs/tri-state.eps,width=3in}   {cc|c} EN& IN& OUT  0& x& Z 1& 0& 0 1& 1& 1    (upper and lower) pair of transistors in the logic diagram.  When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground.  When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.  What benefit does this logical disconnection provide?  So long as only one memory's chip select input is high at any time, the same output line can be shared by more than one memory without the need for additional multiplexers.  Memory chips were often combined in this way to produce larger memories.   The figure to the right illustrates how larger memories can be constructed using multiple chips.  In the case shown, two {2^-bit}  memories are used to implement a {2^{k+1}-bit} memory. One of the address bits---in the case shown, the most significant bit---is used to drive a decoder that determines which of the two chips is  active (CS=1).  The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired.  The   {file=part3/figs/mem-larger.eps,width=4.75in}   rest of the address bits, as well as the external data inputs and write enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories.   Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.   Multiple chips can also be used to construct wider memories, as shown to the right.  In the case shown, two {2^-bit}  memories are used to implement a {2^-bit} memory. Both chips are either active or inactive at the same time, so the external  address, write enable, and chip select inputs are routed to both chips. In contrast, the data inputs and outputs are separate: the left chip handles the high N bits of input on writes and produces the high N bits of output on reads, while the right chip handles the low N bits of  input and produces the low N bits of output.   {file=part3/figs/mem-wider.eps,width=4.05in}   Historically, tri-state buffers were also used to reduce the number of pins needed on chips.  Pins have long been a scarce resource, and the  amount of data that can cross a chip's pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly as rapidly  as the number of transistors packed into a fixed area.  By combining inputs and outputs, chip designers were able to halve the number of pins needed.  For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals.  When performing a read from a memory chip, the memory chip drove the data pins with the bits being read (tri-state buffers on the memory chip were enabled).  When performing a write, other logic such as  a processor wrote the value to be stored onto the data pins (tri-state  buffers were not enabled).   {Dynamic Random Access Memory*}  Dynamic random access memory, or DRAM, is used for main memory in computers and for other applications in which size is more important than speed.  While slower than SRAM, DRAM is denser (has more bits per chip area).  A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the  bit lines), while DRAM cells use only a single transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the  benefit available from simple transistor"
    },
    {
        "question": " How does DRAM work?\\n",
        "answers": [
            "\nDRAM is a type of capacitive storage, where a bit is stored by charging or not charging a capacitor. The capacitor is attached to a bit line through a transistor controlled by a select line. When the select line is high, the capacitor is isolated and holds its charge. However, the transistor's resistance is finite, and some charge leaks out onto the bit line. Charge also leaks into the substrate on which the transistor is constructed.",
            "One could design a game controller for a logic puzzle that would have buttons corresponding to the different operations that could be performed on the logic puzzle. For example, there could be a button to add a new element to the puzzle, a button to remove an element from the puzzle, and a button to change the value of an element in the puzzle. The game controller could also have a display that would show the current state of the puzzle and the player's progress.",
            "The uninterrupted counting cycle represents the duration of each state in the FSM. This is used to calculate the duration of each state in the FSM when F=0."
        ],
        "score": [
            4.595577239990234,
            -11.071589469909668,
            -10.980341911315918
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " How is a DRAM cell refreshed?\\n",
        "answers": [
            "\nA DRAM cell is refreshed by reading the contents and writing them back with active logic. This process is necessary to avoid data loss due to charge leakage from the capacitor.",
            "There is no best state representation for an FSM. The most important factor is that the representation is complete, meaning that all states are represented and all inputs are accounted for. Other considerations include the size of the state space and the number of inputs and outputs.",
            "The desired pattern is the one that represents the number in the 2's complement representation. This is because the 2's complement representation is defined by choosing a set of integers balanced around zero from the groups. So, if we want to represent a number in the 2's complement representation, we need to choose the pattern that represents the number in that representation."
        ],
        "score": [
            8.3561372756958,
            -11.277547836303711,
            -11.268693923950195
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " What is a sense amplifier?\\n",
        "answers": [
            "\nA sense amplifier is a device that is used to amplify the signal from a sensor. It is typically used to amplify the signal from a small sensor, such as a pressure sensor, to a level that can be read by a larger device, such as a computer.",
            "The FSM handles multiple input events in a single clock cycle by first storing the input events in a queue. When the FSM is ready to process an input event, it dequeues the event from the queue and processes it.",
            "The FSM is responsible for controlling the stoplights at a busy intersection. It ensures that the lights on one or both roads are red at all times, that a road has a green light, and that the controller should show a yellow light before showing a red light."
        ],
        "score": [
            8.081411361694336,
            -11.285651206970215,
            -11.316336631774902
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What is a row address strobe (RAS)?\\n",
        "answers": [
            "\nThe RAS signal indicates when the row component of an address is available. Raising the RAS signal for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh. The row is thus a natural basis for the refresh cycle.",
            "A computer memory is a storage device for data. There are two main types of memory: SRAM and DRAM. SRAM is faster but more expensive, while DRAM is slower but cheaper. DRAM is more commonly used for main memory in computers.",
            "The main advantage of SRAM over DRAM is its speed. SRAM is faster than DRAM because it does not need to refresh its memory cells as often. DRAM is denser than SRAM, so it can store more data in a given area."
        ],
        "score": [
            3.1175358295440674,
            -11.223735809326172,
            -11.245428085327148
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": " What is a column address strobe (CAS)?']",
        "answers": [
            "\nThe column address strobe (CAS) is an input used to indicate when address components are available. It is used to control the type of operation (read or write).",
            "The FSM is initialized to one of the six states in the desired loop. This ensures that the lights on both roads are not red when cars are waiting on one of the roads.",
            "A DRAM cell stores a bit by using a charged capacitor. The charge on the capacitor drains over time and must be replaced in order to maintain the stored value."
        ],
        "score": [
            9.952840805053711,
            -11.209076881408691,
            -11.277233123779297
        ],
        "text": "count.   A diagram of a DRAM cell appears to the right.   DRAM storage is capacitive: a bit is stored by charging or not charging  a capacitor.  The capacitor is attached to a BIT line  through a transistor controlled by a SELECT line.    When SELECT is low, the capacitor is isolated and  holds its charge.  However, the transistor's resistance is finite, and some charge leaks out onto the bit line.  Charge also leaks into the substrate on which the transistor is constructed.  After some amount of time, all of the charge dissipates, and the bit is lost.  To avoid such loss, the cell must be { refreshed} periodically by reading the contents and writing them back with active logic.   {file=part3/figs/lec18-8.eps,width=1.1in}   When the SELECT line is high during a write operation, logic driving the bit line forces charge onto the capacitor or removes all charge from it.  For a read operation, the bit line is first brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line.  The resulting change in voltage is then detected by a { sense amplifier} at the end of the bit line.  A sense amp  is analogous to a marble on a mountaintop: a small push causes the marble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sense amp's output to move rapidly to a logical 0 or 1, depending on the direction of the small change.  As mentioned earlier, sense amplifiers also appear in  SRAM implementations. While not technically necessary, as they are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster.  Each read operation on a DRAM cell brings the voltage on its capacitor closer to the intermediate voltage level, in effect destroying the data in the cell.  DRAM is thus said to have { destructive reads}. To preserve data during a read, the bits must be written back into the cells after a read.  For example, the output of the sense  amplifiers can be used to drive the bit lines, rewriting the cells with the appropriate data.  At the chip level, typical DRAM inputs and outputs differ from those of SRAM.    Due to the large size and high density of DRAM, addresses are split into row and column components and provided through a common set of pins.  The DRAM stores the components in registers to support this approach.  Additional inputs, known as the { row} and { column address} {{ strobes}---RAS} and CAS, {respectively---are} used to indicate when address components are available.  As you might guess from the structure of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices---on columns rather than rows in the figures earlier in these notes, but the terminology of DRAM is a row).  Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh.  The row is thus a natural basis for the refresh cycle.  The DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An { output enable} input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present.   Timing diagrams for writes and reads on a historical DRAM implementation appear to the right.  In both cases, the row component of the address is  first applied to the address pins, then RAS is raised.  In the next cycle of the controlling logic, the column component is applied to the address pins, and CAS is raised.    For a write, as shown on the left, the WE signal and the data can   {file=part3/figs/lec18-9.eps,width=4in}   also be applied in the second cycle.  The DRAM has internal timing and control logic that prevent races from overwriting an incorrect element (remember that the row"
    },
    {
        "question": "What is the FSM for the lab?\\n",
        "answers": [
            "\nThe FSM for the lab is a coin-counting FSM that requires the use of a Moore machine model to avoid sending the servo controlling the coin's path an output pulse that was too short to enforce the FSM's decision about which way to send the coin. By adding more states to the FSM, the problem is solved.",
            "S_2 is 1 when the counter is in the HALT A state and HOLD=1. This is because when S_2=1, the counter is halted, and when HOLD=1, the counter should retain its current value.",
            "The possible outputs for this FSM are A=1 or P=1. F is a signal that is used to set the timer, and it can be either 0 or 1. If F=0, then the timer is only set when the state S changes. If F=1, then the timer is set when the state S changes or when the light on one of the roads is green."
        ],
        "score": [
            7.527739524841309,
            -11.030410766601562,
            -1.5073530673980713
        ],
        "text": "such timing guarantees are needed, the designer simply adds state to the FSM to accommodate the need, and the problem is solved.  The coin-counting FSM that we designed for our class' lab assignments, for example,  required that we use a Moore machine model to avoid sending the servo controlling the coin's path an output pulse that was too short to enforce the FSM's decision about which way to send the coin.  By adding more states to the FSM, we were able to hold the servo in place, as desired.  Why are we protecting you from the model used in practice?  First, timing issues add complexity to a topic that is complex enough  for an introductory course.  And, second, most software FSMs are Moore machines, so the abstraction is a useful one in that context, too.  In many design contexts, the timing issues implied by a Mealy model can be relatively simple to manage.  When working in a single clock domain, all of the input signals come from flip-flops in the same  domain, and are thus stable for most of the clock cycle.  Only rarely does one need to keep additional state to improve timing characteristics in these contexts.  In contrast, when interacting across clock domains, more care is sometimes needed to ensure correct behavior.  We now illustrate the state reduction benefit of the Mealy machine model with a simple example, an FSM that recognizes the  pattern of a 0 followed by a 1 on a single input and outputs a 1 when it observes the pattern.  As already mentioned, Mealy machines often require fewer flip-flops. Intuitively, the number of combinations of states and inputs is greater than the number of combinations of states alone, and allowing a function to depend on inputs reduces the number of internal states needed.    A Mealy implementation of the FSM appears on the left below, and an example timing diagram illustrating the FSM's behavior is shown on the right.  The machine shown below occupies state A when the last bit seen was a 0, and state B when the last bit seen was a 1.  Notice that the transition arcs in the state diagram are labeled with two values instead of one.  Since outputs can depend on input values as well as state, transitions in a Mealy machine are labeled with input/output combinations, while states are labeled only with their internal bits (or just their names, as shown below).  Labeling states with outputs does not make sense for a Mealy machine, since outputs may vary with inputs.  Notice that the outputs indicated on any given transition hold only until that transition is taken (at the rising clock edge), as is apparent in the timing diagram.  When inputs are asynchronous,  that is, not driven by the same clock signal, output pulses from a Mealy machine can be arbitrarily short, which can lead to problems.    {{file=part3/figs/lec17-3.eps,width=5in}}    For a Moore machine, we must create a special state in which the output is high.  Doing so requires that we split state B into two states, a state C in which the last two bits seen were 01, and a state D in which the last two bits seen were 11.  Only state C generates output 1.  State D also becomes the starting state for the new state machine.  The state diagram on the left below illustrates  the changes, using the transition diagram style that we introduced  earlier to represent Moore machines. Notice in the associated timing diagram that the output pulse lasts a full clock cycle.    {{file=part3/figs/lec17-4.eps,width=5in}}      {Summary of Part 3 of the Course}   In this short summary, we  give you lists at several levels of difficulty  of what we expect you to be able to do as a result of the last few weeks  of studying (reading, listening, doing homework, discussing your  understanding with your classmates, and so forth).  Students often find this part of the course more challenging than the earlier parts of the course.  In addition to these notes, you should read Chapters 4 and 5 of the  Patt and Patel textbook, which cover the von Neumann"
    },
    {
        "question": "What does the lab simulate?\\n",
        "answers": [
            "\nThe lab simulates two-level logic and how it can be used to map an expression into CMOS gates. It also shows how to graphically manipulate expressions to understand gate diagrams.",
            "The main difference between SRAM and DRAM is that SRAM is faster but uses more transistors, while DRAM is slower but uses fewer transistors. DRAM is also denser than SRAM, meaning that more bits can be stored in a given area.",
            "One way to design an FSM is to start with a flow chart of the program to be implemented. This flow chart can be used to identify high-level states that can be implemented in a fixed number of clock cycles. Another way to design an FSM is to start with a list of components that are needed to implement the program. This list of components can be used to identify high-level states that can be implemented in a fixed number of clock cycles."
        ],
        "score": [
            5.347430229187012,
            -11.208431243896484,
            -11.166940689086914
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the purpose of the FSM?\\n",
        "answers": [
            "\nThe FSM is a stoplight controller that ensures that the lights on one or both roads are red at all times. The controller also alternates green lights between the two roads.",
            "The fourth step in the design process is to choose a particular design from the set of possible designs. This choice is made based on a number of factors, including the objectives of the design, the constraints of the problem, and the preferences of the designer.",
            "The next state is FXGC=0101. The farmer is not on the same bank as the fox, nor as the corn, so the B_X and B_C buttons are ignored, leading to the next-state table to the right. Notice that B_G is accepted even if B_C is pressed because the farmer is not on the same bank as the corn."
        ],
        "score": [
            5.820594310760498,
            -10.47348690032959,
            -11.232343673706055
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "Who designed the vending machine application and FSM?\\n",
        "answers": [
            "\nThe vending machine application and FSM were designed by a team of engineers. The FSM was designed to control a simple vending machine that accepts US coins as payment and offers a choice of three items for sale. The machine has adjustable item prices, and the FSM needs to keep track of how much money has been inserted in order to decide whether a user can purchase one of the items.",
            "One possibility is that the FSM is not resetting properly, so it starts in an unexpected state. Another possibility is that there is a race condition in the FSM, so that it can get into an unexpected state if the inputs change too quickly.",
            "Interrupts are more powerful than exceptions because they can occur asynchronously with respect to the program. This means that the processor can check for interrupts between instructions, which allows for more flexibility in the design of the processor."
        ],
        "score": [
            8.36976432800293,
            -8.215128898620605,
            -11.250080108642578
        ],
        "text": "0& 0  EXTRA1& 011& 000& 100& x& x EXTRA2& 111& 000& 100& x& x    states, but left the output bits as don't cares.  A state transition diagram appears at the bottom of this page.  {Testing the Design}  Having a complete design on paper is a good step forward, but humans make mistakes at all stages.  How can we know that a circuit that we build in the lab correctly implements the FSM that we have outlined  in these notes?  For the lab design, we have two problems to solve.  First, we have not specified an initialization scheme for the FSM. We may want the FSM to start in one of the PAID states, but adding initialization logic to the design may mean requiring you to wire together significantly more chips.  Second, we need a sequence of inputs that manages to test that all of the next-state and output logic implementations are correct.  Testing sequential logic, including FSMs, is in general extremely difficult. In fact, large sequential systems today are generally converted into  combinational logic by using shift registers to fill the  flip-flops with a particular pattern,  executing the logic for one clock cycle, and checking that the resulting  pattern of bits in the flip-flops is correct.  This approach is called  { scan-based testing}, and is discussed in ECE 543.  You  will make use of a similar approach when you test your combinational logic in the second week of the lab, before wiring up the flip-flops.  We have designed our FSM to be easy to test (even small FSMs may be challenging) with a brute force approach.  In particular, we  identify two input sequences that together serve both to initialize and  to test a correctly implemented variant of our FSM.  Our initialization sequence forces the FSM into a specific state regardless of its initial state.  And our test sequence crosses every transition arc leaving the six valid states.    In terms of T, the coin type, we initialize the FSM with the input sequence 001.  Notice that such a sequence takes any initial  state into PAID2.  For testing, we use the input sequence 111010010001.  You should trace  this sequence, starting from PAID2, on the diagram below to see how the test sequence covers all of the possible arcs.  As we test, we need also to observe the A and P outputs in each state to check the output logic.  {{file=part3/figs/lab-diag-notes.eps,width=4.25in}}    {Finite State Machine Design Examples, Part II}  This set of notes provides several additional examples of FSM design. We first design an FSM to control a vending machine, introducing encoders and decoders as components that help us to implement our design.  We then design a game controller for a logic puzzle implemented as a children's game.  Finally, we analyze a digital FSM designed to control the stoplights at the intersection of two roads.   {Design of a Vending Machine}  For the next example, we design an FSM to control a simple vending machine.   The machine accepts {U.S. coins}{Most countries have small  bills or coins in demoninations suitable for vending machine prices, so think  about some other currency if you prefer.} as payment and offers a choice of three items for sale.  What states does such an FSM need?  The FSM needs to keep track of how much money has been inserted in order to decide whether a user can  purchase one of the items.  That information alone is enough for the simplest machine, but let's create a machine with adjustable item prices.  We can use registers to hold the item prices, which  we denote P_1, P_2, and P_3.  Technically, the item prices are also part of the internal state of the  FSM.  However,  we leave out discussion (and, indeed, methods) for setting the item prices, so no state with a given combination of prices has any  transition to a state with a different set of item prices. In other words, any given combination of item prices induces a subset  of states that operate independently of the subset induced by a distinct  combination of item prices.  By abstracting"
    },
    {
        "question": "Who created the wiki documentation and the Altera Quartus portions of the lab?', \"",
        "answers": [
            "\nProf. Volodymyr Kindratenko together with Prof. Geoffrey Herman created both the wiki documentation and the Altera Quartus portions of the lab. Prof. Kindratenko also helped to scale the design in a way that made it possible to deliver to the over 400 students entering ECE every semester.",
            "Pressing the ``go'' button causes the counter to start counting.",
            "Tri-state buffers were used to reduce the number of pins needed on chips by combining inputs and outputs. This allowed chip designers to halve the number of pins needed."
        ],
        "score": [
            9.825713157653809,
            -11.305139541625977,
            -11.316143035888672
        ],
        "text": "and column addresses have to be stored in registers).  The DRAM again specifies a write cycle, after which the operation is guaranteed to be complete.  In order, the WE, CAS, and RAS signals are then lowered.    For a read operation, the output enable signal, OE, is raised after CAS is raised.  The DATA pins, which should be floating (in other words, not driven by any logic), are then driven by the DRAM.  After the  read cycle, valid data appear on the DATA pins, and OE, CAS, and RAS are lowered in order after the data are read.  Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integrated onto the chips themselves.  As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor's 8Gb ({2^b}, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.  The ability to synchronize to an external clock has become prevalent in  the industry, leading to the somewhat confusing term SDRAM, which stands  for { synchronous DRAM}.  The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize  accesses to the external clock without the need for additional  logic.  The clock provided to the Micron chip just mentioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of the clock (hence the name DDR, or { double data rate}).    In addition to row and column components of the address, these chips further separate cells into { banks} and groups of banks.  These allow a user to exploit parallelism by starting reads or writes to separate banks at the same time, thus improving the speed at which data can move in and out of the memory.  For the {2^b} version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.    DRAM implementations provide interfaces for specifying refresh operations in addition to reads and writes. Managing refresh timing and execution is generally left to an external DRAM controller.  For the Micron chip, refresh commands must be issued every  7.8 microseconds at normal temperatures.  Each command refreshes about 2^ cells, so 8,192 commands refresh the whole chip in less than 64 milliseconds.  Alternatively, the chip can handle refresh on-chip in order to maintain memory contents when the rest of the system is  powered down.    {Design of the Finite State Machine for the Lab}  This set of notes explains the process that Prof. Doug Jones used to develop the FSM for the lab.  The lab simulates a vending machine mechanism for automatically  identifying coins (dimes and quarters only), tracking the amount  of money entered by the user, accepting or rejecting  coins, and emitting a signal when a total of 35 cents has been  accepted.  In the lab, we will only drive a light with  the ``paid in full'' signal.    Sorry, neither candy nor Dew will be distributed!  Prof. Doug Jones designed the vending machine application and the FSM, while Prof. Chris Schmitz prototyped and constructed the physical elements  with some help from the ECE shop.  Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created  both the wiki documentation and the Altera Quartus portions of the lab (the latter were based on earlier Mentor Graphics work by Prof. Herman).  Prof. Kindratenko also helped to scale the design  in a way that made it possible to deliver to the over 400 students entering ECE every semester.    Prof. Juan Jos'e Jaramillo later identified common failure modes, including variability caused by sunshine through  the windows in ECEB,{No wonder people say that engineers hate  sunlight!} and made some changes to improve robustness.  He also created the PowerPoint slides that are typically used to describe the lab in lecture.  Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other"
    },
    {
        "question": "What is the FSM for the lab?\\n",
        "answers": [
            "\nThe FSM for the lab is a coin-counting FSM that requires the use of a Moore machine model to avoid sending the servo controlling the coin's path an output pulse that was too short to enforce the FSM's decision about which way to send the coin. By adding more states to the FSM, the servo can be held in place, as desired.",
            "The vertical lines carrying the current state values and their inverses back to the next state logic inputs are ordered from left to right (on the left side of the figure) as {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.",
            "The initial state of the three flip-flops in our implementation is 0s and 1s. We can use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1)."
        ],
        "score": [
            7.534551620483398,
            -11.051044464111328,
            -6.275094032287598
        ],
        "text": "such timing guarantees are needed, the designer simply adds state to the FSM to accommodate the need, and the problem is solved.  The coin-counting FSM that we designed for our class' lab assignments, for example,  required that we use a Moore machine model to avoid sending the servo controlling the coin's path an output pulse that was too short to enforce the FSM's decision about which way to send the coin.  By adding more states to the FSM, we were able to hold the servo in place, as desired.  Why are we protecting you from the model used in practice?  First, timing issues add complexity to a topic that is complex enough  for an introductory course.  And, second, most software FSMs are Moore machines, so the abstraction is a useful one in that context, too.  In many design contexts, the timing issues implied by a Mealy model can be relatively simple to manage.  When working in a single clock domain, all of the input signals come from flip-flops in the same  domain, and are thus stable for most of the clock cycle.  Only rarely does one need to keep additional state to improve timing characteristics in these contexts.  In contrast, when interacting across clock domains, more care is sometimes needed to ensure correct behavior.  We now illustrate the state reduction benefit of the Mealy machine model with a simple example, an FSM that recognizes the  pattern of a 0 followed by a 1 on a single input and outputs a 1 when it observes the pattern.  As already mentioned, Mealy machines often require fewer flip-flops. Intuitively, the number of combinations of states and inputs is greater than the number of combinations of states alone, and allowing a function to depend on inputs reduces the number of internal states needed.    A Mealy implementation of the FSM appears on the left below, and an example timing diagram illustrating the FSM's behavior is shown on the right.  The machine shown below occupies state A when the last bit seen was a 0, and state B when the last bit seen was a 1.  Notice that the transition arcs in the state diagram are labeled with two values instead of one.  Since outputs can depend on input values as well as state, transitions in a Mealy machine are labeled with input/output combinations, while states are labeled only with their internal bits (or just their names, as shown below).  Labeling states with outputs does not make sense for a Mealy machine, since outputs may vary with inputs.  Notice that the outputs indicated on any given transition hold only until that transition is taken (at the rising clock edge), as is apparent in the timing diagram.  When inputs are asynchronous,  that is, not driven by the same clock signal, output pulses from a Mealy machine can be arbitrarily short, which can lead to problems.    {{file=part3/figs/lec17-3.eps,width=5in}}    For a Moore machine, we must create a special state in which the output is high.  Doing so requires that we split state B into two states, a state C in which the last two bits seen were 01, and a state D in which the last two bits seen were 11.  Only state C generates output 1.  State D also becomes the starting state for the new state machine.  The state diagram on the left below illustrates  the changes, using the transition diagram style that we introduced  earlier to represent Moore machines. Notice in the associated timing diagram that the output pulse lasts a full clock cycle.    {{file=part3/figs/lec17-4.eps,width=5in}}      {Summary of Part 3 of the Course}   In this short summary, we  give you lists at several levels of difficulty  of what we expect you to be able to do as a result of the last few weeks  of studying (reading, listening, doing homework, discussing your  understanding with your classmates, and so forth).  Students often find this part of the course more challenging than the earlier parts of the course.  In addition to these notes, you should read Chapters 4 and 5 of the  Patt and Patel textbook, which cover the von Neumann"
    },
    {
        "question": "How was the vending machine application and FSM designed?\\n",
        "answers": [
            "\nThe vending machine application and FSM were designed by connecting the buttons B_1, B_2, and B_3 to the priority encoder (and feeding 0 into the fourth input), which produces a signal P indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants. The signal R is also used to enable a decoder that generates the three individual item release outputs. The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next cycle.",
            "A multiplexer is a digital logic block that allows one digital signal to select which of several others is forwarded to an output. The simplest form of the multiplexer is the 2-to-1 multiplexer, which has two inputs from the left and one from the top. The top input allows us to choose which of the left inputs is forwarded to the output.",
            "There are a few possible ways to answer this question. One way would be to say that if the user presses the ``halt'' button while in state { HALT A}, the system should reset to state { COUNT A}. Another way to answer this question would be to say that if the user presses the ``halt'' button while in state { HALT A}, the system should remain in state { HALT A}."
        ],
        "score": [
            8.840094566345215,
            -11.181662559509277,
            -11.209364891052246
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": "Who created the wiki documentation and the Altera Quartus portions of the lab?\\n",
        "answers": [
            "\nProf. Volodymyr Kindratenko together with Prof. Geoffrey Herman created the wiki documentation and the Altera Quartus portions of the lab.",
            "The next-state logic for a three-bit Gray code counter is as follows:\n\nS_2^+ = S_2 + S_0\nS_1^+ = S_2\nS_0^+ = S_2 S_1",
            "A RAM is a type of memory that is used to store data. It is a volatile memory, which means that the data stored in the memory is erased when the power is turned off or fails. RAM is a random access memory, which means that the time required to access an element in the memory is independent of the element being accessed."
        ],
        "score": [
            9.566850662231445,
            -11.31324577331543,
            -11.23431396484375
        ],
        "text": "and column addresses have to be stored in registers).  The DRAM again specifies a write cycle, after which the operation is guaranteed to be complete.  In order, the WE, CAS, and RAS signals are then lowered.    For a read operation, the output enable signal, OE, is raised after CAS is raised.  The DATA pins, which should be floating (in other words, not driven by any logic), are then driven by the DRAM.  After the  read cycle, valid data appear on the DATA pins, and OE, CAS, and RAS are lowered in order after the data are read.  Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integrated onto the chips themselves.  As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor's 8Gb ({2^b}, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.  The ability to synchronize to an external clock has become prevalent in  the industry, leading to the somewhat confusing term SDRAM, which stands  for { synchronous DRAM}.  The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize  accesses to the external clock without the need for additional  logic.  The clock provided to the Micron chip just mentioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of the clock (hence the name DDR, or { double data rate}).    In addition to row and column components of the address, these chips further separate cells into { banks} and groups of banks.  These allow a user to exploit parallelism by starting reads or writes to separate banks at the same time, thus improving the speed at which data can move in and out of the memory.  For the {2^b} version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.    DRAM implementations provide interfaces for specifying refresh operations in addition to reads and writes. Managing refresh timing and execution is generally left to an external DRAM controller.  For the Micron chip, refresh commands must be issued every  7.8 microseconds at normal temperatures.  Each command refreshes about 2^ cells, so 8,192 commands refresh the whole chip in less than 64 milliseconds.  Alternatively, the chip can handle refresh on-chip in order to maintain memory contents when the rest of the system is  powered down.    {Design of the Finite State Machine for the Lab}  This set of notes explains the process that Prof. Doug Jones used to develop the FSM for the lab.  The lab simulates a vending machine mechanism for automatically  identifying coins (dimes and quarters only), tracking the amount  of money entered by the user, accepting or rejecting  coins, and emitting a signal when a total of 35 cents has been  accepted.  In the lab, we will only drive a light with  the ``paid in full'' signal.    Sorry, neither candy nor Dew will be distributed!  Prof. Doug Jones designed the vending machine application and the FSM, while Prof. Chris Schmitz prototyped and constructed the physical elements  with some help from the ECE shop.  Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created  both the wiki documentation and the Altera Quartus portions of the lab (the latter were based on earlier Mentor Graphics work by Prof. Herman).  Prof. Kindratenko also helped to scale the design  in a way that made it possible to deliver to the over 400 students entering ECE every semester.    Prof. Juan Jos'e Jaramillo later identified common failure modes, including variability caused by sunshine through  the windows in ECEB,{No wonder people say that engineers hate  sunlight!} and made some changes to improve robustness.  He also created the PowerPoint slides that are typically used to describe the lab in lecture.  Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other"
    },
    {
        "question": "What did Juan Jos'e Jaramillo do to improve the lab?\\n",
        "answers": [
            "\nJuan Jos'e Jaramillo identified common failure modes, including variability caused by sunshine through the windows in ECEB, and made some changes to improve robustness. He also created the PowerPoint slides that are typically used to describe the lab in lecture.",
            "A computer memory is a storage device for data. There are two main types of memory: SRAM and DRAM. SRAM is faster but more expensive, while DRAM is slower but cheaper. DRAM is more commonly used for main memory in computers.",
            "A decoder is used in an FSM design to generate the release signals for the vending machine by connecting the signal S produced by the priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input. The outputs D_1, D_2, and D_3 then correspond to the individual item release signals R_1, R_2, and R_3 for our vending machine."
        ],
        "score": [
            7.45819091796875,
            -11.284574508666992,
            -11.247236251831055
        ],
        "text": "and column addresses have to be stored in registers).  The DRAM again specifies a write cycle, after which the operation is guaranteed to be complete.  In order, the WE, CAS, and RAS signals are then lowered.    For a read operation, the output enable signal, OE, is raised after CAS is raised.  The DATA pins, which should be floating (in other words, not driven by any logic), are then driven by the DRAM.  After the  read cycle, valid data appear on the DATA pins, and OE, CAS, and RAS are lowered in order after the data are read.  Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integrated onto the chips themselves.  As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor's 8Gb ({2^b}, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.  The ability to synchronize to an external clock has become prevalent in  the industry, leading to the somewhat confusing term SDRAM, which stands  for { synchronous DRAM}.  The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize  accesses to the external clock without the need for additional  logic.  The clock provided to the Micron chip just mentioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of the clock (hence the name DDR, or { double data rate}).    In addition to row and column components of the address, these chips further separate cells into { banks} and groups of banks.  These allow a user to exploit parallelism by starting reads or writes to separate banks at the same time, thus improving the speed at which data can move in and out of the memory.  For the {2^b} version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.    DRAM implementations provide interfaces for specifying refresh operations in addition to reads and writes. Managing refresh timing and execution is generally left to an external DRAM controller.  For the Micron chip, refresh commands must be issued every  7.8 microseconds at normal temperatures.  Each command refreshes about 2^ cells, so 8,192 commands refresh the whole chip in less than 64 milliseconds.  Alternatively, the chip can handle refresh on-chip in order to maintain memory contents when the rest of the system is  powered down.    {Design of the Finite State Machine for the Lab}  This set of notes explains the process that Prof. Doug Jones used to develop the FSM for the lab.  The lab simulates a vending machine mechanism for automatically  identifying coins (dimes and quarters only), tracking the amount  of money entered by the user, accepting or rejecting  coins, and emitting a signal when a total of 35 cents has been  accepted.  In the lab, we will only drive a light with  the ``paid in full'' signal.    Sorry, neither candy nor Dew will be distributed!  Prof. Doug Jones designed the vending machine application and the FSM, while Prof. Chris Schmitz prototyped and constructed the physical elements  with some help from the ECE shop.  Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created  both the wiki documentation and the Altera Quartus portions of the lab (the latter were based on earlier Mentor Graphics work by Prof. Herman).  Prof. Kindratenko also helped to scale the design  in a way that made it possible to deliver to the over 400 students entering ECE every semester.    Prof. Juan Jos'e Jaramillo later identified common failure modes, including variability caused by sunshine through  the windows in ECEB,{No wonder people say that engineers hate  sunlight!} and made some changes to improve robustness.  He also created the PowerPoint slides that are typically used to describe the lab in lecture.  Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other"
    },
    {
        "question": "Who developed an inexpensive and portable replacement for the physical ``vending machine'' systems?\", \"\\n\\n",
        "answers": [
            "\nIn the mid-1980's, the VAX architecture dominated the workstation and minicomputer markets, which included most universities.  Digital Equipment Corporation, the creator of the VAX, was second only to IBM in terms of computer sales.  VAXen, as the machines were called, used microprogrammed control units and supported numerous addressing modes as well as complex instructions ranging from ``square root'' to ``find roots of polynomial equation.''  The impact of increasingly dense integrated circuit technology had begun to have its effect, however, and in view of increasing processor clock speeds, more and more programmers were using high-level languages rather than writing assembly code.  Although assembly programmers often made use of the complex VAX instructions, compilers were usually unable to recognize the corresponding high-level language constructs and thus were unable to make use of the instructions.\n\nThe VAX architecture was developed by Digital Equipment Corporation in the mid-1980's. The architecture dominated the workstation and minicomputer markets at the time and was second only to IBM in terms of computer sales. The VAX architecture used microprogrammed control units and supported numerous addressing modes as well as complex instructions.",
            "SRAM cells are combined into a bit slice by sharing bit lines and analog read and write logic. The bit slice in the figure can be thought of as a 16-address, 1-bit-addressable memory (2^4b). The cells in a bit slice share bit lines and analog read and write logic, which appears to the right in the figure.",
            "The output behavior is that the add3 procedure takes three integers as arguments, adds them together, and returns the sum. The procedure is called with the constants 10, 20, and 30, and the result is printed. By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30."
        ],
        "score": [
            -7.719306945800781,
            -11.16616439819336,
            -11.187819480895996
        ],
        "text": "architectures expose it as a general-purpose register, although its distinct role in the implementation keeps such a model from becoming as common as the use of a general-purpose register for the SP.  The { processor status register (PSR)}, also known as the { processor status word (PSW)}, contains all status bits as well as a mode bit indicating whether the processor is operating in user mode or privileged (operating system) mode.  Having a register with this information allows more general access than is possible solely through the use of control flow instructions.  The { zero register} appears in modern architectures of the RISC variety (defined in the next section of these notes).  The register is read-only and serves both as a useful constant and as a destination for operations performed only for their side-effects (for example, setting status bits).  The availability of a zero register also allows certain opcodes to serve double duty.  A register-to-register add instruction becomes a register move instruction when one source operand is zero. Similarly, an immediate add instruction becomes an immediate load instruction when one source operand is zero.   {Reduced Instruction Set Computers*}  By the mid-1980's, the VAX architecture dominated the workstation and minicomputer markets, which included most universities.  Digital Equipment Corporation, the creator of the VAX, was second only to IBM in terms of computer sales.  VAXen, as the machines were called, used microprogrammed control units and supported numerous addressing modes as well as complex instructions ranging from ``square root'' to ``find roots of polynomial equation.''  The impact of increasingly dense integrated circuit technology had begun to have its effect, however, and in view of increasing processor clock speeds, more and more programmers were using high-level languages rather than writing assembly code.  Although assembly programmers often made use of the complex VAX instructions, compilers were usually unable to recognize the corresponding high-level language constructs and thus were unable to make use of the instructions.  Increasing density also led to rapid growth in memory sizes, to the point that researchers began to question the need for variable-length instructions.  Recall that variable-length instructions allow shorter codes by providing more efficient instruction encodings.  With the trend toward larger memories, code length was less important.  The performance advantage of fixed-length instructions, which simplifies the datapath and enables pipelining, on the other hand, was  attractive.  Researchers leveraged these ideas, which had been floating around the research community (and had appeared in some commercial architectures) to create { reduced instruction set computers}, or { RISC} machines.  The competing VAXen were labeled { CISC} machines, which stands for { complex instruction set computers}.  RISC machines employ fixed-length instructions and a load-store architecture, allowing only a few addressing modes and small offsets. This combination of design decisions enables deep pipelines and multiple instruction issues in a single cycle (termed superscalar implementations), and for years, RISC machines were viewed by many researchers as the proper design for future ISAs.  However, companies such as Intel soon learned to pipeline microoperations after decoding instructions, and CISC architectures now offer competitive if not superior performance in comparison with RISC machines.  The VAXen are dead, of course,{Unless you talk with customer support employees, for whom no machine ever dies.}  having been replaced by the Alpha, which in turn fell to x86, which is now struggling with ARM to enter the mobile market.     {Procedure and System Calls*}  A { procedure} is a sequence of instructions that executes a particular task.  Procedures are used as building blocks for multiple, larger tasks.  The concept of a procedure is fundamental to"
    },
    {
        "question": " How did Prof. Doug Jones design the vending machine application and the FSM?\\n",
        "answers": [
            "\nProf. Doug Jones designed the vending machine application and the FSM by first identifying the different states that the machine could be in. He then created a state diagram that showed how the machine would transition from one state to another. Finally, he implemented the FSM in hardware using the Altera Quartus software.",
            "The input of the FSM is a reset button R, and four buttons B_F, B_X, B_G, B_C. The FSM produces a signal P indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants. The signal R is also used to enable a decoder that generates the three individual item release outputs.",
            "The use of S_2 separates out the logic for deciding whether to advance the counter by allowing the counter to be in a halted state (S_2=1) or a counting state (S_2=0). When HOLD=1, the counter retains its state, and when HOLD=0, it counts."
        ],
        "score": [
            9.897843360900879,
            -3.361534595489502,
            -11.208080291748047
        ],
        "text": "and column addresses have to be stored in registers).  The DRAM again specifies a write cycle, after which the operation is guaranteed to be complete.  In order, the WE, CAS, and RAS signals are then lowered.    For a read operation, the output enable signal, OE, is raised after CAS is raised.  The DATA pins, which should be floating (in other words, not driven by any logic), are then driven by the DRAM.  After the  read cycle, valid data appear on the DATA pins, and OE, CAS, and RAS are lowered in order after the data are read.  Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integrated onto the chips themselves.  As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor's 8Gb ({2^b}, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.  The ability to synchronize to an external clock has become prevalent in  the industry, leading to the somewhat confusing term SDRAM, which stands  for { synchronous DRAM}.  The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize  accesses to the external clock without the need for additional  logic.  The clock provided to the Micron chip just mentioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of the clock (hence the name DDR, or { double data rate}).    In addition to row and column components of the address, these chips further separate cells into { banks} and groups of banks.  These allow a user to exploit parallelism by starting reads or writes to separate banks at the same time, thus improving the speed at which data can move in and out of the memory.  For the {2^b} version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.    DRAM implementations provide interfaces for specifying refresh operations in addition to reads and writes. Managing refresh timing and execution is generally left to an external DRAM controller.  For the Micron chip, refresh commands must be issued every  7.8 microseconds at normal temperatures.  Each command refreshes about 2^ cells, so 8,192 commands refresh the whole chip in less than 64 milliseconds.  Alternatively, the chip can handle refresh on-chip in order to maintain memory contents when the rest of the system is  powered down.    {Design of the Finite State Machine for the Lab}  This set of notes explains the process that Prof. Doug Jones used to develop the FSM for the lab.  The lab simulates a vending machine mechanism for automatically  identifying coins (dimes and quarters only), tracking the amount  of money entered by the user, accepting or rejecting  coins, and emitting a signal when a total of 35 cents has been  accepted.  In the lab, we will only drive a light with  the ``paid in full'' signal.    Sorry, neither candy nor Dew will be distributed!  Prof. Doug Jones designed the vending machine application and the FSM, while Prof. Chris Schmitz prototyped and constructed the physical elements  with some help from the ECE shop.  Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created  both the wiki documentation and the Altera Quartus portions of the lab (the latter were based on earlier Mentor Graphics work by Prof. Herman).  Prof. Kindratenko also helped to scale the design  in a way that made it possible to deliver to the over 400 students entering ECE every semester.    Prof. Juan Jos'e Jaramillo later identified common failure modes, including variability caused by sunshine through  the windows in ECEB,{No wonder people say that engineers hate  sunlight!} and made some changes to improve robustness.  He also created the PowerPoint slides that are typically used to describe the lab in lecture.  Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other"
    },
    {
        "question": " How did Prof. Chris Schmitz prototype and construct the physical elements with some help from the ECE shop?\\n",
        "answers": [
            "\nProf. Chris Schmitz prototyped and constructed the physical elements with some help from the ECE shop. He designed the vending machine application and the FSM, while Prof. Doug Jones created the wiki documentation and the Altera Quartus portions of the lab. Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other changes to improve robustness.",
            "The possible outputs for this FSM are A=1 or P=1. F is a signal that is used to set the timer, and it can be either 0 or 1. If F=0, then the timer is only set when the state S changes. If F=1, then the timer is set when the state S changes or when the light on one of the roads is green.",
            "The color mapping for a three-bit RGB encoding system is that the colors are produced by unique bit patterns. The states form a loop, and are marked with the internal state value S_2S_1S_0 and the output RGB."
        ],
        "score": [
            8.918289184570312,
            -11.291004180908203,
            -11.321556091308594
        ],
        "text": "and column addresses have to be stored in registers).  The DRAM again specifies a write cycle, after which the operation is guaranteed to be complete.  In order, the WE, CAS, and RAS signals are then lowered.    For a read operation, the output enable signal, OE, is raised after CAS is raised.  The DATA pins, which should be floating (in other words, not driven by any logic), are then driven by the DRAM.  After the  read cycle, valid data appear on the DATA pins, and OE, CAS, and RAS are lowered in order after the data are read.  Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integrated onto the chips themselves.  As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor's 8Gb ({2^b}, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.  The ability to synchronize to an external clock has become prevalent in  the industry, leading to the somewhat confusing term SDRAM, which stands  for { synchronous DRAM}.  The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize  accesses to the external clock without the need for additional  logic.  The clock provided to the Micron chip just mentioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of the clock (hence the name DDR, or { double data rate}).    In addition to row and column components of the address, these chips further separate cells into { banks} and groups of banks.  These allow a user to exploit parallelism by starting reads or writes to separate banks at the same time, thus improving the speed at which data can move in and out of the memory.  For the {2^b} version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.    DRAM implementations provide interfaces for specifying refresh operations in addition to reads and writes. Managing refresh timing and execution is generally left to an external DRAM controller.  For the Micron chip, refresh commands must be issued every  7.8 microseconds at normal temperatures.  Each command refreshes about 2^ cells, so 8,192 commands refresh the whole chip in less than 64 milliseconds.  Alternatively, the chip can handle refresh on-chip in order to maintain memory contents when the rest of the system is  powered down.    {Design of the Finite State Machine for the Lab}  This set of notes explains the process that Prof. Doug Jones used to develop the FSM for the lab.  The lab simulates a vending machine mechanism for automatically  identifying coins (dimes and quarters only), tracking the amount  of money entered by the user, accepting or rejecting  coins, and emitting a signal when a total of 35 cents has been  accepted.  In the lab, we will only drive a light with  the ``paid in full'' signal.    Sorry, neither candy nor Dew will be distributed!  Prof. Doug Jones designed the vending machine application and the FSM, while Prof. Chris Schmitz prototyped and constructed the physical elements  with some help from the ECE shop.  Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created  both the wiki documentation and the Altera Quartus portions of the lab (the latter were based on earlier Mentor Graphics work by Prof. Herman).  Prof. Kindratenko also helped to scale the design  in a way that made it possible to deliver to the over 400 students entering ECE every semester.    Prof. Juan Jos'e Jaramillo later identified common failure modes, including variability caused by sunshine through  the windows in ECEB,{No wonder people say that engineers hate  sunlight!} and made some changes to improve robustness.  He also created the PowerPoint slides that are typically used to describe the lab in lecture.  Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other"
    },
    {
        "question": " How did Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman create the wiki documentation and the Altera Quartus portions of the lab?\\n",
        "answers": [
            "\nProf. Volodymyr Kindratenko together with Prof. Geoffrey Herman created the wiki documentation and the Altera Quartus portions of the lab. They based the lab on earlier Mentor Graphics work by Prof. Herman. Prof. Kindratenko also helped to scale the design in a way that made it possible to deliver to the over 400 students entering ECE every semester.",
            "One important reason to consider all possible input conditions from all possible states is that it can help ensure that a design is correct. For example, if a design only ever receives input values that are within a certain range, it is possible that the design is not actually correct for all input values. Considering all possible input conditions can help catch errors like this.\n\nAnother reason to consider all possible input conditions is that it can help optimize a design. For example, if a design only ever receives input values that are within a certain range, it is possible to simplify the design or make it more efficient. Considering all possible input conditions can help identify opportunities for optimization.",
            "The final state table and state transition diagram appear as follows: \n\nThe final state table lists the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The table also includes the states outside of the desired loop. The lights are all red in both of these states, which is necessary for safety.\n\nThe state transition diagram shows the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The diagram also shows the duration of each state."
        ],
        "score": [
            9.447528839111328,
            -11.296828269958496,
            -11.203142166137695
        ],
        "text": "and column addresses have to be stored in registers).  The DRAM again specifies a write cycle, after which the operation is guaranteed to be complete.  In order, the WE, CAS, and RAS signals are then lowered.    For a read operation, the output enable signal, OE, is raised after CAS is raised.  The DATA pins, which should be floating (in other words, not driven by any logic), are then driven by the DRAM.  After the  read cycle, valid data appear on the DATA pins, and OE, CAS, and RAS are lowered in order after the data are read.  Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integrated onto the chips themselves.  As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor's 8Gb ({2^b}, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.  The ability to synchronize to an external clock has become prevalent in  the industry, leading to the somewhat confusing term SDRAM, which stands  for { synchronous DRAM}.  The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize  accesses to the external clock without the need for additional  logic.  The clock provided to the Micron chip just mentioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of the clock (hence the name DDR, or { double data rate}).    In addition to row and column components of the address, these chips further separate cells into { banks} and groups of banks.  These allow a user to exploit parallelism by starting reads or writes to separate banks at the same time, thus improving the speed at which data can move in and out of the memory.  For the {2^b} version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.    DRAM implementations provide interfaces for specifying refresh operations in addition to reads and writes. Managing refresh timing and execution is generally left to an external DRAM controller.  For the Micron chip, refresh commands must be issued every  7.8 microseconds at normal temperatures.  Each command refreshes about 2^ cells, so 8,192 commands refresh the whole chip in less than 64 milliseconds.  Alternatively, the chip can handle refresh on-chip in order to maintain memory contents when the rest of the system is  powered down.    {Design of the Finite State Machine for the Lab}  This set of notes explains the process that Prof. Doug Jones used to develop the FSM for the lab.  The lab simulates a vending machine mechanism for automatically  identifying coins (dimes and quarters only), tracking the amount  of money entered by the user, accepting or rejecting  coins, and emitting a signal when a total of 35 cents has been  accepted.  In the lab, we will only drive a light with  the ``paid in full'' signal.    Sorry, neither candy nor Dew will be distributed!  Prof. Doug Jones designed the vending machine application and the FSM, while Prof. Chris Schmitz prototyped and constructed the physical elements  with some help from the ECE shop.  Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created  both the wiki documentation and the Altera Quartus portions of the lab (the latter were based on earlier Mentor Graphics work by Prof. Herman).  Prof. Kindratenko also helped to scale the design  in a way that made it possible to deliver to the over 400 students entering ECE every semester.    Prof. Juan Jos'e Jaramillo later identified common failure modes, including variability caused by sunshine through  the windows in ECEB,{No wonder people say that engineers hate  sunlight!} and made some changes to improve robustness.  He also created the PowerPoint slides that are typically used to describe the lab in lecture.  Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other"
    },
    {
        "question": " How did Prof. Kindratenko also help to scale the design in a way that made it possible to deliver to the over 400 students entering ECE every semester?\\n",
        "answers": [
            "\nProf. Kindratenko helped to scale the design in a way that made it possible to deliver to the over 400 students entering ECE every semester by creating both the wiki documentation and the Altera Quartus portions of the lab. He also helped to improve the robustness of the lab by identifying common failure modes and making some changes to the design.",
            "The light should be red on one or both roads at all times. If a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. Finally, for fairness, the controller should alternate green lights between the two roads.",
            "A sense amplifier is a circuit that is used to read the data stored in a memory cell. The sense amplifier amplifies the small signal from the memory cell and converts it into a digital signal that can be read by the processor."
        ],
        "score": [
            9.993230819702148,
            -11.28414535522461,
            -11.197205543518066
        ],
        "text": "and column addresses have to be stored in registers).  The DRAM again specifies a write cycle, after which the operation is guaranteed to be complete.  In order, the WE, CAS, and RAS signals are then lowered.    For a read operation, the output enable signal, OE, is raised after CAS is raised.  The DATA pins, which should be floating (in other words, not driven by any logic), are then driven by the DRAM.  After the  read cycle, valid data appear on the DATA pins, and OE, CAS, and RAS are lowered in order after the data are read.  Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integrated onto the chips themselves.  As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor's 8Gb ({2^b}, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.  The ability to synchronize to an external clock has become prevalent in  the industry, leading to the somewhat confusing term SDRAM, which stands  for { synchronous DRAM}.  The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize  accesses to the external clock without the need for additional  logic.  The clock provided to the Micron chip just mentioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of the clock (hence the name DDR, or { double data rate}).    In addition to row and column components of the address, these chips further separate cells into { banks} and groups of banks.  These allow a user to exploit parallelism by starting reads or writes to separate banks at the same time, thus improving the speed at which data can move in and out of the memory.  For the {2^b} version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.    DRAM implementations provide interfaces for specifying refresh operations in addition to reads and writes. Managing refresh timing and execution is generally left to an external DRAM controller.  For the Micron chip, refresh commands must be issued every  7.8 microseconds at normal temperatures.  Each command refreshes about 2^ cells, so 8,192 commands refresh the whole chip in less than 64 milliseconds.  Alternatively, the chip can handle refresh on-chip in order to maintain memory contents when the rest of the system is  powered down.    {Design of the Finite State Machine for the Lab}  This set of notes explains the process that Prof. Doug Jones used to develop the FSM for the lab.  The lab simulates a vending machine mechanism for automatically  identifying coins (dimes and quarters only), tracking the amount  of money entered by the user, accepting or rejecting  coins, and emitting a signal when a total of 35 cents has been  accepted.  In the lab, we will only drive a light with  the ``paid in full'' signal.    Sorry, neither candy nor Dew will be distributed!  Prof. Doug Jones designed the vending machine application and the FSM, while Prof. Chris Schmitz prototyped and constructed the physical elements  with some help from the ECE shop.  Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created  both the wiki documentation and the Altera Quartus portions of the lab (the latter were based on earlier Mentor Graphics work by Prof. Herman).  Prof. Kindratenko also helped to scale the design  in a way that made it possible to deliver to the over 400 students entering ECE every semester.    Prof. Juan Jos'e Jaramillo later identified common failure modes, including variability caused by sunshine through  the windows in ECEB,{No wonder people say that engineers hate  sunlight!} and made some changes to improve robustness.  He also created the PowerPoint slides that are typically used to describe the lab in lecture.  Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other"
    },
    {
        "question": " How did Prof. Juan Jos'e Jaramillo later identify common failure modes, including variability caused by sunshine through the windows in ECEB, and make some changes to improve robustness?\"]",
        "answers": [
            "\nJaramillo later identified common failure modes, including variability caused by sunshine through the windows in ECEB, and made some changes to improve robustness. He also created the PowerPoint slides that are typically used to describe the lab in lecture.",
            "The vertical lines carrying the current state values and their inverses back to the next state logic inputs are ordered from left to right (on the left side of the figure) as {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.",
            "A is an input operand for the AND function. The AND function evaluates to 1 if all input operands are equal to 1. A is an input operand for the OR function. The OR function evaluates to 1 if any input operand is equal to 1."
        ],
        "score": [
            8.916126251220703,
            -11.03352165222168,
            -11.201117515563965
        ],
        "text": "and column addresses have to be stored in registers).  The DRAM again specifies a write cycle, after which the operation is guaranteed to be complete.  In order, the WE, CAS, and RAS signals are then lowered.    For a read operation, the output enable signal, OE, is raised after CAS is raised.  The DATA pins, which should be floating (in other words, not driven by any logic), are then driven by the DRAM.  After the  read cycle, valid data appear on the DATA pins, and OE, CAS, and RAS are lowered in order after the data are read.  Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integrated onto the chips themselves.  As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor's 8Gb ({2^b}, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.  The ability to synchronize to an external clock has become prevalent in  the industry, leading to the somewhat confusing term SDRAM, which stands  for { synchronous DRAM}.  The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize  accesses to the external clock without the need for additional  logic.  The clock provided to the Micron chip just mentioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of the clock (hence the name DDR, or { double data rate}).    In addition to row and column components of the address, these chips further separate cells into { banks} and groups of banks.  These allow a user to exploit parallelism by starting reads or writes to separate banks at the same time, thus improving the speed at which data can move in and out of the memory.  For the {2^b} version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.    DRAM implementations provide interfaces for specifying refresh operations in addition to reads and writes. Managing refresh timing and execution is generally left to an external DRAM controller.  For the Micron chip, refresh commands must be issued every  7.8 microseconds at normal temperatures.  Each command refreshes about 2^ cells, so 8,192 commands refresh the whole chip in less than 64 milliseconds.  Alternatively, the chip can handle refresh on-chip in order to maintain memory contents when the rest of the system is  powered down.    {Design of the Finite State Machine for the Lab}  This set of notes explains the process that Prof. Doug Jones used to develop the FSM for the lab.  The lab simulates a vending machine mechanism for automatically  identifying coins (dimes and quarters only), tracking the amount  of money entered by the user, accepting or rejecting  coins, and emitting a signal when a total of 35 cents has been  accepted.  In the lab, we will only drive a light with  the ``paid in full'' signal.    Sorry, neither candy nor Dew will be distributed!  Prof. Doug Jones designed the vending machine application and the FSM, while Prof. Chris Schmitz prototyped and constructed the physical elements  with some help from the ECE shop.  Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created  both the wiki documentation and the Altera Quartus portions of the lab (the latter were based on earlier Mentor Graphics work by Prof. Herman).  Prof. Kindratenko also helped to scale the design  in a way that made it possible to deliver to the over 400 students entering ECE every semester.    Prof. Juan Jos'e Jaramillo later identified common failure modes, including variability caused by sunshine through  the windows in ECEB,{No wonder people say that engineers hate  sunlight!} and made some changes to improve robustness.  He also created the PowerPoint slides that are typically used to describe the lab in lecture.  Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other"
    },
    {
        "question": "What is the purpose of the signal A?\\n",
        "answers": [
            "\nA is a signal that is used to set the timer. When F is 0, the timer is only set when the state of S changes.",
            "The final state table and state transition diagram appear as follows: \n\nThe final state table lists the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The table also includes the states outside of the desired loop. The lights are all red in both of these states, which is necessary for safety.\n\nThe state transition diagram shows the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The diagram also shows the duration of each state.",
            "Different state representations can lead to different FSM designs. For example, if states are represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits. If an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits."
        ],
        "score": [
            2.4184000492095947,
            -11.068170547485352,
            -10.98379898071289
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "What type of FSM is implemented in this design?\\n",
        "answers": [
            "\nThis is a synchronous FSM implemented with digital logic. The FSM has a finite set of states, a set of possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.",
            "The problem that the code is trying to solve is the halting problem. The halting problem is a problem that cannot be solved by a Turing machine. The problem is that given a Turing machine and an input to the Turing machine, it is not possible to determine whether the machine will finish computing in a finite number of steps.",
            "The light sometimes glows cyan or red briefly before appearing white because the controller must ensure that the lights on one or both roads are red at all times. Similarly, if a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down."
        ],
        "score": [
            4.312828063964844,
            -11.045215606689453,
            -11.18179702758789
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": "What is the purpose of the first optical sensor?\\n",
        "answers": [
            "\nThe first optical sensor is used to detect the presence of a chip. If a chip is present, the sensor turns on and the chip can then be used. If no chip is present, the sensor turns off and the chip cannot be used. This sensor is used to save energy by turning off the chip when it is not needed.",
            "There are a few disadvantages of using an FSM. First, if the FSM is not designed carefully, it can be hard to understand and debug. Second, an FSM can use more resources than a simpler design, such as a state machine. Finally, an FSM can be more difficult to change or extend than a simpler design.",
            "The FSM is responsible for controlling the stoplights at a busy intersection. It ensures that the lights are always red on one or both roads, that a green light is always followed by a yellow light, and that the green lights are alternated between the two roads."
        ],
        "score": [
            7.442971229553223,
            -10.915423393249512,
            -10.332403182983398
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": "What is the purpose of the second optical sensor?\\n",
        "answers": [
            "\nThe second optical sensor is used to detect when a chip finishes its work so that the chip can turn itself off and save energy. This metric is used to guide the process of optimizing logic expressions so that a balance can be struck between the different metrics involved.",
            "SDRAM is a type of DRAM that is used for main memory in computers. DDR4 SDRAM is a newer type of DRAM that is faster than SDRAM.",
            "The next state value for S2 is 0. This is because the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes. The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1."
        ],
        "score": [
            7.901358604431152,
            -11.15967845916748,
            -11.135163307189941
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": "What would happen if A were to rise with T and fall with the rising clock edge?', '\\n\\n",
        "answers": [
            "\nIf A were to rise with T and fall with the rising clock edge, then the processor would not be able to recognize interrupts between instructions. This would cause the program to terminate unexpectedly.",
            "The main difference between a mux and a decoder is that a mux has multiple inputs and one output, while a decoder has one input and multiple outputs. A mux is used to select one of several input signals and send it to the output, while a decoder is used to interpret an input signal and produce several output signals.",
            "The stoplight controller alternates green lights between the two roads by changing the value of the register S. The value of S is changed when the timer reaches 0, and the timer is set when S enters a new state. The duration of each state is determined by the value of the register S."
        ],
        "score": [
            8.774919509887695,
            -11.123611450195312,
            -11.119600296020508
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": "What is the purpose of the device?\\n",
        "answers": [
            "\nThe device is used to turn itself off and save energy. This is done by using heuristic approaches to find solutions that strike a balance between different metrics, such as cost, power, or performance. The human engineer can impose constraints, such as limits on the chip area or limits on the minimum performance, in order to guide the process.",
            "The color mapping for a three-bit RGB encoding system is that the colors are produced by unique bit patterns. The states form a loop, and are marked with the internal state value S_2S_1S_0 and the output RGB.",
            "A computer memory is a group of storage elements and the logic necessary to move data in and out of the elements. The size of the elements in a memory—called the addressability of the memory—varies from a single binary digit, or bit, to a byte (8 bits) or more. Typically, we refer to data elements larger than a byte as words, but the size of a word depends on context."
        ],
        "score": [
            1.430384635925293,
            -10.170292854309082,
            -7.656028747558594
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": "How does a user interact with the device?\\n",
        "answers": [
            "\nTo use the device, a user presses one or more buttons. If the user presses more than one button, the device produces lychee ice cream. If the user presses the blend button, the device produces a blend. If the user presses the mango button, the device produces mango ice cream.",
            "The FSM is implemented using registers, counters, and a comparator. The comparator is used to implement the if statement in the program. The registers and counters are used to implement the variables idx and min in the program. The memory is used to implement the array values in the program.",
            "Tri-state buffers were used to reduce the number of pins needed on chips by combining inputs and outputs. This allowed chip designers to halve the number of pins needed."
        ],
        "score": [
            0.3089728355407715,
            -10.906942367553711,
            -10.985095024108887
        ],
        "text": "However, if the person who designed the dispenser assumed that no one would ever ask for more than one serving, something worse might happen.  In other words, giving an input of C_L=11 to the ice cream dispenser may lead to other unexpected behavior if its designer decided that that input  pattern was a ``don't care.''  The root of the problem is that { while we don't care about the value of any particular output marked ``x'' for any particular input combination, we do actually care about the relationship between the outputs}.    What can we do?  When in doubt, it is safest to make  choices and to add the new decisions to the specification rather than  leaving output values specified as ``don't care.''  For our ice cream dispenser logic, rather than leaving the outputs  unspecified whenever a user presses more than one button, we could  choose an acceptable outcome for each input combination and  replace the x's with 0s and 1s.  We might, for example, decide to produce lychee ice cream whenever the lychee button is pressed, regardless of other buttons (LBM=1xx, which means that we don't care about the inputs B and M, so LBM=100, LBM=101, LBM=110, or LBM=111).   That decision alone covers three of the four unspecified input patterns.  We might also decide that when the  blend and mango buttons are pushed together (but without the lychee button, LBM=011), our logic produces a blend.    The resulting K-maps are shown below, again with shading and green italics  identifying the combinations in which our original design specified ``don't care.''  {{file=part2/figs/CLhigh-priority.eps,width=1.00in}{file=part2/figs/CLlow-priority.eps,width=1.00in}{file=part2/figs/CMhigh-priority.eps,width=1.00in}{file=part2/figs/CMlow-priority.eps,width=1.00in}}   The logic in the dashed box to the right implements the set of choices just discussed, and matches the K-maps above.  Based on our additional choices, this implementation enforces a strict priority scheme on the user's button presses.  If a user requests lychee, they can also press either or both of the other buttons with no effect.  The lychee button has priority.  Similarly, if the user does not press lychee, but press-      es the blend button, pressing the mango button at the same time has no effect.  Choosing mango requires that no other buttons be pressed. We have thus chosen a prioritization order for the buttons and imposed  this order on the design.  We can view this same implementation in another way.  Note the one-to-one correspondence between inputs (on the left) and  outputs (on the right) for the dashed box.  This logic takes the user's button presses and chooses at most one of the buttons to pass along to our original controller implementation  (to the right of the dashed box).  In other words, rather than thinking of the logic in the dashed box as implementing a specific set of decisions, we can think of the logic as cleaning up the inputs to ensure that only valid combinations are passed to our original implementation.  Once the inputs are cleaned up, the original implementation is  acceptable, because input combinations containing more than a  single 1 are in fact impossible.  Strict prioritization is one useful way to clean up our inputs.  In general, we can design logic to map each of the four undesirable input patterns into one of the permissible  combinations (the four that we specified explicitly in our original design, with LBM  in the set ).  Selecting a prioritization scheme is just one approach for making these choices in a way that is easy for a user to understand and  is fairly easy to implement.   A second simple approach is to ignore illegal combinations by mapping them into the ``no buttons pressed''  input pattern.  Such an implementation appears to the right, laid out to show that one can again view the logic in the dashed box either as cleaning up  the inputs (by mentally grouping the logic with the inputs) or as a specific  set of choices for our ``don't"
    },
    {
        "question": "What does the signal T indicate?\\n",
        "answers": [
            "\nThe signal T indicates that an interrupt has occurred. This interrupt could be generated by an external device, such as a disk drive or a keyboard, or by an error in the program being executed, such as an undefined instruction or an attempt to divide by zero.",
            "The desired pattern is the one that represents the number in the 2's complement representation. This is because the 2's complement representation is defined by choosing a set of integers balanced around zero from the groups. So, if we want to represent a number in the 2's complement representation, we need to choose the pattern that represents the number in that representation.",
            "A tri-state buffer is a type of digital logic device that is used to either connect or disconnect a signal line from a power source or ground. This is achieved by using two transistors, one to connect the signal line to the power source, and the other to connect the signal line to ground. When the buffer is enabled, both transistors are turned on, and the signal line is connected to either the power source or ground, depending on the value of the input signal. When the buffer is disabled, both transistors are turned off, effectively disconnecting the signal line from the power source or ground."
        ],
        "score": [
            7.302055358886719,
            -10.954983711242676,
            -9.940035820007324
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": "What is the significance of the rising edge of the signal from the second sensor?\\n",
        "answers": [
            "\nThe significance of the rising edge of the signal from the second sensor is that it indicates the presence of an object in the field of view of the sensor. This is because the rising edge of the signal indicates that the sensor has detected a change in the intensity of the light that it is receiving.",
            "A transition diagram can help with this process by helping to identify the most important metrics and by helping to balance the design effectively according to those metrics.",
            "There are two input bits."
        ],
        "score": [
            9.819786071777344,
            -11.271687507629395,
            -10.179478645324707
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the significance of the signal A?', '\\n",
        "answers": [
            "\nA is an input operand for the AND function. The AND function evaluates to 1 if all input operands are equal to 1. A is an input operand for the OR function. The OR function evaluates to 1 if any input operand is equal to 1.",
            "The START state is the first state in the FSM sequence and it is used to initialize the FSM.",
            "SRAM cells are combined into a bit slice by sharing bit lines and analog read and write logic. The bit slice in the figure can be thought of as a 16-address, 1-bit-addressable memory (2^4b). The cells in a bit slice share bit lines and analog read and write logic, which appears to the right in the figure."
        ],
        "score": [
            -9.667232513427734,
            -10.62603759765625,
            -11.005958557128906
        ],
        "text": "confusing associations made even by engineering educators, we want to provide you with a concise set of definitions that generalizes correctly to more than two operands. You may have learned these functions based on truth values  (true and false), but we define them based on bits,  with 1 representing true and 0 representing false.  Table  on the next page lists logic operations.  The first column in the table lists the name of each function.  The second provides a fairly complete set of the notations that you are likely to encounter for each function, including both the forms used in engineering and those used in mathematics.  The third column defines  the function's value for two or more input operands (except for NOT, which operates on a single value).  The last column shows the form generally used in logic schematics/diagrams and mentions the important features used in distinguishing each function (in pictorial form usually called a { gate}, in reference to common physical implementations) from the  others.   { {|c|c|c|c|} { Function}& { Notation}& { Explanation}& { Schematic}  AND& {A AND B}{AB}{A}{A}{A}& {the ``all'' function: result is 1 iff}{{ all} input operands are equal to 1}& {flat input, round output}   OR& {A OR B}{A+B}{A}& {the ``any'' function: result is 1 iff}{{ any} input operand is equal to 1}& {{-6pt}}{round input, pointed output}  NOT& {NOT A}{A'}{}{}& {logical complement/negation:}{NOT 0 is 1, and NOT 1 is 0}& {triangle and circle}  {exclusive OR}& {A XOR B}{A}& {the ``odd'' function: result is 1 iff an { odd}}{number of input operands are equal to 1}& {{-6pt}}{OR with two lines}{on input side}  {``or''}& A, B, or C& {the ``one of'' function: result is 1 iff exactly}{{ one of} the input operands is equal to 1}& (not used)   } {Boolean logic operations, notation, definitions, and symbols.}{-12pt}      The first function of importance is { AND}.  Think of { AND} as the ``all'' function: given a set of input values as operands, AND evaluates  to 1 if and only if { all} of the input values are 1.  The first notation line simply uses the name of the function.  In Boolean algebra, AND is typically represented as multiplication, and the middle three forms reflect various ways in which we write multiplication.  The last notational variant is from mathematics, where the AND function is formally called { conjunction}.  The next function of importance is { OR}.  Think of { OR} as the ``any'' function: given a set of input values as operands, OR evaluates to 1 if and only if { any} of the input values is 1.  The actual number of input values equal to 1 only matters in the sense of whether it is at least one.  The notation for OR is organized in the same way as for AND, with the function name at the top, the algebraic variant that we will use in class---in this case addition---in the middle, and the mathematics variant, in this case called { disjunction}, at the bottom.  { The definition of Boolean OR is not the same as our use of  the word ``or''  in English.}  For example, if you are fortunate enough to enjoy a meal on a plane, you might be offered several choices: ``Would you like the chicken, the beef, or the vegetarian lasagna today?''  Unacceptable answers to this English question include: ``Yes,'' ``Chicken and lasagna,'' and any other combination that involves more than a single choice!  You may have noticed that we might have instead mentioned that AND evaluates to 0 if any input value is 0, and that OR evaluates to 0 if all input values are 0.  These relationships reflect a mathematical duality underlying Boolean logic that has important practical value in terms of making it easier for humans to digest complex logic expressions. We will talk more about duality later in the course, but you should learn some of the practical value now: if you are trying to evaluate an AND function, look for an input with value 0; if you are trying to evaluate an OR function, look for an  input with value 1.  If you find such"
    },
    {
        "question": " How does the device know when a coin has been inserted?\\n",
        "answers": [
            "\nThere are two optical sensors positioned at different heights on the device. The first sensor is positioned high enough above the slope that a dime passes beneath the sensor, allowing the signal T produced by the sensor to tell us whether the coin is a dime or a quarter. The second sensor is positioned so that all coins pass in front of it. The signal from the second sensor can be used as a clock for our FSM.",
            "A bad state representation can be fixed by using a code with a larger Hamming distance. This will enable the system to detect and correct more errors.",
            "A good state representation is important because it can help reduce the number of transistors needed for a design, and thus save space and energy. Additionally, it can help improve the performance of a design by making it easier for the computer to manipulate the logic expressions."
        ],
        "score": [
            0.05418035760521889,
            -11.087715148925781,
            -11.168672561645508
        ],
        "text": "hardware  improvements to reduce the rate of student headaches. Finally, Prof. Kirill Levchenko together with UA Saidivya Ashok struck a blow against COVID-19 by developing an inexpensive and portable replacement for the physical ``vending machine'' systems used for testing in previous semesters.  {Physical Design, Sensors, and Timing}  A user inserts a coin into a slot at one end of the device.  The coin then rolls down a slope towards a gate controlled by a servo.  The gate can be raised or lowered, and determines whether the coin exits from the other side or the bottom of the device.  As the coin rolls, it passes two optical sensors.{The full system actually allows four sensors to differentiate four types of coins, but our lab uses only two of these sensors.}  One of these sensors is  positioned high enough above the slope that a dime passes beneath the sesnor, allowing the signal T produced by the sensor to tell us whether  the coin is a dime or a quarter.  The second sensor is positioned so that all coins pass in front of it.  The sensor positions are chosen  carefully to ensure that, in the case of a quarter, the coin is still blocking the first sensor when it reaches the second sensor.   Blocked sensors give a signal of 1 in this design, so the rising edge  the signal from the second sensor can be used as a ``clock'' for our  FSM.  When the rising edge occurs, the signal T from the first sensor  indicates whether the coin is a quarter (T=1) or a dime (T=0).   A sample timing diagram for the lab appears to the right.  The clock signal generated by the lab is not only not a square wave---in other words, the high and low portions are not equal---but is also unlikely to be periodic. Instead, the ``cycle'' is defined by the time between coin insertions. The T signal serves as the single input to our FSM.  In the timing   {file=part3/figs/lab-timing.eps,width=2.55in}   diagram, T is shown as rising and falling before the clock edge. We use positive edge-triggered flip-flops to implement our FSM, thus the aspect of the relative timing that matters to our design is that, when the clock rises, the value of T is stable and indicates  the type of coin entered.  The signal T may fall before or after the clock does---the two are equivalent for our FSM's needs.  The signal A in the timing diagram is an output from the FSM, and indicates whether or not the coin should be accepted.  This signal  controls the servo that drives the gate, and thus determines whether the coin is accepted (A=1) as payment or rejected (A=0) and returned to the user.    Looking at the timing diagram, you should note that our FSM makes  a decision based on its current state and the input T and enters a  new state at the rising clock edge.  The value of A in the next cycle thus determines the position of the gate when the coin eventually rolls to the end of the slope.  As we said earlier, our FSM is thus a Moore machine: the output A does not depend on the input T, but only on the current internal  state bits of the the FSM.  However, you should also now realize that making A depend on T is not adequate for this lab.  If A were to rise with T and fall with the rising clock edge (on entry to the next state), or even fall with the falling edge of T, the gate would return to the reject position by the time the coin reached the gate, regardless of our FSM's decision!  {An Abstract Model}   We start by writing down states for a user's expected behavior. Given the fairly tight constraints that we have placed on our lab, few combinations are pos-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& & PAID& yes& no QUARTER& PAID& & yes& no PAID& & & yes& yes    sible.  For a total of 35 cents, a user should either insert a dime  followed by a quarter, or a quarter followed by a dime.  We begin in a START state, which transitions to states DIME or QUARTER when the user inserts the first coin.  With no previous coin, we"
    },
    {
        "question": " How does the device differentiate between a dime and a quarter?\\n",
        "answers": [
            "\nThe device uses two optical sensors to differentiate between a dime and a quarter. The first sensor is positioned so that a dime passes beneath it, while the second sensor is positioned so that all coins pass in front of it. When the rising edge of the clock signal from the second sensor occurs, the signal from the first sensor indicates whether the coin is a dime or a quarter.",
            "The min variable is a heuristic metric used to estimate the time needed to compute a function. This metric is based on the longest chain of operations performed on any instance of a variable. The rationale for this metric is that gate outputs do not change instantaneously when their inputs change.",
            "Jaramillo later identified common failure modes, including variability caused by sunshine through the windows in ECEB, and made some changes to improve robustness. He also created the PowerPoint slides that are typically used to describe the lab in lecture."
        ],
        "score": [
            9.036182403564453,
            -11.270208358764648,
            -11.321533203125
        ],
        "text": "hardware  improvements to reduce the rate of student headaches. Finally, Prof. Kirill Levchenko together with UA Saidivya Ashok struck a blow against COVID-19 by developing an inexpensive and portable replacement for the physical ``vending machine'' systems used for testing in previous semesters.  {Physical Design, Sensors, and Timing}  A user inserts a coin into a slot at one end of the device.  The coin then rolls down a slope towards a gate controlled by a servo.  The gate can be raised or lowered, and determines whether the coin exits from the other side or the bottom of the device.  As the coin rolls, it passes two optical sensors.{The full system actually allows four sensors to differentiate four types of coins, but our lab uses only two of these sensors.}  One of these sensors is  positioned high enough above the slope that a dime passes beneath the sesnor, allowing the signal T produced by the sensor to tell us whether  the coin is a dime or a quarter.  The second sensor is positioned so that all coins pass in front of it.  The sensor positions are chosen  carefully to ensure that, in the case of a quarter, the coin is still blocking the first sensor when it reaches the second sensor.   Blocked sensors give a signal of 1 in this design, so the rising edge  the signal from the second sensor can be used as a ``clock'' for our  FSM.  When the rising edge occurs, the signal T from the first sensor  indicates whether the coin is a quarter (T=1) or a dime (T=0).   A sample timing diagram for the lab appears to the right.  The clock signal generated by the lab is not only not a square wave---in other words, the high and low portions are not equal---but is also unlikely to be periodic. Instead, the ``cycle'' is defined by the time between coin insertions. The T signal serves as the single input to our FSM.  In the timing   {file=part3/figs/lab-timing.eps,width=2.55in}   diagram, T is shown as rising and falling before the clock edge. We use positive edge-triggered flip-flops to implement our FSM, thus the aspect of the relative timing that matters to our design is that, when the clock rises, the value of T is stable and indicates  the type of coin entered.  The signal T may fall before or after the clock does---the two are equivalent for our FSM's needs.  The signal A in the timing diagram is an output from the FSM, and indicates whether or not the coin should be accepted.  This signal  controls the servo that drives the gate, and thus determines whether the coin is accepted (A=1) as payment or rejected (A=0) and returned to the user.    Looking at the timing diagram, you should note that our FSM makes  a decision based on its current state and the input T and enters a  new state at the rising clock edge.  The value of A in the next cycle thus determines the position of the gate when the coin eventually rolls to the end of the slope.  As we said earlier, our FSM is thus a Moore machine: the output A does not depend on the input T, but only on the current internal  state bits of the the FSM.  However, you should also now realize that making A depend on T is not adequate for this lab.  If A were to rise with T and fall with the rising clock edge (on entry to the next state), or even fall with the falling edge of T, the gate would return to the reject position by the time the coin reached the gate, regardless of our FSM's decision!  {An Abstract Model}   We start by writing down states for a user's expected behavior. Given the fairly tight constraints that we have placed on our lab, few combinations are pos-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& & PAID& yes& no QUARTER& PAID& & yes& no PAID& & & yes& yes    sible.  For a total of 35 cents, a user should either insert a dime  followed by a quarter, or a quarter followed by a dime.  We begin in a START state, which transitions to states DIME or QUARTER when the user inserts the first coin.  With no previous coin, we"
    },
    {
        "question": " How does the device use the sensors to generate a clock signal?\\n",
        "answers": [
            "\nThe device uses the sensors to generate a clock signal by alternating between 0 and 1 with a regular period. When the clock is 0, one set of latches holds its bit values fixed, and combinational logic uses those latches as inputs to produce  bits that are copied into a second set of latches. When the clock is 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches.",
            "Juan Jos'e Jaramillo identified common failure modes, including variability caused by sunshine through the windows in ECEB, and made some changes to improve robustness. He also created the PowerPoint slides that are typically used to describe the lab in lecture.",
            "A decoder is a logic device that converts binary information from the form of one code to another, usually a natural binary code. A multiplexer is a device that selects one of several analog or digital input signals and forwards the selected input into a single output line."
        ],
        "score": [
            8.790948867797852,
            -11.269229888916016,
            -8.49575138092041
        ],
        "text": "enable input (we called it WE) to control when the latch copies its input into the stored bit. A symbol for a gated D latch appears to the right.  Note that we have dropped the name P in favor of , since P= in a gated D latch.   {The Clock Abstraction}  High-speed logic designs often use latches directly.  Engineers specify the number of latches as well as combinational logic  functions needed to connect one latch to the next, and the CAD tools optimize the combinational logic. The enable inputs of successive groups of latches are then driven by what we call a clock signal, a single bit line distributed across most of the chip that alternates between 0 and 1 with a regular period.  While the clock is 0, one set of latches holds its bit values fixed, and combinational logic uses those latches as inputs to produce  bits that are copied into a second set of latches.  When the clock switches to 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches. Of course, some of the latches in the first and third sets may be the same.  The timing of signals in such designs plays a critical role in their correct operation.  Fortunately, we have developed powerful abstractions  that allow engineers to ignore much of the complexity while thinking about the Boolean logic needed for a given design.  Towards that end, we make a simplifying assumption for the rest of our class, and for most of your career as an undergraduate: the clock signal is a { square wave} delivered uniformly across a chip.  For example, if the period of a clock is 0.5 nanoseconds (2 GHz), the clock signal is a 1 for 0.25 nanoseconds, then a 0 for 0.25 nanoseconds.  We assume that the clock signal changes instantaneously and at the same time across the chip.  Such a signal can never exist in the real world: voltages do not change instantaneously, and the  phrase ``at the same time'' may not even make sense at these scales. However, circuit designers can usually provide a clock signal that is close enough, allowing us to forget for now that no physical signal can meet our abstract definition.    SSL altered terminology on 3 Dec 21     The device shown to the right is a { master-slave} implementation of a   The device shown to the right is a { dual-latch} implementation of a  { positive edge-triggered} D flip-flop.  As you can see, we have  constructed it from two gated D latches with opposite senses of write enable.  The ``D'' part of the name has the same meaning as with a gated D latch: the bit stored is the same as the one delivered   {file=part2/figs/latch-step-8.eps,width=2.75in}   {file=part2/figs/latch-step-9.eps,width=0.95in}   to the input.  Other variants of flip-flops have also been built, but this  type dominates designs today.  Most are actually generated automatically from hardware ``design'' languages (that is, computer programming languages for hardware design).  When the clock is low (0), the first latch copies its value from the flip-flop's D input to the midpoint (marked X in our figure, but not usually given a name).  When the clock is high (1), the second latch copies its value from X to the flip-flop's output Q. Since X can not change when the clock is high, the result is that the output changes each time the clock changes from 0 to 1, which is called the { rising edge} or { positive edge} (the derivative) of the clock signal.  Hence the qualifier  ``positive edge-triggered,'' which describes the flip-flop's behavior.  The ``master-slave'' implementation refers to the use of two latches.  The ``dual-latch'' implementation refers to the use of two  latches.{Historically, this implementation was called ``master-slave,'' but ECE Illinois has decided to eliminate use of such terminology.} latches. In practice, flip-flops are almost never built this way.  To see a  commercial design, look up 74LS74, which uses six"
    },
    {
        "question": " How does the FSM use the signal T to determine whether to accept or reject the coin?\\n",
        "answers": [
            "\nThe FSM uses the signal T to determine whether to accept or reject the coin by checking the rising edge of the signal from the second sensor. If the rising edge occurs, the signal T from the first sensor indicates whether the coin is a quarter (T=1) or a dime (T=0). A sample timing diagram for the lab appears to the right. The clock signal generated by the lab is not only not a square wave---in other words, the high and low portions are not equal---but is also unlikely to be periodic. Instead, the ``cycle'' is defined by the time between coin insertions.",
            "One reason it is often best to start by selecting a representation that makes sense to a human is that it can be easier to understand and work with. Additionally, starting with a representation that makes sense to a human can help ensure that the final product is also easy to understand and use.",
            "The user purchases an item by inserting either a dime followed by a quarter, or a quarter followed by a dime. If a user inserts a second dime, the machine will reject the coin. If a user inserts a second quarter, the machine will accept the coin and the user will have paid the full price of 35 cents."
        ],
        "score": [
            9.317497253417969,
            -11.24983024597168,
            2.446110248565674
        ],
        "text": "hardware  improvements to reduce the rate of student headaches. Finally, Prof. Kirill Levchenko together with UA Saidivya Ashok struck a blow against COVID-19 by developing an inexpensive and portable replacement for the physical ``vending machine'' systems used for testing in previous semesters.  {Physical Design, Sensors, and Timing}  A user inserts a coin into a slot at one end of the device.  The coin then rolls down a slope towards a gate controlled by a servo.  The gate can be raised or lowered, and determines whether the coin exits from the other side or the bottom of the device.  As the coin rolls, it passes two optical sensors.{The full system actually allows four sensors to differentiate four types of coins, but our lab uses only two of these sensors.}  One of these sensors is  positioned high enough above the slope that a dime passes beneath the sesnor, allowing the signal T produced by the sensor to tell us whether  the coin is a dime or a quarter.  The second sensor is positioned so that all coins pass in front of it.  The sensor positions are chosen  carefully to ensure that, in the case of a quarter, the coin is still blocking the first sensor when it reaches the second sensor.   Blocked sensors give a signal of 1 in this design, so the rising edge  the signal from the second sensor can be used as a ``clock'' for our  FSM.  When the rising edge occurs, the signal T from the first sensor  indicates whether the coin is a quarter (T=1) or a dime (T=0).   A sample timing diagram for the lab appears to the right.  The clock signal generated by the lab is not only not a square wave---in other words, the high and low portions are not equal---but is also unlikely to be periodic. Instead, the ``cycle'' is defined by the time between coin insertions. The T signal serves as the single input to our FSM.  In the timing   {file=part3/figs/lab-timing.eps,width=2.55in}   diagram, T is shown as rising and falling before the clock edge. We use positive edge-triggered flip-flops to implement our FSM, thus the aspect of the relative timing that matters to our design is that, when the clock rises, the value of T is stable and indicates  the type of coin entered.  The signal T may fall before or after the clock does---the two are equivalent for our FSM's needs.  The signal A in the timing diagram is an output from the FSM, and indicates whether or not the coin should be accepted.  This signal  controls the servo that drives the gate, and thus determines whether the coin is accepted (A=1) as payment or rejected (A=0) and returned to the user.    Looking at the timing diagram, you should note that our FSM makes  a decision based on its current state and the input T and enters a  new state at the rising clock edge.  The value of A in the next cycle thus determines the position of the gate when the coin eventually rolls to the end of the slope.  As we said earlier, our FSM is thus a Moore machine: the output A does not depend on the input T, but only on the current internal  state bits of the the FSM.  However, you should also now realize that making A depend on T is not adequate for this lab.  If A were to rise with T and fall with the rising clock edge (on entry to the next state), or even fall with the falling edge of T, the gate would return to the reject position by the time the coin reached the gate, regardless of our FSM's decision!  {An Abstract Model}   We start by writing down states for a user's expected behavior. Given the fairly tight constraints that we have placed on our lab, few combinations are pos-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& & PAID& yes& no QUARTER& PAID& & yes& no PAID& & & yes& yes    sible.  For a total of 35 cents, a user should either insert a dime  followed by a quarter, or a quarter followed by a dime.  We begin in a START state, which transitions to states DIME or QUARTER when the user inserts the first coin.  With no previous coin, we"
    },
    {
        "question": " Why is it important that the output A does not depend on the input T?']",
        "answers": [
            "\nThe output A does not depend on the input T because the function of AND is to evaluate to 1 if all of the input values are 1. If there is an input with value 0, then the function will evaluate to 0.",
            "Starting with a human-based design process typically results in a design that is more efficient and effective than one that is mapped from a high-level programming language. This is because human designers can more easily optimize a design for specific metrics, such as power consumption or performance. Additionally, human designers can more easily impose constraints on a design, such as limits on chip area or minimum performance, in order to guide the design process.",
            "The problem that the code is trying to solve is the halting problem. The halting problem is a problem that cannot be solved by a Turing machine. The problem is that given a Turing machine and an input to the Turing machine, it is not possible to determine whether the machine will finish computing in a finite number of steps."
        ],
        "score": [
            8.705078125,
            -10.64999008178711,
            -7.0146565437316895
        ],
        "text": "confusing associations made even by engineering educators, we want to provide you with a concise set of definitions that generalizes correctly to more than two operands. You may have learned these functions based on truth values  (true and false), but we define them based on bits,  with 1 representing true and 0 representing false.  Table  on the next page lists logic operations.  The first column in the table lists the name of each function.  The second provides a fairly complete set of the notations that you are likely to encounter for each function, including both the forms used in engineering and those used in mathematics.  The third column defines  the function's value for two or more input operands (except for NOT, which operates on a single value).  The last column shows the form generally used in logic schematics/diagrams and mentions the important features used in distinguishing each function (in pictorial form usually called a { gate}, in reference to common physical implementations) from the  others.   { {|c|c|c|c|} { Function}& { Notation}& { Explanation}& { Schematic}  AND& {A AND B}{AB}{A}{A}{A}& {the ``all'' function: result is 1 iff}{{ all} input operands are equal to 1}& {flat input, round output}   OR& {A OR B}{A+B}{A}& {the ``any'' function: result is 1 iff}{{ any} input operand is equal to 1}& {{-6pt}}{round input, pointed output}  NOT& {NOT A}{A'}{}{}& {logical complement/negation:}{NOT 0 is 1, and NOT 1 is 0}& {triangle and circle}  {exclusive OR}& {A XOR B}{A}& {the ``odd'' function: result is 1 iff an { odd}}{number of input operands are equal to 1}& {{-6pt}}{OR with two lines}{on input side}  {``or''}& A, B, or C& {the ``one of'' function: result is 1 iff exactly}{{ one of} the input operands is equal to 1}& (not used)   } {Boolean logic operations, notation, definitions, and symbols.}{-12pt}      The first function of importance is { AND}.  Think of { AND} as the ``all'' function: given a set of input values as operands, AND evaluates  to 1 if and only if { all} of the input values are 1.  The first notation line simply uses the name of the function.  In Boolean algebra, AND is typically represented as multiplication, and the middle three forms reflect various ways in which we write multiplication.  The last notational variant is from mathematics, where the AND function is formally called { conjunction}.  The next function of importance is { OR}.  Think of { OR} as the ``any'' function: given a set of input values as operands, OR evaluates to 1 if and only if { any} of the input values is 1.  The actual number of input values equal to 1 only matters in the sense of whether it is at least one.  The notation for OR is organized in the same way as for AND, with the function name at the top, the algebraic variant that we will use in class---in this case addition---in the middle, and the mathematics variant, in this case called { disjunction}, at the bottom.  { The definition of Boolean OR is not the same as our use of  the word ``or''  in English.}  For example, if you are fortunate enough to enjoy a meal on a plane, you might be offered several choices: ``Would you like the chicken, the beef, or the vegetarian lasagna today?''  Unacceptable answers to this English question include: ``Yes,'' ``Chicken and lasagna,'' and any other combination that involves more than a single choice!  You may have noticed that we might have instead mentioned that AND evaluates to 0 if any input value is 0, and that OR evaluates to 0 if all input values are 0.  These relationships reflect a mathematical duality underlying Boolean logic that has important practical value in terms of making it easier for humans to digest complex logic expressions. We will talk more about duality later in the course, but you should learn some of the practical value now: if you are trying to evaluate an AND function, look for an input with value 0; if you are trying to evaluate an OR function, look for an  input with value 1.  If you find such"
    },
    {
        "question": " What are the possible inputs for this FSM?\\nA: The possible inputs are a dime (T=0) or a quarter (T=1)\\n\\n",
        "answers": [
            "\nThe possible inputs for this FSM are a dime (T=0) or a quarter (T=1). The machine will reject any other input. If the user tries to purchase an item without inserting enough money first, the FSM will also reject the request.",
            "The AND gate on the left is used to determine if all of the input values are 1. If all of the input values are 1, then the output of the AND gate will be 1. If any of the input values are 0, then the output of the AND gate will be 0.",
            "One reason it is often best to start by selecting a representation that makes sense to a human is that it can be easier to understand and work with. Additionally, starting with a representation that makes sense to a human can help ensure that the final product is also easy to understand and use."
        ],
        "score": [
            9.44049072265625,
            -3.7847089767456055,
            -8.43079662322998
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What are the possible outputs for this FSM?\\nA: The possible outputs are A=1 or P=1\\n\\n",
        "answers": [
            "\nThe possible outputs for this FSM are A=1 or P=1. F is a signal that is used to set the timer, and it can be either 0 or 1. If F=0, then the timer is only set when the state S changes. If F=1, then the timer is set when the state S changes or when the light on one of the roads is green.",
            "A DRAM cell is refreshed by reading the contents and writing them back with active logic. This process is necessary to avoid data loss due to charge leakage from the capacitor.",
            "The idx variable is used to keep track of the number of operations in the logic expression. This metric is used to estimate the number of transistors needed for the design, which in turn is an estimate for the area required for the design."
        ],
        "score": [
            8.073083877563477,
            -8.281461715698242,
            -7.372773170471191
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the purpose of the START state?\\nA: The START state is the initial state of the FSM. In this state, no money has been deposited yet.\\n\\n",
        "answers": [
            "\nThe START state is the initial state of the FSM. In this state, no money has been deposited yet. The FSM needs to keep track of how much money has been inserted in order to decide whether a user can purchase one of the items. That information alone is enough for the simplest machine, but let's create a machine with adjustable item prices.",
            "S_2 is 1 when the counter is in the HALT A state and HOLD=1. This is because when S_2=1, the counter is halted, and when HOLD=1, the counter should retain its current value.",
            "There are a few reasons why it is important to make implicit assumptions clear in the design process. First, if assumptions are not made clear, it can lead to confusion and misunderstanding among the team members working on the design. Second, making assumptions explicit can help to ensure that everyone is on the same page and working towards the same goal. Finally, explicit assumptions can help to prevent errors and mistakes in the design process."
        ],
        "score": [
            7.380366325378418,
            -8.362863540649414,
            -10.066326141357422
        ],
        "text": "0& 0  EXTRA1& 011& 000& 100& x& x EXTRA2& 111& 000& 100& x& x    states, but left the output bits as don't cares.  A state transition diagram appears at the bottom of this page.  {Testing the Design}  Having a complete design on paper is a good step forward, but humans make mistakes at all stages.  How can we know that a circuit that we build in the lab correctly implements the FSM that we have outlined  in these notes?  For the lab design, we have two problems to solve.  First, we have not specified an initialization scheme for the FSM. We may want the FSM to start in one of the PAID states, but adding initialization logic to the design may mean requiring you to wire together significantly more chips.  Second, we need a sequence of inputs that manages to test that all of the next-state and output logic implementations are correct.  Testing sequential logic, including FSMs, is in general extremely difficult. In fact, large sequential systems today are generally converted into  combinational logic by using shift registers to fill the  flip-flops with a particular pattern,  executing the logic for one clock cycle, and checking that the resulting  pattern of bits in the flip-flops is correct.  This approach is called  { scan-based testing}, and is discussed in ECE 543.  You  will make use of a similar approach when you test your combinational logic in the second week of the lab, before wiring up the flip-flops.  We have designed our FSM to be easy to test (even small FSMs may be challenging) with a brute force approach.  In particular, we  identify two input sequences that together serve both to initialize and  to test a correctly implemented variant of our FSM.  Our initialization sequence forces the FSM into a specific state regardless of its initial state.  And our test sequence crosses every transition arc leaving the six valid states.    In terms of T, the coin type, we initialize the FSM with the input sequence 001.  Notice that such a sequence takes any initial  state into PAID2.  For testing, we use the input sequence 111010010001.  You should trace  this sequence, starting from PAID2, on the diagram below to see how the test sequence covers all of the possible arcs.  As we test, we need also to observe the A and P outputs in each state to check the output logic.  {{file=part3/figs/lab-diag-notes.eps,width=4.25in}}    {Finite State Machine Design Examples, Part II}  This set of notes provides several additional examples of FSM design. We first design an FSM to control a vending machine, introducing encoders and decoders as components that help us to implement our design.  We then design a game controller for a logic puzzle implemented as a children's game.  Finally, we analyze a digital FSM designed to control the stoplights at the intersection of two roads.   {Design of a Vending Machine}  For the next example, we design an FSM to control a simple vending machine.   The machine accepts {U.S. coins}{Most countries have small  bills or coins in demoninations suitable for vending machine prices, so think  about some other currency if you prefer.} as payment and offers a choice of three items for sale.  What states does such an FSM need?  The FSM needs to keep track of how much money has been inserted in order to decide whether a user can  purchase one of the items.  That information alone is enough for the simplest machine, but let's create a machine with adjustable item prices.  We can use registers to hold the item prices, which  we denote P_1, P_2, and P_3.  Technically, the item prices are also part of the internal state of the  FSM.  However,  we leave out discussion (and, indeed, methods) for setting the item prices, so no state with a given combination of prices has any  transition to a state with a different set of item prices. In other words, any given combination of item prices induces a subset  of states that operate independently of the subset induced by a distinct  combination of item prices.  By abstracting"
    },
    {
        "question": " What is the purpose of the DIME and QUARTER states?\\nA: The DIME and QUARTER states represent the user having entered one coin. The first coin should be accepted, but more money is needed, so both of these states output A=1 and P=0.\\n\\n",
        "answers": [
            "\nThe DIME and QUARTER states are used to represent the user having inserted one coin. The purpose of these states is to output A=1 and P=0, indicating that the user has not yet paid the full price and that another coin is needed.",
            "This is a synchronous FSM implemented with digital logic. The FSM has a finite set of states, a set of possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.",
            "The device uses the sensors to generate a clock signal by alternating between 0 and 1 with a regular period. When the clock is 0, one set of latches holds its bit values fixed, and combinational logic uses those latches as inputs to produce  bits that are copied into a second set of latches. When the clock is 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches."
        ],
        "score": [
            8.191826820373535,
            -2.162249803543091,
            -3.7050318717956543
        ],
        "text": "need not specify a value for A.  No money has been deposited, so we set  output P=0 in the START state.  We next create DIME and QUARTER states corresponding to the user having entered one coin.  The first coin should be accepted, but more money is needed, so both of these states output A=1 and P=0. When a coin of the opposite type is entered, each state moves to a state called PAID, which we use for the case in which a total of 35 cents has been received.  For now, we ignore the possibility that the same type of coin is deposited more than once.  Finally, the PAID state accepts the second coin (A=1) and indicates that the user has paid the full price of 35 cents (P=1).   We next extend our design to handle user mistakes.  If a user enters a second dime in the DIME state, our FSM should reject the coin.  We create a REJECTD state and add it as the next state from   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no PAID& & & yes& yes    DIME when a dime is entered. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0).  What should we use as next  states from REJECTD?  If the user enters a third dime (or a fourth,  or a fifth, and so on), we want to reject the new dime as well.   If the user enters a quarter, we want to accept the coin, at which point we have received 35 cents (counting the first dime).  We use this reasoning to complete the description of REJECTD.  We also create an analogous state, REJECTQ, to handle a user who inserts more than one quarter.  What should happen after a user has paid 35 cents and bought  one item?  The FSM at that point is in the PAID state, which delivers the item by setting P=1.  Given that we want the FSM to allow the user to purchase another item,  how should we choose the next states from PAID?  The behavior that we want from PAID is identical to the behavior that we defined from START.  The 35 cents already  deposited was used to pay for the item delivered, so the machine is no longer holding any of the user's money.  We can thus simply set the next states from PAID to be DIME when a  dime is inserted and QUARTER when a quarter is inserted.     At this point, we make a decision intended primarily to simplify the logic needed to build the lab.  Without a physical item delivery  mechanism with a specification for how its in-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  PAID& DIME& QUARTER& yes& yes DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no    put must be driven,  the behavior of the output signal P can be fairly flexible.   For example, we could build a delivery mechanism that used the rising edge of P to open a chute.  In this case, the output P=0 in the start state is not relevant, and we can merge the state START with the state PAID.  The way that we handle P in the lab, we might find it strange to have a ``paid'' light turn on before inserting any money, but keeping the design simple enough for a first lab exercise  is more important.  Our final abstract state table appears above.  {Picking the Representation}  We are now ready to choose the state representation for the lab FSM.  With five states, we need three bits of internal state.  Prof. Jones decided to leverage human meaning in assigning the bit patterns, as follows:  {  S_2& type of last coin inserted (0 for dime, 1 for quarter) S_1& more than one quarter inserted? (1 for yes, 0 for no) S_0& more than one dime inserted? (1 for yes, 0 for no)  }   These meanings are not easy to apply to all of our states.  For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since we decided to start our FSM in that state as  well.  However, for the other four states, the meanings provide a clear"
    },
    {
        "question": " What is the purpose of the REJECTD and REJECTQ states?\\nA: The REJECTD and REJECTQ states are used to handle user mistakes. If a user enters a second dime in the DIME state, our FSM should reject the coin. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0). The REJECTQ state is used to handle a user who inserts more than one quarter.', \"\\n\\n\\n\\n",
        "answers": [
            "\nThe REJECTD and REJECTQ states are used to handle user mistakes. If a user enters a second dime in the DIME state, our FSM should reject the coin. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0). The REJECTQ state is used to handle a user who inserts more than one quarter.",
            "There are a few benefits of using a Gray code counter. First, it is easy to design and implement. Second, it is easy to check the correctness of the logic. Third, it is easy to read and understand the logic.",
            "The purpose of the FSM is to control the stoplights at a busy intersection. The FSM ensures that the lights are always red on one or both roads, that a yellow light is shown before a green light, and that the green lights are alternated between the two roads."
        ],
        "score": [
            6.427389621734619,
            4.641952037811279,
            5.641730308532715
        ],
        "text": "need not specify a value for A.  No money has been deposited, so we set  output P=0 in the START state.  We next create DIME and QUARTER states corresponding to the user having entered one coin.  The first coin should be accepted, but more money is needed, so both of these states output A=1 and P=0. When a coin of the opposite type is entered, each state moves to a state called PAID, which we use for the case in which a total of 35 cents has been received.  For now, we ignore the possibility that the same type of coin is deposited more than once.  Finally, the PAID state accepts the second coin (A=1) and indicates that the user has paid the full price of 35 cents (P=1).   We next extend our design to handle user mistakes.  If a user enters a second dime in the DIME state, our FSM should reject the coin.  We create a REJECTD state and add it as the next state from   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no PAID& & & yes& yes    DIME when a dime is entered. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0).  What should we use as next  states from REJECTD?  If the user enters a third dime (or a fourth,  or a fifth, and so on), we want to reject the new dime as well.   If the user enters a quarter, we want to accept the coin, at which point we have received 35 cents (counting the first dime).  We use this reasoning to complete the description of REJECTD.  We also create an analogous state, REJECTQ, to handle a user who inserts more than one quarter.  What should happen after a user has paid 35 cents and bought  one item?  The FSM at that point is in the PAID state, which delivers the item by setting P=1.  Given that we want the FSM to allow the user to purchase another item,  how should we choose the next states from PAID?  The behavior that we want from PAID is identical to the behavior that we defined from START.  The 35 cents already  deposited was used to pay for the item delivered, so the machine is no longer holding any of the user's money.  We can thus simply set the next states from PAID to be DIME when a  dime is inserted and QUARTER when a quarter is inserted.     At this point, we make a decision intended primarily to simplify the logic needed to build the lab.  Without a physical item delivery  mechanism with a specification for how its in-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  PAID& DIME& QUARTER& yes& yes DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no    put must be driven,  the behavior of the output signal P can be fairly flexible.   For example, we could build a delivery mechanism that used the rising edge of P to open a chute.  In this case, the output P=0 in the start state is not relevant, and we can merge the state START with the state PAID.  The way that we handle P in the lab, we might find it strange to have a ``paid'' light turn on before inserting any money, but keeping the design simple enough for a first lab exercise  is more important.  Our final abstract state table appears above.  {Picking the Representation}  We are now ready to choose the state representation for the lab FSM.  With five states, we need three bits of internal state.  Prof. Jones decided to leverage human meaning in assigning the bit patterns, as follows:  {  S_2& type of last coin inserted (0 for dime, 1 for quarter) S_1& more than one quarter inserted? (1 for yes, 0 for no) S_0& more than one dime inserted? (1 for yes, 0 for no)  }   These meanings are not easy to apply to all of our states.  For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since we decided to start our FSM in that state as  well.  However, for the other four states, the meanings provide a clear"
    },
    {
        "question": "What is the significance of the 'START' state?\\n",
        "answers": [
            "\nThe START state is the first state in the FSM sequence and it is used to initialize the FSM.",
            "The design process involves the manipulation of logic expressions in order to optimize them for specific metrics, such as cost, power, or performance. Today, this manipulation is performed almost entirely by computers, with human engineers supplying the logic functions of interest and imposing constraints on the process.",
            "Scan-based testing is a type of testing where the input is scanned for errors before it is processed. This type of testing is often used to test software for errors."
        ],
        "score": [
            3.517836570739746,
            -11.198143005371094,
            -11.166915893554688
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "What is the significance of the 'DIME' state?\\n",
        "answers": [
            "\nThe DIME state is the state in which the machine is waiting for a dime to be inserted. This is the first state in which the machine accepts a coin. The machine outputs A=1 and P=0 in this state, indicating that the user has not yet paid the full price of 35 cents.",
            "The right strategy is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning.",
            "The RAS signal indicates when the row component of an address is available. Raising the RAS signal for a row destructively reads the contents of all cells on that row, forcing the cells to be rewritten and effecting a refresh. The row is thus a natural basis for the refresh cycle."
        ],
        "score": [
            4.285802841186523,
            -11.307809829711914,
            -11.132268905639648
        ],
        "text": "need not specify a value for A.  No money has been deposited, so we set  output P=0 in the START state.  We next create DIME and QUARTER states corresponding to the user having entered one coin.  The first coin should be accepted, but more money is needed, so both of these states output A=1 and P=0. When a coin of the opposite type is entered, each state moves to a state called PAID, which we use for the case in which a total of 35 cents has been received.  For now, we ignore the possibility that the same type of coin is deposited more than once.  Finally, the PAID state accepts the second coin (A=1) and indicates that the user has paid the full price of 35 cents (P=1).   We next extend our design to handle user mistakes.  If a user enters a second dime in the DIME state, our FSM should reject the coin.  We create a REJECTD state and add it as the next state from   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no PAID& & & yes& yes    DIME when a dime is entered. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0).  What should we use as next  states from REJECTD?  If the user enters a third dime (or a fourth,  or a fifth, and so on), we want to reject the new dime as well.   If the user enters a quarter, we want to accept the coin, at which point we have received 35 cents (counting the first dime).  We use this reasoning to complete the description of REJECTD.  We also create an analogous state, REJECTQ, to handle a user who inserts more than one quarter.  What should happen after a user has paid 35 cents and bought  one item?  The FSM at that point is in the PAID state, which delivers the item by setting P=1.  Given that we want the FSM to allow the user to purchase another item,  how should we choose the next states from PAID?  The behavior that we want from PAID is identical to the behavior that we defined from START.  The 35 cents already  deposited was used to pay for the item delivered, so the machine is no longer holding any of the user's money.  We can thus simply set the next states from PAID to be DIME when a  dime is inserted and QUARTER when a quarter is inserted.     At this point, we make a decision intended primarily to simplify the logic needed to build the lab.  Without a physical item delivery  mechanism with a specification for how its in-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  PAID& DIME& QUARTER& yes& yes DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no    put must be driven,  the behavior of the output signal P can be fairly flexible.   For example, we could build a delivery mechanism that used the rising edge of P to open a chute.  In this case, the output P=0 in the start state is not relevant, and we can merge the state START with the state PAID.  The way that we handle P in the lab, we might find it strange to have a ``paid'' light turn on before inserting any money, but keeping the design simple enough for a first lab exercise  is more important.  Our final abstract state table appears above.  {Picking the Representation}  We are now ready to choose the state representation for the lab FSM.  With five states, we need three bits of internal state.  Prof. Jones decided to leverage human meaning in assigning the bit patterns, as follows:  {  S_2& type of last coin inserted (0 for dime, 1 for quarter) S_1& more than one quarter inserted? (1 for yes, 0 for no) S_0& more than one dime inserted? (1 for yes, 0 for no)  }   These meanings are not easy to apply to all of our states.  For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since we decided to start our FSM in that state as  well.  However, for the other four states, the meanings provide a clear"
    },
    {
        "question": "What is the significance of the 'QUARTER' state?\\n",
        "answers": [
            "\nThe 'QUARTER' state refers to the fact that the addition unit for our 4-bit examples is not closed under addition. This means that for any value of N, we can easily find two 4-bit unsigned numbers that, when added together, cannot be represented as a 4-bit unsigned number. With N=4, for example, we can add 12 (1100) and 6 (0110) to obtain 18. Since 18 is outside of the range [0,2^4-1] representable using the 4-bit unsigned representation, our representation breaks if we try to represent the sum using this representation.",
            "The vending machine knows when a purchase is requested when the user pushes one of the item selection buttons. This causes the priority encoder to produce a signal P indicating that the user is trying to make a purchase.",
            "A next-state table lists all of the possible states that the FSM can be in, as well as the inputs and outputs for each state. A transition diagram shows the order in which the states will be visited, based on the inputs and outputs."
        ],
        "score": [
            6.146331787109375,
            -11.273805618286133,
            -10.682194709777832
        ],
        "text": "same idea to perform operations on other representations, such as changing an upper case letter in ASCII to a lower case one, or converting from an ASCII digit to an unsigned representation of the same number.    {The Unsigned Add Unit}  In order to define a representation for signed integers that allows us to reuse a piece of hardware designed for unsigned integers, we must first understand what such a piece of hardware actually does (we do not need to know how it works yet---we'll explore that question  later in our class).  The unsigned representation using {N} bits is not closed under addition.  In other words, for any value of N, we can easily find two {N-bit} unsigned numbers that, when added together, cannot be represented as an {N-bit} unsigned number.  With N=4,  for example, we can add 12 (1100) and 6 (0110) to obtain 18. Since 18 is outside of the range [0,2^4-1] representable using the {4-bit} unsigned representation, our representation breaks if we try to represent the sum using this representation.  We call this failure an { overflow} condition: the representation cannot represent the result of the operation, in this case addition.   Using more bits to represent the answer is not an attractive solution,  since we might then want to use more bits for the inputs, which in turn requires more bits for the outputs, and so on.  We cannot build  something supporting an infinite number of bits.  Instead, we  choose a value for N and build an add unit that adds two {N-bit} numbers and produces an {N-bit} sum (and some overflow  indicators, which we discuss in the next set of notes).  The diagram to the right shows how we might draw such a device, with two {N-bit} numbers entering at from the top, and the {N-bit} sum coming out from the bottom.       The function used for {N-bit} unsigned addition is addition  modulo 2^N.  In a practical sense, you can think of this function as simply keeping the last N bits of the answer; other bits  are simply discarded.  In the example to the right, we add 12 and 6 to obtain 18, but then discard the extra bit on the left, so the add unit produces 2 (an overflow).       { Modular arithmetic} defines a way of performing arithmetic for a finite number of possible values, usually integers.   As a concrete example, let's use modulo 16, which corresponds to the addition unit for our {4-bit} examples.  Starting with the full range of integers, we break the number line into contiguous groups of 16 integers, as shown to the right.      The numbers 0 to 15 form one group.  The numbers -16 to -1 form a second group, and the numbers from 16 to 31 form a third group.  An infinite number of groups are defined in this manner.  We then define 16 { equivalence classes} consisting of the first numbers from all groups, the second numbers from all groups, and so forth. For example, the numbers , -32, -16, 0, 16, 32,  form one such equivalence class.  Mathematically, we say that two numbers A and B are equivalent modulo 16, which we write as  {eqnarray*} (A &=& B)  16, {or sometimes as} A && B {(mod 16)} {eqnarray*}  if and only if A=B+16k for some integer k.  Equivalence as defined by a particular modulus distributes over addition and multiplication.  If, for example, we want to find the equivalence class for (A + B)  16, we can find the equivalence classes for A (call it C) and B  (call it D) and then calculate the equivalence class  of (C + D)  16. As a concrete example of distribution over multiplication,  given (A = 1,083,102,112  7,323,127)  10, find A.  For this problem, we note that the first number is equivalent  to 2  10, while the second number is equivalent  to 7  10.  We then write (A = 2  7)  10, and, since 2  7 = 14, we have (A = 4)  10.   {Deriving 2's Complement}   Given these equivalence classes, we might instead choose to draw a circle to identify the equivalence classes and to associate each class with one of the sixteen possible {4-bit} patterns, as shown to the right. Using this circle"
    },
    {
        "question": "What is the significance of the 'PAID' state?\\n",
        "answers": [
            "\nThe PAID state is the state in which the user has paid the full price of 35 cents. The next states from PAID are DIME when a dime is inserted and QUARTER when a quarter is inserted.",
            "If a user presses the \"go\" button (HG=01), the machine should dispense one and a half servings of lychee and a half serving of mango.",
            "The color mapping for a three-bit RGB encoding system is a system where colors are represented by a combination of red, green, and blue light. The color mapping for such a system is shown to the right. Imagine that you are charged with creating a counter to drive a light through a sequence of colors."
        ],
        "score": [
            3.9456870555877686,
            -11.248997688293457,
            -11.256021499633789
        ],
        "text": "need not specify a value for A.  No money has been deposited, so we set  output P=0 in the START state.  We next create DIME and QUARTER states corresponding to the user having entered one coin.  The first coin should be accepted, but more money is needed, so both of these states output A=1 and P=0. When a coin of the opposite type is entered, each state moves to a state called PAID, which we use for the case in which a total of 35 cents has been received.  For now, we ignore the possibility that the same type of coin is deposited more than once.  Finally, the PAID state accepts the second coin (A=1) and indicates that the user has paid the full price of 35 cents (P=1).   We next extend our design to handle user mistakes.  If a user enters a second dime in the DIME state, our FSM should reject the coin.  We create a REJECTD state and add it as the next state from   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no PAID& & & yes& yes    DIME when a dime is entered. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0).  What should we use as next  states from REJECTD?  If the user enters a third dime (or a fourth,  or a fifth, and so on), we want to reject the new dime as well.   If the user enters a quarter, we want to accept the coin, at which point we have received 35 cents (counting the first dime).  We use this reasoning to complete the description of REJECTD.  We also create an analogous state, REJECTQ, to handle a user who inserts more than one quarter.  What should happen after a user has paid 35 cents and bought  one item?  The FSM at that point is in the PAID state, which delivers the item by setting P=1.  Given that we want the FSM to allow the user to purchase another item,  how should we choose the next states from PAID?  The behavior that we want from PAID is identical to the behavior that we defined from START.  The 35 cents already  deposited was used to pay for the item delivered, so the machine is no longer holding any of the user's money.  We can thus simply set the next states from PAID to be DIME when a  dime is inserted and QUARTER when a quarter is inserted.     At this point, we make a decision intended primarily to simplify the logic needed to build the lab.  Without a physical item delivery  mechanism with a specification for how its in-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  PAID& DIME& QUARTER& yes& yes DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no    put must be driven,  the behavior of the output signal P can be fairly flexible.   For example, we could build a delivery mechanism that used the rising edge of P to open a chute.  In this case, the output P=0 in the start state is not relevant, and we can merge the state START with the state PAID.  The way that we handle P in the lab, we might find it strange to have a ``paid'' light turn on before inserting any money, but keeping the design simple enough for a first lab exercise  is more important.  Our final abstract state table appears above.  {Picking the Representation}  We are now ready to choose the state representation for the lab FSM.  With five states, we need three bits of internal state.  Prof. Jones decided to leverage human meaning in assigning the bit patterns, as follows:  {  S_2& type of last coin inserted (0 for dime, 1 for quarter) S_1& more than one quarter inserted? (1 for yes, 0 for no) S_0& more than one dime inserted? (1 for yes, 0 for no)  }   These meanings are not easy to apply to all of our states.  For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since we decided to start our FSM in that state as  well.  However, for the other four states, the meanings provide a clear"
    },
    {
        "question": "What is the significance of the 'REJECTD' state?\", '\\n\\n",
        "answers": [
            "\nThe 'REJECTD' state is a state in which a processor interrupts a program due to an error or unexpected behavior. This state usually causes the program to terminate.",
            "\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\",
            "The AND gate on the left is used to determine if all of the input values are 1. If all of the input values are 1, then the output of the AND gate will be 1. If any of the input values are 0, then the output of the AND gate will be 0."
        ],
        "score": [
            6.512822151184082,
            -6.5108137130737305,
            -10.991128921508789
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": " How does the user purchase an item?\\n\\nA user purchases an item by inserting either a dime followed by a quarter, or a quarter followed by a dime. If a user inserts a second dime, the machine will reject the coin. If a user inserts a second quarter, the machine will accept the coin and the user will have paid the full price of 35 cents.']",
        "answers": [
            "\nThe user purchases an item by inserting either a dime followed by a quarter, or a quarter followed by a dime. If a user inserts a second dime, the machine will reject the coin. If a user inserts a second quarter, the machine will accept the coin and the user will have paid the full price of 35 cents.",
            "The HOLD variable simplifies the design by allowing the designer to separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.",
            "Scan-based testing is a type of testing that is used to test the functionality of a design. This type of testing is typically used to test digital designs."
        ],
        "score": [
            6.83239221572876,
            -1.1680011749267578,
            -1.9784154891967773
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What is the state representation for the lab FSM?\\n",
        "answers": [
            "\nThe state representation for the lab FSM is a 3-bit binary number, with the following meaning:\n\nS_2: type of last coin inserted (0 for dime, 1 for quarter)\nS_1: more than one quarter inserted? (1 for yes, 0 for no)\nS_0: more than one dime inserted? (1 for yes, 0 for no)",
            "Abstraction in selecting a representation can impact the design metrics by reducing the number of metrics that need to be considered. This can simplify the process of designing a solution and make it easier to find an optimal solution.",
            "One way to build an FSM that allows the pieces to be executed to be specified after the FSM is built is to use a state machine that has an output that can be routed to different inputs. This allows the FSM to be configured to execute different pieces of code based on the input. Another way to build an FSM that allows the pieces to be executed to be specified after the FSM is built is to use a state machine that has multiple states that can be executed in parallel. This allows the FSM to be configured to execute different pieces of code in parallel based on the input."
        ],
        "score": [
            8.328720092773438,
            -11.213022232055664,
            -3.537716865539551
        ],
        "text": "need not specify a value for A.  No money has been deposited, so we set  output P=0 in the START state.  We next create DIME and QUARTER states corresponding to the user having entered one coin.  The first coin should be accepted, but more money is needed, so both of these states output A=1 and P=0. When a coin of the opposite type is entered, each state moves to a state called PAID, which we use for the case in which a total of 35 cents has been received.  For now, we ignore the possibility that the same type of coin is deposited more than once.  Finally, the PAID state accepts the second coin (A=1) and indicates that the user has paid the full price of 35 cents (P=1).   We next extend our design to handle user mistakes.  If a user enters a second dime in the DIME state, our FSM should reject the coin.  We create a REJECTD state and add it as the next state from   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no PAID& & & yes& yes    DIME when a dime is entered. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0).  What should we use as next  states from REJECTD?  If the user enters a third dime (or a fourth,  or a fifth, and so on), we want to reject the new dime as well.   If the user enters a quarter, we want to accept the coin, at which point we have received 35 cents (counting the first dime).  We use this reasoning to complete the description of REJECTD.  We also create an analogous state, REJECTQ, to handle a user who inserts more than one quarter.  What should happen after a user has paid 35 cents and bought  one item?  The FSM at that point is in the PAID state, which delivers the item by setting P=1.  Given that we want the FSM to allow the user to purchase another item,  how should we choose the next states from PAID?  The behavior that we want from PAID is identical to the behavior that we defined from START.  The 35 cents already  deposited was used to pay for the item delivered, so the machine is no longer holding any of the user's money.  We can thus simply set the next states from PAID to be DIME when a  dime is inserted and QUARTER when a quarter is inserted.     At this point, we make a decision intended primarily to simplify the logic needed to build the lab.  Without a physical item delivery  mechanism with a specification for how its in-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  PAID& DIME& QUARTER& yes& yes DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no    put must be driven,  the behavior of the output signal P can be fairly flexible.   For example, we could build a delivery mechanism that used the rising edge of P to open a chute.  In this case, the output P=0 in the start state is not relevant, and we can merge the state START with the state PAID.  The way that we handle P in the lab, we might find it strange to have a ``paid'' light turn on before inserting any money, but keeping the design simple enough for a first lab exercise  is more important.  Our final abstract state table appears above.  {Picking the Representation}  We are now ready to choose the state representation for the lab FSM.  With five states, we need three bits of internal state.  Prof. Jones decided to leverage human meaning in assigning the bit patterns, as follows:  {  S_2& type of last coin inserted (0 for dime, 1 for quarter) S_1& more than one quarter inserted? (1 for yes, 0 for no) S_0& more than one dime inserted? (1 for yes, 0 for no)  }   These meanings are not easy to apply to all of our states.  For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since we decided to start our FSM in that state as  well.  However, for the other four states, the meanings provide a clear"
    },
    {
        "question": " How many bits of internal state are needed?\\n",
        "answers": [
            "\nBased on the passage, at least 33 bits of internal state are needed. This is because the FSM needs to track the number of cycles needed for the serial comparator to execute a comparison, which is at least 32 cycles. Additionally, the FSM needs to track whether or not the loop is done, which requires at least one additional bit.",
            "One way to design an FSM is to start with a flow chart of the program that is to be implemented. This flow chart can then be used to identify high-level states that can be implemented in a fixed number of clock cycles. Another way to design an FSM is to start with a list of the components that are needed to implement the program. This list can then be used to identify high-level states that can be implemented using those components.",
            "A decoder is similar to a mux in that it allows one digital signal to select which of several others is forwarded to an output. A decoder has one input and multiple outputs, while a mux has multiple inputs and one output. A decoder can be thought of as a special case of a mux."
        ],
        "score": [
            7.492146015167236,
            -11.1417236328125,
            -11.229467391967773
        ],
        "text": "least a controllable number of cycles. If we cannot specify clearly when transitions occur from an FSM state, we may not be able to implement the state.    Now let's go through the flow chart and identify states.  Initialization of { min} and { idx} need not occur serially, and the result of the first comparison between { idx} and the constant 10 is known in advance, so we can merge all three operations into a single state, which we  call { INIT}.  We can also merge the updates of { min} and { idx} into a second FSM state, which we call { COPY}.  However, the update to { min}  occurs only when the comparison ({ min > value[idx]}) is true.   We can use logic to predicate execution of the update.  In other words, we  can use the output of the comparator, which is available after the comparator  has finished comparing the two values (in a high-level FSM state that we  have yet to define), to determine whether or not the register holding  { min} loads a new value in the { COPY} state.  Our model of use for this FSM involves external logic filling the memory (the array of integer values), executing the FSM ``code,'' and then checking the answer.  To support this use model, we create a FSM state  called { WAIT} for cycles in which the FSM has no work to do. Later, we also make use of an external input signal { START}  to start the FSM execution.  The { WAIT} state logically corresponds to the ``START''  bubble in the flow chart.   Only the test for the { if} statement remains.  Using a serial comparator to compare two {32-bit} values requires 32 cycles. However, we need an additional cycle to move values into our shift  registers so that the comparator can see the first bit.  Thus our single comparison operation breaks into two high-level states.  In the first state, which we call { PREP}, we copy { min} to one of the shift registers, copy { values[idx]} to the other shift register, and reset the counter that measures the cycles needed for our serial comparator.  We then move to a second high-level state, which we call { COMPARE}, in which we feed one bit per cycle from each shift register to the serial comparator.  The { COMPARE}  state   {{file=part3/figs/part3-min-flow-chart-states.eps,width=3.96in}}   executes for 32 cycles, after which the comparator produces the one-bit answer that we need, and we can move to the { COPY} state.  The association between the flow chart and the high-level FSM states is illustrated in the figure shown to the right above.   We can now also draw an abstract state diagram for our FSM, as shown to the right.  The FSM begins in the { WAIT} state.  After external logic fills the { values} array, it signals the FSM to begin by raising the { START} signal.  The FSM transitions into the  { INIT} state, and in the next cycle into the { PREP} state. From { PREP}, the FSM always moves to { COMPARE}, where it remains for 32 cycles while the serial comparator executes a comparison. After { COMPARE}, the FSM moves to the { COPY}   {{file=part3/figs/part3-min-state-diag.eps,width=3in}}   state, where it remains for one cycle.  The transition from { COPY} depends on how many loop iterations have executed.  If more loop iterations remain, the FSM moves to { PREP} to execute the next iteration.  If the loop is done, the FSM returns to { WAIT} to allow external logic to read the result of the computation.    {Laying Out Components}   Our high-level FSM design tells us what our components need to be able to do in any given cycle.  For example, when we load new values into the shift registers that provide bits to the serial comparator, we always copy  { min} into one shift register and { values[idx]} into the second. Using this information, we can put together our components and simplify our design by fixing the way in which bits flow between them.  The figure at the right shows how we can organize our components. Again, in practice, one goes back and forth thinking about states, components, and flow from state to"
    },
    {
        "question": " What does each bit pattern represent?\\n",
        "answers": [
            "\nEach bit pattern represents a column in an addition problem. The carry in bit (C_) is added to the A and B bits to produce a sum bit (S_) and a carry out bit (C_).",
            "A finite state machine is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs. In any given state, a particular input may cause the FSM to move to another state; this combination is called a transition rule.",
            "The output of the FSM looks like a green light followed by a yellow light followed by a red light."
        ],
        "score": [
            7.2455339431762695,
            -10.756098747253418,
            -10.924840927124023
        ],
        "text": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is"
    },
    {
        "question": " How many bit patterns are available for the PAID state?\\n",
        "answers": [
            "\nThere are 64 bit patterns available for the PAID state.",
            "HOLD = H + S_2\n\nWhen the counter is in the HALT A state and S_2=1, HOLD = 1. This is because the counter should stop (move into a halted state without changing value) when H is true, and in the halted states, when S_2=1, the counter stops (stays in a halted state) when H+ is true.",
            "Memory chips can be combined to provide larger and wider memories by using multiple chips. For example, two 2-bit memories can be used to implement a 4-bit memory. One of the address bits is used to drive a decoder that determines which of the two chips is active (CS=1). The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CS is low, as desired."
        ],
        "score": [
            9.360886573791504,
            -11.161001205444336,
            -11.101572036743164
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What is the final state table?', '\\n\\n",
        "answers": [
            "\nThe final state table for the FSM is given in the passage. The table shows the states of the FSM, the outputs for each state, and the duration of each state.",
            "1. What is the difference between two-level and three-level logic?\n\nTwo-level logic is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. Three-level logic is a way of expressing logic functions using three levels. The three levels refer to the number of functions through which an input passes to reach an output, and the SOP and POS forms are examples of three-level logic.",
            "One important reason to consider all possible input conditions from all possible states is that it can help ensure that a design is correct. For example, if a design only ever receives input values that are within a certain range, it is possible that the design is not actually correct for all input values. Considering all possible input conditions can help catch errors like this.\n\nAnother reason to consider all possible input conditions is that it can help optimize a design. For example, if a design only ever receives input values that are within a certain range, it is possible to simplify the design or make it more efficient. Considering all possible input conditions can help identify opportunities for optimization."
        ],
        "score": [
            5.6766533851623535,
            -11.06070327758789,
            -11.045316696166992
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " Why did Prof. Jones choose to leverage human meaning in assigning the bit patterns?\\n",
        "answers": [
            "\nThe meaning assigned to the bit patterns makes it easier to understand the behavior of the FSM. For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since we decided to start our FSM in that state as well. However, for the other four states, the meanings provide a clear understanding of the state of the FSM.",
            "The FSM is responsible for controlling the stoplights at a busy intersection. It ensures that the lights on one or both roads are red at all times, that a road has a green light, and that the controller should show a yellow light before showing a red light.",
            "Prof. Chris Schmitz prototyped and constructed the physical elements with some help from the ECE shop. He designed the vending machine application and the FSM, while Prof. Doug Jones created the wiki documentation and the Altera Quartus portions of the lab. Casey Smith, head guru of the ECE Instructional Labs, developed a new debounce design and made some other changes to improve robustness."
        ],
        "score": [
            1.1848535537719727,
            -11.315690040588379,
            -7.2804155349731445
        ],
        "text": "need not specify a value for A.  No money has been deposited, so we set  output P=0 in the START state.  We next create DIME and QUARTER states corresponding to the user having entered one coin.  The first coin should be accepted, but more money is needed, so both of these states output A=1 and P=0. When a coin of the opposite type is entered, each state moves to a state called PAID, which we use for the case in which a total of 35 cents has been received.  For now, we ignore the possibility that the same type of coin is deposited more than once.  Finally, the PAID state accepts the second coin (A=1) and indicates that the user has paid the full price of 35 cents (P=1).   We next extend our design to handle user mistakes.  If a user enters a second dime in the DIME state, our FSM should reject the coin.  We create a REJECTD state and add it as the next state from   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no PAID& & & yes& yes    DIME when a dime is entered. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0).  What should we use as next  states from REJECTD?  If the user enters a third dime (or a fourth,  or a fifth, and so on), we want to reject the new dime as well.   If the user enters a quarter, we want to accept the coin, at which point we have received 35 cents (counting the first dime).  We use this reasoning to complete the description of REJECTD.  We also create an analogous state, REJECTQ, to handle a user who inserts more than one quarter.  What should happen after a user has paid 35 cents and bought  one item?  The FSM at that point is in the PAID state, which delivers the item by setting P=1.  Given that we want the FSM to allow the user to purchase another item,  how should we choose the next states from PAID?  The behavior that we want from PAID is identical to the behavior that we defined from START.  The 35 cents already  deposited was used to pay for the item delivered, so the machine is no longer holding any of the user's money.  We can thus simply set the next states from PAID to be DIME when a  dime is inserted and QUARTER when a quarter is inserted.     At this point, we make a decision intended primarily to simplify the logic needed to build the lab.  Without a physical item delivery  mechanism with a specification for how its in-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  PAID& DIME& QUARTER& yes& yes DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no    put must be driven,  the behavior of the output signal P can be fairly flexible.   For example, we could build a delivery mechanism that used the rising edge of P to open a chute.  In this case, the output P=0 in the start state is not relevant, and we can merge the state START with the state PAID.  The way that we handle P in the lab, we might find it strange to have a ``paid'' light turn on before inserting any money, but keeping the design simple enough for a first lab exercise  is more important.  Our final abstract state table appears above.  {Picking the Representation}  We are now ready to choose the state representation for the lab FSM.  With five states, we need three bits of internal state.  Prof. Jones decided to leverage human meaning in assigning the bit patterns, as follows:  {  S_2& type of last coin inserted (0 for dime, 1 for quarter) S_1& more than one quarter inserted? (1 for yes, 0 for no) S_0& more than one dime inserted? (1 for yes, 0 for no)  }   These meanings are not easy to apply to all of our states.  For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since we decided to start our FSM in that state as  well.  However, for the other four states, the meanings provide a clear"
    },
    {
        "question": " How many bits of internal state are needed for five states?\\n",
        "answers": [
            "\nOur machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).",
            "The inputs to a Gray code counter are the current state of the counter and the clock signal. The clock signal triggers the state change of the counter.",
            "The passage states that power consumption is an important metric that engineers must take into account in their designs. However, in some cases, such as desktop and high-end server processors, other metrics (such as performance) may be so important that we always want to operate at the edge of the interesting range. This could be what is going wrong, as engineers are not taking power consumption into account as much as they should be."
        ],
        "score": [
            -10.28645133972168,
            -11.239968299865723,
            -11.305809020996094
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What does the state table represent?\\n",
        "answers": [
            "\nThe state table represents the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The lights are all red in both of these states, which is necessary for safety.",
            "The vending machine keeps track of how much money has been inserted as a multiple of 5 cents (one nickel). It has six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).",
            "The FSM needs to be initialized by external logic filling the memory (the array of integer values), executing the FSM ``code,'' and then checking the answer.  To support this use model, we create a FSM state called { WAIT} for cycles in which the FSM has no work to do. Later, we also make use of an external input signal { START} to start the FSM execution."
        ],
        "score": [
            7.071517467498779,
            -11.269491195678711,
            -10.65842342376709
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What does the K-map represent?\\n",
        "answers": [
            "\nThe K-map represents the domain of a function in the form of a hypercube. The hypercube is divided into regions, each of which corresponds to a product of literals. The function is then evaluated at each point in the hypercube, and the product of literals is an implicant of the function if the function evaluates to 1 at that point.",
            "A FSM is implemented as a digital system, with all states represented as patterns using a fixed number of bits, all inputs translated into bits, and all outputs translated into bits. Transition rules must be complete, meaning that given any state of the FSM and any pattern of input bits, a transition must be defined from that state to another state. Outputs for a digital FSM are calculated using Boolean logic expressions.",
            "Interrupts are more powerful than exceptions because they can occur asynchronously with respect to the program. This means that the processor can check for interrupts between instructions, which allows for more flexibility in the design of the processor."
        ],
        "score": [
            7.880604267120361,
            -10.392881393432617,
            -11.287672996520996
        ],
        "text": "have other names for the smaller values: a point for N=0, a line segment for N=1, a square for N=2, and a cube for N=3.  The diagrams above and to the right illustrate the cases that  are easily drawn on paper.  The black dots represent specific input combinations, and the blue edges connect input combinations that differ in exactly one input value (one bit).    By viewing a function's domain in this way, we can make a connection between a product of literals and the structure of the domain.  Let's use the {3-dimensional} version as an example.  We call the  variables A, B, and C, and note that the cube has 2^3=8 corners corresponding to the 2^3 possible combinations of A, B, and C. The simplest product of literals in this case is 1, which is the product of 0 literals.  Obviously, the product 1  evaluates to 1 for any variable values.  We can thus think of it as covering the entire domain of the function.  In the case of our example, the product 1 covers the whole cube.  In order for the product 1 to  be an implicant of a function, the function itself must be the function 1.  What about a product consisting of a single literal, such as A  or ?  The dividing lines in the diagram illustrate the answer: any such product term evaluates to 1 on a face of the cube, which includes 2^2=4 of the corners.  If a function evaluates to 1 on any of the six faces of the cube, the corresponding product term (consisting of a single literal) is an implicant of the function.  Continuing with products of two literals, we see that any product of  two literals, such as A or C, corresponds to an edge of our {3-dimensional} cube.  The edge includes 2^1=2 corners.  And, if a function evaluates to 1 on any of the 12 edges of the cube, the corresponding product term (consisting of two literals) is an implicant of the function.  Finally, any product of three literals, such as B, corresponds to a corner of the cube.  But for a function on three variables, these are just the minterms.  As you know, if a function evaluates to 1 on any of the 8 corners of the cube, that minterm is an implicant of the function (we used this idea to construct the function to prove logical completeness).  How do these connections help us to simplify functions?  If we're careful, we can map cubes onto paper in such a way that product terms (the possible implicants of the function) usually form contiguous  groups of 1s, allowing us to spot them easily.  Let's work upwards starting from one variable to see how this idea works.  The end result is called a Karnaugh map.   The first drawing shown to the right replicates our view of  the {1-dimensional} hypercube, corresponding to the domain of a function on one variable, in this case the variable A.   To the right of the hypercube (line segment) are two variants of a Karnaugh map on one variable.  The middle variant clearly  indicates the column corresponding to the product A (the other  column corresponds to ).  The right variant simply labels the column with values for A.       The three drawings shown to the right illustrate the three possible product terms on one variable.  { The functions shown in  these Karnaugh maps are arbitrary, except that we have chosen them such that each implicant shown is a prime implicant for the illustrated function.}        Let's now look at two-variable functions.  We have replicated our drawing of the {2-dimensional} hypercube (square) to the right  along with two variants of Karnaugh maps on two variables. With only two variables (A and B), the extension is fairly  straightforward, since we can use the second dimension of the  paper (vertical) to express the second variable (B).       The number of possible products of literals grows rapidly with the  number of variables. For two variables, nine are possible, as shown to the right. Notice that all implicants have two properties.  First, they occupy contiguous regions of the grid.  And, second, their height and width are always powers of two."
    },
    {
        "question": " How many possible choices are there for the PAID state?', '\\n\\n",
        "answers": [
            "\nThere are 63 possible states for the PAID state. This is because the machine can only track up to 3.15 dollars worth of nickels, which is 63 nickels.",
            "The S_2 value specifies the number of inputs that must be satisfied in order for the output to be 1. This cuts the number of possible choices from four to two in each case because there are only two possible combinations of inputs that can satisfy this condition.",
            "The two most common types of memory used in digital systems are static random access memory (SRAM) and dynamic random access memory (DRAM). SRAM is used for high-speed applications such as processor caches and some embedded designs, while DRAM is used for applications with less demanding speed requirements, such as main memory in most computers."
        ],
        "score": [
            6.757582187652588,
            -2.376208782196045,
            -11.211503028869629
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " How does Prof. Jones decide to leverage human meaning in assigning the bit patterns?\\n",
        "answers": [
            "\nProf. Jones assigns bit patterns in a way that makes it easy for humans to understand the meaning of the states. For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since the FSM starts in that state. However, for the other four states, the meanings are clear and easy to apply.",
            "The inputs of the FSM are the states of the two roads, the timer, and the signal F.",
            "The AND gate on the left is used to determine if all of the input values are 1. If all of the input values are 1, then the output of the AND gate will be 1. If any of the input values are 0, then the output of the AND gate will be 0."
        ],
        "score": [
            6.56259822845459,
            -11.312704086303711,
            -11.159086227416992
        ],
        "text": "need not specify a value for A.  No money has been deposited, so we set  output P=0 in the START state.  We next create DIME and QUARTER states corresponding to the user having entered one coin.  The first coin should be accepted, but more money is needed, so both of these states output A=1 and P=0. When a coin of the opposite type is entered, each state moves to a state called PAID, which we use for the case in which a total of 35 cents has been received.  For now, we ignore the possibility that the same type of coin is deposited more than once.  Finally, the PAID state accepts the second coin (A=1) and indicates that the user has paid the full price of 35 cents (P=1).   We next extend our design to handle user mistakes.  If a user enters a second dime in the DIME state, our FSM should reject the coin.  We create a REJECTD state and add it as the next state from   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no PAID& & & yes& yes    DIME when a dime is entered. The REJECTD state rejects the dime (A=0) and continues to wait for a quarter (P=0).  What should we use as next  states from REJECTD?  If the user enters a third dime (or a fourth,  or a fifth, and so on), we want to reject the new dime as well.   If the user enters a quarter, we want to accept the coin, at which point we have received 35 cents (counting the first dime).  We use this reasoning to complete the description of REJECTD.  We also create an analogous state, REJECTQ, to handle a user who inserts more than one quarter.  What should happen after a user has paid 35 cents and bought  one item?  The FSM at that point is in the PAID state, which delivers the item by setting P=1.  Given that we want the FSM to allow the user to purchase another item,  how should we choose the next states from PAID?  The behavior that we want from PAID is identical to the behavior that we defined from START.  The 35 cents already  deposited was used to pay for the item delivered, so the machine is no longer holding any of the user's money.  We can thus simply set the next states from PAID to be DIME when a  dime is inserted and QUARTER when a quarter is inserted.     At this point, we make a decision intended primarily to simplify the logic needed to build the lab.  Without a physical item delivery  mechanism with a specification for how its in-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  PAID& DIME& QUARTER& yes& yes DIME& REJECTD& PAID& yes& no REJECTD& REJECTD& PAID& no& no QUARTER& PAID& REJECTQ& yes& no REJECTQ& PAID& REJECTQ& no& no    put must be driven,  the behavior of the output signal P can be fairly flexible.   For example, we could build a delivery mechanism that used the rising edge of P to open a chute.  In this case, the output P=0 in the start state is not relevant, and we can merge the state START with the state PAID.  The way that we handle P in the lab, we might find it strange to have a ``paid'' light turn on before inserting any money, but keeping the design simple enough for a first lab exercise  is more important.  Our final abstract state table appears above.  {Picking the Representation}  We are now ready to choose the state representation for the lab FSM.  With five states, we need three bits of internal state.  Prof. Jones decided to leverage human meaning in assigning the bit patterns, as follows:  {  S_2& type of last coin inserted (0 for dime, 1 for quarter) S_1& more than one quarter inserted? (1 for yes, 0 for no) S_0& more than one dime inserted? (1 for yes, 0 for no)  }   These meanings are not easy to apply to all of our states.  For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since we decided to start our FSM in that state as  well.  However, for the other four states, the meanings provide a clear"
    },
    {
        "question": " Why does this particular example prove to be slightly tricky?\\n",
        "answers": [
            "\nThis particular example is tricky because it requires the reader to understand the concept of bit-sliced addition in order to understand the passage. Furthermore, the passage does not provide a clear explanation of how the bit-sliced adder circuit works, making it difficult for the reader to follow along.",
            "The output for the third bit gray code counter would be either 0 or 1. If the input is 0, the output is 1. If the input is 1, the output is 0.",
            "The value of HOLD determines whether the counter counts or not by determining whether the counter should hold its current value (stop counting) if the ``halt'' button is pressed or if the counter was already halted and the ``go'' button wasn't pressed."
        ],
        "score": [
            5.5628814697265625,
            -10.865108489990234,
            -10.658397674560547
        ],
        "text": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is"
    },
    {
        "question": " How does specifying the S_2 value cut the number of possible choices from four to two in each case?\\n",
        "answers": [
            "\nThe S_2 value specifies the number of inputs that must be satisfied in order for the output to be 1. This cuts the number of possible choices from four to two in each case because there are only two possible combinations of inputs that can satisfy this condition.",
            "SRAM cells store data using a dual-inverter loop. When writing to a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input. When reading from a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.",
            "If a user presses both buttons (HG=11), the ice cream cup overflows. This is because the outputs are C_L=11 and C_M=10, so our dispenser releases a full serving of each flavor, or two servings total."
        ],
        "score": [
            8.282752990722656,
            -11.081428527832031,
            -10.747740745544434
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " Why might we try to leverage the fact that the next-state patterns from these two states are not relevant?\\n",
        "answers": [
            "\nOne reason to try to leverage the fact that the next-state patterns from these two states are not relevant is that it may simplify the design of the system. If the next-state patterns are not needed, then the system can be designed without them, which may make the design process simpler and faster. Additionally, eliminating the next-state patterns may also reduce the cost of the system.",
            "The choice of representation for the FSM's internal state affects the complexity of the implementation. If the states are represented as bit patterns, the transition rules must be complete. If the states are represented as abstract states, the outputs for each state must be specified.",
            "One builds memories (static RAMs) using logic by connecting a dual-inverter loop to opposing BIT lines through transistors controlled by a SELECT line. When SELECT is high, the transistors connect the inverter loop to the bit lines. When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BIT input. When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs."
        ],
        "score": [
            9.796125411987305,
            -6.575192451477051,
            -11.227306365966797
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How does the final state table and state transition diagram appear?']",
        "answers": [
            "\nThe final state table and state transition diagram appear as follows: \n\nThe final state table lists the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The table also includes the states outside of the desired loop. The lights are all red in both of these states, which is necessary for safety.\n\nThe state transition diagram shows the six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle. The diagram also shows the duration of each state.",
            "The idx variable is used to keep track of the number of operations in the logic expression. This metric is used to estimate the number of transistors needed for the design, which in turn is an estimate for the area required for the design.",
            "What is the reason for the popularity of two-level logic?\n\nThe popularity of two-level logic is due to the fact that it is a simple way to express logic functions. Additionally, two-level logic is easy to map into CMOS gates."
        ],
        "score": [
            10.021232604980469,
            -11.311025619506836,
            -11.314273834228516
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "What is the purpose of the FSM?\\n",
        "answers": [
            "\nThe purpose of the FSM is to control the stoplights at a busy intersection. The FSM ensures that the lights are always red on one or both roads, that a yellow light is shown before a green light, and that the green lights are alternated between the two roads.",
            "The signal T indicates that an interrupt has occurred. This interrupt could be generated by an external device, such as a disk drive or a keyboard, or by an error in the program being executed, such as an undefined instruction or an attempt to divide by zero.",
            "The output behavior is that the add3 procedure takes three integers as arguments, adds them together, and returns the sum. The procedure is called with the constants 10, 20, and 30, and the result is printed. By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30."
        ],
        "score": [
            7.995229721069336,
            -9.893625259399414,
            -10.542673110961914
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "What is the significance of the input sequence 111010010001?\\n",
        "answers": [
            "\nThe input sequence 111010010001 is significant because it determines the duration of each state in the FSM.",
            "The possible inputs for this FSM are a dime (T=0) or a quarter (T=1). The machine will reject any other input. If the user tries to purchase an item without inserting enough money first, the FSM will also reject the request.",
            "A Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all patterns in the second copy."
        ],
        "score": [
            8.116167068481445,
            -8.515752792358398,
            -8.971458435058594
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "What is the significance of the input sequence 001?\\n",
        "answers": [
            "\nThe input sequence 001 causes the stoplight to turn red on both roads. This is necessary for safety.",
            "There are several disadvantages to using DRAM. First, DRAM is slower than SRAM. Second, DRAM is less dense than SRAM, so it requires more chip area to store the same amount of data. Finally, DRAM cells are more complex than SRAM cells, so they are more difficult to design and manufacture.",
            "The number of state bits affects the design complexity because it determines the number of possible states that the FSM can be in. A larger number of state bits means that there are more possible states and the design must be able to handle all of them."
        ],
        "score": [
            3.9658021926879883,
            -11.240201950073242,
            -11.225255966186523
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "In what ways is this design easy to test?\\n",
        "answers": [
            "\nThe design is easy to test because it is a two-level logic design. This means that the inputs and outputs can be easily tested with a NAND or NOR gate. Additionally, the design is easy to test because it is a SOP expression. This means that the inputs and outputs can be easily tested with an OR gate.",
            "SRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors, while a DRAM cell uses only one transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor",
            "The next-state logic for a three-bit Gray code counter is as follows:\n\nS_2^+ = S_2 + S_0\nS_1^+ = S_2\nS_0^+ = S_2 S_1"
        ],
        "score": [
            6.18386697769165,
            -10.934720039367676,
            -11.248037338256836
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is scan-based testing?', '\\n\\n",
        "answers": [
            "\nScan-based testing is a method of testing where the input is scanned and converted into the appropriate result. The programmer is responsible for ensuring that the number of conversions in the format string matches the number of arguments provided. The programmer must also ensure that the type of information produced by each conversion can be stored at the address passed for that conversion.",
            "SRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors (two for each inverter, and two more to connect the inverters to the bit lines), while a DRAM cell uses only a single transistor.",
            "A tri-state buffer is a type of digital logic circuit that is used to either connect or disconnect a signal line from a power supply or ground. This is done by using two transistors, one to connect the signal line to the power supply, and the other to connect the signal line to ground. When the buffer is enabled, both transistors are turned on, and the signal line is connected to either the power supply or ground, depending on the value of the input signal. When the buffer is disabled, both transistors are turned off, and the signal line is effectively disconnected from both the power supply and ground."
        ],
        "score": [
            8.583253860473633,
            -11.064821243286133,
            -10.594491958618164
        ],
        "text": "&a, &b);   /* These have the */} > { scanf (\"d d\", &a, &b);  /* same effect.   */} effect: > try to convert two integers typed in decimal to > 2's complement and store the results in { a} and { b}  > { scanf (\"cx lf\", &c, &u, &d);} effect: > try to read an ASCII character into { c}, a value > typed in hexadecimal into { u}, and a double- > precision > floating-point number into { d}  > { scanf (\"lf f\", &d, &f);} effect: > try to read two real numbers typed as decimal, > convert the first to double-precision and store it  > in { d}, and convert the second to single-precision  > and store it in { f}     ple.  The spaces are ignored by { scanf}.  However, { any non-space characters in the format string must be typed exactly by the user!}  The remaining arguments to { scanf} specify memory addresses where the function can store the converted values.    The ampersand (``&'') in front of each variable name in the examples is an operator that returns the address of a variable in memory.  For each con-   {|c|l|} escape  &                          sequence& { scanf} function's conversion to bits  { c}& store one ASCII character (as { char}) { d}& convert decimal integer to 2's complement { f}& convert decimal real number to float { lf}& convert decimal real number to double { u}& convert decimal integer to unsigned int { x}& convert hexadecimal integer to unsigned int { X}& (as above)     version in the format string, the { scanf} function tries to convert input from the user into the appropriate result, then stores the result in memory at the address given by the next argument.  The programmer is responsible for ensuring that the number of  conversions in the format string matches the number of arguments provided (not counting the format string itself).  The programmer must also ensure that the type of information produced by each conversion can be stored at the address passed for that conversion---in other words, the address of a variable with the correct type must be provided.  Modern compilers often detect missing { &} operators and incorrect variable types, but many only give warnings to the programmer.  The { scanf} function itself cannot tell whether the arguments given to it are valid or not.  If a conversion fails---for example, if a user types ``hello'' when { scanf} expects an integer---{ scanf} does not overwrite the corresponding variable and immediately stops trying to convert input.  The { scanf} function returns the number of successful  conversions, allowing a programmer to check for bad input from the user.  {Types of Statements in C}  Each statement in a C program specifies a complete operation.  There are three types of statements, but two of these types can be constructed from additional statements, which can in turn be constructed from additional statements.  The C language specifies no bound on this type of recursive construction, but code  readability does impose a practical limit.   The three types are shown to the right. They are the { null statement},  { simple statements},  and { compound statements}.  A null statement is just a semicolon, and a compound statement  is just a sequence of statements surrounded by braces.  Simple statements can take several forms.  All of the examples shown to the right, including the call to { printf}, are simple state-   {  aaaa=aaaaaaaaaaaaaa=/* =aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa= ;   > > /* >a null statement (does nothing) >*/  A = B; > > /*  >examples of simple statements >*/ printf (\"Hello, world!n\");  {    > > /* > a compound statement >*/  >  C = D; > /* > (a sequence of statements >*/ >  N = 4; > /* > between braces) >*/  >  L = D - N; }  } {-2pt}   ments consisting of a C expression followed by a  semicolon.  Simple statements can also consist of conditionals or iterations, which we introduce next.  Remember that after variable declarations, the { main} function contains a sequence of statements.  These statements are executed one at a time in the order given in the"
    },
    {
        "question": " How can we know that a circuit that we build in the lab correctly implements the FSM that we have outlined in these notes?\\n",
        "answers": [
            "\nWe can test the circuit that we build in the lab to see if it correctly implements the FSM that we have outlined in these notes. To do this, we can input a known sequence of 0s and 1s and see if the circuit outputs the correct responses. If it does, then we can be confident that the circuit is functioning correctly.",
            "One way to transform code written in a high-level language into an FSM is to use a compiler. A compiler is a program that translates code written in a high-level language into code that can be run on a computer. Another way to transform code written in a high-level language into an FSM is to use an interpreter. An interpreter is a program that executes code written in a high-level language.",
            "A DRAM cell is refreshed by reading the contents and writing them back with active logic. This process is necessary to avoid data loss due to charge leakage from the capacitor."
        ],
        "score": [
            8.09550666809082,
            -7.587602615356445,
            -11.142114639282227
        ],
        "text": "such timing guarantees are needed, the designer simply adds state to the FSM to accommodate the need, and the problem is solved.  The coin-counting FSM that we designed for our class' lab assignments, for example,  required that we use a Moore machine model to avoid sending the servo controlling the coin's path an output pulse that was too short to enforce the FSM's decision about which way to send the coin.  By adding more states to the FSM, we were able to hold the servo in place, as desired.  Why are we protecting you from the model used in practice?  First, timing issues add complexity to a topic that is complex enough  for an introductory course.  And, second, most software FSMs are Moore machines, so the abstraction is a useful one in that context, too.  In many design contexts, the timing issues implied by a Mealy model can be relatively simple to manage.  When working in a single clock domain, all of the input signals come from flip-flops in the same  domain, and are thus stable for most of the clock cycle.  Only rarely does one need to keep additional state to improve timing characteristics in these contexts.  In contrast, when interacting across clock domains, more care is sometimes needed to ensure correct behavior.  We now illustrate the state reduction benefit of the Mealy machine model with a simple example, an FSM that recognizes the  pattern of a 0 followed by a 1 on a single input and outputs a 1 when it observes the pattern.  As already mentioned, Mealy machines often require fewer flip-flops. Intuitively, the number of combinations of states and inputs is greater than the number of combinations of states alone, and allowing a function to depend on inputs reduces the number of internal states needed.    A Mealy implementation of the FSM appears on the left below, and an example timing diagram illustrating the FSM's behavior is shown on the right.  The machine shown below occupies state A when the last bit seen was a 0, and state B when the last bit seen was a 1.  Notice that the transition arcs in the state diagram are labeled with two values instead of one.  Since outputs can depend on input values as well as state, transitions in a Mealy machine are labeled with input/output combinations, while states are labeled only with their internal bits (or just their names, as shown below).  Labeling states with outputs does not make sense for a Mealy machine, since outputs may vary with inputs.  Notice that the outputs indicated on any given transition hold only until that transition is taken (at the rising clock edge), as is apparent in the timing diagram.  When inputs are asynchronous,  that is, not driven by the same clock signal, output pulses from a Mealy machine can be arbitrarily short, which can lead to problems.    {{file=part3/figs/lec17-3.eps,width=5in}}    For a Moore machine, we must create a special state in which the output is high.  Doing so requires that we split state B into two states, a state C in which the last two bits seen were 01, and a state D in which the last two bits seen were 11.  Only state C generates output 1.  State D also becomes the starting state for the new state machine.  The state diagram on the left below illustrates  the changes, using the transition diagram style that we introduced  earlier to represent Moore machines. Notice in the associated timing diagram that the output pulse lasts a full clock cycle.    {{file=part3/figs/lec17-4.eps,width=5in}}      {Summary of Part 3 of the Course}   In this short summary, we  give you lists at several levels of difficulty  of what we expect you to be able to do as a result of the last few weeks  of studying (reading, listening, doing homework, discussing your  understanding with your classmates, and so forth).  Students often find this part of the course more challenging than the earlier parts of the course.  In addition to these notes, you should read Chapters 4 and 5 of the  Patt and Patel textbook, which cover the von Neumann"
    },
    {
        "question": " What is the purpose of scan-based testing?\\n",
        "answers": [
            "\nScan-based testing is a type of testing that is used to test the functionality of a design. This type of testing is typically used to test digital designs.",
            "The value of HOLD when the counter is halted is the value of the last input given to the machine.",
            "The color mapping for a three-bit RGB encoding system is a system where colors are represented by a combination of red, green, and blue light. The color mapping for such a system is shown to the right. Imagine that you are charged with creating a counter to drive a light through a sequence of colors."
        ],
        "score": [
            7.341132164001465,
            -11.079663276672363,
            -10.462553977966309
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What is the input sequence that is used to initialize the FSM?\\n",
        "answers": [
            "\nThe input sequence that is used to initialize the FSM is a sequence of 0s and 1s that are input into the FSM in order to set the state of the FSM to the desired state.",
            "The second version of the table implies that the buttons are not placed in the most ergonomic way possible. The buttons are too close together and there is not enough space between the buttons and the edge of the table.",
            "The fifth step in the design process is to select an internal representation for the bits necessary to encode the state of our FSM. In practice, for small designs, this representation can be selected by a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand."
        ],
        "score": [
            9.612552642822266,
            -11.272668838500977,
            -1.092827558517456
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the input sequence that is used to test the FSM?\\n",
        "answers": [
            "\nThe input sequence that is used to test the FSM is a sequence of 0s and 1s that is applied to the inputs of the FSM. This input sequence is used to determine the output of the FSM.",
            "An FSM is a finite state machine, which is a model of computation used to design both computer programs and hardware. An FSM is defined by a set of states, a set of inputs, a set of outputs, and a set of transitions between states.",
            "The implementation diagram for a three-bit Gray code counter is a logic diagram that shows the inputs, outputs, and next-state logic for the counter. The diagram is ordered from left to right as {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2, which makes it easy to read and check for correctness."
        ],
        "score": [
            9.595264434814453,
            1.6872553825378418,
            -9.396319389343262
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What outputs do we need to observe in each state to check the output logic?', '\\n\\n",
        "answers": [
            "\nThe outputs needed to observe in each state to check the output logic are the values of the state register S and the timer. These outputs can be used to determine the duration of each state and the order in which the states occur.",
            "There are three variables used in the code, which are x, y, and z. The value of x is initialized to 42, and the value of y is initialized to x. The value of z is not initialized.",
            "The input of the FSM is a 3-bit register S and a 6-bit timer. The timer is used to count down and produce an output of Z=1 when it reaches 0. The register S is used to store the state of the FSM."
        ],
        "score": [
            8.529287338256836,
            -10.017830848693848,
            -4.9215312004089355
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " Why is it difficult to test sequential logic?\\n",
        "answers": [
            "\nIt is difficult to test sequential logic because it is difficult to predict the behavior of the system. Sequential logic is difficult to test because it is difficult to determine whether the system will halt in a finite number of steps or not.",
            "If the \"halt\" button is pressed (HG=10), the logic outputs C_L=00 and C_M=00, so the dispenser gives no ice cream. If both buttons are pressed (HG=11), the outputs are C_L=11 and C_M=11, so the dispenser gives two servings of each flavor, or four servings total.",
            "One way to transform code written in a high-level language into an FSM is to use a compiler. A compiler is a program that translates code written in a high-level language into code that can be run on a computer. Another way to transform code written in a high-level language into an FSM is to use an interpreter. An interpreter is a program that executes code written in a high-level language."
        ],
        "score": [
            9.510623931884766,
            -10.275354385375977,
            -11.200222969055176
        ],
        "text": "systematic technique (or a machine) capable of solving a problem that cannot be solved by a Turing machine.  No one has been able to do so to date.   {The Halting Problem*}  You might reasonably ask whether any problems can be shown to be incomputable.  More common terms for such problems---those known to be insolvable by any computer---are { intractable} or  undecidable.  In the same 1936 paper in which he introduced the universal computing machine, Alan Turing also provided an answer to this question by introducing (and proving) that there are in fact problems that cannot be computed by a universal computing machine. The problem that he proved undecidable, using proof techniques almost identical to those developed for similar problems in the 1880s, is now known as { the halting problem}.    The halting problem is easy to state and easy to prove undecidable. The problem is this: given a Turing machine and an input to the Turing machine, does the Turing machine finish computing in a finite number of steps (a finite amount of time)?  In order to solve the problem, an answer, either yes or no, must be given in a finite amount of time regardless of the machine or input in question.  Clearly some machines never finish.  For example, we can write a Turing machine that counts upwards starting from one.  You may find the proof structure for undecidability of the halting problem easier to understand if you first think about a related problem with which you may already be familiar, the Liar's paradox (which is at least 2,300 years old).  In its stengthened form, it is the following sentence: ``This sentence is not true.''   To see that no Turing machine can solve the halting problem, we begin by assuming that such a machine exists, and then show that its existence is self-contradictory.  We call the machine the ``Halting Machine,'' or HM for short.  HM is a machine that operates on  another      Turing machine and its inputs to produce a yes or no answer in finite time: either the machine in question finishes in finite time (HM returns ``yes''), or it does not (HM returns ``no'').  The figure illustrates HM's operation.   From HM, we construct a second machine that we call the HM Inverter, or HMI.  This machine inverts the sense of the answer given by HM.  In particular, the inputs are fed directly into a copy of HM, and if HM answers ``yes,'' HMI enters an infinite loop.  If HM answers ``no,'' HMI halts.  A diagram appears to the right.  The inconsistency can now be seen by asking HM whether HMI halts when given itself as an input (repeatedly), as      shown below.  Two copies of HM are thus being asked the same question.  One copy is the rightmost in the figure below and the second is embedded in the HMI machine that we are using as the input to the rightmost HM.  As the two copies of HM operate on the same input (HMI operating on HMI), they should return the same answer: a Turing machine either halts on an input, or it does not; they are deterministic.    Let's assume that the rightmost HM tells us that HMI operating on itself halts. Then the copy of HM in HMI (when HMI executes on itself, with itself as an input) must also say ``yes.''  But this answer implies that HMI doesn't halt (see the figure above), so the answer should have been no!  Alternatively, we can assume that the rightmost HM says that HMI operating on itself does not halt.  Again, the copy of HM in HMI must give the same answer.  But in this case HMI halts, again contradicting our assumption.  Since neither answer is consistent, no consistent answer can be given, and the original assumption that HM exists is incorrect.  Thus, no Turing machine can solve the halting problem.      {Programming Concepts and the C Language}  This set of notes introduces the C programming language and explains some basic concepts in computer programming.  Our purpose in showing you a high-level language at this early stage of the course is to give you time to become familiar with the"
    },
    {
        "question": " What is scan-based testing?\\n",
        "answers": [
            "\nScan-based testing is a type of testing where the input is scanned for errors before it is processed. This type of testing is often used to test software for errors.",
            "The design is easy to test because it is a two-level logic design. This means that the inputs and outputs can be easily tested with a NAND or NOR gate. Additionally, the design is easy to test because it is a SOP expression. This means that the inputs and outputs can be easily tested with an OR gate.",
            "The second step in the design process is to think carefully about metrics of possible interest. Once you have a set of metrics that you feel is complete, your next step is to get rid of as many as you can."
        ],
        "score": [
            8.809886932373047,
            -9.054356575012207,
            -11.319381713867188
        ],
        "text": "&a, &b);   /* These have the */} > { scanf (\"d d\", &a, &b);  /* same effect.   */} effect: > try to convert two integers typed in decimal to > 2's complement and store the results in { a} and { b}  > { scanf (\"cx lf\", &c, &u, &d);} effect: > try to read an ASCII character into { c}, a value > typed in hexadecimal into { u}, and a double- > precision > floating-point number into { d}  > { scanf (\"lf f\", &d, &f);} effect: > try to read two real numbers typed as decimal, > convert the first to double-precision and store it  > in { d}, and convert the second to single-precision  > and store it in { f}     ple.  The spaces are ignored by { scanf}.  However, { any non-space characters in the format string must be typed exactly by the user!}  The remaining arguments to { scanf} specify memory addresses where the function can store the converted values.    The ampersand (``&'') in front of each variable name in the examples is an operator that returns the address of a variable in memory.  For each con-   {|c|l|} escape  &                          sequence& { scanf} function's conversion to bits  { c}& store one ASCII character (as { char}) { d}& convert decimal integer to 2's complement { f}& convert decimal real number to float { lf}& convert decimal real number to double { u}& convert decimal integer to unsigned int { x}& convert hexadecimal integer to unsigned int { X}& (as above)     version in the format string, the { scanf} function tries to convert input from the user into the appropriate result, then stores the result in memory at the address given by the next argument.  The programmer is responsible for ensuring that the number of  conversions in the format string matches the number of arguments provided (not counting the format string itself).  The programmer must also ensure that the type of information produced by each conversion can be stored at the address passed for that conversion---in other words, the address of a variable with the correct type must be provided.  Modern compilers often detect missing { &} operators and incorrect variable types, but many only give warnings to the programmer.  The { scanf} function itself cannot tell whether the arguments given to it are valid or not.  If a conversion fails---for example, if a user types ``hello'' when { scanf} expects an integer---{ scanf} does not overwrite the corresponding variable and immediately stops trying to convert input.  The { scanf} function returns the number of successful  conversions, allowing a programmer to check for bad input from the user.  {Types of Statements in C}  Each statement in a C program specifies a complete operation.  There are three types of statements, but two of these types can be constructed from additional statements, which can in turn be constructed from additional statements.  The C language specifies no bound on this type of recursive construction, but code  readability does impose a practical limit.   The three types are shown to the right. They are the { null statement},  { simple statements},  and { compound statements}.  A null statement is just a semicolon, and a compound statement  is just a sequence of statements surrounded by braces.  Simple statements can take several forms.  All of the examples shown to the right, including the call to { printf}, are simple state-   {  aaaa=aaaaaaaaaaaaaa=/* =aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa= ;   > > /* >a null statement (does nothing) >*/  A = B; > > /*  >examples of simple statements >*/ printf (\"Hello, world!n\");  {    > > /* > a compound statement >*/  >  C = D; > /* > (a sequence of statements >*/ >  N = 4; > /* > between braces) >*/  >  L = D - N; }  } {-2pt}   ments consisting of a C expression followed by a  semicolon.  Simple statements can also consist of conditionals or iterations, which we introduce next.  Remember that after variable declarations, the { main} function contains a sequence of statements.  These statements are executed one at a time in the order given in the"
    },
    {
        "question": " How does the FSM need to be initialized?\\n",
        "answers": [
            "\nThe FSM needs to be initialized by external logic filling the memory (the array of integer values), executing the FSM ``code,'' and then checking the answer.  To support this use model, we create a FSM state called { WAIT} for cycles in which the FSM has no work to do. Later, we also make use of an external input signal { START} to start the FSM execution.",
            "The internal state value for the third bit gray code counter is S_2S_1S_0=001. This is because when HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired.",
            "The stoplight controller must ensure that the lights on one or both roads are red at all times. Similarly, if a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. Finally, for fairness, the controller should alternate green lights between the two roads."
        ],
        "score": [
            8.517648696899414,
            -10.890652656555176,
            -11.241334915161133
        ],
        "text": "least a controllable number of cycles. If we cannot specify clearly when transitions occur from an FSM state, we may not be able to implement the state.    Now let's go through the flow chart and identify states.  Initialization of { min} and { idx} need not occur serially, and the result of the first comparison between { idx} and the constant 10 is known in advance, so we can merge all three operations into a single state, which we  call { INIT}.  We can also merge the updates of { min} and { idx} into a second FSM state, which we call { COPY}.  However, the update to { min}  occurs only when the comparison ({ min > value[idx]}) is true.   We can use logic to predicate execution of the update.  In other words, we  can use the output of the comparator, which is available after the comparator  has finished comparing the two values (in a high-level FSM state that we  have yet to define), to determine whether or not the register holding  { min} loads a new value in the { COPY} state.  Our model of use for this FSM involves external logic filling the memory (the array of integer values), executing the FSM ``code,'' and then checking the answer.  To support this use model, we create a FSM state  called { WAIT} for cycles in which the FSM has no work to do. Later, we also make use of an external input signal { START}  to start the FSM execution.  The { WAIT} state logically corresponds to the ``START''  bubble in the flow chart.   Only the test for the { if} statement remains.  Using a serial comparator to compare two {32-bit} values requires 32 cycles. However, we need an additional cycle to move values into our shift  registers so that the comparator can see the first bit.  Thus our single comparison operation breaks into two high-level states.  In the first state, which we call { PREP}, we copy { min} to one of the shift registers, copy { values[idx]} to the other shift register, and reset the counter that measures the cycles needed for our serial comparator.  We then move to a second high-level state, which we call { COMPARE}, in which we feed one bit per cycle from each shift register to the serial comparator.  The { COMPARE}  state   {{file=part3/figs/part3-min-flow-chart-states.eps,width=3.96in}}   executes for 32 cycles, after which the comparator produces the one-bit answer that we need, and we can move to the { COPY} state.  The association between the flow chart and the high-level FSM states is illustrated in the figure shown to the right above.   We can now also draw an abstract state diagram for our FSM, as shown to the right.  The FSM begins in the { WAIT} state.  After external logic fills the { values} array, it signals the FSM to begin by raising the { START} signal.  The FSM transitions into the  { INIT} state, and in the next cycle into the { PREP} state. From { PREP}, the FSM always moves to { COMPARE}, where it remains for 32 cycles while the serial comparator executes a comparison. After { COMPARE}, the FSM moves to the { COPY}   {{file=part3/figs/part3-min-state-diag.eps,width=3in}}   state, where it remains for one cycle.  The transition from { COPY} depends on how many loop iterations have executed.  If more loop iterations remain, the FSM moves to { PREP} to execute the next iteration.  If the loop is done, the FSM returns to { WAIT} to allow external logic to read the result of the computation.    {Laying Out Components}   Our high-level FSM design tells us what our components need to be able to do in any given cycle.  For example, when we load new values into the shift registers that provide bits to the serial comparator, we always copy  { min} into one shift register and { values[idx]} into the second. Using this information, we can put together our components and simplify our design by fixing the way in which bits flow between them.  The figure at the right shows how we can organize our components. Again, in practice, one goes back and forth thinking about states, components, and flow from state to"
    },
    {
        "question": " What is the input sequence for initialization?\\n",
        "answers": [
            "\nThe input sequence for initialization is: 1) the value in R0 is 10, 2) the value in R1 is 20, and 3) the value in R2 is 30.",
            "The START state is the initial state of the FSM. In this state, no money has been deposited yet. The FSM needs to keep track of how much money has been inserted in order to decide whether a user can purchase one of the items. That information alone is enough for the simplest machine, but let's create a machine with adjustable item prices.",
            "There are several disadvantages to using DRAM. First, DRAM is slower than SRAM. Second, DRAM is less dense than SRAM, so it requires more chip area to store the same amount of data. Finally, DRAM cells are more complex than SRAM cells, so they are more difficult to design and manufacture."
        ],
        "score": [
            7.845674514770508,
            -9.479459762573242,
            -11.251016616821289
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": " What is the input sequence for testing?']",
        "answers": [
            "\nThe input sequence for testing is as follows: first, the user is prompted to input two integers in decimal form, which are then converted to 2's complement and stored in variables 'a' and 'b'. Next, the user is prompted to input an ASCII character, a value in hexadecimal, and a double-precision floating-point number, which are then stored in variables 'c', 'u', and 'd', respectively. Finally, the user is prompted to input two real numbers in decimal form, which are then converted to single- and double-precision floating-point numbers and stored in variables 'f' and 'd', respectively.",
            "The next-state values for S_2, S_1, and S_0 are all 1.",
            "Tri-state buffers were used to reduce the number of pins needed on chips by combining inputs and outputs. This allowed chip designers to halve the number of pins needed."
        ],
        "score": [
            8.721467018127441,
            -10.042171478271484,
            -10.734672546386719
        ],
        "text": "&a, &b);   /* These have the */} > { scanf (\"d d\", &a, &b);  /* same effect.   */} effect: > try to convert two integers typed in decimal to > 2's complement and store the results in { a} and { b}  > { scanf (\"cx lf\", &c, &u, &d);} effect: > try to read an ASCII character into { c}, a value > typed in hexadecimal into { u}, and a double- > precision > floating-point number into { d}  > { scanf (\"lf f\", &d, &f);} effect: > try to read two real numbers typed as decimal, > convert the first to double-precision and store it  > in { d}, and convert the second to single-precision  > and store it in { f}     ple.  The spaces are ignored by { scanf}.  However, { any non-space characters in the format string must be typed exactly by the user!}  The remaining arguments to { scanf} specify memory addresses where the function can store the converted values.    The ampersand (``&'') in front of each variable name in the examples is an operator that returns the address of a variable in memory.  For each con-   {|c|l|} escape  &                          sequence& { scanf} function's conversion to bits  { c}& store one ASCII character (as { char}) { d}& convert decimal integer to 2's complement { f}& convert decimal real number to float { lf}& convert decimal real number to double { u}& convert decimal integer to unsigned int { x}& convert hexadecimal integer to unsigned int { X}& (as above)     version in the format string, the { scanf} function tries to convert input from the user into the appropriate result, then stores the result in memory at the address given by the next argument.  The programmer is responsible for ensuring that the number of  conversions in the format string matches the number of arguments provided (not counting the format string itself).  The programmer must also ensure that the type of information produced by each conversion can be stored at the address passed for that conversion---in other words, the address of a variable with the correct type must be provided.  Modern compilers often detect missing { &} operators and incorrect variable types, but many only give warnings to the programmer.  The { scanf} function itself cannot tell whether the arguments given to it are valid or not.  If a conversion fails---for example, if a user types ``hello'' when { scanf} expects an integer---{ scanf} does not overwrite the corresponding variable and immediately stops trying to convert input.  The { scanf} function returns the number of successful  conversions, allowing a programmer to check for bad input from the user.  {Types of Statements in C}  Each statement in a C program specifies a complete operation.  There are three types of statements, but two of these types can be constructed from additional statements, which can in turn be constructed from additional statements.  The C language specifies no bound on this type of recursive construction, but code  readability does impose a practical limit.   The three types are shown to the right. They are the { null statement},  { simple statements},  and { compound statements}.  A null statement is just a semicolon, and a compound statement  is just a sequence of statements surrounded by braces.  Simple statements can take several forms.  All of the examples shown to the right, including the call to { printf}, are simple state-   {  aaaa=aaaaaaaaaaaaaa=/* =aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa= ;   > > /* >a null statement (does nothing) >*/  A = B; > > /*  >examples of simple statements >*/ printf (\"Hello, world!n\");  {    > > /* > a compound statement >*/  >  C = D; > /* > (a sequence of statements >*/ >  N = 4; > /* > between braces) >*/  >  L = D - N; }  } {-2pt}   ments consisting of a C expression followed by a  semicolon.  Simple statements can also consist of conditionals or iterations, which we introduce next.  Remember that after variable declarations, the { main} function contains a sequence of statements.  These statements are executed one at a time in the order given in the"
    },
    {
        "question": " What is an FSM?\\n",
        "answers": [
            "\nAn FSM is a finite state machine, which is a model of computation used to design both computer programs and hardware. An FSM is defined by a set of states, a set of inputs, a set of outputs, and a set of transitions between states.",
            "The input behavior of the machine is based on the type of coin inserted and the selection of an item for purchase. If the user inserts a coin that is not a nickel, the machine will reject the coin and remain in the same state. If the user inserts a nickel, the machine will transition to the next state and accept the coin. If the user selects an item for purchase, the machine will release the item if the user has inserted enough money. Otherwise, the machine will remain in the same state.",
            "A computer memory is a group of storage elements and the logic necessary to move data in and out of the elements. The size of the elements in a memory—called the addressability of the memory—varies from a single binary digit, or bit, to a byte (8 bits) or more. Typically, we refer to data elements larger than a byte as words, but the size of a word depends on context."
        ],
        "score": [
            7.534887790679932,
            -11.145246505737305,
            -9.989744186401367
        ],
        "text": "level state, to simplify the implementation.} {Use counters to insert time-based transitions between states (such  as timeouts).} {Implement an FSM using logic components such as registers,  counters, comparators, and adders as building blocks.} {Explain the basic organization of a computer's microarchitecture as well as the role played by elements of a von Neumann design in the processing of instructions.} {Identify the stages of processing an instruction (such as fetch, decode, getting operands, execution, and writing back results) in a  processor control unit state machine diagram.}   And, at the highest level, we expect that you will be able to do the following:  {}{{}{} {}{}{}  {Explain the difference between the Moore and Mealy machine models,  as well as why you might find each of them useful when designing an FSM.} {Understand the need for initialization of an FSM, be able to analyze  and identify potential problems arising from lack of initialization, and  be able to extend an implementation to include initialization to an  appropriate state when necessary.} {Understand how the choice of internal state bits for an FSM can  affect the complexity of the implementation of next-state and output  logic, and be able to select a reasonable state assignment.} {Identify and fix design flaws in simple FSMs by analyzing an existing  implementation, comparing it with the specification, and removing any  differences by making any necessary changes to the implementation.}      {   }  empty 3rd page      {Instruction Set Architecture*}  This set of notes discusses  tradeoffs and design elements of instruction set architectures (ISAs). { The material is beyond the scope of our class, and is provided purely for your interest.}  Those who find these topics interesting may also want to read the ECE391 notes, which describe similar material with a focus on the x86 ISA.  As you know, the ISA defines the interface between software and hardware, abstracting the capabilities of a computer's datapath and standardizing the format of instructions to utilize those capabilities.  Successful ISAs are rarely discarded, as success implies the existence of large amounts of software built to use the ISA.  Rather, they are extended, and their original forms must be supported for decades (consider, for example, the IBM 360 and the Intel x86).  Employing sound design principles is thus imperative in an ISA.  {Formats and Fields*}  The LC-3 ISA  employs fixed-length instructions and a load-store architecture, two aspects that help to reduce the design space to a manageable set of choices.  In a general ISA design, many other options exist for instruction formats.  Recall the idea of separating the bits of an instruction into (possibly non-contiguous) fields.  One of the fields must contain an opcode, which specifies the type of operation to be performed by the instruction.  In the {LC-3} ISA, most opcodes specify both the type of operation and the types of arguments to the operation.   More generally, many addressing modes are possible for each operand, and we can think of the bits that specify the addressing mode as a separate field, known as the { mode} field.    As a simple example, the {LC-3's} ADD and AND instructions  contain a {1-bit} mode field that specifies whether the second operand of the ADD/AND comes from a register or is an immediate value.  Several questions must be answered in order to define the possible instruction formats for an ISA.  First, are instructions fixed-length or variable-length?  Second, how many addresses are needed for each instruction, and how many of the addresses can be memory addresses? Finally, what forms of addresses are possible for each operand? For example, can one use full memory addresses or only limited offsets relative to a register?  The answer to the first question depends on many factors, but several clear advantages exist for both answers.  { Fixed-length instructions} are easy to fetch and decode.  A"
    },
    {
        "question": " What is an encoder?\\n",
        "answers": [
            "\nAn encoder is a device that converts a signal or message into a code. The code is usually a binary code, which is a sequence of 0s and 1s. The code can be used to represent the signal or message in a form that can be read by a machine, such as a computer.",
            "Static random access memory is used for high-speed applications such as processor caches and some embedded designs. SRAM bit density is significantly lower than DRAM bit density, most applications with less demanding speed requirements use DRAM.",
            "One way to build an FSM that allows the pieces to be executed to be specified after the FSM is built is to use a state machine that has an output that can be routed to different inputs. This allows the FSM to be configured to execute different pieces of code based on the input. Another way to build an FSM that allows the pieces to be executed to be specified after the FSM is built is to use a state machine that has multiple states that can be executed in parallel. This allows the FSM to be configured to execute different pieces of code in parallel based on the input."
        ],
        "score": [
            7.4175310134887695,
            -11.095864295959473,
            -10.968892097473145
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " What is a decoder?\\n",
        "answers": [
            "\nA decoder is a logic circuit that converts a binary code into a corresponding decimal code. Decoders are used in a variety of applications, including data storage and transmission, error detection and correction, and address decoding.",
            "The meaning assigned to the bit patterns makes it easier to understand the behavior of the FSM. For example, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since we decided to start our FSM in that state as well. However, for the other four states, the meanings provide a clear understanding of the state of the FSM.",
            "A priority encoder is a circuit that takes in a number of input signals, and outputs a code that represents the highest priority input signal. The output code is usually a binary number."
        ],
        "score": [
            7.260974407196045,
            -11.301946640014648,
            -8.300530433654785
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How do you design an FSM?\\n",
        "answers": [
            "\nOne way to design an FSM is to start with a flow chart of the program that is to be implemented. This flow chart can then be used to identify high-level states that can be implemented in a fixed number of clock cycles. Another way to design an FSM is to start with a list of the components that are needed to implement the program. This list can then be used to identify high-level states that can be implemented using those components.",
            "The state representation has a significant impact on the overall design of the system. It affects the way the system behaves and how it interacts with its environment. A well-chosen state representation can simplify the design of the system and make it more efficient.",
            "We can test the circuit that we build in the lab to see if it correctly implements the FSM that we have outlined in these notes. To do this, we can input a known sequence of 0s and 1s and see if the circuit outputs the correct responses. If it does, then we can be confident that the circuit is functioning correctly."
        ],
        "score": [
            8.071885108947754,
            -10.794363021850586,
            0.23213696479797363
        ],
        "text": "statement initializes the minimum known value ({ min}) to the value stored at index 0 in the  array ({ values[0]}). The second statement is a loop in which the variable { index}  takes on values from 1 to 9.  For each value, an { if} statement compares the current known minimum with the value stored in the array at index given by the { idx} variable.  If the stored value is smaller, the current known  value (again, { min}) is updated to reflect the program's having found a smaller value.  When the loop finishes all nine iterations, the variable { min} holds the smallest value among the set of 10  integers stored in the { values} array.   As a first step towards designing an FSM to implement the code, we transform the code into a flow chart, as shown to the right.  The program again begins with initialization, which appears in the second column of the flow chart.   The loop in the program translates to the third column of the flow chart,  and the { if} statement to the middle comparison and update  of { min}.  Our goal is now to design an FSM to implement the flow chart.  In order to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless entry system with a timer.  Although the timer's value was technically also   {{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}   part of the FSM's state, we treated it as data and integrated it into our next-state decisions in only a couple of cases.  For our minimum value problem, we have two sources of data.  First, an external program supplies data in the form of a set of 10 integers.  If we assume {32-bit} integers, these data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value.  These are technically state bits.  For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it  using the components that we have developed earlier in our course.  {Choosing Components and Identifying States}  Now we are ready to design an FSM that implements the flow chart. What components do we need, other than our state logic? We use registers and counters to implement the variables { idx} and { min} in the program. For the array { values}, we use a {1632-bit}  memory.{We technically only need a {1032-bit}  memory, but we round up the size of the address space to reflect more realistic memory designs; one can always optimize later.} We need a comparator to implement the test for the { if} statement. We choose to use a serial comparator, which allows us to illustrate again how one logical high-level state can be subdivided into many actual states. To operate the serial comparator, we make use of two shift registers that  present the comparator with one bit per cycle on each input, and a counter to keep track of the comparator's progress.  How do we identify high-level states from our flow chart?  Although the flow chart attempts to break down the program into `simple' steps, one step of a flow chart may sometimes require more than one state in an FSM.  Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities.  How we map flow chart elements into FSM states also depends to some  degree on what components we use, which is why we began with some discussion of components.  In practice, one can go back and forth between the two,  adjusting components to better match the high-level states, and adjusting  states to better match the desired components.  Finally, note that we are only concerned with high-level states, so we do  not need to provide details (yet) down to the level of individual clock  cycles, but we do want to define high-level states that can be implemented in a fixed number of cycles, or at"
    },
    {
        "question": " What is a digital FSM?', \"\\n",
        "answers": [
            "\nA digital FSM is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.",
            "The input behavior of the machine is based on the type of coin inserted and the selection of an item for purchase. If the user inserts a coin that is not a nickel, the machine will reject the coin and remain in the same state. If the user inserts a nickel, the machine will transition to the next state and accept the coin. If the user selects an item for purchase, the machine will release the item if the user has inserted enough money. Otherwise, the machine will remain in the same state.",
            "The two-bit Gray code is a sequence of two-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit."
        ],
        "score": [
            7.964992523193359,
            -11.250040054321289,
            -10.676053047180176
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " How does an encoder help to implement an FSM design?\\n",
        "answers": [
            "\nAn encoder helps to implement an FSM design by translating a user's input into a signal that can be used by the FSM. The encoder produces a signal P indicating that the user is trying to make a purchase, and a two-bit signal S indicating which item the user wants. The decoder then takes the signal S and produces the individual item release signals R_1, R_2, and R_3.",
            "A fully-specified transition diagram is a diagram that specifies all of the states that a system can be in, as well as all of the transitions between those states. This allows for a clear understanding of the behavior of the system.",
            "The decoder generates the release signals for the vending machine by connecting the signal S produced by the priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input. The outputs D_1, D_2, and D_3 then correspond to the individual item release signals R_1, R_2, and R_3 for our vending machine."
        ],
        "score": [
            9.533536911010742,
            -11.105029106140137,
            -8.047836303710938
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " How is a decoder used in an FSM design?\\n",
        "answers": [
            "\nA decoder is used in an FSM design to generate the release signals for the vending machine by connecting the signal S produced by the priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input. The outputs D_1, D_2, and D_3 then correspond to the individual item release signals R_1, R_2, and R_3 for our vending machine.",
            "The stoplight controller must ensure that the lights on one or both roads are red at all times. Similarly, if a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. Finally, for fairness, the controller should alternate green lights between the two roads.",
            "A tri-state buffer is a digital circuit that can be used to either connect or disconnect a signal from a circuit. The buffer has three states: high, low, and high impedance. The high impedance state effectively disconnects the signal from the circuit. This allows multiple memories to share the same output line without the need for additional multiplexers."
        ],
        "score": [
            8.94320297241211,
            -11.232973098754883,
            -9.521909713745117
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " What is a digital FSM?\\n",
        "answers": [
            "\nA digital FSM is a finite state machine that is implemented as a digital system. This means that all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits. In addition, transition rules must be complete, meaning that given any state of the FSM and any pattern of input bits, a transition must be defined to another state.",
            "If A were to rise with T and fall with the rising clock edge, then the processor would not be able to recognize interrupts between instructions. This would cause the program to terminate unexpectedly.",
            "A decoder is a type of logic circuit that takes an input signal and produces one output for each possible value of the signal. A decoder is similar to a multiplexer in that it has a number of inputs and outputs. However, a decoder only has one input signal, while a multiplexer has multiple input signals."
        ],
        "score": [
            8.040966033935547,
            -11.321098327636719,
            -10.539267539978027
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " What is an example of a logic puzzle that could be implemented as a children's game?\\n",
        "answers": [
            "\nThe game revolves around an old logic problem in which a farmer must cross a river in order to reach the market. The farmer is traveling to the market to sell a fox, a goose, and some corn. The farmer has a boat, but the boat is only large enough to carry the fox, the goose, or the corn at a time. The fox cannot be left alone with the goose, or the fox will eat the goose. The goose cannot be left alone with the corn, or the goose will eat the corn. How can the farmer get all three items to the market?",
            "The output of the FSM is a sequence of red, yellow, and green lights that alternate between the two roads. The duration of each state is determined by the signal F. If F is 0, the timer is set only when the state S changes. If F is 1, the timer is set when the state S changes or when the light on one of the roads is green.",
            "Tri-state buffers were used to reduce the number of pins needed on chips by combining inputs and outputs. This allowed chip designers to halve the number of pins needed."
        ],
        "score": [
            -6.714736461639404,
            -11.204874038696289,
            -11.228063583374023
        ],
        "text": "C_ to 0, we created  a problem for items that cost nothing (0 nickels): in that case, C_ is always 0.  We could instead store -P_1-1 in PRICE1 (and so forth) and feed P in to C_, but maybe it's better not to allow free items.   How can we support coin insertion?  Let's use the same adder to add each inserted coin's value to N.  The table at the right shows the value of each coin as a {5-bit} unsigned number of nickels. Using this table, we can fill in {K-maps} for each bit of V, as shown below.  Notice that we have marked the two undefined bit patterns for the coin type C as don't cares in the {K-maps}.   {l|c|c} {c|}{coin type}& C_2C_1C_0& V_4V_3V_2V_1V_0  none&        110& 00000 nickel&      010& 00001 dime&        000& 00010 quarter&     011& 00101 half dollar& 001& 01010 dollar&      111& 10100                  Solving the {K-maps} gives the following equations, which we implement as shown to the right.  {eqnarray*} V_4 &=& C_2C_0 V_3 &=& {C_1}C_0 V_2 &=& C_1C_0 V_1 &=& {C_1} V_0 &=& {C_2}C_1 {eqnarray*}       Now we can extend the design to handle coin insertion, as shown to the right with new elements highlighted in blue.  The output of the coin value  calculator is extended with a leading 0 and then fed into a {12-to-6} mux. When a purchase is requested, P=1 and the mux forwards the item price to  the adder---recall that we chose to give purchases priority over coin insertion.  When no purchase is requested, the value of any coin inserted (or 0 when no coin is inserted) is passed to the adder.  Two new gates have been added on the lower left.  First, let's verify that purchases work as before.  When a purchase is requested, P=1, so the NOR gate outputs 0, and the OR gate simple forwards R to control the mux that decides whether the purchase was successful, just as in our original design.  When no purchase is made (P=0, and R=0), the adder adds the value of  any inserted coin to N.  If the addition overflows, C_=1, and the output of the NOR gate is 0.  Note that the NOR gate output is stored as the output A in the next cycle, so a coin that causes overflow in the amount of money stored is rejected.  The OR gate also outputs 0, and N remains unchanged.  If the addition does not overflow, the NOR gate outputs a 1, the coin is accepted (A=1 in the next cycle), and  the mux allows the sum N+V to be written back as the new value of N.      The tables at the top of the next page define all of the state variables, inputs, outputs, and internal signals used in the design, and list the number of bits for each variable.     {|ccl|} {|r|}{{ FSM state}} PRICE1& 6& negated price of item 1 (-P_1) PRICE2& 6& negated price of item 2 (-P_2) PRICE3& 6& negated price of item 3 (-P_3) N& 6& value of money in machine (in nickels) A& 1& stored value of accept coin output R_1& 1& stored value of release item 1 output R_2& 1& stored value of release item 2 output R_3& 1& stored value of release item 3 output  {|r|}{{ internal signals}} V& 5& inserted coin value in nickels P& 1& purchase requested (from priority encoder) S& 2& item # requested (from priority encoder) R& 1& release item (purchase approved)    {|ccl|} {|r|}{{ inputs}} B_1& 1& item 1 selected for purchase B_2& 1& item 2 selected for purchase B_3& 1& item 3 selected for purchase C& 3& coin inserted (see earlier table  & & for meaning)  {|r|}{{ outputs}} A& 1& accept inserted coin (last cycle) R_1& 1& release item 1 R_2& 1& release item 2 R_3& 1& release item 3 {|l|}{{ Note that outputs correspond one-to-one}} {|l|}{{ with four bits of FSM state.}}      {Design of a Game Controller}  For the next example, imagine that you are part of a team building a game for children to play at Engineering Open House.  The game revolves around an old logic problem in which a farmer must cross a river in order to reach the market.  The farmer is traveling to the market to sell a fox, a goose, and some corn.  The farmer has a boat, but the boat is only large enough to carry the fox, the goose, or the"
    },
    {
        "question": " How does a digital FSM control the stoplights at the intersection of two roads?\", '\\n",
        "answers": [
            "\nA digital FSM is used to control the stoplights at the intersection of two roads. The FSM has two inputs, S and F, and two outputs, L1 and L0. The output of the FSM is used to control the stoplights. The FSM is designed so that the stoplights are red on one or both roads at all times. If a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. Finally, for fairness, the controller should alternate green lights between the two roads.",
            "A sense amplifier is a circuit that is used to read the data stored in a memory cell. The sense amplifier amplifies the small signal from the memory cell and converts it into a digital signal that can be read by the processor.",
            "Abstraction is the process of simplifying a complex system by identifying the most important aspects of that system and ignoring the rest. In the context of implementation, abstraction can be used to simplify a design by identifying the most important features of the design and ignoring the rest. This can make the design more manageable and easier to understand."
        ],
        "score": [
            8.919846534729004,
            -11.00302505493164,
            -11.191448211669922
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How would you design an FSM to control a vending machine?\\n",
        "answers": [
            "\nThe FSM should be designed to handle purchases, with the current number of nickels stored in a register. Each cycle, the register is fed into an adder, which subtracts the price of any purchase requested in that cycle. The priority encoder's S signal is then used to select the value of one of three registers as the second input to the adder. The adder calculates a carry out signal C_, which is used to indicate a successful purchase. The signal R is then used to select the next value of N using a mux, and is also used to enable a decoder that generates the three individual item release outputs.",
            "The more general model of a computer is a system that can execute a set of instructions, called a program, and store the resulting data. The system has a set of registers that hold data and instructions, and a set of memory locations that store data and programs. The system also has a set of input and output devices that allow it to interact with the outside world.",
            "The FSM can get into unexpected states if the inputs are not set properly. For example, if the input is set to 111, the FSM will stay in the WHITE state indefinitely."
        ],
        "score": [
            2.409729480743408,
            -11.249293327331543,
            -4.733395576477051
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " How would you design a game controller for a logic puzzle?\\n",
        "answers": [
            "\nOne could design a game controller for a logic puzzle that would have buttons corresponding to the different operations that could be performed on the logic puzzle. For example, there could be a button to add a new element to the puzzle, a button to remove an element from the puzzle, and a button to change the value of an element in the puzzle. The game controller could also have a display that would show the current state of the puzzle and the player's progress.",
            "Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).",
            "There are 63 possible states for the PAID state. This is because the machine can only track up to 3.15 dollars worth of nickels, which is 63 nickels."
        ],
        "score": [
            8.802335739135742,
            -11.257207870483398,
            -11.32059383392334
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How would you analyze a digital FSM designed to control the stoplights at the intersection of two roads?']",
        "answers": [
            "\nThe stoplight controller must ensure that the lights on one or both roads are red at all times. Similarly, if a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. Finally, for fairness, the controller should alternate green lights between the two roads.",
            "One could design a game controller for a logic puzzle that would have buttons corresponding to the different operations that could be performed on the logic puzzle. For example, there could be a button to add a new element to the puzzle, a button to remove an element from the puzzle, and a button to change the value of an element in the puzzle. The game controller could also have a display that would show the current state of the puzzle and the player's progress.",
            "The FSM for the lab is a coin-counting FSM that requires the use of a Moore machine model to avoid sending the servo controlling the coin's path an output pulse that was too short to enforce the FSM's decision about which way to send the coin. By adding more states to the FSM, the servo can be held in place, as desired."
        ],
        "score": [
            1.6500015258789062,
            -11.206375122070312,
            -5.379084587097168
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " ",
        "answers": [
            "\nWhat are some of the ideas that can be used to reduce the number of metrics?\n\nSome of the ideas that can be used to reduce the number of metrics include: identifying metrics that have no practical impact in current technology, setting threshold values for other metrics to simplify reasoning about them, eliminating redundant metrics, calculating linear sums to reduce the count of metrics, and making use of the notion of Pareto optimality.",
            "A is an input operand for the AND function. The AND function evaluates to 1 if all input operands are equal to 1. A is an input operand for the OR function. The OR function evaluates to 1 if any input operand is equal to 1.",
            "The K-map represents the domain of a function in the form of a hypercube. The hypercube is divided into regions, each of which corresponds to a product of literals. The function is then evaluated at each point in the hypercube, and the product of literals is an implicant of the function if the function evaluates to 1 at that point."
        ],
        "score": [
            5.855097770690918,
            7.069773197174072,
            1.3879306316375732
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the purpose of the FSM?\\n\\n",
        "answers": [
            "\nThe purpose of the FSM is to control a stoplight. The stoplight must be red on one or both roads at all times, and the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. The controller should also alternate green lights between the two roads.",
            "A decoder is a logic circuit that converts a binary code into a corresponding decimal code. Decoders are used in a variety of applications, including data storage and transmission, error detection and correction, and address decoding.",
            "A bad state representation can lead to wasted engineering effort, as well as products that are cancelled due to poor power consumption. In some cases, two metrics may be strongly correlated, meaning that a design that is good for one of the metrics is frequently also good for the other metric."
        ],
        "score": [
            7.552268981933594,
            -7.49348258972168,
            -10.680763244628906
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " ",
        "answers": [
            "\nPASSAGE - compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column",
            "The player loses the game if they are not able to get the correct answer to the question. If the player gets the correct answer, they win the game.",
            "The light sometimes glows cyan or red briefly before appearing white because the controller must ensure that the lights on one or both roads are red at all times. Similarly, if a road has a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down."
        ],
        "score": [
            -2.4600985050201416,
            6.744832515716553,
            3.7569403648376465
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What are the inputs and outputs of the FSM?\\n\\n",
        "answers": [
            "\nThe inputs to the FSM are the states of the two roads, and the outputs are the colors of the lights for the two roads. The FSM ensures that the lights on one or both roads are red at all times, and that the lights on the other road are green. The FSM also alternates green lights between the two roads.",
            "The more general model of a computer is a system that can execute a set of instructions, called a program, and store the resulting data. The system has a set of registers that hold data and instructions, and a set of memory locations that store data and programs. The system also has a set of input and output devices that allow it to interact with the outside world.",
            "The FSM ensures that the lights on one or both roads are red at all times by keeping track of the state of the light. If the light is green, it will turn yellow and then red. If the light is red, it will stay red."
        ],
        "score": [
            7.346213340759277,
            -8.567244529724121,
            -1.7845127582550049
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " ",
        "answers": [
            "\nPASSAGE - compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column",
            "The next-state logic for S_1^+ and S_0^+ when HOLD=0 is as follows:\n\nS_1^+ = S_0 + HOLD\n\nS_0^+ = S_1 + HOLD",
            "A finite state machine is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs. In any given state, a particular input may cause the FSM to move to another state; this combination is called a transition rule."
        ],
        "score": [
            -2.4600985050201416,
            -1.6348613500595093,
            5.698890686035156
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the state representation of the FSM?\\n\\n",
        "answers": [
            "\nThe FSM has a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.",
            "A good state representation is important because it can help reduce the number of transistors needed for a design, and thus save space and energy. Additionally, it can help improve the performance of a design by making it easier for the computer to manipulate the logic expressions.",
            "The output for the third bit gray code counter would be either 0 or 1. If the input is 0, the output is 1. If the input is 1, the output is 0."
        ],
        "score": [
            -3.204864501953125,
            -2.691138744354248,
            -10.956890106201172
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " ",
        "answers": [
            "\nWhy is two-level logic popular?\n\nTwo-level logic is popular because it is simple to map expressions into CMOS gates. Additionally, it is easy to understand gate diagrams.",
            "The similarity between the equations for S_1^+S_0^+ and the equations for a {2-to-1} mux is that both can be used to represent a logic function. The two-level logic is a popular way of expressing logic functions because it is simple to map our expression into CMOS gates. Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.",
            "The addressability of a computer memory is the number of bits that can be stored in each element of the memory. The size of the elements in a memory can vary from a single binary digit, or bit, to a byte (8 bits) or more. Typically, data elements larger than a byte are referred to as words, but the size of a word depends on context."
        ],
        "score": [
            7.08111572265625,
            2.602353811264038,
            -2.2539453506469727
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What are the conditions for each state transition?\\n\\n",
        "answers": [
            "\nThe state of the FSM has been split into two pieces: a 3-bit register S and a 6-bit timer. The timer is simply a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle. We can thus think of transitions in the FSM on a cycle by cycle basis as consisting of two types. The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S for some number of cycles.",
            "The next-state values for a three-bit Gray code counter are as follows: S_2^+=0, S_1^+=S_0, and S_0^+={{S_1}}.",
            "The control structure of the code maps into a high-level FSM design by breaking down the program into simple steps, one step of a flow chart may sometimes require more than one state in an FSM. Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously."
        ],
        "score": [
            -7.548596382141113,
            -10.38851547241211,
            -11.060829162597656
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " ",
        "answers": [
            "\n1. What are the three ranges of possible measurement values for a given metric?\n\nThe three ranges of possible measurement values for a given metric are values that are equivalently useful, ordered and interesting with respect to one another, and impossible to use in practice.",
            "The input sequence for initialization is: 1) the value in R0 is 10, 2) the value in R1 is 20, and 3) the value in R2 is 30.",
            "A tri-state buffer is a digital circuit that can be used to either connect or disconnect an output from a signal line. The buffer has three states: high, low, and high impedance. When the buffer is in the high state, the output is connected to the signal line and copies the signal from the input. When the buffer is in the low state, the output is disconnected from the signal line. When the buffer is in the high impedance state, the output is effectively disconnected from the signal line."
        ],
        "score": [
            5.006490707397461,
            2.410306215286255,
            7.19427490234375
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the priority order of the input events?', '\\n\\n",
        "answers": [
            "\nInterrupts have priority over exceptions. This is because interrupts are generated by external devices, while exceptions are generated by the processor itself. Interrupts are also asynchronous, while exceptions are not.",
            "A piece of code can be transformed into an FSM by first mapping the desired task into a high-level programming language, then using components such as registers, counters, and memories to implement the variables needed. In this approach, the control structure of the code maps into a high-level FSM design.",
            "The game revolves around an old logic problem in which a farmer must cross a river in order to reach the market. The farmer is traveling to the market to sell a fox, a goose, and some corn. The farmer has a boat, but the boat is only large enough to carry the fox, the goose, or the corn at a time. The fox cannot be left alone with the goose, or the fox will eat the goose. The goose cannot be left alone with the corn, or the goose will eat the corn. How can the farmer get all three items to the market?"
        ],
        "score": [
            -8.665807723999023,
            -11.142932891845703,
            -11.27836799621582
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": " What is the purpose of the vending machine FSM?\\n\\n",
        "answers": [
            "\nThe vending machine FSM is designed to control the release of items from the vending machine based on the user's purchase request. The FSM uses a priority encoder to determine which item the user has requested, and a decoder to generate the individual item release signals.",
            "The next state value for S1 is 1. This is because the value of S1 is determined by the value of S0, and the value of S0 is 1.",
            "The value of register PRICE1 is the price of the first item in the machine, which is determined by the owner of the machine."
        ],
        "score": [
            7.439291477203369,
            -11.12714958190918,
            -10.226509094238281
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " How many states does the FSM have?\\n\\n",
        "answers": [
            "\nThe FSM has five parts: a finite set of states, a set of possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs. In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.",
            "An encoder is a device that converts a signal or message into a code. The code is usually a binary code, which is a sequence of 0s and 1s. The code can be used to represent the signal or message in a form that can be read by a machine, such as a computer.",
            "The two states in the first column of the table are the states of the FSM when F=0. In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes."
        ],
        "score": [
            3.496448040008545,
            -11.058713912963867,
            3.6529955863952637
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": " What is the purpose of the coin accept bit and item release bits?\\n\\n",
        "answers": [
            "\nThe coin accept bit and item release bits are used to indicate whether a coin should be accepted and whether an item should be released, respectively.",
            "A decoder is a device that converts coded signals into usable information. Decoders are used in a variety of applications, such as error detection and correction.",
            "The FSM needs at least three states in order to have eight states. This is because there are three possible items that can be purchased, and each item can be purchased in one of two ways (with or without enough money)."
        ],
        "score": [
            8.477201461791992,
            -10.492130279541016,
            -11.012413024902344
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " How are the transitions from all states well-defined?\\n\\n",
        "answers": [
            "\nThe transitions from all states are well-defined because the FSM has a timer that counts down and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.",
            "The input sequence for initialization is: 1) the value in R0 is 10, 2) the value in R1 is 20, and 3) the value in R2 is 30.",
            "There are three variables used in the code, which are x, y, and z. The value of x is initialized to 42, and the value of y is initialized to x. The value of z is not initialized."
        ],
        "score": [
            8.132719993591309,
            -10.981376647949219,
            -11.010625839233398
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the purpose of the numeric representation for the money held?', '\\n\\n",
        "answers": [
            "\nThe numeric representation for the money held is used to track the value of money inserted into the machine. This allows the machine to keep track of how much money has been inserted and to reject coins if the value is too high.",
            "A piece of code can be transformed into an FSM by first mapping the desired task into a high-level programming language, then using components such as registers, counters, and memories to implement the variables needed. In this approach, the control structure of the code maps into a high-level FSM design.",
            "The value of S_1S_0 when the counter is in the COUNT A state and HOLD=1 is 01."
        ],
        "score": [
            7.862654209136963,
            -11.088335037231445,
            -5.354312896728516
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What is the purpose of the FSM?\\n\\n",
        "answers": [
            "\nThe FSM is responsible for controlling the stoplights at a busy intersection. It ensures that the lights are always red on one or both roads, that a green light is always followed by a yellow light, and that the green lights are alternated between the two roads.",
            "A tri-state buffer is used to create an electrical connection between the tri-state buffer and the output. This logical disconnection is achieved by using the outer transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT.",
            "The FSM for the lab is a coin-counting FSM that requires the use of a Moore machine model to avoid sending the servo controlling the coin's path an output pulse that was too short to enforce the FSM's decision about which way to send the coin. By adding more states to the FSM, the servo can be held in place, as desired."
        ],
        "score": [
            5.56013298034668,
            -8.696197509765625,
            3.655008316040039
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What does the FSM need to keep track of?\\n\\n",
        "answers": [
            "\nThe FSM needs to keep track of the loop index, the current minimum value, and the progress of the serial comparator.",
            "A decoder is a device that converts coded signals into usable information. Decoders are used in a variety of applications, such as error detection and correction.",
            "An encoder is a device that converts a signal or message into a code. The code is usually a binary code, which is a sequence of 0s and 1s. The code can be used to represent the signal or message in a form that can be read by a machine, such as a computer."
        ],
        "score": [
            7.225207328796387,
            -10.665374755859375,
            -10.80190372467041
        ],
        "text": "statement initializes the minimum known value ({ min}) to the value stored at index 0 in the  array ({ values[0]}). The second statement is a loop in which the variable { index}  takes on values from 1 to 9.  For each value, an { if} statement compares the current known minimum with the value stored in the array at index given by the { idx} variable.  If the stored value is smaller, the current known  value (again, { min}) is updated to reflect the program's having found a smaller value.  When the loop finishes all nine iterations, the variable { min} holds the smallest value among the set of 10  integers stored in the { values} array.   As a first step towards designing an FSM to implement the code, we transform the code into a flow chart, as shown to the right.  The program again begins with initialization, which appears in the second column of the flow chart.   The loop in the program translates to the third column of the flow chart,  and the { if} statement to the middle comparison and update  of { min}.  Our goal is now to design an FSM to implement the flow chart.  In order to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless entry system with a timer.  Although the timer's value was technically also   {{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}   part of the FSM's state, we treated it as data and integrated it into our next-state decisions in only a couple of cases.  For our minimum value problem, we have two sources of data.  First, an external program supplies data in the form of a set of 10 integers.  If we assume {32-bit} integers, these data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value.  These are technically state bits.  For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it  using the components that we have developed earlier in our course.  {Choosing Components and Identifying States}  Now we are ready to design an FSM that implements the flow chart. What components do we need, other than our state logic? We use registers and counters to implement the variables { idx} and { min} in the program. For the array { values}, we use a {1632-bit}  memory.{We technically only need a {1032-bit}  memory, but we round up the size of the address space to reflect more realistic memory designs; one can always optimize later.} We need a comparator to implement the test for the { if} statement. We choose to use a serial comparator, which allows us to illustrate again how one logical high-level state can be subdivided into many actual states. To operate the serial comparator, we make use of two shift registers that  present the comparator with one bit per cycle on each input, and a counter to keep track of the comparator's progress.  How do we identify high-level states from our flow chart?  Although the flow chart attempts to break down the program into `simple' steps, one step of a flow chart may sometimes require more than one state in an FSM.  Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities.  How we map flow chart elements into FSM states also depends to some  degree on what components we use, which is why we began with some discussion of components.  In practice, one can go back and forth between the two,  adjusting components to better match the high-level states, and adjusting  states to better match the desired components.  Finally, note that we are only concerned with high-level states, so we do  not need to provide details (yet) down to the level of individual clock  cycles, but we do want to define high-level states that can be implemented in a fixed number of cycles, or at"
    },
    {
        "question": " How does the FSM need to track the information?\\n\\n",
        "answers": [
            "\nThe FSM needs to track the state of the stoplight and the duration of each state.",
            "A priority encoder is a circuit that takes in a number of input signals, and outputs a code that represents the highest priority input signal. The output code is usually a binary number.",
            "If the \"halt\" button is pressed (HG=10), the logic outputs C_L=00 and C_M=00, so the dispenser gives no ice cream. If both buttons are pressed (HG=11), the outputs are C_L=11 and C_M=11, so the dispenser gives two servings of each flavor, or four servings total."
        ],
        "score": [
            4.7992143630981445,
            -10.282073974609375,
            -10.805578231811523
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What does the FSM do with the information?\\n\\n",
        "answers": [
            "\nThe FSM is responsible for controlling the stoplights at a busy intersection. It ensures that the lights on one or both roads are red at all times, that a road has a green light, and that the controller should show a yellow light before showing a red light.",
            "The device uses the sensors to generate a clock signal by alternating between 0 and 1 with a regular period. When the clock is 0, one set of latches holds its bit values fixed, and combinational logic uses those latches as inputs to produce  bits that are copied into a second set of latches. When the clock is 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches.",
            "PASSAGE - \n\nTwo-level logic is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. In this section, we illustrate one of the reasons for this popularity and show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.\n\nQUESTION -\n\nWhat are the two levels of two-level logic?"
        ],
        "score": [
            3.0504746437072754,
            -9.908632278442383,
            -10.730594635009766
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the output of the FSM?']",
        "answers": [
            "\nThe output of the FSM is a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.",
            "The device is used to turn itself off and save energy. This is done by using heuristic approaches to find solutions that strike a balance between different metrics, such as cost, power, or performance. The human engineer can impose constraints, such as limits on the chip area or limits on the minimum performance, in order to guide the process.",
            "Jaramillo later identified common failure modes, including variability caused by sunshine through the windows in ECEB, and made some changes to improve robustness. He also created the PowerPoint slides that are typically used to describe the lab in lecture."
        ],
        "score": [
            8.035726547241211,
            -11.104883193969727,
            -11.18936538696289
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "What is a priority encoder?\\nA: A priority encoder is a type of encoder that generates a signal whenever any of its input lines are active, and encodes the index of the highest active input as a two-bit unsigned number. \\n\\n",
        "answers": [
            "\nA priority encoder is a type of encoder that generates a signal whenever any of its input lines are active, and encodes the index of the highest active input as a two-bit unsigned number. A priority encoder is used to determine which of its input lines has the highest priority.",
            "The design process for a counter with a number of states that is not a power of two is to choose a representation for the states that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning.",
            "The two most common types of memory used in digital systems are static random access memory (SRAM) and dynamic random access memory (DRAM). SRAM is used for high-speed applications such as processor caches and some embedded designs, while DRAM is used for applications with less demanding speed requirements, such as main memory in most computers."
        ],
        "score": [
            9.065641403198242,
            4.483310699462891,
            3.5641653537750244
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " ",
        "answers": [
            "\nWhat are the three ranges for a given metric?\n\nThe first range is all measurement values are equivalently useful. The second range is possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice.",
            "Multiple chips can be used to construct wider memories by connecting the address, write enable, and chip select inputs to both chips. The data inputs and outputs are separate, with the left chip handling the high N bits of input on writes and producing the high N bits of output on reads, while the right chip handles the low N bits of input and produces the low N bits of output.",
            "The purpose of this process is to find the best formulation of a specific logic function for a given metric. This is done by using heuristic approaches to find solutions that strike a balance between various metrics such as cost, power, or performance."
        ],
        "score": [
            5.366610527038574,
            1.5939686298370361,
            4.941791534423828
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is a decoder?\\nA: A decoder is a type of logic circuit that takes an input signal and produces one output for each possible value of the signal. \\n\\n",
        "answers": [
            "\nA decoder is a type of logic circuit that takes an input signal and produces one output for each possible value of the signal. A decoder is similar to a multiplexer in that it has a number of inputs and outputs. However, a decoder only has one input signal, while a multiplexer has multiple input signals.",
            "The vending machine uses sensors to detect what coin was inserted. The first sensor is positioned so that a dime will pass beneath it, while the second sensor is positioned so that all coins will pass in front of it. The signal from the second sensor can be used as a clock for the FSM, while the signal from the first sensor indicates whether the coin is a dime or a quarter.",
            "One important reason to consider all possible input conditions from all possible states is that it can help ensure that a design is correct. For example, if a design only ever receives input values that are within a certain range, it is possible that the design is not actually correct for all input values. Considering all possible input conditions can help catch errors like this.\n\nAnother reason to consider all possible input conditions is that it can help optimize a design. For example, if a design only ever receives input values that are within a certain range, it is possible to simplify the design or make it more efficient. Considering all possible input conditions can help identify opportunities for optimization."
        ],
        "score": [
            9.720613479614258,
            -4.895650863647461,
            -3.3344054222106934
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " ",
        "answers": [
            "\nWhat is the difference between SOP and POS?\n\nSOP (Sum of Products) is a Boolean algebra expression that is the OR of ANDed terms, while POS (Product of Sums) is an expression that is the AND of ORed terms.",
            "The flow chart represents the steps that should be taken when designing a new product. The first step is to think carefully about metrics of possible interest. The second step is to get rid of as many metrics as possible. The third step is to identify metrics that have no practical impact in current technology. The fourth step is to set threshold values for other metrics to simplify reasoning about them. The fifth step is to eliminate redundant metrics. The sixth step is to calculate linear sums to reduce the count of metrics. The seventh step is to make use of the notion of Pareto optimality.",
            "SRAM is a type of memory that is used in computers. It is made up of a number of transistors that store data in a way that is easy for the computer to access. SRAM is faster than other types of memory, but it is also more expensive."
        ],
        "score": [
            2.8450210094451904,
            1.4976612329483032,
            6.489872932434082
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the difference between a mux and a decoder?\\nA: A mux is a type of logic circuit that takes multiple input signals and produces one output, while a decoder is a type of logic circuit that takes one input signal and produces multiple outputs. \\n\\n",
        "answers": [
            "\nThe main difference between a mux and a decoder is that a mux has multiple inputs and one output, while a decoder has one input and multiple outputs. A mux is used to select one of several input signals and send it to the output, while a decoder is used to interpret an input signal and produce several output signals.",
            "SRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors, while a DRAM cell uses only one transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor",
            "A bad state representation can lead to a number of problems, including wasted engineering effort, poor performance, and even product cancellations."
        ],
        "score": [
            9.703204154968262,
            4.159519195556641,
            7.5356125831604
        ],
        "text": "K-maps, we must start again with new K-maps for the lower-case checker.  If instead we have taken the approach of designing logic for the upper and  lower bits of the ASCII character, we can reuse most of that logic, since the functions T_4 and T_5 are identical when checking for a lower-case character.  Recalling the algebraic form of U(C), we can then write a function L(C) (a lower-case checker) as shown on the left below.   {eqnarray*} U&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5  L&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5  &=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+ &&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0}) {eqnarray*}   {file=part2/figs/ascii-cmp-lower.eps,width=3.6in}   Finally, if we have used a design based on comparators or adders, the design of a lower-case checker becomes trivial: simply change the numbers that we input to these components, as shown in the figure on the right above for the comparator-based design.  The only changes from the upper-case checker design are the inputs to the comparators and the output produced,  highlighted with blue text in the figure.    {The Multiplexer}  Using the more abstract designs for checking ranges of ASCII characters, we can go a step further and create a checker for both upper- and lower-case letters.  To do so, we add another input S that allows us to select the  function that we want---either the upper-case checker U(C) or the  lower-case checker L(C).  For this purpose, we make use of a logic block called a { multiplexer}, or { mux}. Multiplexers are an important abstraction for digital logic.  In  general, a multiplexer allows us to use one digital signal to  select which of several others is forwarded to an output.   The simplest form of the multiplexer is the 2-to-1 multiplexer shown to  the right.   The logic diagram illustrates how the mux works.  The block  has two inputs from the left and one from the top.  The top input allows  us to choose which of the left inputs is forwarded to the output.   When the input S=0, the upper AND gate outputs 0, and the lower AND gate outputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0. Similarly, when input S=1, the upper AND gate outputs D_1, and the lower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1.      The symbolic form of the mux is a trapezoid with data inputs on the  larger side, an output on the smaller side, and a select input on the angled part of the trapezoid.  The labels inside the trapezoid indicate  the value of the select input S for which the adjacent data signal,  D_1 or D_0, is copied to the output Q.  We can generalize multiplexers in two ways.  First, we can extend the  single select input to a group of select inputs.  An {N-bit} select input allows selection from amongst 2^N inputs.  A {4-to-1}  multiplexer is shown below, for example.  The logic diagram on the left shows how the {4-to-1 mux} operates.  For any combination of S_1S_0, three of the AND gates produce 0, and the fourth outputs the D input corresponding to the interpretation of S as an unsigned number. Given three zeroes and one D input, the OR gate thus reproduces one of  the D's.  When S_1S_0=10, for example, the third AND gate copies D_2, and Q=D_2.  {{file=part2/figs/mux4-to-1.eps,width=5.60in}}  As shown in the middle figure, a {4-to-1} mux can also be built from three {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux  appears on the right in the figure.    The second way in which we can generalize multiplexers is by using several multiplexers of the same type and using the same signals for  selection.  For example, we might use a single select bit T to choose  between any number of paired inputs.  Denote input pair by i D_1^i  and D_0^i.  For each pair, we have an output Q_i.    When T=0, Q_i=D_0^i for each value of i.  And, when T=1, Q_i=D_1^i for each value of i.  Each value of i requires a {2-to-1} mux with its select input driven by the global select signal T.  Returning to the example of the upper-"
    },
    {
        "question": " ",
        "answers": [
            "\nPASSAGE - compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column",
            "Pressing the ``halt'' button does nothing when the counter is halted. The counter remains in the same state until the ``go'' button is pressed.",
            "The value of register PRICE1 is the price of the first item in the machine, which is determined by the owner of the machine."
        ],
        "score": [
            -2.4600985050201416,
            4.5911478996276855,
            0.5076766610145569
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "How does a decoder work?\\nA: A decoder takes an input signal---typically one coded as a binary number---and produces one output for each possible value of the signal. When the decoder is enabled (EN=1), each of the AND gates produces one minterm of the input signal S. When the decoder is not enabled (EN=0), the decoder produces all 0 bits. \\n\\n",
        "answers": [
            "\nA decoder is a logic circuit that converts a binary code into a set of signals that can be used to operate a device. A decoder takes an input signal, typically one coded as a binary number, and produces one output for each possible value of the signal. When the decoder is enabled (EN=1), each of the AND gates produces one minterm of the input signal S. When the decoder is not enabled (EN=0), the decoder produces all 0 bits.",
            "The FSM ensures that the lights on one or both roads are red at all times by keeping track of the state of the light. If the light is green, it will turn yellow and then red. If the light is red, it will stay red.",
            "The AND gate on the left is used to determine if all of the input values are 1. If all of the input values are 1, then the output of the AND gate will be 1. If any of the input values are 0, then the output of the AND gate will be 0."
        ],
        "score": [
            7.12878942489624,
            2.8152999877929688,
            4.793400287628174
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " ",
        "answers": [
            "\nHow is multi-metric optimization useful in engineering?\n\nMulti-metric optimization is useful in engineering because it allows engineers to identify the most important metrics and to balance their design effectively according to them. Multi-metric optimization also allows engineers to get rid of as many metrics as possible so that they can focus on the most important ones.",
            "The choice of state representation affects design complexity in a few ways. First, it can affect the number of bits needed to represent the states. Second, it can affect the number of possible states that can be represented. Third, it can affect the number of transitions that need to be considered when designing the state machine.",
            "The initial state of the three flip-flops in our implementation is 0s and 1s. We can use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1)."
        ],
        "score": [
            9.194194793701172,
            5.221985816955566,
            0.9736868143081665
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the purpose of a decoder?\\nA: Decoders are used in a variety of applications, such as converting coded signals into usable information, error detection and correction', '\\n\\n",
        "answers": [
            "\nA decoder is a device that converts coded signals into usable information. Decoders are used in a variety of applications, such as error detection and correction.",
            "We can test the circuit that we build in the lab to see if it correctly implements the FSM that we have outlined in these notes. To do this, we can input a known sequence of 0s and 1s and see if the circuit outputs the correct responses. If it does, then we can be confident that the circuit is functioning correctly.",
            "The FSM alternates green lights between the two roads by setting the timer to a different value when the state of the FSM changes. This causes the lights to change color on a cycle-by-cycle basis, with the green light alternating between the two roads."
        ],
        "score": [
            8.425058364868164,
            -5.939740180969238,
            -7.01554536819458
        ],
        "text": "In contrast, the Hamming distance of the {2-out-of-5} code that we discussed earlier is 2.  Similarly, the Hamming distance of any representation extended with a parity bit is at least 2.  Now let's think about the problem slightly differently.  Given a particular representation,   how many bit errors can we detect in values using that representation?  { A representation with Hamming distance d can detect up to d-1 bit errors.}  To understand this claim, start by selecting a code word from the representation and changing up to d-1 of the bits.  No matter how one chooses to change the bits, these changes cannot result in another code word, since we know that any other code word has to  require at least d changes from our original code word, by the definition of the representation's Hamming distance.  A digital system using the representation can thus detect up to d-1 errors.  However, if d or more errors occur, the system might sometimes fail to detect any error in the stored value.   {Error Correction}  Detection of errors is important, but may sometimes not be enough. What can a digital system do when it detects an error?  In some cases, the system may be able to find the original value elsewhere, or may be able to re-compute the value from other values.  In other  cases, the value is simply lost, and the digital system may need to reboot or even shut down until a human can attend to it.  Many real systems cannot afford such a luxury.  Life-critical systems such as medical equipment and airplanes should not turn themselves off and wait for a human's attention.  Space vehicles face a similar dilemma, since no human may be able to reach them.  Can we use a strategy similar to the one that we have developed for error detection in order to try to perform { error correction}, recovering the original value?  Yes, but the overhead---the extra bits that we need to provide such functionality---is higher.    Let's start by thinking about a code with Hamming distance 2, such as {4-bit} 2's complement with odd parity.  We know that such a  code can detect one bit error.  Can it correct such a bit error, too?  Imagine that a system has stored the decimal value 6 using the  pattern 0110{ 1}, where the last bit is the odd parity bit. A bit error occurs, changing the stored pattern to 0111{ 1}, which is not a valid pattern, since it has an even number of 1s.  But can the system know that the original value stored was 6?  No, it cannot. The original value may also have been 7, in which case the original pattern was 0111{ 0}, and the bit error occurred in the final bit.  The original value may also have been -1, 3, or 5.  The system has no way of resolving this ambiguity.  The same problem arises if a digital system uses a code with Hamming distance d to detect up to d-1 errors.   Error correction is possible, however, if we assume that fewer bit errors occur (or if we instead use a representation with a larger Hamming distance).  As a simple example, let's create a representation for the numbers 0 through 3 by making three copies of the {2-bit} unsigned  representation, as shown to the right.  The Hamming distance of the resulting code is 3, so any two bit errors can be detected.  However, this code also enables us to correct a single bit error.  Intuitively,  think of the three copies as voting on the right answer.   {c|c} value      & three-copy represented& code  0& 000000 1& 010101 2& 101010 3& 111111    Since a single bit error can only corrupt one copy, a majority vote always gives the right answer! Tripling the number of bits needed in a representation is not a good general strategy, however.  Notice also that ``correcting'' a pattern with two bit errors can produce the wrong result.  Let's think about the problem in terms of Hamming distance.  Assume that we use a code with Hamming distance d and imagine that up to k bit errors affect a stored value. The resulting pattern then falls within a neighborhood of distance k from the"
    },
    {
        "question": " How does a decoder work?\\n",
        "answers": [
            "\nA decoder is a logic circuit that converts coded inputs into coded outputs, usually one output code for each input code. Decoders are used in a variety of applications, including data storage and transmission, error detection and correction, and data compression.",
            "The vending machine FSM is designed to control the release of items from the vending machine based on the user's purchase request. The FSM uses a priority encoder to determine which item the user has requested, and a decoder to generate the individual item release signals.",
            "A tri-state buffer is a digital circuit that can be used to either connect or disconnect an output from a signal line. The buffer has three states: high, low, and high impedance. When the buffer is in the high state, the output is connected to the signal line and copies the signal from the input. When the buffer is in the low state, the output is disconnected from the signal line. When the buffer is in the high impedance state, the output is effectively disconnected from the signal line."
        ],
        "score": [
            5.746232032775879,
            -3.2291007041931152,
            -8.685969352722168
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How is a decoder similar to a mux?\\n",
        "answers": [
            "\nA decoder is similar to a mux in that it allows one digital signal to select which of several others is forwarded to an output. A decoder has one input and multiple outputs, while a mux has multiple inputs and one output. A decoder can be thought of as a special case of a mux.",
            "One reason it is important to start with a representation that makes sense to a human is that humans must supply the logic functions of interest, and must program the acceptable transformations between equivalent forms. A second reason is that the complexity of the problem is a good way to introduce you to real engineering. Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.",
            "A mux is a logic block that allows us to use one digital signal to select which of several others is forwarded to an output. A decoder is a logic block that converts a binary code to a set of signals."
        ],
        "score": [
            9.088761329650879,
            -11.288212776184082,
            4.785193920135498
        ],
        "text": "K-maps, we must start again with new K-maps for the lower-case checker.  If instead we have taken the approach of designing logic for the upper and  lower bits of the ASCII character, we can reuse most of that logic, since the functions T_4 and T_5 are identical when checking for a lower-case character.  Recalling the algebraic form of U(C), we can then write a function L(C) (a lower-case checker) as shown on the left below.   {eqnarray*} U&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5  L&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5  &=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+ &&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0}) {eqnarray*}   {file=part2/figs/ascii-cmp-lower.eps,width=3.6in}   Finally, if we have used a design based on comparators or adders, the design of a lower-case checker becomes trivial: simply change the numbers that we input to these components, as shown in the figure on the right above for the comparator-based design.  The only changes from the upper-case checker design are the inputs to the comparators and the output produced,  highlighted with blue text in the figure.    {The Multiplexer}  Using the more abstract designs for checking ranges of ASCII characters, we can go a step further and create a checker for both upper- and lower-case letters.  To do so, we add another input S that allows us to select the  function that we want---either the upper-case checker U(C) or the  lower-case checker L(C).  For this purpose, we make use of a logic block called a { multiplexer}, or { mux}. Multiplexers are an important abstraction for digital logic.  In  general, a multiplexer allows us to use one digital signal to  select which of several others is forwarded to an output.   The simplest form of the multiplexer is the 2-to-1 multiplexer shown to  the right.   The logic diagram illustrates how the mux works.  The block  has two inputs from the left and one from the top.  The top input allows  us to choose which of the left inputs is forwarded to the output.   When the input S=0, the upper AND gate outputs 0, and the lower AND gate outputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0. Similarly, when input S=1, the upper AND gate outputs D_1, and the lower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1.      The symbolic form of the mux is a trapezoid with data inputs on the  larger side, an output on the smaller side, and a select input on the angled part of the trapezoid.  The labels inside the trapezoid indicate  the value of the select input S for which the adjacent data signal,  D_1 or D_0, is copied to the output Q.  We can generalize multiplexers in two ways.  First, we can extend the  single select input to a group of select inputs.  An {N-bit} select input allows selection from amongst 2^N inputs.  A {4-to-1}  multiplexer is shown below, for example.  The logic diagram on the left shows how the {4-to-1 mux} operates.  For any combination of S_1S_0, three of the AND gates produce 0, and the fourth outputs the D input corresponding to the interpretation of S as an unsigned number. Given three zeroes and one D input, the OR gate thus reproduces one of  the D's.  When S_1S_0=10, for example, the third AND gate copies D_2, and Q=D_2.  {{file=part2/figs/mux4-to-1.eps,width=5.60in}}  As shown in the middle figure, a {4-to-1} mux can also be built from three {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux  appears on the right in the figure.    The second way in which we can generalize multiplexers is by using several multiplexers of the same type and using the same signals for  selection.  For example, we might use a single select bit T to choose  between any number of paired inputs.  Denote input pair by i D_1^i  and D_0^i.  For each pair, we have an output Q_i.    When T=0, Q_i=D_0^i for each value of i.  And, when T=1, Q_i=D_1^i for each value of i.  Each value of i requires a {2-to-1} mux with its select input driven by the global select signal T.  Returning to the example of the upper-"
    },
    {
        "question": " How is a decoder different from a mux?\\n",
        "answers": [
            "\nA decoder is a logic device that converts binary information from the form of one code to another, usually a natural binary code. A multiplexer is a device that selects one of several analog or digital input signals and forwards the selected input into a single output line.",
            "There are several benefits of using an FSM. One benefit is that an FSM can be used to implement a wide range of logic functions. Another benefit is that an FSM can be used to construct larger memories from multiple smaller memories. Finally, an FSM can be used to reduce the number of pins needed on a chip.",
            "An abstract model is a mathematical model that is used to describe a system. It is usually simplified and does not include all the details of the system."
        ],
        "score": [
            2.236402750015259,
            -11.2401123046875,
            -11.234334945678711
        ],
        "text": "K-maps, we must start again with new K-maps for the lower-case checker.  If instead we have taken the approach of designing logic for the upper and  lower bits of the ASCII character, we can reuse most of that logic, since the functions T_4 and T_5 are identical when checking for a lower-case character.  Recalling the algebraic form of U(C), we can then write a function L(C) (a lower-case checker) as shown on the left below.   {eqnarray*} U&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5  L&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5  &=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+ &&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0}) {eqnarray*}   {file=part2/figs/ascii-cmp-lower.eps,width=3.6in}   Finally, if we have used a design based on comparators or adders, the design of a lower-case checker becomes trivial: simply change the numbers that we input to these components, as shown in the figure on the right above for the comparator-based design.  The only changes from the upper-case checker design are the inputs to the comparators and the output produced,  highlighted with blue text in the figure.    {The Multiplexer}  Using the more abstract designs for checking ranges of ASCII characters, we can go a step further and create a checker for both upper- and lower-case letters.  To do so, we add another input S that allows us to select the  function that we want---either the upper-case checker U(C) or the  lower-case checker L(C).  For this purpose, we make use of a logic block called a { multiplexer}, or { mux}. Multiplexers are an important abstraction for digital logic.  In  general, a multiplexer allows us to use one digital signal to  select which of several others is forwarded to an output.   The simplest form of the multiplexer is the 2-to-1 multiplexer shown to  the right.   The logic diagram illustrates how the mux works.  The block  has two inputs from the left and one from the top.  The top input allows  us to choose which of the left inputs is forwarded to the output.   When the input S=0, the upper AND gate outputs 0, and the lower AND gate outputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0. Similarly, when input S=1, the upper AND gate outputs D_1, and the lower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1.      The symbolic form of the mux is a trapezoid with data inputs on the  larger side, an output on the smaller side, and a select input on the angled part of the trapezoid.  The labels inside the trapezoid indicate  the value of the select input S for which the adjacent data signal,  D_1 or D_0, is copied to the output Q.  We can generalize multiplexers in two ways.  First, we can extend the  single select input to a group of select inputs.  An {N-bit} select input allows selection from amongst 2^N inputs.  A {4-to-1}  multiplexer is shown below, for example.  The logic diagram on the left shows how the {4-to-1 mux} operates.  For any combination of S_1S_0, three of the AND gates produce 0, and the fourth outputs the D input corresponding to the interpretation of S as an unsigned number. Given three zeroes and one D input, the OR gate thus reproduces one of  the D's.  When S_1S_0=10, for example, the third AND gate copies D_2, and Q=D_2.  {{file=part2/figs/mux4-to-1.eps,width=5.60in}}  As shown in the middle figure, a {4-to-1} mux can also be built from three {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux  appears on the right in the figure.    The second way in which we can generalize multiplexers is by using several multiplexers of the same type and using the same signals for  selection.  For example, we might use a single select bit T to choose  between any number of paired inputs.  Denote input pair by i D_1^i  and D_0^i.  For each pair, we have an output Q_i.    When T=0, Q_i=D_0^i for each value of i.  And, when T=1, Q_i=D_1^i for each value of i.  Each value of i requires a {2-to-1} mux with its select input driven by the global select signal T.  Returning to the example of the upper-"
    },
    {
        "question": " What is the purpose of a priority encoder?\\n",
        "answers": [
            "\nA priority encoder is a type of encoder that assigns priority to certain input signals. It is used to determine which input signal should be given priority when multiple input signals are present.",
            "A computer memory is a storage device for data. There are two main types of memory: SRAM and DRAM. SRAM is faster but more expensive, while DRAM is slower but cheaper. DRAM is more commonly used for main memory in computers.",
            "Abstraction in selecting a representation can impact the design metrics by reducing the number of metrics that need to be considered. This can simplify the process of designing a solution and make it easier to find an optimal solution."
        ],
        "score": [
            7.976846218109131,
            -11.172271728515625,
            -11.061622619628906
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " How does a priority encoder work?', '\\n\\n",
        "answers": [
            "\nA priority encoder is a type of encoder that assigns priority to certain input signals. The priority encoder takes in a signal and produces an output signal that indicates which input signal has the highest priority. The output signal is typically a binary number that is used to select one of the input signals.",
            "The FSM ensures that the lights on one or both roads are red at all times by keeping track of the state of the light. If the light is green, it will turn yellow and then red. If the light is red, it will stay red.",
            "DRAM is used for storing data in a capacitor. The capacitor is attached to a bit line through a transistor controlled by a select line. When the select line is low, the capacitor is isolated and holds its charge. However, the transistor's resistance is finite, and some charge leaks out onto the bit line."
        ],
        "score": [
            8.011378288269043,
            -10.951679229736328,
            -10.509801864624023
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " How does a decoder work?\\n",
        "answers": [
            "\nA decoder is a logic circuit that converts signals from one form to another. In binary form, a decoder can be used to convert a binary number into a decimal number.",
            "The FSM alternates green lights between the two roads by setting the timer to a different value when the state of the FSM changes. This causes the lights to change color on a cycle-by-cycle basis, with the green light alternating between the two roads.",
            "Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels)."
        ],
        "score": [
            5.653107643127441,
            -10.282540321350098,
            -11.125927925109863
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How does a priority encoder work?\\n",
        "answers": [
            "\nA priority encoder is a circuit that takes in a number of input signals, and outputs a code that represents the highest priority input signal. The output code is usually a binary number.",
            "Abstraction can be used to build a three-bit Gray code counter from the two-bit design by using copies of the bit-sliced adder circuit. This approach is less efficient than designing a circuit specifically for a three-bit Gray code counter, but it is simpler.",
            "The states of the FSM act like a counter, and the value of the FSM is set whenever the state changes. The timer is set only when the state changes, and the duration of each state is determined by the value of the timer. The FSM can also be set under other conditions, such as when the signal F is calculated."
        ],
        "score": [
            7.3080291748046875,
            -10.735828399658203,
            -10.363486289978027
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " What is the difference between a mux and a decoder?\\n",
        "answers": [
            "\nA mux is a logic block that allows us to use one digital signal to select which of several others is forwarded to an output. A decoder is a logic block that converts a binary code to a set of signals.",
            "From the passage, it can be inferred that logical completeness does not necessarily help with efficient design of logic functions. In other words, a single Boolean logic function can be expressed in many different ways. Therefore, it is important for engineers to learn how to develop an efficient implementation of a function, as well as how to determine whether two logic expressions are identical without actually writing out truth tables.",
            "The more general model of a computer is a system that can execute a set of instructions, called a program, and store the resulting data. The system has a set of registers that hold data and instructions, and a set of memory locations that store data and programs. The system also has a set of input and output devices that allow it to interact with the outside world."
        ],
        "score": [
            7.654180526733398,
            -11.26309871673584,
            -10.782276153564453
        ],
        "text": "K-maps, we must start again with new K-maps for the lower-case checker.  If instead we have taken the approach of designing logic for the upper and  lower bits of the ASCII character, we can reuse most of that logic, since the functions T_4 and T_5 are identical when checking for a lower-case character.  Recalling the algebraic form of U(C), we can then write a function L(C) (a lower-case checker) as shown on the left below.   {eqnarray*} U&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5  L&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5  &=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+ &&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0}) {eqnarray*}   {file=part2/figs/ascii-cmp-lower.eps,width=3.6in}   Finally, if we have used a design based on comparators or adders, the design of a lower-case checker becomes trivial: simply change the numbers that we input to these components, as shown in the figure on the right above for the comparator-based design.  The only changes from the upper-case checker design are the inputs to the comparators and the output produced,  highlighted with blue text in the figure.    {The Multiplexer}  Using the more abstract designs for checking ranges of ASCII characters, we can go a step further and create a checker for both upper- and lower-case letters.  To do so, we add another input S that allows us to select the  function that we want---either the upper-case checker U(C) or the  lower-case checker L(C).  For this purpose, we make use of a logic block called a { multiplexer}, or { mux}. Multiplexers are an important abstraction for digital logic.  In  general, a multiplexer allows us to use one digital signal to  select which of several others is forwarded to an output.   The simplest form of the multiplexer is the 2-to-1 multiplexer shown to  the right.   The logic diagram illustrates how the mux works.  The block  has two inputs from the left and one from the top.  The top input allows  us to choose which of the left inputs is forwarded to the output.   When the input S=0, the upper AND gate outputs 0, and the lower AND gate outputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0. Similarly, when input S=1, the upper AND gate outputs D_1, and the lower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1.      The symbolic form of the mux is a trapezoid with data inputs on the  larger side, an output on the smaller side, and a select input on the angled part of the trapezoid.  The labels inside the trapezoid indicate  the value of the select input S for which the adjacent data signal,  D_1 or D_0, is copied to the output Q.  We can generalize multiplexers in two ways.  First, we can extend the  single select input to a group of select inputs.  An {N-bit} select input allows selection from amongst 2^N inputs.  A {4-to-1}  multiplexer is shown below, for example.  The logic diagram on the left shows how the {4-to-1 mux} operates.  For any combination of S_1S_0, three of the AND gates produce 0, and the fourth outputs the D input corresponding to the interpretation of S as an unsigned number. Given three zeroes and one D input, the OR gate thus reproduces one of  the D's.  When S_1S_0=10, for example, the third AND gate copies D_2, and Q=D_2.  {{file=part2/figs/mux4-to-1.eps,width=5.60in}}  As shown in the middle figure, a {4-to-1} mux can also be built from three {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux  appears on the right in the figure.    The second way in which we can generalize multiplexers is by using several multiplexers of the same type and using the same signals for  selection.  For example, we might use a single select bit T to choose  between any number of paired inputs.  Denote input pair by i D_1^i  and D_0^i.  For each pair, we have an output Q_i.    When T=0, Q_i=D_0^i for each value of i.  And, when T=1, Q_i=D_1^i for each value of i.  Each value of i requires a {2-to-1} mux with its select input driven by the global select signal T.  Returning to the example of the upper-"
    },
    {
        "question": " How does the decoder generate the release signals for the vending machine?\\n",
        "answers": [
            "\nThe decoder generates the release signals for the vending machine by connecting the signal S produced by the priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input. The outputs D_1, D_2, and D_3 then correspond to the individual item release signals R_1, R_2, and R_3 for our vending machine.",
            "This particular example is tricky because it requires the reader to understand the concept of bit-sliced addition in order to understand the passage. Furthermore, the passage does not provide a clear explanation of how the bit-sliced adder circuit works, making it difficult for the reader to follow along.",
            "To write data into a memory element, an external circuit sends a signal to the memory indicating the address of the element to be written. The data to be written is then sent to the memory, and the memory stores the data in the element at the specified address."
        ],
        "score": [
            10.164501190185547,
            -11.329977035522461,
            -10.529401779174805
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " Why is a decoder used in this context?']",
        "answers": [
            "\nA decoder is used in this context to simplify the expression of a logic function. By using a decoder, the function can be expressed as a two-level logic function, which is easier to understand and manipulate.",
            "If the \"halt\" button is pressed (HG=10), the logic outputs C_L=00 and C_M=00, so the dispenser gives no ice cream. If both buttons are pressed (HG=11), the outputs are C_L=11 and C_M=11, so the dispenser gives two servings of each flavor, or four servings total.",
            "The FSM is implemented using registers, counters, and a comparator. The comparator is used to implement the if statement in the program. The registers and counters are used to implement the variables idx and min in the program. The memory is used to implement the array values in the program."
        ],
        "score": [
            8.993722915649414,
            -10.89466667175293,
            -9.133798599243164
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What is the value of register PRICE1?\\n\\n",
        "answers": [
            "\nThe value of register PRICE1 is the price of the first item in the machine, which is determined by the owner of the machine.",
            "The value of register PRICE3 is the price of the third item in the machine, which is determined by the owner of the machine.",
            "The output of the FSM is the position of the farmer, fox, goose, and corn. The output is either the left bank or the right bank of the river. The game also has two indicators: W for reaching the winning state, and L for reaching a losing state."
        ],
        "score": [
            7.9790143966674805,
            6.067304611206055,
            -11.277179718017578
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What is the value of register PRICE2?\\n\\n",
        "answers": [
            "\nThe value of register PRICE2 is the number of nickels in the machine multiplied by the price of the item in question. In this way, the machine can keep track of how much money has been inserted and how much is owed for the item.",
            "An FSM is a finite state machine, which is a mathematical model of computation used to design both computer programs and digital logic circuits. It is an abstract machine that can be in one of a finite number of states.",
            "Jaramillo later identified common failure modes, including variability caused by sunshine through the windows in ECEB, and made some changes to improve robustness. He also created the PowerPoint slides that are typically used to describe the lab in lecture."
        ],
        "score": [
            7.640774726867676,
            -11.067551612854004,
            -11.30759334564209
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What is the value of register PRICE3?\\n\\n",
        "answers": [
            "\nThe value of register PRICE3 is the price of the third item in the machine, which is determined by the owner of the machine.",
            "Tri-state buffers are used to provide logical disconnection between the tri-state buffer and the output. This logical disconnection is achieved by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.",
            "The PAID state is the state in which the user has paid the full price of 35 cents. The next states from PAID are DIME when a dime is inserted and QUARTER when a quarter is inserted."
        ],
        "score": [
            7.9132080078125,
            -11.127362251281738,
            -10.235523223876953
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What is the value of register N?\\n\\n",
        "answers": [
            "\nThe value of register N corresponds to the number of nickels that have been inserted into the machine. If the user inserts a quarter, the machine will transition to a state with N+5, indicating that 5 more nickels have been inserted. If the user attempts to purchase an item without inserting enough money first, the machine will remain in the same state and reject the request.",
            "A decoder is a logic circuit that converts coded inputs into coded outputs, usually one output code for each input code. Decoders are used in a variety of applications, including data storage and transmission, error detection and correction, and data compression.",
            "Tri-state buffers can be used to construct wider memories by connecting the data inputs and outputs of multiple memories together. This allows the same output line to be shared by more than one memory without the need for additional multiplexers."
        ],
        "score": [
            7.465561866760254,
            -10.832038879394531,
            -11.130472183227539
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " What is the value of register A?', '\\n",
        "answers": [
            "\nThe value of register A is the equivalence class of the sum of the two 4-bit numbers. In this case, the sum is 18, which is equivalent to 2 (mod 16).",
            "Interrupts are more powerful than exceptions because they can occur asynchronously with respect to the program. This means that the processor can check for interrupts between instructions, which allows for more flexibility in the design of the processor.",
            "One could design a game controller for a logic puzzle that would have buttons corresponding to the different operations that could be performed on the logic puzzle. For example, there could be a button to add a new element to the puzzle, a button to remove an element from the puzzle, and a button to change the value of an element in the puzzle. The game controller could also have a display that would show the current state of the puzzle and the player's progress."
        ],
        "score": [
            7.5205979347229,
            -11.247275352478027,
            -11.212484359741211
        ],
        "text": "same idea to perform operations on other representations, such as changing an upper case letter in ASCII to a lower case one, or converting from an ASCII digit to an unsigned representation of the same number.    {The Unsigned Add Unit}  In order to define a representation for signed integers that allows us to reuse a piece of hardware designed for unsigned integers, we must first understand what such a piece of hardware actually does (we do not need to know how it works yet---we'll explore that question  later in our class).  The unsigned representation using {N} bits is not closed under addition.  In other words, for any value of N, we can easily find two {N-bit} unsigned numbers that, when added together, cannot be represented as an {N-bit} unsigned number.  With N=4,  for example, we can add 12 (1100) and 6 (0110) to obtain 18. Since 18 is outside of the range [0,2^4-1] representable using the {4-bit} unsigned representation, our representation breaks if we try to represent the sum using this representation.  We call this failure an { overflow} condition: the representation cannot represent the result of the operation, in this case addition.   Using more bits to represent the answer is not an attractive solution,  since we might then want to use more bits for the inputs, which in turn requires more bits for the outputs, and so on.  We cannot build  something supporting an infinite number of bits.  Instead, we  choose a value for N and build an add unit that adds two {N-bit} numbers and produces an {N-bit} sum (and some overflow  indicators, which we discuss in the next set of notes).  The diagram to the right shows how we might draw such a device, with two {N-bit} numbers entering at from the top, and the {N-bit} sum coming out from the bottom.       The function used for {N-bit} unsigned addition is addition  modulo 2^N.  In a practical sense, you can think of this function as simply keeping the last N bits of the answer; other bits  are simply discarded.  In the example to the right, we add 12 and 6 to obtain 18, but then discard the extra bit on the left, so the add unit produces 2 (an overflow).       { Modular arithmetic} defines a way of performing arithmetic for a finite number of possible values, usually integers.   As a concrete example, let's use modulo 16, which corresponds to the addition unit for our {4-bit} examples.  Starting with the full range of integers, we break the number line into contiguous groups of 16 integers, as shown to the right.      The numbers 0 to 15 form one group.  The numbers -16 to -1 form a second group, and the numbers from 16 to 31 form a third group.  An infinite number of groups are defined in this manner.  We then define 16 { equivalence classes} consisting of the first numbers from all groups, the second numbers from all groups, and so forth. For example, the numbers , -32, -16, 0, 16, 32,  form one such equivalence class.  Mathematically, we say that two numbers A and B are equivalent modulo 16, which we write as  {eqnarray*} (A &=& B)  16, {or sometimes as} A && B {(mod 16)} {eqnarray*}  if and only if A=B+16k for some integer k.  Equivalence as defined by a particular modulus distributes over addition and multiplication.  If, for example, we want to find the equivalence class for (A + B)  16, we can find the equivalence classes for A (call it C) and B  (call it D) and then calculate the equivalence class  of (C + D)  16. As a concrete example of distribution over multiplication,  given (A = 1,083,102,112  7,323,127)  10, find A.  For this problem, we note that the first number is equivalent  to 2  10, while the second number is equivalent  to 7  10.  We then write (A = 2  7)  10, and, since 2  7 = 14, we have (A = 4)  10.   {Deriving 2's Complement}   Given these equivalence classes, we might instead choose to draw a circle to identify the equivalence classes and to associate each class with one of the sixteen possible {4-bit} patterns, as shown to the right. Using this circle"
    },
    {
        "question": " ",
        "answers": [
            "\nWhat is the significance of DeMorgan's law?\n\nDeMorgan's law is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. In this section, we illustrate one of the reasons for this popularity and show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.",
            "The implementation diagram for a three-bit Gray code counter is a logic diagram that shows the inputs, outputs, and next-state logic for the counter. The diagram is ordered from left to right as {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2, which makes it easy to read and check for correctness.",
            "Tri-state buffers can be used to reduce the number of pins needed on a chip by combining inputs and outputs. For example, data inputs and outputs of memory can be combined into a single set of data wires, with bidirectional signals. This reduces the number of pins needed while still allowing for the same amount of data to be transferred."
        ],
        "score": [
            4.624730587005615,
            2.349425792694092,
            -0.010986465960741043
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "How many bits are in register PRICE1?\\n",
        "answers": [
            "\nThe register PRICE1 has six bits. This allows the machine to keep track of up to 3.15 (63 nickels).",
            "From the passage, it seems that a digital FSM is designed from an abstract model by first identifying a finite set of states, a set of possible inputs, a set of possible outputs, and a set of transition rules. Then, the FSM is implemented as a digital system by representing all states as patterns using a fixed number of bits, translating all inputs into bits, and translating all outputs into bits. Finally, transition rules are defined so that given any state of the FSM and any pattern of input bits, a transition is defined from that state to another state.",
            "A write operation on a SRAM cell is performed by bringing the bit line to an intermediate voltage level, then raising the SELECT line, which allows the capacitor to either pull a small amount of charge from the bit line or to push a small amount of charge onto the bit line. The resulting change in voltage is then detected by a sense amplifier at the end of the bit line."
        ],
        "score": [
            8.071959495544434,
            -11.076937675476074,
            -11.181730270385742
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " ",
        "answers": [
            "\nWhat is the difference between two-level logic and other forms of logic?\n\nTwo-level logic is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. In this section, we illustrate one of the reasons for this popularity and show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.",
            "The two approaches to FSM design are the state-transition table and the state diagram. The state-transition table lists all the possible states that the system can be in, and the state diagram shows the sequence of states that the system goes through. The state-transition table is more compact and easier to use when comparing functions, but the state diagram is more intuitive and easier to understand.",
            "A chip select input is an input to a chip that allows the chip to be turned on or off. This input is used to save energy by turning off the chip when it is not needed."
        ],
        "score": [
            6.2964372634887695,
            3.462879180908203,
            5.0306291580200195
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the value of register PRICE2?\\n",
        "answers": [
            "\nThe value of register PRICE2 is the number of nickels that have been inserted into the machine.",
            "There are 63 possible states for the PAID state. This is because the machine can only track up to 3.15 dollars worth of nickels, which is 63 nickels.",
            "The vending machine uses sensors to detect what coin was inserted. The first sensor is positioned so that a dime will pass beneath it, while the second sensor is positioned so that all coins will pass in front of it. The signal from the second sensor can be used as a clock for the FSM, while the signal from the first sensor indicates whether the coin is a dime or a quarter."
        ],
        "score": [
            7.628649711608887,
            -10.980002403259277,
            -11.27309513092041
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " ",
        "answers": [
            "\nWhat is the reason for the popularity of two-level logic?\n\nThe reason for the popularity of two-level logic is that it is a simple way to express logic functions. Two-level logic is a popular way of expressing logic functions because it is a simple way to express logic functions.",
            "What are the three ranges for a given metric?\n\nThe first range is all measurement values are equivalently useful. The second range is possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice.",
            "The vending machine uses sensors to detect what coin was inserted. The first sensor is positioned so that a dime will pass beneath it, while the second sensor is positioned so that all coins will pass in front of it. The signal from the second sensor can be used as a clock for the FSM, while the signal from the first sensor indicates whether the coin is a dime or a quarter."
        ],
        "score": [
            5.578704357147217,
            5.366610527038574,
            2.7921433448791504
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "When is the output of the OR gate 1?\\n",
        "answers": [
            "\nThe output of the OR gate is 1 when the signal F is 1. F is 1 when the state of the FSM is S_0S_1S_2=001 or S_0S_1S_2=011.",
            "Tri-state buffers are used to provide logical disconnection between the tri-state buffer and the output. This logical disconnection is achieved by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.",
            "A sense amplifier is a device that is used to amplify the signal from a sensor. It is typically used to amplify the signal from a small sensor, such as a pressure sensor, to a level that can be read by a larger device, such as a computer."
        ],
        "score": [
            8.305916786193848,
            -8.733731269836426,
            -11.266288757324219
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " ",
        "answers": [
            "\nWhat is the difference between SOP and POS?\n\nSOP is a form of two-level logic, while POS is a form of multi-level logic. SOP is more popular because it is simpler to manipulate expressions algebraically and graphically.",
            "The next-state logic for a three-bit Gray code counter is as follows:\n\nS_2^+ = S_2 + S_0\nS_1^+ = S_2\nS_0^+ = S_2 S_1",
            "A Gray code counter is a type of counter that uses a Gray code, which is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit. A regular counter is a type of counter that does not use a Gray code."
        ],
        "score": [
            5.505491256713867,
            0.3576936721801758,
            6.891287803649902
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the value of V when a dime is inserted?\\n",
        "answers": [
            "\nThe value of V is 10 cents when a dime is inserted. This is because a dime is worth 10 cents, and the machine tracks the value of money inserted in terms of nickels. Therefore, when a dime is inserted, the machine records it as 2 nickels.",
            "There are two input bits.",
            "The FSM is responsible for controlling the stoplights at a busy intersection. It ensures that the lights on one or both roads are red at all times, that a road has a green light, and that the controller should show a yellow light before showing a red light."
        ],
        "score": [
            9.082763671875,
            -10.954421997070312,
            -11.315885543823242
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " ",
        "answers": [
            "\nPASSAGE - \n\nTwo-level logic is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. In this section, we illustrate one of the reasons for this popularity and show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.\n\nQUESTION -\n\nWhat are the two levels of two-level logic?",
            "The first approach is to use the flip-flop design to add a single bit and pass any necessary information into another copy of itself. By using copies of this bit-sliced adder circuit, we can mimic our approach as humans and build adders of any size.",
            "The main difference between SRAM and DRAM is that SRAM is faster but uses more transistors, while DRAM is slower but uses fewer transistors. DRAM is also denser than SRAM, meaning that more bits can be stored in a given area."
        ],
        "score": [
            5.982404708862305,
            1.9901821613311768,
            4.9587202072143555
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the purpose of the AND gate on the left?', '\\n",
        "answers": [
            "\nThe AND gate on the left is used to determine if all of the input values are 1. If all of the input values are 1, then the output of the AND gate will be 1. If any of the input values are 0, then the output of the AND gate will be 0.",
            "One way to verify that an implementation is correct is to test it against a known set of inputs and compare the results to the expected results. If the results match, then the implementation is likely correct. Another way to verify an implementation is to examine the code and look for errors. This can be done manually or with the help of a software tool.",
            "One way to transform code written in a high-level language into an FSM is to use a compiler. A compiler is a program that translates code written in a high-level language into code that can be run on a computer. Another way to transform code written in a high-level language into an FSM is to use an interpreter. An interpreter is a program that executes code written in a high-level language."
        ],
        "score": [
            6.590216636657715,
            -11.212381362915039,
            -11.148759841918945
        ],
        "text": "confusing associations made even by engineering educators, we want to provide you with a concise set of definitions that generalizes correctly to more than two operands. You may have learned these functions based on truth values  (true and false), but we define them based on bits,  with 1 representing true and 0 representing false.  Table  on the next page lists logic operations.  The first column in the table lists the name of each function.  The second provides a fairly complete set of the notations that you are likely to encounter for each function, including both the forms used in engineering and those used in mathematics.  The third column defines  the function's value for two or more input operands (except for NOT, which operates on a single value).  The last column shows the form generally used in logic schematics/diagrams and mentions the important features used in distinguishing each function (in pictorial form usually called a { gate}, in reference to common physical implementations) from the  others.   { {|c|c|c|c|} { Function}& { Notation}& { Explanation}& { Schematic}  AND& {A AND B}{AB}{A}{A}{A}& {the ``all'' function: result is 1 iff}{{ all} input operands are equal to 1}& {flat input, round output}   OR& {A OR B}{A+B}{A}& {the ``any'' function: result is 1 iff}{{ any} input operand is equal to 1}& {{-6pt}}{round input, pointed output}  NOT& {NOT A}{A'}{}{}& {logical complement/negation:}{NOT 0 is 1, and NOT 1 is 0}& {triangle and circle}  {exclusive OR}& {A XOR B}{A}& {the ``odd'' function: result is 1 iff an { odd}}{number of input operands are equal to 1}& {{-6pt}}{OR with two lines}{on input side}  {``or''}& A, B, or C& {the ``one of'' function: result is 1 iff exactly}{{ one of} the input operands is equal to 1}& (not used)   } {Boolean logic operations, notation, definitions, and symbols.}{-12pt}      The first function of importance is { AND}.  Think of { AND} as the ``all'' function: given a set of input values as operands, AND evaluates  to 1 if and only if { all} of the input values are 1.  The first notation line simply uses the name of the function.  In Boolean algebra, AND is typically represented as multiplication, and the middle three forms reflect various ways in which we write multiplication.  The last notational variant is from mathematics, where the AND function is formally called { conjunction}.  The next function of importance is { OR}.  Think of { OR} as the ``any'' function: given a set of input values as operands, OR evaluates to 1 if and only if { any} of the input values is 1.  The actual number of input values equal to 1 only matters in the sense of whether it is at least one.  The notation for OR is organized in the same way as for AND, with the function name at the top, the algebraic variant that we will use in class---in this case addition---in the middle, and the mathematics variant, in this case called { disjunction}, at the bottom.  { The definition of Boolean OR is not the same as our use of  the word ``or''  in English.}  For example, if you are fortunate enough to enjoy a meal on a plane, you might be offered several choices: ``Would you like the chicken, the beef, or the vegetarian lasagna today?''  Unacceptable answers to this English question include: ``Yes,'' ``Chicken and lasagna,'' and any other combination that involves more than a single choice!  You may have noticed that we might have instead mentioned that AND evaluates to 0 if any input value is 0, and that OR evaluates to 0 if all input values are 0.  These relationships reflect a mathematical duality underlying Boolean logic that has important practical value in terms of making it easier for humans to digest complex logic expressions. We will talk more about duality later in the course, but you should learn some of the practical value now: if you are trying to evaluate an AND function, look for an input with value 0; if you are trying to evaluate an OR function, look for an  input with value 1.  If you find such"
    },
    {
        "question": " How does the vending machine know how much each item costs?\\n\\n",
        "answers": [
            "\nThe vending machine knows how much each item costs because the owner of the machine sets the prices dynamically. The machine keeps track of how much money has been inserted as a multiple of 5 cents (one nickel). The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.",
            "A random access memory (RAM) is a type of memory that allows data to be stored and retrieved in any order. RAM is volatile, which means that it loses all data when the power supply is removed. There are two types of RAM: static RAM (SRAM) and dynamic RAM (DRAM).",
            "A sense amplifier is a circuit that is used to read the data stored in a memory cell. The sense amplifier amplifies the small signal from the memory cell and converts it into a digital signal that can be read by the processor."
        ],
        "score": [
            8.383125305175781,
            -11.086968421936035,
            -11.123912811279297
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " How does the vending machine know how much money is currently inside of it?\\n\\n",
        "answers": [
            "\nThe vending machine keeps track of how much money has been inserted as a multiple of 5 cents (one nickel). It has six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).",
            "A priority encoder is a type of encoder that generates a signal whenever any of its input lines are active, and encodes the index of the highest active input as a two-bit unsigned number. A priority encoder is used to determine which of its input lines has the highest priority.",
            "One way to design an FSM is to start with a flow chart of the program to be implemented. This flow chart can be used to identify high-level states that can be implemented in a fixed number of clock cycles. Another way to design an FSM is to start with a list of components that are needed to implement the program. This list of components can be used to identify high-level states that can be implemented in a fixed number of clock cycles."
        ],
        "score": [
            5.289551734924316,
            -11.021894454956055,
            -11.226476669311523
        ],
        "text": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we"
    },
    {
        "question": " How does the vending machine know what coin was inserted?\\n\\n",
        "answers": [
            "\nThe vending machine uses sensors to detect what coin was inserted. The first sensor is positioned so that a dime will pass beneath it, while the second sensor is positioned so that all coins will pass in front of it. The signal from the second sensor can be used as a clock for the FSM, while the signal from the first sensor indicates whether the coin is a dime or a quarter.",
            "A tri-state buffer achieves logical disconnection by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT.",
            "The counter's implementation diagram simplifies understanding the diagram by breaking it down into smaller, more manageable pieces. By doing this, it is easier to see how the various parts of the diagram interact with each other. Additionally, it makes it easier to identify potential problems or areas for improvement."
        ],
        "score": [
            8.133432388305664,
            -11.151228904724121,
            -11.15831470489502
        ],
        "text": "hardware  improvements to reduce the rate of student headaches. Finally, Prof. Kirill Levchenko together with UA Saidivya Ashok struck a blow against COVID-19 by developing an inexpensive and portable replacement for the physical ``vending machine'' systems used for testing in previous semesters.  {Physical Design, Sensors, and Timing}  A user inserts a coin into a slot at one end of the device.  The coin then rolls down a slope towards a gate controlled by a servo.  The gate can be raised or lowered, and determines whether the coin exits from the other side or the bottom of the device.  As the coin rolls, it passes two optical sensors.{The full system actually allows four sensors to differentiate four types of coins, but our lab uses only two of these sensors.}  One of these sensors is  positioned high enough above the slope that a dime passes beneath the sesnor, allowing the signal T produced by the sensor to tell us whether  the coin is a dime or a quarter.  The second sensor is positioned so that all coins pass in front of it.  The sensor positions are chosen  carefully to ensure that, in the case of a quarter, the coin is still blocking the first sensor when it reaches the second sensor.   Blocked sensors give a signal of 1 in this design, so the rising edge  the signal from the second sensor can be used as a ``clock'' for our  FSM.  When the rising edge occurs, the signal T from the first sensor  indicates whether the coin is a quarter (T=1) or a dime (T=0).   A sample timing diagram for the lab appears to the right.  The clock signal generated by the lab is not only not a square wave---in other words, the high and low portions are not equal---but is also unlikely to be periodic. Instead, the ``cycle'' is defined by the time between coin insertions. The T signal serves as the single input to our FSM.  In the timing   {file=part3/figs/lab-timing.eps,width=2.55in}   diagram, T is shown as rising and falling before the clock edge. We use positive edge-triggered flip-flops to implement our FSM, thus the aspect of the relative timing that matters to our design is that, when the clock rises, the value of T is stable and indicates  the type of coin entered.  The signal T may fall before or after the clock does---the two are equivalent for our FSM's needs.  The signal A in the timing diagram is an output from the FSM, and indicates whether or not the coin should be accepted.  This signal  controls the servo that drives the gate, and thus determines whether the coin is accepted (A=1) as payment or rejected (A=0) and returned to the user.    Looking at the timing diagram, you should note that our FSM makes  a decision based on its current state and the input T and enters a  new state at the rising clock edge.  The value of A in the next cycle thus determines the position of the gate when the coin eventually rolls to the end of the slope.  As we said earlier, our FSM is thus a Moore machine: the output A does not depend on the input T, but only on the current internal  state bits of the the FSM.  However, you should also now realize that making A depend on T is not adequate for this lab.  If A were to rise with T and fall with the rising clock edge (on entry to the next state), or even fall with the falling edge of T, the gate would return to the reject position by the time the coin reached the gate, regardless of our FSM's decision!  {An Abstract Model}   We start by writing down states for a user's expected behavior. Given the fairly tight constraints that we have placed on our lab, few combinations are pos-   {c|cc|cc} state& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P)  START& DIME& QUARTER& & no DIME& & PAID& yes& no QUARTER& PAID& & yes& no PAID& & & yes& yes    sible.  For a total of 35 cents, a user should either insert a dime  followed by a quarter, or a quarter followed by a dime.  We begin in a START state, which transitions to states DIME or QUARTER when the user inserts the first coin.  With no previous coin, we"
    },
    {
        "question": " How does the vending machine release the item?\\n\\n",
        "answers": [
            "\nThe vending machine releases the item by using a decoder to generate the three individual item release outputs. The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next cycle.",
            "There are three variables used in the code, which are x, y, and z. The value of x is initialized to 42, and the value of y is initialized to x. The value of z is not initialized.",
            "The purpose of the FSM is to control the stoplights at a busy intersection. The FSM ensures that the lights on one or both roads are red at all times, that a road has a green light, and that the controller should show a yellow light before showing a red light."
        ],
        "score": [
            8.657143592834473,
            -11.05378246307373,
            -11.151135444641113
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " How does the vending machine know when a purchase is requested?']",
        "answers": [
            "\nThe vending machine knows when a purchase is requested when the user pushes one of the item selection buttons. This causes the priority encoder to produce a signal P indicating that the user is trying to make a purchase.",
            "The second step in the design process is to think carefully about metrics of possible interest. Once you have a set of metrics that you feel is complete, your next step is to get rid of as many as you can.",
            "SRAM is faster than DRAM, but DRAM is denser (has more bits per chip area). A typical SRAM cell uses six transistors, while a DRAM cell uses only one transistor. However, memory designers have also made significant advances in further miniaturizing DRAM cells to improve density beyond the benefit available from simple transistor"
        ],
        "score": [
            9.787164688110352,
            -11.250181198120117,
            -11.203563690185547
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": "['\\n\\n\\n\\n",
        "answers": [
            "\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\",
            "The FSM alternates green lights between the two roads by setting the timer to a different value when the state of the FSM changes. This causes the lights to change color on a cycle-by-cycle basis, with the green light alternating between the two roads.",
            "The next state value for S0 is 0. This is because the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes. The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1. The top mux selects B when S_0=1, and selects C when S_0=0."
        ],
        "score": [
            7.947632312774658,
            -9.456941604614258,
            -9.320371627807617
        ],
        "text": "representation, we can add by counting clockwise around the circle, and we can subtract by counting in a counterclockwise direction around the circle.  With an unsigned representation, we choose to use the group from [0,15] (the middle group in the diagram markings to the right) as the number represented by each of the patterns.  Overflow occurs with unsigned addition (or subtraction) because we can only choose one value for each binary pattern.      In fact, we can choose any single value for each pattern to create a  representation, and our add unit will always produce results that are correct modulo 16.  Look back at our overflow example, where we added 12 and 6 to obtain 2, and notice that (2=18)  16. Normally, only a contiguous sequence of integers makes a useful representation, but we do not have to restrict ourselves to  non-negative numbers.  The 2's complement representation can then be defined by choosing a  set of integers balanced around zero from the groups.  In the circle  diagram, for example, we might choose to represent numbers in the range [-7,7] when using 4 bits.  What about the last pattern, 1000? We could choose to represent either -8 or 8.  The number of arithmetic operations that overflow is the same with both choices (the choices are symmetric around 0, as are the combinations of input operands that  overflow), so we gain nothing in that sense from either choice. If we choose to represent -8, however, notice that all patterns starting with a 1 bit then represent negative numbers.  No such simple check arises with the opposite choice, and thus an {N-bit} 2's complement  representation is defined to represent the range [-2^{N-1},2^{N-1}-1], with patterns chosen as shown in the circle.  {An Algebraic Approach}  Some people prefer an algebraic approach to understanding the definition of 2's complement, so we present such an approach next. Let's start by writing f(A,B) for the result of our add unit:  {eqnarray*} f(A,B) = (A + B)  2^N {eqnarray*}  We assume that we want to represent a set of integers balanced around 0 using our signed representation, and that we will use the same binary patterns as we do with an unsigned representation to represent non-negative numbers.  Thus, with an {N-bit} representation, the patterns in the range [0,2^{N-1}-1] are the same as those used with an unsigned representation.  In this case, we are left with all patterns beginning with a 1 bit.  The question then is this: given an integer k, 2^{N-1}>k>0, for which we  want to find a pattern to represent -k, and any integer m that we might want to add to -k,  can we find another integer p>0 such that    (-k + m = p + m)  2^N   ?   If we can, we can use p's representation to represent -k and our unsigned addition unit f(A,B) will work correctly.  To find the value p, start by subtracting m from both sides of Equation () to obtain:   (-k = p)  2^N   Note that (2^N=0)  2^N, and add this equation to  Equation () to obtain  {eqnarray*} (2^N-k = p)  2^N {eqnarray*}  Let p=2^N-k.    For example, if N=4, k=3 gives p=16-3=13, which is the pattern 1101. With N=4 and k=5, we obtain p=16-5=11, which is the pattern 1011. In general, since 2^{N-1}>k>0,  we have 2^{N-1}<p<2^N.  But these patterns are all unused---they all start with a 1 bit!---so the patterns that we have defined for negative numbers are disjoint from those that we used for positive numbers, and the meaning of each pattern is unambiguous.  The algebraic definition of bit patterns for negative numbers also matches our circle diagram from the last section exactly, of course.    {Negating 2's Complement Numbers}  The algebraic approach makes understanding negation of an integer represented using 2's complement fairly straightforward, and gives  us an easy procedure for doing so. Recall that given an integer k in an {N-bit} 2's complement representation, the {N-bit} pattern for -k is given by 2^N-k  (also true for k=0 if we keep only the low N bits of the result).   But"
    },
    {
        "question": " What is the input of the FSM?\\nA: The input is a reset button R, and four buttons B_F, B_X, B_G, B_C.\\n\\n",
        "answers": [
            "\nThe input of the FSM is a reset button R, and four buttons B_F, B_X, B_G, B_C. The FSM produces a signal P indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants. The signal R is also used to enable a decoder that generates the three individual item release outputs.",
            "Why is two-level logic popular?\n\nTwo-level logic is popular because it is simple to map expressions into CMOS gates. Additionally, it is easy to understand gate diagrams.",
            "The fourth step in the design process is to choose a particular design from the set of possible designs. In making this choice, engineers typically consider a variety of factors, including cost, performance, manufacturability, and reliability."
        ],
        "score": [
            8.966412544250488,
            -9.328535079956055,
            -9.062488555908203
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " What is the output of the FSM?\\nA: The output is the position of the farmer, fox, goose, and corn.\\n\\n",
        "answers": [
            "\nThe output of the FSM is the position of the farmer, fox, goose, and corn. The output is either the left bank or the right bank of the river. The game also has two indicators: W for reaching the winning state, and L for reaching a losing state.",
            "Abstraction is the process of simplifying a complex system by identifying the most important aspects of that system and ignoring the rest. In the context of implementation, abstraction can be used to simplify a design by identifying the most important features of the design and ignoring the rest. This can make the design more manageable and easier to understand.",
            "What are the three ranges for a given metric?\n\nThe first range is all measurement values are equivalently useful. The second range is possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice."
        ],
        "score": [
            9.06120491027832,
            -10.509489059448242,
            -9.965042114257812
        ],
        "text": "corn along with the farmer.  The farmer knows that if he leaves the fox alone with the goose, the fox will eat the goose.  Similarly, if the farmer leaves the goose alone with the corn, the goose will  eat the corn.  How can the farmer cross the river?  Your team decides to build a board illustrating the problem with a river from top to bottom and lights illustrating the positions of  the farmer (always with the boat), the fox, the goose, and the corn on either the left bank or the right bank of the river. Everything starts on the left bank, and the children can play  the game until they win by getting everything to the right bank or until they make a mistake.  As the ECE major on your team, you get to design the FSM!  {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates}  Since the four entities (farmer, fox, goose, and corn) can be only on one bank or the other, we can use one bit to represent the location of each entity.  Rather than giving the states names, let's just call a state FXGC.  The value of F represents the location of the farmer, either on the left bank (F=0) or the right bank (F=1).  Using the same representation (0 for the left bank, 1 for the right bank), the value of X represents the location of the fox, G represents the location of the goose, and C represents the location of the corn.   We can now put together an abstract next-state table, as shown to the right.  Once the player wins or loses, let's have the game indicate their final status and stop accepting requests to have the farmer cross the river.  We can use a reset button to force the game back into the original state for the next player.  Note that we have included conditions for some of the input events, as  we did previously   {c|l|c|c} initial state& {|c|}{input event}& condition& final state  FXGC& no input& always& FXGC FXGC& reset & always& 0000 FXGC& cross alone& always& XGC FXGC& cross with fox& F=X& GC FXGC& cross with goose& F=G& XC FXGC& cross with corn& F=C& XG    with the vending machine design.  The conditions here require that the farmer be on the same bank as any entity that the player wants the farmer to carry across the river.  Next, we specify the I/O interface.   For input, the game has five buttons.  A reset button R forces the FSM back into the initial state.  The other four buttons cause the farmer to cross the river: B_F crosses alone, B_X with the fox, B_G with the goose, and B_C with the corn.  For output, we need position indicators for the four entities, but let's assume that we can simply output the current state FXGC and have appropriate images or lights appear on the correct banks of the  river.  We also need two more indicators: W for reaching the winning state, and L for reaching a losing state.  Now we are ready to complete the specification.  We could use a strict prioritization of input events, as we did with earlier examples.  Instead, in order to vary the designs a bit, we use a strict prioritization among  allowed inputs.  The reset button R has the highest priority, followed by B_F, B_C, B_G, and finally B_X.  However, only those buttons  that result in an allowed move are considered when selecting one button  among several pressed in a single clock cycle.   As an example, consider the  state FXGC=0101.  The farmer is not on the same bank as the fox, nor as the corn, so the B_X and B_C buttons are ignored, leading to the next-state table to the right.  Notice that B_G is accepted even if B_C is pressed because the farmer is not on the same   {cccccc|c} FXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+  0101& 1& x& x& x& x& 0000 0101& 0& 1& x& x& x& 1101 0101& 0& 0& x& 1& x& 1111 0101& 0& 0& x& 0& x& 0101    bank as the corn. As shown later, this approach to prioritization of inputs is equally simple in terms of implementation.     Recall that we want to stop the game when the player wins or loses. In these states, only the reset button"
    },
    {
        "question": " What is the state representation?\\nA: The state is represented by FXGC, where F is the position of the farmer, X is the position of the fox, G is the position of the goose, and C is the position of the corn.\\n\\n",
        "answers": [
            "\nThe state is represented by FXGC, where F is the position of the farmer, X is the position of the fox, G is the position of the goose, and C is the position of the corn. The game has five buttons: reset, cross alone, cross with fox, cross with goose, and cross with corn. The game stops when the player wins or loses.",
            "The benefits of using DRAM are that it is denser than SRAM and uses fewer transistors. This makes DRAM ideal for applications where size is more important than speed.",
            "An FSM must be initialized to ensure that it starts in a known state. This allows the FSM to function correctly and avoid any potential problems that could arise from an unknown starting state."
        ],
        "score": [
            6.623587608337402,
            -10.082265853881836,
            -7.930938720703125
        ],
        "text": "corn along with the farmer.  The farmer knows that if he leaves the fox alone with the goose, the fox will eat the goose.  Similarly, if the farmer leaves the goose alone with the corn, the goose will  eat the corn.  How can the farmer cross the river?  Your team decides to build a board illustrating the problem with a river from top to bottom and lights illustrating the positions of  the farmer (always with the boat), the fox, the goose, and the corn on either the left bank or the right bank of the river. Everything starts on the left bank, and the children can play  the game until they win by getting everything to the right bank or until they make a mistake.  As the ECE major on your team, you get to design the FSM!  {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates}  Since the four entities (farmer, fox, goose, and corn) can be only on one bank or the other, we can use one bit to represent the location of each entity.  Rather than giving the states names, let's just call a state FXGC.  The value of F represents the location of the farmer, either on the left bank (F=0) or the right bank (F=1).  Using the same representation (0 for the left bank, 1 for the right bank), the value of X represents the location of the fox, G represents the location of the goose, and C represents the location of the corn.   We can now put together an abstract next-state table, as shown to the right.  Once the player wins or loses, let's have the game indicate their final status and stop accepting requests to have the farmer cross the river.  We can use a reset button to force the game back into the original state for the next player.  Note that we have included conditions for some of the input events, as  we did previously   {c|l|c|c} initial state& {|c|}{input event}& condition& final state  FXGC& no input& always& FXGC FXGC& reset & always& 0000 FXGC& cross alone& always& XGC FXGC& cross with fox& F=X& GC FXGC& cross with goose& F=G& XC FXGC& cross with corn& F=C& XG    with the vending machine design.  The conditions here require that the farmer be on the same bank as any entity that the player wants the farmer to carry across the river.  Next, we specify the I/O interface.   For input, the game has five buttons.  A reset button R forces the FSM back into the initial state.  The other four buttons cause the farmer to cross the river: B_F crosses alone, B_X with the fox, B_G with the goose, and B_C with the corn.  For output, we need position indicators for the four entities, but let's assume that we can simply output the current state FXGC and have appropriate images or lights appear on the correct banks of the  river.  We also need two more indicators: W for reaching the winning state, and L for reaching a losing state.  Now we are ready to complete the specification.  We could use a strict prioritization of input events, as we did with earlier examples.  Instead, in order to vary the designs a bit, we use a strict prioritization among  allowed inputs.  The reset button R has the highest priority, followed by B_F, B_C, B_G, and finally B_X.  However, only those buttons  that result in an allowed move are considered when selecting one button  among several pressed in a single clock cycle.   As an example, consider the  state FXGC=0101.  The farmer is not on the same bank as the fox, nor as the corn, so the B_X and B_C buttons are ignored, leading to the next-state table to the right.  Notice that B_G is accepted even if B_C is pressed because the farmer is not on the same   {cccccc|c} FXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+  0101& 1& x& x& x& x& 0000 0101& 0& 1& x& x& x& 1101 0101& 0& 0& x& 1& x& 1111 0101& 0& 0& x& 0& x& 0101    bank as the corn. As shown later, this approach to prioritization of inputs is equally simple in terms of implementation.     Recall that we want to stop the game when the player wins or loses. In these states, only the reset button"
    },
    {
        "question": " What is the next state when the farmer is on the left bank, the fox is on the right bank, the goose is on the left bank, and the corn is on the right bank?\\nA: The next state is FXGC=0101.', '\\n\\n",
        "answers": [
            "\nThe next state is FXGC=0101. The farmer is not on the same bank as the fox, nor as the corn, so the B_X and B_C buttons are ignored, leading to the next-state table to the right. Notice that B_G is accepted even if B_C is pressed because the farmer is not on the same bank as the corn.",
            "One important reason to consider all possible input conditions from all possible states is that it can help ensure that a design is correct. For example, if a design only ever receives input values that are within a certain range, it is possible that the design is not actually correct for all input values. Considering all possible input conditions can help catch errors like this.\n\nAnother reason to consider all possible input conditions is that it can help optimize a design. For example, if a design only ever receives input values that are within a certain range, it is possible to simplify the design or make it more efficient. Considering all possible input conditions can help identify opportunities for optimization.",
            "Tri-state buffers were used to reduce the number of pins needed on chips by combining inputs and outputs. This allowed chip designers to halve the number of pins needed."
        ],
        "score": [
            6.3396124839782715,
            -6.744234085083008,
            -4.023640155792236
        ],
        "text": "corn along with the farmer.  The farmer knows that if he leaves the fox alone with the goose, the fox will eat the goose.  Similarly, if the farmer leaves the goose alone with the corn, the goose will  eat the corn.  How can the farmer cross the river?  Your team decides to build a board illustrating the problem with a river from top to bottom and lights illustrating the positions of  the farmer (always with the boat), the fox, the goose, and the corn on either the left bank or the right bank of the river. Everything starts on the left bank, and the children can play  the game until they win by getting everything to the right bank or until they make a mistake.  As the ECE major on your team, you get to design the FSM!  {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates}  Since the four entities (farmer, fox, goose, and corn) can be only on one bank or the other, we can use one bit to represent the location of each entity.  Rather than giving the states names, let's just call a state FXGC.  The value of F represents the location of the farmer, either on the left bank (F=0) or the right bank (F=1).  Using the same representation (0 for the left bank, 1 for the right bank), the value of X represents the location of the fox, G represents the location of the goose, and C represents the location of the corn.   We can now put together an abstract next-state table, as shown to the right.  Once the player wins or loses, let's have the game indicate their final status and stop accepting requests to have the farmer cross the river.  We can use a reset button to force the game back into the original state for the next player.  Note that we have included conditions for some of the input events, as  we did previously   {c|l|c|c} initial state& {|c|}{input event}& condition& final state  FXGC& no input& always& FXGC FXGC& reset & always& 0000 FXGC& cross alone& always& XGC FXGC& cross with fox& F=X& GC FXGC& cross with goose& F=G& XC FXGC& cross with corn& F=C& XG    with the vending machine design.  The conditions here require that the farmer be on the same bank as any entity that the player wants the farmer to carry across the river.  Next, we specify the I/O interface.   For input, the game has five buttons.  A reset button R forces the FSM back into the initial state.  The other four buttons cause the farmer to cross the river: B_F crosses alone, B_X with the fox, B_G with the goose, and B_C with the corn.  For output, we need position indicators for the four entities, but let's assume that we can simply output the current state FXGC and have appropriate images or lights appear on the correct banks of the  river.  We also need two more indicators: W for reaching the winning state, and L for reaching a losing state.  Now we are ready to complete the specification.  We could use a strict prioritization of input events, as we did with earlier examples.  Instead, in order to vary the designs a bit, we use a strict prioritization among  allowed inputs.  The reset button R has the highest priority, followed by B_F, B_C, B_G, and finally B_X.  However, only those buttons  that result in an allowed move are considered when selecting one button  among several pressed in a single clock cycle.   As an example, consider the  state FXGC=0101.  The farmer is not on the same bank as the fox, nor as the corn, so the B_X and B_C buttons are ignored, leading to the next-state table to the right.  Notice that B_G is accepted even if B_C is pressed because the farmer is not on the same   {cccccc|c} FXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+  0101& 1& x& x& x& x& 0000 0101& 0& 1& x& x& x& 1101 0101& 0& 0& x& 1& x& 1111 0101& 0& 0& x& 0& x& 0101    bank as the corn. As shown later, this approach to prioritization of inputs is equally simple in terms of implementation.     Recall that we want to stop the game when the player wins or loses. In these states, only the reset button"
    },
    {
        "question": " How does the player win the game?\\n",
        "answers": [
            "\nThe player wins the game by getting all four entities (farmer, fox, goose, and corn) to the right bank of the river. If the player makes a mistake, the game is over and they lose.",
            "The purpose of the vertical lines carrying the current state values and their inverses back to the next state logic inputs is to ensure that the state of the FSM is always consistent. This is necessary for safety reasons, as the FSM must always ensure that the lights on one or both roads are red at all times.",
            "A bad state representation can lead to wasted engineering effort, as well as products that have to be cancelled due to poor performance. In some cases, a bad state representation can also lead to increased costs for fabrication and testing."
        ],
        "score": [
            7.523202896118164,
            -11.214384078979492,
            -11.217972755432129
        ],
        "text": "corn along with the farmer.  The farmer knows that if he leaves the fox alone with the goose, the fox will eat the goose.  Similarly, if the farmer leaves the goose alone with the corn, the goose will  eat the corn.  How can the farmer cross the river?  Your team decides to build a board illustrating the problem with a river from top to bottom and lights illustrating the positions of  the farmer (always with the boat), the fox, the goose, and the corn on either the left bank or the right bank of the river. Everything starts on the left bank, and the children can play  the game until they win by getting everything to the right bank or until they make a mistake.  As the ECE major on your team, you get to design the FSM!  {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates}  Since the four entities (farmer, fox, goose, and corn) can be only on one bank or the other, we can use one bit to represent the location of each entity.  Rather than giving the states names, let's just call a state FXGC.  The value of F represents the location of the farmer, either on the left bank (F=0) or the right bank (F=1).  Using the same representation (0 for the left bank, 1 for the right bank), the value of X represents the location of the fox, G represents the location of the goose, and C represents the location of the corn.   We can now put together an abstract next-state table, as shown to the right.  Once the player wins or loses, let's have the game indicate their final status and stop accepting requests to have the farmer cross the river.  We can use a reset button to force the game back into the original state for the next player.  Note that we have included conditions for some of the input events, as  we did previously   {c|l|c|c} initial state& {|c|}{input event}& condition& final state  FXGC& no input& always& FXGC FXGC& reset & always& 0000 FXGC& cross alone& always& XGC FXGC& cross with fox& F=X& GC FXGC& cross with goose& F=G& XC FXGC& cross with corn& F=C& XG    with the vending machine design.  The conditions here require that the farmer be on the same bank as any entity that the player wants the farmer to carry across the river.  Next, we specify the I/O interface.   For input, the game has five buttons.  A reset button R forces the FSM back into the initial state.  The other four buttons cause the farmer to cross the river: B_F crosses alone, B_X with the fox, B_G with the goose, and B_C with the corn.  For output, we need position indicators for the four entities, but let's assume that we can simply output the current state FXGC and have appropriate images or lights appear on the correct banks of the  river.  We also need two more indicators: W for reaching the winning state, and L for reaching a losing state.  Now we are ready to complete the specification.  We could use a strict prioritization of input events, as we did with earlier examples.  Instead, in order to vary the designs a bit, we use a strict prioritization among  allowed inputs.  The reset button R has the highest priority, followed by B_F, B_C, B_G, and finally B_X.  However, only those buttons  that result in an allowed move are considered when selecting one button  among several pressed in a single clock cycle.   As an example, consider the  state FXGC=0101.  The farmer is not on the same bank as the fox, nor as the corn, so the B_X and B_C buttons are ignored, leading to the next-state table to the right.  Notice that B_G is accepted even if B_C is pressed because the farmer is not on the same   {cccccc|c} FXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+  0101& 1& x& x& x& x& 0000 0101& 0& 1& x& x& x& 1101 0101& 0& 0& x& 1& x& 1111 0101& 0& 0& x& 0& x& 0101    bank as the corn. As shown later, this approach to prioritization of inputs is equally simple in terms of implementation.     Recall that we want to stop the game when the player wins or loses. In these states, only the reset button"
    },
    {
        "question": " How does the player lose the game?\\n",
        "answers": [
            "\nThe player loses the game if they are not able to correctly answer the question. If the player gets the question wrong, they will have to start the game over from the beginning.",
            "The input sequence for initialization is: 1) the value in R0 is 10, 2) the value in R1 is 20, and 3) the value in R2 is 30.",
            "The control structure of the code maps into a high-level FSM design by breaking down the program into simple steps, one step of a flow chart may sometimes require more than one state in an FSM. Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously."
        ],
        "score": [
            7.596487045288086,
            -11.213409423828125,
            -11.277536392211914
        ],
        "text": "range corresponds to systems in which when a processor's power consumption in a digital  system is extremely low relative to the power consumption of the system. For example, the processor in a computer might use less than 1  of the total used by  the system including the disk drive, the monitor, the power  supply, and so forth.  One power consumption value in this range  is just as good as any another, and no one cares about the power consumption of the processor  in such cases.  In the second range, power consumption of the processor makes a difference.  Cell phones use most of their energy in radio operation, for example, but if you own a phone with a powerful processor, you may have noticed that you can turn off the phone and  drain the battery fairly quickly by playing a game.  Designing a processor that uses half as much power lengthens the battery life in such cases.  Finally, the third region of power consumption measurements is impossible: if you use so much power, your chip will overheat or even burst into flames.  Consumers get unhappy when such things happen.  As a first step, you can remove any metrics for which all solutions are effectively equivalent.  Until a little less than a decade ago, for example, the power  consumption of a desktop processor actually was in the first range that we discussed.  Power was simply not a concern to engineers: all designs of  interest consumed so little power that no one cared.  Unfortunately, at that point, power consumption jumped into the third range rather quickly.  Processors hit a wall, and  products had to be cancelled.  Given that the time spent designing a processor has historically been about five years, a lot of engineering effort was wasted because people had not thought carefully enough about power (since it had never mattered in the past).  Today, power is an important metric that engineers must take into account in their designs.   However, in some areas, such as desktop and high-end server processors, other metrics (such as performance) may be so  important that we always want to operate at the edge of the interesting range.  In such cases, we might choose to treat  a metric such as power consumption as a { threshold}: stay below 150 Watts for a desktop processor, for example.  One still has to make a coordinated effort to ensure that the system as a whole does not exceed the threshold, but reasoning about  threshold values, a form of constraint, is easier than trying to think about multiple metrics at once.  Some metrics may only allow discrete quantification.  For example, one could choose to define compatibility with previous processor generations as binary: either an existing piece of software (or operating system) runs out of the box on your new processor, or it does not.  If you  want people who own that software to make use of your new processor, you must ensure that the value of this binary metric is 1, which can also be viewed as a threshold.  In some cases, two metrics may be strongly { correlated}, meaning that a design that is good for one of the metrics is frequently  good for the other metric as well.  Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider them separately.  A design that requires a larger chip is probably more complex, and thus takes more engineering time to get right (engineering time costs money).    Each silicon wafer costs money to fabricate, and fewer copies of a  large design fit on one wafer, so large chips mean more fabrication cost.  Physical defects in silicon can cause some chips not to work.  A large chip uses more silicon than a small one, and is thus more likely to suffer from defects (and not work).  Cost thus goes up again for large chips relative to small ones.  Finally, large chips usually require more careful testing to ensure that they work properly (even ignoring the cost of getting the design right, we have to test for the presence of defects), which"
    },
    {
        "question": " What is the input of the FSM?\\n",
        "answers": [
            "\nThe input of the FSM is a 3-bit register S and a 6-bit timer. The timer is used to count down and produce an output of Z=1 when it reaches 0. The register S is used to store the state of the FSM.",
            "The first approach is to use the flip-flop design to add a single bit and pass any necessary information into another copy of itself. By using copies of this bit-sliced adder circuit, we can mimic our approach as humans and build adders of any size.",
            "A row is a natural basis for the refresh cycle because the DRAM data pins provide bidirectional signals for reading and writing elements of the DRAM. An output enable input, OE, controls tri-state buffers with the DRAM to determine whether or not the DRAM drives the data pins. The WE input, which controls the type of operation, is also present."
        ],
        "score": [
            7.284553527832031,
            -11.267501831054688,
            -9.309195518493652
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the output of the FSM?\\n",
        "answers": [
            "\nThe output of the FSM is a sequence of red, yellow, and green lights that alternate between the two roads. The duration of each state is determined by the signal F. If F is 0, the timer is set only when the state S changes. If F is 1, the timer is set when the state S changes or when the light on one of the roads is green.",
            "The issue with the FSM getting into unexpected states is that it can cause the output of the circuit to glitch. This can be problematic in some applications where a stable output is required.",
            "The design continues by choosing a state representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning."
        ],
        "score": [
            7.01418399810791,
            1.20225191116333,
            -11.299173355102539
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the purpose of the priority encoder?', '\\n\\n\\n\\n",
        "answers": [
            "\nThe priority encoder is used to determine which item the user wants to purchase, and to generate a signal indicating that the user is trying to make a purchase. The output of the priority encoder is used to select the value of one of the price registers, which is then subtracted from the current number of nickels stored in a register. If the purchase is successful, the price is subtracted from the number of nickels and the item is released.",
            "The transitions from all states are well-defined because the FSM has a timer that counts down and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.",
            "The HOLD variable is used to save energy. When a chip finishes its work, it can turn itself off, saving energy. This variable allows the chip to turn itself off when it is finished, saving energy."
        ],
        "score": [
            7.398893356323242,
            -10.282516479492188,
            -9.202279090881348
        ],
        "text": "for our priority encoder appears on the  left below, with {K-maps} for each of the output bits on the right.   {cccc|cc} B_3& B_2& B_1& B_0& P& S  1&x&x&x& 1& 11 0&1&x&x& 1& 10 0&0&1&x& 1& 01 0&0&0&1& 1& 00 0&0&0&0& 0& xx           From the {K-maps}, we extract the following equations: {eqnarray*} P &=& B_3 + B_2 + B_1 + B_0 S_1 &=& B_3 + B_2 S_0 &=& B_3 + {B_2}B_1 {eqnarray*} which allow us to implement our encoder as shown to the right.  If we connect our buttons B_1, B_2, and B_3 to the priority  encoder (and feed 0 into the fourth input), it produces a signal P  indicating that the user is trying to make a purchase and a two-bit signal S indicating which item the user wants.      We also need to build logic to control the item release outputs R_1, R_2, and R_3.  An item should be released only when it has been selected  (as indicated by the priority encoder signal S) and the vending machine has enough money.  For now, let's leave aside calculation of the item  release signal, which we call R, and focus on how we can produce the correct values of R_1, R_2, and R_3 from S and R.  The component to the right is a { decoder} with an enable input.  A  decoder takes an input signal---typically one coded as a binary number---and  produces one output for each possible value of the signal.  You may notice the similarity with the structure of a mux: when the decoder is enabled (EN=1), each of the AND gates produces      one minterm of the input signal S.  In the mux, each of the inputs is then included in one minterm's AND gate, and the outputs of all AND gates are ORd together. In the decoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produces exactly one 1 bit on its outputs. When not enabled (EN=0), the decoder produces all 0 bits.  We use a decoder to generate the release signals for the vending machine by connecting the signal S produced by the  priority encoder to the decoder's S input and connecting the item release signal R to the decoder's EN input.  The outputs D_1, D_2, and D_3 then correspond to the individual item release  signals R_1, R_2, and R_3 for our vending machine.  {Vending Machine Implementation}   We are now ready to implement the FSM to handle purchases, as shown to the  right.  The current number of nickels, N, is stored in a register in the center of the diagram.  Each cycle, N is fed into a {6-bit} adder, which subtracts the price of any purchase requested in that cycle.   Recall that we chose to record item prices in registers.  We avoid the  need to negate prices before adding them by storing the negated prices in our registers.  Thus, the value of register PRICE1 is -P_1, the the value of register PRICE2 is -P_2, and the the value of register PRICE3 is -P_3.  The priority encoder's S signal is then used to select the value of  one of these three registers (using a {24-to-6} mux) as the second input to the adder.  We use the adder to execute a subtraction, so the carry out C_  is 1 whenever the value of N is at least as great as the amount  being subtracted.  In that case, the purchase is successful.  The AND gate on the left calculates the signal R indicating a successful purchase, which is then used to select the next value of N using the {12-to-6} mux below the adder.    When no item selection buttons are pushed, P and thus R are both 0,  and the mux below the adder keeps N unchanged in the next cycle.   Similarly, if P=1 but N is insufficient, C_ and thus R are both 0, and again N does not change.  Only when P=1 and C_=1 is the purchase successful, in which case the price is subtracted from N in the next cycle.      The signal R is also used to enable a decoder that generates the three individual item release outputs.  The correct output is generated based on the decoded S signal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the next  cycle.  One minor note on the design so far: by hardwiring"
    },
    {
        "question": " How does the player win the game?\\n",
        "answers": [
            "\nThe player wins the game by getting all four entities (farmer, fox, goose, and corn) to the right bank of the river. If the player makes a mistake, the game is over and they lose.",
            "The color mapping for a three-bit RGB encoding system is a system where colors are represented by a combination of red, green, and blue light. The color mapping for such a system is shown to the right. Imagine that you are charged with creating a counter to drive a light through a sequence of colors.",
            "Yes, we can choose to use the counter's internal state directly as our output values. This is because the colors we need to produce as outputs are all unique bit patterns. By using the counter's internal state directly as our output values, we can simplify our design."
        ],
        "score": [
            7.523202896118164,
            -11.19443130493164,
            -11.201147079467773
        ],
        "text": "corn along with the farmer.  The farmer knows that if he leaves the fox alone with the goose, the fox will eat the goose.  Similarly, if the farmer leaves the goose alone with the corn, the goose will  eat the corn.  How can the farmer cross the river?  Your team decides to build a board illustrating the problem with a river from top to bottom and lights illustrating the positions of  the farmer (always with the boat), the fox, the goose, and the corn on either the left bank or the right bank of the river. Everything starts on the left bank, and the children can play  the game until they win by getting everything to the right bank or until they make a mistake.  As the ECE major on your team, you get to design the FSM!  {choose a state representation}{step-repn} {calculate logic expressions}{step-logic} {implement with flip-flops and gates}{step-gates}  Since the four entities (farmer, fox, goose, and corn) can be only on one bank or the other, we can use one bit to represent the location of each entity.  Rather than giving the states names, let's just call a state FXGC.  The value of F represents the location of the farmer, either on the left bank (F=0) or the right bank (F=1).  Using the same representation (0 for the left bank, 1 for the right bank), the value of X represents the location of the fox, G represents the location of the goose, and C represents the location of the corn.   We can now put together an abstract next-state table, as shown to the right.  Once the player wins or loses, let's have the game indicate their final status and stop accepting requests to have the farmer cross the river.  We can use a reset button to force the game back into the original state for the next player.  Note that we have included conditions for some of the input events, as  we did previously   {c|l|c|c} initial state& {|c|}{input event}& condition& final state  FXGC& no input& always& FXGC FXGC& reset & always& 0000 FXGC& cross alone& always& XGC FXGC& cross with fox& F=X& GC FXGC& cross with goose& F=G& XC FXGC& cross with corn& F=C& XG    with the vending machine design.  The conditions here require that the farmer be on the same bank as any entity that the player wants the farmer to carry across the river.  Next, we specify the I/O interface.   For input, the game has five buttons.  A reset button R forces the FSM back into the initial state.  The other four buttons cause the farmer to cross the river: B_F crosses alone, B_X with the fox, B_G with the goose, and B_C with the corn.  For output, we need position indicators for the four entities, but let's assume that we can simply output the current state FXGC and have appropriate images or lights appear on the correct banks of the  river.  We also need two more indicators: W for reaching the winning state, and L for reaching a losing state.  Now we are ready to complete the specification.  We could use a strict prioritization of input events, as we did with earlier examples.  Instead, in order to vary the designs a bit, we use a strict prioritization among  allowed inputs.  The reset button R has the highest priority, followed by B_F, B_C, B_G, and finally B_X.  However, only those buttons  that result in an allowed move are considered when selecting one button  among several pressed in a single clock cycle.   As an example, consider the  state FXGC=0101.  The farmer is not on the same bank as the fox, nor as the corn, so the B_X and B_C buttons are ignored, leading to the next-state table to the right.  Notice that B_G is accepted even if B_C is pressed because the farmer is not on the same   {cccccc|c} FXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+  0101& 1& x& x& x& x& 0000 0101& 0& 1& x& x& x& 1101 0101& 0& 0& x& 1& x& 1111 0101& 0& 0& x& 0& x& 0101    bank as the corn. As shown later, this approach to prioritization of inputs is equally simple in terms of implementation.     Recall that we want to stop the game when the player wins or loses. In these states, only the reset button"
    },
    {
        "question": " How does the player lose the game?\\n",
        "answers": [
            "\nThe player loses the game if they are not able to get the correct answer to the question. If the player gets the correct answer, they win the game.",
            "If A were to rise with T and fall with the rising clock edge, then the processor would not be able to recognize interrupts between instructions. This would cause the program to terminate unexpectedly.",
            "A decoder is a logic device that converts binary information from the form of one code to another, usually a natural binary code. A multiplexer is a device that selects one of several analog or digital input signals and forwards the selected input into a single output line."
        ],
        "score": [
            7.860374927520752,
            -10.825838088989258,
            -11.139217376708984
        ],
        "text": "range corresponds to systems in which when a processor's power consumption in a digital  system is extremely low relative to the power consumption of the system. For example, the processor in a computer might use less than 1  of the total used by  the system including the disk drive, the monitor, the power  supply, and so forth.  One power consumption value in this range  is just as good as any another, and no one cares about the power consumption of the processor  in such cases.  In the second range, power consumption of the processor makes a difference.  Cell phones use most of their energy in radio operation, for example, but if you own a phone with a powerful processor, you may have noticed that you can turn off the phone and  drain the battery fairly quickly by playing a game.  Designing a processor that uses half as much power lengthens the battery life in such cases.  Finally, the third region of power consumption measurements is impossible: if you use so much power, your chip will overheat or even burst into flames.  Consumers get unhappy when such things happen.  As a first step, you can remove any metrics for which all solutions are effectively equivalent.  Until a little less than a decade ago, for example, the power  consumption of a desktop processor actually was in the first range that we discussed.  Power was simply not a concern to engineers: all designs of  interest consumed so little power that no one cared.  Unfortunately, at that point, power consumption jumped into the third range rather quickly.  Processors hit a wall, and  products had to be cancelled.  Given that the time spent designing a processor has historically been about five years, a lot of engineering effort was wasted because people had not thought carefully enough about power (since it had never mattered in the past).  Today, power is an important metric that engineers must take into account in their designs.   However, in some areas, such as desktop and high-end server processors, other metrics (such as performance) may be so  important that we always want to operate at the edge of the interesting range.  In such cases, we might choose to treat  a metric such as power consumption as a { threshold}: stay below 150 Watts for a desktop processor, for example.  One still has to make a coordinated effort to ensure that the system as a whole does not exceed the threshold, but reasoning about  threshold values, a form of constraint, is easier than trying to think about multiple metrics at once.  Some metrics may only allow discrete quantification.  For example, one could choose to define compatibility with previous processor generations as binary: either an existing piece of software (or operating system) runs out of the box on your new processor, or it does not.  If you  want people who own that software to make use of your new processor, you must ensure that the value of this binary metric is 1, which can also be viewed as a threshold.  In some cases, two metrics may be strongly { correlated}, meaning that a design that is good for one of the metrics is frequently  good for the other metric as well.  Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider them separately.  A design that requires a larger chip is probably more complex, and thus takes more engineering time to get right (engineering time costs money).    Each silicon wafer costs money to fabricate, and fewer copies of a  large design fit on one wafer, so large chips mean more fabrication cost.  Physical defects in silicon can cause some chips not to work.  A large chip uses more silicon than a small one, and is thus more likely to suffer from defects (and not work).  Cost thus goes up again for large chips relative to small ones.  Finally, large chips usually require more careful testing to ensure that they work properly (even ignoring the cost of getting the design right, we have to test for the presence of defects), which"
    },
    {
        "question": " What does the output of the FSM look like when the player wins?\\n",
        "answers": [
            "\nThe output of the FSM looks like a green light followed by a yellow light followed by a red light.",
            "The two approaches to FSM design are the state-transition table and the state diagram. The state-transition table lists all the possible states that the system can be in, and the state diagram shows the sequence of states that the system goes through. The state-transition table is more compact and easier to use when comparing functions, but the state diagram is more intuitive and easier to understand.",
            "There are 64 bit patterns available for the PAID state."
        ],
        "score": [
            6.309934616088867,
            -6.701812267303467,
            -11.188628196716309
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What does the output of the FSM look like when the player loses?\\n",
        "answers": [
            "\nWhen the player loses, the output of the FSM looks like it is red in both roads. This is because the controller must ensure that the lights on one or both roads are red at all times.",
            "The state representation for the lab FSM is a 3-bit binary number, with the following meaning:\n\nS_2: type of last coin inserted (0 for dime, 1 for quarter)\nS_1: more than one quarter inserted? (1 for yes, 0 for no)\nS_0: more than one dime inserted? (1 for yes, 0 for no)",
            "The 'QUARTER' state refers to the fact that the addition unit for our 4-bit examples is not closed under addition. This means that for any value of N, we can easily find two 4-bit unsigned numbers that, when added together, cannot be represented as a 4-bit unsigned number. With N=4, for example, we can add 12 (1100) and 6 (0110) to obtain 18. Since 18 is outside of the range [0,2^4-1] representable using the 4-bit unsigned representation, our representation breaks if we try to represent the sum using this representation."
        ],
        "score": [
            8.993245124816895,
            -7.521942615509033,
            -10.832630157470703
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How does the FSM handle multiple input events in a single clock cycle?']",
        "answers": [
            "\nThe FSM handles multiple input events in a single clock cycle by first storing the input events in a queue. When the FSM is ready to process an input event, it dequeues the event from the queue and processes it.",
            "The next-state logic for a three-bit Gray code counter is as follows:\n\nS_2^+ = S_2 + S_0\nS_1^+ = S_2\nS_0^+ = S_2 S_1",
            "The vending machine knows when a purchase is requested when the user pushes one of the item selection buttons. This causes the priority encoder to produce a signal P indicating that the user is trying to make a purchase."
        ],
        "score": [
            10.646621704101562,
            -11.022848129272461,
            -11.055450439453125
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What are the inputs of the FSM?\\n",
        "answers": [
            "\nThe inputs of the FSM are the states of the two roads, the timer, and the signal F.",
            "The FSM alternates green lights between the two roads by setting the timer to a different value when the state of the FSM changes. This causes the lights to change color on a cycle-by-cycle basis, with the green light alternating between the two roads.",
            "The state of the FSM has been split into two pieces: a {3-bit} register S and a {6-bit} timer. The timer is simply a binary counter that counts downward and produces an output of Z=1 when it reaches 0. Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.\n\nWe can thus think of transitions in the FSM on a cycle by cycle basis as consisting of two types. The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S for some number of cycles."
        ],
        "score": [
            7.698310375213623,
            -0.5199398398399353,
            -0.3280162513256073
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What are the outputs of the FSM?\\n",
        "answers": [
            "\nThe outputs of the FSM are the states of the stoplights. The stoplights are either red, green, or yellow. The FSM controls the duration of each light.",
            "One could design a game controller for a logic puzzle that would have buttons corresponding to the different operations that could be performed on the logic puzzle. For example, there could be a button to add a new element to the puzzle, a button to remove an element from the puzzle, and a button to change the value of an element in the puzzle. The game controller could also have a display that would show the current state of the puzzle and the player's progress.",
            "A fully-specified transition diagram is a diagram that specifies all of the states that a system can be in, as well as all of the transitions between those states. This allows for a clear understanding of the behavior of the system."
        ],
        "score": [
            7.216278553009033,
            -11.274892807006836,
            -10.755998611450195
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What is the purpose of the FSM?\\n",
        "answers": [
            "\nThe purpose of the FSM is to control the stoplights at a busy intersection. The FSM ensures that the lights on one or both roads are red at all times, that a road has a green light, and that the controller should show a yellow light before showing a red light.",
            "The benefits of using DRAM are that it is denser than SRAM and uses fewer transistors. This makes DRAM ideal for applications where size is more important than speed.",
            "Scan-based testing is a method of testing where the input is scanned and converted into the appropriate result. The programmer is responsible for ensuring that the number of conversions in the format string matches the number of arguments provided. The programmer must also ensure that the type of information produced by each conversion can be stored at the address passed for that conversion."
        ],
        "score": [
            7.969118118286133,
            -11.069326400756836,
            -9.061639785766602
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What are the conditions for the signal F to be 1?\\n",
        "answers": [
            "\nThe signal F is 1 when the timer reaches 0 and the state S changes.",
            "A Karnaugh map is a graphical representation of a Boolean function that helps to simplify the function. The map consists of a grid of squares, each of which corresponds to a possible combination of input values. The map is used to find groups of product terms that are implicants of the function.",
            "A Gray code counter is a type of counter that uses a Gray code, which is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit. A regular counter is a type of counter that does not use a Gray code."
        ],
        "score": [
            5.664515495300293,
            -11.266365051269531,
            -11.162351608276367
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " Why is it necessary to initialize the state S to one of the six states in the desired loop?', '\\n\\n3.46\\n\\n",
        "answers": [
            "\nThe state of the FSM has been split into two pieces: a {3-bit} register S and a {6-bit} timer. The timer is simply a binary counter that counts downward and produces an output of Z=1 when it reaches 0. Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.\n\nWe can thus think of transitions in the FSM on a cycle by cycle basis as consisting of two types. The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S for some number of cycles.",
            "The player loses the game if they are not able to get the correct answer to the question. If the player gets the correct answer, they win the game.",
            "One way to combine memory chips is to use multiple chips to implement a larger memory. This can be done by using one address bit to drive a decoder that determines which of the two chips is active. Another way to combine memory chips is to use multiple chips to implement a wider memory. This can be done by routing the external address, write enable, and chip select inputs to both chips. The data inputs and outputs are separate, with the left chip handling the high N bits of input on writes and producing the high N bits of output on reads, while the right chip handles the low N bits of input and produces the low N bits of output."
        ],
        "score": [
            -8.98208999633789,
            -10.959762573242188,
            -10.557706832885742
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How does the FSM ensure that the lights on one or both roads are red at all times?\\n",
        "answers": [
            "\nThe FSM ensures that the lights on one or both roads are red at all times by keeping track of the state of the light. If the light is green, it will turn yellow and then red. If the light is red, it will stay red.",
            "The value of S_2 when the counter is in the COUNT A state and HOLD=0 is undefined. This is because, as the passage states, overflow occurs when either both operands are negative or both operands are positive. If both operands are negative, then the resulting sum S is equal to C. However, if both operands are positive, then the resulting sum S is less than C. Thus, it is impossible to determine the value of S_2 without knowing the values of both operands.",
            "The design process involves the manipulation of logic expressions in order to optimize them for specific metrics, such as cost, power, or performance. Today, this manipulation is performed almost entirely by computers, with human engineers supplying the logic functions of interest and imposing constraints on the process."
        ],
        "score": [
            9.82939338684082,
            -11.148941040039062,
            -11.268567085266113
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How does the FSM show a yellow light before showing a red light?\\n",
        "answers": [
            "\nThe FSM shows a yellow light before showing a red light by setting the timer to a specific value when the state of the FSM changes. This value is set by the signal F, which is calculated at the bottom of the FSM logic diagram.",
            "The choice of state representation affects design complexity in a few ways. First, it can affect the number of bits needed to represent the states. Second, it can affect the number of possible states that can be represented. Third, it can affect the number of transitions that need to be considered when designing the state machine.",
            "Pressing the ``halt'' button does nothing when the counter is halted. The counter remains in the same state until the ``go'' button is pressed."
        ],
        "score": [
            9.964210510253906,
            -11.257482528686523,
            -11.149909973144531
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How does the FSM alternate green lights between the two roads?\\n",
        "answers": [
            "\nThe FSM alternates green lights between the two roads by setting the timer to a different value when the state of the FSM changes. This causes the lights to change color on a cycle-by-cycle basis, with the green light alternating between the two roads.",
            "The S_2 value specifies the number of inputs that must be satisfied in order for the output to be 1. This cuts the number of possible choices from four to two in each case because there are only two possible combinations of inputs that can satisfy this condition.",
            "The \"go\" button initiates a system call that causes the processor to enter kernel mode and execute the instructions associated with the system call. These instructions are part of the operating system and are used to perform the desired action, such as printing a character to the console."
        ],
        "score": [
            9.808687210083008,
            -11.275714874267578,
            -11.285955429077148
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What does the signal F do?\\n",
        "answers": [
            "\nThe signal F is used to set the timer under certain conditions. In particular, it is used to set the timer when the state S changes.",
            "PASSAGE - \n\nTwo-level logic is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. In this section, we illustrate one of the reasons for this popularity and show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.\n\nQUESTION -\n\nWhat are the two levels of two-level logic?",
            "There are a few disadvantages of using an FSM. First, if the FSM is not designed carefully, it can be hard to understand and debug. Second, an FSM can use more resources than a simpler design, such as a state machine. Finally, an FSM can be more difficult to change or extend than a simpler design."
        ],
        "score": [
            6.761960983276367,
            -11.08114242553711,
            -6.876993179321289
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How do we initialize the state S to one of the six states in the desired loop?', '\\n\\n",
        "answers": [
            "\nThe FSM is initialized to the state \"000\" in order to ensure that the lights on one or both roads are red at all times. The timer is set to \"A\" in order to maintain the new value of S for some number of cycles.",
            "A random access memory (RAM) is a type of memory that allows data to be stored and retrieved in any order. RAM is volatile, which means that it loses all data when the power supply is removed. There are two types of RAM: static RAM (SRAM) and dynamic RAM (DRAM).",
            "A decoder is a logic device that converts binary information from the form of one code to another, usually a natural binary code. A multiplexer is a device that selects one of several analog or digital input signals and forwards the selected input into a single output line."
        ],
        "score": [
            -2.7400336265563965,
            -11.19755744934082,
            -11.078326225280762
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How does the stoplight controller work?\\n",
        "answers": [
            "\nThe stoplight controller has a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.",
            "The output for the third bit gray code counter would be either 0 or 1. If the input is 0, the output is 1. If the input is 1, the output is 0.",
            "The output of the FSM is a 3-bit register S and a 6-bit timer. The timer is a binary counter that counts downward and produces an output of Z=1 when it reaches 0. The register S only takes a new value when the timer reaches 0, and the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle."
        ],
        "score": [
            6.624048709869385,
            -10.896063804626465,
            -9.806427001953125
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How does the stoplight controller ensure that the lights on one or both roads are red at all times?\\n",
        "answers": [
            "\nThe stoplight controller ensures that the lights on one or both roads are red at all times by using a state machine with six states. The states are determined by the inputs to the FSM, which are the states of the two roads. The output of the FSM controls the lights on the two roads. The controller alternates between green and red lights on the two roads.",
            "Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).",
            "There are 64 bit patterns available for the PAID state."
        ],
        "score": [
            9.809343338012695,
            -11.210271835327148,
            -11.123361587524414
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How does the stoplight controller show a yellow light before showing a red light?\\n",
        "answers": [
            "\nThe stoplight controller shows a yellow light before showing a red light in order to give drivers some warning and allow them to slow down.",
            "How is multi-metric optimization useful in engineering?\n\nMulti-metric optimization is useful in engineering because it allows engineers to identify the most important metrics and to balance their design effectively according to them. Multi-metric optimization also allows engineers to get rid of as many metrics as possible so that they can focus on the most important ones.",
            "The player wins the game by getting all four entities (farmer, fox, goose, and corn) to the right bank of the river. If the player makes a mistake, the game is over and they lose."
        ],
        "score": [
            8.6580228805542,
            -11.197202682495117,
            -11.25567626953125
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " How does the stoplight controller alternate green lights between the two roads?\\n",
        "answers": [
            "\nThe stoplight controller alternates green lights between the two roads by changing the value of the register S. The value of S is changed when the timer reaches 0, and the timer is set when S enters a new state. The duration of each state is determined by the value of the register S.",
            "The vending machine knows how much each item costs because the owner of the machine sets the prices dynamically. The machine keeps track of how much money has been inserted as a multiple of 5 cents (one nickel). The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.",
            "Tri-state buffers were used to reduce the number of pins needed on chips by combining inputs and outputs. This allowed chip designers to halve the number of pins needed."
        ],
        "score": [
            9.561370849609375,
            -11.263273239135742,
            -11.298354148864746
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": " What does the signal F do?']",
        "answers": [
            "\nThe signal F allows the timer to be set under other conditions, in addition to when the state S changes. This allows for more flexibility in the duration of each state.",
            "One way to improve a bad state representation is to make it more compact. This can be done by using two-level logic instead of three-level logic. Another way to improve a bad state representation is to make it more readable. This can be done by using clear and consistent naming conventions.",
            "If the \"halt\" button is pressed (HG=10), the logic outputs C_L=00 and C_M=00, so the dispenser gives no ice cream. If both buttons are pressed (HG=11), the outputs are C_L=11 and C_M=11, so the dispenser gives two servings of each flavor, or four servings total."
        ],
        "score": [
            7.196232795715332,
            -11.136000633239746,
            -10.245681762695312
        ],
        "text": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that"
    },
    {
        "question": "What is the difference between starting with a human-based design process and mapping the desired task into a high-level programming language?\\n",
        "answers": [
            "\nStarting with a human-based design process typically results in a design that is more efficient and effective than one that is mapped from a high-level programming language. This is because human designers can more easily optimize a design for specific metrics, such as power consumption or performance. Additionally, human designers can more easily impose constraints on a design, such as limits on chip area or minimum performance, in order to guide the design process.",
            "The addressability of a computer memory is the number of bits that can be stored in each element of the memory. The size of the elements in a memory can vary from a single binary digit, or bit, to a byte (8 bits) or more. Typically, data elements larger than a byte are referred to as words, but the size of a word depends on context.",
            "One can build memories using logic by creating a tri-state buffer. This logical disconnection is achieved by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table."
        ],
        "score": [
            7.628187656402588,
            -11.008793830871582,
            -10.926612854003906
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": "What does it mean to map down to bits and gates?\\n",
        "answers": [
            "\nMapping down to bits and gates means to design a circuit that performs an operation on a single bit and passes any necessary information into another copy of itself. By using copies of this bit-sliced adder circuit, we can mimic our approach as humans and build adders of any size.",
            "What are the three ranges for a given metric?\n\nThe first range is all measurement values are equivalently useful. The second range is possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice.",
            "The inputs to a Gray code counter are the current state of the counter and the clock signal. The clock signal triggers the state change of the counter."
        ],
        "score": [
            9.051002502441406,
            -11.296846389770508,
            -11.287500381469727
        ],
        "text": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is"
    },
    {
        "question": "How does the control structure of code map into a high-level FSM design?\\n",
        "answers": [
            "\nThe control structure of code maps into a high-level FSM design by breaking down the program into simple steps and mapping each step into a state in the FSM. The number of states in the FSM will depend on the number of steps in the program.",
            "A priority encoder is a type of encoder that assigns priority to certain input signals. The priority encoder takes in a signal and produces an output signal that indicates which input signal has the highest priority. The output signal is typically a binary number that is used to select one of the input signals.",
            "A mux is a logic block that allows us to use one digital signal to select which of several others is forwarded to an output. A decoder is a logic block that converts a binary code to a set of signals."
        ],
        "score": [
            8.865071296691895,
            -10.99895191192627,
            -11.151397705078125
        ],
        "text": "statement initializes the minimum known value ({ min}) to the value stored at index 0 in the  array ({ values[0]}). The second statement is a loop in which the variable { index}  takes on values from 1 to 9.  For each value, an { if} statement compares the current known minimum with the value stored in the array at index given by the { idx} variable.  If the stored value is smaller, the current known  value (again, { min}) is updated to reflect the program's having found a smaller value.  When the loop finishes all nine iterations, the variable { min} holds the smallest value among the set of 10  integers stored in the { values} array.   As a first step towards designing an FSM to implement the code, we transform the code into a flow chart, as shown to the right.  The program again begins with initialization, which appears in the second column of the flow chart.   The loop in the program translates to the third column of the flow chart,  and the { if} statement to the middle comparison and update  of { min}.  Our goal is now to design an FSM to implement the flow chart.  In order to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless entry system with a timer.  Although the timer's value was technically also   {{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}   part of the FSM's state, we treated it as data and integrated it into our next-state decisions in only a couple of cases.  For our minimum value problem, we have two sources of data.  First, an external program supplies data in the form of a set of 10 integers.  If we assume {32-bit} integers, these data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value.  These are technically state bits.  For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it  using the components that we have developed earlier in our course.  {Choosing Components and Identifying States}  Now we are ready to design an FSM that implements the flow chart. What components do we need, other than our state logic? We use registers and counters to implement the variables { idx} and { min} in the program. For the array { values}, we use a {1632-bit}  memory.{We technically only need a {1032-bit}  memory, but we round up the size of the address space to reflect more realistic memory designs; one can always optimize later.} We need a comparator to implement the test for the { if} statement. We choose to use a serial comparator, which allows us to illustrate again how one logical high-level state can be subdivided into many actual states. To operate the serial comparator, we make use of two shift registers that  present the comparator with one bit per cycle on each input, and a counter to keep track of the comparator's progress.  How do we identify high-level states from our flow chart?  Although the flow chart attempts to break down the program into `simple' steps, one step of a flow chart may sometimes require more than one state in an FSM.  Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities.  How we map flow chart elements into FSM states also depends to some  degree on what components we use, which is why we began with some discussion of components.  In practice, one can go back and forth between the two,  adjusting components to better match the high-level states, and adjusting  states to better match the desired components.  Finally, note that we are only concerned with high-level states, so we do  not need to provide details (yet) down to the level of individual clock  cycles, but we do want to define high-level states that can be implemented in a fixed number of cycles, or at"
    },
    {
        "question": "What is the more general model of a computer?\\n",
        "answers": [
            "\nThe more general model of a computer is a system that can execute a set of instructions, called a program, and store the resulting data. The system has a set of registers that hold data and instructions, and a set of memory locations that store data and programs. The system also has a set of input and output devices that allow it to interact with the outside world.",
            "The device is used to turn itself off and save energy. This is done by using heuristic approaches to find solutions that strike a balance between different metrics, such as cost, power, or performance. The human engineer can impose constraints, such as limits on the chip area or limits on the minimum performance, in order to guide the process.",
            "SRAM cells are combined into a bit slice by sharing bit lines and analog read and write logic. The bit slice in the figure can be thought of as a 16-address, 1-bit-addressable memory (2^4b). The cells in a bit slice share bit lines and analog read and write logic, which appears to the right in the figure."
        ],
        "score": [
            8.104486465454102,
            -11.261045455932617,
            -11.108583450317383
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": "What is the purpose of this process?', '",
        "answers": [
            "\nThe purpose of this process is to find the best formulation of a specific logic function for a given metric. This is done by using heuristic approaches to find solutions that strike a balance between various metrics such as cost, power, or performance.",
            "One reason it is important to start with a representation that makes sense to a human is that humans must supply the logic functions of interest, and must program the acceptable transformations between equivalent forms. A second reason is that the complexity of the problem is a good way to introduce you to real engineering. Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.",
            "A priority encoder is a type of encoder that generates a signal whenever any of its input lines are active, and encodes the index of the highest active input as a two-bit unsigned number. A priority encoder is used to determine which of its input lines has the highest priority."
        ],
        "score": [
            7.336607933044434,
            -6.524175643920898,
            -4.176512718200684
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": "What is the difference between the two approaches to FSM design?\\n",
        "answers": [
            "\nThe two approaches to FSM design are the state-transition table and the state diagram. The state-transition table lists all the possible states that the system can be in, and the state diagram shows the sequence of states that the system goes through. The state-transition table is more compact and easier to use when comparing functions, but the state diagram is more intuitive and easier to understand.",
            "The design is easy to test because it is a two-level logic design. This means that the inputs and outputs can be easily tested with a NAND or NOR gate. Additionally, the design is easy to test because it is a SOP expression. This means that the inputs and outputs can be easily tested with an OR gate.",
            "The state representation for the lab FSM is a 3-bit binary number, with the following meaning:\n\nS_2: type of last coin inserted (0 for dime, 1 for quarter)\nS_1: more than one quarter inserted? (1 for yes, 0 for no)\nS_0: more than one dime inserted? (1 for yes, 0 for no)"
        ],
        "score": [
            8.009166717529297,
            -7.4726667404174805,
            -3.5892114639282227
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "What is the purpose of this set of notes?\\n",
        "answers": [
            "\nThe purpose of these notes is to introduce an important abstraction for logic design, and to use this abstraction to design a ripple-carry adder. The ripple-carry adder is an adder that adds a single bit and passes any necessary information into another copy of itself. This design is less efficient than designs that are optimized for their specific purpose, but the simplicity of the approach makes the tradeoff an interesting one.",
            "The output of the FSM looks like a green light followed by a yellow light followed by a red light.",
            "The next-state values for S_2, S_1, and S_0 are given in the table to the right. The states of S act like a counter, and by examining the connections, we can derive equations for the next state and draw a transition diagram. There are six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle."
        ],
        "score": [
            3.6374263763427734,
            -11.06418514251709,
            -11.110955238342285
        ],
        "text": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is"
    },
    {
        "question": "How can a piece of code be transformed into an FSM?\\n",
        "answers": [
            "\nA piece of code can be transformed into an FSM by first mapping the desired task into a high-level programming language, then using components such as registers, counters, and memories to implement the variables needed. In this approach, the control structure of the code maps into a high-level FSM design.",
            "Yes, we can choose to use the counter's internal state directly as our output values. This is because the colors we need to produce as outputs are all unique bit patterns. By using the counter's internal state directly as our output values, we can simplify our design.",
            "Tri-state buffers can be used to construct wider memories by connecting the data inputs and outputs of multiple memories together. This allows the same output line to be shared by more than one memory without the need for additional multiplexers."
        ],
        "score": [
            9.468114852905273,
            -11.190277099609375,
            -11.066020965576172
        ],
        "text": "produce it gives  F=S_1S_0{V^+{S_1}S_0{V^.  If we  ignore the two states outside of the main loop for S, the first term  is 1 only when the lights are green on the East and West roads and the  detector for the North and South roads indicates that no vehicles are  approaching.  Similarly, the second term is 1 only when the lights are  green on the North and South roads and the detector for the East and  West roads indicates that no vehicles are approaching.  What happens when F=1?  First, the OR gate feeding into the timer's LD input produces a 1, meaning that the timer loads a new value instead of counting down.  Second, the OR gate controlling the lower mux selects the A input.  In other words, the timer is reset to A cycles, corresponding to the initial value for the green light states. In other words, the light stays green until vehicles approach on  the other road, plus A more cycles.  Unfortunately, the signal F may also be 1 in the unused states of S, in which case the lights on both roads may remain red even though cars are waiting on one of the roads.  To avoid this behavior, we must be  sure to initialize the state S to one of the six states in the desired loop.    {From FSM to Computer}  The FSM designs we have explored so far have started with a human-based design process in which someone writes down the desired behavior in terms of states, inputs, outputs, and transitions.  Such an approach makes it easier to build a digital FSM, since the abstraction used corresponds almost directly to the implementation.  As an alternative, one can start by mapping the desired task into a high-level programming language, then using components such as registers, counters, and memories to implement the variables needed.  In this approach, the control structure of the code maps into a high-level FSM design. Of course, in order to implement our FSM with digital logic, we eventually still need to map down to bits and gates.  In this set of notes, we show how one can transform a piece of code written in a high-level language into an FSM.  This process is meant to help you understand how we can design an FSM that executes simple pieces of a flow chart such as assignments, { if} statements, and  loops.  Later, we generalize this concept and build an FSM that allows the pieces to be executed to be specified after the FSM is built---in  other words, the FSM executes a program specified by bits stored in  memory.  This more general model, as you might have already guessed,  is a computer.    {Specifying the Problem}  Let's begin by specifying the problem that we want to solve. Say that we want to find the minimum value in a set of 10 integers. Using the C programming language, we can write the following fragment of  code:   {  aaaa=aaaa= int >values[10];    /* 10 integers--filled in by other code */ int >idx; int >min  min = values[0]; for (idx = 1; 10 > idx; idx = idx + 1) { >  if (min > values[idx]) { >  >  min = values[idx]; >  } } /* The minimum value from array is now in min. */  }   The code uses array notation, which we have not used previously in our  class, so let's first discuss the meaning of the code.  The code uses three variables.  The variable { values} represents the 10 values in our set. The suffix ``[10]'' after the variable name tells the compiler that we want an array of 10 integers ({ int}) indexed from 0 to 9. These integers can be treated as 10 separate variables, but can be accessed using the single name ``{ values}'' along with an index (again, from 0 to 9 in this case).  The variable { idx} holds a loop index that we use to examine each of the values one by one in order to find the minimum value in the set.  Finally, the variable { min} holds the smallest known value as  the program examines each of the values in the set.  The program body consists of two statements.    We assume that some other piece of code---one not shown here---has  initialized the 10 values in our set before the code above executes.  The first"
    },
    {
        "question": "What is the more general model of an FSM?\\n",
        "answers": [
            "\nA finite state machine is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs. In any given state, a particular input may cause the FSM to move to another state; this combination is called a transition rule.",
            "The states in the transition diagram represent the different colors of the stoplight. The first two states are red, followed by yellow, and then green. The last two states are again red.",
            "A word in the context of computer memory is a sequence of bits that represents a unit of data. Words can represent numbers, characters, or other data types. The size of a word is typically determined by the computer's word size, which is the number of bits the computer can process at a time."
        ],
        "score": [
            2.808006525039673,
            -11.23350715637207,
            -11.125444412231445
        ],
        "text": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout"
    },
    {
        "question": "What is the difference between a computer and an FSM?', '\\n\\n",
        "answers": [
            "\nA computer is a machine that can be programmed to carry out a set of arithmetic or logical operations automatically. An FSM is a model of behavior composed of a finite set of states, transitions between those states, and actions.",
            "The two approaches to FSM design are the state-transition table and the state diagram. The state-transition table lists all the possible states that the system can be in, and the state diagram shows the sequence of states that the system goes through. The state-transition table is more compact and easier to use when comparing functions, but the state diagram is more intuitive and easier to understand.",
            "There are a few benefits of using a Gray code counter. First, it is easy to design and implement. Second, it is easy to check the correctness of the logic. Third, it is easy to read and understand the logic."
        ],
        "score": [
            7.405887603759766,
            -1.4659074544906616,
            -11.167418479919434
        ],
        "text": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur"
    },
    {
        "question": " How does one transform a piece of code written in a high-level language into an FSM?\\n",
        "answers": [
            "\nOne way to transform code written in a high-level language into an FSM is to use a compiler. A compiler is a program that translates code written in a high-level language into code that can be run on a computer. Another way to transform code written in a high-level language into an FSM is to use an interpreter. An interpreter is a program that executes code written in a high-level language.",
            "We can test the circuit that we build in the lab to see if it correctly implements the FSM that we have outlined in these notes. To do this, we can input a known sequence of 0s and 1s and see if the circuit outputs the correct responses. If it does, then we can be confident that the circuit is functioning correctly.",
            "Abstraction is the process of simplifying a complex system by identifying the most important aspects of that system and ignoring the rest. In the context of implementation, abstraction can be used to simplify a design by identifying the most important features of the design and ignoring the rest. This can make the design more manageable and easier to understand."
        ],
        "score": [
            8.691730499267578,
            -6.352144241333008,
            -10.959845542907715
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How does the control structure of the code map into a high-level FSM design?\\n",
        "answers": [
            "\nThe control structure of the code maps into a high-level FSM design by breaking down the program into simple steps, one step of a flow chart may sometimes require more than one state in an FSM. Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously.",
            "The purpose of the FSM is to control a stoplight. The stoplight must be red on one or both roads at all times, and the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. The controller should also alternate green lights between the two roads.",
            "A decoder is used in this context to simplify the expression of a logic function. By using a decoder, the function can be expressed as a two-level logic function, which is easier to understand and manipulate."
        ],
        "score": [
            8.669631004333496,
            -2.8258702754974365,
            -11.172122955322266
        ],
        "text": "statement initializes the minimum known value ({ min}) to the value stored at index 0 in the  array ({ values[0]}). The second statement is a loop in which the variable { index}  takes on values from 1 to 9.  For each value, an { if} statement compares the current known minimum with the value stored in the array at index given by the { idx} variable.  If the stored value is smaller, the current known  value (again, { min}) is updated to reflect the program's having found a smaller value.  When the loop finishes all nine iterations, the variable { min} holds the smallest value among the set of 10  integers stored in the { values} array.   As a first step towards designing an FSM to implement the code, we transform the code into a flow chart, as shown to the right.  The program again begins with initialization, which appears in the second column of the flow chart.   The loop in the program translates to the third column of the flow chart,  and the { if} statement to the middle comparison and update  of { min}.  Our goal is now to design an FSM to implement the flow chart.  In order to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless entry system with a timer.  Although the timer's value was technically also   {{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}   part of the FSM's state, we treated it as data and integrated it into our next-state decisions in only a couple of cases.  For our minimum value problem, we have two sources of data.  First, an external program supplies data in the form of a set of 10 integers.  If we assume {32-bit} integers, these data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value.  These are technically state bits.  For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it  using the components that we have developed earlier in our course.  {Choosing Components and Identifying States}  Now we are ready to design an FSM that implements the flow chart. What components do we need, other than our state logic? We use registers and counters to implement the variables { idx} and { min} in the program. For the array { values}, we use a {1632-bit}  memory.{We technically only need a {1032-bit}  memory, but we round up the size of the address space to reflect more realistic memory designs; one can always optimize later.} We need a comparator to implement the test for the { if} statement. We choose to use a serial comparator, which allows us to illustrate again how one logical high-level state can be subdivided into many actual states. To operate the serial comparator, we make use of two shift registers that  present the comparator with one bit per cycle on each input, and a counter to keep track of the comparator's progress.  How do we identify high-level states from our flow chart?  Although the flow chart attempts to break down the program into `simple' steps, one step of a flow chart may sometimes require more than one state in an FSM.  Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities.  How we map flow chart elements into FSM states also depends to some  degree on what components we use, which is why we began with some discussion of components.  In practice, one can go back and forth between the two,  adjusting components to better match the high-level states, and adjusting  states to better match the desired components.  Finally, note that we are only concerned with high-level states, so we do  not need to provide details (yet) down to the level of individual clock  cycles, but we do want to define high-level states that can be implemented in a fixed number of cycles, or at"
    },
    {
        "question": " How can we design an FSM that executes simple pieces of a flow chart?\\n",
        "answers": [
            "\nOne can design an FSM that executes simple pieces of a flow chart by breaking down the program into smaller steps and mapping each step to a state in the FSM. One can also use registers and counters to implement variables in the program, and use a comparator to implement the test for the if statement.",
            "Tri-state buffers are used to provide logical disconnection between the tri-state buffer and the output. This logical disconnection is achieved by using the outer pair of transistors in the logic diagram. When EN=0, both transistors turn off, meaning that regardless of the value of IN, OUT is connected neither to high voltage nor to ground. When EN=1, both transistors turn on, and the tri-state buffer acts as a pair of back-to-back inverters, copying the signal from IN to OUT, as shown in the truth table.",
            "A transition diagram can help with this process by helping to identify the most important metrics and by helping to balance the design effectively according to those metrics."
        ],
        "score": [
            9.32504653930664,
            -11.234081268310547,
            -10.38912582397461
        ],
        "text": "statement initializes the minimum known value ({ min}) to the value stored at index 0 in the  array ({ values[0]}). The second statement is a loop in which the variable { index}  takes on values from 1 to 9.  For each value, an { if} statement compares the current known minimum with the value stored in the array at index given by the { idx} variable.  If the stored value is smaller, the current known  value (again, { min}) is updated to reflect the program's having found a smaller value.  When the loop finishes all nine iterations, the variable { min} holds the smallest value among the set of 10  integers stored in the { values} array.   As a first step towards designing an FSM to implement the code, we transform the code into a flow chart, as shown to the right.  The program again begins with initialization, which appears in the second column of the flow chart.   The loop in the program translates to the third column of the flow chart,  and the { if} statement to the middle comparison and update  of { min}.  Our goal is now to design an FSM to implement the flow chart.  In order to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless entry system with a timer.  Although the timer's value was technically also   {{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}   part of the FSM's state, we treated it as data and integrated it into our next-state decisions in only a couple of cases.  For our minimum value problem, we have two sources of data.  First, an external program supplies data in the form of a set of 10 integers.  If we assume {32-bit} integers, these data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value.  These are technically state bits.  For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it  using the components that we have developed earlier in our course.  {Choosing Components and Identifying States}  Now we are ready to design an FSM that implements the flow chart. What components do we need, other than our state logic? We use registers and counters to implement the variables { idx} and { min} in the program. For the array { values}, we use a {1632-bit}  memory.{We technically only need a {1032-bit}  memory, but we round up the size of the address space to reflect more realistic memory designs; one can always optimize later.} We need a comparator to implement the test for the { if} statement. We choose to use a serial comparator, which allows us to illustrate again how one logical high-level state can be subdivided into many actual states. To operate the serial comparator, we make use of two shift registers that  present the comparator with one bit per cycle on each input, and a counter to keep track of the comparator's progress.  How do we identify high-level states from our flow chart?  Although the flow chart attempts to break down the program into `simple' steps, one step of a flow chart may sometimes require more than one state in an FSM.  Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities.  How we map flow chart elements into FSM states also depends to some  degree on what components we use, which is why we began with some discussion of components.  In practice, one can go back and forth between the two,  adjusting components to better match the high-level states, and adjusting  states to better match the desired components.  Finally, note that we are only concerned with high-level states, so we do  not need to provide details (yet) down to the level of individual clock  cycles, but we do want to define high-level states that can be implemented in a fixed number of cycles, or at"
    },
    {
        "question": " What is the more general model of a computer?\\n",
        "answers": [
            "\nThe most general model of a computer is the Turing machine. It is a mathematical model of a computer that can be used to simulate any other computer.",
            "The K-map represents the domain of a function in the form of a hypercube. The hypercube is divided into regions, each of which corresponds to a product of literals. The function is then evaluated at each point in the hypercube, and the product of literals is an implicant of the function if the function evaluates to 1 at that point.",
            "There are two possible ways that the user could press both buttons while in state { HALT A}. The first way is that the user could press the \"halt\" button and then the \"go\" button. In this case, the FSM would move to state { HALT A} and then to state { COUNT B}. The second way is that the user could press the \"go\" button and then the \"halt\" button. In this case, the FSM would move to state { COUNT B} and then to state { HALT A}."
        ],
        "score": [
            7.709192276000977,
            -10.753528594970703,
            -11.184654235839844
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " How can we build an FSM that allows the pieces to be executed to be specified after the FSM is built?']",
        "answers": [
            "\nOne way to build an FSM that allows the pieces to be executed to be specified after the FSM is built is to use a state machine that has an output that can be routed to different inputs. This allows the FSM to be configured to execute different pieces of code based on the input. Another way to build an FSM that allows the pieces to be executed to be specified after the FSM is built is to use a state machine that has multiple states that can be executed in parallel. This allows the FSM to be configured to execute different pieces of code in parallel based on the input.",
            "\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\",
            "The FSM is responsible for controlling the stoplights at a busy intersection. It ensures that the lights on one or both roads are red at all times, that a road has a green light, and that the controller should show a yellow light before showing a red light."
        ],
        "score": [
            10.132414817810059,
            -8.686028480529785,
            -5.287748336791992
        ],
        "text": "statement initializes the minimum known value ({ min}) to the value stored at index 0 in the  array ({ values[0]}). The second statement is a loop in which the variable { index}  takes on values from 1 to 9.  For each value, an { if} statement compares the current known minimum with the value stored in the array at index given by the { idx} variable.  If the stored value is smaller, the current known  value (again, { min}) is updated to reflect the program's having found a smaller value.  When the loop finishes all nine iterations, the variable { min} holds the smallest value among the set of 10  integers stored in the { values} array.   As a first step towards designing an FSM to implement the code, we transform the code into a flow chart, as shown to the right.  The program again begins with initialization, which appears in the second column of the flow chart.   The loop in the program translates to the third column of the flow chart,  and the { if} statement to the middle comparison and update  of { min}.  Our goal is now to design an FSM to implement the flow chart.  In order to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless entry system with a timer.  Although the timer's value was technically also   {{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}   part of the FSM's state, we treated it as data and integrated it into our next-state decisions in only a couple of cases.  For our minimum value problem, we have two sources of data.  First, an external program supplies data in the form of a set of 10 integers.  If we assume {32-bit} integers, these data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value.  These are technically state bits.  For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it  using the components that we have developed earlier in our course.  {Choosing Components and Identifying States}  Now we are ready to design an FSM that implements the flow chart. What components do we need, other than our state logic? We use registers and counters to implement the variables { idx} and { min} in the program. For the array { values}, we use a {1632-bit}  memory.{We technically only need a {1032-bit}  memory, but we round up the size of the address space to reflect more realistic memory designs; one can always optimize later.} We need a comparator to implement the test for the { if} statement. We choose to use a serial comparator, which allows us to illustrate again how one logical high-level state can be subdivided into many actual states. To operate the serial comparator, we make use of two shift registers that  present the comparator with one bit per cycle on each input, and a counter to keep track of the comparator's progress.  How do we identify high-level states from our flow chart?  Although the flow chart attempts to break down the program into `simple' steps, one step of a flow chart may sometimes require more than one state in an FSM.  Similarly, one FSM state may be able to implement several steps in a flow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities.  How we map flow chart elements into FSM states also depends to some  degree on what components we use, which is why we began with some discussion of components.  In practice, one can go back and forth between the two,  adjusting components to better match the high-level states, and adjusting  states to better match the desired components.  Finally, note that we are only concerned with high-level states, so we do  not need to provide details (yet) down to the level of individual clock  cycles, but we do want to define high-level states that can be implemented in a fixed number of cycles, or at"
    },
    {
        "question": " What is the problem that the code is trying to solve?\\n\\n",
        "answers": [
            "\nThe problem that the code is trying to solve is the halting problem. The halting problem is a problem that cannot be solved by a Turing machine. The problem is that given a Turing machine and an input to the Turing machine, it is not possible to determine whether the machine will finish computing in a finite number of steps.",
            "The purpose of adding a few AND gates and a input is to create a logic function that is more efficient in terms of area and power. This is done by reducing the number of variables in the expression.",
            "The right strategy for choosing a representation is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. This will make the design process simpler and more efficient in the long run."
        ],
        "score": [
            8.777975082397461,
            -9.740217208862305,
            -10.892148971557617
        ],
        "text": "systematic technique (or a machine) capable of solving a problem that cannot be solved by a Turing machine.  No one has been able to do so to date.   {The Halting Problem*}  You might reasonably ask whether any problems can be shown to be incomputable.  More common terms for such problems---those known to be insolvable by any computer---are { intractable} or  undecidable.  In the same 1936 paper in which he introduced the universal computing machine, Alan Turing also provided an answer to this question by introducing (and proving) that there are in fact problems that cannot be computed by a universal computing machine. The problem that he proved undecidable, using proof techniques almost identical to those developed for similar problems in the 1880s, is now known as { the halting problem}.    The halting problem is easy to state and easy to prove undecidable. The problem is this: given a Turing machine and an input to the Turing machine, does the Turing machine finish computing in a finite number of steps (a finite amount of time)?  In order to solve the problem, an answer, either yes or no, must be given in a finite amount of time regardless of the machine or input in question.  Clearly some machines never finish.  For example, we can write a Turing machine that counts upwards starting from one.  You may find the proof structure for undecidability of the halting problem easier to understand if you first think about a related problem with which you may already be familiar, the Liar's paradox (which is at least 2,300 years old).  In its stengthened form, it is the following sentence: ``This sentence is not true.''   To see that no Turing machine can solve the halting problem, we begin by assuming that such a machine exists, and then show that its existence is self-contradictory.  We call the machine the ``Halting Machine,'' or HM for short.  HM is a machine that operates on  another      Turing machine and its inputs to produce a yes or no answer in finite time: either the machine in question finishes in finite time (HM returns ``yes''), or it does not (HM returns ``no'').  The figure illustrates HM's operation.   From HM, we construct a second machine that we call the HM Inverter, or HMI.  This machine inverts the sense of the answer given by HM.  In particular, the inputs are fed directly into a copy of HM, and if HM answers ``yes,'' HMI enters an infinite loop.  If HM answers ``no,'' HMI halts.  A diagram appears to the right.  The inconsistency can now be seen by asking HM whether HMI halts when given itself as an input (repeatedly), as      shown below.  Two copies of HM are thus being asked the same question.  One copy is the rightmost in the figure below and the second is embedded in the HMI machine that we are using as the input to the rightmost HM.  As the two copies of HM operate on the same input (HMI operating on HMI), they should return the same answer: a Turing machine either halts on an input, or it does not; they are deterministic.    Let's assume that the rightmost HM tells us that HMI operating on itself halts. Then the copy of HM in HMI (when HMI executes on itself, with itself as an input) must also say ``yes.''  But this answer implies that HMI doesn't halt (see the figure above), so the answer should have been no!  Alternatively, we can assume that the rightmost HM says that HMI operating on itself does not halt.  Again, the copy of HM in HMI must give the same answer.  But in this case HMI halts, again contradicting our assumption.  Since neither answer is consistent, no consistent answer can be given, and the original assumption that HM exists is incorrect.  Thus, no Turing machine can solve the halting problem.      {Programming Concepts and the C Language}  This set of notes introduces the C programming language and explains some basic concepts in computer programming.  Our purpose in showing you a high-level language at this early stage of the course is to give you time to become familiar with the"
    },
    {
        "question": " ",
        "answers": [
            "\nWhat is the difference between SOP and POS?\n\nSOP and POS are two forms of two-level logic. In SOP, the output is true if any of the inputs are true. In POS, the output is true if all of the inputs are true.",
            "The state of the FSM has been split into two pieces: a {3-bit} register S and a {6-bit} timer. The timer is simply a binary counter that counts downward and produces an output of Z=1 when it reaches 0. Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next cycle.\n\nWe can thus think of transitions in the FSM on a cycle by cycle basis as consisting of two types. The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S for some number of cycles.",
            "The vending machine knows how much each item costs because the owner of the machine sets the prices dynamically. The machine keeps track of how much money has been inserted as a multiple of 5 cents (one nickel). The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value."
        ],
        "score": [
            5.670361518859863,
            -0.9072772860527039,
            2.4811782836914062
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What does the array notation in the code mean?\\n\\n",
        "answers": [
            "\nThe array notation in the code means that the code is trying to convert input from the user into the appropriate result, then store the result in memory at the address given by the next argument.",
            "The output of the FSM is a sequence of red, yellow, and green lights that alternate between the two roads. The duration of each state is determined by the signal F. If F is 0, the timer is set only when the state S changes. If F is 1, the timer is set when the state S changes or when the light on one of the roads is green.",
            "What is the difference between two-level logic and other forms of logic?\n\nTwo-level logic is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. In this section, we illustrate one of the reasons for this popularity and show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams."
        ],
        "score": [
            8.24034309387207,
            -10.415024757385254,
            -10.241446495056152
        ],
        "text": "&a, &b);   /* These have the */} > { scanf (\"d d\", &a, &b);  /* same effect.   */} effect: > try to convert two integers typed in decimal to > 2's complement and store the results in { a} and { b}  > { scanf (\"cx lf\", &c, &u, &d);} effect: > try to read an ASCII character into { c}, a value > typed in hexadecimal into { u}, and a double- > precision > floating-point number into { d}  > { scanf (\"lf f\", &d, &f);} effect: > try to read two real numbers typed as decimal, > convert the first to double-precision and store it  > in { d}, and convert the second to single-precision  > and store it in { f}     ple.  The spaces are ignored by { scanf}.  However, { any non-space characters in the format string must be typed exactly by the user!}  The remaining arguments to { scanf} specify memory addresses where the function can store the converted values.    The ampersand (``&'') in front of each variable name in the examples is an operator that returns the address of a variable in memory.  For each con-   {|c|l|} escape  &                          sequence& { scanf} function's conversion to bits  { c}& store one ASCII character (as { char}) { d}& convert decimal integer to 2's complement { f}& convert decimal real number to float { lf}& convert decimal real number to double { u}& convert decimal integer to unsigned int { x}& convert hexadecimal integer to unsigned int { X}& (as above)     version in the format string, the { scanf} function tries to convert input from the user into the appropriate result, then stores the result in memory at the address given by the next argument.  The programmer is responsible for ensuring that the number of  conversions in the format string matches the number of arguments provided (not counting the format string itself).  The programmer must also ensure that the type of information produced by each conversion can be stored at the address passed for that conversion---in other words, the address of a variable with the correct type must be provided.  Modern compilers often detect missing { &} operators and incorrect variable types, but many only give warnings to the programmer.  The { scanf} function itself cannot tell whether the arguments given to it are valid or not.  If a conversion fails---for example, if a user types ``hello'' when { scanf} expects an integer---{ scanf} does not overwrite the corresponding variable and immediately stops trying to convert input.  The { scanf} function returns the number of successful  conversions, allowing a programmer to check for bad input from the user.  {Types of Statements in C}  Each statement in a C program specifies a complete operation.  There are three types of statements, but two of these types can be constructed from additional statements, which can in turn be constructed from additional statements.  The C language specifies no bound on this type of recursive construction, but code  readability does impose a practical limit.   The three types are shown to the right. They are the { null statement},  { simple statements},  and { compound statements}.  A null statement is just a semicolon, and a compound statement  is just a sequence of statements surrounded by braces.  Simple statements can take several forms.  All of the examples shown to the right, including the call to { printf}, are simple state-   {  aaaa=aaaaaaaaaaaaaa=/* =aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa= ;   > > /* >a null statement (does nothing) >*/  A = B; > > /*  >examples of simple statements >*/ printf (\"Hello, world!n\");  {    > > /* > a compound statement >*/  >  C = D; > /* > (a sequence of statements >*/ >  N = 4; > /* > between braces) >*/  >  L = D - N; }  } {-2pt}   ments consisting of a C expression followed by a  semicolon.  Simple statements can also consist of conditionals or iterations, which we introduce next.  Remember that after variable declarations, the { main} function contains a sequence of statements.  These statements are executed one at a time in the order given in the"
    },
    {
        "question": " ",
        "answers": [
            "\n1. What is the difference between two-level and three-level logic?\n\nTwo-level logic is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are examples of two-level logic. Three-level logic is a way of expressing logic functions using three levels. The three levels refer to the number of functions through which an input passes to reach an output, and the SOP and POS forms are examples of three-level logic.",
            "The fifth step in the design process is to select an internal representation for the bits necessary to encode the state of our FSM. In practice, for small designs, this representation can be selected by a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instruction set architecture that we study later in this class, humans do most of the work by hand.",
            "Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created the wiki documentation and the Altera Quartus portions of the lab."
        ],
        "score": [
            5.56789493560791,
            2.349905252456665,
            0.5255499482154846
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What is the purpose of the { idx} variable?\\n\\n",
        "answers": [
            "\nThe idx variable is used to keep track of the number of operations in the logic expression. This metric is used to estimate the number of transistors needed for the design, which in turn is an estimate for the area required for the design.",
            "The number of state bits affects the design complexity because it determines the number of possible states that the FSM can be in. A larger number of state bits means that there are more possible states and the design must be able to handle all of them.",
            "Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created the wiki documentation and the Altera Quartus portions of the lab."
        ],
        "score": [
            6.110801696777344,
            -9.761941909790039,
            -10.726480484008789
        ],
        "text": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three"
    },
    {
        "question": " ",
        "answers": [
            "\nPASSAGE - compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column",
            "The implementation diagram for a three-bit Gray code counter is a logic diagram that shows the inputs, outputs, and next-state logic for the counter. The diagram is ordered from left to right as {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2, which makes it easy to read and check for correctness.",
            "An abstract model is a simplified representation of a system, typically used to aid in understanding the system. In the context of logic optimization, an abstract model can be used to help understand the tradeoffs between different logic expressions."
        ],
        "score": [
            -2.4600985050201416,
            2.349425792694092,
            8.532641410827637
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What is the purpose of the { min} variable?\\n\\n",
        "answers": [
            "\nThe min variable is a heuristic metric used to estimate the time needed to compute a function. This metric is based on the longest chain of operations performed on any instance of a variable. The rationale for this metric is that gate outputs do not change instantaneously when their inputs change.",
            "F has two effects on the operation of the FSM.  First, it prevents the timer from being reset when S enters a new state.  This has the effect of lengthening the duration of each state.  Second, it prevents the FSM from entering the loop defined by the next-state logic.  This has the effect of halting the FSM in the current state.",
            "The decoder in the memory chip is responsible for translating the memory address into the correct location in the memory chip. This is necessary to ensure that the correct data is read from or written to the correct location in the memory chip."
        ],
        "score": [
            5.1088948249816895,
            -9.969694137573242,
            -9.973342895507812
        ],
        "text": "ANDS, one OR, and 8 variables,  and a count of 31 for Equation ()---six ANDs, one OR, and 24 variables.  The relative differences between these equations  are reduced when one counts operations.  A second metric of interest is a heuristic for the performance of a design.  Performance is inversely related to the delay necessary for a design to produce an output once its inputs are available.  For example, if you know how many seconds it takes to produce a result, you can easily calculate the number of results that can be produced per second, which  measures performance.  The measurement needed is the longest chain of operations performed on any instance of a variable.  The complement of a variable is included if the variable's complement is not  available without using an inverter.  The rationale for this metric is that gate outputs do not change  instantaneously when their inputs change. Once an input to a gate has reached an appropriate voltage to represent  a 0 or a 1, the transistors in the gate switch (on or off) and electrons  start to move. Only when the output of the gate reaches the appropriate new voltage  can the gates driven by the output start to change.  If we count each function/gate as one delay (we call this time a { gate delay}), we  get an estimate of the time needed to compute the function.  Referring again to our example equations, we find that Equation () requires 3 gate delays, Equation () requires 2 gate delays, Equation () requires 2 or 3 gate delays, depending on whether we have variable complements available.  Now  Equation () looks more attractive: better performance than Equation () in return for a small extra cost in area.  Heuristics for estimating energy use are too  complex to introduce at this point,  but you should be aware that every time electrons move, they generate heat, so we might favor an expression that minimizes the number of bit transitions inside the computation.  Such a measurement is not easy to calculate by hand, since you need to know the likelihood of input combinations.     We use many technical terms when we talk about simplification of logic expressions, so we now introduce those terms so as to make the description of the tools and processes easier to understand.  Let's assume that we have a logic function F(A,B,C,D) that we want to express concisely.  A { literal} in an expression of F refers  to either one of the variables or its complement.  In other words, for our function F, the following is a complete set of literals: A, , B, , C, , D, and .  When we introduced the AND and OR functions, we also introduced  notation borrowed from arithmetic, using multiplication to represent AND and addition to represent OR.  We also borrow the related terminology,  so a { sum}  in Boolean algebra refers to a number of terms OR'd together  (for example, A+B, or AB+CD), and a { product} in Boolean algebra refers to a number of terms AND'd together (for example, A, or AB(C+D).  Note that the terms in a sum or product may themselves be sums, products, or other types of expressions (for example, A).  The construction method that we used to demonstrate logical completeness made use of minterms for each input combination for which the  function F produces a 1.  We can now use the idea of a literal to give a simpler definition of minterm: a { minterm} for a function on N variables is a product (AND  function) of N literals in which each variable or its complement  appears exactly once.  For our function F, examples of minterms include ABC, ACD, and BC. As you know, a minterm produces a 1 for exactly one combination of inputs.  When we sum minterms for each output value of 1 in a truth table to express a function, as we did to obtain Equation (), we produce an example of the sum-of-products form. In particular, a { sum-of-products} ({ SOP}) is a sum composed of  products of literals. Terms in a sum-of-products need not be minterms, however. Equation () is also in sum-of-products form. Equation"
    },
    {
        "question": " ",
        "answers": [
            "\nPASSAGE - compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column",
            "A Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all patterns in the second copy.",
            "The flow chart represents the steps that should be taken when designing a new product. The first step is to think carefully about metrics of possible interest. The second step is to get rid of as many metrics as possible. The third step is to identify metrics that have no practical impact in current technology. The fourth step is to set threshold values for other metrics to simplify reasoning about them. The fifth step is to eliminate redundant metrics. The sixth step is to calculate linear sums to reduce the count of metrics. The seventh step is to make use of the notion of Pareto optimality."
        ],
        "score": [
            -2.4600985050201416,
            7.36284875869751,
            1.4976611137390137
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " What does the flow chart represent?', '\\n\\n",
        "answers": [
            "\nThe flow chart represents the steps that should be taken when designing a new product. The first step is to think carefully about metrics of possible interest. The second step is to get rid of as many metrics as possible. The third step is to identify metrics that have no practical impact in current technology. The fourth step is to set threshold values for other metrics to simplify reasoning about them. The fifth step is to eliminate redundant metrics. The sixth step is to calculate linear sums to reduce the count of metrics. The seventh step is to make use of the notion of Pareto optimality.",
            "The uninterrupted counting cycle represents the duration of each state in the FSM. This is used to calculate the duration of each state in the FSM when F=0.",
            "The value of V is 10 cents when a dime is inserted. This is because a dime is worth 10 cents, and the machine tracks the value of money inserted in terms of nickels. Therefore, when a dime is inserted, the machine records it as 2 nickels."
        ],
        "score": [
            5.760190963745117,
            -8.024236679077148,
            -10.795764923095703
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": " ",
        "answers": [
            "\nWhat is the reason for the popularity of two-level logic?\n\nThe popularity of two-level logic is due to the fact that it is a simple way to express logic functions. Additionally, two-level logic is easy to map into CMOS gates.",
            "S_2 is 1 when the counter is in the HALT A state and HOLD=1. This is because when S_2=1, the counter is halted, and when HOLD=1, the counter should retain its current value.",
            "A computer memory is a group of storage elements and the logic necessary to move data in and out of the elements. The size of the elements in a memory—called the addressability of the memory—varies from a single binary digit, or bit, to a byte (8 bits) or more. Typically, we refer to data elements larger than a byte as words, but the size of a word depends on context."
        ],
        "score": [
            5.180834770202637,
            5.231083869934082,
            4.20796012878418
        ],
        "text": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first"
    },
    {
        "question": "How many variables are used in the code?\\n",
        "answers": [
            "\nThere are three variables used in the code, which are x, y, and z. The value of x is initialized to 42, and the value of y is initialized to x. The value of z is not initialized.",
            "A tri-state buffer is a digital circuit that can be used to either connect or disconnect an output from a signal line. The buffer has three states: high, low, and high impedance. When the buffer is in the high state, the output is connected to the signal line and copies the signal from the input. When the buffer is in the low state, the output is disconnected from the signal line. When the buffer is in the high impedance state, the output is effectively disconnected from the signal line.",
            "The HOLD variable is used to save energy. When a chip finishes its work, it can turn itself off, saving energy. This variable allows the chip to turn itself off when it is finished, saving energy."
        ],
        "score": [
            6.061981201171875,
            -10.809378623962402,
            -8.16404914855957
        ],
        "text": "{|c|}  8 bits& {  int8_t}& {  uint8_t} 16 bits& { int16_t}& { uint16_t} 32 bits& { int32_t}& { uint32_t} 64 bits& { int64_t}& { uint64_t}  {-14pt}  in the C library, reflecting the fact that many companies had already developed and were using such definitions to make their programs platform-independent.  We encourage you to make use of these types, which are shown in  the table above.  In Linux, they can be made available by including  the { stdint.h} header file.  Floating-point types in C include { float} and { double}, which correspond respectively to single- and double-precision IEEE floating-point values.  Although the {32-bit} { float} type can save memory compared with use of {64-bit} { double} values, C's math library works with double-precision values, and single-precision data are uncommon in scientific and engineering codes.  In contrast, single-precision floating-point operations dominated the graphics industry until recently, and are still well-supported even on today's graphics processing units.   {Variable Declarations}  The function { main} executed by a program begins with a list of { variable declarations}.  Each declaration consists of two parts: a data type specification and a comma-separated list of variable names. Each variable declared can also  be { initialized} by assigning an initial value.  A few examples  appear below.  Notice that one can initialize a variable to have the same value as a second variable.  {  aaaaaaa=aa=aaaaaaaaaaaaaaaaaaaaa=/=* a third 2's complement variable with unknown initial value = int > x > = 42; >/>* a 2's complement variable, initially equal to 42 > */ int > y > = x;  >/>* a second 2's complement variable, initially equal to x > */ int > z;>       >/>* a third 2's complement variable with unknown initial value > */ double> a, b, c, pi = 3.1416; > >/>* >>>>* four double-precision IEEE floating-point variables >>>>* a, b, and c are initially of unknown value, while pi is >>>>* initially 3.1416 >>>>*/  }  What happens if a programmer declares a variable but does not  initialize it?  Remember that bits can only be 0 or 1.  An uninitialized variable does have a value, but its value is unpredictable.  The compiler tries to detect uses of uninitialized variables, but sometimes it fails to do so, so { until you are more familiar with programming, you should always initialize every variable}.  Variable names, also called { identifiers}, can include both letters and digits in C.  Good programming style requires that programmers select variable names that are meaningful and are easy to distinguish from one another.  Single letters are acceptable in some situations, but longer names with meaning are likely to help people (including you!) understand your  program.  Variable names are also case-sensitive in C, which allows programmers to use capitalization to differentiate behavior and meaning, if desired. Some programs, for example, use identifiers with all capital letters to indicate variables with values that remain constant for the program's entire execution.  However, the fact that identifiers are case-sensitive also means  that a programmer can declare distinct variables  named { variable}, { Variable}, { vaRIable}, { vaRIabLe},  and { VARIABLE}.  We strongly discourage you from doing so.     {Expressions and Operators}   The { main} function also contains a sequence of statements.  A statement is a complete specification of a single step in the program's execution.  We explain the structure of statements in the next section.    Many statements in C include one or more { expressions}, which represent calculations such as arithmetic, comparisons, and logic operations.  Each expression is in turn composed of { operators} and { operands}.  Here we give only a brief introduction to some of the operators available in the C language.  We deliberately omit operators with more complicated meanings, as well as operators for which the original purpose was to make writing common operations a"
    }
]