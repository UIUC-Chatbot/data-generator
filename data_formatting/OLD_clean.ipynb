{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import more_itertools\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "import os \n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(text, length, overlap):\n",
    "    overlapped_words = []\n",
    "    textbook_list = text.split()\n",
    "    all_words = []\n",
    "    num_of_sections = int(np.floor(len(textbook_list) / length))\n",
    "    group = more_itertools.divide(num_of_sections, textbook_list)\n",
    "    final_list = []\n",
    "    for section in group:\n",
    "        all_words.append(list(section))\n",
    "        \n",
    "    for i in range(len(all_words)):\n",
    "        if i == 0:\n",
    "            overlapped_words.append(all_words[i])\n",
    "            overlapped_words.append(all_words[i][-overlap:] + all_words[i + 1][:-overlap])\n",
    "        elif i == len(all_words) - 1:\n",
    "            overlapped_words.append(all_words[i])\n",
    "        else:\n",
    "            overlapped_words.append(all_words[i][overlap*2:] + all_words[i + 1][:-overlap])\n",
    "    for section in overlapped_words:\n",
    "        patel_text = ' '.join(section)\n",
    "        final_list.append(patel_text)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex = ''\n",
    "directory = '../../raw_data/patel_textbook/'\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    with open(f, 'r') as file:\n",
    "        print(f)\n",
    "        # text = file.read()\n",
    "        # latex += text\n",
    "# textbook = LatexNodes2Text().latex_to_text(latex)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spacy <br>\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "conda install -c conda-forge cupy\n",
    "python -m spacy download en_core_web_trf\n",
    "\n",
    "pip install langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "textbook = textbook.replace(\"\\n\", \" \")\n",
    "\n",
    "doc = nlp(textbook)\n",
    "print(doc)\n",
    "# list of sentences availble in: doc.sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sentences to a .txt file\n",
    "# Define the file name\n",
    "from typing import List\n",
    "\n",
    "def save_to_file(sentences: List, filename:str):\n",
    "    filename = \"cleaned_patel_sentences.txt\"\n",
    "\n",
    "    # Open the file in write mode\n",
    "    with open(filename, \"w\") as file:\n",
    "        # Iterate through the list of sentences\n",
    "        for sentence in sentences:\n",
    "            # Write each sentence to the file followed by a newline\n",
    "            \n",
    "            file.write(sentence.text.encode(encoding='ascii',errors='ignore').decode()) # ASCII\n",
    "\n",
    "def load_from_txt_file(filename: str) -> List:\n",
    "    # Open the file in read mode\n",
    "    with open(filename, \"r\") as file:\n",
    "        # Read the contents of the file\n",
    "        text = file.read()\n",
    "    # Split the contents of the file into a list of sentences\n",
    "    sentences = text.splitlines()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_file(doc.sents, \"cleaned_patel_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from huggingface import AutoTokenizer\n",
    "\n",
    "# my_text_splitter = \n",
    "my_splitter = langchain.text_splitter().from_huggingface_tokenier(AutoTokenizer('google/flan-t5-xl'))\n",
    "my_splitter.split(textbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(textbook) # 878461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_list = textbook.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(textbook_list) % 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200 words with some percent overlap\n",
    "all_words = []\n",
    "num_of_sections = int(np.floor(len(textbook_list) / 200))\n",
    "group = more_itertools.divide(num_of_sections, textbook_list)\n",
    "for section in group:\n",
    "    all_words.append(list(section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 20\n",
    "overlapped_words = []\n",
    "for i in range(len(all_words)):\n",
    "    if i == 0:\n",
    "        overlapped_words.append(all_words[i])\n",
    "        overlapped_words.append(all_words[i][-overlap:] + all_words[i + 1][:-overlap])\n",
    "    elif i == len(all_words) - 1:\n",
    "        overlapped_words.append(all_words[i])\n",
    "    else:\n",
    "        overlapped_words.append(all_words[i][overlap*2:] + all_words[i + 1][:-overlap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(overlapped_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patel_json = {}\n",
    "\n",
    "for i, section in enumerate(overlapped_words):\n",
    "    patel_text = ' '.join(section)\n",
    "    patel_json[i] = patel_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = overlap(textbook, 200, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('patel_overlapped_textbook.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(patel_json, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can test the code to see how it works yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[i for i in range(20)], [i for i in range(20, 40)], [i for i in range(40, 60)], [i for i in range(60, 80)], [i for i in range(80, 100)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "test[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 5\n",
    "overlapped_words_test = []\n",
    "for i in range(len(test)):\n",
    "    if i == 0:\n",
    "        overlapped_words_test.append(test[i])\n",
    "        overlapped_words_test.append(test[i][-overlap:] + test[i + 1][:-overlap])\n",
    "    elif i == len(test) - 1:\n",
    "        overlapped_words_test.append(test[i])\n",
    "    else:\n",
    "        overlapped_words_test.append(test[i][overlap*2:] + test[i + 1][:-overlap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapped_words_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:49:35) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a405b64c2df2f161fa14d0a5cd4eca74575679b4df8dce77b0534f824bfab47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
