{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spacy <br>\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "conda install -c conda-forge cupy\n",
    "python -m spacy download en_core_web_trf\n",
    "\n",
    "pip install langchain pinecone-client PyPDF2\n",
    "# maybe: conda install -c conda-forge -y ipykernel=6\n",
    "```\n",
    "\n",
    "Note: \n",
    "* Flan T5 XL max length is 512\n",
    "* Flan T5 XXL max length is 1024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Parse textbook (retain page numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages:  164\n"
     ]
    }
   ],
   "source": [
    "# parse textbook. \n",
    "# pip install PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    " \n",
    "reader = PdfReader('../../non-public-datasets/raw_data/ece120-notes/Student_Notes.pdf')\n",
    "# reader = PdfReader('../raw_data/patel_textbook/Yale Patt - Introduction to Computing Systems_ From Bits & Gates to C & Beyond.pdf')\n",
    "print(\"Total pages: \", len(reader.pages))\n",
    " \n",
    "# extracting text from page\n",
    "textbook = []\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text().replace(\"\\n\", \" \").replace(\"c/⌋ir⌋l⌉⌋opyrt2000-2017 Steven S. Lumetta. All rights reserved.\", \"\").replace(\"c/⌋ir⌋l⌉⌋opyrt2000-2017 Steven S. Lumetta. All rights reserved.\", \"\")\n",
    "    # skip empty pages\n",
    "    if text:\n",
    "        textbook.append(dict(\n",
    "                            text=text,\n",
    "                            page_number=i, \n",
    "                            textbook_name='ECE-120-student-notes'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up useless pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' 1 ECE120: Introduction to Computer Engineering Notes Set 1.1 The Halting Problem For some of the topics in this course, we plan to cover the material m ore deeply than does the textbook. We will provide notes in this format to supplement the textbook for this purpose. In order to make these notes more useful as a reference, deﬁnitions are highlighted with boldfac e, and italicization emphasizes pitfalls or other important points. Sections marked with an asterisk are provided solely for you r interest, but you probably need to learn this material in later classes. These notes are broken up into four parts, corresponding to the three midterm exams and the ﬁnal exam. Each part is covered by one examination in our class. The last section of each of the four parts gives you a summary of material that you are expected to know for the cor responding exam. Feel free to read it in advance. As discussed in the textbook and in class, a universal computational device (orcomputing machine ) is a device that is capable of computing the solution to any problem tha t can be computed, provided that the device is given enough storage and time for the computation to ﬁ nish. One might ask whether we can describe problems that we cannot ans wer (other than philosophical ones, such as the meaning of life). The answer is yes: there are problems t hat are provably undecidable , for which no amount of computation can solve the problem in general. This set of notes describes the ﬁrst problem known to be undecidable, the halting problem . For our class, you need only recognize the name and realize that one can, in fact, give examples of problems that can not be solved by computation. In the fu- ture, you should be able to recognizethis type of problem so as to av oidspending your time trying to solve it. 1.1.1 Universal Computing Machines* The things that we call computers today, whether we are talking ab out a programmable microcontroller in a microwave oven or the Blue Waters supercomputer sitting on the s outh end of our campus (the United States’ main resource to support computational science resear ch), are all equivalent in the sense of what problems they can solve. These machines do, of course, have acce ss to diﬀerent amounts of memory, and compute at diﬀerent speeds. The idea that a single model of computation could be described and pr oven to be equivalent to all other models came out of a 1936 paper by Alan Turing, and today we genera lly refer to these devices as Turing machines . All computers mentioned earlier, as well as all computers with which you are familiar in your daily life, are provably equivalent to Turing machines. Turing also conjectured that his deﬁnition of computable was identic al to the “natural” deﬁnition (today, this claim is known as the Church-Turing conjecture ). In other words, a problem that cannot be solved by a Turing machine cannot be solved in any systematic manner, with a ny machine, or by any person. This conjecture remains unproven! However, neither has anyone bee n able to disprove the conjecture, and it is widely believed to be true. Disproving the conjecture requires that one demonstrate a systematic technique (or a machine) capable of solving a problem that cannot be solved by a Turing machine. No one has been able to do so to date. 1.1.2 The Halting Problem* You might reasonably ask whether any problems can be shown to be in computable. More common terms for such problems—those known to be insolvable by any computer—a reintractable or undecidable. In the same 1936 paper in which he introduced the universal computing mac hine, Alan Turing also provided an answertothis questionby introducing(andproving)that therear ein factproblemsthat cannotbe computed by a universal computing machine. The problem that he proved unde cidable, using proof techniques almost identical to those developed for similar problems in the 1880s, is now k nown as the halting problem .', 'page_number': 6, 'textbook_name': 'ECE-120-student-notes'}\n"
     ]
    }
   ],
   "source": [
    "# delete first 5 pages, that's all the cleaning we're doing.\n",
    "textbook = textbook[6:]\n",
    "print(textbook[0])\n",
    "\n",
    "# save cleaned version to file\n",
    "import json\n",
    "with open('./student_notes_cleaned.json', 'w') as f:\n",
    "    json.dump(textbook, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textbook) # 158 pages remaining in Student Notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas = [dict(page_number=page['page_number'], textbook_name=page['textbook_name']) for page in textbook]\n",
    "textbook_texts = [page['text'] for page in textbook]\n",
    "assert len(textbook_texts) == len(metadatas), 'must be equal sizes'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the textbook string into context-size contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num chunks 286\n",
      "Num chunks after filtering short ones 283\n"
     ]
    }
   ],
   "source": [
    "from langchain import text_splitter\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import CharacterTextSplitter, NLTKTextSplitter, SpacyTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# good examples here: https://langchain.readthedocs.io/en/latest/modules/utils/combine_docs_examples/textsplitter.html\n",
    "tokenizer = AutoTokenizer.from_pretrained('OpenAssistant/oasst-sft-1-pythia-12b')\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=682, chunk_overlap=100, separators = \". \",)\n",
    "# texts = text_splitter.split_text(textbook)\n",
    "texts = text_splitter.create_documents(texts=textbook_texts, metadatas=metadatas)\n",
    "print(\"Num chunks\", len(texts))\n",
    "\n",
    "# 250 --> 2723\n",
    "# 450 chunks --> 452 passages\n",
    "# 682 chunks --> 286 passages\n",
    "\n",
    "# must be bigger than 50 chars\n",
    "clean_meta = [text.metadata     for text in texts if len(text.page_content) > 50]\n",
    "clean_text = [text.page_content for text in texts if len(text.page_content) > 50]\n",
    "print(\"Num chunks after filtering short ones\",len(clean_text))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embed each context, and save it to a vector database, this one is hosted by Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/kastanday/.cache/torch/sentence_transformers/intfloat_e5-large. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import HuggingFaceEmbeddings #OpenAIEmbeddings, \n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load API keys from globally-availabe .env file\n",
    "load_dotenv(dotenv_path='/mnt/project/chatbotai/huggingface_cache/internal_api_keys.env', override=True)\n",
    "\n",
    "pinecone.init(api_key=os.environ['PINECONE_API_KEY_NEW_ACCT'], environment=\"us-east4-gcp\")\n",
    "\n",
    "model_name = \"intfloat/e5-large\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "pinecone_index = Pinecone.from_texts(\n",
    "    texts=clean_text,\n",
    "    metadatas=clean_meta,\n",
    "    embedding=embeddings,\n",
    "    index_name=\"uiuc-chatbot-deduped\" \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe from clean_text, clean_meta, embeddings\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'text': clean_text, 'metadata': clean_meta})\n",
    "df.to_parquet('../finalized_datasets/Lumetta_student_notes-chunk_size_682-chunk_overlap_100.parquet', index=False)\n",
    "df.to_csv('../finalized_datasets/Lumetta_student_notes-chunk_size_682-chunk_overlap_100.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ Done with critical steps, the rest is for demonstration only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Easily run simliarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='82  3.1.3 Finite State Machines Aﬁnite state machine (orFSM) is a model for understanding the behavior of a system by describin g the system as occupying one of a ﬁnite set of states, moving betwe en these states in response to external inputs, and producing external outputs. In any given state, a pa rticular input may cause the FSM to move to another state; this combination is called a transition rule . An FSM comprises ﬁve parts: a ﬁnite set of states, a set of possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs. When an FSM is implemented as a digital system, all states must be rep resented as patterns using a ﬁxed number of bits, all inputs must be translated into bits, and all outpu ts must be translated into bits. For a digital FSM, transition rules must be complete ; in other words, given any state of the FSM, and any pattern of input bits, a transition must be deﬁned from that state to another state (transitions from a state to itself, called self-loops , are acceptable). And, of course, calculation of outputs for a dig ital FSM reduces to Boolean logic expressions. In this class, we focus on clocked sync hronous FSM implementations, in which the FSM’s internal state bits are stored in ﬂip-ﬂops. In this section, we introduce the tools used to describe, develop, a nd analyze implementations of FSMs with digital logic. In the next few weeks, we will show you how an FSM can se rve as the central control logic in a computer. At the same time, we will illustrate connections between F SMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital control systems. The table below gives a list of abstract states for a typical keyless entry system for a car. In this case, we have merely named the states rather than specifying the bit pat terns to be used for each state—for this reason, we refer to them as abstract states. The description of the states in the ﬁrst column is an optional element often included in the early design stages for an FSM, when ide ntifying the states needed for the design. A list may also include the outputs for each state. Again, in th e list below, we have speciﬁed these outputs abstractly. By including outputs for each state, we implicit ly assume that outputs depend only on the state of the FSM. We discuss this assumption in more detail later in these notes (see “Machine Models”), but will make the assumption throughout our class. meaning state driver’s door other doors alarm on vehicle locked LOCKED locked locked no driver door unlocked DRIVER unlocked locked no all doors unlocked UNLOCKED unlocked unlocked no alarm sounding ALARM locked locked yes Another tool used with FSMs is the next-state table (sometimes called a state transition table , or just astate table ), which maps the current state and input combination into the next state of the FSM. The abstract variant shown below outlines desired behavior at a high leve l, and is often ambiguous, incomplete, and even inconsistent. For example, what happens if a user pushes two buttons? What happens if they push unlock while the alarm is sounding? These questions should event ually be considered. However, we can already start to see the intended use of the design: starting f rom a locked car, a user can push “unlock” once to gain entry to the driver’s seat, or push “unlock” twice to op en the car fully for passengers. To lock the car, a user can push the “lock” button at any time. And, if a use r needs help, pressing the “panic” button sets oﬀ an alarm. state action/input next state LOCKED push “unlock” DRIVER DRIVER push “unlock” UNLOCKED (any) push “lock” LOCKED (any) push “panic” ALARM' lookup_str='' metadata={'page_number': 87.0, 'textbook_name': 'ECE-120-student-notes'} lookup_index=0\n",
      "\n",
      "page_content='3.1 Serialization and Finite State Machines 79 ECE120: Introduction to Computer Engineering Notes Set 3.1 Serialization and Finite State Machines The third part of our class builds upon the basic combinational and se quential logic elements that we developed in the second part. After discussing a simple application of stored state to trade between area and performance, we introduce a powerful abstraction for formalizin g and reasoning about digital systems, the Finite State Machine (FSM). General FSM models are broadly applicab le in a range of engineering contexts, including not only hardware and software design but also the design o f control systems and distributed systems. We limit our model so as to avoid circuit timing issues in your ﬁr st exposure, but provide some amount of discussion as to how, when, and why you should eventually learn the more sophisticated models. Through development a range of FSM examples, we illustrate importa nt design issues for these systems and motivateacoupleofmoreadvancedcombinationallogicdevicesthat canbe usedasbuilding blocks. Together with the idea of memory, another form of stored state, these elem ents form the basis for development of our ﬁrst computer. At this point we return to the textbook, in whic h Chapters 4 and 5 provide a solid introduction to the von Neumann model of computing systems and t he LC-3 (Little Computer, version 3) instruction set architecture. By the end of this part of the cours e, you will have seen an example of the boundary between hardware and software, and will be ready to wr ite some instructions yourself. In this set of notes, we cover the ﬁrst few parts of this material. W e begin by describing the conversion of bit-sliced designs into serial designs, which store a single bit slice’s out put in ﬂip-ﬂops and then feed the outputs back into the bit slice in the next cycle. As a speciﬁc example, we use our bit-sliced comparator to discuss tradeoﬀs in area and performance. We introduce Finite S tate Machines and some of the tools used to design them, then develop a handful of simple counter desig ns. Before delving too deeply into FSM design issues, we spend a little time discussing other strategies for c ounter design and placing the material covered in our course in the broader context of digital system des ign. Remember that sections marked with an asterisk are provided solely for your interest, but you may need to learn this material in later classes. 3.1.1 Serialization: General Strategy In previous notes, we discussed and illustrated the development of bit-sliced logic, in which one designs a logic block to handle one bit of a multi-bit operation, then replicates th e bit slice logic to construct a design for the entire operation. We developed ripple carry adders in this wa y in Notes Set 2.3 and both unsigned and 2’s complement comparators in Notes Set 2.4. Another interesting design strategy is serialization : rather than replicating the bit slice, we can use ﬂip- ﬂops to store the bits passed from one bit slice to the next, then pr esent the stored bits to the same bit slice in the next cycle. Thus, in a serial design, we only need one copy of th e bit slice logic! The area needed for a serial design is usually much less than for a bit-sliced design, but such a design is also usually slower. After illustrating the general design strategy, we’ll consider thes e tradeoﬀs more carefully in the context of a detailed example. Recall the general bit-sliced design ap- proach, as illustrated to the right. Some number of copies of the logic for a single bit slice are connected in sequence. Each bit slice accepts Pbits of operand input and produces Qbits of external output. Adjacent bit slices receive an addi- tionalMbits of information from the previous bit slice and pass along Mbits to the next bit slice, generallyusing some representation chosen by the designer.P Qsecond bit sliceMP Qlast bit sliceMP QM Moutput logicRinitial values . . .first bit sliceresults per−slice outputsper−slice inputsa general bit−sliced design The ﬁrst bit slice is initialized by passing in constant values, and some ca lculation may be performed on the ﬁnal bit slice’s results to produce Rbits more external output.' lookup_str='' metadata={'page_number': 84.0, 'textbook_name': 'ECE-120-student-notes'} lookup_index=0\n",
      "\n",
      "page_content='3.3 Design of the Finite State Machine for the Lab 103 be reached from any of the states in our design. We might then try to leverage the fact that the next- state patterns from these two states are not relevant (recall that we ﬁxed the next-state patterns for all four of the possible PAID states) to further simplify our logic, but doing so does not provide any advan- tage (you may want to check our claim). The ﬁnal state table is shown to the right. We have included the extra states at the bottom of the table. Wehavespeciﬁedthenext-statelogicfortheseS+ 2S+ 1S+ 0 state S2S1S0T= 0T= 1A P PAID1 010 000 100 1 1 PAID2 101 000 100 1 1 DIME 000 001 101 1 0 REJECTD 001 001 101 0 0 QUARTER 100 010 110 1 0 REJECTQ 110 010 110 0 0 EXTRA1 011 000 100 x x EXTRA2 111 000 100 x x states, but left the output bits as don’t cares. A state transition diagram appears at the bottom of this page. 3.3.4 Testing the Design Having a complete design on paper is a good step forward, but human s make mistakes at all stages. How can we know that a circuit that we build in the lab correctly implements t he FSM that we have outlined in these notes? For the lab design, we have two problems to solve. First, we have not speciﬁed an initialization scheme for the FSM. We may want the FSM to start in one of the PAID states, bu t adding initialization logic to the design may mean requiring you to wire together signiﬁcantly more chip s. Second, we need a sequence of inputs that manages to test that all of the next-state and outpu t logic implementations are correct. Testing sequential logic, including FSMs, is in general extremely diﬃcu lt. In fact, large sequential systems today are generally converted into combinational logic by using shift registers to ﬁll the ﬂip-ﬂops with a particular pattern, executing the logic for one clock cycle, and che cking that the resulting pattern of bits in the ﬂip-ﬂops is correct. This approach is called scan-based testing , and is discussed in ECE 543. You will make use of a similar approach when you test your combinational logic in the second week of the lab, before wiring up the ﬂip-ﬂops. We have designed our FSM to be easy to test (even small FSMs may be challenging) with a brute force approach. In particular, we identify two input sequences that tog ether serve both to initialize and to test a correctly implemented variant of our FSM. Our initialization sequence forces the FSM into a speciﬁc state regardless of its initial state. And our test sequence crosses eve ry transition arc leaving the six valid states. In terms of T, the coin type, we initialize the FSM with the input sequence 001. Notic e that such a sequence takes any initial state into PAID2. For testing, we use the input sequence 111010010001. You should trace this sequence, starting from PAID2, on the diagram below to see how the test sequence covers all of the possible arcs. As we test, we need also to observe the AandPoutputs in each state to check the output logic. T=0 T=0 T=1T=1 T=1T=1 T=1T=0 T=0T=0 T=1T=1 T=0T=1T=0 T=0 QTR 100/10PAID1 010/11PAID2 101/11EXTRA1 011/xxEXTRA2 111/xxDIME 000/10REJECTD 001/00 REJECTQ 110/00' lookup_str='' metadata={'page_number': 108.0, 'textbook_name': 'ECE-120-student-notes'} lookup_index=0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Easily run similarity search on the Pinecone index\n",
    "question = \"What is a finite state machine in electrical engineering?\"\n",
    "relevant_context_list = pinecone_index.similarity_search(question, k=3)\n",
    "\n",
    "for d in relevant_context_list:\n",
    "    print(d)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3 Instruction Set Architecture* 147 ECE120: Introduction to Computer Engineering Notes Set 4.3 Instruction Set Architecture* This set of notes discusses tradeoﬀs and design elements of instru ction set architectures (ISAs). The material is beyond the scope of our class, and is provided purely for yo ur interest. Those who ﬁnd these topics interesting may also want to read the ECE391 notes, which describe similar material with a focus on the x86 ISA. As you know, the ISA deﬁnes the interface between software and hardware, abstracting the capabilities of a computer’s datapath and standardizing the format of instructio ns to utilize those capabilities. Successful ISAs are rarely discarded, as success implies the existence of large amounts of software built to use the ISA. Rather, they are extended, and their original forms must be supp orted for decades (consider, for example, the IBM 360 and the Intel x86). Employing sound design principles is t hus imperative in an ISA. 4.3.1 Formats and Fields* The LC-3 ISA employs ﬁxed-length instructions and a load-store ar chitecture, two aspects that help to reduce the design space to a manageable set of choices. In a gener al ISA design, many other options exist for instruction formats. Recall the idea of separating the bits of an instruction into (possibly non-contiguous) ﬁelds. One of the ﬁelds must contain an opcode, which speciﬁes the type of operation to be performed by the instruction. In the LC-3 ISA, most opcodes specify both the type of operation and th e types of arguments to the operation. More generally, many addressing modes are possible for each opera nd, and we can think of the bits that specify the addressing mode as a separate ﬁeld, known as the modeﬁeld. As a simple example, the LC-3’s ADD and AND instructions contain a 1-bit mode ﬁeld that speciﬁes whe ther the second operand of the ADD/AND comes from a register or is an immediate value. Several questions must be answered in order to deﬁne the possible instruction formats for an ISA. First, are instructions ﬁxed-length or variable-length? Second, how many ad dresses are needed for each instruction, and how many of the addresses can be memory addresses? Finally, w hat forms of addresses are possible for each operand? For example, can one use full memory addresses or only limited oﬀsets relative to a register? The answertothe ﬁrstquestiondepends on manyfactors, but se veralclearadvantagesexist forboth answers. Fixed-length instructions are easy to fetch and decode. A processor knows in advance how m any bits must be fetched to fetch a full instruction; fetching the opcode a nd mode ﬁelds in order to decide how many more bits are necessary to complete the instruction may requ ire more than one cycle. Fixing the time necessary for instruction fetch also simpliﬁes pipelining. Finally, ﬁ xed-length instructions simplify the datapath by restricting instructions to the size of the bus and alwa ys fetching properly aligned instructions. As an example of this simpliﬁcation, note that the LC-3 ISA does not s upport addressing for individual bytes, only for 16-bit words. Variable-length instructions also have beneﬁts, however. Variable-length encodings allow more e ﬃcient encodings, saving both memory and disk space. A register transfe r operation, for example, clearly requires fewer bits than addition of values at two direct memory addresses f or storage at a third. Fixed-length in- structions must be ﬁxed at the length of the longest possible instru ction, whereasvariable-length instructions can use lengths appropriate to each mode. The same tradeoﬀ has a nother form in the sense that ﬁxed-length ISAs typically eliminate many addressing modes in order to limit the size o f the instructions. Variable- length instructions thus allow more ﬂexibility; indeed, extensions to a variable-length ISA can incorporate new addressing modes that require longer instructions without aﬀe cting the original ISA. For example, the maximum length of x86 instructions has grown from six bytes in 1978 ( the 8086 ISA) to ﬁfteen bytes in today’s version of the ISA.\n",
      "{'page_number': 152.0, 'textbook_name': 'ECE-120-student-notes'}\n",
      "\n",
      "158  We expect you to be able to exercise the following skills: •Map RTL (register transfer language) operations into control wo rds for a given processor datapath. •Systematically decompose a (simple enough) problem to the level of L C-3 instructions. •Encode LC-3 instructions into machine code. •Read and understand programs written in LC-3 assembly/machine c ode. •Test and debug a small program in LC-3 assembly/machine code. •Be able to calculate the Hamming distance of a code/representation . •Know the relationships between Hamming distance and the abilities to d etect and to correct bit errors. We expect that you will understand the concepts and ideas to the e xtent that you can do the following: •Explain the role of diﬀerent types of instructions in allowing a program mer to express a computation. •Explain the importance of the three types of subdivisions in systema tic decomposition (sequential, conditional, and iterative). •Explainthe processoftransformingassemblycodeintomachinecod e(that is, explainhowanassembler works, including describing the use of the symbol table). •Be able to use parity for error detection, and Hamming codes for er ror correction. At the highest level, we hope that, while you do not have direct subst antial experience in this regard from our class (and should not expect to be tested on these skills), that you will nonetheless be able to begin to do the following when designing combinational logic: •Design and compare implementations using gates, decoders, muxes , and/or memories as appropriate, and including reasoning about the relevant design tradeoﬀs in terms of area and delay. •Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based design, again in- cluding reasoning about the relevant design tradeoﬀs in terms of ar ea and delay. •Design and compare implementations of processor control units us ing both hardwired and micropro- grammed strategies, and again including reasoning about the releva nt design tradeoﬀs in terms of area and delay. •Understandbasictradeoﬀsinthesparsityofcodewordswith erro rdetectionandcorrectioncapabilities.\n",
      "{'page_number': 163.0, 'textbook_name': 'ECE-120-student-notes'}\n",
      "\n",
      "138  The ﬁgure to the right illus- trates a general multi-cycle, hardwired control unit. The three blocks on the left are the control unit state. The combinational logic in the middle uses the control unit state along with some dat- apath status bits to com- pute the control signals for the datapath and the extra controls needed for the FSM counter, IR, and PC. The datapathappearstothe right in the ﬁgure.IRLD DATA PCLD DATAlogiccombinational datapathN−cycle binary counter control signals datapath statusPAUSERESET How complex is the combinational logic? As mentioned earlier, we assum e that the PC does not directly aﬀect control. But we still have 16 bits of IR, the FSM counter stat e, and the datapath status signals. Perhaps we need 24-variable K-maps? Here’s where engineering and human design come to the rescue: by careful design of the ISA and the encoding, the authors have mad e many of the datapath control signals for the LC-3 ISA quite simple. For example, the register that appears o n the register ﬁle’s SR2 output is always speciﬁed by IR[2:0]. The SR1 output requires a mux, but the choices a re limited to IR[11:9] and IR[8:6] (and R6 in the design with support for interrupts). Similarly, the des tination register in the register ﬁle is always R7 or IR[11:9] (or, again, R6 when supporting interrupts). T he control signals for an LC-3 datapath depend almost entirely on the state of the control unit FSM (count er bits in a hardwired design) and the opcode IR[15:12]. The control signals are thus reduced to fairly simp le functions. Let’s imagine building a hardwired control unit for the LC-3. Let’s sta rt by being more precise about the number of inputs to the combinational logic. Although most decisions are based on the opcode, the datapath and state diagram shown earlier for the LC-3 ISA do have one instan ce of using another instruction bit to determine behavior. Speciﬁcally, the JSR instruction has two modes , and the control unit uses IR[11] to choose between them. So we need to have ﬁve bits of IR instead of f our as input to our logic. Howmanydatapathstatussignalsareneeded? When thecontrolu nit accessesmemory, itmust waituntil the memory ﬁnishes the access, as indicated by a memory ready signal R . And the control unit must implement the conditional part of conditional branches, for which it uses the datapath’s branch enable signal BEN. These two datapath status signals suﬃce for our design. How many bits do we need for the counter? Instruction fetch requ ires three cycles: one to move the PC to the MAR and increment the PC, a second to read from memory into MDR, and a third to move the instruction bits across the bus from MDR into IR. Instruction deco ding in a hardwired design is implicit and requires no cycles: since all of our control signals can depend o n the IR, we do not need a cycle to change the FSM state to reﬂect the opcode. Looking at the LC-3 s tate diagram, we see that processing an instruction requires at most ﬁve cycles. In total, at most eight ste ps are needed to fetch and process any LC-3 instruction, so we can use a 3-bit binary counter. We thus have a total of ten bits of input: IR[15:11], R, BEN, and a 3-b it counter. Adding the RESET and PAUSE controls for our FSM counter to the 25 control signals listed earlier, we need to ﬁnd 27 functions on 10 variables. That’s still a lot of big K-maps to solve. Is there an easie r way? 4.1.5 Using a Memory for Logic Functions Consider a case in which you need to compute many functions on a sma ll number of bits, such as we just described for the multi-cycle, hardwired control unit. One strate gy is to use a memory (possibly a read-only memory). A 2m×Nmemory can be viewed as computing Narbitrary functions on mvariables. The functions to be computed are speciﬁed by ﬁlling in the bits of the memo ry. So long as the value of mis fairly small, the memory (especially SRAM) can be fast.\n",
      "{'page_number': 143.0, 'textbook_name': 'ECE-120-student-notes'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Easily run similarity search on the Pinecone index\n",
    "question = \"What is LC-3?\"\n",
    "relevant_context_list = pinecone_index.similarity_search(question, k=3)\n",
    "\n",
    "for d in relevant_context_list:\n",
    "    print(d.page_content)\n",
    "    print(d.metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 3.074671985814348e-05, 'start': 1981, 'end': 2020, 'answer': 'wishes to secure a bicycle with a lock,'}\n",
      "3.6 Sequential Logic Circuits 79 Combinational logic ci rcuit Sto rage elementsOutput Input Figure 3.22 Sequential logic circuit block diagram. In this section, we discuss digital logic structures that can both process infor- mation (i.e., make decisions) andstore information. That is, these structures base their decisions not only on the input values now present, but also (and this is very important) on what has happened before. These structures are usually called sequential logic circuits . They are distinguishable from combinational logic cir- cuits because, unlike combinational logic circuits, they contain storage elements that allow them to keep track of prior history information. Figure 3.22 shows a block diagram of a sequential logic circuit. Note the storage elements. Note also that the output can be dependent on both the inputs now and the values stored in the storage elements. The values stored in the storage elements reﬂect the history of what has happened before. Sequential logic circuits are used to implement a very important class of mechanisms called ﬁnite state machines . We use ﬁnite state machines in essen- tially all branches of engineering. For example, they are used as controllers of electrical systems, mechanical systems, and aeronautical systems. A traﬃc light controller that sets the traﬃc light to red, yellow, or green depends on the light that is currently on (history information) and input information from sensors such as trip wires on the road, a timer keeping track of how long the current light has been on, and perhaps optical devices that are monitoring traﬃc. We will see in Chapter 4 when we introduce the von Neumann model of a computer that a ﬁnite state machine is at the heart of the computer. It controls the processing of information by the computer. 3.6.1 A Simple Example: The Combination Lock A simple example shows the diﬀerence between combinational logic structures and sequential logic structures. Suppose one wishes to secure a bicycle with a lock, but does not want to carry a key. A common solution is the combination lock. The person memorizes a “combination” and uses it to open the lock. Two common types of locks are shown in Figure 3.23. In Figure 3.23a, the lock consists of a dial, with the numbers from 0 to 30 equally spaced around its circumference. To open the lock, one needs to know the “combination.” One such combination could be: R13-L22-R3. If this were the case, one would open the lock by turning the dial two complete turns to the right (clockwise), and then continuing until the dial points to 13, followed by one\n",
      "{'score': 3.898281283909455e-05, 'start': 1035, 'end': 1068, 'answer': 'the transition arc for each state'}\n",
      "Exercises 119 b.Complete the state machine. (We have provided nine states. You will not need all of them. Use only as many as you need): ★3.62 You are taking three courses, one each in computing (C), engineering (E), and math (M). In each course, you periodically receive assignments. You never receive more than one assignment at a time. You also never receive another assignment in a course if you currently have an assignment in that course that has not been completed. You must procrastinate (i.e., do nothing) unless you have unﬁnished assignments in both computing and engineering. Design a ﬁnite state machine to describe the state of the work you have to do and whether you are working or procrastinating. a.Label each state with the unﬁnished assignments (with letters C,E,M) for when you are in that state. There are far more states provided than you actually need. Use only what you need. b.There are six inputs: c, e, m, c,e,m. c, e, m refer to you receiving an assignment. c,e,mrefer to you completing an assignment. Draw the transition arc for each state/input pair. For example, if you had previously only had an unﬁnished assignment in math and you received an assignment in computing, you would transition from state M to state CM, as shown below.\n",
      "{'score': 3.661367009044625e-05, 'start': 469, 'end': 475, 'answer': 'is the'}\n",
      "Exercises 117 The contents of the memory is shown below to the left. The next state transition table is shown below to the right. Address Content A[2:0] D[1:0] 000 11 001 10 010 01 011 10 100 01 101 00 110 00 111 01Current State Next State S[2:0] D[1:0] D[1:0] D[1:0] D[1:0] 00 01 10 11 000 001 010 110 100 001 100 000 011 110 010 010 100 111 010 011 001 100 100 010 100 110 011 011 111 101 100 010 100 110 110 001 110 100 010 111 000 101 111 101 The output Z0, Z1, Z2 is the current state of the ﬁnite state machine. That is, Z0=S0, Z1=S1, Z2=S2. The cycle time of the ﬁnite state machine is long enough so that during a single cycle, the following happens: The output of the ﬁnite state machine accesses the memory, and the values supplied by the memory are input to the combinational logic, which determines the next state of the machine. a.Complete the following table. Cycles State Data Cycle 0 000 11 Cycle 1 Cycle 2 Cycle 3 b.What will the state of the FSM be just before the end of cycle 100? Why?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "reader = pipeline(\n",
    "  tokenizer='roberta-large',\n",
    "  model='roberta-large',\n",
    "  task='question-answering',\n",
    "  device='cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "question=\"What is a programmable logic array (PLA)?\"\n",
    "for doc in relevant_context_list:\n",
    "  answer = reader(question=question, context=doc.page_content)\n",
    "  print(answer)\n",
    "  print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    separator='. ', \n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3793 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1597, 30587,  ...,  2530,     5,     1]])\n"
     ]
    }
   ],
   "source": [
    "# todo: check to see if chunks are less than chunk_size\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-xl')\n",
    "tokens = tokenizer.encode(texts[22], return_tensors='pt')\n",
    "print(tokens)\n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flan_xl_text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(AutoTokenizer.from_pretrained('google/flan-t5-xl'), chunk_size=300, chunk_overlap=0)\n",
    "texts = flan_xl_text_splitter.split_text(\" \".join(textbook))\n",
    "# print(texts[0])\n",
    "# print(len(texts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8513702ffebcddd0565c7cb8940121422c1007cb2eaee71f9f7918f25ee15d0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
