{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('GPT-3_paragraphs.json')\n",
    "paragraphs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what was the problem that Turing proved undecidable?'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"voidful/bart-eqg-question-generator\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"voidful/bart-eqg-question-generator\")\n",
    "\n",
    "def get_question(context, max_length=1024):\n",
    "  input_text = context \n",
    "  features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "  output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'],\n",
    "               max_length=max_length)\n",
    "\n",
    "  return tokenizer.decode(output[0])\n",
    "\n",
    "context = \"In the same 1936 paper in which he introduced the universal computing\\nmachine, Alan Turing also provided an answer to this question\\nby introducing (and proving) that there are in fact problems that cannot be\\ncomputed by a universal computing machine.\\nThe problem that\\nhe proved undecidable, using proof techniques almost identical to those\\ndeveloped for similar problems in the 1880s, is now known as { the\\nhalting problem}.\"\n",
    "\n",
    "question = get_question(context).strip('</s>')\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alan Turing'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ainize/klue-bert-base-mrc\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"ainize/klue-bert-base-mrc\")\n",
    "\n",
    "encodings = tokenizer(context, question, max_length=512, truncation=True,\n",
    "                      padding=\"max_length\", return_token_type_ids=False)\n",
    "encodings = {key: torch.tensor([val]) for key, val in encodings.items()}             \n",
    "\n",
    "input_ids = encodings[\"input_ids\"]\n",
    "attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "pred = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "start_logits, end_logits = pred.start_logits, pred.end_logits\n",
    "\n",
    "token_start_index, token_end_index = start_logits.argmax(dim=-1), end_logits.argmax(dim=-1)\n",
    "\n",
    "pred_ids = input_ids[0][token_start_index: token_end_index + 1]\n",
    "\n",
    "prediction = tokenizer.decode(pred_ids)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ainze(context, question):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ainize/klue-bert-base-mrc\")\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\"ainize/klue-bert-base-mrc\")\n",
    "\n",
    "    encodings = tokenizer(context, question, max_length=512, truncation=True,\n",
    "                        padding=\"max_length\", return_token_type_ids=False)\n",
    "    encodings = {key: torch.tensor([val]) for key, val in encodings.items()}             \n",
    "\n",
    "    input_ids = encodings[\"input_ids\"]\n",
    "    attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "    pred = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    start_logits, end_logits = pred.start_logits, pred.end_logits\n",
    "\n",
    "    token_start_index, token_end_index = start_logits.argmax(dim=-1), end_logits.argmax(dim=-1)\n",
    "\n",
    "    pred_ids = input_ids[0][token_start_index: token_end_index + 1]\n",
    "\n",
    "    prediction = tokenizer.decode(pred_ids)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1936 paper in which he introduced the universal computing machine, Alan Turing also provided an answer to this question by introducing ( and proving ) that there are in fact problems that cannot be computed by a universal computing machine. The problem that he proved undecidable, using proof techniques almost identical to those developed for similar problems in the 1880s'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Transformers library\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "# Load fine-tuned MRC model by HuggingFace Model Hub\n",
    "HUGGINGFACE_MODEL_PATH = \"bespin-global/klue-bert-base-mrc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL_PATH )\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(HUGGINGFACE_MODEL_PATH )\n",
    "\n",
    "# Encoding\n",
    "encodings = tokenizer(context, question, \n",
    "                      max_length=512, \n",
    "                      truncation=True,\n",
    "                      padding=\"max_length\", \n",
    "                      return_token_type_ids=False\n",
    "                      )\n",
    "encodings = {key: torch.tensor([val]) for key, val in encodings.items()}             \n",
    "input_ids = encodings[\"input_ids\"]\n",
    "attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "# Predict\n",
    "pred = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "start_logits, end_logits = pred.start_logits, pred.end_logits\n",
    "token_start_index, token_end_index = start_logits.argmax(dim=-1), end_logits.argmax(dim=-1)\n",
    "pred_ids = input_ids[0][token_start_index: token_end_index + 1]\n",
    "\n",
    "# Decoding\n",
    "prediction = tokenizer.decode(pred_ids)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bespin(context, question):\n",
    "    HUGGINGFACE_MODEL_PATH = \"bespin-global/klue-bert-base-mrc\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL_PATH )\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(HUGGINGFACE_MODEL_PATH )\n",
    "\n",
    "    # Encoding\n",
    "    encodings = tokenizer(context, question, \n",
    "                        max_length=512, \n",
    "                        truncation=True,\n",
    "                        padding=\"max_length\", \n",
    "                        return_token_type_ids=False\n",
    "                        )\n",
    "    encodings = {key: torch.tensor([val]) for key, val in encodings.items()}             \n",
    "    input_ids = encodings[\"input_ids\"]\n",
    "    attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "    # Predict\n",
    "    pred = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    start_logits, end_logits = pred.start_logits, pred.end_logits\n",
    "    token_start_index, token_end_index = start_logits.argmax(dim=-1), end_logits.argmax(dim=-1)\n",
    "    pred_ids = input_ids[0][token_start_index: token_end_index + 1]\n",
    "\n",
    "    # Decoding\n",
    "    prediction = tokenizer.decode(pred_ids)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bespin_data = []\n",
    "ainze_data = []\n",
    "for row in paragraphs:\n",
    "    context = row['positive_ctxs']['text']\n",
    "    question = get_question(context).strip('</s>')\n",
    "\n",
    "    b = bespin(context, question)\n",
    "    a = ainze(context, question)\n",
    "\n",
    "    data = {}\n",
    "    data['positive_ctxs'] = {}\n",
    "    data['positive_ctxs']['text'] = context\n",
    "    data['positive_ctxs']['title'] = row['positive_ctxs']['title']\n",
    "    data['quesiton'] = question\n",
    "    data['answer'] = a\n",
    "    ainze_data.append(data)\n",
    "\n",
    "    datab = {}\n",
    "    datab['positive_ctxs'] = {}\n",
    "    datab['positive_ctxs']['text'] = context\n",
    "    datab['positive_ctxs']['title'] = row['positive_ctxs']['title']\n",
    "    datab['quesiton'] = question\n",
    "    datab['answer'] = b\n",
    "    bespin_data.append(datab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('bespin.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(bespin_data, f, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('ainze.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(ainze_data, f, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28596858067060ee176f7bab50a17c769bfd8a96306468e7ae0695529617abaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
