{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "#import transformers\n",
    "#from transformers import GPT2Tokenizer\n",
    "import backoff\n",
    "from openai.error import RateLimitError\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "        return infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def gpt3_embedding(content, model='text-similarity-ada-001'):\n",
    "    try:\n",
    "        response = openai.Embedding.create(input=content, engine=model)\n",
    "    except openai.error.APIConnectionError:\n",
    "        print(\"Failed\") \n",
    "    return response['data'][0]['embedding']  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API response for any prompt - change settings : temperature to be low, best_of=5, n=3 -> captures completions response\n",
    "@backoff.on_exception(backoff.expo, RateLimitError)\n",
    "def response_API(prompt, myKwargs = {}):\n",
    "    \n",
    "    kwargs = {\"model\" :\"text-davinci-002\",\n",
    "            \"temperature\" :0.46,\n",
    "            \"max_tokens\": 600,\n",
    "            \"frequency_penalty\":1,\n",
    "            \"presence_penalty\":0}\n",
    "    #assign changed values and keep everything else the same\n",
    "    for kwarg in myKwargs:\n",
    "        kwargs[kwarg] = myKwargs[kwarg]\n",
    "        \n",
    "    try:\n",
    "        response = openai.Completion.create(prompt=prompt, **kwargs)\n",
    "    except openai.error.APIConnectionError:\n",
    "        print(\"Failed\")\n",
    "    \n",
    "    return response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_completions_with_backoff(passages): \n",
    "       \n",
    "    question_prompts = ['''Generate one multiple-choice type question about this context. The question should consist of reasoning and procedural steps including all the possiblities of the context. \\n\n",
    "                        The question should be precise and factual, and should address the real world curiosity of the topic.''',\n",
    "                        \n",
    "                        '''Generate one objective, to-the-point and firm question about this context. The question should break down the context and specifically pin-point key terminologies and concepts. \\n\n",
    "                        It should be precise, factual and truthful. The question should identify the important terms and accurately summarize them.''' ,\n",
    "                        \n",
    "                        '''Generate one thoughtful, creative, steps-based procedural question about this context that starts with Why/How/Where/Who/When. \\n\n",
    "                        It should be detailed, free-flowing, conversational and analytical. The question should spur curiority and inspire discussions and debates.''' ]\n",
    "    \n",
    "    n=len(question_prompts)\n",
    "    questions = []\n",
    "    qprompts = []\n",
    "    for p in passages:\n",
    "        for j in question_prompts:\n",
    "                #prompt_tokens = calculate_tokens(j)\n",
    "                #context_tokens = calculate_tokens(p)\n",
    "                #max_tokens = 300\n",
    "                \n",
    "                #while(max_tokens+prompt_tokens+context_tokens < 4096):\n",
    "                prompt= \"%s \\n Context : %s\" % (j, p)\n",
    "                #print(prompt)\n",
    "    \n",
    "                response = response_API(prompt)\n",
    "                questions.append(response)\n",
    "                qprompts.append(j)\n",
    "                print(\"Done\")\n",
    "                      \n",
    "    question_list = [questions[i:i + n] for i in range(0, len(questions), n)]\n",
    "    \n",
    "    return question_list, qprompts #for each prompt there is one question generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_completions_with_backoff(question_list, passages):\n",
    "    \n",
    "    answer_prompt = \"\"\" Generate an objective, free flowing, conversational and formal answer about this question, based on the given context. \n",
    "    The answer must spur curiosity and enable interactive discussions. It should explain the concept of the context accurately, and be truthful and factual.\n",
    "    It should be interested and use advanced vocabulary and complex sentence structures. Generate the answer now.\"\"\"\n",
    "    \n",
    "    \n",
    "    answers = []\n",
    "    QA_pairs = []\n",
    "    QA_data_full = []\n",
    "    \n",
    "    for index, par in enumerate(passages):\n",
    "        for i, q in enumerate(question_list[index]):\n",
    "            \n",
    "            ans_prompt = \"Context : %s \\n Question : %s \\n Prompt : %s\" % (par, q, answer_prompt)\n",
    "            \n",
    "            ans_response = response_API(ans_prompt)\n",
    "            print(\"Done\")\n",
    "            answers.append(ans_response)\n",
    "            qa_dict = {\"question\" : q, \"answer\" : ans_response}\n",
    "            \n",
    "            QA_pairs.append(qa_dict)\n",
    "            \n",
    "            data = {}\n",
    "            data['textbook-paragraph'] = par\n",
    "            data['GPT-3-Generations'] = {}\n",
    "            data['GPT-3-Generations']['question'] = q\n",
    "            data['GPT-3-Generations']['answer'] = ans_response   \n",
    "            QA_data_full.append(data)\n",
    "            \n",
    "    \n",
    "    return answers, QA_pairs, QA_data_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sections data\n",
    "s = open(\"/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/gpt-3/GPT-3_section_level.json\")\n",
    "sections_data = json.load(s)\n",
    "\n",
    "\"\"\" #full textbook embeddings - vectors\n",
    "with open(\"/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/prompt_engineering/index.json\") as input_file:\n",
    "    data = json.load(input_file) \"\"\"\n",
    "    \n",
    "s = open(\"/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/split_textbook/paragraphs.json\")\n",
    "paragraphs_data = json.load(s)\n",
    "paragraphs_list = list(paragraphs_data.values())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of only texts from the json file\n",
    "sections_list = []\n",
    "for p, item in enumerate(sections_data):\n",
    "    subtext = item['positive_ctxs']['text']\n",
    "    sections_list.append(subtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating questions \n",
    "question_list, qprompts = question_completions_with_backoff(sections_list[0:2]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers, QA_pairs, QA_data_full = answer_completions_with_backoff(question_list, sections_list[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ##OUTPUT FORMAT NEEDED - \n",
    "  { \n",
    "    \"textbook_paragraph\": \"For some of the....\",        \n",
    "    \"GPT-3-Davinci-Generations\": {\n",
    "      \"prompt_0\": {\n",
    "        \"prompt\": \"Please write a why question about the above paragrpah.\",\n",
    "        \"completions\": [\n",
    "          {\n",
    "            \"question\": \"Generated question 1\",\n",
    "            \"answer\": \"Generated answer 1\",\n",
    "            \"QA_quality_score\": 0.5,\n",
    "            \"QA_ranking\": 69\n",
    "          },\n",
    "          {\n",
    "            \"question\": \"Generated question 1\",\n",
    "            \"answer\": \"Generated answer 1\",\n",
    "            \"QA_quality_score\": 0.5,\n",
    "            \"QA_ranking\": 69\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"prompt_1\": {\n",
    "        \"prompt\": \"Please write a what/when/where question about the above paragrpah.\",\n",
    "        \"completions\": [\n",
    "          {\n",
    "            \"question\": \"Generated question 1\",\n",
    "            \"answer\": \"Generated answer 1\",\n",
    "            \"QA_quality_score\": 0.5,\n",
    "            \"QA_ranking\": 69\n",
    "          },\n",
    "          {\n",
    "            \"question\": \"Generated question 1\",\n",
    "            \"answer\": \"Generated answer 1\",\n",
    "            \"QA_quality_score\": 0.5,\n",
    "            \"QA_ranking\": 69\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GPT-3_generations_new.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(QA_data_full, f, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/prompt_engineering'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "s= open(\"/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/prompt_engineering/GPT-3_generations_new.json\")\n",
    "qa = json.load(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paragraph level generations - approx upto 100 words each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paragraph_questions, paragraph_qprompts = question_completions_with_backoff(paragraphs_list[0:500]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_answers, paragraph_QA_pairs, paragraph_QA_data_full = answer_completions_with_backoff(paragraph_questions, paragraphs_list[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GPT-3_generations_paragraph_level.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(paragraph_QA_data_full, f, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7d114a4298214147e5de026dbeaca42830d8ddf5eb827aba9347105a0e910fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
