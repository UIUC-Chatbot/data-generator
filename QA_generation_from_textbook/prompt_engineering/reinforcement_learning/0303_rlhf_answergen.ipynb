{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "import backoff\n",
    "from openai.error import RateLimitError\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "import os\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full textbook embeddings - vectors\n",
    "with open(\"/home/nehasheth/chatbotai/neha/data-generator/QA_generation_from_textbook/prompt_engineering/reinforcement_learning/index_pateltextbook.json\") as input_file:\n",
    "    index_data = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_embedding(content, model='text-embedding-ada-002'):\n",
    "    try:\n",
    "        response = openai.Embedding.create(input=content, engine=model)\n",
    "    except openai.error.APIConnectionError:\n",
    "        print(\"Failed\") \n",
    "    return response['data'][0]['embedding'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute cosine similarity\n",
    "def get_similarity(v1, v2):\n",
    "    cosine = np.dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching through textbook \n",
    "def search_index(query, index_data, count=1):\n",
    "    question_vector = gpt3_embedding(query)\n",
    "    scores = []\n",
    "    for i in index_data:\n",
    "        score = get_similarity(question_vector, i['vector'])\n",
    "        scores.append({'content' : i['content'], 'score' : score})\n",
    "    most_relevant= sorted(scores, key=lambda d: d['score'], reverse=True)\n",
    "    return most_relevant[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_API(prompt, myKwargs = {}):\n",
    "\n",
    "  #default arguments to send the API, unless changed in function\n",
    "  kwargs = {\"model\" :\"gpt-3.5-turbo\",\n",
    "            \"temperature\" :0.6,\n",
    "            \"max_tokens\": 500,\n",
    "            \"frequency_penalty\":1,\n",
    "            \"presence_penalty\":0}\n",
    "\n",
    "\n",
    "  for kwarg in myKwargs:\n",
    "    kwargs[kwarg] = myKwargs[kwarg]\n",
    "  \n",
    "  r=openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a teaching assistant answering student questions in a concise way, the students are college freshmen.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "            ])\n",
    "  return r['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(fin_question):\n",
    "    \n",
    "    results = search_index(fin_question, index_data) #get top 3 relevant contexts.\n",
    "\n",
    "    fin_answers = []\n",
    "\n",
    "    prompt1 = \"Context : %s %s Answer this question based on the above context. The answer should have a university freshmen-level language and be very concise and to-the-point. Answer: \" % (results[0], fin_question)\n",
    "    response1 = response_API(prompt1)\n",
    "    fin_answers.append(response1)\n",
    "\n",
    "    prompt2 = \"Context : %s %s Answer this question based on the above context. The answer should have a university freshmen-level language and be very concise and to-the-point. Answer: \" % (results[1], fin_question)\n",
    "    response2 = response_API(prompt2)\n",
    "    fin_answers.append(response2)\n",
    "\n",
    "    #gpt answer\n",
    "    prompt3 = \"Answer this question. The answer should have a university freshmen-level language and be very concise and to-the-point. Question:%s Answer: \" % fin_question\n",
    "    response3 = response_API(prompt3)\n",
    "    fin_answers.append(response3)\n",
    "\n",
    "    return fin_answers #returns a list. 1Q - 3A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding patel textbook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages:  801\n"
     ]
    }
   ],
   "source": [
    "# reader = PdfReader(\"../raw_data/notes/Student_Notes.pdf\")\n",
    "reader = PdfReader(\"/home/nehasheth/chatbotai/neha/non-public-datasets/raw_data/patel_textbook/Yale Patt - Introduction to Computing Systems_ From Bits & Gates to C & Beyond.pdf\")\n",
    "print(\"Total pages: \", len(reader.pages))\n",
    " \n",
    "# extracting text from page\n",
    "textbook = []\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text().replace(\"\\n\", \" \")\n",
    "    # skip empty pages\n",
    "    if text:\n",
    "        textbook.append(dict(\n",
    "                            text=text,\n",
    "                            page_number=i, \n",
    "                            textbook_name=\"Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_textbook = \"\"\n",
    "for i, content in enumerate(textbook[25:281]):\n",
    "    full_textbook+=content[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"patel_textbook_chap7.txt\", \"wt\")\n",
    "n = text_file.write(full_textbook)\n",
    "text_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get questions\n",
    "with open('questions_set1.pkl', 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    q1 = pickle.load(file)\n",
    "\n",
    "with open('questions_set2.pkl', 'rb') as file:\n",
    "    # Load the data from the pickle file\n",
    "    q2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = [q.split('. ', 1)[1] for q in q1] #remove all numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = q1 + q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting the data\n",
    "qa_data = []\n",
    "answer_list=[]\n",
    "for j, ques in enumerate(questions):\n",
    "        print(j)\n",
    "        print(ques)\n",
    "        data = {}\n",
    "        data['GPT-3-RLHF-Generations'] = {}\n",
    "        data['GPT-3-RLHF-Generations']['question'] = ques\n",
    "        answers = []\n",
    "        answers = get_answers(ques)\n",
    "        data['GPT-3-RLHF-Generations']['answers'] = answers\n",
    "        print(\"done\")\n",
    "        qa_data.append(data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RLHF_Keywords_Set1.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(qa_data, f, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/project/chatbotai/neha/data-generator/QA_generation_from_textbook/prompt_engineering/reinforcement_learning'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7d114a4298214147e5de026dbeaca42830d8ddf5eb827aba9347105a0e910fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
