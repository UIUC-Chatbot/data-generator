{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "import backoff\n",
    "from openai.error import RateLimitError\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = open(\"/Users/nehasheth/Desktop/Research/Research - AI Chatbot TA/github/data-generator/QA_generation_from_textbook/gpt-3/GPT-3_section_level.json\")\n",
    "sections_data = json.load(s)\n",
    "\n",
    "#full textbook embeddings - vectors\n",
    "with open(\"/Users/nehasheth/Desktop/Research/Research - AI Chatbot TA/github/data-generator/QA_generation_from_textbook/reinforcement_learning/index.json\") as input_file:\n",
    "    index_data = json.load(input_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sections_data) #144 sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_data[16] #16 !! till 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_embedding(content, model='text-embedding-ada-002'):\n",
    "    try:\n",
    "        response = openai.Embedding.create(input=content, engine=model)\n",
    "    except openai.error.APIConnectionError:\n",
    "        print(\"Failed\") \n",
    "    return response['data'][0]['embedding'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, RateLimitError)\n",
    "def response_API(prompt, myKwargs = {}):\n",
    "\n",
    "  #default arguments to send the API, unless changed in function\n",
    "  kwargs = {\"model\" :\"text-davinci-003\",\n",
    "            \"temperature\" :0.6,\n",
    "            \"max_tokens\": 500,\n",
    "            \"frequency_penalty\":1,\n",
    "            \"presence_penalty\":0}\n",
    "\n",
    "\n",
    "  for kwarg in myKwargs:\n",
    "    kwargs[kwarg] = myKwargs[kwarg]\n",
    "\n",
    "  r = openai.Completion.create(prompt=prompt, **kwargs)\n",
    "  return r['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute cosine similarity\n",
    "def get_similarity(v1, v2):\n",
    "    cosine = np.dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching through textbook \n",
    "def search_index(query, index_data, count=1):\n",
    "    question_vector = gpt3_embedding(query)\n",
    "    scores = []\n",
    "    for i in index_data:\n",
    "        score = get_similarity(question_vector, i['vector'])\n",
    "        scores.append({'content' : i['content'], 'score' : score})\n",
    "    most_relevant= sorted(scores, key=lambda d: d['score'], reverse=True)\n",
    "    return most_relevant[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(fin_question, section):\n",
    "    \n",
    "    results = search_index(fin_question, index_data) #get top 3 relevant contexts.\n",
    "\n",
    "    fin_answers = []\n",
    "\n",
    "    prompt1 = \"Context : %s %s Answer this question based on the above context. The answer should have a university freshmen-level language and be very concise and to-the-point. Answer: \" % (results[0], fin_question)\n",
    "    response1 = response_API(prompt1)\n",
    "    fin_answers.append(response1)\n",
    "\n",
    "    prompt2 = \"Context : %s %s Answer this question based on the above context. The answer should have a university freshmen-level language and be very concise and to-the-point. Answer: \" % (results[1], fin_question)\n",
    "    response2 = response_API(prompt2)\n",
    "    fin_answers.append(response2)\n",
    "\n",
    "    prompt3 = \"Context : %s %s Answer this question based on the above context. The answer should have a university freshmen-level language and be very concise and to-the-point. Answer: \" % (results[2], fin_question)\n",
    "    response3 = response_API(prompt3)\n",
    "    fin_answers.append(response3)\n",
    "\n",
    "    return fin_answers #returns a list. 1Q - 3A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_completions_with_backoff(passages): \n",
    "       \n",
    "    question_prompts = [\n",
    "    '''Generate exactly 2 objective, to-the-point and firm questions about this context. \n",
    "    The questions must specify the concept discussed in the context and be complete on its own. The questions should be different from one another. \n",
    "    Begin each question with a “[Q]” sign.''']\n",
    "    \n",
    "    n=len(question_prompts)\n",
    "    questions = []\n",
    "    for p in passages:\n",
    "        for j in question_prompts:\n",
    "                #prompt_tokens = calculate_tokens(j)\n",
    "                #context_tokens = calculate_tokens(p)\n",
    "                #max_tokens = 300\n",
    "                \n",
    "                #while(max_tokens+prompt_tokens+context_tokens < 4096):\n",
    "                prompt= \"%s \\n %s\" % (j, p)\n",
    "    \n",
    "                response = response_API(prompt)\n",
    "                \n",
    "                questions.append(response)\n",
    "                print(response)\n",
    "                      \n",
    "    question_list = [questions[i:i + n] for i in range(0, len(questions), n)]\n",
    "    \n",
    "    return question_list "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_list = []\n",
    "for p, item in enumerate(sections_data[17:]):\n",
    "    subtext = item['positive_ctxs']['text']\n",
    "    sections_list.append(subtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question_list = question_completions_with_backoff(sections_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually cleaned \n",
    "file1 = open(\"/Users/nehasheth/Desktop/Research/Research - AI Chatbot TA/github/data-generator/QA_generation_from_textbook/reinforcement_learning/GPT-3_questions_cleaned.json\")\n",
    "questions_manual = json.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "questions = questions_manual\n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "sections_list1 = sections_list[50:]\n",
    "print(len(sections_list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions1 = questions[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting the data\n",
    "qa_data = []\n",
    "answer_list=[]\n",
    "for i, section in enumerate(sections_list1):\n",
    "    for j, ques in enumerate(questions1[i]):\n",
    "        #print(i)\n",
    "        #print(section)\n",
    "        #print(ques)\n",
    "        data = {}\n",
    "        data['textbook-paragraph'] = section\n",
    "        data['GPT-3-RLHF-Generations'] = {}\n",
    "        data['GPT-3-RLHF-Generations']['question'] = ques\n",
    "        answers = []\n",
    "        match = re.search(r'{([^}]*)}', section)\n",
    "        if match:\n",
    "            sec_title = match.group(1)\n",
    "        fin_question =  f\"Topic : {match.group(0)} Question : {ques} \"\n",
    "        answers = get_answers(fin_question, section)\n",
    "        data['GPT-3-RLHF-Generations']['answers'] = answers\n",
    "        print(\"done\")\n",
    "        qa_data.append(data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RLHF_Version3_set2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(qa_data, f, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
