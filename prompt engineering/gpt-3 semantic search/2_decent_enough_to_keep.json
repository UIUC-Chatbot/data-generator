[
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What are the concrete aspects of our first few examples?\\n",
      "answer": "\nThe first few examples in the passage are of two-level logic, DeMorgan's law, and SOP form. Two-level logic is a popular way of expressing logic functions, and DeMorgan's law is a way to simplify expressions. SOP form is a way of representing a logic function."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": "How does the choice of representation for the FSM's internal state affect the complexity of the implementation?\", '\\n",
      "answer": "\nThe choice of representation for the FSM's internal state affects the complexity of the implementation in terms of the number of states, the number of inputs, the number of outputs, and the number of transition rules."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What are the concrete aspects of the design process?\\n",
      "answer": "\nThe design process involves the manipulation of logic expressions in order to optimize them for specific metrics, such as cost, power, or performance. Today, this manipulation is performed almost entirely by computers, with human engineers supplying the logic functions of interest and imposing constraints on the process."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is an abstract model?\\n",
      "answer": "\nAn abstract model is a mathematical model that is used to describe a system. It is usually simplified and does not include all the details of the system."
    }
  },
  {
    "textbook-paragraph": "level state, to simplify the implementation.} {Use counters to insert time-based transitions between states (such  as timeouts).} {Implement an FSM using logic components such as registers,  counters, comparators, and adders as building blocks.} {Explain the basic organization of a computer's microarchitecture as well as the role played by elements of a von Neumann design in the processing of instructions.} {Identify the stages of processing an instruction (such as fetch, decode, getting operands, execution, and writing back results) in a  processor control unit state machine diagram.}   And, at the highest level, we expect that you will be able to do the following:  {}{{}{} {}{}{}  {Explain the difference between the Moore and Mealy machine models,  as well as why you might find each of them useful when designing an FSM.} {Understand the need for initialization of an FSM, be able to analyze  and identify potential problems arising from lack of initialization, and  be able to extend an implementation to include initialization to an  appropriate state when necessary.} {Understand how the choice of internal state bits for an FSM can  affect the complexity of the implementation of next-state and output  logic, and be able to select a reasonable state assignment.} {Identify and fix design flaws in simple FSMs by analyzing an existing  implementation, comparing it with the specification, and removing any  differences by making any necessary changes to the implementation.}      {   }  empty 3rd page      {Instruction Set Architecture*}  This set of notes discusses  tradeoffs and design elements of instruction set architectures (ISAs). { The material is beyond the scope of our class, and is provided purely for your interest.}  Those who find these topics interesting may also want to read the ECE391 notes, which describe similar material with a focus on the x86 ISA.  As you know, the ISA defines the interface between software and hardware, abstracting the capabilities of a computer's datapath and standardizing the format of instructions to utilize those capabilities.  Successful ISAs are rarely discarded, as success implies the existence of large amounts of software built to use the ISA.  Rather, they are extended, and their original forms must be supported for decades (consider, for example, the IBM 360 and the Intel x86).  Employing sound design principles is thus imperative in an ISA.  {Formats and Fields*}  The LC-3 ISA  employs fixed-length instructions and a load-store architecture, two aspects that help to reduce the design space to a manageable set of choices.  In a general ISA design, many other options exist for instruction formats.  Recall the idea of separating the bits of an instruction into (possibly non-contiguous) fields.  One of the fields must contain an opcode, which specifies the type of operation to be performed by the instruction.  In the {LC-3} ISA, most opcodes specify both the type of operation and the types of arguments to the operation.   More generally, many addressing modes are possible for each operand, and we can think of the bits that specify the addressing mode as a separate field, known as the { mode} field.    As a simple example, the {LC-3's} ADD and AND instructions  contain a {1-bit} mode field that specifies whether the second operand of the ADD/AND comes from a register or is an immediate value.  Several questions must be answered in order to define the possible instruction formats for an ISA.  First, are instructions fixed-length or variable-length?  Second, how many addresses are needed for each instruction, and how many of the addresses can be memory addresses? Finally, what forms of addresses are possible for each operand? For example, can one use full memory addresses or only limited offsets relative to a register?  The answer to the first question depends on many factors, but several clear advantages exist for both answers.  { Fixed-length instructions} are easy to fetch and decode.  A",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is the need for FSM initialization?\\n",
      "answer": "\nAn FSM must be initialized to ensure that it starts in a known state. This allows the FSM to function correctly and avoid any potential problems that could arise from an unknown starting state."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How is a digital FSM designed from an abstract model?\\n",
      "answer": "\nFrom the passage, it seems that a digital FSM is designed from an abstract model by first identifying a finite set of states, a set of possible inputs, a set of possible outputs, and a set of transition rules. Then, the FSM is implemented as a digital system by representing all states as patterns using a fixed number of bits, translating all inputs into bits, and translating all outputs into bits. Finally, transition rules are defined so that given any state of the FSM and any pattern of input bits, a transition is defined from that state to another state."
    }
  },
  {
    "textbook-paragraph": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the design process for a counter with a number of states that is not a power of two?\\n",
      "answer": "\nThe design process for a counter with a number of states that is not a power of two is to choose a representation for the states that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How does the choice of representation for the FSM's internal state affect the complexity of the implementation?\"]",
      "answer": "\nThe choice of representation for the FSM's internal state affects the complexity of the implementation. If the states are represented as bit patterns, the transition rules must be complete. If the states are represented as abstract states, the outputs for each state must be specified."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How does the choice of state representation affect the implementation of an FSM?\\n",
      "answer": "\nThe choice of state representation affects the implementation of an FSM in terms of the number of gates and inverters required, as well as the delay of the overall design. A serial comparator design based on the 2-bit slice variant, for example, requires 24 gates and 10 inverters, which is not much larger than the earlier serial design. In terms of gate delays, however, the new design is identical, meaning that comparisons are finished in half the time. More realistic area and timing metrics show slightly more difference between the two designs. These differences can dominate the results if the design is not carefully considered."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " Why is it important for the components of a digital system to be compatible?\"]",
      "answer": "\nOne important reason for the compatibility of components in a digital system is that it helps to ensure that the system will function correctly. If the components are not compatible, the system may not work correctly, or may not work at all. Additionally, compatibility among components can help to reduce the cost of manufacturing the system, as well as the cost of maintaining and repairing the system."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the inputs to a Gray code counter?\\n",
      "answer": "\nThe inputs to a Gray code counter are the current state of the counter and the clock signal. The clock signal triggers the state change of the counter."
    }
  },
  {
    "textbook-paragraph": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the inputs to the counter?\\n",
      "answer": "\nThe inputs to the counter are the coins that are inserted into the machine. The machine will track the value of the coins in terms of nickels and will reject any coins that are inserted that would cause the value to exceed the maximum value that the machine can track."
    }
  },
  {
    "textbook-paragraph": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the difference between a Gray code counter and a regular counter?\\n",
      "answer": "\nA Gray code counter is a type of counter that uses a Gray code, which is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit. A regular counter is a type of counter that does not use a Gray code."
    }
  }
]