{
    "0": "This set of notes uses a series of examples to illustrate design principles \nfor the implementation of finite state machines (FSMs) using digital logic.\nWe begin with an overview of the design process for a digital FSM, from\nthe development of an abstract model through the implementation of\nfunctions for the next-state variables and output signals.\nOur first few examples cover only the concrete aspects:\nwe implement several counters, which illustrate the basic \nprocess of translating a concrete and complete state transition diagram\ninto an implementation based on flip-flops and logic gates.\nWe next consider a counter with a number of states that is not a power of\ntwo, with which we illustrate the need for FSM initialization.\n As part of solving the initialization problem, we also introduce \n a general form of selection logic called a multiplexer.",
    "1": "We then consider the design process as a whole through a more general\nexample of a counter with multiple inputs to control its behavior. \nWe work from\nan abstract model down to an implementation, illustrating how semantic\nknowledge from the abstract model can be used to simplify the \nimplementation.  Finally, we illustrate how the choice of representation\nfor the FSM's internal state affects the complexity of the implementation.\nFortunately, designs that are more intuitive and easier for humans to\nunderstand also typically make the best designs in terms of \nother metrics, such as logic complexity.",
    "2": "Before we begin exploring designs, let's talk briefly about the general\napproach that we take when designing an FSM.  We follow a six-step\nprocess:{-8pt}",
    "3": "{{}{}\n{}{}{}\n{develop an abstract model}{step-abs}\n{specify I/O behavior}{step-io}\n{complete the specification}{step-complete}\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n}\n{-8pt}",
    "4": "In Step {step-abs}, we translate our description in human language\ninto a model with states and desired behavior.  At this stage, we \nsimply try to capture the intent of the description and are not\nparticularly thorough nor exact.",
    "5": "Step {step-io} begins to formalize the model, starting with its\ninput and output behavior.  If we eventually plan to develop an\nimplementation of our FSM as a digital system (which is not the \nonly choice, of course!), all input and output\nmust consist of bits.  Often, input and/or output specifications\nmay need to match other digital systems to which we plan to connect\nour FSM.  In fact, { most problems in developing large digital systems\ntoday arise because of incompatibilities when composing two or more\nseparately designed pieces} (or { modules}) into an integrated system.",
    "6": "Once we know the I/O behavior for our FSM, in Step {step-complete}\nwe start to make\nany implicit assumptions clear and to make any other decisions\nnecessary to the design.  Occasionally, we may choose to leave\nsomething undecided in the hope of simplifying the design with\n``don't care'' entries in the logic formulation.",
    "7": "In Step {step-repn}, we select an internal representation\nfor the bits necessary to encode the state of our FSM.  In practice,\nfor small designs, this representation can be selected by a computer \nin such a way as to optimize the implementation.  However, for large\ndesigns, such as the LC-3 instruction set architecture that we\nstudy later in this class, humans do most of the work by hand.",
    "8": "In the later examples in this set of notes, we show how even a \nsmall design can\nleverage meaningful information from the design when selecting\nthe representation, leading to an implementation that is simpler\nand is easier to build correctly.",
    "9": "We also show how one can\nuse abstraction to simplify an implementation.",
    "10": "By Step {step-logic}, our design is a complete specification in\nterms of bits, and we need merely derive logic expressions for the\nnext-state variables and the output signals.  This process is no\ndifferent than for combinational logic, and should already be fairly \nfamiliar to you.",
    "11": "Finally, in Step {step-gates}, we translate our logic expressions\ninto gates and use flip-flops (or registers) to hold the internal\nstate bits of the FSM.  In later notes, we use more complex\nbuilding blocks when implementing an FSM, building up abstractions\nin order to simplify the design process in much the same way that\nwe have shown for combinational logic.",
    "12": "Let's begin with a two-bit Gray code counter with no inputs.\nAs we mentioned in Notes Set 2.1, a Gray code is a cycle over all\nbit patterns of a certain length in which consecutive patterns differ\nin exactly one bit.",
    "13": "For simplicity, our first few examples are based on counters and\nuse the internal state\nof the FSM as the output values.  You should already know\nhow to design combinational logic for the outputs if it were necessary.",
    "14": "The inputs to a counter, if any, are typically limited to functions\nsuch as starting and stopping the counter, controlling the counting \ndirection, and resetting the counter to a particular state.",
    "15": "A fully-specified transition diagram for \na two-bit Gray code counter appears below.\nWith no inputs, the states simply form a loop, with\nthe counter moving from one state to the next each cycle.",
    "16": "Each state in the diagram is marked with the internal state value S_1S_0 \n(before the ``/'') and the output Z_1Z_0 (after the ``/''), which are \nalways equal for this counter.",
    "17": "Based on the transition diagram, we can fill in the K-maps for the \nnext-state values S_1^+ and S_0^+ as shown to the right of the\ntransition diagram, then \nderive algebraic expressions in the usual way to obtain\nS_1^+=S_0 and S_0^+={{S_1}}.",
    "18": "We then use the next-state logic to develop the implementation\nshown on the far right, completing our first counter design.",
    "19": "Now we'll add a third bit to our counter, but again use a Gray code\nas the basis for the state sequence.",
    "20": "A fully-specified transition diagram for such a counter appears to \nthe right.  As before, with no inputs, the states simply form a loop, \nwith the counter moving from one state to the next each cycle.",
    "21": "Each state in the diagram is marked with the internal state value S_2S_1S_0 \n(before ``/'') and the output Z_2Z_1Z_0 (after ``/'').",
    "22": "Based on the transition diagram, we can fill in the K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then \nderive algebraic expressions.  The results are more complex this \ntime.",
    "23": "For our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_0 + S_1 {{S_0}} \nS_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}} \nS_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1\n{eqnarray*}",
    "24": "Notice that the equations for S_2^+ and S_1^+ share a common term,\nS_1{{S_0}}.",
    "25": "This design does not allow much choice in developing good equations for\nthe next-state logic, but some designs may enable you to reduce \nthe design complexity by explicitly identifying and making use of \ncommon algebraic terms and sub-expressions for different outputs.\nIn modern design processes, identifying such opportunities is generally\nperformed by a computer program, but it's important to understand\nhow they arise.  Note that the common term becomes a single AND gate\nin the implementation of our counter, as shown to the right.",
    "26": "Looking at the counter's implementation diagram, notice that the vertical\nlines carrying the current state values and their inverses back to the\nnext state\nlogic inputs have been carefully ordered to simplify\nunderstanding the diagram.  In particular, they are ordered from\nleft to right (on the left side of the figure) as \n{{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.\nWhen designing any logic diagram, be sure to make use of a reasonable\norder so as to make it easy for someone (including yourself!) to read \nand check the correctness of the logic.",
    "27": "Early graphics systems used a three-bit red-green-blue (RGB) \nencoding for colors.  The color mapping for such a system is shown to\nthe right.",
    "28": "Imagine that you are charged with creating a counter to drive a light\nthrough a sequence of colors.  The light takes an RGB input as just\ndescribed, and the desired pattern is",
    "29": "{off (black)     yellow     violet     green     blue}",
    "30": "You immediately recognize that you merely need a counter with five\nstates.  How many flip-flops will we need?  At least three, since\n_2 (5)=3.  Given that we need three flip-flops, \nand that the colors we need to produce as",
    "31": "{c|l}\nRGB& color \n000& black\n001& blue\n010& green\n011& cyan\n100& red\n101& violet\n110& yellow\n111& white",
    "32": "outputs are all unique\nbit patterns, we can again choose to use the counter's internal \nstate directly as our output values.",
    "33": "A fully-specified transition diagram for our color sequencer\nappears to the right.  The states again form a loop,\nand are marked with the internal state value S_2S_1S_0 \nand the output RGB.",
    "34": "As before, we can use the transition diagram to fill in K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+, as shown to the right.\nFor each of the three states not included in our transition diagram,\nwe have inserted x's",
    "35": "into the K-maps to indicate ``don't care.'' \nAs you know, we can treat each x as either a 0 or a 1, whichever\nproduces better results (where ``better'' usually means simpler \nequations).  The terms that we have chosen for our algebraic \nequations are illustrated in the K-maps.  The x's within the ellipses\nbecome 1s in the implementation, and the x's outside of the ellipses\nbecome 0s.",
    "36": "For our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}} \nS_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}} \nS_0^+ &=& S_1\n{eqnarray*}",
    "37": "Again our equations for S_2^+ and S_1^+ share a common term,\nwhich becomes a single AND gate in the implementation shown to the\nright.",
    "38": "Let's say that you go the lab and build the implementation above, \nhook it up\nto the light, and turn it on.  Does it work?  Sometimes.\nSometimes it works perfectly, but sometimes\nthe light glows cyan or red briefly first.\nAt other times, the light is an\nunchanging white.",
    "39": "What could be going wrong?",
    "40": "Let's try to understand.  We begin by deriving\nK-maps for the implementation, as shown to the right.  In these\nK-maps, each of the x's in our design has been replaced by either a 0\nor a 1.  These entries are highlighted with green italics.",
    "41": "{file=part3/figs/colS2-bad.eps,width=1.00in}",
    "42": "{file=part3/figs/colS1-bad.eps,width=1.00in}",
    "43": "{file=part3/figs/colS0-bad.eps,width=1.00in}",
    "44": "Now let's imagine what might happen if somehow our FSM got into the\nS_2S_1S_0=111 state.  In such a state, the light would appear white,\nsince RGB=S_2S_1S_0=111.",
    "45": "What happens in the next cycle?",
    "46": "Plugging into the equations or looking into the K-maps gives (of\ncourse) the same answer: the next state is the\nS_2^+S_1^+S_0^+=111 state.\nIn other words, the light stays white indefinitely!",
    "47": "As an exercise, you should check what happens \nif the light is red or cyan.",
    "48": "We can extend the transition diagram that we developed for our design\nwith the extra states possible in the implementation, as shown below.\nAs with the five states in the design, the extra states are named with\nthe color of light that they produce.",
    "49": "{{file=part3/figs/colors-full.eps,width=5.8in}}",
    "50": "Notice that the FSM does not move out of the WHITE state (ever).",
    "51": "You may at this point wonder whether more careful decisions \nin selecting our next-state expressions might address this issue.\nTo some extent, yes.  For example, if we replace the \nS_2S_1 term in the equation for S_2^+ with S_2{{S_0}}, \na decision allowed\nby the ``don't care'' boxes in the K-map for our design,\nthe resulting transition diagram does not suffer from the problem\nthat we've found.",
    "52": "However, even if we do change our implementation slightly, we need\nto address another aspect of the problem:",
    "53": "how can the FSM ever get into the unexpected states?",
    "54": "What is the initial state of the three flip-flops in our implementation?",
    "55": "{ The initial state may not even be 0s and 1s unless we have an \nexplicit mechanism for initialization.}",
    "56": "Initialization can work in two ways.",
    "57": "The first approach makes use of the flip-flop design.\nAs you know, a flip-flop is built from a pair of latches, and\nwe can \nmake use of the internal reset lines on these latches\nto force each flip-flop into the 0 state (or the 1 state) using an\nadditional input.",
    "58": "Alternatively, we can add some extra logic to our design.",
    "59": "Consider adding a few AND gates and a  input\n(active low), as shown in the dashed box in the figure to the right.\nIn this case, when we assert  by setting it to 0,\nthe FSM moves to state 000 in the next cycle, putting it into\nthe BLACK state.  The approach taken here is for clarity; one can\noptimize the design, if desired.  For example, we could simply connect\n as an extra input into the three AND gates on the\nleft rather than adding new ones, with the same effect.",
    "60": "We may sometimes want a more powerful initialization mechanism---one\nthat allows us to force the FSM into any specific state in the next\ncycle.  In such a case, we can add multiplexers to each of our \nflip-flop inputs, allowing us to use the INIT input to choose between\nnormal operation (INIT=0) of the FSM and forcing the FSM into the\nnext state given by I_2I_1I_0 (when INIT=1).",
    "61": "We are now ready to discuss the design process for an FSM from start\nto finish.",
    "62": "For this first abstract FSM example, we build upon something\nthat we have already seen: a two-bit Gray code counter.\nWe now want a counter that allows us to start and stop the",
    "63": "{c|ccc}\nstate&    no input&  halt button& go button \ncounting& counting&      halted& \nhalted&   halted&              & counting",
    "64": "count.",
    "65": "What is the mechanism for stopping and starting?  To\nbegin our design, we could sketch out an abstract next-state\ntable such as the one shown to the right above.  In this form of the table,\nthe first column lists the states, while each of the other columns lists\nstates to which the FSM transitions after a clock cycle for a particular\ninput combination.",
    "66": "The table contains two states, counting and halted, and specifies\nthat the design uses two distinct buttons to move between the\nstates.\nThe table further implies that if the counter is halted,\nthe ``halt'' button has no additional effect, and if the counter\nis counting, the ``go'' button has no additional effect.",
    "67": "A counter with a single counting state, of course, does not provide\nmuch value.  We extend the table with four counting states and four\nhalted states, as shown to the right.  This version of the\ntable also introduces more formal state names, for which these notes \nuse all capital letters.",
    "68": "The upper four states represent uninterrupted counting, in which \nthe counter cycles through these states indefinitely.",
    "69": "A user can stop the counter in any state by pressing the ``halt''\nbutton, causing the counter to retain its current value until the\nuser presses the ``go'' button.",
    "70": "Below the state table is an abstract transition diagram, which provides\nexactly the same information in graphical form.  Here circles represent\nstates (as labeled) and arcs represent transitions from one state\nto another based on an input combination (which is used to label the\narc).",
    "71": "We have already implicitly made a few choices about our counter design.",
    "72": "First, the counter",
    "73": "{c|ccc}\nstate&    no input&  halt button& go button \n{ COUNT A}& { COUNT B}& { HALT A}& \n{ COUNT B}& { COUNT C}& { HALT B}& \n{ COUNT C}& { COUNT D}& { HALT C}& \n{ COUNT D}& { COUNT A}& { HALT D}& \n{ HALT A}&  { HALT A}&              & { COUNT B}\n{ HALT B}&  { HALT B}&              & { COUNT C}\n{ HALT C}&  { HALT C}&              & { COUNT D}\n{ HALT D}&  { HALT D}&              & { COUNT A}",
    "74": "shown retains the current state of the system when\n``halt'' is pressed.\nWe could instead reset the counter state whenever it\nis restarted, in which case we need only five states: four for\ncounting and one more for a halted counter.",
    "75": "Second, we've designed the counter to stop\nwhen the user presses ``halt'' and to resume counting \nwhen the user presses ``go.''  We could instead choose to delay these \neffects by a cycle.  For example, pressing ``halt'' in state { COUNT B}\ncould take the counter to state { HALT C}, and pressing ``go'' \nin state { HALT C} could take the system to state { COUNT C}.",
    "76": "In these notes, we implement only the diagrams shown.",
    "77": "We next start to formalize our design by specifying its input and \noutput behavior digitally.  Each of the two control buttons provides\na single bit of input.  The ``halt'' button we call H, and the\n``go'' button we call G.",
    "78": "For the output, we use a two-bit \nGray code.  With these choices, we can redraw the transition diagram \nas show to the right.",
    "79": "In this figure, the states are marked with output values Z_1Z_0 and\ntransition arcs are labeled in terms of our two input buttons, G and H.  \nThe uninterrupted counting cycle is labeled with \nto indicate that it continues until we press H.",
    "80": "Now we need to think about how the system should behave if something \noutside of our initial expectations occurs.  Having drawn out a partial\ntransition diagram can help with this process, since we can use the\ndiagram to systematically consider all possible input conditions from\nall possible states.  The state table form can make the missing\nparts of the specification even more obvious.",
    "81": "For our counter, the symmetry between counting states makes the problem \nsubstantially simpler.  Let's write out part of a list of states and\npart of a state table with one \ncounting state and one halt state, as shown to the right.\nFour values of the inputs HG \nare possible (recall that N bits allow 2^N possible patterns).\nWe list the columns in Gray code order, since we may want to\ntranscribe this table into K-maps later.",
    "82": "{",
    "83": "& \nfirst counting state& { COUNT A}& counting, output Z_1Z_0=00\n  first halted state&  { HALT A}& halted, output Z_1Z_0=00",
    "84": "{c|cccc}\n&{HG}\n        state&            00&            01&          11&           10 \n{ COUNT A}& { COUNT B}&   unspecified& unspecified& { HALT A}\n { HALT A}&  { HALT A}& { COUNT B}& unspecified&  unspecified",
    "85": "Let's start with the { COUNT A} state.",
    "86": "We know that if neither button is pressed (HG=00), we want \nthe counter to move to the { COUNT B} state.  And, if we press the\n``halt'' button (HG=10), we want the counter to move to the { HALT A}\nstate.  What should happen if a user presses the ``go'' button (HG=01)?\nOr if the user presses both buttons (HG=11)?",
    "87": "Answering these questions is part of fully specifying our design.  We\ncan choose to leave some parts unspecified, but { any implementation of\nour system will imply answers}, and thus we must be careful.",
    "88": "We choose to ignore the ``go'' button while counting, and to have the\n``halt'' button override the ``go'' button.  Thus, if HG=01 when the\ncounter is in state { COUNT A}, the counter moves to state { COUNT B}.\nAnd, if HG=11, the counter moves to state { HALT A}.",
    "89": "Use of explicit bit patterns for the inputs HG may help you to check \nthat all four possible input values are covered from each state.  If \nyou choose to use a transition diagram instead of a state table,\nyou might even want to add four arcs from each state, each labeled \nwith a specific\nvalue of HG.  When two arcs connect the same two states, we can either \nuse multiple labels or can indicate bits that do not matter using a\n{ don't-care} symbol, x.  For example, the arc from state { COUNT A}\nto state { COUNT B} could be labeled HG=00,01 or HG=0x.  The\narc from state { COUNT A} to state { HALT A} could be labeled\nHG=10,11 or HG=1x.  We can also use logical expressions as labels,\nbut such notation can obscure unspecified transitions.",
    "90": "Now consider the state { HALT A}.  The transitions specified so far\nare that when we press ``go'' (HG=01), the counter moves to \nthe { COUNT B} state, and that the counter remains halted in \nstate { HALT A} if no buttons are pressed (HG=00).\nWhat if the ``halt'' button is pressed (HG=10), or\nboth buttons are pressed (HG=11)?  For consistency, we decide that\n``halt'' overrides ``go,'' but does nothing special if it alone is pressed\nwhile the counter is halted.  Thus, input patterns HG=10 and HG=11 also \ntake state { HALT A} back to itself.\nHere the arc could be labeled HG=00,10,11 or, equivalently,\nHG=00,1x or HG=x0,11.",
    "91": "To complete our design, we apply the same decisions that we made for \nthe { COUNT A} state to all of the other counting states, and the \ndecisions that we made for the { HALT A} state to all of the other \nhalted states.  If we had chosen not to specify an answer, an implementation\ncould produce different behavior from the different counting\nand/or halted states, which might confuse a user.",
    "92": "The resulting design appears to the right.",
    "93": "Now we need to select a representation for the states.  Since our counter\nhas eight states, we need at least three (_2 (8)=3)\nstate bits S_2S_1S_0 to keep track of the current state.",
    "94": "As we show later, { the choice of representation for an FSM's states\ncan dramatically affect the design complexity}.  For a design as simple as \nour counter, you could just let a computer implement all possible \nrepresentations (there aren't more than 840, if we consider simple \nsymmetries) and select one according to whatever metrics are interesting.",
    "95": "For bigger designs, however, the number of possibilities quickly becomes\nimpossible to explore completely.",
    "96": "Fortunately, { use of abstraction in selecting a representation \nalso tends to produce better designs} for a wide variety of metrics\n(such as design complexity, area, power consumption, and performance).",
    "97": "The right strategy is thus often to start by selecting a representation \nthat makes sense to a human, even if it requires more bits than are\nstrictly necessary.  The\nresulting implementation will be easier to\ndesign and to debug than an implementation in which only the global \nbehavior has any meaning.",
    "98": "Let's return to our specific example, the counter.  We can use one bit, \nS_2, to record whether or not our counter is counting (S_2=0) or\nhalted (S_2=1).  The other two bits can then record the counter state\nin terms of the desired output.  Choosing this representation\nimplies that only wires will be necessary to compute outputs Z_1 \nand Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting\ndesign, in which states are now labeled with both internal state and\noutputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version,\nwe have changed the arc labeling to use logical expressions, which\ncan sometimes help us to think about the implementation.",
    "99": "The equivalent state listing and state table appear below.  We have ordered\nthe rows of the state table in Gray code order to simplify transcription\nof K-maps.",
    "100": "& S_2S_1S_0& \n{ COUNT A}& 000& counting, output Z_1Z_0=00\n{ COUNT B}& 001& counting, output Z_1Z_0=01\n{ COUNT C}& 011& counting, output Z_1Z_0=11\n{ COUNT D}& 010& counting, output Z_1Z_0=10\n { HALT A}& 100& halted, output Z_1Z_0=00\n { HALT B}& 101& halted, output Z_1Z_0=01\n { HALT C}& 111& halted, output Z_1Z_0=11\n { HALT D}& 110& halted, output Z_1Z_0=10",
    "101": "{rc|cccc}\n&&{HG}\n&S_2S_1S_0& 00& 01& 11& 10 \n{ COUNT A}&000& 001& 001& 100& 100\n{ COUNT B}&001& 011& 011& 101& 101\n{ COUNT C}&011& 010& 010& 111& 111\n{ COUNT D}&010& 000& 000& 110& 110\n { HALT D}&110& 110& 000& 110& 110\n { HALT C}&111& 111& 010& 111& 111\n { HALT B}&101& 101& 011& 101& 101\n { HALT A}&100& 100& 001& 100& 100",
    "102": "Having chosen a representation, we can go ahead and implement our\ndesign in the usual way.  As shown to the right, K-maps for the \nnext-state logic are complicated, since we have five variables\nand must consider implicants that are not contiguous in the K-maps.\nThe S_2^+ logic is easy enough: we only need two terms, \nas shown.",
    "103": "Notice that we have used color and\nline style to distinguish different",
    "104": "implicants in the K-maps.  Furthermore, the symmetry of the design\nproduces symmetry in the S_1^+ and S_0^+ formula, so we have\nused the same color and line style for analogous terms in these\ntwo K-maps.",
    "105": "For S_1^+, we need four terms.  The green \nellipses in the HG=01 column are part of the same term, as are\nthe two halves of the dashed blue circle.  In S_0^+, we still\nneed four terms, but three of them are split into two pieces \nin the K-map.  As you can see, the utility of the K-map is starting\nto break down with five variables.",
    "106": "Rather than implementing the design as two-level logic, let's try to\ntake advantage of our design's symmetry to further simplify the\nlogic (we reduce gate count at the expense of longer, slower paths).",
    "107": "Looking back to the last transition diagram, in which the arcs\nwere labeled with logical expressions, let's calculate an expression\nfor when the counter should retain its current value in the next\ncycle.  We call \nthis variable HOLD.  In the counting states, when S_2=0, \nthe counter stops (moves into a halted state without changing value) \nwhen H is true.\nIn the halted states, when S_2=1, the counter stops (stays in \na halted state) when H+ is true.  We can thus write",
    "108": "{eqnarray*}\nHOLD &=& {S_2}  H + S_2  ( H +  )\nHOLD &=& {S_2} H + S_2 H + S_2 \nHOLD &=& H + S_2 \n{eqnarray*}",
    "109": "In other words, the counter should hold its current \nvalue (stop counting) if we press the ``halt'' button or if the counter\nwas already halted and we didn't press the ``go'' button.  As desired,\nthe current value of the counter (S_1S_0) has no impact on this \ndecision.  You may have noticed that the expression we derived for\nHOLD also matches S_2^+, the next-state value of S_2 in the \nK-map on the previous page.",
    "110": "Now let's re-write our state transition table in terms of HOLD.  The\nleft version uses state names for clarity; the right uses state values\nto help us transcribe K-maps.",
    "111": "{\n{rc|cc}\n&&{HOLD}\n&S_2S_1S_0& 0& 1 \n{ COUNT A}&000& { COUNT B}& { HALT A}\n{ COUNT B}&001& { COUNT C}& { HALT B}\n{ COUNT C}&011& { COUNT D}& { HALT C}\n{ COUNT D}&010& { COUNT A}& { HALT D}\n { HALT A}&100& { COUNT B}& { HALT A}\n { HALT B}&101& { COUNT C}& { HALT B}\n { HALT C}&111& { COUNT D}& { HALT C}\n { HALT D}&110& { COUNT A}& { HALT D}",
    "112": "{rc|cc}\n&&{HOLD}\n&S_2S_1S_0& 0& 1 \n{ COUNT A}&000& 001& 100\n{ COUNT B}&001& 011& 101\n{ COUNT C}&011& 010& 111\n{ COUNT D}&010& 000& 110\n { HALT A}&100& 001& 100\n { HALT B}&101& 011& 101\n { HALT C}&111& 010& 111\n { HALT D}&110& 000& 110",
    "113": "The K-maps based on the HOLD abstraction are shown to the right.\nAs you can see, the necessary logic has been simplified substantially,\nrequiring only two terms each for both S_1^+ and S_0^+.  Writing\nthe next-state logic algebraically, we obtain",
    "114": "{eqnarray*}\nS_2^+ &=& HOLD\nS_1^+ &=&   S_0 + HOLD  S_1\nS_0^+ &=&   {{S_1}} + HOLD  S_0\n{eqnarray*}",
    "115": "Notice the similarity between the equations for S_1^+S_0^+ and the \nequations for a {2-to-1} mux: when HOLD=1, the counter retains \nits state, and when HOLD=0, it counts.",
    "116": "An implementation appears below.",
    "117": "By using semantic meaning in our choice of representation---in\nparticular the use of S_2 to record whether\nthe counter is currently halted (S_2=1) or counting (S_2=0)---we\nhave enabled ourselves to \nseparate out the logic for deciding whether to advance the counter\nfairly cleanly from the logic for advancing the counter itself.\nOnly the HOLD bit in the diagram is used to determine\nwhether or not the counter should advance in the current cycle.",
    "118": "Let's check that the implementation matches our original design.",
    "119": "Start by verifying that the HOLD variable is calculated correctly,\nHOLD=H+S_2,\nthen look back at the K-map for S_2^+ in the low-level design to\nverify that the expression we used does indeed match.",
    "120": "Next, check the mux abstraction.",
    "121": "When HOLD=1, the next-state logic for S_1^+ and S_0^+ \nreduces to S_1^+=S_1 and S_0^+=S_0;\nin other words, the counter stops counting and simply stays in its \ncurrent state.  When HOLD=0, these equations become\nS_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating\nsequence for S_1S_0 of 00, 01, 11, 10, as desired.\nYou may want to look back at our two-bit Gray code counter design\nto compare the next-state equations.",
    "122": "We can now verify that the implementation produces the correct transition\nbehavior.  In the counting states, S_2=0, and the HOLD value simplifies\nto HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and\nand the counter continues to count in the correct sequence.\nWhen H=1, HOLD=1, and the counter stops at its current value\n(S_2^+S_1^+S_0^+=1S_1S_0, \nwhich is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).",
    "123": "In any of the halted states, S_2=1, and we can reduce HOLD to\nHOLD=H+.  Here, so long as we press the ``halt'' button\nor do not press the ``go'' button, the counter stays in its current\nstate, because HOLD=1.  If we release ``halt'' and press ``go,''\nwe have HOLD=0, and the counter resumes counting\n(S_2^+S_1^+S_0^+=0S_0{{S_1}},\nwhich is shorthand for S_2^+=0, S_1^+=S_0, and \nS_0^+={{S_1}}).",
    "124": "We have now verified the implementation.",
    "125": "What if you wanted to build a three-bit Gray code counter with the same\ncontrols for starting and stopping?  You could go back to basics and struggle \nwith six-variable {K-maps}.  Or you could simply copy the HOLD \nmechanism from the two-bit design above, insert muxes between the next \nstate logic and the flip-flops of the three-bit Gray code counter that \nwe designed earlier, and control the muxes with the HOLD bit.  \nAbstraction is a powerful tool.",
    "126": "What happens if we choose a bad representation?  For the same FSM---the\ntwo-bit Gray code counter with start and stop inputs---the \ntable below shows a poorly chosen mapping from states to internal \nstate representation.",
    "127": "Below the table is a diagram of an implementation using that\nrepresentation.",
    "128": "Verifying that the implementation's behavior\nis correct is left as an exercise for the determined reader.",
    "129": "{\n{|c|c|c|c|c|}{1-2}{4-5}\nstate& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5}\n{ COUNT A}& 000& & { HALT A}& 111 \n{ COUNT B}& 101& & { HALT B}& 110 \n{ COUNT C}& 011& & { HALT C}& 100 \n{ COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}",
    "130": "}",
    "131": "This set of notes describes random access memories (RAMs), providing slightly\nmore detail than is available in the textbook.  We begin with a discussion\nof the memory abstraction and the types of memory most commonly used in\ndigital systems, then examine how one can build memories (static RAMs) \nusing logic.  We next introduce tri-state buffers as a way of simplifying\nouput connections, and illustrate how memory chips can be combined to\nprovide larger and wider memories.  A more detailed description of dynamic \nRAMs finishes this set.",
    "132": "{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}",
    "133": "A computer { memory} is a group of storage elements and the logic\nnecessary to move data in and out of the elements.  The size of the\nelements in a memory---called the { addressability} of the \nmemory---varies from a single binary digit, or { bit},\nto a { byte} (8 bits) or more.  Typically, we refer to data\nelements larger than a byte as { words}, but the size of a word\ndepends on context.",
    "134": "Each element in a memory is assigned a unique name, called an {\naddress}, that allows an external circuit to identify the particular\nelement of interest.  These addresses are not unlike the street\naddresses that you use when you send a letter.  Unlike street\naddresses, however, memory addresses usually have little or no\nredundancy; each possible combination of bits in an address identifies\na distinct set of bits in the memory.  The figure on the right below \nillustrates the concept.  Each house represents a storage element and \nis associated with a unique address.",
    "135": "{{file=part3/figs/lec18-1.eps,width=4in}}",
    "136": "The memories that we consider in this class have several properties in\ncommon.  These memories support two operations: { write} places a\nword of data into an element, and { read} retrieves a copy of a\nword of data from an element.  The memories are also { volatile},\nwhich means that the data held by a memory are erased when electrical\npower is turned off or fails.  { Non-volatile} forms of memory\ninclude magnetic and optical storage media such as DVDs, CD-ROMs, disks, \nand tapes, capacitive storage media such as Flash drives,\nand some programmable logic devices.\nFinally, the memories considered in this class are { random access\nmemories (RAMs)}, which means that the time required to access an\nelement in the memory is independent of the element being accessed.\nIn contrast, { serial memories} such as magnetic tape require much\nless time to access data near the current location in the tape than\ndata far away from the current location.",
    "137": "The figure on the left above shows a generic RAM structure.  The\nmemory contains 2^k elements of N bits each.  A {k-bit}\naddress input, ADDR, identifies the memory element of interest for\nany particular operation.  The write enable\ninput, WE, selects the operation to be performed: if\nWE is high, the operation is a write; if it is low, the\noperation is a read.  Data to be written into an element are provided\nthrough N inputs at the top, and data read from an element appear on\nN outputs at the bottom.  Finally, a { chip select} input, CS,\nfunctions as an enable control for the memory; when CS is low, the\nmemory neither reads nor writes any location.",
    "138": "Random access memory further divides into two important types: {\nstatic RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}.\nSRAM employs active logic in the form of a two-inverter loop to\nmaintain stored values.  DRAM uses a charged capacitor to store a bit;\nthe charge drains over time and must be replaced, giving rise to the\nqualifier ``dynamic.''  ``Static'' thus serves only to differentiate\nmemories with active logic elements from those with capacitive\nelements.  Both types are volatile, that is, both lose all data when the\npower supply is removed.  We consider both SRAM and DRAM \nin this course, but the details of DRAM operation are beyond our scope.",
    "139": "Static random access memory is used for high-speed applications such\nas processor caches and some embedded designs.  As SRAM bit\n{density---the} number of bits in a given chip {area---is}\nsignificantly lower than DRAM bit density, most applications with less\ndemanding speed requirements use DRAM.  The main memory in most\ncomputers, for example, is DRAM, whereas the memory on the same chip\nas a processor is SRAM.{Chips combining both DRAM and processor\nlogic are available, and are used by some processor manufacturers (such \nas IBM).  Research is underway to couple such logic types more efficiently\nby building 3D stacks of chips.}  DRAM is also unavailable\nwhen recharging its capacitors, which can be a problem for\napplications with stringent real-time needs.",
    "140": "A diagram of an SRAM { cell} (a single bit) appears to\nthe right.  A dual-inverter loop stores the bit, and is connected\nto opposing BIT lines through transistors controlled by a SELECT\nline.",
    "141": "The cell works as follows.  When SELECT is high, the\ntransistors connect the inverter loop to the bit lines.  When writing\na cell, the bit lines are held at opposite logic values, forcing the\ninverters to match the values on the lines and storing the value from\nthe BIT input.  When reading a cell, the bit lines are disconnected\nfrom other logic, allowing the inverters to drive the lines with\ntheir current outputs.",
    "142": "{file=part3/figs/lec18-2.eps,width=2.20in}",
    "143": "The value stored previously is thus copied onto\nthe BIT line as an output, and the opposite value is placed on the\n line.  When SELECT is low, the transistors\ndisconnect the inverters from the bit lines, and the cell\nholds its current value until SELECT goes high again.",
    "144": "The actual operation of an SRAM cell is more complicated than we\nhave described.  For example, when writing a bit, the BIT lines \ncan temporarily connect high voltage to ground (a short).  The \ncircuit must be designed carefully to minimize the power consumed\nduring this process.  When reading a bit, the BIT lines\nare pre-charged halfway between high-voltage and ground, and \nanalog devices called sense amplifiers are used to detect the\nvoltage changes on the BIT lines (driven by the inverter loop)\nas quickly as possible.  These analog design issues are outside of \nthe scope of our class.",
    "145": "A number of cells are combined into a { bit slice}, as shown to\nthe right.",
    "146": "The labels along the bottom of the figure are external inputs to the \nbit slice, and match the labels for the abstract",
    "147": "{file=part3/figs/lec18-3.eps,width=5in}",
    "148": "memory discussed earlier.  The \nbit slice in the figure can be thought of as a {16-address},\n{1-bit-addressable} memory (2^4b).",
    "149": "The cells in a bit slice\nshare bit lines and analog read and write logic, which appears to the\nright in the figure.  Based on the ADDR input, a decoder sets one\ncell's SELECT line high to enable a read or write operation to the\ncell.",
    "150": "The chip select input CS drives the enable input of\nthe decoder, so none of the memory cells is active when chip select is\nlow (CS=0), and exactly one of the memory cells is active when\nchip select is high (CS=1).",
    "151": "Actual bit slices can contain many more cells than are shown in the \nfigure---more cells means less extra logic per cell, but slower memory,\nsince longer wires have higher capacitance.",
    "152": "A read operation is performed as follows.  We set CS=1 and WE=0,\nand place the address of the cell to be read on the ADDR input.\nThe decoder outputs a 1 on the appropriate cell's SELECT line,\nand the read logic reads the bit from the cell and delivers it\nto its Q output, which is then available on the bit \nslice's {DATA-OUT} output.",
    "153": "For a write operation, we set CS=1 and WE=1.  We again place the\naddress of the cell to be written on the ADDR input and set the\nvalue of the bit slice's {DATA-IN} input to the value to be written\ninto the memory cell.  When the decoder activates the cell's SELECT line,\nthe write logic writes the new value from its D input into\nthe memory cell.  Later reads from that cell then produce the new value.",
    "154": "{{file=part3/figs/lec18-4.eps,width=6.15in}}",
    "155": "The outputs of the cell selection decoder can be used to control\nmultiple bit slices, as shown in the figure above of a {2^6b}\nmemory.  Selection between bit slices is\nthen based on other bits from the address (ADDR).  In the figure\nabove, a {2-to-4} decoder is used to deliver write requests to\none of four bit slices, and a {4-to-1} mux is used to choose\nthe appropriate output bit for read requests.",
    "156": "The {4-to-16} decoder now activates one cell in each of the four \nbit slices.  For a read operation, WE=0, and the {2-to-4} decoder \nis not enabled, so it outputs all 0s.  All four bit slices thus perform\nreads, and the desired result bit is forwarded to {DATA-OUT} by the \n{4-to-1} mux.  The tri-state buffer between the mux \nand {DATA-OUT} is explained in a later section.",
    "157": "For a write operation, exactly one of the bit\nslices has its WE input set to 1 by the {2-to-4} decoder.\nThat bit slice writes the bit value delivered to all bit slices\nfrom {DATA-IN}.  The other three bit slices perform reads, but their \nresults are simply discarded.",
    "158": "The approach shown above, in which a cell is selected\nthrough a two-dimensional indexing scheme, is known as { coincident\nselection}.  The qualifier ``coincident'' arises from the notion that\nthe desired cell coincides with the intersection of the active row and\ncolumn outputs from the decoders.",
    "159": "The benefit of coincident selection is easily calculated in terms of\nthe number of gates required for the decoders.  Decoder complexity is\nroughly equal to the number of outputs, as each output is a minterm\nand requires a unique gate to calculate it.",
    "160": "Fanout trees for input terms and inverted terms add relatively few gates.",
    "161": "Consider a 1M8b RAM chip.  The number of addresses is 2^,\nand the total number of memory cells is 8,388,608 (2^).\nOne option is to use eight bit slices and a {20-to-1,048,576}\ndecoder, or about 2^ gates.  Alternatively, we can use 8,192 bit\nslices of 1,024 cells.  For the second implementation, we need \ntwo {10-to-1024} decoders, or about 2^ gates.  As chip \narea is roughly proportional to the number of gates, the savings are \nsubstantial.  Other schemes are possible as well: if we want a more \nsquare chip area, we might choose to use 4,096 bit slices of 2,048 \ncells along with one {11-to-2048} decoder and\none {9-to-512} decoder.  This approach requires roughly 25 more\ndecoder gates than our previous example, but is still far superior to\nthe eight-bit-slice implementation.",
    "162": "Memories are typically unclocked devices.  However, as you have seen,\nthe circuits are highly structured, which enables engineers to cope\nwith the complexity of sequential feedback design.  Devices used to\ncontrol memories are typically clocked, and the interaction between\nthe two can be fairly complex.",
    "163": "Timing diagrams for reads and writes\nto SRAM are shown to the right.  A write operation\nappears on the left.  In the first cycle, the controller raises the\nchip select signal and places the memory address to be written on the\naddress inputs.  Once the memory has had time to set up the \nappropriate",
    "164": "{file=part3/figs/lec18-6.eps,width=4in}",
    "165": "select lines\ninternally, the WE input is raised, and data are placed\non the data inputs.  The delay, which is specified by the memory\nmanufacturer, is necessary to avoid writing data to the incorrect\nelement within the memory.  The timing shown in the\nfigure rounds this delay up to a single clock cycle, but the\nactual delay needed depends on the clock speed and the memory's \nspecification.  At some point after new data have been\ndelivered to the memory, the write operation completes within the\nmemory.  The time from the application of the address until the\n(worst-case) completion of the write operation is called the {\nwrite cycle} of the memory, and is also specified by the memory \nmanufacturer.  Once the write cycle has passed, the controlling logic \nlowers WE, waits for the change to settle within the memory,\nthen removes the address and lowers the chip select signal.  The\nreason for the delay between these signal changes is the same: to \navoid mistakenly overwriting another memory location.",
    "166": "A read operation is quite similar.  As shown on the right, the\ncontrolling logic places the address on the input lines and raises the\nchip select signal.  No races need be considered, as read operations\non SRAM do not affect the stored data.  After a delay called the {\nread cycle}, the data can be read from the data outputs.  The address\ncan then be removed and the chip select signal lowered.",
    "167": "For both reads and writes, the number of cycles required for an\noperation depends on a combination of the clock cycle of the\ncontroller and the cycle time of the memory.  For example, with a\n25 nanosecond write cycle and a 10 nanosecond clock cycle, a write\nrequires three cycles.  In general, the number of cycles required is\ngiven by the formula {memory cycle time}/{clock cycle\ntime}.",
    "168": "Recall the buffer symbol---a triangle like an inverter, but with no\ninversion bubble---between the mux and the {DATA-OUT} \nsignal of the {2^6b} memory shown earlier.  This \n{ tri-state buffer} serves to disconnect the memory logic \nfrom the output line when the memory is not performing a read.",
    "169": "An implementation diagram for a tri-state buffer appears to the right \nalong with the symbolic\nform and a truth table.  The ``Z'' in the truth table output means \nhigh impedance (and is sometimes written ``hi-Z'').  In other words,\nthere is effectively no electrical connection between the tri-state \nbuffer and the output OUT.",
    "170": "This logical disconnection is achieved by using the outer",
    "171": "{file=part3/figs/tri-state.eps,width=3in}",
    "172": "{cc|c}\nEN& IN& OUT \n0& x& Z\n1& 0& 0\n1& 1& 1",
    "173": "(upper and lower)\npair of transistors in the logic diagram.  When EN=0, both transistors\nturn off, meaning that regardless of the value of IN, OUT is connected\nneither to high voltage nor to ground.",
    "174": "When EN=1, both transistors turn on, and the tri-state buffer acts as\na pair of back-to-back inverters, copying the signal from IN to OUT,\nas shown in the truth table.",
    "175": "What benefit does this logical disconnection provide?",
    "176": "So long as only one memory's chip select input is high at any time,\nthe same output line can be shared by more than one memory\nwithout the need for additional multiplexers.",
    "177": "Memory chips were often combined in this way to produce larger memories.",
    "178": "The figure to the right illustrates how larger memories can be constructed\nusing multiple chips.  In the case shown, two {2^-bit} \nmemories are used to implement a {2^{k+1}-bit} memory.\nOne of the address bits---in the case shown, the most significant bit---is\nused to drive a decoder that determines which of the two chips is \nactive (CS=1).  The decoder is enabled with the chip select signal for\nthe larger memory, so neither chip is enabled when the external CS is\nlow, as desired.  The",
    "179": "{file=part3/figs/mem-larger.eps,width=4.75in}",
    "180": "rest of the address bits, as well as the external\ndata inputs and write enable signal, are simply delivered to both memories.\nThe external data outputs are also connected to both memories.  \nEnsuring that at most one chip select signal is high at any time\nguarantees that at most one of the two memory chips drives logic values\non the data outputs.",
    "181": "Multiple chips can also be used to construct wider memories, as shown to\nthe right.  In the case shown, two {2^-bit} \nmemories are used to implement a {2^-bit} memory.\nBoth chips are either active or inactive at the same time, so the external \naddress, write enable, and chip select inputs are routed to both chips.\nIn contrast, the data inputs and outputs are separate: the left chip\nhandles the high N bits of input on writes and produces the high N\nbits of output on reads, while the right chip handles the low N bits of \ninput and produces the low N bits of output.",
    "182": "{file=part3/figs/mem-wider.eps,width=4.05in}",
    "183": "Historically, tri-state buffers were also used to reduce the number of\npins needed on chips.  Pins have long been a scarce resource, and the \namount of data that can cross a chip's pins in a second (the product of the\nnumber of pins and the data rate per pin) has not grown nearly as rapidly \nas the number of transistors packed into a fixed area.",
    "184": "By combining inputs and outputs, chip designers were able to halve the\nnumber of pins needed.  For example, data inputs and outputs of memory\nwere often combined into a single set of data wires, with bidirectional\nsignals.  When performing a read from a memory chip, the memory chip\ndrove the data pins with the bits being read (tri-state buffers on the\nmemory chip were enabled).  When performing a write, other logic such as \na processor wrote the value to be stored onto the data pins (tri-state \nbuffers were not enabled).",
    "185": "Dynamic random access memory, or DRAM, is used for main memory in\ncomputers and for other applications in which size is more important\nthan speed.  While slower than SRAM, DRAM is denser (has\nmore bits per chip area).  A substantial part of DRAM density is\ndue to transistor count: typical SRAM cells use six transistors\n(two for each inverter, and two more to connect the inverters to the \nbit lines), while DRAM cells use only a single transistor.\nHowever, memory designers have also made significant advances in\nfurther miniaturizing DRAM cells to improve density beyond the \nbenefit available from simple transistor count.",
    "186": "A diagram of a DRAM cell appears to the right.  \nDRAM storage is capacitive: a bit is stored by charging or not charging \na capacitor.  The capacitor is attached to a BIT line \nthrough a transistor controlled by a SELECT line.",
    "187": "When SELECT is low, the capacitor is isolated and \nholds its charge.  However, the transistor's resistance is\nfinite, and some charge leaks out onto the bit line.  Charge also\nleaks into the substrate on which the transistor is constructed.  After\nsome amount of time, all of the charge dissipates, and the bit is\nlost.  To avoid such loss, the cell must be { refreshed}\nperiodically by reading the contents and writing them back with active\nlogic.",
    "188": "{file=part3/figs/lec18-8.eps,width=1.1in}",
    "189": "When the SELECT line is high during a write operation, logic driving\nthe bit line forces charge onto the capacitor or removes all charge\nfrom it.  For a read operation, the bit line is first brought to an\nintermediate voltage level (a voltage level between 0 and 1), then\nSELECT is raised, allowing the capacitor to either pull a small\namount of charge from the bit line or to push a small amount of charge\nonto the bit line.  The resulting change in voltage is then detected\nby a { sense amplifier} at the end of the bit line.  A sense amp \nis analogous to a marble on a mountaintop: a small push causes the\nmarble to roll rapidly downhill in the direction of the push.\nSimilarly, a small change in voltage causes a sense amp's output to\nmove rapidly to a logical 0 or 1, depending on the direction of the\nsmall change.  As mentioned earlier, sense amplifiers also appear in \nSRAM implementations.\nWhile not technically necessary, as they are with DRAM, the use of a\nsense amp to react to small changes in voltage makes reads faster.",
    "190": "Each read operation on a DRAM cell brings the voltage on its capacitor\ncloser to the intermediate voltage level, in effect destroying the\ndata in the cell.  DRAM is thus said to have { destructive reads}.\nTo preserve data during a read, the bits must be written back\ninto the cells after a read.  For example, the output of the sense \namplifiers can\nbe used to drive the bit lines, rewriting the cells with the\nappropriate data.",
    "191": "At the chip level, typical DRAM inputs and outputs differ from those\nof SRAM.",
    "192": "Due to the large size and high density of DRAM,\naddresses are split into row and column components and provided\nthrough a common set of pins.  The DRAM stores the components in\nregisters to support this approach.  Additional inputs, known as the\n{ row} and { column address} {{ strobes}---RAS} and\nCAS, {respectively---are} used to indicate when address\ncomponents are available.  As\nyou might guess from the structure of coincident selection, DRAM\nrefresh occurs on a row-by-row basis (across bit slices---on columns\nrather than rows in the figures earlier in these notes, but the terminology\nof DRAM is a row).  Raising the SELECT line for a\nrow destructively reads the contents of all cells on that row, forcing\nthe cells to be rewritten and effecting a refresh.  The row is thus a\nnatural basis for the refresh cycle.  The DRAM data pins provide\nbidirectional signals for reading and writing elements of the DRAM.\nAn { output enable} input, OE, controls tri-state buffers with\nthe DRAM to determine whether or not the DRAM drives the data pins.\nThe WE input, which controls the type of operation, is\nalso present.",
    "193": "Timing diagrams for writes and reads on a historical DRAM implementation\nappear to the right.  In both cases, the row component of the address is \nfirst applied to the address pins, then RAS is raised.  In the\nnext cycle of the controlling logic, the column component is applied\nto the address pins, and CAS is raised.",
    "194": "For a write, as shown on the left, the WE signal and the\ndata can",
    "195": "{file=part3/figs/lec18-9.eps,width=4in}",
    "196": "also be applied in the second cycle.  The DRAM has internal\ntiming and control logic that prevent races from overwriting an\nincorrect element (remember that the row and column addresses have to\nbe stored in registers).  The DRAM again specifies a write cycle,\nafter which the operation is guaranteed to be complete.  In order, the\nWE, CAS, and RAS signals are then lowered.",
    "197": "For a read operation, the output enable signal, OE, is raised after\nCAS is raised.  The DATA pins, which should be floating (in other\nwords, not driven by any logic), are then driven by the DRAM.  After the \nread cycle, valid data appear on the DATA pins, and OE, CAS, and\nRAS are lowered in order after the data are read.",
    "198": "Modern DRAM chips are substantially more sophisticated than those\ndiscussed here, and many of the functions that used to be provided\nby external logic are now integrated onto the chips themselves.",
    "199": "As an example of modern DRAMs, one can obtain\nthe data sheet for Micron Semiconductor's 8Gb ({2^b},\nfor example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.",
    "200": "The ability to synchronize to an external clock has become prevalent in \nthe industry, leading to the somewhat confusing term SDRAM, which stands \nfor { synchronous DRAM}.  The memory structures themselves\nare still unclocked, but logic is provided on the chip to synchronize \naccesses to the external clock without the need for additional \nlogic.",
    "201": "The clock provided to the Micron chip just mentioned\ncan be as fast as 1.6 GHz, and data can be\ntransferred on both the rising and falling edges of the clock\n(hence the name DDR, or { double data rate}).",
    "202": "In addition to row and\ncolumn components of the address, these chips further separate cells into\n{ banks} and groups of banks.  These allow a user to exploit parallelism\nby starting reads or writes to separate banks at the same time, thus\nimproving the speed at which data can move in and out of the memory.",
    "203": "For the {2^b} version of the Micron chip,\nthe cells are structured into 4 groups of 4 banks (16 banks total),\neach with 131,072 rows and 1,024 columns.",
    "204": "DRAM implementations provide interfaces for specifying\nrefresh operations in addition to reads and writes.\nManaging refresh timing and execution is\ngenerally left to an external DRAM controller.",
    "205": "For the Micron chip, refresh commands must be issued every \n7.8 microseconds at normal temperatures.  Each\ncommand refreshes about 2^ cells, so 8,192 commands refresh\nthe whole chip in less than 64 milliseconds.",
    "206": "Alternatively, the chip can handle refresh on-chip in\norder to maintain memory contents when the rest of the system is \npowered down.",
    "207": "This set of notes explains the process that Prof. Doug Jones used to develop\nthe FSM for the lab.",
    "208": "The lab simulates a vending machine mechanism for automatically \nidentifying coins (dimes and quarters only), tracking the amount \nof money entered by the user, accepting or rejecting \ncoins, and emitting a signal when a total of 35 cents has been \naccepted.  In the lab, we will only drive a light with \nthe ``paid in full'' signal.",
    "209": "Sorry, neither candy nor Dew will be distributed!",
    "210": "Prof. Doug Jones designed the vending machine application and the FSM,\nwhile Prof. Chris Schmitz prototyped and constructed the physical elements \nwith some help from the ECE shop.",
    "211": "Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created \nboth the wiki documentation and the Altera Quartus portions of the lab\n(the latter were based on earlier Mentor Graphics work by Prof. Herman).",
    "212": "Prof. Kindratenko also helped to scale the design \nin a way that made it possible to deliver to the over 400 students entering\nECE every semester.",
    "213": "Prof. Juan Jos'e Jaramillo later identified\ncommon failure modes, including variability caused by sunshine through \nthe windows in ECEB,{No wonder people say that engineers hate \nsunlight!} and made some changes to improve robustness.  He also\ncreated the PowerPoint slides that are typically used to describe the lab in\nlecture.  Casey Smith, head guru of the ECE Instructional Labs,\ndeveloped a new debounce design and made some other hardware \nimprovements to reduce the rate of student headaches.\nFinally, Prof. Kirill Levchenko together with UA Saidivya Ashok\nstruck a blow against COVID-19 by developing an inexpensive and\nportable replacement for the physical ``vending machine'' systems\nused for testing in previous semesters.",
    "214": "A user inserts a coin into a slot at one end of the device.  The coin\nthen rolls down a slope towards a gate controlled by a servo.  The gate\ncan be raised or lowered, and determines whether the coin exits from the\nother side or the bottom of the device.",
    "215": "As the coin rolls, it passes two optical sensors.{The full system\nactually allows four sensors to differentiate four types of coins, but\nour lab uses only two of these sensors.}  One of these sensors is \npositioned high enough above the slope that a dime passes beneath the\nsesnor, allowing the signal T produced by the sensor to tell us whether \nthe coin is a dime or a quarter.  The second sensor is positioned so\nthat all coins pass in front of it.  The sensor positions are chosen \ncarefully to ensure that, in the case of a quarter, the coin is still\nblocking the first sensor when it reaches the second sensor.  \nBlocked sensors give a signal of 1 in this design, so the rising edge \nthe signal from the second sensor can be used as a ``clock'' for our \nFSM.  When the rising edge occurs, the signal T from the first sensor \nindicates whether the coin is a quarter (T=1) or a dime (T=0).",
    "216": "A sample timing diagram for the lab appears to the right.  The clock\nsignal generated by the lab is not only not a square wave---in other words,\nthe high and low portions are not equal---but is also unlikely to be periodic.\nInstead, the ``cycle'' is defined by the time between coin insertions.\nThe T signal serves as the single input to our FSM.  In the timing",
    "217": "{file=part3/figs/lab-timing.eps,width=2.55in}",
    "218": "diagram, T is shown as rising and falling before the clock edge.\nWe use positive edge-triggered flip-flops to implement our FSM,\nthus the aspect of the relative timing that matters to our design\nis that, when the clock rises, the value of T is stable and indicates \nthe type of coin entered.  The signal T may fall before or after\nthe clock does---the two are equivalent for our FSM's needs.",
    "219": "The signal A in the timing diagram is an output from the FSM, and\nindicates whether or not the coin should be accepted.  This signal \ncontrols the servo that drives the gate, and thus determines whether\nthe coin is accepted (A=1) as payment or rejected (A=0) and returned\nto the user.",
    "220": "Looking at the timing diagram, you should note that our FSM makes \na decision based on its current state and the input T and enters a \nnew state at the rising clock edge.  The value of A in the next cycle\nthus determines the position of the gate when the coin eventually\nrolls to the end of the slope.",
    "221": "As we said earlier, our FSM is thus a Moore machine: the output A\ndoes not depend on the input T, but only on the current internal \nstate bits of the the FSM.",
    "222": "However, you should also now realize that making A depend on T\nis not adequate for this lab.  If A were to rise with T and\nfall with the rising clock edge (on entry to the next state), or\neven fall with the falling edge of T, the gate would return to the\nreject position by the time the coin reached the gate, regardless\nof our FSM's decision!",
    "223": "We start by writing down states for a user's expected behavior.\nGiven the fairly tight constraints that we have placed on our lab,\nfew combinations are pos-",
    "224": "{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nSTART& DIME& QUARTER& & no\nDIME& & PAID& yes& no\nQUARTER& PAID& & yes& no\nPAID& & & yes& yes",
    "225": "sible.  For a total of 35 cents, a user should either insert a dime \nfollowed by a quarter, or a quarter followed by a dime.",
    "226": "We begin in a START state, which transitions to states DIME or QUARTER\nwhen the user inserts the first coin.  With no previous coin, we need not\nspecify a value for A.  No money has been deposited, so we set \noutput P=0 in the START state.",
    "227": "We next create DIME and QUARTER states corresponding to the user having\nentered one coin.  The first coin should be accepted, but more money is\nneeded, so both of these states output A=1 and P=0.\nWhen a coin of the opposite type is entered, each state moves to a\nstate called PAID, which we use for the case in which a total of 35 cents has\nbeen received.  For now, we ignore the possibility that the same type\nof coin is deposited more than once.  Finally, the PAID state accepts\nthe second coin (A=1) and indicates that the user has paid the full\nprice of 35 cents (P=1).",
    "228": "We next extend our design to handle user mistakes.  If a user enters\na second dime in the DIME state, our FSM should reject the coin.  We\ncreate a REJECTD state and add it as the next state from",
    "229": "{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nSTART& DIME& QUARTER& & no\nDIME& REJECTD& PAID& yes& no\nREJECTD& REJECTD& PAID& no& no\nQUARTER& PAID& REJECTQ& yes& no\nREJECTQ& PAID& REJECTQ& no& no\nPAID& & & yes& yes",
    "230": "DIME when a dime is entered.\nThe REJECTD state rejects the dime (A=0) and\ncontinues to wait for a quarter (P=0).  What should we use as next \nstates from REJECTD?  If the user enters a third dime (or a fourth, \nor a fifth, and so on), we want to reject the new dime as well.  \nIf the user enters a quarter, we want to accept the coin, at which point\nwe have received 35 cents (counting the first dime).  We use\nthis reasoning to complete the description of REJECTD.  We also create\nan analogous state, REJECTQ, to handle a user who inserts more than\none quarter.",
    "231": "What should happen after a user has paid 35 cents and bought \none item?  The FSM at that point is in the PAID state, which delivers\nthe item by setting P=1.",
    "232": "Given that we want the FSM to allow the user to purchase another item, \nhow should we choose the next states from PAID?",
    "233": "The behavior that we want from PAID is identical to the behavior that\nwe defined from START.  The 35 cents already \ndeposited was used to pay for the item delivered, so the machine is\nno longer holding any of the user's money.",
    "234": "We can thus simply set the next states from PAID to be DIME when a \ndime is inserted and QUARTER when a quarter is inserted.",
    "235": "At this point, we make a decision intended primarily to simplify the\nlogic needed to build the lab.  Without a physical item delivery \nmechanism with a specification for how its in-",
    "236": "{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nPAID& DIME& QUARTER& yes& yes\nDIME& REJECTD& PAID& yes& no\nREJECTD& REJECTD& PAID& no& no\nQUARTER& PAID& REJECTQ& yes& no\nREJECTQ& PAID& REJECTQ& no& no",
    "237": "put must be driven, \nthe behavior of the output signal P can be fairly flexible.  \nFor example, we could build a delivery mechanism that used the rising\nedge of P to open a chute.  In this case, the output P=0 in the\nstart state is not relevant, and we can merge the state START with\nthe state PAID.  The way that we handle P in the lab, we might\nfind it strange to have a ``paid'' light turn on before inserting any\nmoney, but keeping the design simple enough for a first lab exercise \nis more important.  Our final abstract state table appears above.",
    "238": "We are now ready to choose the state representation for the lab FSM.",
    "239": "With five states, we need three bits of internal state.",
    "240": "Prof. Jones decided to leverage human meaning in assigning the\nbit patterns, as follows:",
    "241": "{",
    "242": "S_2& type of last coin inserted (0 for dime, 1 for quarter)\nS_1& more than one quarter inserted? (1 for yes, 0 for no)\nS_0& more than one dime inserted? (1 for yes, 0 for no)",
    "243": "}",
    "244": "These meanings are not easy to apply to all of our states.  For example,\nin the PAID state, the last coin inserted may have been of either type,\nor of no type at all, since we decided to start our FSM in that state as \nwell.  However, for the other four states, the meanings provide a clear\nand unique set of bit pattern assignments, as shown to the right.  We\ncan choose any of the remaining four bit patterns (010, 011, 101, or 111)\nfor the PAID state.  In fact, { we can choose all of the remaining\npatterns} for the PAID state.  We can always represent any state",
    "245": "state& S_2S_1S_0  \nPAID& ???\nDIME& 000\nREJECTD& 001\nQUARTER& 100\nREJECTQ& 110",
    "246": "with more\nthan one pattern if we have spare patterns available.  Prof. Jones\nused this freedom to simplify the logic design.",
    "247": "This particular example is slightly tricky.  The four free patterns do\nnot share any single bit in common, so we cannot simply insert x's\ninto all {K-map} entries for which the next state is PAID.\nFor example, if we insert an x into the {K-map} for  S_2^+,\nand then choose a function for S_2^+ that produces a value of 1\nin place of the don't care, we must also produce a 1 in\nthe corresponding entry of the {K-map} for S_0^+.  Our options\nfor PAID include 101 and 111, but not 100 nor 110.  These latter\ntwo states have other meanings.",
    "248": "Let's begin by writing a next-state table consisting mostly of bits,\nas shown to the right.  We use this table to write out a {K-map}\nfor S_2^+ as follows: any of the patterns that may be used for the\nPAID state obey the next-state rules for PAID.  Any next-state marked\nas PAID is marked as don't care in the {K-map},",
    "249": "{cc|cc}\n&&{|c}{S_2^+S_1^+S_0^+}\nstate& S_2S_1S_0& T=0& T=1 \nPAID& PAID& 000& 100\nDIME& 000& 001& PAID\nREJECTD& 001& 001& PAID\nQUARTER& 100& PAID& 110\nREJECTQ& 110& PAID& 110",
    "250": "since we can\nchoose patterns starting with either or both values to represent our\nPAID state.  The resulting {K-map} appears to the far right.\nAs shown, we simply set S_2^+=T, which matches our\noriginal ``meaning'' for S_2.  That is, S_2 is the type of the\nlast coin inserted.",
    "251": "Based on our choice for S_2^+, we can rewrite the {K-map} as\nshown to the right, with green italics and shading marking the\nvalues produced for the x's in the specification.  Each of these\nboxes corresponds to one transition into the PAID state.  By \nspecifying the S_2 value, we cut the number of possible choices\nfrom four to two in each case.  For those combinations in which the\nimplementation produces S_2^+=0, we must choose S_1^+=1, but\nare still free to leave S_0^+ marked as a don't care.  Similarly,\nfor those combinations in which the implementation produces S_2^+=1, \nwe must choose S_0^+=1, but\nare still free to leave S_1^+ marked as a don't care.",
    "252": "The {K-maps} for S_1^+ and S_0^+ are shown to the right.\nWe have not given algebraic expressions for either, but have indicated\nour choices by highlighting the resulting replacements of don't care\nentries with the values produced by our expressions.",
    "253": "At this point, we can review the state patterns actually produced by\neach of the four next-state transitions into the PAID state.  From\nthe DIME state, we move into the 101 state when the user inserts a",
    "254": "quarter.  The result is the same from the REJECTD state.  From the\nQUARTER state, however, we move into the 010 state when the user \ninserts a dime.  The result is the same from the REJECTQ state.  We\nmust thus classify both patterns, 101 and 010, as PAID states.  The\nremaining two patterns, 011 and 111, cannot",
    "255": "be reached from any of\nthe states in our design.  We might then try to leverage the fact\nthat the next-state patterns from these two states are not relevant \n(recall that we fixed the next-state patterns for all four of the \npossible PAID states) to further simplify our logic, but doing so \ndoes not provide any advantage (you may want to check our claim).",
    "256": "The final state table is shown to the right.  We have included the\nextra states at the bottom of the table.  We have specified the\nnext-state logic for these",
    "257": "{cc|cc|cc}\n&&{|c|}{S_2^+S_1^+S_0^+}\nstate& S_2S_1S_0& T=0& T=1& A& P \nPAID1& 010& 000& 100& 1& 1\nPAID2& 101& 000& 100& 1& 1\nDIME& 000& 001& 101& 1& 0\nREJECTD& 001& 001& 101& 0& 0\nQUARTER& 100& 010& 110& 1& 0\nREJECTQ& 110& 010& 110& 0& 0 \nEXTRA1& 011& 000& 100& x& x\nEXTRA2& 111& 000& 100& x& x",
    "258": "states, but left the output bits as don't\ncares.  A state transition diagram appears at the bottom of this page.",
    "259": "Having a complete design on paper is a good step forward, but humans\nmake mistakes at all stages.  How can we know that a circuit that\nwe build in the lab correctly implements the FSM that we have outlined \nin these notes?",
    "260": "For the lab design, we have two problems to solve.",
    "261": "First, we have not specified an initialization scheme for the FSM.\nWe may want the FSM to start in one of the PAID states, but adding\ninitialization logic to the design may mean requiring you to wire together\nsignificantly more chips.  Second, we need a sequence of inputs that\nmanages to test that all of the next-state and output logic implementations\nare correct.",
    "262": "Testing sequential logic, including FSMs, is in general extremely difficult.\nIn fact, large sequential systems today are generally converted into \ncombinational logic by using shift registers to fill the \nflip-flops with a particular pattern, \nexecuting the logic for one clock cycle, and checking that the resulting \npattern of bits in the flip-flops is correct.  This approach is called \n{ scan-based testing}, and is discussed in ECE 543.  You \nwill make use of a similar approach\nwhen you test your combinational logic in the second week of the lab,\nbefore wiring up the flip-flops.",
    "263": "We have designed our FSM to be easy to test (even small FSMs\nmay be challenging) with a brute force approach.  In particular, we \nidentify two input sequences that together serve both to initialize and \nto test a correctly implemented variant of our FSM.  Our initialization\nsequence forces the FSM into a specific state regardless of its initial\nstate.  And our test sequence crosses every transition arc leaving the\nsix valid states.",
    "264": "In terms of T, the coin type, we initialize the FSM with the\ninput sequence 001.  Notice that such a sequence takes any initial \nstate into PAID2.",
    "265": "For testing, we use the input sequence 111010010001.  You should trace \nthis sequence, starting from PAID2, on the diagram below to see how the\ntest sequence covers all of the possible arcs.  As we test, we need also\nto observe the A and P outputs in each state to check the output\nlogic.",
    "266": "{{file=part3/figs/lab-diag-notes.eps,width=4.25in}}",
    "267": "This set of notes provides several additional examples of FSM design.\nWe first design an FSM to control a vending machine, introducing\nencoders and decoders as components that help us to implement our\ndesign.  We then design a game controller for a logic puzzle\nimplemented as a children's game.  Finally, we analyze a digital FSM\ndesigned to control the stoplights at the intersection of two roads.",
    "268": "For the next example, we design an FSM to control a simple vending machine.  \nThe machine accepts {U.S. coins}{Most countries have small \nbills or coins in demoninations suitable for vending machine prices, so think \nabout some other currency if you prefer.} as payment and offers a choice\nof three items for sale.",
    "269": "What states does such an FSM need?  The FSM needs to keep track of how\nmuch money has been inserted in order to decide whether a user can \npurchase one of the items.  That information alone is enough for the\nsimplest machine, but let's create a machine with adjustable item\nprices.  We can use registers to hold the item prices, which \nwe denote P_1, P_2, and P_3.",
    "270": "Technically, the item prices are also part of the internal state of the \nFSM.  However,  we leave out discussion (and, indeed, methods) for setting\nthe item prices, so no state with a given combination of prices has any \ntransition to a state with a different set of item prices.\nIn other words, any given combination of item prices induces a subset \nof states that operate independently of the subset induced by a distinct \ncombination of item prices.  By abstracting away the prices in this way,\nwe can focus on a general design that allows the owner of the machine\nto set the prices dynamically.",
    "271": "Our machine will not accept pennies, so let's have the FSM keep track of\nhow much money has been inserted as a multiple of 5 cents (one nickel).\nThe table to the right shows five types of coins, their value in \ndollars, and their value in terms of nickels.",
    "272": "The most expensive item in the machine might cost a dollar or two, so\nthe FSM must track at least 20 or 40 nickels of value.  Let's",
    "273": "{l|c|c}\n{c|}{coin type}& value& # of nickels \nnickel&      0.05& 1\ndime&        0.10& 2\nquarter&     0.25& 5\nhalf dollar& 0.50& 10\ndollar&      1.00& 20",
    "274": "decide to\nuse six bits to record the number of nickels, which allows the machine\nto keep track of up to 3.15 (63 nickels).  We call the abstract\nstates { STATE00} through { STATE63}, and refer to a state with\nan inserted value of N nickels as { STATE{<}N{>}}.",
    "275": "Let's now create a next-state table, as shown at the top of the next page.\nThe user can insert one of the five coin types, or can pick one of the \nthree items.  What should happen if the user inserts more money than the \nFSM can track?  Let's make the FSM reject such coins.  Similarly, if the \nuser tries to buy an item without inserting enough money first, the FSM \nmust reject the request.  For each of the possible input events, we add a \ncondition to separate the FSM states that allow the input event to \nbe processed as the user desires from those states that do not.  For example,\nif the user inserts a quarter, those states with N<59 transition to\nstates with value N+5 and accept the quarter.  Those states with\nN reject the coin and remain in { STATE{<}N{>}}.",
    "276": "{c|l|c|l|c|c}\n&&& {|c}{final state}\n&&& {|c}{}& & release \ninitial state& {|c|}{input event}& condition& {|c}& & product \n{ STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none\n{ STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none\n{ STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none\n{ STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none\n{ STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none\n{ STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1\n{ STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2\n{ STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3\n{ STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none",
    "277": "We can now begin to formalize the I/O for our machine.  Inputs include \ninsertion of coins and selection of items for purchase.  Outputs include\na signal to accept or reject an inserted coin as well as signals to release\neach of the three items.",
    "278": "For input to the FSM, we assume that a coin inserted in any given cycle \nis classified and delivered to our FSM using the three-bit representation \nshown to the right.",
    "279": "For item selection, we assume that the user has access to three buttons,\nB_1, B_2, and B_3, that indicate a desire to purchase the \ncorresponding item.",
    "280": "For output, the FSM must produce a signal A indicating whether a coin\nshould be accepted.  To control the release of items that have been purchased,\nthe FSM must produce the signals R_1, R_2, and R_3, corresponding\nto the re-",
    "281": "{l|c}\n{c|}{coin type}& C_2C_1C_0 \nnone&        110\nnickel&      010\ndime&        000\nquarter&     011\nhalf dollar& 001\ndollar&      111",
    "282": "lease of each item.  Since outputs in our class depend only on\nstate, we extend the internal state of the FSM to include bits for each of\nthese output signals.  The output signals go high in the cycle after\nthe inputs that generate them.  Thus, for example, the accept signal A\ncorresponds to a coin inserted in the previous cycle, even if a second\ncoin is inserted in the current cycle.  This meaning must be made clear to\nwhomever builds the mechanical system to return coins.",
    "283": "Now we are ready to complete the specification.  How many states does the\nFSM have?  With six bits to record money inserted and four bits to \ndrive output signals, we have a total of 1,024 (2^) states!\nSix different coin inputs are possible, and the selection buttons allow\neight possible combinations, giving 48 transitions from each state.\nFortunately, we can use the meaning of the bits to greatly simplify\nour analysis.",
    "284": "First, note that the current state of the coin accept bit and item\nrelease bits---the four bits of FSM state that control the outputs---have\nno effect on the next state of the FSM.  Thus, we can consider only the\ncurrent amount of money in a given state when thinking about the \ntransitions from the state.  As you have seen, we can further abstract\nthe states using the number N, the number of nickels currently held by \nthe vending machine.",
    "285": "We must still consider all 48 possible transitions from { STATE{<}N{>}}.\nLooking back at our abstract next-state table, notice that we had only\neight types of input events (not counting ``no input'').  If we\nstrictly prioritize these eight possible events, we can safely ignore\ncombinations.  Recall that we adopted a similar strategy for several \nearlier designs, including the ice cream dispenser in Notes Set 2.2 and\nthe keyless entry system developed in Notes Set 3.1.3.",
    "286": "We choose to prioritize purchases over new coin insertions, and to \nprioritize item 3 over item 2 over item 1.  These prioritizations\nare strict in the sense that if the user presses B_3, both other\nbuttons are ignored, and any coin inserted is rejected, regardless of\nwhether or not the user can actually purchase item 3 (the machine\nmay not contain enough money to cover the item price).",
    "287": "With the choice of strict prioritization, all transitions from all\nstates become well-defined.  We apply the transition rules in order of\ndecreasing priority,\nwith conditions, and with {don't-cares} for lower-priority inputs. \nFor example, for any of the 16 { STATE50}'s (remember that the four\ncurrent output bits do not affect transitions), the table below lists all\npossible transitions assuming that P_3=60, P_2=10, and P_1=35.",
    "288": "{\n{ccccc|ccccc}\n&&&&& {|c}{next state}\ninitial state& B_3& B_2& B_1& C_2C_1C_0& state& A& R_3& R_2& R_1 \n{ STATE50}& 1&x&x& xxx& { STATE50}& 0& 0&0&0\n{ STATE50}& 0&1&x& xxx& { STATE40}& 0& 0&1&0\n{ STATE50}& 0&0&1& xxx& { STATE15}& 0& 0&0&1\n{ STATE50}& 0&0&0& 010& { STATE51}& 1& 0&0&0\n{ STATE50}& 0&0&0& 000& { STATE52}& 1& 0&0&0\n{ STATE50}& 0&0&0& 011& { STATE55}& 1& 0&0&0\n{ STATE50}& 0&0&0& 001& { STATE60}& 1& 0&0&0\n{ STATE50}& 0&0&0& 111& { STATE50}& 0& 0&0&0\n{ STATE50}& 0&0&0& 110& { STATE50}& 0& 0&0&0",
    "289": "}\nNext, we need to choose a state representation.  But this task is \nessentially done: each output bit (A, R_1, R_2, and R_3) is\nrepresented with one bit in the internal representation, and the\nremaining six bits record the number of nickels held by the vending\nmachine using an unsigned representation.",
    "290": "The choice of a numeric representation for the money held is important,\nas it allows us to use an adder to compute the money held in\nthe next state.",
    "291": "Since we chose to prioritize purchases, let's begin by building logic\nto perform state transitions for purchases.  Our first task is to\nimplement prioritization among the three selection buttons.  For this\npurpose, we construct a {4-input} { priority encoder}, which \ngenerates a signal P whenever any of its four input lines is active\nand encodes the index of the highest active input as a two-bit unsigned\nnumber S.   A truth table for our priority encoder appears on the \nleft below, with {K-maps} for each of the output bits on the right.",
    "292": "{cccc|cc}\nB_3& B_2& B_1& B_0& P& S \n1&x&x&x& 1& 11\n0&1&x&x& 1& 10\n0&0&1&x& 1& 01\n0&0&0&1& 1& 00\n0&0&0&0& 0& xx",
    "293": "From the {K-maps}, we extract the following equations:\n{eqnarray*}\nP &=& B_3 + B_2 + B_1 + B_0\nS_1 &=& B_3 + B_2\nS_0 &=& B_3 + {B_2}B_1\n{eqnarray*}\nwhich allow us to implement our encoder as shown to the right.",
    "294": "If we connect our buttons B_1, B_2, and B_3 to the priority \nencoder (and feed 0 into the fourth input), it produces a signal P \nindicating that the user is trying to make a purchase and a two-bit\nsignal S indicating which item the user wants.",
    "295": "We also need to build logic to control the item release outputs R_1, R_2,\nand R_3.  An item should be released only when it has been selected \n(as indicated by the priority encoder signal S) and the vending machine\nhas enough money.  For now, let's leave aside calculation of the item \nrelease signal, which we call R, and focus on how we can produce the\ncorrect values of R_1, R_2, and R_3 from S and R.",
    "296": "The component to the right is a { decoder} with an enable input.  A \ndecoder takes an input signal---typically one coded as a binary number---and \nproduces one output for each possible value of the signal.  You may\nnotice the similarity with the structure of a mux: when the decoder\nis enabled (EN=1), each of the AND gates produces",
    "297": "one minterm of the input signal S.  In\nthe mux, each of the inputs is then included in\none minterm's AND gate, and the outputs of all AND gates are ORd together.\nIn the decoder, the AND gate outputs are the outputs of the decoder.\nThus, when enabled, the decoder produces exactly one 1 bit on its outputs.\nWhen not enabled (EN=0), the decoder produces all 0 bits.",
    "298": "We use a decoder to generate the release signals for the vending machine\nby connecting the signal S produced by the \npriority encoder to the decoder's S input and connecting the item\nrelease signal R to the decoder's EN input.  The outputs D_1,\nD_2, and D_3 then correspond to the individual item release \nsignals R_1, R_2, and R_3 for our vending machine.",
    "299": "We are now ready to implement the FSM to handle purchases, as shown to the \nright.  The current number of nickels, N, is stored in a register in the\ncenter of the diagram.  Each cycle, N is fed into a {6-bit} adder,\nwhich subtracts the price of any purchase requested in that cycle.",
    "300": "Recall that we chose to record item prices in registers.  We avoid the \nneed to negate prices before adding them by storing the negated prices in\nour registers.  Thus, the value of register PRICE1 is -P_1, the\nthe value of register PRICE2 is -P_2, and the\nthe value of register PRICE3 is -P_3.",
    "301": "The priority encoder's S signal is then used to select the value of \none of these three registers (using a {24-to-6} mux) as the second\ninput to the adder.",
    "302": "We use the adder to execute a subtraction, so the carry out C_ \nis 1 whenever the value of N is at least as great as the amount \nbeing subtracted.  In that case, the purchase is successful.  The AND\ngate on the left calculates the signal R indicating a successful purchase,\nwhich is then used to select the next value of N using the {12-to-6}\nmux below the adder.",
    "303": "When no item selection buttons are pushed, P and thus R are both 0, \nand the mux below the adder keeps N unchanged in the next cycle.  \nSimilarly, if P=1 but N is\ninsufficient, C_ and thus R are both 0, and again N does\nnot change.  Only when P=1 and C_=1 is the purchase successful,\nin which case the price is subtracted from N in the next cycle.",
    "304": "The signal R is also used to enable a decoder that generates the three\nindividual item release outputs.  The correct output is generated based on\nthe decoded S signal from the priority encoder, and all three output\nbits are latched into registers to release the purchased item in the next \ncycle.",
    "305": "One minor note on the design so far: by hardwiring C_ to 0, we created \na problem for items that cost nothing (0 nickels): in that case, C_ is\nalways 0.  We could instead store\n-P_1-1 in PRICE1 (and so forth) and feed P in to C_, but maybe\nit's better not to allow free items.",
    "306": "How can we support coin insertion?  Let's use the same adder to add\neach inserted coin's value to N.  The table at the right shows\nthe value of each coin as a {5-bit} unsigned number of nickels.\nUsing this table, we can fill in {K-maps} for each bit of V, as\nshown below.  Notice that we have marked the two undefined bit patterns\nfor the coin type C as don't cares in the {K-maps}.",
    "307": "{l|c|c}\n{c|}{coin type}& C_2C_1C_0& V_4V_3V_2V_1V_0 \nnone&        110& 00000\nnickel&      010& 00001\ndime&        000& 00010\nquarter&     011& 00101\nhalf dollar& 001& 01010\ndollar&      111& 10100",
    "308": "Solving the {K-maps} gives the following equations, which we\nimplement as shown to the right.",
    "309": "{eqnarray*}\nV_4 &=& C_2C_0\nV_3 &=& {C_1}C_0\nV_2 &=& C_1C_0\nV_1 &=& {C_1}\nV_0 &=& {C_2}C_1\n{eqnarray*}",
    "310": "Now we can extend the design to handle coin insertion, as shown to the right\nwith new elements highlighted in blue.  The output of the coin value \ncalculator is extended with a leading 0 and then fed into a {12-to-6} mux.\nWhen a purchase is requested, P=1 and the mux forwards the item price to \nthe adder---recall that we chose to give purchases priority over coin\ninsertion.  When no purchase is requested, the value of any coin inserted\n(or 0 when no coin is inserted) is passed to the adder.",
    "311": "Two new gates have been added on the lower left.  First, let's verify that\npurchases work as before.  When a purchase is requested, P=1, so the\nNOR gate outputs 0, and the OR gate simple forwards R to control the\nmux that decides whether the purchase was successful, just as in our\noriginal design.",
    "312": "When no purchase is made (P=0, and R=0), the adder adds the value of \nany inserted coin to N.  If the addition overflows, C_=1, and\nthe output of the NOR gate is 0.  Note that the NOR gate output is stored\nas the output A in the next cycle, so a coin that causes overflow in the\namount of money stored is rejected.  The OR gate also outputs 0, and N\nremains unchanged.  If the addition does not overflow, the NOR gate\noutputs a 1, the coin is accepted (A=1 in the next cycle), and \nthe mux allows the sum N+V to be written back as the new value of N.",
    "313": "The tables at the top of the next page define all of the state variables,\ninputs, outputs, and internal signals used in the design, and list the\nnumber of bits for each variable.",
    "314": "{|ccl|}\n{|r|}{{ FSM state}}\nPRICE1& 6& negated price of item 1 (-P_1)\nPRICE2& 6& negated price of item 2 (-P_2)\nPRICE3& 6& negated price of item 3 (-P_3)\nN& 6& value of money in machine (in nickels)\nA& 1& stored value of accept coin output\nR_1& 1& stored value of release item 1 output\nR_2& 1& stored value of release item 2 output\nR_3& 1& stored value of release item 3 output \n{|r|}{{ internal signals}}\nV& 5& inserted coin value in nickels\nP& 1& purchase requested (from priority encoder)\nS& 2& item # requested (from priority encoder)\nR& 1& release item (purchase approved)",
    "315": "{|ccl|}\n{|r|}{{ inputs}}\nB_1& 1& item 1 selected for purchase\nB_2& 1& item 2 selected for purchase\nB_3& 1& item 3 selected for purchase\nC& 3& coin inserted (see earlier table \n& & for meaning) \n{|r|}{{ outputs}}\nA& 1& accept inserted coin (last cycle)\nR_1& 1& release item 1\nR_2& 1& release item 2\nR_3& 1& release item 3\n{|l|}{{ Note that outputs correspond one-to-one}}\n{|l|}{{ with four bits of FSM state.}}",
    "316": "For the next example, imagine that you are part of a team building a\ngame for children to play at Engineering Open House.",
    "317": "The game revolves around an old logic problem in which a farmer must\ncross a river in order to reach the market.  The farmer is traveling\nto the market to sell a fox, a goose, and some corn.  The farmer has\na boat, but the boat is only large enough to carry the fox, the goose,\nor the corn along with the farmer.  The farmer knows that if he leaves\nthe fox alone with the goose, the fox will eat the goose.  Similarly,\nif the farmer leaves the goose alone with the corn, the goose will \neat the corn.",
    "318": "How can the farmer cross the river?",
    "319": "Your team decides to build a board illustrating the problem with\na river from top to bottom and lights illustrating the positions of \nthe farmer (always with the boat), the fox, the goose, and the\ncorn on either the left bank or the right bank of the river.\nEverything starts on the left bank, and the children can play \nthe game until they win by getting everything to the right bank or\nuntil they make a mistake.",
    "320": "As the ECE major on your team, you get to design the FSM!",
    "321": "{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}",
    "322": "Since the four entities (farmer, fox, goose, and corn) can be only\non one bank or the other, we can use one bit to represent the location\nof each entity.  Rather than giving the states names, let's just\ncall a state FXGC.  The value of F represents the location of the farmer,\neither on the left bank (F=0) or the right bank (F=1).  Using\nthe same representation (0 for the left bank, 1 for the right bank),\nthe value of X represents the location of the fox, G represents the\nlocation of the goose, and C represents the location of the corn.",
    "323": "We can now put together an abstract next-state table, as shown to\nthe right.  Once the player wins or loses, let's have the game indicate\ntheir final status and stop accepting requests to have the farmer cross\nthe river.  We can use a reset button to force the game back into the\noriginal state for the next player.",
    "324": "Note that we have included conditions for some of the input events, as \nwe did previously",
    "325": "{c|l|c|c}\ninitial state& {|c|}{input event}& condition& final state \nFXGC& no input& always& FXGC\nFXGC& reset & always& 0000\nFXGC& cross alone& always& XGC\nFXGC& cross with fox& F=X& GC\nFXGC& cross with goose& F=G& XC\nFXGC& cross with corn& F=C& XG",
    "326": "with the vending machine design.",
    "327": "The conditions here require that the farmer be on the same bank as any\nentity that the player wants the farmer to carry across the river.",
    "328": "Next, we specify the I/O interface.",
    "329": "For input, the game has five buttons.  A reset button R forces the\nFSM back into the initial state.  The other four buttons cause the\nfarmer to cross the river: B_F crosses alone, B_X with the fox,\nB_G with the goose, and B_C with the corn.",
    "330": "For output, we need position indicators for the four entities, but let's\nassume that we can simply output the current state FXGC and have\nappropriate images or lights appear on the correct banks of the \nriver.  We also need two more indicators: W for reaching the winning\nstate, and L for reaching a losing state.",
    "331": "Now we are ready to complete the specification.  We could use a strict\nprioritization of input events, as we did with earlier examples.  Instead,\nin order to vary the designs a bit, we use a strict prioritization among \nallowed inputs.  The reset button R has the highest priority, followed\nby B_F, B_C, B_G, and finally B_X.  However, only those buttons \nthat result in an allowed move are considered when selecting one button \namong several pressed in a single clock cycle.",
    "332": "As an example, consider the \nstate FXGC=0101.  The farmer is not on the same bank as the fox, nor\nas the corn, so the B_X and B_C buttons are ignored, leading to the\nnext-state table to the right.  Notice that B_G is accepted even if B_C\nis pressed because the farmer is not on the same",
    "333": "{cccccc|c}\nFXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+ \n0101& 1& x& x& x& x& 0000\n0101& 0& 1& x& x& x& 1101\n0101& 0& 0& x& 1& x& 1111\n0101& 0& 0& x& 0& x& 0101",
    "334": "bank as the corn.\nAs shown later, this approach to prioritization\nof inputs is equally simple in terms of implementation.",
    "335": "Recall that we want to stop the game when the player wins or loses.\nIn these states, only the reset button is accepted.  For example, the \nstate FXGC=0110 is a losing state because",
    "336": "{cccccc|c}\nFXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+ \n0110& 1& x& x& x& x& 0000\n0110& 0& x& x& x& x& 0110",
    "337": "the farmer\nhas left the fox with the goose on the opposite side of the river.\nIn this case, the player can reset the game, but other buttons\nare ignored.",
    "338": "As we have already chosen a representation, we now move on to implement\nthe FSM.  Let's begin by calculating the next state ignoring the\nreset button and the winning and losing states, as shown in the logic \ndiagram below.",
    "339": "The left column of XOR gates determines whether the farmer is on the same\nbank as the corn (top gate), goose (middle gate), and fox (bottom gate).\nThe output of each gate is then used to mask out the corresponding button:\nonly when the farmer is on the same bank are these buttons considered.\nThe adjusted button values are then fed into a priority encoder, which\nselects the highest priority input event according to the scheme that\nwe outlined earlier (from highest to lowest, B_F, B_C, B_G, and\nB_X, ignoring the reset button).",
    "340": "The output of the priority encoder is then used to drive another column\nof XOR gates on the right in order to calculate the next state.\nIf any of the allowed buttons is pressed, the priority encoder \noutputs P=1, and the farmer's bank is changed.  If B_C is allowed\nand selected by the priority encoder (only when B_F is not pressed),\nboth the farmer and the corn's banks are flipped.  The goose and the fox\nare handled in the same way.",
    "341": "Next, let's build a component to produce the win and lose signals.\nThe one winning state is FXGC=1111, so we simply need an AND gate.\nFor the lose signal L, we can fill in a {K-map} and derive an \nexpression, as shown to the right, then implement as shown in the logic\ndiagram to the far right.  For the {K-map}, remember that the player\nloses whenever the fox and the goose are on the same side of the river,\nbut opposite from the farmer, or whenever the goose and the corn are on\nthe same side of the river, but opposite from the farmer.",
    "342": "{eqnarray*}\nL &=& F   +  X G +\n&& F   +  G C\n{eqnarray*}",
    "343": "Finally, we complete our design by integrating the next-state\ncalculation and the win-lose calculation with a couple of muxes, as\nshown to the right.",
    "344": "The lower mux controls the final value of the next state: note that\nit selects between the constant state FXGC=0000 when the reset button R\nis pressed and the output of the upper mux when R=0.",
    "345": "The upper mux is controlled by W+L, and retains the current state\nwhenever either signal is 1.  In other words, once the player has won\nor lost, the upper mux prevents further state changes until the reset\nbutton is pressed.",
    "346": "When R, W, and L are all 0, the next state is calculated\naccording to whatever buttons have been pressed.",
    "347": "In this example, we begin with a digital FSM design and analyze it to\nunderstand how it works and to verify that its behavior is appropriate.",
    "348": "The FSM that we analyze has been designed to control the stoplights\nat the intersection of two roads.  For naming purposes, we assume that one\nof the roads runs East and West (EW), and the second runs North and \nSouth (NS).",
    "349": "The stoplight controller has two inputs, each of which \nsenses vehicles approaching from either direction on one of the two roads.\nThe input V^=1 when a vehicle approaches from either the East or the\nWest, and the input V^=1 when a vehicle approaches from either the\nNorth or the South.  These inputs are also active when vehicles are stopped\nwaiting at the corresponding lights.",
    "350": "Another three inputs, A, B, and C, control the timing behavior of the\nsystem; we do not discuss them here except as variables.",
    "351": "The outputs of the controller consist of two {2-bit} values, \nL^ and L^, that specify the light colors for the two roads.\nIn particular, L^ controls the lights facing East and West, and\nL^ controls the lights facing North and South.  The meaning of these\noutputs is given in the table to the right.",
    "352": "{c|c}\nL& light color \n0x& red\n10& yellow\n11& green",
    "353": "Let's think about the basic operation of the controller.",
    "354": "For safety reasons, the controller must ensure that the lights on one\nor both roads are red at all times.",
    "355": "Similarly, if a road has a green light, the controller should \nshow a yellow light before showing a red light to give drivers some\nwarning and allow them to slow down.",
    "356": "Finally, for fairness, the controller should alternate green lights\nbetween the two roads.",
    "357": "Now take a look at the logic diagram below.",
    "358": "The state of the FSM has been split into two pieces: a {3-bit} \nregister S and a {6-bit} timer.  The timer is simply a binary \ncounter that counts downward and produces an output of Z=1 when it \nreaches 0.  Notice that the register S only takes a new value\nwhen the timer reaches 0, and that the Z signal from the timer\nalso forces a new value to be loaded into the timer in the next \ncycle.  We can thus think of transitions in the FSM on a cycle by \ncycle basis as consisting of two types.  The first type simply\ncounts downward for a number of cycles while holding the register S\nconstant, while the second changes the value of S and sets the\ntimer in order to maintain the new value of S \nfor some number of cycles.",
    "359": "3.45",
    "360": "Let's look at the next-state logic for S, which feeds into the IN\ninputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice \nthat none of the inputs to the FSM directly affect these values.  The\nstates of S thus act like a counter.  By examining the connections,\nwe can derive equations for the next state and draw a transition\ndiagram, as shown to the right.",
    "361": "As the figure shows, there are six states in the loop defined by the \nnext-state logic, with the two remaining states converging into the\nloop after a single cycle.",
    "362": "Let's now examine the outputs for each\nstate in order to understand how the stoplight sequencing\nworks.",
    "363": "We derive equations for the outputs that control the lights, as shown\nto the right, then calculate values and colors for each\nstate, as shown to the far right.  For completeness, the table \nincludes the states outside of the desired loop.  The \nlights are all red in both of these states, which is necessary for safety.",
    "364": "{eqnarray*}",
    "365": "S_2^+ &=& {S_2} + S_0\nS_1^+ &=& {S_2}  S_1\nS_0^+ &=& {S_2}\n{eqnarray*}",
    "366": "{eqnarray*}\nL_1^ &=& S_2 S_1\nL_0^ &=& S_0\nL_1^ &=& S_2 {S_1}\nL_0^ &=& S_0\n{eqnarray*}",
    "367": "{c|cc|cc}\n&&& EW& NS\n&&& light& light\nS& L^& L^& color& color \n000& 00& 00&    red&    red\n111& 11& 01&  green&    red\n110& 10& 00& yellow&    red\n010& 00& 00&    red&    red\n101& 01& 11&    red&  green\n100& 00& 10&    red& yellow \n001& 01& 01&    red&    red\n011& 01& 01&    red&    red",
    "368": "Now let's think about how the timer works.  As we already noted, the\ntimer value is set whenever S enters a new state, but it can also be\nset under other conditions---in particular, by the signal F calculated\nat the bottom of the FSM logic diagram.",
    "369": "For now, assume that F=0.  In this case, the timer is set only when\nthe state S changes, and we can find the duration of each state by\nanalyzing the muxes.  The bottom mux selects A when S_2=0, and \nselects the output of the top mux when S_2=1.  The top mux selects B\nwhen S_0=1, and selects C when S_0=0.  Combining these results,\nwe can calculate the duration of the next states of S when F=0, \nas shown in the table to the right.  We can then combine the next\nstate duration with our previous calculation of the state sequencing \n(also the order in the table) to obtain the durations of each state, also\nshown in the rightmost column of the table.",
    "370": "{c|cc|cc}\n& EW& NS& next& current\n& light& light& state& state\nS& color& color& duration& duration \n000&    red&    red& A& C\n111&  green&    red& B& A\n110& yellow&    red& C& B\n010&    red&    red& A& C\n101&    red&  green& B& A\n100&    red& yellow& C& B \n001&    red&    red& A& ---\n011&    red&    red& A& ---",
    "371": "What does F do?  Analyzing the gates that produce it gives \nF=S_1S_0{V^+{S_1}S_0{V^.  If we \nignore the two states outside of the main loop for S, the first term \nis 1 only when the lights are green on the East and West roads and the \ndetector for the North and South roads indicates that no vehicles are \napproaching.  Similarly, the second term is 1 only when the lights are \ngreen on the North and South roads and the detector for the East and \nWest roads indicates that no vehicles are approaching.",
    "372": "What happens when F=1?  First, the OR gate feeding into the timer's\nLD input produces a 1, meaning that the timer loads a new value\ninstead of counting down.  Second, the OR gate controlling the lower\nmux selects the A input.  In other words, the timer is reset to A\ncycles, corresponding to the initial value for the green light states.\nIn other words, the light stays green until vehicles approach on \nthe other road, plus A more cycles.",
    "373": "Unfortunately, the signal F may also be 1 in the unused states of S,\nin which case the lights on both roads may remain red even though cars\nare waiting on one of the roads.  To avoid this behavior, we must be \nsure to initialize the state S to one of the six states in the\ndesired loop.",
    "374": "The FSM designs we have explored so far have started with a human-based\ndesign process in which someone writes down the desired behavior in\nterms of states, inputs, outputs, and transitions.  Such an approach\nmakes it easier to build a digital FSM, since the abstraction used\ncorresponds almost directly to the implementation.",
    "375": "As an alternative, one can start by mapping the desired task into a\nhigh-level programming language, then using components such as registers,\ncounters, and memories to implement the variables needed.  In this approach,\nthe control structure of the code maps into a high-level FSM design.\nOf course, in order to implement our FSM with digital logic, we eventually\nstill need to map down to bits and gates.",
    "376": "In this set of notes, we show how one can transform a piece of code\nwritten in a high-level language into an FSM.  This process is meant to\nhelp you understand how we can design an FSM that executes simple\npieces of a flow chart such as assignments, { if} statements, and \nloops.  Later, we generalize this concept and build an FSM that allows\nthe pieces to be executed to be specified after the FSM is built---in \nother words, the FSM executes a program specified by bits stored in \nmemory.  This more general model, as you might have already guessed, \nis a computer.",
    "377": "Let's begin by specifying the problem that we want to solve.\nSay that we want to find the minimum value in a set of 10 integers.\nUsing the C programming language, we can write the following fragment of \ncode:",
    "378": "{",
    "379": "aaaa=aaaa=\nint >values[10];    /* 10 integers--filled in by other code */\nint >idx;\nint >min",
    "380": "min = values[0];\nfor (idx = 1; 10 > idx; idx = idx + 1) {\n>  if (min > values[idx]) {\n>  >  min = values[idx];\n>  }\n}\n/* The minimum value from array is now in min. */",
    "381": "}",
    "382": "The code uses array notation, which we have not used previously in our \nclass, so let's first discuss the meaning of the code.",
    "383": "The code uses three variables.",
    "384": "The variable { values} represents the 10 values in our set.\nThe suffix ``[10]'' after the variable name tells the compiler that\nwe want an array of 10 integers ({ int}) indexed from 0 to 9.\nThese integers can be treated as 10 separate variables, but can be\naccessed using the single name ``{ values}'' along with an index\n(again, from 0 to 9 in this case).",
    "385": "The variable { idx} holds a loop index that we use to examine each\nof the values one by one in order to find the minimum value in the set.",
    "386": "Finally, the variable { min} holds the smallest known value as \nthe program examines each of the values in the set.",
    "387": "The program body consists of two statements.",
    "388": "We assume that some other piece of code---one not shown here---has \ninitialized the 10 values in our set before the code above executes.",
    "389": "The first statement initializes the\nminimum known value ({ min}) to the value stored at index 0 in the \narray ({ values[0]}).\nThe second statement is a loop in which the variable { index} \ntakes on values from 1 to 9.  For each value, an { if} statement\ncompares the current\nknown minimum with the value stored in the array at index given by the\n{ idx} variable.  If the stored value is smaller, the current known \nvalue (again, { min}) is updated to reflect the program's\nhaving found a smaller value.  When the loop finishes all nine iterations,\nthe variable { min} holds the smallest value among the set of 10 \nintegers stored in the { values} array.",
    "390": "As a first step towards designing an FSM to implement the code, we transform\nthe code into a flow chart, as shown to the right.  The program again begins\nwith initialization, which appears in the second column of the flow chart.  \nThe loop in the program translates to the third column of the flow chart, \nand the { if} statement to the middle comparison and update \nof { min}.",
    "391": "Our goal is now to design an FSM to implement the flow chart.  In order\nto do so, we want to leverage the same kind of abstraction that we used\nearlier, when extending our keyless entry system with a timer.  Although the\ntimer's value was technically also",
    "392": "{{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}",
    "393": "part of the FSM's state, we treated it\nas data and integrated it into our next-state decisions in only a couple\nof cases.",
    "394": "For our minimum value problem, we have two sources of data.  First, an\nexternal program supplies data in the form of a set of 10 integers.  If\nwe assume {32-bit} integers, these data technically form 320 input bits!\nSecond, as with the keyless entry system timer, we have data used internally\nby our FSM, such as the loop index and the current minimum value.  These\nare technically state bits.  For both types of data, we treat them\nabstractly as values rather than thinking of them individually as bits,\nallowing us to develop our FSM at a high-level and then to implement it \nusing the components that we have developed earlier in our course.",
    "395": "Now we are ready to design an FSM that implements the flow chart.\nWhat components do we need, other than our state logic?\nWe use registers and counters to implement the variables { idx}\nand { min} in the program.\nFor the array { values}, we use a {1632-bit} \nmemory.{We technically only need a {1032-bit} \nmemory, but we round up the size of the address space to reflect more\nrealistic memory designs; one can always optimize later.}\nWe need a comparator to implement the test for the { if} statement.\nWe choose to use a serial comparator, which allows us to illustrate again\nhow one logical high-level state can be subdivided into many actual states.\nTo operate the serial comparator, we make use of two shift registers that \npresent the comparator with one bit per cycle on each input, and a counter\nto keep track of the comparator's progress.",
    "396": "How do we identify high-level states from our flow chart?  Although\nthe flow chart attempts to break down the program into `simple' steps,\none step of a flow chart may sometimes require more than one state\nin an FSM.  Similarly, one FSM state may be able to implement several\nsteps in a flow chart, if those steps can be performed simultaneously.\nOur design illustrates both possibilities.",
    "397": "How we map flow chart elements into FSM states also depends to some \ndegree on what components we use, which is why we began with some discussion\nof components.  In practice, one can go back and forth between the two, \nadjusting components to better match the high-level states, and adjusting \nstates to better match the desired components.",
    "398": "Finally, note that we are only concerned with high-level states, so we do \nnot need to provide details (yet) down to the level of individual clock \ncycles, but we do want to define high-level states that can be implemented\nin a fixed number of cycles, or at least a controllable number of cycles.\nIf we cannot specify clearly when transitions occur from an FSM state, we\nmay not be able to implement the state.",
    "399": "Now let's go through the flow chart and identify states.  Initialization of\n{ min} and { idx} need not occur serially, and the result of the\nfirst comparison between { idx} and the constant 10 is known in advance,\nso we can merge all three operations into a single state, which we \ncall { INIT}.",
    "400": "We can also merge the updates of { min} and { idx} into a second\nFSM state, which we call { COPY}.  However, the update to { min} \noccurs only when the comparison ({ min > value[idx]}) is true.  \nWe can use logic to predicate execution of the update.  In other words, we \ncan use the output of the comparator, which is available after the comparator \nhas finished comparing the two values (in a high-level FSM state that we \nhave yet to define), to determine whether or not the register holding \n{ min} loads a new value in the { COPY} state.",
    "401": "Our model of use for this FSM involves external logic filling the memory\n(the array of integer values), executing the FSM ``code,'' and then\nchecking the answer.  To support this use model, we create a FSM state \ncalled { WAIT} for cycles in which the FSM has no work to do.\nLater, we also make use of an external input signal { START} \nto start the FSM\nexecution.  The { WAIT} state logically corresponds to the ``START'' \nbubble in the flow chart.",
    "402": "Only the test for the { if} statement remains.  Using a serial\ncomparator to compare two {32-bit} values requires 32 cycles.\nHowever, we need an additional cycle to move values into our shift \nregisters so that the comparator can see the first bit.  Thus our\nsingle comparison operation breaks into two high-level states.  In the\nfirst state, which we call { PREP}, we copy { min} to one\nof the shift registers, copy { values[idx]} to the other shift\nregister, and reset the counter that measures the cycles needed for\nour serial comparator.  We then move to a second high-level state,\nwhich we call { COMPARE}, in which we feed one bit per cycle\nfrom each shift register to the serial comparator.  The { COMPARE} \nstate",
    "403": "{{file=part3/figs/part3-min-flow-chart-states.eps,width=3.96in}}",
    "404": "executes for 32 cycles, after which the comparator\nproduces the one-bit answer that we need, and we can move to the\n{ COPY} state.  The association between the flow chart and the\nhigh-level FSM states is illustrated in the figure shown to the right\nabove.",
    "405": "We can now also draw an abstract state diagram for our FSM, as shown\nto the right.  The FSM begins in the { WAIT} state.  After external\nlogic fills the { values} array, it signals the FSM to begin by\nraising the { START} signal.  The FSM transitions into the \n{ INIT} state, and in the next cycle into the { PREP} state.\nFrom { PREP}, the FSM always moves to { COMPARE}, where it\nremains for 32 cycles while the serial comparator executes a comparison.\nAfter { COMPARE}, the FSM moves to the { COPY}",
    "406": "{{file=part3/figs/part3-min-state-diag.eps,width=3in}}",
    "407": "state, where\nit remains for one cycle.  The transition from { COPY} depends on\nhow many loop iterations have executed.  If more loop iterations remain,\nthe FSM moves to { PREP} to execute the next iteration.  If the\nloop is done, the FSM returns to { WAIT} to allow external logic\nto read the result of the computation.",
    "408": "Our high-level FSM design tells us what our components need to be able to\ndo in any given cycle.  For example, when we load new values into the shift\nregisters that provide bits to the serial comparator, we always copy \n{ min} into one shift register and { values[idx]} into the second.\nUsing this information, we can put together our components and simplify our\ndesign by fixing the way in which bits flow between them.",
    "409": "The figure at the right shows how we can organize our components.\nAgain, in practice, one goes back and forth thinking about states,\ncomponents, and flow from state to state.  In these notes, we \npresent only a completed design.",
    "410": "Let's take a detailed look at each of the components.",
    "411": "At the upper left of the figure is a {4-bit} binary counter called\n{ IDX} to hold the { idx}\nvariable.  The counter can be reset to 0 using the { RST} input.\nOtherwise, the { CNT} input controls whether or not the counter\nincrements its value.  With this counter design, we can force { idx} \nto 0 in the { WAIT} state and then count upwards in the { INIT}\nand { COPY} states.",
    "412": "{{file=part3/figs/part3-min-components.eps,width=3.84in}}",
    "413": "A memory labeled { VALUES} to hold the array { values} appears \nin the upper right of\nthe figure.  The read/write control for the memory is hardwired to 1 (read)\nin the figure, and the data input lines are unattached.  To integrate \nwith other logic that can operate our FSM, we need to add more \ncontrol logic to allow writing into the memory and to attach the data\ninputs to something that provides the data bits.  The address input of\nthe memory comes always from the { IDX} counter value; in other words,\nwhenever we access this memory by making use of the data output lines,\nwe read { values[idx]}.",
    "414": "In the middle left of the figure is a {32-bit} register for the \n{ min} variable.  It has a control input { LD} that \ndetermines whether or not it loads a new value at the end of the clock\ncycle.  If a new value is loaded, the new value always corresponds to\nthe output of the { VALUES} memory, { values[idx]}.  Recall\nthat { min} always changes in the { INIT} state, and may change\nin the { COPY} state.  But the new value stored in { min}\nis always { values[idx]}. \nNote also that when the FSM completes its task, the result of the \ncomputation is left in the { MIN} register for external logic to\nread (connections for this purpose are not shown in the figure).",
    "415": "Continuing downward in the figure, we see two right shift registers\nlabeled { A} and { B}.  Each has a control input { LD}\nthat enables a parallel load.  Register { A} loads from register\n{ MIN}, and register { B} loads from the memory data output\n({ values[idx]}).  These loads are needed in the { PREP}\nstate of our FSM.  When { LD} is low, the shift registers\nsimply shifts to the right.  The serial output { SO} makes the\nleast significant bit of each shift register available.  Shifting\nis necessary to feed the serial comparator in the { COMPARE} state.",
    "416": "Below register { A} is a {5-bit} binary counter called { CNT}.\nThe counter is used to control the serial comparator in the { COMPARE}\nstate.  A reset input { RST} allows it to be forced to 0 in the \n{ PREP} state.  When the counter value is exactly zero, the \noutput { Z} is high.",
    "417": "The last major component is the serial comparator, which is based on the\ndesign developed in Notes Set 3.1.  The two bits to be compared in \na cycle come from shift registers { A} and { B}.  The first\nbit indicator comes from the zero indicator of counter { CNT}.\nThe comparator actually produces two outputs ({ Z1} and { Z0}),\nbut the meaning of the\n{ Z1} output by itself is { A > B}.  In the diagram,\nthis signal has been labeled { THEN}.",
    "418": "There are two additional elements in the figure that we have yet to discuss.\nEach simply compares the value in a register with a fixed constant and \nproduces a {1-bit} signal.  When the FSM finishes an iteration of\nthe loop in the { COPY} state, it must check the loop condition\n({ 10 > idx}) and move either to the { PREP} state or, when\nthe loop finishes, to the { WAIT} state to let the external logic\nread the answer from the { MIN} register.  The loop is done when the\ncurrent iteration count is nine, so we compare { IDX} with nine to\nproduce the { DONE} signal.  The other constant comparison is between\nthe counter { CNT} and the value 31 to produce the { LAST} signal,\nwhich indicates that the serial comparator is on its last cycle of \ncomparison.  In the cycle after { LAST} is high, the { THEN} \noutput of the comparator indicates whether or not { A > B}.",
    "419": "One can think of the components and the interconnections between them\nas enabling the movement of data between registers, while the high-level \nFSM controls which data move from register to register in each cycle.",
    "420": "With this model in mind, we call the components and interconnections\nfor our design the { datapath}---a term that we will see again when\nwe examine the parts of a computer in the coming weeks.",
    "421": "The datapath requires several inputs to control the operation of the\ncomponents---these we can treat as outputs of the FSM.\nThese signals allow the FSM to control the motion of data in the \ndatapath, so we call them { control signals}.",
    "422": "Similarly, the datapath produces several outputs that we can treat\nas inputs to the FSM.",
    "423": "The tables below summarize the control signals (left) and outputs (right)\nfrom the datapath for our FSM.",
    "424": "[t]\n{|c|l|}\ndatapath& \ninput& {|c|} \n{ IDX.RST}& reset { IDX} counter to 0\n{ IDX.CNT}& increment { IDX} counter\n{ MIN.LD}& load new value into { MIN} register\n{ A.LD}& load new value into shift register { A}\n{ B.LD}& load new value into shift register { B}\n{ CNT.RST}& reset { CNT} counter",
    "425": "[t]\n{|c|l|c|}\ndatapath& &\noutput& {|c|}& based on \n{ DONE}& last loop iteration finished& { IDX = 9}\n{ LAST}& serial comparator executing& { CNT = 31}\n&    last cycle&\n{ THEN}& { if} statement condition true& { A > B} \n{}\n{}",
    "426": "Using the datapath controls signals and outputs, we can now write a more\nformal state transition table for the FSM, as shown below.",
    "427": "The ``actions'' column of the table\nlists the changes to register and counter values\nthat are made in each of the FSM states.  The notation used to represent\nthe actions is called { register transfer language} ({ RTL}).\nThe meaning of an individual action is similar to the meaning of the \ncorresponding statement from our C code or from the flow chart.\nFor example, in the { WAIT} state, ``{ IDX  0}''\nmeans the same thing as ``{ idx = 0;}''.  In particular, both mean\nthat the value currently stored in the { IDX} counter is overwritten \nwith the number 0 (all 0 bits).",
    "428": "The meaning of RTL is slightly different from the usual interpretation of\nhigh-level programming languages, however, in terms of when the actions\nhappen.  A list of C statements is generally executed one at a time.",
    "429": "In contrast, the entire list of RTL actions",
    "430": "{|c|l|c|c|}\nstate& {|c|}{actions (simultaneous)}& condition& next state \n{ WAIT}& { IDX}  0 (to read { VALUES[0]} in { INIT})& { START}& { INIT} \n&& {{ START}}& { WAIT} \n{ INIT}& { MIN}  { VALUES[IDX]} ({ IDX} is 0 in this state)& (always)& { PREP} \n& { IDX}  { IDX} + 1& & \n{ PREP}& { A}  { MIN}& (always)& { COMPARE}\n& { B}  { VALUES[IDX]}& &\n& { CNT}  0& & \n{ COMPARE}& run serial comparator& { LAST}& { COPY}\n&& {{ LAST}}& { COMPARE} \n{ COPY}& { THEN}: { MIN}  { VALUES[IDX]}& { DONE}& { WAIT} \n& { IDX}  { IDX} + 1& {{ DONE}}& { PREP}",
    "431": "for an FSM state is executed\nsimultaneously, at the end of the clock cycle.  As you know, an FSM moves \nfrom its current state into a new state at the end of every clock cycle,\nso actions during different cycles usually are associated with different \nstates.\nWe can, however, change the value in more than one register at the end \nof the same clock cycle, so we can execute more than one RTL action in\nthe same state, so long as the actions do not exceed the capabilities\nof our datapath (the components must be able to support the simultaneous \nexecution of the actions).  Some care must be taken with states that \nexecute for more than one cycle to ensure that repeating the \nRTL actions is appropriate.  In our design, only the { WAIT} and\n{ COMPARE} states execute for more than one cycle.  The { WAIT}\nstate resets the { IDX} counter repeatedly, which causes no problems.\nThe { COMPARE} statement has no RTL actions---all of the shifting,\ncomparison, and counting activity needed to do its work occurs within \nthe datapath itself.",
    "432": "One additional piece of RTL syntax needs explanation.  In the { COPY}\nstate, the first action begins with ``{ THEN}:,'' which means that the \nprefixed RTL action occurs only when the { THEN} signal is high.\nRecall that the { THEN} signal indicates that the comparator has\nfound { A > B}, so the equivalent C code is ``{ if (A > B) \n{min = values[idx]}}''.",
    "433": "Let's think about the representation for the FSM states.  The FSM has \nfive states, so we could use as few as three flip-flops.  Instead, we choose\nto use a { one-hot encoding}, in which any valid bit pattern has exactly\none 1 bit.  In other words, we use five flip-flops instead of three,\nand our states are represented with the bit patterns 10000, 01000, 00100,\n00010, and 00001.",
    "434": "The table below shows the mapping from each high-level state to \nboth the five-bit encoding for the state as well as the six control signals \nneeded for the datapath.  For each state, the values of the control signals\ncan be found by examining the actions necessary in that state.",
    "435": "{\n{|c|c|cccccc|}\nstate& { S_4S_3S_2S_1S_0}& { IDX.RST}& { IDX.CNT}& { MIN.LD}& { A.LD}& { B.LD}& { CNT.RST} \n{ WAIT}& { 1 0 0 0 0}& 1& 0& 0& 0& 0& 0\n{ INIT}& { 0 1 0 0 0}& 0& 1& 1& 0& 0& 0\n{ PREP}& { 0 0 1 0 0}& 0& 0& 0& 1& 1& 1\n{ COMPARE}& { 0 0 0 1 0}& 0& 0& 0& 0& 0& 0\n{ COPY}& { 0 0 0 0 1}& 0& 1& { THEN}& 0& 0& 0 \n{}",
    "436": "}",
    "437": "The { WAIT} state needs to set { IDX} to 0 but need not affect \nother register or counter values, so { WAIT} produces a 1 only for\n{ IDX.RST}.  The { INIT} state needs to load { values[0]} into \nthe { MIN} register while simultaneously incrementing the { IDX}\ncounter (from 0 to 1), so { INIT} produces 1s for { IDX.CNT}\nand { MIN.LD}.  The { PREP} state loads both shift registers\nand resets the counter { CNT} by producing 1s for { A.LD},\n{ B.LD}, and { CNT.RST}.  The { COMPARE} state does not\nchange any register values, so it produces all 0s.  Finally, the { COPY}\nstate increments the { IDX} counter while simultaneously loading a\nnew value into the { MIN} register.  The { COPY} state produces 1\nfor { IDX.CNT}, but must use the signal { THEN} coming from the\ndatapath to decide whether or not { MIN} is loaded.",
    "438": "The advantage of a one-hot encoding becomes obvious when we write\nequations for the six control signals and the next-state logic, as shown\nto the right.",
    "439": "Implementing the logic to complete our design now requires only a handful \nof small logic gates.",
    "440": "{eqnarray*}\n{ IDX.RST} & = & { S}_4\n{ IDX.CNT} & = & { S}_3 + { S}_0\n{ MIN.LD} & = & { S}_3 + { S}_0  { THEN}\n{ A.LD} & = & { S}_2\n{ B.LD} & = & { S}_2\n{ CNT.RST} & = & { S}_2\n{eqnarray*}",
    "441": "{eqnarray*}\n{ S}_4^+ & = & { S}_4  { START} + { S}_0  { DONE}\n{ S}_3^+ & = & { S}_4  { START}\n{ S}_2^+ & = & { S}_3 + { S}_0  { DONE}\n{ S}_1^+ & = & { S}_2 + { S}_1  { LAST}\n{ S}_0^+ & = & { S}_1  { LAST}\n{eqnarray*}",
    "442": "Notice that the terms in each control signal can be read directly from \nthe rows of the state table and OR'd together.  The terms in each of the\nnext-state equations represent the incoming arcs for the corresponding\nstate.  For example, the { WAIT} state has one self-loop (the first\nterm) and a transition arc coming from the { COPY} state when the\nloop is done.",
    "443": "These expressions complete our design.",
    "444": "This set of notes builds on the keyless entry control FSM that we\ndesigned earlier.  In particular, we use a counter to make the alarm\ntime out, turning itself off after a fixed amount of time.  The goal\nof this extension is to illustrate how we can make use of components\nsuch as registers and counters as building blocks for our FSMs\nwithout fully expanding the design to explicitly illustrate all\npossible states.",
    "445": "To begin, let's review the FSM that we designed earlier for keyless \nentry.  The state transition diagram for our design is replicated to \nthe right.",
    "446": "The four states are labeled with state bits and output bits,\nS_1S_0/DRA, where D indicates that the driver's door should\nbe unlocked, R indicates that the rest of the doors should be\nunlocked, and A indicates that the alarm should be on.",
    "447": "Transition arcs in the diagram are labeled with concise versions of \nthe inputs ULP (using don't cares), where U represents an unlock\nbutton, L represents a lock button, and P represents a panic \nbutton.",
    "448": "In this design, once a user presses the panic button P, the alarm\nsounds until the user presses the",
    "449": "{file=part3/figs/ke-trans-diag-brief.eps,width=4.2in}",
    "450": "lock button L to turn it off.\nInstead of sounding the alarm indefinitely, we might want to turn\nthe alarm off after a fixed amount of time.  In other words, after \nthe system has been in the ALARM state for, say, thirty or sixty seconds,\nwe might want to move back to the LOCKED state even if the user has\nnot pushed the lock button.  The blue annotation in the diagram indicates\nthe arc that we must adjust.  But thirty or sixty seconds is a large \nnumber of clock cycles, and our FSM must keep track of the time.\nDo we need to draw all of the states?",
    "451": "Instead of following the design process that we outlined earlier, let's\nthink about how we can modify our existing design to incorporate the new\nfunctionality.  In order to keep track of time, we use a binary counter.",
    "452": "Let's say that we want our timeout to be T cycles.",
    "453": "When we enter the alarm state, we want to set the counter's value \nto T-1, then let the counter count down until it reaches 0, at \nwhich point a timeout occurs.",
    "454": "To load the initial value, our counter should have a parallel load \ncapability that sets the counter value when input LD=1.  When LD=0,\nthe counter counts down.",
    "455": "The counter also has an output Z that indicates that the counter's \nvalue is currently zero, which we can use to indicate a timeout on\nthe alarm.",
    "456": "You should be able to build such a counter based on what you have learned\nearlier in the class.  Here, we will assume that we can just make use of\nit.",
    "457": "How many bits do we need in our counter?  The answer depends on T.\nIf we add the counter to our design, the state of the counter is \ntechnically part of the state of our FSM, but we can treat it\nsomewhat abstractly.  For example, we only plan to make use of the \ncounter value in the ALARM state, so we ignore the counter bits in the\nthree other states.  In other words, S_1S_0=10 means that the system \nis in the LOCKED state regardless of the counter's value.",
    "458": "We expand the ALARM state into T separate states based on the value\nof the counter.  As shown to the right, we name the states ALARM(1) \nthrough ALARM(T).  All of these alarm states use S_1S_0=01, but\nthey can be differentiated using a ``timer'' (the counter value).",
    "459": "We need to make design decisions about how the arcs entering and\nleaving the ALARM state in our original design should be used once\nwe have incorporated the timeout.  As a first step, we decide that\nall arcs entering ALARM from other states now enter ALARM(1).  \nSimilarly, if the user presses the panic button P in any of the\nALARM(t) states, the system returns to ALARM(1).  Effectively, \npressing the panic button resets the timer.",
    "460": "The only arc leaving the ALARM state goes to the LOCKED state on \nULP=x10.  We replicate this arc for all ALARM(t) states: the\nuser can push the lock button at any time to silence the alarm.",
    "461": "Finally, the self-loop back to the ALARM state on ULP=x00 becomes\nthe countdown arcs in our expanded states, taking ALARM(t) to ALARM(t+1),\nand ALARM(T) to LOCKED.",
    "462": "Now that we have a complete specification for the extended design, we\ncan implement it.  We want to reuse our original design as much as possible,\nbut we have three new features that must be considered.  First, when\nwe enter the ALARM(1) state, we need to set the counter value to T-1.\nSecond, we need the counter value to count downward while in the ALARM\nstate.  Finally, we need to move back to the LOCKED state when a timeout\noccurs---that is, when the counter reaches zero.",
    "463": "{file=part3/figs/ke-alarm-expansion.eps,width=1.75in}",
    "464": "The first problem is fairly easy.  Our counter supports parallel load,\nand the only value that we need to load is T-1, so we apply the constant\nbit pattern for T-1 to the load inputs and raise the LD input whenever\nwe enter the ALARM(1) state.  In our original design, we chose to enter\nthe ALARM state whenever the user pressed P, regardless of the other\nbuttons.  Hence we can connect P directly to our counter's LD input.",
    "465": "The second problem is handled by the counter's countdown functionality.\nIn the ALARM(t) states, the counter will count down each cycle, moving\nthe system from ALARM(t) to ALARM(t+1).",
    "466": "The last problem is slightly trickier, since we need to change S_1S_0.\nNotice that S_1S_0=01 for the ALARM state and S_1S_0=00 for the LOCKED\nstate.  Thus, we need only force S_0 to 0 when a timeout occurs.\nWe can use a single {2-to-1} multiplexer for this purpose.  The\n``0'' input of the mux comes from the original S_0^+ logic, and the\n``1'' input is a constant 0.  All other state logic remains unchanged.  \nWhen does a timeout occur?",
    "467": "First, we must be in the ALARM(T) state, so S_1S_0=01 and the \ncounter's Z output is raised.  Second, the input combination must\nbe ULP=xx0---notice that both ULP=x00 and ULP=x10 return to \nLOCKED from ALARM(T).  A single, four-input AND gate thus suffices to\nobtain the timeout signal, {S_1}S_0Z, which we connect \nto the select input of the mux between the S_0^+ logic and the\nS_0 flip-flop.",
    "468": "The extension thus requires only a counter, a mux, and a gate, as shown below.",
    "469": "{{file=part3/figs/ke-alarm-exp-impl.eps,width=2.65in}}",
    "470": "The third part of our class builds upon the basic combinational and\nsequential logic elements that we developed in the second part.",
    "471": "After discussing a simple application of stored state\nto trade between area and performance,",
    "472": "we introduce a powerful abstraction for formalizing and reasoning about\ndigital systems, the Finite State Machine (FSM).",
    "473": "General FSM models are broadly applicable in a range of engineering\ncontexts, including not only hardware and software design but also\nthe design of control systems and distributed systems.  We limit our\nmodel so as to avoid circuit timing issues in your first exposure, but\nprovide some amount of discussion as to how, when, and why you should \neventually learn the more sophisticated models.",
    "474": "Through development a range of FSM examples, we illustrate important \ndesign issues for these systems and motivate a couple of more advanced \ncombinational logic devices that can be used as building blocks.",
    "475": "Together with the idea of memory, another form of stored state,\nthese elements form the basis for development of our first computer.",
    "476": "At this point we return to the textbook, in which Chapters 4 and 5\nprovide a solid introduction to the von Neumann model of computing systems\nand the {LC-3} (Little Computer, version 3) instruction set \narchitecture.  By the end of this part of the course, you will have\nseen an example of the boundary between hardware and software, and will\nbe ready to write some instructions yourself.",
    "477": "In this set of notes, we cover the first few parts of this material.\nWe begin by describing the conversion of bit-sliced designs into \nserial designs, which store a single bit slice's output in \nflip-flops and then feed the outputs back into the bit slice in the next\ncycle.  As a specific example, we use our bit-sliced comparator \nto discuss tradeoffs in area and performance.  We introduce\nFinite State Machines and some of the tools used to design them,\nthen develop a handful of simple counter designs.  Before delving\ntoo deeply into FSM design issues, we spend a little time discussing\nother strategies for counter design and placing the material covered\nin our course in the broader context of digital system design.",
    "478": "Remember that\n{ sections marked with an asterisk are provided solely for your\ninterest,} but you may need to learn this material in later\nclasses.",
    "479": "In previous notes, we discussed and illustrated the development of bit-sliced\nlogic, in which one designs a logic block to handle one bit of a multi-bit\noperation, then replicates the bit slice logic to construct a design for \nthe entire operation.  We developed ripple carry adders in this way in \nNotes Set 2.3 and both unsigned and 2's complement comparators in \nNotes Set 2.4.",
    "480": "Another interesting design strategy is { serialization}: rather than \nreplicating the bit slice, we can use flip-flops to store the bits passed\nfrom one bit slice to the next, then present the stored bits { to the\nsame bit slice} in the next cycle.  Thus, in a serial design, we only\nneed one copy of the bit slice logic!  The area needed for a serial design\nis usually much less than for a bit-sliced design, but such a design is\nalso usually slower.  After illustrating the general design strategy,\nwe'll consider these tradeoffs more carefully\nin the context of a detailed example.",
    "481": "Recall the general bit-sliced design approach, as illustrated to the right.",
    "482": "Some number of copies of the logic for a single bit slice are connected\nin sequence.  Each bit slice accepts P bits of operand input and\nproduces Q bits of external output.  Adjacent bit slices receive\nan additional M bits of information from the previous bit slice\nand pass along M bits to the next bit slice, generally using some\nrepresentation chosen by the designer.",
    "483": "{file=part3/figs/gen-slice-comp.eps,width=3.8in}",
    "484": "The first bit slice is initialized\nby passing in constant values, and some calculation may be performed\non the final bit slice's results to produce R bits more external output.",
    "485": "We can transform this bit-sliced design to a serial design with a single\ncopy of the bit slice logic, M+Q flip-flops, and M gates (and sometimes\nan inverter).  The strategy is illustrated on the right below.  A single\ncopy of the bit slice operates on one set of P external input bits \nand produces one set of Q external output bits each clock cycle.  In\nthe design shown, these output bits are available during the next \ncycle, after they have been stored in the flip-flops. \nThe M bits to be passed to the ``next'' bit slice are also \nstored in flip-flops,\nand in the next cycle are provided back to the same physical bit slice\nas inputs.  The first cycle of a multi-cycle operation\nmust be handled slightly differently, so we\nadd selection logic and an control signal, F.  For the first cycle, we\napply F=1, and the initial values are passed into the bit slice.  \nFor all other bits, we apply F=0, and the values stored in the flip-flops\nare returned to the bit slice's inputs.  After all bits have passed through\nthe bit slice---after N cycles for an {N-bit} design---the\nfinal M bits are stored in the flip-flops, and the results are calculated\nby the output logic.",
    "486": "{",
    "487": "{file=part3/figs/init-ser-slice.eps,width=1.4in}",
    "488": "{file=part3/figs/ser-slice-comp.eps,width=3.25in}",
    "489": "}",
    "490": "The selection logic merits explanation.  Given that the original design\ninitialized the bits to constant values (0s or 1s), we need only simple\nlogic for selection.  The two drawings on the left above illustrate \nhow {B_i}, the complemented flip-flop output for a bit i, can be\ncombined with the first-cycle signal F to produce an appropriate input\nfor the bit slice.  Selection thus requires one extra gate for each of \nthe M inputs, and we need an inverter for F if any of the initial \nvalues is 1.",
    "491": "We now apply the general strategy to a specific example, our bit-sliced\nunsigned comparator from Notes Set 2.4.  The result is shown to the right.\nIn terms of the general model, the single comparator bit slice accepts P=2\nbits of inputs each cycle, in this case a single bit from each of the two\nnumbers being compared, presented to the bit slice in increasing order\nof significance.  The bit slice produces no external output other than\nthe final result (Q=0).  Two bits (M=2) are produced each cycle by \nthe bit slice and stored",
    "492": "{file=part3/figs/ser-compare.eps,width=3.5in}",
    "493": "into flip flops B_1 and B_0.  These bits\nrepresent the relationship between the two numbers compared so far\n(including only the bit already seen by the comparator bit slice).\nOn the first cycle, when the least significant bits of A and B are\nbeing fed into the bit slice, we set F=1, which forces the C_1 \nand C_0 inputs of the bit slice to 0 independent of the values stored\nin the flip-flops.  In all other cycles, F=0, and the NOR gates\nset C_1=B_1 and C_0=B_0.  Finally, after N cycles for an {N-bit}\ncomparison, the output logic---in this case simply wires, as shown \nin the dashed box---places the result of the comparison on \nthe Z_1 and Z_0 outputs (R=2 in the general model).  \nThe result is encoded in the \nrepresentation defined for constructing the bit slice (see Notes Set 2.4,\nbut the encoding does not matter here).",
    "494": "How does the serial design compare with the bit-sliced design?",
    "495": "As an estimate of area, let's count gates.  Our optimized \ndesign is replicated to the right for convenience.",
    "496": "Each bit slice requires six {2-input} gates and two inverters.",
    "497": "Assume that each \nflip-flop requires eight {2-input} gates and two inverters,\nso the serial design overall requires 24 gates",
    "498": "{file=part3/figs/comparator-opt-nn.eps,width=4.1in}",
    "499": "and six inverters to handle any number of bits.",
    "500": "Thus, for any number of bits N, the serial design is smaller\nthan the bit-sliced design, and the benefit grows with N.",
    "501": "What about performance?  In Notes Set 2.4, we counted gate delays for \nour bit-sliced design.  The path from A or B to the outputs is\nfour gate delays, but the C to Z paths are all two gate delays.\nOverall, then, the bit-sliced design requires 2N+2 gate delays \nfor N bits.  What about the serial design?",
    "502": "The performance of the serial design is likely to be much worse\nfor three reasons.",
    "503": "First, all paths in the design matter, not just the paths from bit\nslice to bit slice.  None of the inputs can be assumed to be available\nbefore the start of the clock cycle, so we must consider all paths\nfrom input to output.",
    "504": "Second, we must also count gate delays for the\nselection logic as well as the gates embedded in the flip-flops.",
    "505": "Finally, the result of these calculations may not matter, since the\nclock speed may well be limited by other logic elsewhere in the\nsystem.  If we want a common clock for all of our logic, the clock\nmust not go faster than the slowest element in the entire system, or\nsome of our logic will not work properly.",
    "506": "What is the longest path through our serial comparator?",
    "507": "Let's assume that \nthe path through a flip-flop is eight gate delays, with four on each \nside of the clock's rising edge.  The inputs A and B are\nlikely to be driven by flip-flops elsewhere in our system, so\nwe conservatively count four gate delays to A and B and\nfive gate delays to C_1 and C_0 (the extra one comes from\nthe selection logic).  The A and B paths thus dominate inside\nthe bit slice, adding four more gate delays to the outputs Z_1\nand Z_0.  Finally, we add the last four gate delays to flip\nthe first latch in the flip-flops for a total of 12 gate delays.",
    "508": "If we assume that our serial comparator limits the clock frequency\n(that is, if everything else in the system can use a faster clock),\nwe take 12 gate delays per cycle, or 12N gate delays to compare\ntwo {N-bit} numbers.",
    "509": "You might also notice that adding support for 2's complement is no \nlonger free.  We need extra logic to swap the A and B inputs\nin the cycle corresponding to the sign bits of A and B.  In\nother cycles, they must remain in the usual order.  This extra\nlogic is not complex, but adds further delay to the paths.",
    "510": "The bit-sliced and serial designs represent two extreme points in \na broad space of design possibilities.  Optimization of the entire\nN-bit logic function (for any metric) represents a third extreme.\nAs an engineer, you should realize that you can design systems\nanywhere in between these points as well.  At the end of Notes Set 2.4,\nfor example, we showed a design for a logic slice that compares \ntwo bits at a time.  In general, we can optimize logic for any \nnumber of bits and then apply multiple copies of the resulting\nlogic in space (a generalization of the bit-sliced approach),\nor in time (a generalization of the serialization approach),\nor in a combination of the two.",
    "511": "Sometimes these tradeoffs may happen at a higher level.\nAs mentioned in Notes Set 2.3, computer software uses \nthe carry out of an adder to perform addition of larger \ngroups of bits (over multiple clock cycles)\nthan is supported by the processor's adder hardware.",
    "512": "In computer system design, engineers often design hardware elements\nthat are general enough to support this kind of extension in software.",
    "513": "As a concrete example of the possible tradeoffs,\nconsider a serial comparator design based on the {2-bit} slice variant.",
    "514": "This approach leads to a serial design with 24 gates and 10 inverters, \nwhich is not much\nlarger than our earlier serial design.  In terms of gate delays,\nhowever, the new design is identical, meaning that we finish a comparison\nin half the time.  More realistic area and timing metrics show\nslightly more difference between the two designs.  These differences can \ndominate the \nresults if we blindly scale the idea to handle more bits without \nthinking carefully about the design.  Neither many-input gates\nnor gates driving many outputs work well in practice.",
    "515": "A { finite state machine} (or { FSM}) is a model for understanding\nthe behavior of a system by describing the system as occupying one of a\nfinite set of states, moving between these states in response to\nexternal inputs, and producing external outputs.  In any given state,\na particular input may cause the FSM to move to another state; this \ncombination is called a { transition rule}.",
    "516": "An FSM comprises five parts: a finite set of states, a set of \npossible inputs, a set of possible outputs, a set of transition rules,\nand methods for calculating outputs.",
    "517": "When an FSM is implemented as a digital system, all states must be\nrepresented as patterns using a fixed number of bits, all inputs must\nbe translated into bits, and all outputs must be translated into bits.",
    "518": "For a digital FSM, transition rules must be { complete}; in other words,\ngiven any state of the FSM, and any pattern of input bits,\na transition must be defined from that state to another state \n(transitions from a state to itself, called { self-loops}, \nare acceptable).",
    "519": "And, of course, calculation of outputs for a digital FSM reduces\nto Boolean logic expressions.",
    "520": "In this class, we focus on clocked synchronous FSM implementations,\nin which the FSM's internal state bits are stored in flip-flops.",
    "521": "In this section, we introduce the tools used to describe, develop, and\nanalyze implementations of FSMs with digital logic.  In the next\nfew weeks, we will show you how an FSM can serve as the central control\nlogic in a computer.  At the same time, we will illustrate connections\nbetween FSMs and software and will make some connections with other\nareas of interest in ECE, such as the design and analysis of digital \ncontrol systems.",
    "522": "The table below gives a { list of abstract states} for a typical \nkeyless entry system for a car.  In this case, we have merely named the\nstates rather than specifying the bit patterns to be used for each\nstate---for this reason, we refer to them as abstract states.\nThe description of the states in the first column is an optional element\noften included in the early design stages for an FSM, when identifying\nthe states needed for the design.\nA list may also include the outputs for each state.  Again, in\nthe list below, we have specified these outputs abstractly.",
    "523": "By including outputs for each state,\nwe implicitly assume that outputs depend only on\nthe state of the FSM.  We discuss this assumption in more detail\nlater in these notes (see ``Machine Models''), \nbut will make the assumption throughout our class.",
    "524": "{",
    "525": "meaning& state& driver's door& other doors& alarm on \nvehicle locked& LOCKED& locked& locked& no\ndriver door unlocked& DRIVER& unlocked& locked& no\nall doors unlocked& UNLOCKED& unlocked& unlocked& no\nalarm sounding& ALARM& locked& locked& yes",
    "526": "}",
    "527": "Another tool used with FSMs is the { next-state table} (sometimes\ncalled a { state transition table}, or just a { state table}), \nwhich maps the current state and input combination into the next state of \nthe FSM.  The abstract variant shown below outlines desired behavior at\na high level, and is often ambiguous, incomplete, and even inconsistent.\nFor example, what happens if a user pushes two buttons?  What happens\nif they push unlock while the alarm is sounding?  These questions \nshould eventually be considered.  However, we can already start to see \nthe intended use of the design: starting from a locked car, a user\ncan push ``unlock'' once to gain entry to the driver's seat, or push\n``unlock'' twice to open the car fully for passengers.  To lock the\ncar, a user can push the ``lock'' button at any time.  And, if a user\nneeds help, pressing the ``panic'' button sets off an alarm.",
    "528": "{",
    "529": "state& action/input& next state \nLOCKED& push ``unlock''& DRIVER\nDRIVER& push ``unlock''& UNLOCKED\n(any)& push ``lock''& LOCKED\n(any)& push ``panic''& ALARM",
    "530": "}",
    "531": "A { state transition diagram} (or { transition diagram}, or\n{ state diagram}),\nas shown to the right, illustrates the\ncontents of the next-state table graphically, with each state drawn\nin a circle, and arcs between states labeled with the input combinations\nthat cause these transitions from one state to another.",
    "532": "Putting the FSM design into this graphical form does not solve the\nproblems with the abstract model.  The questions that we asked\nin regard to the next-state table remain unanswered.",
    "533": "Implementing an FSM using digital logic requires that we translate\nthe design into bits, eliminate any ambiguity, and complete the\nspecification.  How many internal bits should we use?  What are the\npossible input values, and how are their meanings represented in bits? \nWhat are the possible output values, and how are their meanings\nrepresented in bits?  We will consider these questions for several\nexamples in the coming weeks.",
    "534": "{file=part3/figs/ke-abs-trans-diag.eps,width=3.45in}",
    "535": "For now, we simply define answers for our example design, the keyless\nentry system.  Given four states, we need at \nleast _2(4)=2 bits of internal state, which we\nstore in two flip-flops and call S_1S_0.  The table below lists input\nand output signals and defines their meaning.",
    "536": "{",
    "537": "outputs&D& driver door; 1 means unlocked\n&R& other doors (remaining doors); 1 means unlocked\n&A& alarm; 1 means alarm is sounding\ninputs& U& unlock button; 1 means it has been pressed\n& L& lock button; 1 means it has been pressed\n& P& panic button; 1 means it has been pressed",
    "538": "}",
    "539": "We can now choose a representation for our states and rewrite the list\nof states, using bits both for the states and for the outputs.  We\nalso include the meaning of each state for clarity in our example.\nNote that we can choose the internal representation in any way.  Here\nwe have matched the D and R outputs when possible to simplify\nthe output logic needed for the implementation.  The order of states\nin the list is not particularly important, but should be chosen for\nconvenience and clarity (including transcribing bits into to {K-maps}, \nfor example).",
    "540": "{",
    "541": "& & & driver's door& other doors& alarm on\nmeaning& state& S_1S_0& D& R& A \nvehicle locked& LOCKED& 00& 0& 0& 0\ndriver door unlocked& DRIVER& 10& 1& 0& 0\nall doors unlocked& UNLOCKED& 11& 1& 1& 0\nalarm sounding& ALARM& 01& 0& 0& 1",
    "542": "}",
    "543": "We can also rewrite the next-state table in terms of bits.  We use\nGray code order on both axes, as these orders make it more \nconvenient to use {K-maps}.  The values represented in this table \nare the next FSM state given the current state S_1S_0 and the\ninputs U, L, and P.  Our symbols for the next-state bits\nare S_1^+ and S_0^+.  The ``+'' superscript\nis a common way of expressing\nthe next value in a discrete series, here induced by the use of\nclocked synchronous logic in implementing the FSM.  In other words,\nS_1^+ is the value of S_1 in the next clock cycle, and S_1^+\nin an FSM implemented as a digital system is a Boolean expression\nbased on the current state and the inputs.  For our example problem, we\nwant to be able to write down expressions for S_1^+(S_1,S_0,U,L,P) \nand S_1^+(S_1,S_0,U,L,P), as well as expressions for the \noutput logic D(S_1,S_0), R(S_1,S_0), and A(S_1,S_0).",
    "544": "{\n{c|cccccccc}\ncurrent state & {|c}{ULP}\nS_1S_0& 000& 001& 011& 010& 110& 111& 101& 100 \n00& 00& 01& 01& 00& 00& 01& 01& 10\n01& 01& 01& 01& 00& 00& 01& 01& 01\n11& 11& 01& 01& 00& 00& 01& 01& 11\n10& 10& 01& 01& 00& 00& 01& 01& 11",
    "545": "}",
    "546": "In the process of writing out the next-state table, \nwe have made decisions for all of the questions that we asked earlier\nregarding the abstract state table.  These decisions are also reflected\nin the complete state transition diagram shown to the right.\nThe states have been extended with state bits and output bits,\nas S_1S_0/DRA.\nYou should recognize that we can\nalso leave some questions unanswered by placing x's (don't cares)\ninto our table.  However, you should also understand at this point\nthat any implementation will produce bits, not x's, so we must be\ncareful not to allow arbitrary choices unless any of the choices\nallowed is indeed acceptable for our FSM's purpose.  We will discuss\nthis process and the considerations necessary as we cover more FSM\ndesign examples.",
    "547": "{file=part3/figs/ke-trans-diag.eps,width=4.2in}",
    "548": "We have deliberately omitted calculation of expressions for the\nnext-state variables S_1^+ and S_0^+,\nand for the outputs D,\nR, and A.  We expect that you are able to do so from the \ndetailed state table above, and may assign such an exercise as\npart of your homework.",
    "549": "A { counter} is a clocked sequential circuit with a state diagram\nconsisting of a single logical cycle.",
    "550": "Not all counters are synchronous.\nIn other words, not all flip-flops in a counter are required to use \nthe same clock signal.  A counter in which all flip-flops do \nutilize the same clock\nsignal is called a { synchronous counter}.",
    "551": "Except for a brief introduction to other types of counters in the\nnext section, our class focuses entirely on clocked synchronous designs,\nincluding counters.",
    "552": "{The definition ignores transition\n arcs resulting from functionalit such as a reset signal that\n forces the counter back into an initial state,",
    "553": "The design of synchronous counter circuits is a fairly straightforward\nexercise given the desired cycle of output patterns.",
    "554": "The task can be more complex if the internal state bits\nare allowed to differ from the output bits, so for now\nwe assume that output Z_i is equal to internal bit S_i.\nNote that distinction between internal states and outputs is necessary \nif any output pattern appears more than once in the desired cycle.",
    "555": "The cycle of states shown to the right corresponds to the\nstates of a {3-bit} binary counter.  The numbers in the states\nrepresent both internal state bits S_2S_1S_0 and output bits Z_2Z_1Z_0.\nWe transcribe this diagram into the next-state table shown on the left below.\nWe then write out {K-maps} for\nthe next state bits S_2^+, S_1^+, and S_0^+, as shown to the right,\nand use the {K-maps} to find expressions for these variables in\nterms of the current state.",
    "556": "{file=part3/figs/lec16-9.eps,width=1.5in}",
    "557": "{",
    "558": "{ccc|ccc}\nS_2& S_1& S_0& S_2^+& S_1^+& S_0^+ \n0& 0& 0& 0& 0& 1\n0& 0& 1& 0& 1& 0\n0& 1& 0& 0& 1& 1\n0& 1& 1& 1& 0& 0\n1& 0& 0& 1& 0& 1\n1& 0& 1& 1& 1& 0\n1& 1& 0& 1& 1& 1\n1& 1& 1& 0& 0& 0",
    "559": "{file=part3/figs/sbin3-s2.eps,width=1in}\n{file=part3/figs/sbin3-s1.eps,width=1in}\n{file=part3/figs/sbin3-s0.eps,width=1in} {-10pt}\n{eqnarray*}\nS_2^+ =& {S_2}S_1S_0+S_2{S_1}+S_2{S_0} &= S_2(S_1S_0) \nS_1^+ =& S_1{S_0}+{S_1}S_0 &= S_1{S_0} \nS_0^+ =& {S_0} &= S_0 \n{eqnarray*}",
    "560": "}",
    "561": "The first form of the expression for each next-state variable is\ntaken directly from the corresponding {K-map}.  We have rewritten\neach expression to make the emerging pattern more obvious.",
    "562": "We can also derive the pattern intuitively by asking the following:",
    "563": "given a binary counter in state S_{N-1}S_{N-2} S_{j+1}S_jS_{j-1}\nS_1S_0, when does S_j change in the subsequent state?  The answer,\nof course, is that S_j changes when all of the\nbits below S_j are 1.  Otherwise, S_j remains the\nsame in the next state.  We thus \nwrite S_j^+=S_j (S_{j-1} S_1S_0) and\nimplement the counter as shown below for a {4-bit} design.\nNote that the \nusual order of\noutput bits along the bottom is reversed in the figure, with the most\nsignificant bit at the right rather than the left.",
    "564": "{{file=part3/figs/ser-gating.eps,width=4.5in}}",
    "565": "The calculation of the left inputs to the XOR gates in the counter \nshown above is performed with a series of two-input AND gates.  Each \nof these gates AND's\nanother flip-flop value into the product.  This\napproach, called { serial gating}, implies that an {N-bit}\ncounter requires more than {N-2} gate delays\nto settle into the next state.  An alternative approach, called\n{ parallel gating}, calculates each input independently with a\nsingle logic gate, as shown below.  The blue inputs to the AND gate\nfor S_3 highlight the difference from the previous figure (note that\nthe two approaches differ only for bits S_3 and above).\nWith parallel gating, the \n{ fan-in} of the gates (the number of inputs)\nand the { fan-out} of the flip-flop outputs (number of other gates\ninto which an output feeds) grow with the size of the counter.  \nIn practice, large counters use a combination of these two approaches.",
    "566": "{{file=part3/figs/par-gating.eps,width=4.5in}}",
    "567": "A second class of\ncounter drives some of its flip-flops with a clock signal and feeds\nflip-flop outputs into the clock inputs of its remaining flip-flops,\npossibly through additional logic.  Such a counter is called a {\nripple counter}, because the effect of a clock edge ripples through\nthe flip-flops.  The delay inherent to the ripple effect, along with\nthe complexity of ensuring that timing issues do not render the design\nunreliable, are the major drawbacks of ripple counters.  Compared with\nsynchronous counters, however, ripple counters consume less energy,\nand are sometimes used for devices with restricted energy supplies.",
    "568": "General ripple counters\ncan be tricky because of timing issues, but certain types are easy.",
    "569": "Consider the design of binary ripple counter.  The state diagram for \na {3-bit} binary counter is replicated to the right.\nLooking\nat the states, notice that the least-significant bit alternates with\neach state, while higher bits flip whenever the next smaller bit (to\nthe right) transitions from one to zero.  To take advantage of these\nproperties, we use positive edge-triggered D flip-flops with\ntheir complemented () outputs wired back to their inputs.\nThe clock input is fed only into the first\nflip-flop, and the complemented output of each flip-flop is also\nconnected to the clock of the next.",
    "570": "{file=part3/figs/lec16-9.eps,width=1.5in}",
    "571": "An implementation of a {4-bit} binary ripple counter appears to the\nright.\nThe order of bits in the figure matches the order used for our synchronous\nbinary counters: least significant on the left, most significant on the\nright.\nAs you can see from the figure, the technique generalizes to arbitrarily \nlarge binary ripple coun-",
    "572": "ters, but the\ntime required for the outputs to settle after a clock edge scales with\nthe number of flip-flops in the counter.  On the other hand, an\naverage of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n), which reduces the power requirements.{Recall that\nflip-flops record the clock state internally.  The logical activity\nrequired to record such state consumes energy.}",
    "573": "Beginning with the state 0000, at the rising clock edge, the left (S_0)\nflip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\nfalling clock edge and does nothing, leaving the counter in\nstate 0001.  When the next rising clock edge arrives, the left\nflip-flop toggles back to 0, which the second flip-flop sees as a\nrising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\nsees the second flip-flop's change as a falling edge and does nothing,\nand the state settles as 0010.  We leave verification of the remainder \nof the cycle as an exercise.",
    "574": "Ripple counters are a form of a more general strategy known as clock\ngating.{Fall 2012 students: This part may seem a little\nredundant, but we're going to remove the earlier mention of clock \ngating in future semesters.}\n{ Clock gating} uses logic to control the visibility of a\nclock signal to flip-flops (or latches).  Historically, digital system\ndesigners rarely used clock gating techniques because of the \ncomplexity introduced for the circuit designers, who must ensure \nthat clock edges are\ndelivered with little skew along a dynamically changing set of\npaths to flip-flops.  Today, however, the power benefits of hiding\nthe clock signal from flip-flops have made clock gating an attractive \nstrategy.\nNevertheless, digital logic designers and computer architects still almost\nnever use clock gating strategies directly.  In most of the industry,\nCAD tools insert logic for clock gating automatically.  \nA handful of companies (such as Intel and Apple/Samsung) design \ncustom circuits rather than relying on CAD tools to synthesize \nhardware designs from standard libraries of elements.\nIn these companies, clock gating is used widely by the circuit\ndesign teams, and some input is occasionally necessary from the \nhigher-level designers.",
    "575": "More aggressive gating strategies are also used in modern designs, but\nthese usually require more time to transition between the on and off \nstates and can be more\ndifficult to get right automatically (with the tools), hence\nhardware designers may need to provide high-level information about\ntheir designs.  A flip-flop that does not see any change in its clock\ninput still has connections to high voltage and ground, and thus allows\na small amount of { leakage current}.  In contrast,\nwith { power gating}, the voltage difference is removed, and the \ncircuit uses no power at all.  Power gating can be tricky---as you know,\nfor example, when you turn the power on, you need to make sure that\neach latch settles into a stable state.  Latches may need to be \ninitialized to guarantee that they settle, which requires time after\nthe power is restored.",
    "576": "If you want a deeper understanding of gating issues, take ECE482, \nDigital Integrated Circuit Design, or ECE527, System-on-a-Chip Design.",
    "577": "Before we dive fully into FSM design, we must point out that we have\nplaced a somewhat artificial restriction on the types of FSMs that\nwe use in our course.",
    "578": "Historically, this restriction was given a name, and machines of the type\nthat we have discussed are called Moore machines.",
    "579": "However, outside of introductory classes, almost no one cares about\nthis name, nor about the name for the more general model used almost\nuniversally in hardware design, Mealy machines.",
    "580": "What is the difference?  In a { Moore machine}, outputs depend only on\nthe internal state bits of the FSM (the values stored in the flip-flops).\nIn a { Mealy machine}, outputs\nmay be expressed as functions both of internal state and FSM inputs.",
    "581": "As we illustrate shortly, the benefit of using input signals to calculate\noutputs (the Mealy machine model) is that input bits effectively serve \nas additional system state, which means that the number of internal \nstate bits can be reduced.",
    "582": "The disadvantage of including input signals in the expressions for \noutput signals is that timing characteristics of input signals may not\nbe known, whereas an FSM designer may want to guarantee certain\ntiming characteristics for output signals.",
    "583": "In practice, when such timing guarantees are needed, the designer simply\nadds state to the FSM to accommodate the need, and the problem is solved.",
    "584": "The coin-counting FSM that we designed for our class' lab assignments,\nfor example, \nrequired that we use a Moore machine model to avoid sending the\nservo controlling the coin's path an output pulse that was too short\nto enforce the FSM's decision about which way to send the coin.",
    "585": "By adding more states to the FSM, we were able to hold the servo in\nplace, as desired.",
    "586": "Why are we protecting you from the model used in practice?",
    "587": "First, timing issues add complexity to a topic that is complex enough \nfor an introductory course.",
    "588": "And, second, most software FSMs are Moore machines, so the abstraction\nis a useful one in that context, too.",
    "589": "In many design contexts, the timing issues implied by a Mealy model\ncan be relatively simple to manage.  When working in a single clock\ndomain, all of the input signals come from flip-flops in the same \ndomain, and are thus stable for most of the clock cycle.  Only rarely\ndoes one need to keep additional state to improve timing characteristics\nin these contexts.  In contrast, when interacting across clock domains,\nmore care is sometimes needed to ensure correct behavior.",
    "590": "We now illustrate the state reduction benefit of the Mealy machine\nmodel with a simple example, an FSM that recognizes the \npattern of a 0 followed by a 1 on a single input and outputs\na 1 when it observes the pattern.",
    "591": "As already mentioned,\nMealy machines often require fewer flip-flops.\nIntuitively, the number of combinations of states and\ninputs is greater than the number of combinations of states alone, and\nallowing a function to depend on inputs reduces the number of internal\nstates needed.",
    "592": "A Mealy implementation of the FSM appears on the left below, and\nan example timing diagram illustrating the FSM's behavior is shown on\nthe right.",
    "593": "The machine shown below occupies state A when the last bit seen was a 0, and\nstate B when the last bit seen was a 1.",
    "594": "Notice that the transition arcs in the state diagram\nare labeled with two values instead\nof one.  Since outputs can depend on input values as well as state,\ntransitions in a Mealy machine are labeled with input/output\ncombinations, while states are labeled only with their internal bits\n(or just their names, as shown below).  Labeling states with outputs\ndoes not make sense for a Mealy machine, since outputs may vary\nwith inputs.",
    "595": "Notice that the outputs indicated on any given transition\nhold only until that transition is taken (at the rising clock edge), as is\napparent in the timing diagram.  When inputs are asynchronous, \nthat is,\nnot driven by the same clock signal, output pulses from a Mealy\nmachine can be arbitrarily short, which can lead to problems.",
    "596": "{{file=part3/figs/lec17-3.eps,width=5in}}",
    "597": "For a Moore machine, we must create a special state in which the\noutput is high.  Doing so requires that we split state B into two\nstates, a state C in which the last two bits seen were 01, and a\nstate D in which the last two bits seen were 11.  Only state C\ngenerates output 1.  State D also becomes the starting state for the\nnew state machine.  The state diagram on the left below illustrates \nthe changes, using the transition diagram style that we introduced \nearlier to represent Moore machines.\nNotice in the associated timing diagram that the output pulse lasts a\nfull clock cycle.",
    "598": "{{file=part3/figs/lec17-4.eps,width=5in}}",
    "599": "In this short summary, we \ngive you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).",
    "600": "Students often find this part of the course more challenging than the\nearlier parts of the course.",
    "601": "In addition to these notes, you should read Chapters 4 and 5 of the \nPatt and Patel textbook, which cover the von Neumann\nmodel, instruction processing, and ISAs.",
    "602": "Students typically find that the homeworks in this part of the course\nrequire more time than did those in earlier parts of the course.\nProblems on the exam will be similar in nature but designed to require \nless actual time to solve (assuming that you have been doing the homeworks).",
    "603": "We'll start with the easy stuff.",
    "604": "You should recognize all of these terms and be able\nto explain what they mean.  For the specific circuits, you should be able \nto draw them and explain how they work.  Actually, we don't care whether \nyou can draw something from memory---a mux, for example---provided that \nyou know what a mux does and can derive a gate diagram correctly for one \nin a few minutes.  Higher-level skills are much more valuable.",
    "605": "[t]\n{}{{}{}\n{}{}{}",
    "606": "{digital systems terms\n{--}{{}{}\n{}{}{}\n module\n fan-in\n fan-out\n machine models: Moore and Mealy",
    "607": "}",
    "608": "{simple state machines\n{--}{{}{}\n{}{}{}\n synchronous counter\n ripple counter\n serialization (of bit-sliced design)",
    "609": "}",
    "610": "{finite state machines (FSMs)\n{--}{{}{}\n{}{}{}\n states and state representation\n transition rule\n self-loop\n next state (+) notation\n meaning of don't care in input  combination\n meaning of don't care in output\n unused states and initialization\n completeness (with regard to  FSM specification)\n list of (abstract) states\n next-state table/state transition table/state table\n state transition diagram/transition  diagram/state diagram",
    "611": "}",
    "612": "{memory\n{--}{{}{}\n{}{}{}\n number of addresses\n addressability\n read/write logic\n serial/random access memory (RAM)\n volatile/non-volatile (N-V)\n static/dynamic RAM (SRAM/DRAM)\n SRAM cell\n DRAM cell\n design as a collection of cells\n coincident selection\n bit lines and sense amplifiers",
    "613": "}",
    "614": "[t]\n{}{{}{}\n{}{}{}",
    "615": "{von Neumann model\n{--}{{}{}\n{}{}{}\n{processing unit\n{--}{{}{}\n{}{}{}\n register file\n arithmetic logic unit (ALU)\n word size",
    "616": "}\n{control unit\n{--}{{}{}\n{}{}{}\n program counter (PC)\n instruction register (IR)\n implementation as FSM",
    "617": "}\n input and output units\n{memory\n{--}{{}{}\n{}{}{}\n memory address register (MAR)\n memory data register (MDR)",
    "618": "}\n{processor datapath}",
    "619": "{control signal}",
    "620": "}",
    "621": "{tri-state buffer\n{--}{{}{}\n{}{}{}\n meaning of Z/hi-Z output\n use in distributed mux",
    "622": "}",
    "623": "{instruction processing}\n{-}{{}{}\n{}{}{}",
    "624": "{register transfer language (RTL)}",
    "625": "{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (of an encoded instruction)}\n{operation code (opcode)}\n{types of instructions}\n{-}{{}{}\n{}{}{}",
    "626": "{data movement}\n{control flow}",
    "627": "{addressing modes}\n{-}{{}{}\n{}{}{}",
    "628": "{PC-relative}",
    "629": "{base + offset}",
    "630": "We expect you to be able to exercise the following skills:",
    "631": "{}{{}{}\n{}{}{}",
    "632": "{Transform a bit-sliced design into a serial design, and explain the \ntradeoffs involved in terms of area and time required to compute a result.}\n{Based on a transition diagram, implement a synchronous counter from \nflip-flops and logic gates.}\n{Implement a binary ripple counter (but not necessarily a more general \ntype of ripple counter) from flip-flops and logic gates.}\n{Given an FSM implemented as digital logic, analyze the FSM to produce \na state transition diagram.}\n{Design an FSM to meet an abstract specification for a task, including \nproduction of specified output signals, and possibly including selection \nof appropriate inputs.}\n{Complete the specification of an FSM by ensuring that each state \nincludes a transition rule for every possible input combination.}\n{Compose memory chips into larger memory systems, using additional\ndecoders when necessary.}\n{Encode {LC-3} instructions into machine code.}\n{Read and understand programs written in {LC-3} assembly/machine code.}",
    "633": "At a higher level, we expect that you understand the concepts and ideas \nsufficiently well to do the following:",
    "634": "{}{{}{}\n{}{}{}",
    "635": "{Abstract design symmetries from an FSM specification in order to \nsimplify the implementation.}\n{Make use of a high-level state design, possibly with many sub-states \nin each high-level state, to simplify the implementation.}\n{Use counters to insert time-based transitions between states (such \nas timeouts).}\n{Implement an FSM using logic components such as registers, \ncounters, comparators, and adders as building blocks.}\n{Explain the basic organization of a computer's microarchitecture\nas well as the role played by elements of a von Neumann design in the\nprocessing of instructions.}\n{Identify the stages of processing an instruction (such as fetch,\ndecode, getting operands, execution, and writing back results) in a \nprocessor control unit state machine diagram.}",
    "636": "And, at the highest level, we expect that you will be able to do the following:",
    "637": "{}{{}{}\n{}{}{}",
    "638": "{Explain the difference between the Moore and Mealy machine models, \nas well as why you might find each of them useful when designing an FSM.}\n{Understand the need for initialization of an FSM, be able to analyze \nand identify potential problems arising from lack of initialization, and \nbe able to extend an implementation to include initialization to an \nappropriate state when necessary.}\n{Understand how the choice of internal state bits for an FSM can \naffect the complexity of the implementation of next-state and output \nlogic, and be able to select a reasonable state assignment.}\n{Identify and fix design flaws in simple FSMs by analyzing an existing \nimplementation, comparing it with the specification, and removing any \ndifferences by making any necessary changes to the implementation.}",
    "639": "{   }  empty 3rd page",
    "640": "This set of notes discusses \ntradeoffs and design elements of instruction set architectures (ISAs).\n{ The material is beyond the scope of our class, and is provided purely\nfor your interest.}  Those who find these topics interesting\nmay also want to read the ECE391 notes, which describe similar material\nwith a focus on the x86 ISA.",
    "641": "As you know, the ISA\ndefines the interface between software and hardware, abstracting the\ncapabilities of a computer's datapath and standardizing the format of\ninstructions to utilize those capabilities.  Successful ISAs are\nrarely discarded, as success implies the existence of large amounts of\nsoftware built to use the ISA.  Rather, they are extended, and their\noriginal forms must be supported for decades (consider, for example,\nthe IBM 360 and the Intel x86).  Employing sound design principles\nis thus imperative in an ISA.",
    "642": "The LC-3 ISA \nemploys fixed-length instructions and a load-store architecture, two\naspects that help to reduce the design space to a manageable set of\nchoices.  In a general ISA design, many other options exist for\ninstruction formats.",
    "643": "Recall the idea of separating the bits of an instruction into\n(possibly non-contiguous) fields.  One of the fields must contain an\nopcode, which specifies the type of operation to be performed by the\ninstruction.  In the {LC-3} ISA, most opcodes specify both\nthe type of operation and the types of arguments to the operation.  \nMore generally, many addressing modes are possible for\neach operand, and we can think of the bits\nthat specify the addressing mode as a separate field, known as the\n{ mode} field.",
    "644": "As a simple example, the {LC-3's} ADD and AND instructions \ncontain a {1-bit} mode field that specifies whether the second\noperand of the ADD/AND comes from a register or is an immediate value.",
    "645": "Several questions must be answered in order to define the possible\ninstruction formats for an ISA.  First, are instructions fixed-length\nor variable-length?  Second, how many addresses are needed for each\ninstruction, and how many of the addresses can be memory addresses?\nFinally, what forms of addresses are possible for each operand?\nFor example, can one use\nfull memory addresses or only limited offsets relative to a register?",
    "646": "The answer to the first question depends on many factors, but several\nclear advantages exist for both answers.  { Fixed-length\ninstructions} are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions.  As an example of this\nsimplification, note that the {LC-3} ISA does not support \naddressing for individual bytes, only for {16-bit} words.",
    "647": "{ Variable-length instructions} also have benefits, however.\nVariable-length encodings allow more efficient encodings, saving both\nmemory and disk space.  A register transfer operation, for example,\nclearly requires fewer bits than addition of values at two direct\nmemory addresses for storage at a third.  Fixed-length instructions\nmust be fixed at the length of the longest possible instruction,\nwhereas variable-length instructions can use lengths appropriate to\neach mode.  The same tradeoff has another form in the sense that\nfixed-length ISAs typically eliminate many addressing modes in order\nto limit the size of the instructions.  Variable-length instructions\nthus allow more flexibility; indeed, extensions to a variable-length\nISA can incorporate new addressing modes that require longer\ninstructions without affecting the original ISA.",
    "648": "For example, the maximum length of x86 instructions has grown from six \nbytes in 1978 (the 8086 ISA) to fifteen bytes in today's version of the\nISA.",
    "649": "Moving to the last of the three questions posed for instruction format\ndefinition, operand address specification,\nwe explore a range of answers developed over the last few\ndecades.  Answers are usually chosen based on the number of bits\nnecessary, and we use this metric to organize the possibilities.  The\nfigure below separates approaches into two dimensions: the vertical\ndimension divides addressing into registers and memory, and the\nhorizontal dimension into varieties within each type.",
    "650": "{{file=part4/figs/lec23-1.eps,width=4in}}",
    "651": "As a register file contains fewer registers than a memory does words,\nthe use of register operands rather than memory addresses reduces the\nnumber of bits required to specify an operand.  The {LC-3} ISA\nuses a restricted set of addressing modes to stay within the limit\nimposed by the use of {16-bit} instructions.  Both\nregister and memory addresses, however, admit a wide range of\nimplementations.",
    "652": "{ Implicit operands} of either type require no additional bits for\nthe implicit address.  The {LC-3} procedure call instruction, JSR,\nfor example, stores the return address in R7.  No bits in the JSR \nencoding name the R7 register; R7 is used implicitly for every JSR\nexecuted.",
    "653": "Similarly, the procedure call instructions in many ISAs push the \nreturn address onto a stack using an implicit register for the top \nof stack pointer.",
    "654": "Memory addresses can also be implicitly equated to other memory\naddresses.  An increment instruction operating on a memory address, for\nexample, implicitly writes the result back to the same address.",
    "655": "At the opposite extreme, an instruction may include a full address,\neither to any register in the register file or to any address in the \nmemory.  The term { general-purpose registers} indicates that \nregisters are used in any operation.",
    "656": "{ Special-purpose registers}, in contrast, split the register file\nand allow only certain registers to be used in each operation.  For\nexample, the Motorola 680x0 series, used in early Apple\nMacintosh computers, provides distinct sets of address and data\nregisters.  Loads and stores use the address registers; arithmetic,\nlogic, and shift operations use the data registers.  As a result, \neach instruction\nselects from a smaller set of registers and thus requires fewer bits\nin the instruction to name the register for use.",
    "657": "As full memory addresses require many more bits than full register\naddresses, a wider range of techniques has been employed to reduce the\nlength.  ``Zero page'' addresses, as defined in the 6510 (6502) ISA\nused by Commodore PET's,{My computer in junior high school.}\nC64's,{My computer in high school.} and VIC 20's, prefixed a\none-byte address with a zero byte, allowing shorter instructions when\nmemory addresses fell within the first 256 memory locations.  Assembly\nand machine language programmers made heavy use of these locations to\nproduce shorter programs.",
    "658": "Relative addressing is quite common in the {LC-3} ISA, in which many\naddresses are PC-relative.  Typical commerical ISAs also make use of\nrelative addressing.  The Alpha ISA, for example, has a PC-relative form of\nprocedure call with a {21-bit} offset (plus or minus a megabyte),\nand the x86 ISA has a ``short'' form of branch instructions that\nuses an {8-bit} offset.",
    "659": "Segmented memory is a form of relative addressing that uses a register\n(usually implicit) to provide the high bits of an address and an\nexplicit memory address (or another register) to provide the low bits.\nIn the early x86 ISAs, for example, {20-bit} addresses are\nfound by adding a {16-bit} segment register extended with four\nzero bits to a {16-bit} offset.",
    "660": "One question remains for the definition of instruction formats: how\nmany addresses are needed for each instruction, and how many of the\naddresses can be memory addresses?  The first part of this question\nusually ranges from zero to three, and is rarely allowed to go\nbeyond three.  The answer to the second part determines the {\naddressing architecture} implemented by an ISA.  We now illustrate the\ntradeoffs between five distinct addressing architectures through the\nuse of a running example, the assignment X=AB+C/D.",
    "661": "A binary operator requires two source operands and one destination\noperand, for a total of three addresses.  The ADD instruction, for\nexample, has a { {3-address}} format:",
    "662": "{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B,C>; M[A]  M[B] + M[C]\n>or>ADD>R1,R2,R3>; R1  R2 + R3\n{-6pt}",
    "663": "If all three addresses can be memory addresses, the ISA is dubbed a\n{ memory-to-memory architecture}.  Such architectures may have\nsmall register sets or even lack a register file completely.  To\nimplement the assignment, we assume the availability of two memory\nlocations, T1 and T2, for temporary storage:",
    "664": "{-6pt}\n=WWW=WW,WW,WW=\n>MUL>T1,A,B>; T1  M[A] * M[B]\n>DIV>T2,C,D>; T2  M[C] / M[D]\n>ADD>X,T1,T2>; X  M[T1] + M[T2]\n{-6pt}",
    "665": "The assignment requires only three instructions to implement, but each\ninstruction contains three full memory addresses, and is thus quite long.",
    "666": "At the other extreme is the { load-store architecture} used by the\n{LC-3} ISA.  In a load-store architecture, only\nloads and stores can use memory addresses; all other operations use\nonly registers.  As most instructions use only registers, this type of\naddressing architecture is also called a { register-to-register\narchitecture}.  The example assignment translates to the code shown below,\nwhich assumes that R1, R2, and R3 are free for use (the instructions\nare { NOT} {LC-3} instructions, but rather a generic assembly\nlanguage for a load-store architecture).",
    "667": "{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>LD>R2,B>; R2  M[B]\n>MUL>R1,R1,R2>; R1  R1 * R2\n>LD>R2,C>; R2  M[C]\n>LD>R3,D>; R3  M[D]\n>DIV>R2,R2,R3>; R2  R2 / R3\n>ADD>R1,R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}",
    "668": "Eight instructions are necessary, but no instruction requires more\nthan one full memory address, and several use only register addresses,\nallowing the use of shorter instructions.  The need to move data in\nand out of memory explicitly, however, also requires a reasonably\nlarge register set, as is available in the ARM, Sparc, Alpha, and IA-64\nISAs.",
    "669": "Architectures that use other combinations of memory and register\naddresses with {3-address} formats are not named.  Unary\noperators and transfer operators require only one source operand, thus\ncan use a {2-address} format (for example, NOT A,B).  Binary operations\ncan also use { {2-address}} format if one operand is implicit,\nas in the following instructions:",
    "670": "{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B>; M[A]  M[A] + M[B]\n>or>ADD>R1,B>; R1  R1 + M[B]\n{-6pt}",
    "671": "The second instruction, in which one address is a register and the\nsecond is a memory address, defines a { register-memory\narchitecture}.  As shown by the code on the next page, \nsuch architectures strike a balance\nbetween the two architectures just discussed.",
    "672": "{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>MUL>R1,B>; R1  R1 * M[B]\n>LD>R2,C>; R2  M[C]\n>DIV>R2,D>; R2  R2 / M[D]\n>ADD>R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}",
    "673": "The assignment now requires six instructions using at most one memory\naddress each; like memory-to-memory architectures, register-memory\narchitectures use relatively few registers.  Note that two-register\noperations are also allowed.  Intel's x86 ISA is a register-memory\narchitecture.",
    "674": "Several ISAs of the past{The 6510/6502 as well, if memory\nserves, as the 8080, Z80, and Z8000, which used to drive parlor video\ngames.}  used a special-purpose register called the accumulator for\nALU operations, and are called { accumulator architectures}.  The\naccumulator in such architectures is implicitly both a source and the\ndestination for any such operation, allowing a { {1-address}}\nformat for instructions, as shown below.",
    "675": "{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>B>; ACC  ACC + M[B]\n>or>ST>E>; M[E]  ACC\n{-6pt}",
    "676": "Accumulator architectures strike the same balance as register-memory\narchitectures, but use fewer registers.  Note that memory location X\nis used as a temporary storage location as well as the final storage\nlocation in the following code:",
    "677": "{-6pt}\n=WWW=WW,WW,WW=\n>LD>A>; ACC  M[A]\n>MUL>B>; ACC  ACC * M[B]\n>ST>X>; M[X]  ACC\n>LD>C>; ACC  M[C]\n>DIV>D>; ACC  ACC / M[D]\n>ADD>X>; ACC  ACC + M[X]\n>ST>X>; M[X]  ACC\n{-6pt}",
    "678": "The last addressing architecture that we discuss is rarely used for\nmodern general-purpose processors, but may be familiar to\nyou because of its historical use in scientific and engineering calculators.",
    "679": "A { stack architecture}\nmaintains a stack of values and draws all ALU operands from this\nstack, allowing these instructions to use a { {0-address}}\nformat.  A special-purpose stack pointer (SP) register points to the\ntop of the stack in memory, and operations analogous to load ({\npush}) and store ({ pop}) are provided to move values on and off\nthe stack.  To implement our example assignment, we first transform it\ninto postfix notation (also called reverse Polish notation):",
    "680": "{-6pt}\n=\n>A  B  *  C  D  /  +\n{-6pt}",
    "681": "The resulting sequence of symbols transforms on a one-to-one basis\ninto instructions for a stack architecture:",
    "682": "{-6pt}\n=WWWW=WW=; M[SP+1]  M[SP] + M[SP + 1], SP  SP + 1WW=AB+C/DW=AB+C/DW=\n>PUSH>A>; SP  SP - 1, M[SP]  M[A]>A\n>PUSH>B>; SP  SP - 1, M[SP]  M[B]>B>A\n>MUL>>; M[SP+1]  M[SP+1] * M[SP], SP  SP + 1>AB\n>PUSH>C>; SP  SP - 1, M[SP]  M[C]>C>AB\n>PUSH>D>; SP  SP - 1, M[SP]  M[D]>D>C>AB\n>DIV>>; M[SP+1]  M[SP+1] / M[SP], SP  SP + 1>C/D>AB\n>ADD>>; M[SP+1]  M[SP+1] + M[SP], SP  SP + 1>AB+C/D\n>POP>X>; M[X]  M[SP], SP  SP + 1\n{-6pt}",
    "683": "The values to the right are the values on the stack, starting with the\ntop value on the left and progressing downwards, { after the\ncompletion of each instruction}.",
    "684": "This section illustrates the uses of special-purpose registers through\na few examples.",
    "685": "The { stack pointer (SP)} points to the top of the stack in memory.\nMost older architectures support push and pop operations that\nimplicitly use the stack pointer.  Modern architectures assign a\ngeneral-purpose register to be the stack pointer and reference it\nexplicitly, although an assembler may support\ninstructions that appear to use implicit operands but in fact\ntranslate to machine instructions with explicit reference to the\nregister defined to be the SP.",
    "686": "The { program counter (PC)} points to the next instruction to be\nexecuted.  Some modern architectures expose it as a general-purpose\nregister, although its distinct role in the implementation keeps such\na model from becoming as common as the use of a general-purpose\nregister for the SP.",
    "687": "The { processor status register (PSR)}, also known as the {\nprocessor status word (PSW)}, contains all status bits as well as a\nmode bit indicating whether the processor is operating in user mode or\nprivileged (operating system) mode.  Having a register with this\ninformation allows more general access than is possible solely through\nthe use of control flow instructions.",
    "688": "The { zero register} appears in modern architectures of the RISC\nvariety (defined in the next section of these notes).  The register is\nread-only and serves both as a useful constant and as a destination\nfor operations performed only for their side-effects (for example, setting\nstatus bits).  The availability of a zero register also allows certain\nopcodes to serve double duty.  A register-to-register add instruction\nbecomes a register move instruction when one source operand is zero.\nSimilarly, an immediate add instruction becomes an immediate load\ninstruction when one source operand is zero.",
    "689": "By the mid-1980's, the VAX architecture dominated the workstation and\nminicomputer markets, which included most universities.  Digital\nEquipment Corporation, the creator of the VAX, was second only to IBM\nin terms of computer sales.  VAXen, as the machines were called, used\nmicroprogrammed control units and supported numerous addressing modes\nas well as complex instructions ranging from ``square root'' to\n``find roots of polynomial equation.''",
    "690": "The impact of increasingly dense integrated circuit technology had\nbegun to have its effect, however, and in view of increasing processor\nclock speeds, more and more programmers were using high-level\nlanguages rather than writing assembly code.  Although assembly\nprogrammers often made use of the complex VAX instructions, compilers\nwere usually unable to recognize the corresponding high-level language\nconstructs and thus were unable to make use of the instructions.",
    "691": "Increasing density also led to rapid growth in memory sizes, to the\npoint that researchers began to question the need for variable-length\ninstructions.  Recall that variable-length instructions allow shorter\ncodes by providing more efficient instruction encodings.  With the\ntrend toward larger memories, code length was less important.  The\nperformance advantage of fixed-length instructions, which simplifies\nthe datapath and enables pipelining, on the other hand, was \nattractive.",
    "692": "Researchers leveraged these ideas, which had been floating around the\nresearch community (and had appeared in some commercial architectures)\nto create { reduced instruction set computers}, or { RISC}\nmachines.  The competing VAXen were labeled { CISC} machines, which\nstands for { complex instruction set computers}.",
    "693": "RISC machines employ fixed-length instructions and a load-store\narchitecture, allowing only a few addressing modes and small offsets.\nThis combination of design decisions enables deep pipelines and\nmultiple instruction issues in a single cycle (termed superscalar\nimplementations), and for years, RISC machines were viewed by many\nresearchers as the proper design for future ISAs.  However, companies\nsuch as Intel soon learned to pipeline microoperations after decoding\ninstructions, and CISC architectures now offer competitive if not\nsuperior performance in comparison with RISC machines.  The VAXen are\ndead, of course,{Unless you talk with customer support\nemployees, for whom no machine ever dies.}  having been replaced by\nthe Alpha, which in turn fell to x86, which is now struggling with ARM\nto enter the mobile market.",
    "694": "A { procedure} is a sequence of instructions that executes a\nparticular task.  Procedures are used as building blocks for multiple,\nlarger tasks.  The concept of a procedure is fundamental to\nprogramming, and appears in some form in every high-level language as\nwell as in most ISAs.",
    "695": "For our purposes, the terms procedure, subroutine,\nfunction, and method are synonymous, although they usually have\nslightly different meanings from the linguistic point of view.\nProcedure calls are supported through { call} and { return}\ncontrol flow instructions.  The first instruction in the code below,\nfor example, transfers control to the procedure ``DoSomeWork,'' which\npresumably does some work, then returns control to the instruction\nfollowing the call.",
    "696": "{-6pt}",
    "697": "=DoSomeWork:WW=WWWW=DoSomeWorkWW= \n>loop:>CALL>DoSomeWork\n>>CMP>R6,#1>; compare return value in R6 to 1\n>>BEQ>loop>; keep doing work until R6 is not 1",
    "698": ">DoSomeWork:>>> ; set R6 to 0 when all work is done, 1 otherwise \n>>RETN",
    "699": "{-6pt}",
    "700": "The procedure also places a return value in R6, which the instruction\nfollowing the call compares with immediate value 1.  Until the two are\nnot equal (when all work is done), the branch returns control to the\ncall and executes the procedure again.",
    "701": "As you may recall, the call and return use the stack pointer to keep\ntrack of nested calls.  Sample RTL for these operations appears below.",
    "702": "{eqnarray*}\n{call RTL}&&SP  SP - 1\n&& M[SP]  PC\n&& PC  {procedure start}\n{eqnarray*}",
    "703": "{eqnarray*}\n{return RTL}&&PC  M[SP]\n&&SP  SP + 1\n{eqnarray*}",
    "704": "While an ISA provides the call and return instructions necessary to\nsupport procedures, it does not specify how information is passed to\nor returned from a procedure.  A standard for such decisions is\nusually developed and included in descriptions of the architecture,\nhowever.  This { calling convention} specifies how information is\npassed between a caller and a callee.  In particular, it specifies the\nfollowing: where arguments must be placed, either in registers or in\nspecific stack memory locations; which registers can be used or\nchanged by the procedure; and where any return value must be placed.",
    "705": "The term ``calling convention'' is also used in the programming\nlanguage community to describe the convention for deciding what\ninformation is passed for a given call operation.  For example, are\nvariables passed by value, by pointers to values, or in some other\nway?  However, once the things to be sent are decided, the\narchitectural calling convention that we discuss here is used\nto determine where to put the data in order for the callee to be able\nto find it.",
    "706": "Calling conventions for architectures with large register sets\ntypically pass arguments in registers, and nearly all conventions\nplace the return value in a register.  A calling convention also\ndivides the register set into { caller-saved} and \n{ callee-saved} registers.  Caller-saved registers can be modified \narbitrarily\nby the called procedure, whereas any value in a callee-saved register\nmust be preserved.  Similarly, before calling a procedure, a caller\nmust preserve the values of any caller saved registers that are needed\nafter the call.  Registers of both types usually saved on the stack by\nthe appropriate code (caller or callee).",
    "707": "A typical stack structure appears in the figure to the right.  In\npreparation for a call, a caller first stores any caller-saved\nregisters on the stack.  Arguments to the procedure to be called are\npushed next.  The procedure is called next, implicitly pushing the\nreturn address (the address of the instruction following the call\ninstruction).  Finally, the called procedure may allocate space on the\nstack for storage of callee-saved registers as well as local\nvariables.",
    "708": "As an example, the following calling convention can be applied to \nan {8-register} load-store architecture similar to the {LC-3}\nISA: the first three arguments must be placed in R0 through R2 (in order), \nwith any remaining arguments on the stack; the return value must be placed \nin R6; R0 through R2 are caller-saved, as\nis R6, while R3 through R5 are callee-saved; R7 is used as the stack\npointer.  The code fragments below use this calling convention to\nimplement a procedure and a call of that procedure.",
    "709": "{file=part4/figs/lec23-2.eps,width=1.25in}",
    "710": "[t]\n{\n[t]",
    "711": "int =add3 (int n1, int n2, int n3) {\n>return (n1 + n2 + n3);\n}",
    "712": "printf (``d'', add3 (10, 20, 30));",
    "713": "by convention:\n= n1 is in R0\n>n2 is in R1\n>n3 is in R2\n>return value is in R6",
    "714": "[t]",
    "715": "add3:  = WWWW= WWWWW= \nadd3:>ADD>R0,R0,R1\n>ADD>R6,R0,R2\n>RETN\n>\n>PUSH>R1>; save the value in R1\n>LDI>R0,#10>; marshal arguments\n>LDI>R1,#20\n>LDI>R2,#30\n>CALL>add3\n>MOV>R1,R6>; return value becomes 2nd argument\n>LDI>R0,``d''>; load a pointer to the string\n>CALL>printf\n>POP>R1>; restore R1",
    "716": "The add3 procedure takes three integers as arguments, adds them\ntogether, and returns the sum.  The procedure is called with the\nconstants 10, 20, and 30, and the result is printed.  By the calling\nconvention, when the call is made, R0 must contain the value 10, R1\nthe value 20, and R2 the value 30.  We assume that the caller wants to\npreserve the value of R1, but does not care about R3 or R5.  In the\nassembly language version on the right, R1 is first saved to the\nstack, then the arguments are marshaled into position, and finally the\ncall is made.  The procedure itself needs no local storage and does\nnot change any callee-saved registers, thus must simply add the\nnumbers together and place the result in R6.  After add3 returns, its\nreturn value is moved from R6 to R1 in preparation for the call to\nprintf.  After loading a pointer to the format string into R0, the\nsecond call is made, and R1 is restored, completing the translation.",
    "717": "{ System calls} are almost identical to procedure calls.  As with\nprocedure calls, a calling convention is used: before invoking a\nsystem call, arguments are marshaled into the appropriate registers or\nlocations in the stack; after a system call returns, any result\nappears in a pre-specified register.  The calling convention used for\nsystem calls need not be the same as that used for procedure calls.\nRather than a call instruction, system calls are usually initiated\nwith a { trap} instruction, and system calls are also known as\ntraps.  With many architectures, a system call places the processor in\nprivileged or kernel mode, and the instructions that implement the\ncall are considered to be part of the operating system.  The term\nsystem call arises from this fact.",
    "718": "Unexpected processor interruptions arise both from interactions\nbetween a processor and external devices and from errors or unexpected\nbehavior in the program being executed.  The term { interrupt} is\nreserved for asynchronous interruptions generated by other devices,\nincluding disk drives, printers, network cards, video cards,\nkeyboards, mice, and any number of other possibilities.  {\nExceptions} occur when a processor encounters an unexpected opcode or\noperand.  An undefined instruction, for example, gives rise to an\nexception, as does an attempt to divide by zero.  Exceptions usually\ncause the current program to terminate, although many operating\nsystems will allow the program to catch the exception and to handle it\nmore intelligently.  The table below summarizes the characteristics of\nthe two types and compares them to system calls.",
    "719": "{\n{|l|l|l|c|c|}\n{|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected \ninterrupt& external device& packet arrived at network card& yes& yes\nexception& invalid opcode or operand& divide by zero& no& yes\ntrap/system call& deliberate, via trap instruction& print character to console& no& no",
    "720": "}",
    "721": "Interrupts occur asynchronously with respect to the program.  Most\ndesigns only recognize interrupts between instructions.  In other words, \nthe presence of interrupts is checked only after completing an instruction\nrather than in every cycle.  In pipelined designs, however,\ninstructions execute simultaneously, and the decision as to which\ninstructions occur ``before'' an interrupt and which occur ``after''\nmust be made by the processor.  Exceptions are not asynchronous in the\nsense that they occur for a particular instruction, thus no decision\nneed be made as to instruction ordering.  After determining which\ninstructions were before an interrupt, a pipelined processor discards\nthe state of any partially executed instructions that occur ``after''\nthe interrupt and completes all instructions that occur ``before.''\nThe terminated instructions are simply restarted after the interrupt\ncompletes.  Handling the decision, the termination, and the\ncompletion, however, significantly increases the design complexity of\nthe system.",
    "722": "The code associated with an interrupt, an exception, or a system call\nis a form of procedure called a { handler}, and is found by looking\nup the interrupt number, exception number, or trap number in a table\nof functions called a { vector table}.  Vector tables\nfor each type (interrupts, exceptions, and system calls) may be separate,\nor may be combined into a single table.\nInterrupts and exceptions share a need to save all registers and\nstatus bits before execution of the corresponding handler code (and to\nrestore those values afterward).  Generally, the\n{values---including} the status word {register---are} placed\non the stack.  With system calls, saving and restoring any necessary\nstate is part of the calling convention.  A special return from\ninterrupt instruction is used to return control from the interrupt\nhandler to the interrupted code; a similar instruction forces the\nprocessor back into user mode when returning from a system call.",
    "723": "Interrupts are also interesting in the sense that typical computers\noften have many interrupt-generating devices but only a few\ninterrupts.  Interrupts are prioritized by number, and only an\ninterrupt with higher priority can interrupt another interrupt.\nInterrupts with equal or lower priority are blocked while an interrupt\nexecutes.  Some interrupts can also be blocked in some architectures\nby setting bits in a special-purpose register called an interrupt\nmask.  While an interrupt number is masked, interrupts of that type\nare blocked, and can not occur.",
    "724": "As several devices may generate interrupts with the same interrupt\nnumber, interrupt handlers can be { chained} together.  Each\nhandler corresponds to a particular device.  When an interrupt occurs,\ncontrol is passed to the handler for the first device, which accesses\ndevice registers to determine whether or not that device generated an\ninterrupt.  If it did, the appropriate service is provided.  If not,\nor after the service is complete, control is passed to the next\nhandler in the chain, which handles interrupts from the second device,\nand so forth until the last handler in the chain completes.  At this\npoint, registers and processor state are restored and control is\nreturned to the point at which the interrupt occurred.",
    "725": "Control flow instructions may change the PC, loading it with an\naddress specified by the instruction.  Although any addressing mode\ncan be supported, the most common specify an address directly in the\ninstruction, use a register as an address, or use an address relative\nto a register.",
    "726": "Unconditional control flow instructions typically provided by an ISA\ninclude procedure calls and returns, traps, and jumps.  Conditional\ncontrol flow instructions are branches, and are logically based on\nstatus bits set by two types of instructions: { comparisons} and\n{ bit tests}.  Comparisons subtract one value from another to set\nthe status bits, whereas bit tests use an AND operation to\ncheck whether certain bits are set or not in a value.",
    "727": "Many ISAs implement\nstatus bits as special-purpose registers and implicitly set them when\nexecuting\ncertain instructions.  A branch based on R2 being less or equal to R3\ncan then be written as shown below.  The status bits are set by\nsubtracting R3 from R2 with the ALU.",
    "728": "=WWWWW=WW,WW,WW=\n>CMP>R2,R3>; R2 < R3: CNZ  110, R2 = R3: CNZ  001,\n>>>;     R2 > R3: CNZ  000\n>BLE>R1>; Z  C = 1: PC  R1\n{-6pt}",
    "729": "The status bits are not always implemented as special-purpose\nregisters; instead, they may be kept in general-purpose registers or\nnot kept at all.  For example, the Alpha ISA stores the results of\ncomparisons in general-purpose registers, and the same branch is\ninstead implemented as follows:",
    "730": "{-6pt}\n=WWWWW=WW,WW,WW=\n>CMPLE>R4,R2,R3>; R2  R3: R4  1, R2 > R3: R4  0\n>BNE>R4,R1>; R4  0: PC  R1\n{-6pt}",
    "731": "Finally, status bits can be calculated, used, and discarded within\na single instruction, in which case the branch is written as follows:",
    "732": "{-6pt}\n=WWWWW=WW,WW,WW=\n>BLE>R1,R2,R3>; R2  R3: PC  R1\n{-6pt}",
    "733": "The three approaches have advantages and disadvantages similar to\nthose discussed in the section on addressing architectures: the first\nhas the shortest instructions, the second is the most general and\nsimplest to implement, and the third requires the fewest instructions.",
    "734": "Two types of stack operations are commonly supported.  Push and pop\nare the basic operations in many older architectures, and values can\nbe placed upon or removed from the stack using these instructions.  In\nmore modern architectures, in which the SP becomes a general-purpose\nregister, push and pop are replaced with indexed loads and stores,\nthat is, loads and stores using the stack pointer and an offset as the\naddress for the memory operation.  Stack updates are performed using\nthe ALU, subtracting and adding immediate values from the SP as\nnecessary to allocate and deallocate local storage.",
    "735": "Stack operations serve three purposes in a typical architecture.  The\nfirst is to support procedure calls, as illustrated in a previous\nsection.  The second is to provide temporary storage during\ninterrupts, which was also mentioned earlier.",
    "736": "The third use of stack operations is to support { spill code}\ngenerated by compilers.  Compilers first translate high-level\nlanguages into an intermediate representation much like assembly code\nbut with an extremely large (theoretically infinite) register set.\nThe final translation step translates this intermediate representation\ninto assembly code for the target architecture, assigning\narchitectural registers as necessary.  However, as real ISAs support\nonly a finite number of registers, the compiler must occasionally\nspill values into memory.  For example, if ten values are in use at\nsome point in the code, but the architecture has only eight registers,\nspill code must be generated to store the remaining two values on the\nstack and to restore them when they are needed.",
    "737": "As a final topic, we now consider how a processor\nconnects to other devices to allow input and output.  We have already\ndiscussed interrupts, which are a special form of I/O in which only\nthe signal requesting attention is conveyed to the processor.\nCommunication of data occurs through instructions similar to loads and\nstores.  A processor is designed with a number of {{ I/O\nports}---usually} read-only or write-only registers to which devices\ncan be attached with opposite semantics.  That is, a port is usually\nwritten by the processor and read by a device or written by a device\nand read by the processor.",
    "738": "The question of exactly how I/O ports are accessed is an interesting\none.  One option is to create special instructions, such as the {\nin} and { out} instructions of the x86 architecture.  Port\naddresses can then be specified in the same way that memory addresses\nare specified, but use a distinct address space.  Just as two sets of\nspecial-purpose registers can be separated by the ISA, such an {\nindependent I/O} system separates I/O ports from memory addresses by\nusing distinct instructions for each class of operation.",
    "739": "Alternatively, device registers can be accessed using the same load and store\ninstructions as are used to access memory.  This approach, known as\n{ memory-mapped I/O}, requires no new instructions for I/O, but\ndemands that a region of the memory address space be set aside for\nI/O.  The memory words with those addresses, if they exist, can not be\naccessed during normal processor operations.",
    "740": "Appendix C of the Patt and Patel textbook describes a microarchitecture for\nthe LC-3 ISA, including a control unit implementation.",
    "741": "In this set of notes, we introduce a few concepts and strategies for\ncontrol unit design, using the textbook's {LC-3} microarchitecture \nto help illustrate them.  Several figures from the textbook are\nreproduced with permission in these notes as an aid to understanding.",
    "742": "The control unit of a computer based on the von Neumann model can be viewed\nas an FSM that fetches instructions from memory and executes them.  Many\npossible implementations exist both for the control unit itself and for the\nresources that it controls, the other components in the von Neumann model,\nwhich we collectively call the { datapath}.",
    "743": "In this set of notes, we discuss two strategies for structured control\nunit design and introduce the idea of using memories to encode logic \nfunctions.",
    "744": "Let's begin by recalling that the control unit consists of three parts: a \nhigh-level FSM that controls instruction processing, a program counter (PC)\nregister that holds the address of the next instruction to be executed,\nand an instruction register (IR) that holds the current instruction as\nit executes.",
    "745": "Other von Neumann components provide inputs to the control unit.  The\nmemory unit, for example, contains the instructions and data on which \nthe program executes.",
    "746": "The processing unit contains a register file and condition \ncodes (N, Z, and P for the {LC-3} ISA).",
    "747": "The outputs of the control unit are signals that control operation of the\ndatapath: the processing unit, the memory, and the I/O interfaces.",
    "748": "The basic problem that we must solve, then, for control unit design, is to \nmap instruction processing and the state of the FSM (including the PC and\nthe IR) into appropriate sequences of { control signals} for the datapath.",
    "749": "As we have skipped over the implementation details of interrupts and \nprivilege of the {LC-3} in our class, let's consider a \nmicroarchitecture and datapath without those capabilities.",
    "750": "The figure on the next page (Patt and Patel Figure C.3) shows an {LC-3}\ndatapath and control signals without support for interrupts and privilege.",
    "751": "Some of the datapath control signals mentioned\nin the textbook are no longer necessary in the simplified design.",
    "752": "Let's discuss the signals that remain and give some examples of how they\nare used.  A list appears to the right.",
    "753": "First, we have a set of seven {1-bit} control signals (starting \nwith ``LD.'') that specifies whether registers in the datapath load \nnew values.",
    "754": "Next, there are four {1-bit} signals (starting with ``Gate'') for \ntri-state buffers that control access to the bus.  These four implement \na distributed mux for the bus.  Only one value can",
    "755": "signal& meaning \nLD.MAR& load new value into memory address register\nLD.MDR& load new value into memory data register\nLD.IR& load new value into instruction register\nLD.BEN& load new value into branch enable register\nLD.REG& load new value into register file\nLD.CC& load new values into condition code registers (N,Z,P)\nLD.PC& load new value into program counter \nGatePC& write program counter value onto bus\nGateMDR& write memory data register onto bus\nGateALU& write arithmetic logic unit result onto bus\nGateMARMUX& write memory address register mux output onto bus \nPCMUX& select value to write to program counter (2 bits)\nDRMUX& select value to write to destination register (2 bits)\nSR1MUX& select register to read from register file (2 bits)\nADDR1MUX& select register component of address (1 bit)\nADDR2MUX& select offset component of address (2 bits)\nMARMUX& select type of address generation (1 bit) \nALUK& select arithmetic logic unit operation (2 bits) \nMIO.EN& enable memory \nR.W& read or write from memory",
    "756": "appear on the bus in \nany cycle, so at most one of these signals can be 1; others must all \nbe 0 to avoid creating a short.",
    "757": "{file=part4/patt-patel-appendix-C/APPC03.eps,width=6.5in}",
    "758": "The third group (ending with ``MUX'') of signals\ncontrols multiplexers in the datapath.\nThe number of bits for each depends on the number of inputs to the mux;\nthe total number of signals is 10.\nThe last two groups of signals control the ALU and the memory, \nrequiring a total of 4 more signals.  The total of all groups is thus\n25 control signals for the datapath without support for privilege\nand interrupts.",
    "759": "Before we begin to discuss control unit design in more detail, let's\nwork through a couple of examples of implementing specific RTL with\nthe control signals available.  The figure below (Patt and Patel \nFigure C.2) shows a state machine for the {LC-3} ISA (again \nwithout detail on interrupts and privilege).",
    "760": "{{file=part4/patt-patel-appendix-C/APPC02.eps,width=6.25in}}",
    "761": "Consider now the state that implements the ADD instruction---state\nnumber 1 in the figure on the previous page, just below and to the \nleft of the decode state.",
    "762": "The RTL for the state is: DR  SR + OP2, set CC.",
    "763": "We can think of the 25 control signals that implement the desired RTL\nas a { control word} for the datapath.",
    "764": "Let's begin with the register load control signals.  The RTL writes\nto two types of registers: the register file and the condition codes.\nTo accomplish these simultaneous writes, LD.REG and LD.CC must be high.\nNo other registers in the datapath should change, so the other five\nLD signals---LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC---should be low.",
    "765": "What about the bus?  In order to write the result of the add operation\ninto the register file, the control signals must allow the ALU to write\nits result on to the bus.  So we need GateALU=1.  And the other Gate\nsignals---GatePC, GateMDR, and GateMARMUX---must all be 0.  The \ncondition codes are also calculated from the value on the bus, but\nthey are calculated on the same value as is written to the register\nfile (by the definition of the {LC-3} ISA).",
    "766": "If the RTL for an FSM state implicitly requires more than one value \nto appear on the bus in the same cycle, that state is impossible to \nimplement using the given datapath.  Either the datapath or the state \nmachine must be changed in such a case.  The textbook's design has been \nfairly thoroughly tested and debugged.",
    "767": "The earlier figure of the datapath does not \nshow all of the muxes.  The remaining muxes appear in the figure \nto the right (Patt and Patel Figure C.6).",
    "768": "Some of the muxes in the datapath must be used to enable the\naddition needed for ADD to occur.  The\nDRMUX must select its IR[11:9] \ninput in order to write to the destination register specified by the \nADD instruction.  Similarly, the SR1MUX must select its IR[8:6] input \nin order to",
    "769": "{file=part4/patt-patel-appendix-C/APPC06.eps,width=4.45in}",
    "770": "pass the first source register specified by the ADD to the ALU as input A\n(see the datapath figure).  SR2MUX is always controlled by the \nmode bit IR[5], so the control unit does not need to generate anything\n(note that this signal was not in the list given earlier).",
    "771": "The rest of the muxes in the datapath---PCMUX, ADDR1MUX, ADDR2MUX, and\nMARMUX---do not matter, and the signals controlling them are don't cares.\nFor example, since the PC does not change, the output of the PCMUX\nis simply discarded, thus which input the PCMUX forwards to its output\ncannot matter.",
    "772": "The ALU must perform an addition, so we must set the operation type\nALUK appropriately.  And memory should not be enabled (MIO.EN=0),\nin which case the read/write control for memory, R.W, is a don't\ncare.  These 25 signal values together (including seven don't cares)\nimplement the RTL for the single state of execution for an \nADD instruction.",
    "773": "As a second example, consider the first state in the sequence that \nimplements the LDR instruction---state number 6 in the figure on the \nprevious page.",
    "774": "The RTL for the state is: MAR  BaseR + off6, but BaseR is\nabbreviated to ``B'' in the state diagram.",
    "775": "What is the control word for this state?",
    "776": "Let's again begin with the register load control signals.  Only the\nMAR is written by the RTL, so we need LD.MAR=1 and the other load\nsignals all equal to 0.",
    "777": "The address (BaseR + off6) is generated by the address adder, then \npasses through the MARMUX to the bus, from which it can be written into\nthe MAR.  To allow the MARMUX to write to the bus, we set GateMARMUX\nhigh and set the other three Gate control signals low.",
    "778": "More of the muxes are needed for this state's RTL than we needed for\nthe ADD execution state's RTL.",
    "779": "The SR1MUX must again select its IR[8:6] input, this time in order to \npass the BaseR specified by the instruction to ADDR1MUX.  ADDR1MUX\nmust then select the output of the register file in order to pass the \nBaseR to the address adder.  The other input of the address adder should\nbe off6, which corresponds to the sign-\nextended version of IR[5:0].\nADDR2MUX must select this input to pass to the address adder. \nFinally, the MARMUX must select the output of the address adder.\nThe PCMUX and DRMUX do not matter for this case, and can be left as\ndon't cares.  Neither the PC nor any register in the register file\nis written.",
    "780": "The output of the ALU is not used, so which operation it performs is \nirrelevant, and the ALUK controls are also don't cares.  Memory is\nalso not used, so again we set MIO.EN=0.  And, as before, the R.W\ncontrol for memory is a don't care.",
    "781": "These 25 signal values together (again including seven don't cares)\nimplement the RTL for the first state of LDR execution.",
    "782": "Now we are ready to think about how control signals can be generated.",
    "783": "As illustrated to the right, instruction processing consists of two \nsteps repeated infinitely: fetch an instruction, then execute the \ninstruction.",
    "784": "Let's say that we choose a fixed number of cycles for each of these two\nsteps.  We can then control our system with a",
    "785": "{file=part4/figs/inst-loop.eps,width=2.5in}",
    "786": "counter, using the counter's value and the IR to generate the\ncontrol signals through combinational\nlogic.  The PC is used only as data and has little or no direct effect on \nhow the system fetches and processes instructions.{Some ISAs do \nsplit the address space into privileged and non-privileged regions, but \nwe ignore that possibility here.}",
    "787": "This approach in general is called { hardwired control}.",
    "788": "How many cycles do we need for instruction fetch?  How many cycles do\nwe need for instruction processing?  The answers depend on the factors:\nthe complexity of the ISA, and the complexity of the datapath.",
    "789": "Given a simple ISA, we can design a datapath that\nis powerful enough to process any instruction in a single cycle.  \nThe control unit for such a design is\nan example of { single-cycle, hardwired control}.  While this\napproach simplifies the control unit, the cycle time\nfor the clock is limited by the slowest instruction.\nThe clock must also be slow enough to let memory \noperations complete in single cycle, both for instruction fetch and\nfor instructions that require a memory access.",
    "790": "Such a low clock rate is usually not acceptable.",
    "791": "More generally, we can use a simpler datapath and break both instruction\nfetch and instruction processing into multiple steps.  Using the datapath\nin the figure from Patt and Patel, for example, instruction fetch requires \nthree steps, and the number of steps for instruction processing depends \non the type of instruction being processed.  The state diagram shown\nearlier illustrates the steps for each opcode.",
    "792": "Although the datapath is not powerful enough to complete instructions\nin a single cycle, we can build a { multi-cycle, hardwired control} \nunit (one based on combinational logic).",
    "793": "In fact, this type of control unit is not much more complex than the \nsingle-cycle version.  For the control unit's FSM, we can use a binary \ncounter to enumerate first the steps of fetch, then the steps\nof processing.  The counter value along with the IR register can \ndrive combinational logic to produce control signals.  And, to avoid\nprocessing at the speed of the slowest instruction (the opcode that\nrequires the largest number of steps; LDI and STI in the {LC-3}\nstate diagram), we can add a reset signal to the counter to force it \nback to instruction fetch.  The FSM counter reset signal is simply \nanother control signal.  Finally, we can add one more signal that \npauses the counter while waiting for a memory operation to complete.  \nThe system clock can then run at the speed of the logic rather than \nat the speed of memory.",
    "794": "The control unit implementation discussed in Appendix C of Patt and Patel \nis not hardwired, but it does make use of a memory ready signal to \nachieve this decoupling between the clock speed of the processor and the\naccess time of the memory.",
    "795": "The figure to the right illustrates a general \nmulti-cycle, hardwired control unit.  The three blocks on\nthe left are the control unit state.  The combinational logic in\nthe middle uses the control unit state along with some\ndatapath status bits to compute the control signals for the datapath\nand the extra controls needed for the FSM counter, IR, and PC.\nThe datapath appears to the right in the figure.",
    "796": "How complex is the combinational logic?  As mentioned earlier, we \nassume that the PC does not directly affect control.  But we still \nhave 16 bits of IR, the FSM counter state, and the datapath status\nsignals.  Perhaps we need 24-variable {K-maps}?",
    "797": "Here's where engineering and human design come to \nthe rescue: by careful design of the ISA and the encoding, the\nauthors have made many of the datapath control signals for the {LC-3}\nISA quite simple.\nFor example, the register that appears on the register file's SR2\noutput is always specified by IR[2:0].  The SR1 output \nrequires a mux, but the choices are limited to IR[11:9] and IR[8:6]\n(and R6 in the design with support for interrupts).\nSimilarly, the destination register in the register file\nis always R7 or IR[11:9] (or, again, R6 when supporting interrupts).",
    "798": "The control signals for an {LC-3}\ndatapath depend almost entirely on the state of the control unit \nFSM (counter bits in a hardwired design) and the opcode IR[15:12].",
    "799": "The control signals are thus reduced to fairly simple functions.",
    "800": "Let's imagine building a hardwired control unit for the {LC-3}.",
    "801": "Let's start by being more precise about the number of inputs to\nthe combinational logic.",
    "802": "Although most decisions are based on the opcode, the datapath\nand state diagram shown earlier for the {LC-3} ISA do have\none instance of using another instruction bit to determine behavior.\nSpecifically, the JSR instruction has two modes, and the control\nunit uses IR[11] to choose between them.  So we need to have\nfive bits of IR instead of four as input to our logic.",
    "803": "How many datapath status signals are needed?",
    "804": "When the control unit accesses memory, it must wait until the \nmemory finishes the access, as indicated by a memory ready signal R.\nAnd the control unit must implement the conditional part of conditional\nbranches, for which it uses the datapath's branch enable signal BEN.",
    "805": "These two datapath status signals suffice for our design.",
    "806": "How many bits do we need for the counter?\nInstruction fetch requires three cycles: one to move\nthe PC to the MAR and increment the PC, a second to read from\nmemory into MDR, and a third to move the instruction bits across\nthe bus from MDR into IR.  Instruction decoding in a hardwired design\nis implicit and \nrequires no cycles: since all of our control signals can depend on\nthe IR, we do not need a cycle to change the FSM state to reflect the opcode.\nLooking at the {LC-3} state diagram, we see\nthat processing an instruction requires at most five cycles.  In\ntotal, at\nmost eight steps are needed to fetch and process any {LC-3} \ninstruction, so we can use a {3-bit} binary counter.",
    "807": "We thus have a total of ten bits of input: IR[15:11], R, BEN, and\na {3-bit} counter.",
    "808": "Adding the RESET and PAUSE controls for our FSM counter to the \n25 control signals listed earlier, we need to find 27 functions\non 10 variables.",
    "809": "That's still a lot of big {K-maps} to solve.  Is there an\neasier way?",
    "810": "Consider a case in which you need to compute many functions on a small \nnumber of bits, such as we just described for the multi-cycle, hardwired\ncontrol unit.  One strategy is to use a memory (possibly a read-only memory).\nA {2^m} memory can be viewed as computing N arbitrary\nfunctions on m variables.  The functions to be computed are specified\nby filling in the bits of the memory.  So long as the value of m is fairly\nsmall, the memory (especially SRAM) can be fast.",
    "811": "Synthesis tools (or hard work) can, of course, produce smaller designs\nthat use fewer gates.  Actually, tools may be able to optimize a fixed\ndesign expressed as read-only memory, too.  But designing the functions\nwith a memory makes them easier to modify later.  If we make a mistake,\nfor example, in computing one of the functions, we need only change a bit\nor two in the memory instead of solving equations and reoptimizing and\nreplacing logic gates.  We can also extend our design if we have space\nremaining (that is, if the functions are undefined for some combinations \nof the m inputs).  The\nCray T3D supercomputer, for example, used a similar approach to add\nnew instructions to the Alpha processors on which it was based.",
    "812": "This strategy is effective in many contexts, so let's briefly discuss two \nanalogous cases.  In software, a memory becomes a lookup table.  Before\nhandheld calculators, lookup tables were used by humans to compute \ntranscendental functions such as sines, cosines, logarithms.  Computer \ngraphics hardware and software used a similar approach for transcendental\nfunctions in order to reduce cost and improve \nspeed.  Functions such as counting 1 bits in a word are useful for \nprocessor scheduling and networking, but not all ISAs provide this type\nof instruction.  In such cases, lookup tables in software are often the\nbest solution.",
    "813": "In programmable hardware such as Field Programmable Gate Arrays (FPGAs),\nlookup tables (called LUTs in this context) have played an important role\nin implementing arbitrary logic functions.",
    "814": "The FPGA is the modern form of the programmable logic array (PLA)\nmentioned in the textbook, and will be your main tool \nfor developing digital hardware in ECE385.",
    "815": "For many years, FPGAs served as a hardware prototyping platform, but\nmany companies today ship their first round products using \ndesigns mapped to FPGAs.  Why?  Chips are more and more expensive\nto design, and mistakes are costly to fix.  In contrast, while\ncompanies pay more to buy an FPGA than to produce a chip (after the \nfirst chip!), errors in the design can usually be fixed by sending \ncustomers a new version through the Internet.",
    "816": "Let's return to our {LC-3} example.",
    "817": "Instead of solving the {K-maps}, we can use a small memory:\n2^ bits (27,648 bits total).",
    "818": "We just need calculate the bits, put\nthem into the memory, and use the memory to produce the control signals.",
    "819": "The ``address'' input to the memory are the same 10 bits that we\nneeded for our combinational logic: IR[15:11], R, BEN, and the FSM counter.\nThe data outputs of the memory are the control signals and the\nRESET and PAUSE inputs to the FSM counter.  And we're done.",
    "820": "We can do a little better, though.  The datapath in the textbook was\ndesigned to work with the textbook's control unit.  If we add a little\nlogic, we can significantly simplify our memory-based, hardwired implementation.\nFor example, we only need to pause the FSM counter when waiting for memory.\nIf we can produce a control signal that indicates a need to wait for\nmemory, say {WAIT-MEM}, we can use a couple of gates to compute the\nFSM counter's PAUSE signal as {WAIT-MEM} AND (NOT R).  Making this change\nshrinks our memory to 2^9 bits.  The extra two control\nsignals in this case are RESET and {WAIT-MEM}.",
    "821": "Next, look at how BEN is used in the state diagram:\nthe only use is to terminate the\nprocessing of branch instructions when no branch should occur (when \nBEN=0).  We can fold that functionality into the FSM counter's RESET\nsignal by producing a branch reset signal, {BR-RESET}, to reset\nthe counter to end a branch and a second signal, {INST-DONE},\nwhen an instruction is done.  The RESET input for the FSM counter\nis then ({BR-RESET} AND (NOT BEN)) OR {INST-DONE}.",
    "822": "And our memory further shrinks to 2^8 bits, where the extra\nthree control signals are {WAIT-MEM}, {BR-RESET}, and \n{INST-DONE}.",
    "823": "Finally, recall that the only need for IR[11] is to implement the two\nforms of JSR.  But we can add wires to connect SR1 to PCMUX's fourth \ninput, then control the PCMUX output selection using IR[11] when appropriate\n(using another control signal).  With this extension, we can implement\nboth forms with a single state, writing to both R7 and PC in the same\ncycle.",
    "824": "Our final memory can then be 2^7 bits (3,712 bits total),\nwhich is less than one-seventh the number of bits that we needed before \nmodifying the datapath.",
    "825": "We are now ready to discuss the second approach to control unit design.\nTake another look at the state diagram for the the {LC-3} ISA.  Does it \nremind you of anything?  Like a flowchart, it has relatively few arcs \nleaving each state---usually only one or two.",
    "826": "What if we treat the state diagram as a program?  We can use a small\nmemory to hold { microinstructions} (another name for control words) \nand use the FSM state number as the memory address.",
    "827": "Without support for interrupts or privilege, and with the datapath \nextension for JSR mentioned for hardwired control, the {LC-3} state \nmachine requires fewer than 32 states.",
    "828": "The datapath has 25 control signals, but we need one more for the\ndatapath extension for JSR.",
    "829": "We thus start with {5-bit} state number (in a register)\nand a {2^5 bit} memory, which we call\nour control ROM (read-only memory) to distinguish\nit from the big, slow, von Neumann memory.",
    "830": "Each cycle, the { microprogrammed control} unit applies the FSM state \nnumber to the control ROM (no IR bits, \njust the state number), gets back a set of control signals, and\nuses them to drive the datapath.",
    "831": "To write our microprogram, we need to calculate the control signals\nfor each microinstruction and put them in the control ROM, but we also\nneed to have a way to decide which microinstruction should execute \nnext.  We call the latter problem { sequencing} or microsequencing.",
    "832": "Notice that most of the time there's no choice: we have only { one}\nnext microinstruction.  One simple approach is then to add the address\n(the {5-bit} state ID) of the next microinstruction to the control ROM.\nInstead of 26 bits per FSM state, we now have 31 bits per FSM state.",
    "833": "Sometimes we do need to have two possible next states.  When waiting\nfor memory (the von Neumann memory, not the control ROM) to\nfinish an access, for example, we want our FSM to stay\nin the same state, then move to the next state when the access completes.\nLet's add a second address to each microinstruction, and add\na branch control signal for the microprogram to decide whether we\nshould use the first address or the second for the next\nmicroinstruction.  This design, using a 2^5 bit memory\n(1,152 bits total), appears to the right.",
    "834": "{file=part4/figs/microprogrammed-no-decode.eps,width=2in}",
    "835": "The microprogram branch control signal is a Boolean logic expression\nbased on the memory ready signal R and \nIR[11].  We can implement it with a state ID comparison and\na mux, as shown to the right.  For the branch instruction execution state, \nthe mux selects input 1, BEN.  For all other states, the mux selects \ninput 0, R.  When an FSM state has only a single next state, we set both \nIDs in the control ROM to the ID for that state, so the value of R \nhas no effect.",
    "836": "We can simplify the implementation of the microprogram branch control\n by setting both next-state addresses to the same value when a state\n does not branch.  Then the branch control becomes a don't care for that\n state.",
    "837": "What's missing?  Decode!  We do have one FSM state in which we need to be\nable to branch to one of sixteen possible next states, one for each\nopcode.  Let's just add another mux and choose the state IDs for starting\nto process each opcode in an easy way, as shown to the right with\nextensions highlighted in blue.  The textbook assigns the first state \nfor processing each opcode the IDs IR[15:12] preceeded by two 0s (the\ntextbook's design requires {6-bit} state IDs).  We adopt the same\nstrategy.  For example, the first FSM state for processing an ADD is 00001, \nand the first state for a TRAP is 01111.  In each case, the opcode \nencoding specifies the last four bits.",
    "838": "Now we can pick the remaining state IDs arbitrarily and fill in the \ncontrol ROM with control \nsignals that implement each state's RTL and the two possible next states.\nTransitions from the decode state are handled by the extra mux.",
    "839": "The microprogrammed control unit implementation in Appendix C of \nPatt and Patel is similar to the one that we have developed here,\nbut is slightly more complex so that it can handle interrupts\nand privilege.",
    "840": "This set of notes introduces the idea of using sparsely populated\nrepresentations to protect against accidental changes to bits.\nToday, such representations are used in almost every type of storage\nsystem, from bits on a chip to main memory to disk to archival tapes.",
    "841": "We begin our discussion with examples of representations in which some \nbit patterns have no meaning, then consider what happens when a bit \nchanges accidentally.  We next outline a general scheme \nthat allows a digital system to detect a single bit error.",
    "842": "Building on the mechanism underlying this scheme,\nwe describe a distance metric that enables us to think more broadly about \nboth detecting and correcting such errors, and then show a general\napproach that allows correction of a single bit error.",
    "843": "We leave discussion of more sophisticated schemes to classes on\ncoding and information theory.",
    "844": "Representations used by computers must avoid ambiguity: a single\nbit pattern in a representation cannot be used to represent more than\none value.  However, the converse need not be true.  A representation can\nhave several bit patterns representing the same value, and\nnot all bit patterns in a representation need be\nused to represent values.",
    "845": "Let's consider a few example of representations with unused patterns.\nHistorically, one common class of representations of this type was \nthose used to represent individual decimal digits.  We examine three\nexamples from this class.",
    "846": "The first is Binary-coded Decimal (BCD), in which\ndecimal digits are encoded individually \nusing their representations in the unsigned\n(binary) representation.  Since we have 10 decimal digits, we need \n10 patterns, and four bits for each digit.  But four bits allow \n2^4=16 bit patterns.  In BCD, the patterns 1010,1011,,1111\nare unused.  It is important to note that BCD is not the same as the\nunsigned representation.  The decimal number 732, for example, requires 12 bits\nwhen encoded as BCD: 0111 0011 0010.  When written using a {12-bit}\nunsigned representation, 732 is written 001011011100.  Operations\non BCD values were implemented in early processors, including the\n8086, and are thus still available in the x86 instruction set architecture\ntoday!",
    "847": "The second example is an {Excess-3} code, in which each decimal\ndigit d is represented by the pattern corresponding to the \n{4-bit} unsigned pattern for d+3.  For example, the digit 4\nis represented as 0111, and the digit 7 is represented as 1010.\nThe {Excess-3} code has some attractive aspects when using \nsimple hardware.  For example, we can use a {4-bit} binary \nadder to add two digits c and d represented in the {Excess-3} code, \nand the carry out signal produced by the adder is the same as the carry\nout for the decimal addition, since c+d is equivalent to \n(c+3)+(d+3).",
    "848": "The third example of decimal digit representations is a {2-out-of-5}\ncode.  In such a code, five bits are used to encode each digit.",
    "849": "Only patterns with exactly two 1s are used.  There are exactly ten \nsuch patterns, and an example representation is shown to the right (more\nthan one assignment of values to patterns has been used in real systems).",
    "850": "{c|c}\n& a 2-out-of-5\ndigit& representation \n1& 00011\n2& 00101\n3& 00110\n4& 01001\n5& 01010\n6& 01100\n7& 10001\n8& 10010\n9& 10100\n0& 11000",
    "851": "Errors in digital systems\ncan occur for many reasons, ranging from cosmic ray strikes to\ndefects in chip fabrication to errors in the design of the digital system.",
    "852": "As a simple model, we assume that an error takes the form of changes to\nsome number of bits.  In other words, a bit that should have the value 0\ninstead has the value 1, or a bit that should have the value 1 instead\nhas the value 0.  Such an error is called a { bit error}.",
    "853": "Digital systems can be designed with or without tolerance to errors.\nWhen an error occurs, no notification nor identification of the error\nis provided.  Rather, if error tolerance is needed,\nthe system must be designed to be able to \nrecognize and identify errors automatically.",
    "854": "Often, we assume that each of the bits may be in error independently\nof all of the others, each with some low probability.  With such\nan assumption, multiple bit errors\nare much less likely than single bit errors, and we can focus on \ndesigns that tolerate a single bit error.",
    "855": "When a bit error occurs, however, we must assume that \nit can happen to any of the bits.",
    "856": "The use of many patterns to represent a smaller number of values, as\nis the case in a {2-out-of-5} code, enables a system\nto perform { error detection}.",
    "857": "Let's consider what happens when a value represented using a \n{2-out-of-5} code is subjected to a single bit error.\nImagine that we have the digit 7.  In the table on the previous page,\nnotice that the digit 7 is represented with the pattern 10001.",
    "858": "As we mentioned, we must assume that the bit error can occur in any \nof the five bits, thus we have five possible bit patterns after the error \noccurs.  If the error occurs in the first bit, we have the pattern 00001.\nIf the error occurs in the second bit, we have the pattern 11001.\nThe complete set of possible error patterns is 00001, 11001, 10101,\n10011, and 10000.",
    "859": "Notice that none of the possible error patterns has exactly two 1s, and \nthus none of them is a meaningful pattern in our {2-out-of-5} code.\nIn other words, whenever a digital system represents the digit 7 and \na single bit error occurs, the system will be able to detect that an \nerror has occurred.",
    "860": "What if the system needs to represent a different digit?  \nRegardless of which digit is represented, the pattern with no errors \nhas exactly two 1s,\nby the definition of our representation.  If we then flip one of the \nfive bits by subjecting it to a bit error, the resulting error pattern\nhas either one 1 (if the bit error changes a 1 to a 0) or three 1s\n(if the bit error changes a 0 to a 1).  In other words, regardless of\nwhich digit is represented, and regardless of which bit has an error,\nthe resulting error pattern never has a meaning in \nthe {2-out-of-5} code.  So this representation enables a digital\nsystem to detect any single bit error!",
    "861": "The ability to detect any single bit error is certainly useful.\nHowever, so far we have only shown how to protect ourselves when we\nwant to represent decimal digits.  Do we need to develop a separate\nerror-tolerant representation for every type of information that\nwe might want to represent?  Or can we instead come up with a more\ngeneral approach?",
    "862": "The answer to the second question is yes: we can, in fact, systematically\ntransform any representation into a representation that allows detection of a\nsingle bit error.  The key to this transformation is the idea of\n{ parity}.",
    "863": "Consider an arbitrary representation for some type of information.\nBy way of example, we use the {3-bit} unsigned representation.\nFor each pattern used in the representation, we can count the number\nof 1s.  The resulting count is either odd or even.  By adding an extra\nbit---called a { parity bit}---to the representation, and \nselecting the parity bit's value \nappropriately for each bit pattern, we can ensure that the count of 1s\nis odd (called { odd parity}) or even (called { even parity})\nfor all values represented.  The idea is",
    "864": "illustrated in the table\nto the right for the {3-bit} unsigned representation.  The parity\nbits are shown in bold.",
    "865": "{c|c|c|c|c}\nvalue      &   3-bit & number& with odd& with even \nrepresented& unsigned& of 1s & parity  & parity  \n0& 000& 0& 000{ 1}& 000{ 0}\n1& 001& 1& 001{ 0}& 001{ 1}\n2& 010& 1& 010{ 0}& 010{ 1}\n3& 011& 2& 011{ 1}& 011{ 0}\n4& 100& 1& 100{ 0}& 100{ 1}\n5& 101& 2& 101{ 1}& 101{ 0}\n6& 110& 2& 110{ 1}& 110{ 0}\n7& 111& 3& 111{ 0}& 111{ 1}",
    "866": "Either approach to selecting the parity bits ensures that any single\nbit error can be detected.  For example, if we choose to use odd\nparity, a single\nbit error changes either a 0 into a 1 or a 1 into a 0.\nThe number of 1s in the resulting error pattern thus\ndiffers by exactly one from the original pattern, and the parity of\nthe error pattern is even.  But all valid patterns have odd parity,\nso any single bit error can be detected by simply counting the number\nof 1s.",
    "867": "Next, let's think about how we might use representations---we might\nalso think of them as { codes}---to protect a system against \nmultiple bit errors.  As we have seen with parity, one strategy that\nwe can use to provide such error tolerance is the use of representations\nin which only some of the patterns actually represent values.\nLet's call such patterns { code words}.  In other words, the\ncode words in a representation are those patterns that correspond to \nreal values of information.  Other patterns in the representation have no \nmeaning.",
    "868": "As a tool to help us understand error tolerance, let's define\na measure of the distance between code words in a representation.\nGiven two code words X and Y, we can calculate the number N_ of \nbits that must change to transform X into Y.  Such a calculation\nmerely requires that we compare the patterns bit by bit and count\nthe number of places in which they differ.  Notice that this\nrelationship is symmetric: the same number of changes are required\nto transform Y into X, so N_=N_.",
    "869": "We refer to this number N_ as the { Hamming distance} between\ncode word X and code word Y.",
    "870": "The metric is named after Richard Hamming, a computing pioneer and an \nalumnus of the UIUC Math department.",
    "871": "The Hamming distance between two code words tells us how many bit errors\nare necessary in order for a digital system to mistake one code word\nfor the other.  Given a representation, we can calculate the minimum\nHamming distance between any pair of code words used by the \nrepresentation.  The result is called the { Hamming distance of the \nrepresentation}, and represents the minimum of bit errors that must\noccur before a system might fail to detect errors in a stored value.",
    "872": "The Hamming distance for nearly all of the representations that we \nintroduced in earlier sections is 1.  Since more than half of the\npatterns (and often all of the patterns!) correspond to meaningful\nvalues, some pairs of code words must differ in only one bit, and\nthese representations cannot tolerate any errors.  For example,\nthe decimal value 42 is stored as 101010 using a {6-bit} unsigned\nrepresentation, but any bit error in that pattern produces another \nvalid pattern corresponding to one of the following \ndecimal numbers: 10, 58, 34, 46, 40, 43.  Note that the Hamming distance\nbetween any two patterns is not necessarily 1.  Rather, the Hamming \ndistance of the unsigned representation, which corresponds to the \nminimum between any pair of valid patterns, is 1.",
    "873": "In contrast, the Hamming distance of the {2-out-of-5} code that\nwe discussed earlier is 2.  Similarly, the Hamming distance of any\nrepresentation extended with a parity bit is at least 2.",
    "874": "Now let's think about the problem slightly differently.",
    "875": "Given a particular representation,",
    "876": "how many bit errors can we detect in values using that representation?",
    "877": "{ A representation with Hamming distance d can detect up to d-1 bit errors.}",
    "878": "To understand this claim, start by selecting a code word from the\nrepresentation and changing up to d-1 of the bits.  No matter\nhow one chooses to change the bits, these changes cannot result in\nanother code word, since we know that any other code word has to \nrequire at least d changes from our original code word, by the\ndefinition of the representation's Hamming distance.",
    "879": "A digital system using the representation can thus detect up to d-1\nerrors.  However, if d or more errors occur, the system might sometimes\nfail to detect any error in the stored value.",
    "880": "Detection of errors is important, but may sometimes not be enough.\nWhat can a digital system do when it detects an error?  In some\ncases, the system may be able to find the original value elsewhere, or\nmay be able to re-compute the value from other values.  In other \ncases, the value is simply lost, and the digital system may need\nto reboot or even shut down until a human can attend to it.",
    "881": "Many real systems cannot afford such a luxury.  Life-critical systems\nsuch as medical equipment and airplanes should not turn themselves off\nand wait for a human's attention.  Space vehicles face a similar dilemma,\nsince no human may be able to reach them.",
    "882": "Can we use a strategy similar to the one that we have developed for error\ndetection in order to try to perform { error correction}, recovering\nthe original value?  Yes, but the overhead---the\nextra bits that we need to provide such functionality---is higher.",
    "883": "Let's start by thinking about a code with Hamming distance 2, such\nas {4-bit} 2's complement with odd parity.  We know that such a \ncode can detect one bit error.  Can it correct such a bit error, too?",
    "884": "Imagine that a system has stored the decimal value 6 using the \npattern 0110{ 1}, where the last bit is the odd parity bit.\nA bit error occurs, changing the stored pattern to 0111{ 1}, which is\nnot a valid pattern, since it has an even number of 1s.  But can the\nsystem know that the original value stored was 6?  No, it cannot.\nThe original value may also have been 7, in which case the original\npattern was 0111{ 0}, and the bit error occurred in the final\nbit.  The original value may also have been -1, 3, or 5.  The system\nhas no way of resolving this ambiguity.",
    "885": "The same problem arises if a digital system uses a code with\nHamming distance d to detect up to d-1 errors.",
    "886": "Error correction is possible, however, if we assume that fewer bit\nerrors occur (or if we instead use a representation with a larger Hamming\ndistance).",
    "887": "As a simple example, let's create a representation for the numbers 0\nthrough 3 by making three copies of the {2-bit} unsigned \nrepresentation, as shown to the right.  The Hamming distance of the\nresulting code is 3, so any two bit errors can be detected.  However,\nthis code also enables us to correct a single bit error.  Intuitively, \nthink of the three copies as voting on the right answer.",
    "888": "{c|c}\nvalue      & three-copy\nrepresented& code \n0& 000000\n1& 010101\n2& 101010\n3& 111111",
    "889": "Since a\nsingle bit error can only corrupt one copy, a majority vote always\ngives the right answer!\nTripling the number of bits needed in a representation is not a good\ngeneral strategy, however. \nNotice also that ``correcting'' a pattern with two bit errors can produce\nthe wrong result.",
    "890": "Let's think about the problem in terms\nof Hamming distance.  Assume that we use a code with Hamming distance d\nand imagine that up to k bit errors affect a stored value.\nThe resulting pattern then falls within a neighborhood of distance k\nfrom the original code word.  This neighborhood contains all bit \npatterns within Hamming distance k of the original pattern.\nWe can define such a neighborhood around each code word.  Now, \nsince d bit errors are needed to transform a code word into\nany other code word, these neighborhoods are disjoint so long\nas 2k{d-1}.  In other words, if the inequality holds,\nany bit pattern in the representation can be in at most one code word's \nneighborhood.  The digital system can then correct the errors by \nselecting the unique value identified by the associated neighborhood.\nNote that patterns encountered as a result of up to k bit errors\nalways fall within the original code word's neighborhood; the inequality\nensures that the neighborhood identified in this way is unique.\nWe can manipulate the inequality to express the number of errors k that\ncan be corrected in terms of the Hamming distance d of the code.\n{ A code with Hamming distance d allows up to {d-1}\nerrors to be corrected}, where  represents the\ninteger floor function on x, or rounding x down to the nearest \ninteger.",
    "891": "Hamming also developed a general and efficient approach for \nextending an arbitrary representation to allow correction of\na single bit error.  The approach yields codes with Hamming distance 3.",
    "892": "To understand how a { Hamming code} works, think of the bits in the\nrepresentation as being numbered starting from 1.  For example, if\nwe have seven bits in the code, we might write a bit pattern X \nas x_7x_6x_5x_4x_3x_2x_1.",
    "893": "The bits with indices that are powers of two are parity check bits.\nThese include x_1, x_2, x_4, x_8, and so forth.  The remaining\nbits can be used to hold data.  For example, we could use a {7-bit}\nHamming code and map the bits from a {4-bit} unsigned representation \ninto bits x_7, x_6, x_5, and x_3.  Notice that Hamming codes are\nnot so useful for small numbers of bits, but require only logarithmic\noverhead for large numbers of bits.  That is, in an {N-bit}\nHamming code, only _2(N+1) bits are used for parity\nchecks.",
    "894": "How are the parity checks defined?  Each parity bit is used to\nprovide even parity for those bits with indices for which the index,\nwhen written in binary, includes a 1 in the single position in which\nthe parity bit's index contains a 1.  The x_1 bit, for example,\nprovides even parity on all bits with odd indices.  The x_2 bit\nprovides even parity on x_2, x_3, x_6, x_7, x_, and so\nforth.",
    "895": "In a {7-bit} Hamming code, for example, \nx_1 is chosen so that\nit has even parity together with x_3, x_5, and x_7.",
    "896": "Similarly, x_2 is chosen so that\nit has even parity together with x_3, x_6, and x_7.",
    "897": "Finally, x_4 is chosen so that\nit has even parity together with x_5, x_6, and x_7.",
    "898": "The table to the right shows the result of embedding \na {4-bit} unsigned representation into a {7-bit}\nHamming code.",
    "899": "A Hamming code provides a convenient way to identify\nwhich bit should be corrected when a single bit error occurs.  \nNotice that each bit is protected by a unique subset of the parity bits\ncorresponding to the binary form of the bit's index.  Bit x_6, for\nexample, is protected by bits x_4 and x_2, because the number 6\nis written 110 in binary.\nIf a bit is affected by an error, the parity bits\nthat register the error are those corresponding to 1s in the binary\nnumber of the index.  So if we calculate check bits as 1 to represent\nan error (odd parity) and 0 to represent no error (even parity),\nthen concatenate those bits into a binary number, we obtain the\nbinary value of the index of the single bit affected by an error (or \nthe number 0\nif no error has occurred).",
    "900": "{c|c|c|c|c|c}\n           &  4-bit         &     &     &     &7-bit\nvalue      & unsigned       &     &     &     &Hamming\nrepresented&(x_7x_6x_5x_3)&x_4&x_2&x_1&code \n 0& 0000& 0& 0& 0& 0000000\n 1& 0001& 0& 1& 1& 0000111\n 2& 0010& 1& 0& 1& 0011001\n 3& 0011& 1& 1& 0& 0011110\n 4& 0100& 1& 1& 0& 0101010\n 5& 0101& 1& 0& 1& 0101101\n 6& 0110& 0& 1& 1& 0110011\n 7& 0111& 0& 0& 0& 0110100\n 8& 1000& 1& 1& 1& 1001011\n 9& 1001& 1& 0& 0& 1001100\n10& 1010& 0& 1& 0& 1010010\n11& 1011& 0& 0& 1& 1010101\n12& 1100& 0& 0& 1& 1100001\n13& 1101& 0& 1& 0& 1100110\n14& 1110& 1& 0& 0& 1111000\n15& 1111& 1& 1& 1& 1111111",
    "901": "Let's do a couple of examples based on the pattern for the decimal\nnumber 9, 1001100.  First, assume that no error occurs.  We calculate\ncheck bit c_4 by checking whether x_4, x_5, x_6, and x_7 together\nhave even parity.  Since no error occurred, they do, so c_4=0.\nSimilarly, for c_2 we consider x_2, x_3, x_6, and x_7.\nThese also have even parity, so c_2=0.  Finally, for c_1, we \nconsider x_1, x_3, x_5, and x_7.  As with the others, these\ntogether have even parity, so c_1=0.  Writing c_4c_2c_1, we obtain 000,\nand conclude that no error has occurred.",
    "902": "Next assume that bit 3 has an error, giving us the pattern 1001000.\nIn this case, we have again that c_4=0, but the bits corresponding to\nboth c_2 and c_1 have odd parity, so c2=1 and c_1=1.  Now when\nwe write the check bits c_4c_2c_1, we obtain 011,\nand we are able to recognize that bit 3 has been changed.",
    "903": "A Hamming code can only correct one bit error, however.  If two bit\nerrors occur, correction will produce the wrong answer.  Let's imagine\nthat both bits 3 and 5 have been flipped in our example pattern for\nthe decimal number 9, producing the pattern 1011000.  Calculating the\ncheck bits as before and writing them as c_4c_2c_1, we obtain 110,\nwhich leads us to incorrectly conclude that bit 6 has been flipped.\nAs a result, we ``correct'' the pattern to 1111000, which represents\nthe decimal number 14.",
    "904": "We now consider one final extension of Hamming codes to enable a system\nto perform single error correction while also detecting any two bit errors.\nSuch codes are known as { Single Error Correction, Double Error \nDetection (SEC-DED)} codes.  Creating such a code from a Hamming code is\ntrivial: add a parity bit covering the entire Hamming code.  The extra\nparity bit increases the Hamming distance to 4.  A Hamming distance of 4\nstill allows only single bit error correction, but avoids the problem\nof Hamming distance 3 codes when two bit errors occur, since patterns\nat Hamming distance 2 from a valid code word cannot be within distance 1\nof another code word, and thus cannot be ``corrected'' to the wrong\nresult.",
    "905": "In fact, one can add a parity bit to any representation with an odd\nHamming distance to create a new representation with Hamming distance\none greater than the original representation.  To proof this convenient\nfact, begin with a representation with Hamming distance d, where d\nis odd.  If we choose two code words from the representation, and their \nHamming distance is already greater than d, their distance in the \nnew representation will also be greater than d.  Adding a parity\nbit cannot decrease the distance.  On the other hand, if the two code\nwords are exactly distance d apart, they must have opposite parity,\nsince they differ by an odd number of bits.  Thus the new parity bit\nwill be a 0 for one of the code words and a 1 for the other, increasing\nthe Hamming distance to d+1 in the new representation.  Since all\npairs of code words have Hamming distance of at least d+1, the\nnew representation also has Hamming distance d+1.",
    "906": "With the exception of control unit design strategies and redundancy \nand coding, most of the material in this part of the course is drawn from\nPatt and Patel Chapters 4 through 7.  You may also want to read Patt and \nPatel's Appendix C for details of their control unit design.",
    "907": "In this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).",
    "908": "We'll start with the easy stuff.  You should recognize all of these terms\nand be able to explain what they mean.  \n For the specific circuits, you \n should be able to draw them and explain how they work.\n(You may skip the *'d terms in Fall 2012.)",
    "909": "[t]\n{}{{}{}\n{}{}{}",
    "910": "{von Neumann elements}\n{-}{{}{}\n{}{}{}\n{program counter (PC)}\n{instruction register (IR)}\n{memory address register (MAR)}\n{memory data register (MDR)}\n{processor datapath}",
    "911": "{control signal}\n{instruction processing}",
    "912": "{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (in an encoded instruction)}\n{operation code (opcode)}",
    "913": "{assemblers and assembly code}\n{-}{{}{}\n{}{}{}\n{opcode mnemonic (such as ADD, JMP)}\n{two-pass process}",
    "914": "{symbol table}\n{pseudo-op / directive}",
    "915": "[t]\n{}{{}{}\n{}{}{}",
    "916": "{systematic decomposition}\n{-}{{}{}\n{}{}{}",
    "917": "no documentation, and advanced topics ... no testing",
    "918": "{logic design optimization}\n {-}{{}{}\n {}{}{}\n {bit-sliced (including multiple bits per slice)}\n \n {pipelined logic}\n {tree-based}",
    "919": "{control unit design strategies}\n{-}{{}{}\n{}{}{}\n{control word / microinstruction}\n{sequencing / microsequencing}\n{hardwired control}\n{-}{{}{}\n{}{}{}\n{single-cycle}\n{multi-cycle}",
    "920": "{microprogrammed control}\n {pipelining (of instruction processing)}",
    "921": "{error detection and correction\n{--}{{}{}\n{}{}{}\n code/sparse representation\n code word\n bit error\n odd/even parity bit\n Hamming distance between code words\n Hamming distance of a code\n Hamming code\n SEC-DED",
    "922": "}",
    "923": "We expect you to be able to exercise the following skills:",
    "924": "{}{{}{}\n{}{}{}",
    "925": "FIXME ... should write something about critical path, but expecting\n them to do much with it isn't reasonable",
    "926": "{Implement arbitrary Boolean logic, and be able to reason about\n alternative designs in terms of their critical path delays and number\n of gates required to implement.}",
    "927": "{Map RTL (register transfer language) operations into control words\nfor a given processor datapath.}",
    "928": "{Systematically decompose a (simple enough) problem to the level \nof {LC-3} instructions.}",
    "929": "{Encode {LC-3} instructions into machine code.}",
    "930": "{Read and understand programs written in {LC-3} assembly/machine code.}",
    "931": "{Test and debug a small program in {LC-3} assembly/machine code.}",
    "932": "{Be able to calculate the Hamming distance of a code/representation.}",
    "933": "{Know the relationships between Hamming distance and the abilities\nto detect and to correct bit errors.}",
    "934": "We expect that you will understand the concepts and ideas to the extent\nthat you can do the following:",
    "935": "{}{{}{}\n{}{}{}",
    "936": "{Explain the role of different types of instructions in allowing\na programmer to express a computation.}",
    "937": "FIXME: ISA design is beyond the scope of this course; just include\n for interest, if at all",
    "938": "{Explain the tradeoffs in different addressing modes so as to motivate\n inclusion of multiple addressing modes in an ISA.}",
    "939": "{Explain the importance of the three types of subdivisions in systematic\ndecomposition (sequential, conditional, and iterative).}",
    "940": "{Explain the process of transforming assembly code into machine code\n(that is, explain how an assembler works, including describing the use of\nthe symbol table).}",
    "941": "{Be able to use parity for error detection, and Hamming codes for\nerror correction.}",
    "942": "At the highest level, \nwe hope that, while you do not have direct substantial experience in \nthis regard from our class (and should not expect to be tested on these\nskills), that you will nonetheless be able to begin\nto do the following when designing combinational logic:",
    "943": "{}{{}{}\n{}{}{}",
    "944": "{Design and compare implementations using gates, decoders, muxes, and/or memories \nas appropriate, and including reasoning about the relevant design tradeoffs \nin terms of area and delay.}",
    "945": "{Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based \ndesign, again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}",
    "946": "{Design and compare implementations of processor control units\nusing both hardwired and microprogrammed strategies,\nand again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}",
    "947": "{Understand basic tradeoffs in the sparsity of code words with error \ndetection and correction capabilities.}",
    "948": "{   }   blank 3rd page",
    "949": "In this set of notes, we illustrate basic logic design using integer\naddition as an example.  By recognizing and mimicking the structured \napproach used by humans to perform addition, we introduce an important \nabstraction for logic design.  We follow this approach to design an\nadder known as a ripple-carry adder, then discuss some of the \nimplications of the approach and highlight how the same approach can \nbe used in software.  In the next set of notes, we use the same\ntechnique to design a comparator for two integers.",
    "950": "Many of the operations that we want to perform on groups of bits can \nbe broken down into repeated operations on individual bits.",
    "951": "When we add two binary numbers, for example, we first add the least\nsignificant bits, then move to the second least significant, and so on.\nAs we go, we may need to carry from lower bits into higher bits.",
    "952": "When we compare two (unsigned) binary numbers with the same number of bits,\nwe usually start with the most significant bits and move downward in\nsignificance until we find a difference or reach the end of the two \nnumbers.  In the latter case, the two numbers are equal.",
    "953": "When we build combinational logic to implement this kind of calculation,\nour approach as humans can be leveraged as an abstraction technique.\nRather than building and optimizing a different Boolean function for \nan 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size\nthat we might want, we can instead design a circuit that adds a single\nbit and passes any necessary information into another copy of itself.\nBy using copies of this { bit-sliced} adder circuit, we can\nmimic our approach as humans and build adders of any size, just as\nwe expect that a human could add two binary numbers of any size.",
    "954": "The resulting designs are, of course, slightly less efficient than\ndesigns that are optimized for their specific purpose (such as adding\ntwo 17-bit numbers), but the simplicity of the approach makes the\ntradeoff an interesting one.",
    "955": "Think about how we as humans add two {N-bit}\nnumbers, A and B.",
    "956": "An illustration appears to the right, using N=8.",
    "957": "For now, let's assume that our numbers are stored in an unsigned \nrepresentation.",
    "958": "As you know, addition for 2's complement is identical except for the\ncalculation of overflow.",
    "959": "We start adding from the least significant bit and move to the left.\nSince adding two 1s can overflow a single bit, we carry a 1 when\nnecessary into the next column.  Thus, in general, we are actually\nadding three input bits.  The carry from the previous column is usually\nnot written explicitly by humans, but in a digital system\nwe need to write a 0 instead of leaving the value blank.",
    "960": "Focus now on the addition of a single column.  Except for the\nfirst and last bits, which we might choose to handle slightly \ndifferently, the addition process is identical \nfor any column.  We add a carry in bit (possibly 0) with one\nbit from each of our numbers to produce a sum bit and a carry\nout bit for the next column.  Column addition is the task\nthat our bit slice logic must perform.",
    "961": "The diagram to the right shows an abstract model of our \nadder bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming\nfrom the top or left \nand outputs going to the bottom or right.  Outside\nof the bit slice logic, we index the carry bits using the",
    "962": "{{file=part2/figs/add-abs.eps,width=1.90in}}",
    "963": "bit number.  The bit slice has C^M provided as an input and \nproduces C^{M+1} as an output.",
    "964": "Internally, we use C_ to denote the carry input,\nand C_ to denote the carry output.",
    "965": "Similarly, the\nbits A_M and B_M from the numbers A and B are\nrepresented internally as A and B, and the bit S_M produced for\nthe sum S is represented internally as S.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.",
    "966": "The abstract device for adding three inputs bits and producing\ntwo output bits is called a { full adder}.  You may \nalso encounter the term { half adder}, which adds only two\ninput bits.  To form an {N-bit} adder, we integrate N\ncopies of the full adder---the bit slice that we design next---as \nshown below.  The result is called a { ripple carry adder}\nbecause the carry information moves from the low bits to the high\nbits slowly, like a ripple on the surface of a pond.",
    "967": "{{file=part2/figs/add-integrated.eps,width=5.5in}}",
    "968": "Now we are ready to design our adder bit slice.\nLet's start by writing a truth table for C_ and S,\nas shown on the left below.",
    "969": "To the right of the truth tables are {K-maps} for each output,\nand equations for each output are then shown to the right of the \n{K-maps}.",
    "970": "We suggest that you work through identification of the prime implicants \nin the {K-maps} and check your work with the equations.",
    "971": "{\n[t]\n{ccc|cc}\nA& B& C_& C_& S \n0& 0& 0& 0& 0\n0& 1& 0& 0& 1\n0& 0& 1& 0& 1\n0& 1& 1& 1& 0\n1& 0& 0& 0& 1\n1& 0& 1& 1& 0\n1& 1& 0& 1& 0\n1& 1& 1& 1& 1",
    "972": "{file=part2/figs/add-cin.eps,width=1in}\n{file=part2/figs/add-s.eps,width=1in}",
    "973": "{eqnarray*}\nC_&=&A B+A C_+B C_   \nS&=&A B C_+A  {C_+\n&& B {C_+  C_\n&=&A{C_\n{eqnarray*}",
    "974": "}",
    "975": "The equation for C_ implements a { majority function}\non three bits.  In particular, a carry is produced whenever at least two \nout of the three input bits (a majority) are 1s.  Why do we mention\nthis name?  Although we know that we can build any logic function from\nNAND gates, common functions such as those used to add numbers may\nbenefit from optimization.  Imagine that in some technology, creating\na majority function directly may produce a better result than implementing\nsuch a function from logic gates.  In such a case, we want the \nperson designing the circuit to know that can make use of such an \nimprovement.",
    "976": "We rewrote the equation for S to make use of the XOR operation for \na similar reason: the implementation of XOR gates from transistors\nmay be slightly better than the implementation of XOR based on NAND gates.\nIf a circuit designer provides an optimized variant of XOR, we want\nour design to make use of the optimized version.",
    "977": "{{file=part2/figs/adder-bit.eps,width=2.55in}\n{file=part2/figs/adder-bit-nn.eps,width=2.55in}}",
    "978": "The gate diagrams above implement a single bit slice for an adder.\nThe version on\nthe left uses AND and OR gates (and an XOR for the sum), while the\nversion on the right uses NAND gates, leaving the XOR as an XOR.",
    "979": "Let's discuss the design in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\nFor each bit, we need three {2-input} NAND gates, one {3-input}\nNAND gate, and a {3-input} XOR gate (a big gate; around 30 transistors).",
    "980": "For speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.",
    "981": "We can thus estimate our design's\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer.\nHere we have two gate delays from any of the inputs\nto the C_ output.  The XOR gate may be a little slower, but\nnone of its inputs come from other gates anyway.",
    "982": "When we connect multiple copies of our bit slice logic together to \nform an adder, the A and B inputs\nto the outputs is not as important as the delay from C_\nto the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis---this propagation delay\ngives rise to the name ``ripple carry.''\nLooking again at the diagram, \nnotice that we have two gate delays from C_\nto C_.\nThe total delay for an {N-bit} comparator based on this\nimplementation is thus two gate delays per bit, \nfor a total of 2N gate delays.",
    "983": "Now that we know how to build an {N-bit} adder, we can add\nsome detail to the diagram that we drew when we \nintroduced 2's complement back in Notes Set 1.2, as shown to the right.",
    "984": "The adder is important enough to computer systems to merit its own\nsymbol in logic diagrams, which is shown to the right with the inputs\nand outputs from our design added as labels.  The text in the middle\nmarking the symbol as an adder is only included for clarity: { any time \nyou see a symbol of the shape shown to the right, it is an adder} (or \nsometimes a device that can add and do other operations).  The width \nof the operand input and output lines then tells you the size of the \nadder.",
    "985": "{file=part2/figs/adder-trad.eps,width=1.3in}",
    "986": "You may already know that most computers have a { word size}\nspecified as part of the Instruction Set Architecture.  The word\nsize specifies the number of bits in each operand when the computer\nadds two numbers, and is often used widely within the \nmicroarchitecture as well (for example, to decide the number of \nwires to use when moving bits around).  Most desktop and laptop machines\nnow have a word size of 64 bits, but many phone processors (and\ndesktops/laptops a few years ago) use a {32-bit} word size.\nEmbedded microcontrollers may use a {16-bit} or even \nan {8-bit} word size.",
    "987": "Having seen how we can build an {N-bit} adder from simple\nchunks of logic operating on each pair of bits, you should not have\nmuch difficulty in understanding the diagram to the right.",
    "988": "If we start with a design for an {N-bit} adder---even if that\ndesign is not built from bit slices, but is instead optimized for\nthat particular size---we can create a {2N-bit} adder by \nsimply connecting two copies of the {N-bit} adder.  We give\nthe adder for the less significant bits (the one on the right\nin the figure) an initial carry of 0,\nand pass the carry produced by the adder for the less significant\nbits into the carry input of the adder for the more significant\nbits.  We calculate overflow based on the results of the adder\nfor more significant bits (the one on the left in the figure), \nusing the method appropriate to the \ntype of operands we are adding (either unsigned or 2's complement).",
    "989": "{file=part2/figs/adder-x2.eps,width=2.15in}",
    "990": "You should also realize that this connection need not be physical.\nIn other words, if a computer has an {N-bit} adder, it can\nhandle operands with 2N bits (or 3N, or 10N, or 42N) by\nusing the {N-bit} adder repeatedly, starting with the\nleast significant bits and working upward until all of the bits\nhave been added.  The computer must of course arrange to have the\noperands routed to the adder a few bits at a time, and must\nensure that the carry produced by each addition is then delivered to\nthe carry input (of the same adder!) for the next addition.\nIn the coming months, you will learn how to design hardware that\nallows you to manage bits in this way, so that by the end of our\nclass, you will be able to design a simple computer on your own.",
    "991": "These notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.",
    "992": "The difficulty of learning depends on the type of task involved.\nRemembering new terminology is relatively easy, while applying\nthe ideas underlying design decisions shown by example to new problems \nposed as human tasks is relatively hard.",
    "993": "In this short summary, we give you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).",
    "994": "We'll start with the skills, and leave the easy stuff for the next page.",
    "995": "We expect you to be able to exercise the following skills:",
    "996": "{}{{}{}\n{}{}{}",
    "997": "{Design a CMOS gate for a simple Boolean function from n-type \nand p-type transistors.}",
    "998": "{Apply DeMorgan's laws repeatedly to simplify the form of\nthe complement of a Boolean expression.}",
    "999": "{Use a K-map to find a reasonable expression for a Boolean function (for\nexample, in POS or SOP form with the minimal number of terms).}",
    "1000": "{More generally, translate Boolean logic functions among \nconcise algebraic, truth table, K-map, and canonical (minterm/maxterm) forms.}",
    "1001": "When designing combinational logic, we expect you to be able to apply\nthe following design strategies:",
    "1002": "{}{{}{}\n{}{}{}",
    "1003": "{Make use of human algorithms \n(for example, multiplication from addition).}",
    "1004": "{Determine whether a bit-sliced approach is applicable, and, if so,\nmake use of one.}",
    "1005": "{Break truth tables into parts so as to solve each part of a function \nseparately.}",
    "1006": "{Make use of known abstractions (adders, comparators, muxes, or other\nabstractions available to you) to simplify the problem.}",
    "1007": "And, at the highest level, we expect that you will be able to do the following:",
    "1008": "{}{{}{}\n{}{}{}",
    "1009": "{Understand and be able to reason at a high-level about circuit design\ntradeoffs between area/cost and performance (and to know that power is also \nimportant, but we haven't given you any quantification methods).}",
    "1010": "{Understand the tradeoffs typically made to develop bit-sliced \ndesigns---typically, bit-sliced designs are simpler but bigger and \nslower---and how one can develop variants between the extremes of\nthe bit-sliced approach and optimization of functions specific\nto an {N-bit} design.}",
    "1011": "{Understand the pitfalls of marking a function's value as ``don't care'' \nfor some input combinations, and recognize that implementations do not \nproduce ``don't care.''}",
    "1012": "{Understand the tradeoffs involved in selecting a representation for\ncommunicating information between elements in a design, such as the bit \nslices in a bit-sliced design.}",
    "1013": "{Explain the operation of a latch or a flip-flop, particularly in \nterms of the bistable states used to hold a bit.}",
    "1014": "{Understand and be able to articulate the value of the clocked \nsynchronous design abstraction.}",
    "1015": "You should recognize all of these terms\nand be able to explain what they mean.  For the specific circuits, you \nshould be able to draw them and explain how they work.",
    "1016": "Actually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.",
    "1017": "[t]\n{}{{}{}\n{}{}{}",
    "1018": "{Boolean functions and logic gates}\n{-}{{}{}\n{}{}{}",
    "1019": "{majority function}",
    "1020": "{specific logic circuits}\n{-}{{}{}\n{}{}{}\n{full adder}\n{half adder}\n{ripple carry adder}\n N-to-M multiplexer (mux)\n N-to-2N decoder\n{{- latch}}\n{{R-S latch}}\n{gated D latch}",
    "1021": "SSL altered term 3 Dec 21",
    "1022": "{master-slave implementation of a positive edge-triggered D flip-flop}\n{dual-latch implementation of a positive edge-triggered D flip-flop}\n{(bidirectional) shift register}\n{register supporting parallel load}",
    "1023": "{design metrics}\n{-}{{}{}\n{}{}{}",
    "1024": "{power, area/cost, performance}\n{computer-aided design (CAD) tools}\n{gate delay}",
    "1025": "{general math concepts}\n{-}{{}{}\n{}{}{}\n{canonical form}\n{domain of a function}\n{{N-dimensional} hypercube}",
    "1026": "{tools for solving logic problems}\n{-}{{}{}\n{}{}{}\n{truth table}\n{Karnaugh map (K-map)}",
    "1027": "{prime implicant}\n{bit-slicing}\n{timing diagram}",
    "1028": "[t]\n{}{{}{}\n{}{}{}",
    "1029": "{device technology}\n{-}{{}{}\n{}{}{}\n{complementary metal-oxide semiconductor (CMOS)}\n{field effect transistor (FET)}\n{transistor gate, source, drain}",
    "1030": "{Boolean logic terms}\n{-}{{}{}\n{}{}{}",
    "1031": "{algebraic properties}\n{dual form, principle of duality}\n{sum, product}\n{minterm, maxterm}\n{sum-of-products (SOP)}\n{product-of-sums (POS)}\n{canonical sum/SOP form}\n{canonical product/POS form}\n{logical equivalence}",
    "1032": "{digital systems terms}\n{-}{{}{}\n{}{}{}\n{word size}\n{{N-bit} Gray code}\n{combinational/combinatorial logic}\n{-}{{}{}\n{}{}{}\n{two-level logic}\n{``don't care'' outputs (x's)}",
    "1033": "{sequential logic}\n{-}{{}{}\n{}{}{}",
    "1034": "{active low input}\n{set a bit (to 1)}\n{reset a bit (to 0)}",
    "1035": "SSL altered term 3 Dec 21",
    "1036": "{master-slave implementation}\n{dual-latch implementation}\n{positive edge-triggered}",
    "1037": "{clock signal}\n{-}{{}{}\n{}{}{}\n{square wave}\n{rising/positive clock edge}\n{falling/negative clock edge}\n{clock gating}",
    "1038": "{clocked synchronous sequential circuit}\n{parallel/serial load of register}\n FIXME?  too informal to ask them to remember it\n {glue logic}\n{logical/arithmetic/cyclic shift}",
    "1039": "{   }  blank 3rd page",
    "1040": "This set of notes introduces registers, an abstraction used for \nstorage of groups of bits in digital systems.  We introduce some\nterminology used to describe aspects of register design and\nillustrate the idea of a shift register.  The registers shown here\nare important abstractions for digital system design.",
    "1041": "{ In the Fall 2012 offering of our course, we will cover this\n material on the third midterm.}",
    "1042": "A { register} is a storage element composed from one or more\nflip-flops operating on a common clock.",
    "1043": "In addition to the flip-flops,\nmost registers include logic to control the bits stored by the register.",
    "1044": "For example, D flip-flops \ncopy their inputs at the rising edge of each clock cycle,\ndiscarding whatever bits they have stored before the rising edge\n(in the previous clock cycle).",
    "1045": "To enable a flip-flop to retain its value, we might try to hide the \nrising edge of the clock from the flip-flop, as shown to the right.",
    "1046": "The LOAD input controls the clock signals through a method known as\n{ clock gating}.",
    "1047": "{{file=part2/figs/lec16-1a.eps,width=2.45in}}\n{file=part2/figs/lec16-1b.eps,width=3.4in}",
    "1048": "When LOAD is high, the circuit reduces to a\nregular D flip-flop.  When LOAD is low, the flip-flop clock \ninput, c, is held high, and the flip-flop stores its \ncurrent value.",
    "1049": "The problems with clock gating are twofold.  First, adding logic to\nthe clock path introduces clock skew, which may cause timing problems\nlater in the development process (or, worse, in future projects that",
    "1050": "use your circuits as components).  Second, in the design shown above,\nthe LOAD signal\ncan only be lowered while the clock is high to prevent spurious rising\nedges from causing incorrect behavior, as shown in the timing diagram.",
    "1051": "A better approach is to use a mux and a feedback loop from the \nflip-flop's output, as shown in the figure to the right.",
    "1052": "When LOAD is low, the mux selects the feedback line, and the \nregister reloads its current value.",
    "1053": "When LOAD is high, the mux selects the IN input, and the register \nloads a new value.",
    "1054": "The result is similar to a gated D latch with distinct write enable \nand clock lines.",
    "1055": "{file=part2/figs/lec16-2.eps,width=2in}",
    "1056": "We can use this extended flip-flop as a bit slice for a multi-bit register.",
    "1057": "A four-bit register of this type\nis shown to the right.  Four data {lines---one} for each\n{bit---enter} the registers from the top of the figure.  \nWhen LOAD is low, the logic copies each flip-flop's value back to its\ninput,",
    "1058": "{file=part2/figs/lec16-3.eps,width=5in}",
    "1059": "and the IN input lines are ignored.  When LOAD is high,\nthe muxes forward each IN line to the corresponding flip-flop's D\ninput, allowing the register to load the new {4-bit} value.\nThe use of one input line per bit to load a multi-bit\nregister in a single cycle is termed a { parallel load}.",
    "1060": "Certain types of registers include logic to manipulate data held\nwithin the register.  A { shift register} is an important example\nof this",
    "1061": "{file=part2/figs/lec16-4.eps,width=5in}",
    "1062": "type.  The simplest shift register is a series of D flip-flops,\nwith the output of each attached to the input of the next, as shown to the\nright above.  In the circuit shown, a serial input SI accepts a single bit \nof data per cycle and delivers the bit four cycles later to a serial \noutput SO.  Shift registers serve many purposes in modern systems, from the\nobvious uses of providing a fixed delay and performing bit shifts for\nprocessor arithmetic to rate matching between components and reducing\nthe pin count on programmable logic devices such as field programmable\ngate arrays (FPGAs), the modern form of the programmable logic array\nmentioned in the textbook.",
    "1063": "An example helps to illustrate the rate matching problem: \nhistorical I/O buses used fairly slow clocks, as they had to\ndrive signals and be arbitrated over relatively long distances.\nThe Peripheral Control\nInterconnect (PCI) standard, for example, provided for 33 and 66 MHz\nbus speeds.  To provide adequate data rates, such buses use many wires\nin parallel, either 32 or 64 in the case of PCI.  In contrast, a\nGigabit Ethernet (local area network) signal travelling over a fiber\nis clocked at 1.25 GHz, but sends only one bit per cycle.  Several\nlayers of shift registers sit between the fiber and the I/O bus to\nmediate between the slow, highly parallel signals that travel over the\nI/O bus and the fast, serial signals that travel over the \nfiber.  The latest variant of PCI, PCIe (e for ``express''),\nuses serial lines at much higher clock rates.",
    "1064": "Returning to the figure above, imagine that the outputs Q_i feed\ninto logic clocked at 1/4^ the rate of the shift register \n(and suitably synchronized).  Every four cycles, the flip-flops fill\nup with another four bits, at which point the outputs are read in\nparallel.  The shift register shown can thus serve to transform serial\ndata to {4-bit-parallel} data at one-quarter the clock speed.\nUnlike the registers discussed earlier, the shift register above does\nnot support parallel load, which prevents it from transforming a slow,\nparallel stream of data into a high-speed serial stream.  The use of\n{ serial load} requires N cycles for an {N-bit}\nregister, but can reduce the number of wires needed to support the\noperation of the shift register.  How would you add support for\nparallel load?  How many additional inputs would be necessary?",
    "1065": "The shift register above also shifts continuously, and cannot store a \nvalue.  A set of muxes, analogous to those that we used to control \nregister loading, can be applied to control shifting, as shown \nbelow.",
    "1066": "{{file=part2/figs/lec16-5.eps,width=5.3in}}",
    "1067": "Using a {4-to-1} mux, we can construct a shift\nregister with additional functionality.  The bit slice at the top\nof the next page allows us to build a { bidirectional shift register} with \nparallel load capability and the ability to retain its value indefinitely.\nThe two-bit control input C uses a representation that\nwe have chosen for the four operations supported by our shift register, \nas shown in the table below the bit slice design.",
    "1068": "The bit slice allows us to build {N-bit} shift registers by\nreplicating the slice and adding a fixed amount of ``{ glue logic}.''\nFor example, the figure below represents a {4-bit} bidirectional \nshift register constructed in this way.  The mux\nused for the SO output logic is the glue logic needed in addition\nto the four bit slices.",
    "1069": "At each rising clock edge, the action specified by C_1C_0 is taken.  \nWhen C_1C_0=00, the\nregister holds its current value, with the register\nvalue appearing on\nQ[3:0] and each flip-flop feeding its output back into its input.\nFor C_1C_0=01, the shift register shifts left: the serial input,\nSI, is fed into flip-flop 0, and Q_3 is passed to the serial\noutput, SO.  Similarly, when C_1C_0=11, the shift register shifts\nright: SI is fed into flip-flop 3, and Q_0 is passed to SO.\nFinally, the case C_1C_0=10 causes all flip-flops to accept new\nvalues from IN[3:0], effecting a parallel load.",
    "1070": "{file=part2/figs/lec16-6.eps,width=2.3in}\n{c|c}\nC_1C_0& meaning \n00& retain current value\n01& shift left (low to high)\n10& load new value (from IN)\n11& shift right (high to low)",
    "1071": "{-4pt}",
    "1072": "{{file=part2/figs/lec16-7.eps,width=5.2in}}",
    "1073": "Several specialized shift operations are used to support data\nmanipulation in modern processors (CPUs).  Essentially, these\nspecializations dictate the glue logic for a shift\nregister as well as the serial input value.  The simplest is a {\nlogical shift}, for which SI is hardwired to 0: incoming\nbits are always 0.  A { cyclic shift} takes SO and feeds it\nback into SI, forming a circle of register bits through which the\ndata bits cycle.",
    "1074": "Finally, an { arithmetic shift} treats the shift register contents\nas a number in 2's complement form.  For non-negative numbers and left\nshifts, an arithmetic shift is the same as a logical\nshift.  When a negative number is arithmetically shifted to\nthe right, however, the sign bit is retained, resulting in a function\nsimilar to division by two.  The difference lies in the rounding\ndirection.  Division by two rounds towards zero in most \nprocessors: -5/2 gives -2.\nArithmetic shift right rounds away from zero for negative numbers (and\ntowards zero for positive numbers): -5>>1 gives -3.  We transform our\nprevious shift register into one capable of arithmetic shifts by\neliminating the serial input and feeding the most significant bit,\nwhich represents the sign in 2's complement form, back into itself for\nright shifts, as shown below.  The bit shifted in for left shifts\nhas been hardwired to 0.",
    "1075": "{{file=part2/figs/lec16-8.eps,width=5.2in}}",
    "1076": "This set of notes begins with a brief illustration of a few properties\nof Boolean logic, which may be of use to you in manipulating algebraic\nexpressions and in identifying equivalent logic functions without resorting \nto truth tables.",
    "1077": "We then discuss the value of underspecifying a logic function so as\nto allow for selection of the simplest possible implementation.",
    "1078": "This technique must be used carefully to avoid incorrect behavior,\nso we illustrate the possibility of misuse with an example, then\ntalk about several ways of solving the example correctly.",
    "1079": "We conclude by generalizing the ideas in the example to several\nimportant application areas and talking about related problems.",
    "1080": "Table  (on the next page)\nlists a number of properties of Boolean logic.\nMost of these are easy to derive from our earlier definitions, but\na few may be surprising to you.  In particular, in the algebra of\nreal numbers, multiplication\ndistributes over addition, but addition does not distribute over\nmultiplication.  For example, 3{(4+7)}=(3)+(3),\nbut 3+(4)=(3+4){(3+7)}.  In Boolean algebra,\nboth operators distribute over one another, as indicated in\nTable .  The consensus properties may also be \nnonintuitive.  Drawing a {K-map} may help you understand the\nconsensus property on the right side of the table.  For the \nconsensus variant on the left side of the table,\nconsider that since either A or  must be 0,\neither B or C or both must be 1 for the first two factors on the left\nto be 1 when ANDed together.  But in that case, the third factor \nis also 1, and is thus redundant.",
    "1081": "As mentioned previously, Boolean\nalgebra has an elegant symmetry known as a duality, in which any\nlogic statement (an expression or an equation) is related to a\nsecond logic statement.",
    "1082": "To calculate the { dual form} of a Boolean expression or equation, \nreplace 0 with 1, replace 1 with 0, \nreplace AND with OR, and\nreplace OR with AND.",
    "1083": "{ Variables are not changed when finding the dual form.}",
    "1084": "The dual form of a dual form is the original logic statement.",
    "1085": "Be careful when calculating a dual form: our convention for ordering \narithmetic operations is broken by the exchange, so you may want\nto add explicit parentheses before calculating the dual.  For\nexample, the dual of AB+C is not A+BC.\nRather, the dual of AB+C is (A+B)C.",
    "1086": "{ Add parentheses as necessary when calculating a dual form to ensure\nthat the order of operations does not change.}",
    "1087": "Duality has several useful practical applications.  First, the \n{ principle of duality} states that any theorem or identity has \nthe same truth value in dual form (we do not prove the principle\nhere).",
    "1088": "The rows of Table  are organized according to this\nprinciple: each row contains two equations that are the duals \nof one another.  \n However, we rename variables freely to\n simplify the dual form.  For example, taking the dual of 1+A=1,\n we obtain 0=0.  But we created variable A \n solely for the purpose of expressing the property, so we might\n instead have called the complementary value A.  We rename\n (using the same name!) to obtain 0=0, as shown in the table.",
    "1089": "Second, the dual form is useful when designing certain types of logic,\nsuch as the networks of transistors connecting the output of a CMOS\ngate to high voltage and ground.  If you look at the gate designs in\nthe textbook (and particularly those in the exercises),\nyou will notice that these networks are duals.",
    "1090": "A function/expression is not a theorem nor an identity,\nthus the principle of duality does not apply to the dual of an\nexpression.",
    "1091": "However, if you treat the value 0 as ``true,'' the dual form of an\nexpression has the same truth values as the original (operating with\nvalue 1 as ``true'').",
    "1092": "Finally, you can calculate the complement of a Boolean function\n(any expression) by calculating the dual form and then complementing\neach variable.",
    "1093": "When we specify how something works using a human language, we\nleave out details.  Sometimes we do so deliberately, assuming that a\nreader or listener can provide the details themselves: ``Take me to the \nairport!''\nrather than ``Please bend your right arm at the elbow and shift your\nright upper arm forward so as to place your hand near the ignition key.\nNext, ...''",
    "1094": "{\n{|lll|}\n1+A=1& 0=0&\n1=A& 0+A=A&\nA+A=A& A=A&\nA=0& A+=1&\n{A+B}= &\n=+& DeMorgan's laws\n(A+B)C=AC+BC&\nA B+C=(A+C)(B+C)&distribution\n(A+B)(+C)(B+C)=(A+B)(+C)&\nA B+ C+B C=A B+ C& consensus",
    "1095": "}\n{Boolean logic properties.  The two columns are dual forms of\none another.}",
    "1096": "You know the basic technique for implementing a Boolean function\nusing { combinational logic}: use a {K-map} to identify a\nreasonable SOP or POS form, draw the resulting design, and perhaps\nconvert to NAND/NOR gates.",
    "1097": "When we develop combinational logic designs, we may also choose to\nleave some aspects unspecified.  In particular, the value of a\nBoolean logic function to be implemented may not matter for some\ninput combinations.  If we express the function as a truth table,\nwe may choose to mark the function's value for some input combinations \nas ``{ don't care},'' which is written as ``x'' (no quotes).",
    "1098": "What is the benefit of using ``don't care'' values?",
    "1099": "Using ``don't care'' values allows you to choose from among several\npossible logic functions, all of which produce the desired results\n(as well as some combination of 0s and 1s in place of the ``don't\ncare'' values).",
    "1100": "Each input combination marked as ``don't care'' doubles the number\nof functions that can be chosen to implement the design, often enabling \nthe logic needed for implementation to be simpler.",
    "1101": "For example, the {K-map} to the right specifies a function F(A,B,C)\nwith two ``don't care'' entries.",
    "1102": "If you are asked to design combinational logic for this function,\nyou can\nchoose any values for the two ``don't care'' entries.  When identifying\nprime implicants, each ``x'' can either be a 0 or a 1.",
    "1103": "Depending on the choices made for the x's, we obtain one of \nthe following four functions:",
    "1104": "{eqnarray*}\nF&=& B+B C\nF&=& B+B C+A  \nF&=&B\nF&=&B+A \n{eqnarray*}",
    "1105": "Given this set of choices, a designer typically chooses the third: F=B,\nwhich corresponds to the {K-map} shown to the right of the \nequations.  The design then \nproduces F=1 when A=1, B=1, and C=0 (ABC=110), \nand produces F=0 when A=1, B=0, and C=0 (ABC=100).\nThese differences are marked with shading and green italics in the new\n{K-map}.  No implementation ever produces an ``x.''",
    "1106": "What can go wrong?",
    "1107": "In the context of a digital system, unspecified details\nmay or may not be important.  However, { any implementation of \na specification \nimplies decisions} about these details, so decisions should only be left \nunspecified if any of the possible answers is indeed acceptable.",
    "1108": "As a concrete example, let's design logic to control an ice cream \ndispenser.  The dispenser has two flavors, lychee and mango, but\nalso allows us to create a blend of the two flavors.\nFor each of the two flavors, our logic must output two bits\nto control the amount of ice cream that comes out of the dispenser.\nThe two-bit C_L[1:0] output of our logic must specify the number \nof half-servings of lychee ice cream as a binary number, and\nthe two-bit C_M[1:0] output must specify the number of \nhalf-servings of mango ice cream.  Thus, for either flavor,\n00 indicates none of that flavor,\n01 indicates one-half of a serving, and \n10 indicates a full serving.",
    "1109": "Inputs to our logic will consist of three buttons: an L button to\nrequest a serving of lychee ice cream, a B button to request a\nblend---half a serving of each flavor, and an M button to request\na serving of mango ice cream.  Each button produces a 1 \nwhen pressed and a 0 when not pressed.",
    "1110": "Let's start with the assumption that the user only presses one button\nat a time.  In this case, we can treat input combinations in which\nmore than one button is pressed as ``don't care'' values in the truth\ntables for the outputs.  K-maps for all four output bits appear below.\nThe x's indicate ``don't care'' values.",
    "1111": "When we calculate the logic function for an output, each ``don't care''\nvalue can be treated as either 0 or 1, whichever is more convenient\nin terms of creating the logic.  In the case of C_M[1], for \nexample, we can treat the three x's in the ellipse as 1s, treat the x\noutside of the ellipse as a 0, and simply\nuse M (the implicant represented by the ellipse) for C_M[1].  The\nother three output bits are left as an exercise, although the result \nappears momentarily.",
    "1112": "The implementation at right takes full advantage of the ``don't care''\nparts of our specification.  In this case, we require no logic at all;\nwe need merely connect the inputs to the correct outputs.  Let's verify\nthe operation.  We have four cases to consider.  First, if none of the \nbuttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00 \nand C_L=00).  Second, if we request lychee ice cream (LBM=100), the\noutputs are C_L=10 and C_M=00, so we get a full serving of lychee\nand no mango.  Third, if we request a blend (LBM=010), the outputs\nare C_L=01 and C_M=01, giving us half a serving of each flavor.\nFinally, if we request mango ice cream (LBM=001), we get no lychee\nbut a full serving of mango.",
    "1113": "The K-maps for this implementation appear below.  Each of\nthe ``don't care'' x's from the original design has been replaced with\neither a 0 or a 1 and highlighted with shading and green italics.\nAny implementation produces \neither 0 or 1 for every output bit for every possible input combination.",
    "1114": "{{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}",
    "1115": "As you can see, leveraging ``don't care'' output bits can sometimes\nsignificantly simplify our logic.  In the case of this example, we were\nable to completely eliminate any need for gates!  Unfortunately, \nthe resulting implementation may sometimes produce unexpected results.",
    "1116": "Based on the implementation, what happens if a user presses more\nthan one button?  The ice cream cup overflows!",
    "1117": "Let's see why.",
    "1118": "Consider the case LBM=101, in which we've pressed both the lychee and\nmango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a \nfull serving of each flavor, or two servings total.  Pressing other \ncombinations may have other repercussions as well.\nConsider pressing lychee and blend (LBM=110).  The outputs are then\nC_L=11 and C_M=01.  Hopefully the dispenser simply gives us one\nand a half servings of lychee and a half serving of mango.  However,\nif the person who designed the dispenser assumed that no one would ever\nask for more than one serving, something worse might happen.  In other\nwords, giving an input of C_L=11 to the ice cream dispenser may lead to\nother unexpected behavior if its designer decided that that input \npattern was a ``don't care.''",
    "1119": "The root of the problem is that { while we don't care about the value of\nany particular output marked ``x'' for any particular input combination,\nwe do actually care about the relationship between the outputs}.",
    "1120": "What can we do?  When in doubt, it is safest to make \nchoices and to add the new decisions to the specification rather than \nleaving output values specified as ``don't care.''",
    "1121": "For our ice cream dispenser logic, rather than leaving the outputs \nunspecified whenever a user presses more than one button, we could \nchoose an acceptable outcome for each input combination and \nreplace the x's with 0s and 1s.  We might, for example, decide to\nproduce lychee ice cream whenever the lychee button is pressed, regardless\nof other buttons (LBM=1xx, which means that we don't care about the\ninputs B and M, so LBM=100, LBM=101, LBM=110, or LBM=111).  \nThat decision alone covers three of the\nfour unspecified input patterns.  We might also decide that when the \nblend and mango buttons are pushed together (but without the lychee\nbutton, LBM=011), our logic produces a blend.",
    "1122": "The resulting K-maps are shown below, again with shading and green italics \nidentifying\nthe combinations in which our original design specified ``don't care.''",
    "1123": "{{file=part2/figs/CLhigh-priority.eps,width=1.00in}{file=part2/figs/CLlow-priority.eps,width=1.00in}{file=part2/figs/CMhigh-priority.eps,width=1.00in}{file=part2/figs/CMlow-priority.eps,width=1.00in}}",
    "1124": "The logic in the dashed box to the right implements the set of choices\njust discussed, and matches the K-maps above.",
    "1125": "Based on our additional choices, this implementation enforces\na strict priority scheme on the user's button presses.",
    "1126": "If a user requests lychee, they can also press either or\nboth of the other buttons with no effect.  The lychee button has\npriority.  Similarly, if the user does not press lychee, but press-",
    "1127": "es the blend button, pressing the mango button at the same time has no\neffect.  Choosing mango requires that no other buttons be pressed.\nWe have thus chosen a prioritization order for the buttons and imposed \nthis order on the design.",
    "1128": "We can view this same implementation in another way.",
    "1129": "Note the one-to-one correspondence between inputs (on the left) and \noutputs (on the right) for the dashed box.",
    "1130": "This logic takes the user's button presses and chooses at most one of\nthe buttons to pass along to our original controller implementation \n(to the right of the dashed box).",
    "1131": "In other words, rather than thinking of the logic in the dashed\nbox as implementing a specific set of decisions, we can think of the\nlogic as cleaning up the inputs to ensure that only valid\ncombinations are passed to our original implementation.",
    "1132": "Once the inputs are cleaned up, the original implementation is \nacceptable, because input combinations containing more than a \nsingle 1 are in fact impossible.",
    "1133": "Strict prioritization is one useful way to clean up our inputs.",
    "1134": "In general, we can design logic to map each\nof the four undesirable input patterns into one of the permissible \ncombinations (the\nfour that we specified explicitly in our original design, with LBM \nin the set ).  Selecting a prioritization scheme\nis just one approach for making these choices in a way\nthat is easy for a user to understand and \nis fairly easy to implement.",
    "1135": "A second simple approach is to ignore illegal\ncombinations by mapping them into the ``no buttons pressed'' \ninput pattern.",
    "1136": "Such an implementation appears to the right, laid out to show that\none can again view the logic in the dashed box either as cleaning up \nthe inputs (by mentally grouping the logic with the inputs) or as a specific \nset of choices for our ``don't care'' output values (by grouping the \nlogic with the outputs).",
    "1137": "In either case, the logic shown \nenforces our as-",
    "1138": "sumptions in a fairly conservative way:\nif a user presses more than one button, the logic squashes all button\npresses.  Only a single 1 value at a time can pass through to \nthe wires on the right of the figure.",
    "1139": "For completeness, the K-maps corresponding to this implementation are given\nhere.",
    "1140": "{{file=part2/figs/CLhigh-conserve.eps,width=1.00in}{file=part2/figs/CLlow-conserve.eps,width=1.00in}{file=part2/figs/CMhigh-conserve.eps,width=1.00in}{file=part2/figs/CMlow-conserve.eps,width=1.00in}}",
    "1141": "The approaches that we illustrated to clean up the input signals to\nour design have application in many areas.  The ideas in this \nsection are drawn from the field and are sometimes the subjects of \nlater classes, but { are not exam material for our class.}",
    "1142": "Prioritization of distinct\ninputs is used to arbitrate between devices attached to a\nprocessor.  Processors typically execute much more quickly than do devices.\nWhen a device needs attention, the device signals the processor\nby changing the voltage on an interrupt line (the name comes from the\nidea that the device interrupts the processor's current activity, such\nas running a user program).  However, more than\none device may need the attention of the processor simultaneously, so\na priority encoder is used to impose a strict order on the devices and\nto tell the processor about their needs one at a time.",
    "1143": "If you want to learn more about this application, take ECE391.",
    "1144": "When components are designed together, assuming that some input patterns\ndo not occur is common practice, since such assumptions can dramatically\nreduce the number of gates required, improve performance, reduce power\nconsumption, and so forth.  As a side effect, when we want to test a\nchip to make sure that no defects or other problems prevent the chip\nfrom operating correctly, we have to be careful so as not to ``test''\nbit patterns that should never occur in practice.  Making up random bit\npatterns is easy, but can produce bad results or even destroy the chip\nif some parts of the design have assumed that a combination produced \nrandomly can never occur.  To avoid these problems, designers add extra\nlogic that changes the disallowed patterns into allowed patterns, just\nas we did with our design.  The use of random bit patterns is common in\nBuilt-In Self Test (BIST), and so the process of inserting extra logic\nto avoid problems is called BIST hardening.  BIST hardening can add\n{10-20} additional logic to a design.",
    "1145": "Our graduate class on digital system testing, ECE543, covers this\nmaterial, but has not been offered recently.",
    "1146": "The second part of the course covers digital design more\ndeeply than does the textbook.  The lecture notes will explain the\nadditional material, and we will provide further examples in lectures\nand in discussion sections.  Please let us know if you need further\nmaterial for study.",
    "1147": "In the last notes, we introduced Boolean logic operations and showed\nthat with AND, OR, and NOT, we can express any Boolean function on any\nnumber of variables.",
    "1148": "Before you begin these notes, please read the first two sections \nin Chapter 3 of the textbook,\nwhich discuss the operation of { complementary metal-oxide semiconductor}\n({ CMOS}) transistors, illustrate how gates implementing the\nAND, OR, and NOT operations can be built using transistors,\nand introduce DeMorgan's laws.",
    "1149": "This set of notes exposes you to a mix of techniques,\nterminology, tools, and philosophy.  Some of the material is not\ncritical to our class (and will not be tested), but is useful for\nyour broader education, and may help you in later classes.  The value\nof this material has changed substantially in the last couple of decades,\nand particularly in the last few years, as algorithms for tools\nthat help with hardware design have undergone rapid advances.  We\ntalk about these issues as we introduce the ideas.",
    "1150": "The notes begin with a discussion of the ``best'' way to\nexpress a Boolean function and some techniques used historically\nto evaluate such decisions.",
    "1151": "We next introduce the terminology necessary to understand manipulation\nof expressions, and use these terms to explain the Karnaugh map, or\n{K-map}, a tool that we will use for many purposes this semester.",
    "1152": "We illustrate the use of {K-maps} with a couple of\nexamples, then touch on a few important questions and useful\nways of thinking about Boolean logic.",
    "1153": "We conclude with a discussion of the general problem of \nmulti-metric optimization, introducing some ideas and approaches\nof general use to engineers.",
    "1154": "In the notes on logic operations, you learned how to express an\narbitrary function on bits as an OR of minterms (ANDs with one\ninput per variable on which the function operates). \nAlthough this approach demonstrates logical completeness, the results\noften seem inefficient, as you can see by comparing the following \nexpressions for the carry out C from the\naddition of two {2-bit} unsigned numbers, A=A_1A_0 and B=B_1B_0.",
    "1155": "C &=& A_1B_1 + (A_1+B_1)A_0B_0  \n&=& A_1B_1 + A_1A_0B_0 + A_0B_1B_0  \n&=& \n {A_1} A_0 B_1 B_0 +\nA_1 {A_0} B_1 {B_0} +\nA_1 {A_0} B_1 B_0 +\n&&A_1 A_0 {B_1} B_0 +\nA_1 A_0 B_1 {B_0} +\nA_1 A_0 B_1 B_0",
    "1156": "These three expressions are identical in the sense that they have\nthe same truth tables---they are the same mathematical function.",
    "1157": "Equation () is the form that we gave when we introduced\nthe idea of using logic to calculate overflow.  In this form, we were\nable to explain the terms intuitively.",
    "1158": "Equation () results from distributing the parenthesized OR\nin Equation ().",
    "1159": "Equation () is the result of our logical completeness\nconstruction.",
    "1160": "Since the functions are identical, does the form actually matter at all?\nCertainly either of the first two forms is easier for us to write \nthan is the third.  If we think\nof the form of an expression as a mapping from the function that we\nare trying to calculate into the AND, OR, and NOT functions that we\nuse as logical building blocks, we might also say that the first two\nversions use fewer building blocks.  That observation does have some\ntruth, but let's try to be more precise by framing a question.",
    "1161": "For any given function, there are an infinite number of ways that we can\nexpress the function (for example, given one variable A on which the\nfunction depends, you can OR together any number of copies \nof A without changing the function).",
    "1162": "{ What exactly makes one expression better than another?}",
    "1163": "In 1952, Edward Veitch wrote an article on simplifying truth functions.\nIn the introduction, he said, ``This general problem can be very complicated\nand difficult.  Not only does the complexity increase greatly with the\nnumber of inputs and outputs, but the criteria of the best circuit will\nvary with the equipment involved.''",
    "1164": "Sixty years later, the answer is largely the same: the criteria depend\nstrongly on the underlying technology (the gates and the devices used\nto construct the gates), and no single { metric}, or way of measuring,\nis sufficient to capture the important differences between expressions \nin all cases.",
    "1165": "Three high-level metrics commonly used to evaluate chip\ndesigns are cost, power, and performance.  Cost usually represents the\nmanufacturing cost, which is closely related to the physical silicon\narea required for the design: the larger the chip, the more expensive\nthe chip is to produce.  Power measures energy consumption over\ntime.  A chip that consumes more power means that a user's energy bill\nis higher and, in a portable device, either that the device is heavier or has\na shorter battery life.  Performance measures the speed at which the\ndesign operates.  A faster design can offer more functionality, such as\nsupporting the latest games, or can just finish the same work in less\ntime than a slower design.  These metrics are sometimes related: if\na chip finishes its work, the chip can turn itself off, saving energy.",
    "1166": "How do such high-level metrics relate to the problem at hand?  Only\nindirectly in practice.  There are too many factors involved to make\ndirect calculations of cost, power, or performance at the level of\nlogic expressions.",
    "1167": "Finding an { optimal} solution---the best formulation of a specific\nlogic function for a given metric---is often impossible using the \ncomputational resources and algorithms available to us.",
    "1168": "Instead, tools typically use heuristic approaches\nto find solutions that strike a balance between these metrics.\nA { heuristic} approach is one that is\nbelieved to yield fairly good solutions to a problem, but does\nnot necessarily find an optimal solution.",
    "1169": "A human engineer can typically impose { constraints}, such as\nlimits on the chip area or limits on the minimum performance,\nin order to guide the process.",
    "1170": "Human engineers may also restructure the implementation of a \nlarger design, such as a design to perform floating-point arithmetic,\nso as to change the logic functions used in the design.",
    "1171": "{ Today, manipulation of logic expressions for the purposes of \noptimization is performed almost entirely by computers.}  Humans must\nsupply the logic functions of interest, and must program the acceptable \ntransformations between equivalent forms, but computers do the grunt\nwork of comparing alternative formulations and deciding which one is\nbest to use in context.",
    "1172": "Although we believe that hand optimization of Boolean expressions is no\nlonger an important skill for our graduates, we do think that you\nshould be exposed to the ideas and metrics historically used for\nsuch optimization.  The rationale for retaining this exposure is \nthreefold.",
    "1173": "First, we believe that you still need to be able to perform basic\nlogic reformulations (slowly is acceptable)\nand logical equivalence checking (answering the question, ``Do two \nexpressions represent the same function?'').",
    "1174": "Second, the complexity of the problem is a good way to introduce you\nto real engineering.",
    "1175": "Finally, the contextual information will help you to develop a better\nunderstanding of finite state machines and higher-level abstractions\nthat form the core of digital systems and are still defined directly\nby humans today.",
    "1176": "Towards that end, we conclude this introduction by discussing two\nmetrics that engineers traditionally used to optimize \nlogic expressions.  These metrics are now embedded in { computer-aided \ndesign} ({ CAD}) tools and tuned to specific underlying technologies,\nbut the reasons for their use are still interesting.",
    "1177": "The first metric of interest is a heuristic for the area needed for\na design.",
    "1178": "The measurement is simple: count the number of variable occurrences in\nan expression.  Simply go through and add up how many variables you see.\nUsing our example function C, \nEquation () gives a count of 6,\nEquation () gives a count of 8, and\nEquation () gives a count of 24.\nSmaller numbers represent better expressions, so \nEquation () is the best choice by this metric.",
    "1179": "Why is this metric interesting?\nRecall how gates are built from transistors.\nAn {N-input} gate requires roughly 2N transistors, so if you \ncount up the number of variables in the\nexpression, you get an estimate of the number of transistors needed,\nwhich is in turn an estimate for the area required for the design.",
    "1180": "A variation on variable counting is to add the number of operations,\nsince each gate also takes space for wiring (within as well as between\ngates).  Note that we ignore the number of inputs to the operations,\nso a {2-input} AND counts as 1, but a {10-input} AND also counts\nas 1.  We do not usually count complementing variables as an operation\nfor this metric because the complements of variables are sometimes \navailable at no extra cost in gates or wires.",
    "1181": "If we add the number of operations in our example, we get\na count of 10 for Equation ()---two ANDs, two ORs,\nand 6 variables,",
    "1182": "a count of 12 for Equation ()---three ANDS, one OR,\nand 8 variables,",
    "1183": "and a count of 31 for Equation ()---six ANDs, one OR,\nand 24 variables.",
    "1184": "The relative differences between these equations \nare reduced when one counts operations.",
    "1185": "A second metric of interest is a heuristic for the performance of a design.",
    "1186": "Performance is inversely related to the delay necessary for a design\nto produce an output once its inputs are available.  For example, if\nyou know how many seconds it takes to produce a result, you can easily\ncalculate the number of results that can be produced per second, which \nmeasures performance.",
    "1187": "The measurement needed is the longest chain of operations\nperformed on any instance of a variable.  The complement of\na variable is included if the variable's complement is not \navailable without using an inverter.",
    "1188": "The rationale for this metric is that gate outputs do not change \ninstantaneously when their inputs\nchange. Once an input to a gate has reached an appropriate voltage to represent \na 0 or a 1, the transistors in the gate switch (on or off) and electrons \nstart to move.\nOnly when the output of the gate reaches the appropriate new voltage \ncan the gates driven by the output start to change. \nIf we count each\nfunction/gate as one delay (we call this time a { gate delay}), we \nget an estimate of the time needed to compute\nthe function.",
    "1189": "Referring again to our example equations, we find that\nEquation () requires 3 gate delays,\nEquation () requires 2 gate delays,\nEquation () requires 2 or 3 gate delays, depending\non whether we have variable complements available.  Now \nEquation () looks more attractive: better performance than\nEquation () in return for a small extra cost in area.",
    "1190": "Heuristics for estimating energy use are too \ncomplex to introduce at this point, \nbut you should be aware that every time electrons move, they\ngenerate heat, so we might favor an expression that minimizes the number\nof bit transitions inside the computation.  Such a measurement is\nnot easy to calculate by hand,\nsince you need to know the likelihood of input combinations.",
    "1191": "We use many technical terms when we talk about simplification of\nlogic expressions, so we now introduce those terms so as to make\nthe description of the tools and processes easier to understand.",
    "1192": "Let's assume that we have a logic function F(A,B,C,D) that we want\nto express concisely.  A { literal} in an expression of F refers \nto either one of the variables or its complement.  In other words,\nfor our function F, the following is a complete set of literals:\nA, , B, ,\nC, , D, and .",
    "1193": "When we introduced the AND and OR functions, we also introduced \nnotation borrowed from arithmetic, using multiplication to represent AND\nand addition to represent OR.  We also borrow the related terminology, \nso a { sum} \nin Boolean algebra refers to a number of terms OR'd together \n(for example, A+B, or AB+CD), and\na { product} in Boolean algebra refers to a number of terms AND'd\ntogether (for example, A, or AB(C+D).  Note that\nthe terms in a sum or product may themselves be sums, products, or\nother types of expressions (for example, A).",
    "1194": "The construction method that we used to demonstrate logical completeness\nmade use of minterms for each input combination for which the \nfunction F produces a 1.",
    "1195": "We can now use the idea of a literal to give a simpler definition of minterm:\na { minterm} for a function on N variables is a product (AND \nfunction) of N literals in which each variable or its complement \nappears exactly once.  For our function F, examples of minterms\ninclude ABC,\nACD, and\nBC.\nAs you know, a minterm produces a 1 for exactly\none combination of inputs.",
    "1196": "When we sum minterms for each output value of 1\nin a truth table to express a function, as we did to obtain\nEquation (), we\nproduce an example of the sum-of-products form.\nIn particular, a { sum-of-products} ({ SOP}) is a sum composed of \nproducts of literals.\nTerms in a sum-of-products need not be minterms, however.\nEquation () is also in sum-of-products form.\nEquation (), however, is not, since the last\nterm in the sum is not a product of literals.",
    "1197": "Analogously to the idea of a minterm, \nwe define a { maxterm} for a function on N variables as a sum (OR \nfunction) of N literals in which each variable or its complement \nappears exactly once.  Examples for F include (A+B++D),\n(A+++D), and\n(++C+).\nA maxterm produces a 0 for exactly\none combination of inputs.  Just as we did with minterms, we can\nmultiply a maxterm corresponding to each input combination for which\na function produces 0 (each row in a truth table that produces a 0\noutput) to create\nan expression for the function.  The resulting expression is in\na { product-of-sums} ({ POS}) form: a product of sums of literals.",
    "1198": "The carry out function that we used to produce \nEquation () has 10 input combinations that produce 0,\nso the expression formed in this way is unpleasantly long:",
    "1199": "{eqnarray*}\nC&=&\n({A_1}+{A_0}+B_1+B_0)\n({A_1}+A_0+B_1+{B_0})\n({A_1}+A_0+B_1+B_0)\n(A_1+{A_0}+{B_1}+B_0)\n&&(A_1+{A_0}+B_1+{B_0})\n(A_1+{A_0}+B_1+B_0)\n(A_1+A_0+{B_1}+{B_0})\n(A_1+A_0+{B_1}+B_0)\n&&(A_1+A_0+B_1+{B_0})\n(A_1+A_0+B_1+B_0)\n{eqnarray*}",
    "1200": "However, the approach can be helpful with functions that produce mostly 1s.\nThe literals in maxterms are complemented with respect to the\nliterals used in minterms.  For example, the maxterm\n({A_1}+{A_0}+B_1+B_0) in the equation above\nproduces a zero for input combination A_1=1, A_0=1, B_1=0, B_0=0.",
    "1201": "An { implicant} G of a function F is defined to be a second\nfunction operating on the same variables for which\nthe implication G is true.  In terms of logic functions\nthat produce 0s and 1s, { if G is an implicant of F, \nthe input combinations for which G produces 1s are a subset of \nthe input combinations for which F produces 1s}.",
    "1202": "Any minterm for which F produces a 1, for example, is an implicant of F.",
    "1203": "In the context of logic design, { the term implicant is used\nto refer to a single product of literals.}  In other words, if we\nhave a function F(A,B,C,D), examples of possible implicants of F\ninclude AB, B, ABC, and .\nIn contrast, although they may technically imply F, we typically\ndo not call expressions such as (A+B), C(+D), nor\nA+C implicants.",
    "1204": "Let's say that we have expressed function F in sum-of-products \nform.\nAll of the individual product terms in the expression are implicants of F.",
    "1205": "As a first step in simplification, we can ask: for each implicant, is it\npossible to remove any of the literals that make up the product?",
    "1206": "If we have an implicant G for which the answer is no, we \ncall G a { prime implicant} of F.",
    "1207": "In other words, if one removes any of the literals from a prime \nimplicant G of F,\nthe resulting product is not an implicant of F.",
    "1208": "Prime implicants are the main idea that we use to simplify logic expressions,\nboth algebraically and with graphical tools (computer tools use algebra\ninternally---by graphical here we mean drawings on paper).",
    "1209": "Veitch's 1952 paper was the first to introduce the idea of\nusing a graphical representation to simplify logic expressions.\nEarlier approaches were algebraic.\nA year later, Maurice Karnaugh published\na paper showing a similar idea with a twist.  The twist makes the use of \n{ Karnaugh maps} to simplify expressions\nmuch easier than the use of Veitch charts.  As a result,\nfew engineers have heard of Veitch, but everyone who has ever\ntaken a class on digital logic knows how to make use of a { K-map}.",
    "1210": "Before we introduce the Karnaugh map, let's think about the structure\nof the domain of a logic function.  Recall that a function's { domain}\nis the space on which the function is defined, that is, for which the\nfunction produces values.  For a Boolean logic function on N variables,\nyou can think of the domain as sequences of N bits, but you can also\nvisualize the domain as an {N-dimensional} hypercube.  \nAn",
    "1211": "{ {N-dimensional} hypercube} is the generalization\nof a cube to N dimensions.  Some people only use the term hypercube when\nN, since we have other names for the smaller values: a point\nfor N=0, a line segment for N=1, a square for N=2, and a cube\nfor N=3.  The diagrams above and to the right illustrate the cases that \nare easily drawn on paper.  The black dots represent specific input\ncombinations, and the blue edges connect input combinations that differ\nin exactly one input value (one bit).",
    "1212": "By viewing a function's domain in this way, we can make a connection\nbetween a product of literals and the structure of the domain.  Let's\nuse the {3-dimensional} version as an example.  We call the \nvariables A, B, and C, and note that the cube has 2^3=8 corners\ncorresponding to the 2^3 possible combinations\nof A, B, and C.\nThe simplest product of literals in this\ncase is 1, which is the product of 0 literals.  Obviously, the product 1 \nevaluates to 1 for any variable values.  We can thus think of it as\ncovering the entire domain of the function.  In the case of our example,\nthe product 1 covers the whole cube.  In order for the product 1 to \nbe an implicant of a function, the function itself must be the function 1.",
    "1213": "What about a product consisting of a single literal, such as A \nor ?  The dividing lines in the diagram illustrate the\nanswer: any such product term evaluates to 1 on a face of the cube,\nwhich includes 2^2=4 of the corners.  If a function evaluates to 1\non any of the six faces of the cube, the corresponding product term\n(consisting of a single literal) is an implicant of the function.",
    "1214": "Continuing with products of two literals, we see that any product of \ntwo literals, such as A or C, corresponds\nto an edge of our {3-dimensional} cube.  The edge includes 2^1=2\ncorners.  And, if a function evaluates to 1 on any of the 12 edges\nof the cube, the corresponding product term (consisting of two literals)\nis an implicant of the function.",
    "1215": "Finally, any product of three literals, such as B,\ncorresponds to a corner of the cube.  But for a function on three variables,\nthese are just the minterms.  As you know, if a function evaluates to 1\non any of the 8 corners of the cube, that minterm is an implicant of the\nfunction (we used this idea to construct the function to prove\nlogical completeness).",
    "1216": "How do these connections help us to simplify functions?  If we're\ncareful, we can map cubes onto paper in such a way that product terms\n(the possible implicants of the function) usually form contiguous \ngroups of 1s, allowing us to spot them easily.  Let's work upwards\nstarting from one variable to see how this idea works.  The end result\nis called a Karnaugh map.",
    "1217": "The first drawing shown to the right replicates our view of \nthe {1-dimensional} hypercube, corresponding to the domain of a\nfunction on one variable, in this case the variable A.  \nTo the right of the hypercube (line segment) are two variants\nof a Karnaugh map on one variable.  The middle variant clearly \nindicates the column corresponding to the product A (the other \ncolumn corresponds to ).  The right variant simply labels\nthe column with values for A.",
    "1218": "The three drawings shown to the right illustrate the three possible product\nterms on one variable.  { The functions shown in \nthese Karnaugh maps are arbitrary, except that we have chosen\nthem such that each implicant shown is a prime implicant for the\nillustrated function.}",
    "1219": "Let's now look at two-variable functions.  We have replicated\nour drawing of the {2-dimensional} hypercube (square) to the right \nalong with two variants of Karnaugh maps on two variables.\nWith only two variables (A and B), the extension is fairly \nstraightforward, since we can use the second dimension of the \npaper (vertical) to express the second variable (B).",
    "1220": "The number of possible products of literals grows rapidly with the \nnumber of variables.\nFor two variables, nine are possible, as shown to the right.\nNotice that all implicants have two properties.  First, they\noccupy contiguous regions of the grid.  And, second, their height\nand width are always powers of two.  These properties seem somewhat\ntrivial at this stage, but they are the key to the utility of\n{K-maps} on more variables.",
    "1221": "Three-variable functions are next.  The cube diagram is again replicated\nto the right.  But now we have a problem: how can we map four points\n(say, from the top half of the cube) into a line in such a way that \nany points connected by a blue line are adjacent in the {K-map}?\nThe answer is that we cannot, but we can preserve most of the connections\nby choosing an order such as the one illustrated by the arrow.\nThe result",
    "1222": "is called a Gray code.  Two {K-map} variants again appear\nto the right of the cube.  Look closely at the order of the two-variable\ncombinations along the top, which allows us to have as many contiguous\nproducts of literals as possible.  Any product of literals that contains\n but not A nor  wraps around the edges\nof the {K-map}, so you should think of it as rolling up into a cylinder \nrather than a grid.  Or you can think that we're unfolding the cube to\nfit the corners onto a sheet of paper, but the place that we split\nthe cube should still be considered to be adjacent when looking for \nimplicants.  The use of a Gray code is the one difference between a\n{K-map} and a Veitch chart; Veitch used the base 2 order, which\nmakes some implicants hard to spot.",
    "1223": "With three variables, we have 27 possible products of literals.  You\nmay have noticed that the count scales as 3^N for N variables;\ncan you explain why?  We illustrate several product terms\nbelow.  Note that we sometimes\nneed to wrap around the end of the {K-map}, but that if we account\nfor wrapping, the squares covered by all product terms are contiguous.\nAlso notice that both the width and the height of all product terms \nare powers of two.  { Any square or rectangle that meets these two \nconstraints corresponds to a product term!}  And any such square or \nrectangle that is filled with 1s is an implicant of the function\nin the {K-map}.",
    "1224": "Let's keep going.  With a function on four variables---A, B, C, \nand D---we can use a Gray code order on two of the variables in \neach dimension.  Which variables go with which dimension in the grid\nreally doesn't matter, so we'll assign AB to the horizontal dimension\nand CD to the vertical dimension.  A few of the 81 possible product\nterms are illustrated at the top of the next page.  Notice that while wrapping \ncan now occur\nin both dimensions, we have exactly the same rule for finding implicants\nof the function: any square or rectangle (allowing for wrapping) \nthat is filled with 1s and has both height and width equal to (possibly\ndifferent) powers of two is an implicant of the function.  { \nFurthermore, unless such a square or rectangle is part of a larger\nsquare or rectangle that meets these criteria, the corresponding\nimplicant is a prime implicant of the function.}",
    "1225": "Finding a simple expression for a function using a {K-map} then\nconsists of solving the following problem: pick a minimal set of prime \nimplicants such that every 1 produced by the function is covered\nby at least one prime implicant.  The metric that you choose to\nminimize the set may vary in practice, but for simplicity, let's say \nthat we minimize the number of prime implicants chosen.",
    "1226": "Let's try a few!  The table on the left below reproduces (from\nNotes Set 1.4) the truth\ntable for addition of two {2-bit} unsigned numbers, A_1A_0\nand B_1B_0, to produce a sum S_1S_0 and a carry out C.\n{K-maps} for each output bit appear to the right.\nThe colors are used only to make the different prime implicants\neasier to distinguish.\nThe equations produced by summing these prime implicants appear\nbelow the {K-maps}.",
    "1227": "{cccc|ccc}\n{c|}& \nA_1& A_0& B_1& B_0& C& S_1& S_0 \n0&0& 0&0& 0& 0&0\n0&0& 0&1& 0& 0&1\n0&0& 1&0& 0& 1&0\n0&0& 1&1& 0& 1&1\n0&1& 0&0& 0& 0&1\n0&1& 0&1& 0& 1&0\n0&1& 1&0& 0& 1&1\n0&1& 1&1& 1& 0&0\n1&0& 0&0& 0& 1&0\n1&0& 0&1& 0& 1&1\n1&0& 1&0& 1& 0&0\n1&0& 1&1& 1& 0&1\n1&1& 0&0& 0& 1&1\n1&1& 0&1& 1& 0&0\n1&1& 1&0& 1& 0&1\n1&1& 1&1& 1& 1&0",
    "1228": "{eqnarray*}\nC&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0",
    "1229": "S_1&=&\nA_1 {B_1} {B_0}+\nA_1 {A_0} {B_1}+\n{A_1} {A_0} B_1+\n{A_1} B_1 {B_0}+\n&&{A_1} A_0 {B_1} B_0+\nA_1 A_0 B_1 B_0",
    "1230": "S_0&=&A_0 {B_0}+{A_0} B_0\n{eqnarray*}",
    "1231": "In theory, {K-maps} extend to an arbitrary number of variables.\nCertainly Gray codes can be extended.  An { {N-bit} Gray code} \nis a sequence of {N-bit} patterns that includes all possible\npatterns such that any two adjacent patterns differ in only one bit.\nThe code is actually a cycle: the first and last patterns also differ\nin only one bit.  You can construct a Gray code recursively as follows:\nfor an {(N+1)-bit} Gray code, write the sequence for an\n{N-bit} Gray code, then add a 0 in front of all patterns.\nAfter this sequence, append a second copy of the {N-bit} Gray\ncode in reverse order, then put a 1 in front of all patterns in the\nsecond copy.  The result is an {(N+1)-bit} Gray code.\nFor example, the following are Gray codes:",
    "1232": "1-bit& 0, 1\n2-bit& 00, 01, 11, 10\n3-bit& 000, 001, 011, 010, 110, 111, 101, 100\n4-bit& \n0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100,\n1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000",
    "1233": "Unfortunately, some of the beneficial properties of {K-maps} do\nnot extend beyond two variables in a dimension.  { Once you have three\nvariables in one dimension}, as is necessary if a function operates\non five or more variables, { not all product terms are contiguous\nin the grid}.  The terms still require a total number of rows and columns\nequal to a power of two, but they don't all need to be a contiguous\ngroup.  Furthermore, { some contiguous groups of appropriate size do\nnot correspond to product terms}.  So you can still make use of {K-maps}\nif you have more variables, but their use is a little trickier.",
    "1234": "What if we want to compare two expressions to determine whether they\nrepresent the same logic function?  Such a comparison is a test of\n{ logical equivalence}, and is an important part of hardware design.\nTools today provide help with this problem, but you should understand\nthe problem.",
    "1235": "You know that any given function can be expressed in many ways, and that\ntwo expressions that look quite different may in fact represent the\nsame function (look back at Equations () to ()\nfor an example).  But what if we rewrite the function using only \nprime implicants?  Is the result unique?  Unfortunately, no.",
    "1236": "In general, { a sum of products is not unique (nor is a product of sums),\neven if the sum contains only prime implicants}.",
    "1237": "For example, consensus terms may or may not be included in our\nexpressions.  (They are necessary for reliable design of \ncertain types of systems, as you will learn in a later ECE class.)",
    "1238": "The green ellipse in the K-map to the right represents the consensus \nterm BC.\n{eqnarray*}\nZ &=& A C +  B + B C\nZ &=& A C +  B\n{eqnarray*}",
    "1239": "Some functions allow several equivalent formulations as\nsums of prime implicants, even without consensus terms.  \nThe K-maps shown to the right, for example, illustrate \nhow one function might be written in either of the following ways:",
    "1240": "{eqnarray*}\nZ &=&\n  D +\n C  +\nA B C +\nB  D\nZ &=&\n  C +\nB C  +\nA B D +\n  D\n{eqnarray*}",
    "1241": "When we need to compare two things (such as functions), we need to\ntransform them into what in mathematics is known as a { canonical form},\nwhich simply means a form that is defined so as to be unique \nfor each thing of the given type.\nWhat can we use for logic functions?  You already know two answers!",
    "1242": "The { canonical sum} of a function (sometimes called the\n{ canonical SOP form}) is the sum of minterms.",
    "1243": "The { canonical product} of a function (sometimes called the\n{ canonical POS form}) is the product of maxterms.",
    "1244": "These forms technically\nonly meet the mathematical definition of canonical if we agree on an order\nfor the min/maxterms, but that problem is solvable.",
    "1245": "However, as you already know, the forms are not particularly \nconvenient to use.",
    "1246": "In practice, people and tools in the industry use more compact \napproaches when comparing functions, but those solutions are a subject for\na later class (such as ECE 462).",
    "1247": "{ Two-level logic} is a popular way of expressing logic functions.\nThe two levels refer simply to the number of functions through which an\ninput passes to reach an output, and both the SOP and POS forms are \nexamples of two-level logic.  In this section, we illustrate one of the \nreasons for this popularity and \nshow you how to graphically manipulate\nexpressions, which can sometimes help when trying to understand gate\ndiagrams.",
    "1248": "We begin with one of DeMorgan's laws, which we can illustrate both \nalgebraically and graphically:\nC  =  B+A  =",
    "1249": "{file=part2/figs/demorgan-nand.eps,width=0.95in}",
    "1250": "Let's say that we have a function expressed in SOP form, such as\nZ=ABC+DE+FGHJ.  The diagram on the left below shows the function\nconstructed from three AND gates and an OR gate.  Using DeMorgan's\nlaw, we can replace the OR gate with a NAND with inverted inputs.\nBut the bubbles that correspond to inversion do not need to sit at the\ninput to the gate.  We can invert at any point along the wire,\nso we slide each bubble down the wire to the output of the first column\nof AND gates.  { Be careful: if the wire splits, which does not\nhappen in our example, you have to replicate\nthe inverter onto the other output paths as you slide past the split\npoint!}  The end result is shown on the right: we have not changed the \nfunction, but now we use only NAND gates.  Since CMOS technology\nonly supports NAND and NOR directly, using two-level logic makes\nit simple to map our expression into CMOS gates.",
    "1251": "{file=part2/figs/SOP-equiv.eps,width=6.5in}",
    "1252": "You may want to make use of DeMorgan's other law, illustrated graphically\nto the right, to perform the same transformation on a POS expression.  What\ndo you get?",
    "1253": "{file=part2/figs/demorgan-nor.eps,width=0.95in}",
    "1254": "As engineers, almost every real problem that you encounter will admit \nmultiple metrics for evaluating possible designs.  Becoming a good\nengineer thus requires not only that you be able to solve problems\ncreatively so as to improve the quality of your solutions, but also\nthat you are aware of how people might evaluate those solutions and\nare able both to identify the most important metrics and to balance \nyour design effectively according to them.  In this section, we\nintroduce some general ideas and methods that may be of use to you\nin this regard.",
    "1255": "{ We will not test you on the concepts in this section.}",
    "1256": "When you start thinking about a new problem, your first step\nshould be to think carefully about metrics of possible interest.",
    "1257": "Some important metrics may not be easy to quantify.",
    "1258": "For example, compatibility of a design with other products already \nowned by a customer has frequently defined the success or failure\nof computer hardware and software solutions.",
    "1259": "But how can you compute the compability of your approach as\na number?",
    "1260": "Humans---including engineers---are not good at\ncomparing multiple metrics simultaneously.",
    "1261": "Thus, once you have a set of metrics that you feel is complete, \nyour next step is to get rid of as many as you can.",
    "1262": "Towards this end, you may identify metrics that have no practical \nimpact in current technology, set threshold values for other metrics\nto simplify reasoning about them, eliminate redundant metrics,\ncalculate linear sums to reduce the count of metrics, and, finally,\nmake use of the notion of Pareto optimality.  All of these ideas are\ndescribed in the rest of this section.",
    "1263": "Let's start by considering metrics that we can quantify as real\nnumbers.",
    "1264": "For a given metric, we can divide possible measurement values into\nthree ranges.",
    "1265": "In the first range,\nall measurement values are equivalently useful.\nIn the second range, \npossible values are ordered and interesting with respect to\none another.\nValues in the third range are all impossible to use in practice.\nUsing power consumption as\nour example, the first range corresponds to systems in which\nwhen a processor's power consumption in a digital \nsystem is extremely low relative to the\npower consumption\nof the system.\nFor example, the processor in a computer might use less than 1 \nof the total used by \nthe system including the disk drive, the monitor, the power \nsupply, and so forth.  One power consumption value in this range \nis just as good as any\nanother, and no one cares about the power consumption of the processor \nin such cases.  In the second range, power consumption of the\nprocessor makes a difference.  Cell phones use most of their energy\nin radio operation, for example, but if you own a phone with a powerful\nprocessor, you may have noticed that you can turn off the phone and \ndrain the battery fairly quickly by playing a game.  Designing a\nprocessor that uses half as much power lengthens the battery life in\nsuch cases.  Finally,\nthe third region of power consumption measurements is impossible:\nif you use so much power, your chip will overheat or even burst\ninto flames.  Consumers get unhappy when such things happen.",
    "1266": "As a first step, you can remove any metrics for which all solutions\nare effectively equivalent.",
    "1267": "Until a little less than a decade ago, for example, the power \nconsumption of a desktop\nprocessor actually was in the first range that we discussed.  Power\nwas simply not a concern to engineers: all designs of \ninterest consumed so little power that no one cared.",
    "1268": "Unfortunately, at that point, power consumption jumped into the\nthird range rather quickly.  Processors hit a wall, and \nproducts had to be cancelled.  Given that the time spent designing\na processor has historically\nbeen about five years, a lot of engineering effort\nwas wasted because people had not thought carefully enough about\npower (since it had never mattered in the past).",
    "1269": "Today, power is an important metric that engineers must take into\naccount in their designs.",
    "1270": "However, in some areas, such as desktop and high-end server\nprocessors,\nother metrics (such as performance) may be so \nimportant that we always want to operate at the edge of the\ninteresting range.  In such cases, we might choose to treat \na metric such as power consumption as a { threshold}: stay\nbelow 150 Watts for a desktop processor, for example.  One still\nhas to make a coordinated effort to ensure that the system as\na whole does not exceed the threshold, but reasoning about \nthreshold values, a form of constraint, is easier than trying to\nthink about multiple metrics at once.",
    "1271": "Some metrics may only allow discrete quantification.  For example,\none could choose to define compatibility with previous processor\ngenerations as binary: either an existing piece of software\n(or operating system)\nruns out of the box on your new processor, or it does not.  If you \nwant people who own that software to make use of your new processor,\nyou must ensure that the value of this binary metric is 1, which\ncan also be viewed as a threshold.",
    "1272": "In some cases, two metrics may be strongly { correlated}, meaning\nthat a design that is good for one of the metrics is frequently \ngood for the other metric as well.",
    "1273": "Chip area and cost, for\nexample, are technically distinct ways to measure a digital design,\nbut we rarely consider them separately.",
    "1274": "A design that requires a larger chip is probably more complex,\nand thus takes more engineering time to get right (engineering time\ncosts money).",
    "1275": "Each silicon wafer costs money to fabricate, and fewer copies of a \nlarge design fit on one wafer, so large chips mean more fabrication\ncost.",
    "1276": "Physical defects in silicon can cause some chips not to work.  A large\nchip uses more silicon than a small one, and is thus more likely to suffer\nfrom defects (and not work).  Cost thus goes up again for large chips\nrelative to small ones.",
    "1277": "Finally, large chips usually require more careful testing to ensure\nthat they work properly (even ignoring the cost of getting the design\nright, we have to test for the presence of defects), which adds still\nmore cost for a larger chip.",
    "1278": "All of these factors tend to correlate chip area and chip cost, to the\npoint that most engineers do not consider both metrics.",
    "1279": "After you have tried to reduce your set of metrics as much as possible,\nor simplified them by turning them into thresholds, you should consider\nturning the last few metrics into a weighted linear sum.  All remaining\nmetrics must be quantifiable in this case.",
    "1280": "For example, if you are left with three metrics for which a given\ndesign has values A, B, and C, you might reduce these to one\nmetric by calculating D=w_AA+w_BB+w_CC.  What are the w values?\nThey are weights for the three metrics.  Their values represent the\nrelative importance of the three metrics to the overall evaluation.\nHere we've assumed that larger values of A, B, and C are either\nall good or all bad.  If you have metrics with different senses,\nuse the reciprocal values.  For example, if a large value of A is good,\na small value of 1/A is also good.",
    "1281": "The difficulty with linearizing metrics is that not everyone agrees\non the weights.  Is using less power more important than having a cheaper\nchip?  The answer may depend on many factors.",
    "1282": "When you are left with several metrics of interest, you can use the\nidea of Pareto optimality to identify interesting designs.",
    "1283": "Let's say that you have two metrics.  If a design D_1 is better than\na second design D_2 for both metrics, we say that D_1 \n{ dominates} D_2.",
    "1284": "A design D is then said to be { Pareto optimal} if no other design\ndominates D.  Consider the figure on the left below, which illustrates\nseven possible designs measured with two metrics.  The design corresponding\nto point B dominates the designs corresponding to points A and C, so \nneither of the latter designs is Pareto optimal.  No other point \nin the figure dominates B, however, so that design is Pareto optimal.\nIf we remove all points that do not represent Pareto optimal designs,\nand instead include only those designs that are Pareto optimal, we\nobtain the version shown on the right.  These are points in a two-dimensional\nspace, not a line, but we can imagine a line going through the points,\nas illustrated in the figure: the points that make up the line are\ncalled a { Pareto curve}, or, if you have more than two metrics,\na { Pareto surface}.",
    "1285": "{",
    "1286": "As an example of the use of Pareto optimality, consider the figure\nto the right, which is copied with permission from Neal Crago's Ph.D. \ndissertation (UIUC ECE, 2012).  The figure compares hundreds of thousands \nof possible\ndesigns based on a handful of different core approaches for\nimplementing a processor.  The axes in the graph are two metrics \nof interest.  The horizontal axis measures the average performance of a \ndesign when\nexecuting a set of benchmark applications, normalized to\na baseline processor design.  The vertical axis measures the energy\nconsumed by a design when",
    "1287": "{file=part2/cited/bench_pareto.eps,width=3in}",
    "1288": "executing the same benchmarks, normalized\nagain to the energy consumed by a baseline design.  The six sets of\npoints in the graph represent alternative design techniques for the\nprocessor, most of which are in commercial use today.  The points\nshown for each set are the subset of many thousands of possible variants\nthat are Pareto optimal.  In this case, more performance and less energy\nconsumption are the good directions, so any point in a set for which\nanother point is both further to the right and further down is not\nshown in the graph.  The black line represents an absolute power\nconsumption of 150 Watts, which is a nominal threshold for a desktop\nenvironment.  Designs above and to the right of that line are not\nas interesting for desktop use.  The { design-space exploration} that\nNeal reported in this figure was of course done by many computers using\nmany hours of computation, but he had to design the process by which the\ncomputers calculated each of the points.",
    "1289": "These notes introduce logic components for storing bits, building up\nfrom the idea of a pair of cross-coupled inverters through an\nimplementation of a flip-flop, the storage abstractions used in most \nmodern logic design processes.  We then introduce simple forms of\ntiming diagrams, which we use to illustrate the behavior of\na logic circuit.",
    "1290": "After commenting on the benefits of using a clocked synchronous \nabstraction when designing systems that store and manipulate bits,\nwe illustrate timing issues and explain how these are abstracted\naway in clocked synchronous designs.",
    "1291": "{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later\nclasses.}",
    "1292": "So far we have discussed only implementation of Boolean functions:\ngiven some bits as input, how can we design a logic circuit to calculate\nthe result of applying some function to those bits?  The answer to\nsuch questions is called { combinational logic} (sometimes \n{ combinatorial logic}), a name stemming from the fact that we\nare combining existing bits with logic, not storing new bits.",
    "1293": "You probably already know, however, that combinational logic alone is\nnot sufficient to build a computer.  We need the ability to store bits,\nand to change those bits.  Logic designs that make use of stored \nbits---bits that can be changed, not just wires to high voltage and \nground---are called { sequential logic}.  The name comes from the idea \nthat such a system moves through a sequence of stored bit patterns (the \ncurrent stored bit pattern is called the { state} of the system).",
    "1294": "Consider the diagram to the right.  What is it?  A {1-input} \nNAND gate, or an inverter drawn badly?  If you think carefully about\nhow these two gates are built, you will realize that they are the\nsame thing.  Conceptually, we use two inverters to store a bit, but\nin most cases we make use of NAND gates to simplify the \nmechanism for changing the stored bit.",
    "1295": "{file=part2/figs/latch-step-1.eps,width=1.9in}",
    "1296": "Take a look at the design to the right.  Here we have taken two \ninverters (drawn as NAND gates) and coupled each gate's output to\nthe other's input.  What does the circuit do?  Let's make some\nguesses and see where they take us.  Imagine that the value at Q\nis 0.  In that case, the lower gate drives P to 1.  \nBut P drives the upper gate, which forces Q to 0.\nIn other words, this combination forms a",
    "1297": "{file=part2/figs/latch-step-2.eps,width=1.65in}",
    "1298": "{|cc}\nQ& P \n0& 1\n1& 0",
    "1299": "stable state of the system:\nonce the gates reach this state, they continue to hold these values.\nThe first row of the truth table to the right (outputs only) shows\nthis state.",
    "1300": "What if Q=1, though?  In this case, the lower gate forces P to 0,\nand the upper gate in turn forces Q to 1.  Another stable state!\nThe Q=1 state appears as the second row of the truth table.",
    "1301": "We have identified all of the stable states.{Most logic\nfamilies also allow unstable states in which the values alternate rapidly \nbetween 0 and 1.  These metastable states are \nbeyond the scope of our class, but ensuring that they do not occur\nin practice is important for real designs.}  Notice that our cross-coupled\ninverters can store a bit.  Unfortunately, we have no way to \nspecify which value should be stored, nor to change the bit's value\nonce the gates have settled into a stable state.\nWhat can we do?",
    "1302": "Let's add an input to the upper gate, as shown to the \nright.  We call the input .  The ``S'' stands for set---as\nyou will see, our new input allows us to { set} our stored bit Q to 1.\nThe use of a complemented name for the input indicates that the input\nis { active low}.  In other words, the input performs its intended \ntask (setting Q to 1) when its value is 0 (not 1).",
    "1303": "{file=part2/figs/latch-step-3.eps,width=2.1in}",
    "1304": "{c|cc}\n& Q& P \n1& 0& 1\n1& 1& 0\n0& 1& 0",
    "1305": "Think about what happens when the new input is not active, \n=1.  As you know, ANDing any value with 1 produces the \nsame value, \nso our new input has no effect when =1.  The first two\nrows of the truth table are simply a copy of our previous table: the \ncircuit can store either bit value when =1.\nWhat happens when =0?  In that case, the upper gate's\noutput is forced to 1, and thus the lower gate's is forced to 0.\nThis third possibility is reflected in the last row of the truth table.",
    "1306": "Now we have the ability to force bit Q to have value 1, but if we\nwant Q=0, we just have to hope that the circuit happens to settle into\nthat state when we turn on the power.  What can we do?",
    "1307": "As you probably guessed, we add an input to the other gate, as \nshown to the right.  We call the new input : the input's\npurpose is to { reset} bit Q to 0, and the input\nis active low.  We extend the truth table to include a row \nwith =0 and =1, which forces Q=0 and P=1.",
    "1308": "{file=part2/figs/latch-step-4.eps,width=2.1in}",
    "1309": "{cc|cc}\n& & Q& P \n1& 1& 0& 1\n1& 1& 1& 0\n1& 0& 1& 0\n0& 1& 0& 1\n0& 0& 1& 1",
    "1310": "is active low.  We extend the truth table to include a row \n with =0 and =1, which forces Q=0 and P=1.",
    "1311": "The circuit that we have drawn has a name: an \n{ {- latch}}. \nOne can also build {R-S latches} (with active high set and reset\ninputs).  The textbook also shows an\n{- latch} (labeled \nincorrectly).\nCan you figure out how to build an {R-S} latch yourself?",
    "1312": "Let's think a little more about\nthe {- latch}. \nWhat happens if we set =0 and =0 at\nthe same time?  Nothing bad happens immediately.  Looking at the design,\nboth gates produce 1, so Q=1 and P=1.  The bad part happens later:\nif we raise both  and  back to 1 at around\nthe same time, the stored bit may end up in either state.{Or,\nworse, in a metastable state, as mentioned earlier.}",
    "1313": "We can avoid the problem by adding gates to prevent the\ntwo control inputs ( and )\nfrom ever being 1 at the same time.  A single inverter\nmight technically suffice, but let's build up the structure shown below,\nnoting that the two inverters in sequence connecting D to \nhave no practical effect at the moment.",
    "1314": "A truth table is shown to the right of the logic diagram.",
    "1315": "When D=0,  is forced to 0, and the bit is reset.",
    "1316": "Similarly, when D=1,  is forced to 0, and the bit is set.",
    "1317": "{",
    "1318": "{file=part2/figs/latch-step-5.eps,width=3.25in}",
    "1319": "{c|cccc}\nD& & & Q& P \n0& 0& 1& 0& 1\n1& 1& 0& 1& 0",
    "1320": "}",
    "1321": "Unfortunately, except for some interesting timing characteristics, \nthe new design has the same functionality as a piece of wire.\nAnd, if you ask a circuit designer, thin wires also have some \ninteresting timing characteristics.  What can we do?  Rather than \nhaving Q always reflect the current value of D, let's add\nsome extra inputs to the new NAND gates that allow us to control\nwhen the value of D is copied to Q, as shown on the next page.",
    "1322": "{",
    "1323": "{file=part2/figs/latch-step-6.eps,width=3.35in}",
    "1324": "{cc|cccc}\nWE& D& & & Q& P \n1& 0& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0\n0& 0& 1& 1& 0& 1\n0& 1& 1& 1& 0& 1\n0& 0& 1& 1& 1& 0\n0& 1& 1& 1& 1& 0",
    "1325": "}",
    "1326": "The WE (write enable) input controls whether or not Q mirrors\nthe value of D.  The first two rows in the truth table are replicated\nfrom our ``wire'' design: a value of WE=1 has no effect on the first\ntwo NAND gates, and Q=D.  A value of WE=0 forces the\nfirst two NAND gates to output 1, thus =1, =1,\nand the bit Q can\noccupy either of the two possible states, regardless of the value\nof D, as reflected in the lower four lines of the truth table.",
    "1327": "The circuit just shown is called a { gated D latch}, and is an\nimportant mechanism",
    "1328": "{file=part2/figs/latch-step-7.eps,width=1.1in}",
    "1329": "for storing state in sequential \nlogic.  (Random-access memory uses a slightly different \ntechnique to connect the cross-coupled inverters, but latches are\nused for nearly every other application of stored state.)\nThe ``D''\nstands for ``data,'' meaning that the bit stored is matches\nthe value of the input.  Other types of latches (including\n{S-R latches}) have been used historically, but\nD latches are used predominantly today, so we omit discussion of\nother types.\nThe ``gated'' qualifier refers to the presence of an enable input\n(we called it WE) to control\nwhen the latch copies its input into the stored bit.\nA symbol for a gated D latch appears to the right.  Note that we have\ndropped the name P in favor of , since P=\nin a gated D latch.",
    "1330": "High-speed logic designs often use latches directly.  Engineers\nspecify the number of latches as well as combinational logic \nfunctions needed to connect one latch to the next,\nand the CAD tools optimize the combinational logic.\nThe enable inputs of successive groups of latches are then driven\nby what we call a clock signal, a single bit line distributed across\nmost of the chip that alternates between 0 and 1 with a regular\nperiod.  While the clock is 0, one set of latches holds its bit values fixed,\nand combinational logic uses those latches as inputs to produce \nbits that are copied into a\nsecond set of latches.  When the clock switches to 1, the second\nset of latches stops storing their data inputs and\nretains their bit values in order to drive other combinational logic,\nthe results of which are copied into a third set of latches.\nOf course, some of the latches in the first and third sets may be the\nsame.",
    "1331": "The timing of signals in such designs plays a critical role in their\ncorrect operation.  Fortunately, we have developed powerful abstractions \nthat allow engineers to ignore much of the complexity while thinking\nabout the Boolean logic needed for a given design.",
    "1332": "Towards that end, we make a simplifying assumption for the\nrest of our class, and for most of your career as an undergraduate:\nthe clock signal is a { square wave} delivered uniformly across a\nchip.  For example, if the period of a clock is 0.5 nanoseconds (2 GHz),\nthe clock signal is a 1 for 0.25 nanoseconds, then a 0 for\n0.25 nanoseconds.  We assume that the clock signal changes instantaneously\nand at the same time across the chip.  Such a signal can never exist in\nthe real world: voltages do not change instantaneously, and the \nphrase ``at the same time'' may not even make sense at these scales.\nHowever, circuit designers can usually provide a clock signal that\nis close enough, allowing us to forget for now that no physical signal can\nmeet our abstract definition.",
    "1333": "SSL altered terminology on 3 Dec 21",
    "1334": "The device shown to the right is a { master-slave} implementation of a",
    "1335": "The device shown to the right is a { dual-latch} implementation of a \n{ positive edge-triggered} D flip-flop.  As you can see, we have \nconstructed it from two gated D latches with opposite senses of write\nenable.  The ``D'' part of the name has the same meaning as with a\ngated D latch: the bit stored is the same as the one delivered",
    "1336": "{file=part2/figs/latch-step-8.eps,width=2.75in}",
    "1337": "{file=part2/figs/latch-step-9.eps,width=0.95in}",
    "1338": "to the\ninput.  Other variants of flip-flops have also been built, but this \ntype dominates designs today.  Most are actually generated automatically\nfrom hardware ``design'' languages (that is, computer programming languages\nfor hardware design).",
    "1339": "When the clock is low (0), the first latch copies its value from\nthe flip-flop's D input to the midpoint (marked X in our figure, but\nnot usually given a name).  When the clock is high (1), the second\nlatch copies its value from X to the flip-flop's output Q.\nSince X can not change when the clock is high,\nthe result is that the output changes each time the clock changes\nfrom 0 to 1, which is called the { rising edge} or { positive edge}\n(the derivative) of the clock signal.  Hence the qualifier \n``positive edge-triggered,'' which describes the flip-flop's behavior.",
    "1340": "The ``master-slave'' implementation refers to the use of two latches.",
    "1341": "The ``dual-latch'' implementation refers to the use of two \nlatches.{Historically, this implementation was called ``master-slave,''\nbut ECE Illinois has decided to eliminate use of such terminology.}\nlatches.\nIn practice, flip-flops are almost never built this way.  To see a \ncommercial design, look up 74LS74, which uses six {3-input} NAND\ngates and allows set/reset of the flip-flop (using two\nextra inputs).",
    "1342": "The { timing diagram} to the right illustrates the operation of\nour flip-flop.  In a timing diagram, the horizontal axis represents\n(continuous) increasing time, and the individual lines represent\nvoltages for logic signals.  The relatively simple version shown here\nuses only binary values for each signal.  One can also draw \ntransitions more realistically (as taking finite time).  The dashed\nvertical lines here represent the times at which the clock rises.\nTo make the",
    "1343": "example interesting, we have varied D over two clock\ncycles.  Notice that even though D rises and falls during the second\nclock cycle, its value is not copied to the output of our flip-flop.\nOne can build flip-flops that ``catch'' this kind of behavior\n(and change to output 1), but we leave such designs for later in your\ncareer.",
    "1344": "Circuits such as latches and flip-flops are called { sequential\nfeedback} circuits, and the process by which they are designed \nis beyond the scope of our course.  The ``feedback'' part of the\nname refers to the fact that the outputs of some gates are fed back \ninto the inputs of others.  Each cycle in a sequential feedback \ncircuit can store one bit.",
    "1345": "Circuits that merely use\nlatches and flip-flops as building blocks are called\n{ clocked synchronous sequential circuits}.  Such designs are\nstill sequential: their behavior depends on the bits currently\nstored in the latches and flip-flops.  However, their behavior\nis substantially simplified by the use of a clock signal (the ``clocked''\npart of the name) in a way that all elements change at the same\ntime (``synchronously'').",
    "1346": "The value of using flip-flops and assuming a square-wave\nclock signal with uniform timing may not be clear to you yet,\nbut it bears emphasis.",
    "1347": "With such assumptions, { we can treat time as having \ndiscrete values.}  In other words, time ``ticks'' along discretely,\nlike integers instead of real numbers.",
    "1348": "We can look at the state of the system, calculate the inputs to our\nflip-flops through the combinational logic that drives their D\ninputs, and be confident that, when time moves to the next discrete\nvalue, we will know the new bit values stored in our flip-flops,\nallowing us to repeat the process for the next clock cycle without\nworrying about exactly when things change.  Values change only\non the rising edge of the clock!",
    "1349": "Real systems, of course, are not so simple, and we do not have one\nclock to drive the universe, so engineers must also design systems\nthat interact even though each has its own private clock signal\n(usually with different periods).",
    "1350": "Before we forget about the fact that real designs do not provide\nperfect clocks, let's explore some of the issues that engineers\nmust sometimes face.",
    "1351": "We discuss these primarily to ensure that you appreciate the power\nof the abstraction that we use in the rest of our course.\nIn later classes (probably our 298, which will absorb material \nfrom 385), you may be required to master this material.\n{ For now, we provide it simply for your interest.}",
    "1352": "Consider the circuit shown below, for which the output is given by \nthe equation S=AB+.",
    "1353": "{{file=part2/figs/lec15-1.eps,width=4in}}",
    "1354": "The timing diagram on the right shows a { glitch} in the output\nwhen the input shifts from ABC=110 to 100, that is, when B falls.\nThe problem lies in the possibility that the upper AND gate, driven \nby B, might go low before the lower AND gate, driven by , goes\nhigh.  In such a case, the OR gate output S falls until the second\nAND gate rises, and the output exhibits a glitch.",
    "1355": "A circuit that might exhibit a glitch in an output that functionally\nremains stable at 1 is said to have a { static-1 hazard}.  The\nqualifier ``static'' here refers to the fact that we expect the output\nto remain static, while the ``1'' refers to the expected value of the\noutput.",
    "1356": "The presence of hazards in circuits can be problematic in certain\ncases.  In domino logic, for example, an output is precharged and kept\nat 1 until the output of a driving circuit pulls it to 0, at which\npoint it stays low (like a domino that has been knocked over).  If the\ndriving circuit contains {static-1} hazards, the output may fall\nin response to a glitch.",
    "1357": "Similarly, hazards can lead to unreliable behavior in sequential\nfeedback circuits.  Consider the addition of a feedback loop to the\ncircuit just discussed, as shown in the figure below.  The output of\nthe circuit is now given by the equation S^*=AB+S,\nwhere S^* denotes the state after S feeds back through the lower\nAND gate.  In the case discussed previously, the transition from\nABC=110 to 100, the glitch in S can break the feedback, leaving\nS low or unstable.  The resulting sequential feedback circuit is\nthus unreliable.",
    "1358": "{{file=part2/figs/lec15-2.eps,width=4in}}",
    "1359": "Eliminating static hazards from {two-level} circuits\nis fairly straightforward.  The Karnaugh map to the right corresponds\nto our original circuit; the solid lines indicate the \nimplicants selected by the AND gates.  A {static-1} hazard is\npresent when two adjacent 1s in the {K-map} are not covered by\na common implicant.  {Static-0} hazards do not occur in\n{two-level} SOP circuits.",
    "1360": "{file=part2/figs/lec15-3.eps,width=1in}",
    "1361": "Eliminating static hazards requires merely extending the circuit with\nconsensus terms in order to ensure that some AND gate remains high\nthrough every transition between input states with\noutput 1.{Hazard elimination is not in general simple; we\nhave considered only {two-level} circuits.}  In the\n{K-map} shown, the dashed line indicates the necessary\nconsensus term, A.",
    "1362": "Consider an input transition for which we expect to see a change in an\noutput.  Under certain timing conditions, the output may not\ntransition smoothly, but instead bounce between its original value\nand its new value before coming to rest at the new value.  A circuit\nthat might exhibit such behavior is said to contain a { dynamic hazard}.\nThe qualifier ``dynamic'' refers to the expected change in the output.",
    "1363": "Dynamic hazards appear only in more complex circuits, such as the one\nshown below.  The output of this circuit is defined by the equation Q=B+++BD.",
    "1364": "{{file=part2/figs/lec15-4.eps,width=3in}}",
    "1365": "Consider the transition from the input state ABCD=1111 to 1011,\nin which B falls from 1 to 0.  For simplicity, assume that each\ngate has a delay of 1 time unit.  If B goes low at time T=0, the\ntable shows the progression over time of logic levels at several\nintermediate points in the circuit and at the output Q.  Each gate\nmerely produces the appropriate output based on its inputs in the\nprevious time step.  After one delay, the three gates with B as a\ndirect input change their outputs (to stable, final values).  After\nanother delay, at T=2, the other three gates\nre-{-8pt}",
    "1366": "{|c|c|c|c|c|c|c|}\nT&f&g&h&i&j&Q \n0& 0& 0& 0& 1& 1& 1\n1& 1& 1& 1& 1& 1& 1\n2& 1& 1& 1& 0& 0& 0\n3& 1& 1& 1& 0& 1& 1\n4& 1& 1& 1& 0& 1& 0",
    "1367": "spond to the initial changes and flip their outputs.  The resulting\nchanges induce another set of changes at T=3, which in turn causes\nthe output Q to change a final time at T=4.",
    "1368": "The output column in the table illustrates the possible impact of a\ndynamic hazard: rather than a smooth transition from 1 to 0, the\noutput drops to 0, rises back to 1, and finally falls to 0\nagain.  The dynamic hazard in this case can be attributed to the\npresence of a static hazard in the logic that produces intermediate\nvalue j.",
    "1369": "{ Essential hazards} are inherent to the function of a circuit and\nmay appear in any implementation.  In sequential feedback circuit\ndesign, they must be addressed at a low level to ensure that\nvariations in logic path lengths ({ timing skew}) through a circuit\ndo not expose them.  With clocked synchronous circuits, essential\nhazards are abstracted into a single form: { clock skew}, or\ndisparate clock edge arrival times at a circuit's flip-flops.",
    "1370": "An example demonstrates the possible effects: consider the\nconstruction of a clocked synchronous circuit to recognize {0-1}\nsequences on an input IN.  Output Q should be held high for one\ncycle after recognition, that is, until the next rising clock edge.  A\ndescription of states and a state diagram for such a circuit appear\nbelow.",
    "1371": "{",
    "1372": "S_1S_0 &state& meaning\n00& A& nothing, 1, or 11 seen last\n01& B& 0 seen last\n10& C& 01 recognized (output high)\n11& unused&",
    "1373": "}",
    "1374": "{{file=part2/figs/lec15-5.eps,width=2in}}",
    "1375": "For three states, we need two (=_2 3) flip-flops.\nDenote the internal state S_1S_0.  The specific internal\nstate values for each logical state (A, B, and C) simplify the\nimplementation and the example.  A { state table} \nand {K-maps} for the next-state logic appear\nbelow.  The state table uses one line per state with separate \ncolumns\nfor each input combination, making the table more compact than one\nwith one line per state/input combination.  Each column contains the\nfull next-state information, including output.  Using this form of the\nstate table, the {K-maps} can be read directly from the table.",
    "1376": "{",
    "1377": "& {IN}\nS_1S_0& 0& 1 \n00& 01/0& 00/0\n01& 01/0& 10/0\n11& x& x\n10& 01/1& 00/1",
    "1378": "}",
    "1379": "{{file=part2/figs/lec15-6.eps,width=3.5in}}",
    "1380": "Examining the {K-maps}, we see that the excitation and output\nequations are S_1^+=IN S_0, S_0^+=, and Q=S_1.\nAn implementation of the circuit using two D flip-flops appears below.\nImagine that mistakes in routing or process variations have made the\nclock signal's path to flip-flop 1 much longer than its path into\nflip-flop 0, as illustrated.",
    "1381": "{{file=part2/figs/lec15-7.eps,width=3in}}",
    "1382": "Due to the long delays, we cannot assume that rising clock edges\narrive at the flip-flops at the same time.  The result is called clock\nskew, and can make the circuit behave improperly by exposing essential\nhazards.  In the logical B to C transition, for example, we begin in\nstate S_1S_0=01 with IN=1 and the clock edge rising.  Assume that\nthe edge reaches flip-flop 0 at time T=0.  After a flip-flop delay\n(T=1), S_0 goes low.  After another AND gate delay (T=2), input\nD_1 goes low, but the second flip-flop has yet to change state!\nFinally, at some later time, the clock edge reaches flip-flop 1.\nHowever, the output S_1 remains at 0, leaving the system in state A\nrather than state C.",
    "1383": "Fortunately, in clocked synchronous sequential circuits, all essential\nhazards are related to clock skew.  This fact implies that we can\neliminate a significant amount of complexity from circuit design by\ndoing a good job of distributing the clock signal.  It also implies\nthat, as a designer, you should avoid specious addition of logic in a\nclock path, as you may regret such a decision later, as you try to\ndebug the circuit timing.",
    "1384": "This section outlines a proof of the claim made regarding clock\nskew being the only source of essential hazards for clocked\nsynchronous sequential circuits.  A { proof outline} suggests\nthe form that a proof might take and provides some of the logical\narguments, but is not rigorous enough to be considered a proof.\nHere we use a D flip-flop to illustrate a method for identifying\nessential hazards ({ the D flip-flop has no essential hazards, however}),\nthen argue that the method can be applied generally to collections\nof flip-flops in a clocked synchronous design to show that essential\nhazards occur only in the form of clock skew.",
    "1385": "{",
    "1386": "&{1-2}\nlow&L&clock low, last input low\nhigh&H&clock high, last input low\npulse low&PL&clock low, last input high (output high, too)\npulse high&PH&clock high, last input high (output high, too)",
    "1387": "}",
    "1388": "{{file=part2/figs/lec15-8.eps,width=2in}}",
    "1389": "Consider the sequential feedback state table for a positive\nedge-triggered D flip-flop, shown above.  In designing and analyzing\nsuch circuits, we assume that only one input bit changes at a time.\nThe state table consists of one row for each state and one column for\neach input combination.  Within a row, input combinations that have no\neffect on the internal state of the circuit (that is, those that do \nnot cause any change in\nthe state) are said to be stable; these states are circled.  Other\nstates are unstable, and the circuit changes state in response to\nchanges in the inputs.",
    "1390": "For example, given an initial state L with low output, low clock,\nand high input D, the solid arcs trace the reaction of the circuit to a\nrising clock edge.  From the 01 input combination, we move along the\ncolumn to the 11 column, which indicates the new state, PH.  Moving\ndown the column to that state's row, we see that the new state is\nstable for the input combination 11, and we stop.  If PH were not\nstable, we would continue to move within the column until coming to\nrest on a stable state.",
    "1391": "An essential hazard appears in such a table as a difference between\nthe final state when flipping a bit once and the final state when\nflipping a bit thrice in succession.  The dashed arcs in the figure\nillustrate the concept: after coming to rest in the PH state, we reset\nthe input to 01 and move along the PH row to find a new state of PL.\nMoving up the column, we see that the state is stable.  We then flip\nthe clock a third time and move back along the row to 11, which\nindicates that PH is again the next state.  Moving down the column, we\ncome again to rest in PH, the same state as was reached after one\nflip.  Flipping a bit three times rather than once evaluates the\nimpact of timing skew in the circuit; if a different state is reached\nafter two more flips, timing skew could cause unreliable behavior.  As\nyou can verify from the table, a D flip-flop has no essential hazards.",
    "1392": "A group of flip-flops, as might appear in a clocked synchronous\ncircuit, can and usually does have essential hazards, but only dealing\nwith the clock.  As you know, the inputs to a clocked synchronous\nsequential circuit consist of a clock signal and other inputs (either\nexternal of fed back from the flip-flops).  Changing an input other\nthan the clock can change the internal state of a flip-flop (of the",
    "1393": "master-slave variety), but flip-flop designs do not capture the number",
    "1394": "dual-latch variety), but flip-flop designs do not capture the number\nof input changes in a clock cycle beyond one, and changing an input\nthree times is the same as changing it once.  Changing the clock, of\ncourse, results in a synchronous state machine transition.",
    "1395": "The detection of essential hazards in a clocked synchronous design\nbased on flip-flops thus reduces to examination of the state machine.\nIf the next state of the machine has any dependence on the current\nstate, an essential hazard exists, as a second rising clock edge moves\nthe system into a second new state.  For a single D flip-flop, the\nnext state is independent of the current state, and no essential\nhazards are present.",
    "1396": "In this set of notes, we illustrate the use of abstraction to simplify\nproblems, then introduce a component called a multiplexer that allows\nselection among multiple inputs.",
    "1397": "We begin by showing how two specific \nexamples---integer subtraction and identification of letters\nin ASCII---can be implemented using logic functions that we have already\ndeveloped.  We also introduce a conceptual technique for\nbreaking functions into smaller pieces, which allows us to solve\nseveral simpler problems and then to compose a full solution from \nthese partial solutions.",
    "1398": "Together with the idea of bit-sliced designs that we introduced earlier,\nthese techniques help to simplify the process of designing logic that\noperates correctly.  The techniques can, of course, lead to \nless efficient designs,\nbut { correctness is always more important than performance}.",
    "1399": "The potential loss of efficiency is often acceptable for three reasons.",
    "1400": "First, as we mentioned earlier, computer-aided design tools for \noptimizing logic functions are fairly effective, and in many cases\nproduce better results than human engineers (except in the rare cases \nin which the human effort required to beat the tools is worthwhile).",
    "1401": "Second, as you know from the design of the {2's complement} \nrepresentation, we may be able to reuse specific pieces of hardware if\nwe think carefully about how we define our problems and representations.",
    "1402": "Finally, many tasks today are executed in software, which is designed\nto leverage the fairly general logic available via an instruction set\narchitecture.  A programmer cannot easily add new logic to a user's\nprocessor.  As a result, the hardware used to execute a\nfunction typically is not optimized for that function.",
    "1403": "The approaches shown in this set of notes illustrate how abstraction\ncan be used to design logic.",
    "1404": "Our discussion of arithmetic implementation has focused so far on \naddition.  What about other operations, such as subtraction, multiplication,\nand division?  The latter two require more work, and we will not\ndiscuss them in detail until later in our class (if at all).",
    "1405": "Subtraction, however, can be performed almost trivially using logic that\nwe have already designed.",
    "1406": "Let's say that we want to calculate the difference D between \ntwo {N-bit} numbers A and B.  In particular, we want \nto find D=A-B.  For now, think of A, B, and D \nas 2's complement values.",
    "1407": "Recall how we defined the 2's complement representation: \nthe {N-bit} pattern that we use to represent -B is the \nsame as the base 2 bit pattern for (2^N-B), so we can use an adder if we\nfirst calculate the bit pattern for -B, then add the resulting\npattern to A.\nAs you know, our {N-bit} adder always produces a result that\nis correct modulo 2^N, so the result of such an operation,\nD=2^N+A-B, is correct so long as the subtraction does not overflow.",
    "1408": "How can we calculate 2^N-B?  The same way that we do by hand!\nCalculate the 1's complement, (2^N-1)-B, then add 1.",
    "1409": "The diagram to the right shows how we can use the {N-bit} adder\nthat we designed in Notes Set 2.3 to build an {N-bit} subtracter.",
    "1410": "New elements appear in blue in the figure---the rest of the logic\nis just an adder.  The box labeled ``1's comp.'' calculates \nthe {1's complement} of the value B, which together with the\ncarry in value of 1 correspond to calculating -B.  What's in the\n``1's comp.'' box?  One inverter per bit in B.  That's all \nwe need to calculate the 1's complement.",
    "1411": "You might now ask: does this approach also work for unsigned numbers?\nThe answer is yes, absolutely.  However, the overflow conditions for\nboth 2's complement and unsigned subtraction are different than the\noverflow condition for either type of addition.  What does the carry\nout of our adder signify, for example?  The answer may not be \nimmediately obvious.",
    "1412": "Let's start with the overflow condition for unsigned subtraction.",
    "1413": "Overflow means that we cannot represent the result.  With an {N-bit}\nunsigned number, we have A-B[0,2^N-1].  Obviously, the difference\ncannot be larger than the upper limit, since A is representable and\nwe are subtracting a non-negative (unsigned) value.  We can thus assume\nthat overflow occurs only when A-B<0.  In other words, when A<B.",
    "1414": "To calculate the unsigned subtraction overflow condition in terms of the\nbits, recall that our adder is calculating 2^N+A-B.  The carry out\nrepresents the 2^N term.  When A, the result of the adder\nis at least 2^N, and we see a carry out, C_=1.  However, when\nA<B, the result of the adder is less than 2^N, and we see no carry\nout, C_=0.  { Overflow for unsigned subtraction is thus inverted \nfrom overflow for unsigned addition}: a carry out of 0 indicates an \noverflow for subtraction.",
    "1415": "What about overflow for 2's complement subtraction?  We can use arguments\nsimilar to those that we used to reason about overflow of 2's complement\naddition to prove that subtraction of one negative number from a second\nnegative number can never overflow.  Nor can subtraction of a non-negative\nnumber from a second non-negative number overflow.",
    "1416": "If A and B<0, the subtraction overflows iff A-B{2^{N-1}}.\nAgain using similar arguments as before, we can prove that the difference D\nappears to be negative in the case of overflow, so the product\n{A_{N-1}} B_{N-1} D_{N-1} evaluates to 1 when this type of\noverflow occurs (these variables represent the most significant bits of the \ntwo operands and the difference; in the case of 2's complement, they are\nalso the sign bits).",
    "1417": "Similarly, if A<0 and B, we have overflow when A-B<-2^{N-1}.\nHere we can prove that D on overflow, so \nA_{N-1} {B_{N-1}} {D_{N-1}} evaluates to 1.",
    "1418": "Our overflow condition for {N-bit} 2's complement subtraction\nis thus given by the following:",
    "1419": "{eqnarray*}\n{A_{N-1}} B_{N-1} D_{N-1}+A_{N-1} {B_{N-1}} {D_{N-1}}\n{eqnarray*}",
    "1420": "If we calculate all four overflow conditions---unsigned and 2's complement,\naddition and subtraction---and provide some way to choose whether or not to \ncomplement B and to control the C_ input, we can use the same hardware\nfor addition and subtraction of either type.",
    "1421": "Let's now consider how we can check whether or not an ASCII character is\nan upper-case letter.  Let's call the {7-bit}\nletter C=C_6C_5C_4C_3C_2C_1C_0 and the function that we want to\ncalculate U(C).  The function U should equal 1 whenever C represents\nan upper-case letter, and should equal 0 whenever C does not.",
    "1422": "In ASCII, the {7-bit} patterns from 0x41 through 0x5A correspond\nto the letters A through Z in alphabetic order.  Perhaps you want to draw \na {7-input} {K-map}?  Get a few large sheets of paper!",
    "1423": "Instead, imagine that we've written the full {128-row} truth\ntable.  Let's break the truth table into pieces.  Each piece will\ncorrespond to one specific pattern of the three high bits C_6C_5C_4,\nand each piece will have 16 entries for the four low bits C_3C_2C_1C_0.\nThe truth tables for high bits 000, 001, 010, 011, 110, and 111\nare easy: the function is exactly 0.  The other two truth tables appear \non the left below.  We've called the two functions T_4 and T_5, where\nthe subscripts correspond to the binary value of the three high bits of C.",
    "1424": "{",
    "1425": "{cccc|cc}\nC_3& C_2& C_1& C_0& T_4& T_5 \n0& 0& 0& 0& 0& 1\n0& 0& 0& 1& 1& 1\n0& 0& 1& 0& 1& 1\n0& 0& 1& 1& 1& 1 \n0& 1& 0& 0& 1& 1\n0& 1& 0& 1& 1& 1\n0& 1& 1& 0& 1& 1\n0& 1& 1& 1& 1& 1 \n1& 0& 0& 0& 1& 1\n1& 0& 0& 1& 1& 1\n1& 0& 1& 0& 1& 1\n1& 0& 1& 1& 1& 0 \n1& 1& 0& 0& 1& 0\n1& 1& 0& 1& 1& 0\n1& 1& 1& 0& 1& 0\n1& 1& 1& 1& 1& 0",
    "1426": "{file=part2/figs/ascii-t4.eps,width=1.15in}\n{file=part2/figs/ascii-t5.eps,width=1.15in}",
    "1427": "{eqnarray*}\nT_4&=&C_3+C_2+C_1+C_0      \nT_5&=&{C_3}+{C_2} {C_1}+{C_2} {C_0}\n{eqnarray*}",
    "1428": "As shown to the right of the truth tables, we can then draw simpler\n{K-maps} for T_4 and T_5, and can solve the {K-maps}\nto find equations for each, as shown to the right (check that you get\nthe same answers).",
    "1429": "How do we merge these results to form our final expression for U?",
    "1430": "We AND each of the term functions (T_4 and T_5) with the \nappropriate minterm for the\nhigh bits of C, then OR the results together, as shown here:",
    "1431": "{eqnarray*}\nU&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5",
    "1432": "&=&C_6 {C_5} {C_4} (C_3+C_2+C_1+C_0)+\nC_6 {C_5} C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0})\n{eqnarray*}",
    "1433": "Rather than trying to optimize by hand, we can at this point let the CAD\ntools take over, confident that we have the right function to identify\nan upper-case ASCII letter.",
    "1434": "Breaking the truth table into pieces and using simple logic to reconnect\nthe pieces is one way to make use of abstraction when solving complex\nlogic problems.",
    "1435": "In fact, recruiters for some companies often ask\nquestions that involve using specific logic elements as building blocks\nto implement other functions.  Knowing that you can implement\na truth table one piece at a time will help you to solve this type of\nproblem.",
    "1436": "Let's think about other ways to tackle the problem of calculating U.\nIn Notes Sets 2.3 and 2.4, we developed adders and comparators.  Can\nwe make use of these components as building blocks to check whether C \nrepresents an\nupper-case letter?  Yes, of course we can: by comparing C with the\nends of the range of upper-case letters, we can check whether or not C\nfalls in that range.",
    "1437": "The idea is illustrated on the left below using two {7-bit} comparators\nconstructed as discussed in Notes Set 2.4.\nThe comparators are the black parts of the drawing, while the blue parts\nrepresent our extensions to calculate U.  Each comparator is given\nthe value C as one input.  The second value to the comparators is \neither the letter A (0x41) or the letter Z (0x5A).  The meaning of \nthe {2-bit} input to and result of each comparator is given in the \ntable on the right below.  The inputs on the right of each comparator\nare set to 0 to ensure that equality is produced if C matches\nthe second input (B).",
    "1438": "One output from each comparator is then routed to\na NOR gate to calculate U.  Let's consider how this combination works.\nThe left comparator compares C with the letter A (0x41).  If C 0x41,\nthe comparator produces Z_0=0.  In this case, we may have a letter.\nOn the other hand, if C< 0x41, the comparator produces Z_0=1, and\nthe NOR gate outputs U=0, since we do not have a letter in this case.",
    "1439": "The right comparator compares C with the letter Z (0x5A).  If C 0x5A,\nthe comparator produces Z_1=0.  In this case, we may have a letter.\nOn the other hand, if C> 0x5A, the comparator produces Z_1=1, and\nthe NOR gate outputs U=0, since we do not have a letter in this case.",
    "1440": "Only when 0x41  0x5A does U=1, as desired.",
    "1441": "{",
    "1442": "{file=part2/figs/ascii-cmp-based.eps,width=3.6in}",
    "1443": "{cc|l}\nZ_1& Z_0& meaning \n0& 0& A=B\n0& 1& A<B\n1& 0& A>B\n1& 1& not used",
    "1444": "What if we have only {8-bit} adders available for our use, such as\nthose developed in Notes Set 2.3?  Can we still calculate U?  Yes.\nThe diagram shown to the right illustrates the approach, again with black\nfor the adders and blue for our extensions.  Here we are \nactually using the adders as subtracters, but calculating the 1's complements\nof the constant values by hand.  The ``zero extend'' box simply adds\na leading 0 to our {7-bit} ASCII letter.  The left adder \nsubtracts the letter A from C: if no carry is produced, we know that\nC< 0x41 and thus C does not represent an upper-case letter, and U=0.",
    "1445": "Similarly, the right adder subtracts 0x5B (the letter Z plus one)\nfrom C.  If a carry is produced, we know that C 0x5B, and \nthus C does not represent an upper-case letter, and U=0.",
    "1446": "With the right combination of carries (1 from the left and 0 from the\nright), we obtain U=1.",
    "1447": "{file=part2/figs/ascii-add-based.eps,width=2.75in}",
    "1448": "Looking carefully at this solution, however, you might be struck by the\nfact that we are calculating two sums and then discarding them.  Surely\nsuch an approach is inefficient?",
    "1449": "We offer two answers.  First, given the design shown above, a good CAD tool\nrecognizes that the sum outputs of the adders are not being used,\nand does not generate logic to calculate them.  The logic for the two \ncarry bits used to calculate U can then be optimized.  Second, the design\nshown, including the calculation of the sums, is similar in efficiency\nto what happens at the rate of about 10^ times per second, \n24 hours a day, seven days a week,\ninside processors in data centers processing HTML, XML, and other types\nof human-readable Internet traffic.  Abstraction is a powerful tool.",
    "1450": "Later in our class, you will learn how to control logical connections\nbetween hardware blocks so that you can make use of the same hardware\nfor adding, subtracting, checking for upper-case letters, and so forth.",
    "1451": "Having developed several approaches for checking for an upper-case letter,\nthe task of checking for a lower-case letter should be straightforward.\nIn ASCII, lower-case letters are represented by the {7-bit} patterns \nfrom 0x61 through 0x7A.",
    "1452": "One can now easily see how more abstract designs make solving similar\ntasks easier.  If we have designed our upper-case checker \nwith {7-variable} K-maps,\nwe must start again with new K-maps for the lower-case checker.  If\ninstead we have taken the approach of designing logic for the upper and \nlower bits of the ASCII character, we can reuse most of that logic,\nsince the functions T_4 and T_5 are identical when checking for\na lower-case character.  Recalling the algebraic form of U(C), we can\nthen write a function L(C) (a lower-case checker) as shown on the\nleft below.",
    "1453": "{eqnarray*}\nU&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5",
    "1454": "L&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5",
    "1455": "&=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+\n&&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0})\n{eqnarray*}",
    "1456": "{file=part2/figs/ascii-cmp-lower.eps,width=3.6in}",
    "1457": "Finally, if we have used a design based on comparators or adders, the\ndesign of a lower-case checker becomes trivial: simply change the numbers\nthat we input to these components, as shown in the figure on the right above\nfor the comparator-based design.  The only changes from the upper-case checker\ndesign are the inputs to the comparators and the output produced, \nhighlighted with blue text in the figure.",
    "1458": "Using the more abstract designs for checking ranges of ASCII characters,\nwe can go a step further and create a checker for both upper- and lower-case\nletters.  To do so, we add another input S that allows us to select the \nfunction that we want---either the upper-case checker U(C) or the \nlower-case checker L(C).",
    "1459": "For this purpose, we make use of a logic block called a { multiplexer},\nor { mux}.\nMultiplexers are an important abstraction for digital logic.  In \ngeneral, a multiplexer allows us to use one digital signal to \nselect which of several others is forwarded to an output.",
    "1460": "The simplest form of the multiplexer is the 2-to-1 multiplexer shown to \nthe right.   The logic diagram illustrates how the mux works.  The block \nhas two inputs from the left and one from the top.  The top input allows \nus to choose which of the left inputs is forwarded to the output.  \nWhen the input S=0, the upper AND gate outputs 0, and the lower AND gate\noutputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0.\nSimilarly, when input S=1, the upper AND gate outputs D_1, and the\nlower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1.",
    "1461": "The symbolic form of the mux is a trapezoid with data inputs on the \nlarger side, an output on the smaller side, and a select input on the\nangled part of the trapezoid.  The labels inside the trapezoid indicate \nthe value of the select input S for which the adjacent data signal, \nD_1 or D_0, is copied to the output Q.",
    "1462": "We can generalize multiplexers in two ways.  First, we can extend the \nsingle select input to a group of select inputs.  An {N-bit}\nselect input allows selection from amongst 2^N inputs.  A {4-to-1} \nmultiplexer is shown below, for example.  The logic diagram on the left\nshows how the {4-to-1 mux} operates.  For any combination of S_1S_0,\nthree of the AND gates produce 0, and the fourth outputs the D input\ncorresponding to the interpretation of S as an unsigned number.\nGiven three zeroes and one D input, the OR gate thus reproduces one of \nthe D's.  When S_1S_0=10, for example, the third AND gate copies D_2,\nand Q=D_2.",
    "1463": "{{file=part2/figs/mux4-to-1.eps,width=5.60in}}",
    "1464": "As shown in the middle figure, a {4-to-1} mux can also be built from\nthree {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux \nappears on the right in the figure.",
    "1465": "The second way in which we can generalize multiplexers is by using\nseveral multiplexers of the same type and using the same signals for \nselection.  For example, we might use a single select bit T to choose \nbetween any number of paired inputs.  Denote input pair by i D_1^i \nand D_0^i.  For each pair, we have an output Q_i.",
    "1466": "When T=0, Q_i=D_0^i for each value of i.",
    "1467": "And, when T=1, Q_i=D_1^i for each value of i.",
    "1468": "Each value of i requires a {2-to-1} mux with its select input\ndriven by the global select signal T.",
    "1469": "Returning to the example of the upper- and lower-case checker, we\ncan make use of two groups of seven {2-to-1} muxes, all controlled by\na single bit select signal S, to choose between the inputs needed\nfor an upper-case checker and those needed for a lower-case checker.",
    "1470": "Specific configurations of multiplexers are often referred to\nas { N-to-M multiplexers}.  Here the value N refers to the\nnumber of inputs, and M refers to the number of outputs.  The\nnumber of select bits can then be calculated as _2(N/M)---N/M \nis generally a power of two---and one way to build such a \nmultiplexer is to use M copies of\nan  (N/M)-to-1 multiplexer.",
    "1471": "Let's extend our upper- and lower-case checker to check\nfor four different ranges of ASCII characters, as shown below.\nThis design uses two {28-to-7} muxes to create a single\nchecker for the four ranges.  Each of the muxes in the figure logically \nrepresents seven {4-to-1} muxes.",
    "1472": "{{file=part2/figs/ascii-four-range.eps,width=3.75in}}",
    "1473": "The table to the right describes the behavior of the checker.",
    "1474": "When the select input S is set to 00, the left mux selects the value 0x00,\nand the right mux selects the value 0x1F, which checks whether the ASCII\ncharacter represented by C is a control character.",
    "1475": "When the select input S=01, the muxes produce the values needed to check\nwhether C is an upper-case letter.",
    "1476": "Similarly, when the select input S=10,",
    "1477": "{c|c|c|c}\n& left& right& \n& comparator& comparator& \nS_1S_0& input& input& R(C) produced  \n00& 0x00& 0x1F& control character?\n01& 0x41& 0x5A& upper-case letter?\n10& 0x61& 0x7A& lower-case letter?\n11& 0x30& 0x39& numeric digit?",
    "1478": "the muxes produce the values \nneeded to check whether C is a lower-case letter.",
    "1479": "Finally, when the select input S=11, the left mux selects the value 0x30,\nand the right mux selects the value 0x39, which checks whether the ASCII\ncharacter represented by C is a digit (0 to 9).",
    "1480": "This set of notes develops comparators for unsigned and 2's complement \nnumbers using the bit-sliced approach that we introduced in Notes Set 2.3.  \nWe then use algebraic manipulation and variation of the internal \nrepresentation to illustrate design tradeoffs.",
    "1481": "Let's begin by thinking about how we as humans compare two {N-bit}\nnumbers, A and B.",
    "1482": "An illustration appears to the right, using N=8.",
    "1483": "For now, let's assume that our numbers are stored in an unsigned \nrepresentation, so we can just think of them as binary numbers\nwith leading 0s.",
    "1484": "We handle 2's complement values later in these notes.",
    "1485": "As humans, we typically start comparing at the most significant bit.\nAfter all, if we find a difference in that bit, we are done, saving\nourselves some time.  In the example to the right, we know that A<B \nas soon as we reach bit 4 and observe that A_4<B_4.",
    "1486": "If we instead start from the least significant bit,\nwe must always look at all of the bits.",
    "1487": "When building hardware to compare all of the bits at once, however,\nhardware for comparing each bit must exist, and the final result \nmust be able to consider",
    "1488": "all of the bits.  Our choice of direction should thus instead depend on \nhow effectively we can build the \ncorresponding functions.  For a single bit slice, the two directions \nare almost identical.  Let's develop a bit slice for\ncomparing from least to most significant.",
    "1489": "{ NOTE TO SELF: We should either do the bit-by-bit comparator \nstate machine in the notes or as an FSM homework problem.  Probably\nthe lattter.}",
    "1490": "Comparison of two numbers, A and B, can produce three possible\nanswers: A<B, A=B, or A>B (one can also build an equality\ncomparator that combines the A<B and A>B cases into a single \nanswer).",
    "1491": "As we move from bit to bit in our design, how much information needs \nto pass from one bit to the next?",
    "1492": "Here you may want to think about how you perform the task yourself.\nAnd perhaps to focus on the calculation for the most significant bit.\nYou need to know the values of the two bits that you are comparing.\nIf those two are not equal, you are done.",
    "1493": "But if the two bits are equal, what do you do?",
    "1494": "The answer is fairly simple: pass along the result\nfrom the less significant bits.",
    "1495": "Thus our bit slice logic for bit M needs to be able to accept \nthree possible answers from the bit slice logic for bit M-1 and must\nbe able to pass one of three possible answers to the logic for bit M+1.",
    "1496": "Since _2(3)=2, we need two bits of input and two bits\nof output in addition to our input bits from numbers A and B.",
    "1497": "The diagram to the right shows an abstract model of our \ncomparator bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming from the top or left \nand outputs going to the bottom or right.  Outside of the bit \nslice logic, we index\nthese comparison bits using the bit number.  The bit slice\nhas C_1^{M-1} and C_0^{M-1} provided as inputs and \nproduces C_1^M and C_0^M as outputs.",
    "1498": "Internally, we use C_1 and C_0 to denote these inputs,\nand Z_1 and Z_0 to denote the outputs.",
    "1499": "Similarly, the",
    "1500": "bits A_M and B_M from the numbers A and B are\nrepresented internally simply as A and B.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.",
    "1501": "We need to select a representation for our three possible answers before\nwe can design any logic.  The representation chosen affects the\nimplementation, as we discuss later in these notes.  For now, we simply\nchoose the representation to the right, which seems reasonable.",
    "1502": "Now we can design the logic for the first bit (bit 0).  In keeping with\nthe bit slice philosophy, in practice we simply use another\ncopy of the full bit slice design for bit 0 and attach the C_1C_0 \ninputs to ground (to denote A=B).  Here we tackle the simpler problem\nas a warm-up exercise.",
    "1503": "{cc|l}\nC_1& C_0& meaning \n0& 0& A=B\n0& 1& A<B\n1& 0& A>B\n1& 1& not used",
    "1504": "The truth table for bit 0 appears to the right (recall that we use Z_1\nand Z_0 for the output names).  Note that the bit 0 function has\nonly two meaningful inputs---there is no bit to the right of bit 0.",
    "1505": "If the two inputs A and B are the same, we output equality.\nOtherwise, we do a {1-bit} comparison and use our representation mapping\nto select the outputs.",
    "1506": "These functions are fairly straightforward to derive by inspection.\nThey are:{-12pt}",
    "1507": "{eqnarray*}\nZ_1 &=& A \nZ_0 &=&  B\n{eqnarray*}",
    "1508": "{\n{cc|cc}\nA& B& Z_1& Z_0 \n0& 0& 0& 0\n0& 1& 0& 1\n1& 0& 1& 0\n1& 1& 0& 0",
    "1509": "}",
    "1510": "These forms should also be intuitive, given the representation\nthat we chose: \nA>B if and only if A=1 and B=0;\nA<B if and only if A=0 and B=1.",
    "1511": "Implementation diagrams for our one-bit functions appear to the right.\nThe diagram to the immediate right shows the implementation as we might\ninitially draw it, and the diagram on the far right shows the \nimplementation",
    "1512": "converted to NAND/NOR gates for a more accurate estimate of complexity\nwhen implemented in CMOS.",
    "1513": "The exercise of designing the logic for bit 0 is also useful in the sense\nthat the logic structure illustrated forms the core of the full design\nin that it identifies the two cases that matter: A<B and A>B.",
    "1514": "Now we are ready to design the full function.  Let's start by writing\na full truth table, as shown on the left below.",
    "1515": "[t]\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0\n0& 0& 1& 1& x& x \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1\n0& 1& 1& 1& x& x \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0\n1& 0& 1& 1& x& x \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0\n1& 1& 1& 1& x& x",
    "1516": "{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0 \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1 \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0 \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0 \nx& x& 1& 1& x& x",
    "1517": "{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0 \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1 \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0 \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0 \n{c|}& x& x",
    "1518": "In the truth table, we \nmarked the outputs as ``don't care'' (x's) whenever C_1C_0=11.\nYou might recall that we ran into problems with our ice cream dispenser\ncontrol in Notes Set 2.2.  However, in that case we could not safely\nassume that a user did not push multiple buttons.  Here, our bit\nslice logic only accepts inputs from other copies of itself (or a fixed\nvalue for bit 0), and---assuming that we design the logic\ncorrectly---our bit slice never generates the 11 combination.\nIn other words, that input combination is impossible (rather than\nundesirable or unlikely), so the result produced on the outputs is \nirrelevant.",
    "1519": "It is tempting to shorten the full truth table by replacing groups of rows.\nFor example, if AB=01, we know that A<B, so the less significant\nbits (for which the result is represented by the C_1C_0 inputs)\ndon't matter.  We could write one row with input pattern ABC_1C_0=01xx and\noutput pattern Z_1Z_0=01.  We might also collapse our ``don't care'' output\npatterns: whenever the input matches ABC_1C_0=xx11, we don't care\nabout the output, so Z_1Z_0=xx.  But these two rows overlap in the\ninput space!  In other words, some input patterns, such as ABC_1C_0=0111,\nmatch both of our suggested new rows.  Which output should take precedence?\nThe answer is that a reader should not have to guess.  { Do not use \noverlapping rows to shorten a truth table.}  In fact, the first of the \nsuggested new rows is not valid: we don't need to produce output 01 \nif we see C_1C_0=11.  Two valid short forms of this truth table appear to \nthe right of the full table.  If you have an ``other'' entry, as\nshown in the rightmost table, this entry should always appear as the \nlast row.  Normal rows, including rows representing multiple input\npatterns, are not required\nto be in any particular order.  Use whatever order makes the table\neasiest to read for its purpose (usually by treating the input pattern\nas a binary number and ordering rows in increasing numeric order).",
    "1520": "In order to translate our design into algebra, we transcribe the truth\ntable into a {K-map} for each output variable, as shown to the right.\nYou may want to perform this exercise yourself and check that you \nobtain the same solution.  Implicants for each output are marked\nin the {K-maps}, giving the following equations:",
    "1521": "{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\nZ_0 &=&  B +  C_0 + B C_0 \n{eqnarray*}",
    "1522": "An implementation based on our equations appears to the right.  \nThe figure makes it easy to see the symmetry between the inputs, which\narises from the representation that we've chosen.  Since the design\nonly uses two-level logic (not counting the inverters on the A\nand B inputs, since inverters can be viewed as {1-input} \nNAND or NOR gates), converting to NAND/NOR simply requires replacing\nall of the AND and OR gates with NAND gates.",
    "1523": "Let's discuss the design's efficiency roughly in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.",
    "1524": "Our initial design uses two inverters, six {2-input} gates, and \ntwo {3-input} gates.",
    "1525": "For speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.",
    "1526": "We can thus estimate our design's",
    "1527": "{file=part2/figs/comparator-try-one.eps,width=2.8in}",
    "1528": "speed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer,\nbut, as we have discussed, the diagram above is equivalent on a \ngate-for-gate basis.  Here we have three gate delays from the A \nand B inputs to the outputs (through the inverters).",
    "1529": "But when we connect multiple copies of our bit slice logic together to \nform a comparator, as shown on the next page, the delay from \nthe A and B inputs\nto the outputs is not as important as the delay from the C_1 and C_0 \ninputs to the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis.  Looking again at the diagram, \nnotice that we have only two gate delays from the C_1 and C_0 inputs \nto the\noutputs.  The total delay for an {N-bit} comparator based on this\nimplementation is thus three gate delays for bit 0 and two more gate\ndelays per additional bit, for a total of 2N+1 gate delays.",
    "1530": "We have a fairly good design at this point---good enough for a homework\nor exam problem in this class, certainly---but let's consider how we\nmight further optimize it.  Today, optimization of logic at this level is \ndone mostly by computer-aided design (CAD) tools, but we want you to \nbe aware of the sources of optimization potential and the tradeoffs \ninvolved.  And, if the topic interests you, someone has to continue to \nimprove CAD software!",
    "1531": "The first step is to manipulate our algebra to expose common terms that\noccur due to the design's symmetry.  Starting with our original equation\nfor Z_1, we have",
    "1532": "{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\n    &=& A  + ( A +  )  C_1\n    &=& A  +  B} C_1\n     Z_0 &=&  B + {A  C_0\n{eqnarray*}{-12pt}",
    "1533": "Notice that the second term in each equation now includes the complement of \nfirst term from the other equation.  For example, the Z_1 equation includes\nthe complement of the B product that we need to compute Z_0.  \nWe may be able to improve our design by combining these computations.",
    "1534": "An implementation based on our new algebraic formulation appears to the \nright.  In this form, we seem to have kept the same number of gates,\nalthough we have replaced the {3-input} gates with inverters.\nHowever, the middle inverters disappear when we convert to NAND/NOR form,\nas shown below to the right.  Our new design requires only two inverters\nand six {2-input} gates, a substantial reduction relative to the\noriginal implementation.",
    "1535": "Is there a disadvantage?  Yes, but only a slight one.  Notice that the\npath from the A and B inputs to the outputs is now four gates (maximum)\ninstead of three.  Yet the path from C_1 and C_0 to the outputs is\nstill only two gates.  Thus, overall, we have merely increased our\n{N-bit} comparator's delay from 2N+1 gate delays to\n2N+2 gate delays.",
    "1536": "{file=part2/figs/comparator-opt.eps,width=4.1in}",
    "1537": "{file=part2/figs/comparator-opt-nn.eps,width=4.1in}",
    "1538": "What about comparing 2's complement numbers?  Can we make use of the\nunsigned comparator that we just designed?",
    "1539": "Let's start by thinking about the sign of the numbers A and B.\nRecall that 2's complement records a number's sign in the most \nsignificant bit.  For example, in the {8-bit} numbers shown in the \nfirst diagram in this set of notes, the sign \nbits are A_7 and B_7.",
    "1540": "Let's denote these sign bits in the general case by A_s and B_s.",
    "1541": "Negative numbers have a sign bit equal to 1, and non-negative numbers\nhave a sign bit equal to 0.",
    "1542": "The table below outlines an initial evaluation of the four possible\ncombinations of sign bits.",
    "1543": "{cc|c|l}\nA_s& B_s& interpretation& solution \n0& 0& A AND B& use unsigned comparator on remaining bits\n0& 1& A AND B<0& A>B\n1& 0& A<0 AND B& A<B\n1& 1& A<0 AND B<0& unknown",
    "1544": "What should we do when both numbers are negative?  Need we design \na completely separate logic circuit?  Can we somehow convert a \nnegative value to a positive one?",
    "1545": "The answer is in fact much simpler.  Recall that {2's complement}\nis defined based on modular arithmetic.  Given an {N-bit} negative\nnumber A, the representation for the bits A[N-2:0] is the same\nas the binary (unsigned) representation of A+2^{N-1}.  An example\nappears to the right.",
    "1546": "{file=part2/figs/comparing-2s.eps,width=2.55in}",
    "1547": "Let's define A_r=A+2^{N-1} as the value of the \nremaining bits for A and B_r similarly for B.",
    "1548": "What happens if we just go ahead and compare A_r and B_r using \nan {(N-1)-bit} unsigned comparator?",
    "1549": "If we find that A_r<B_r we know that A_r-2^{N-1}<B_r-2^{N-1} as well, \nbut that means A<B!  We can do the same with either of the other\npossible results.  In other words, simply comparing A_r with B_r\ngives the correct answer for two negative numbers as well.",
    "1550": "All we need to design is a logic block for the sign bits.  At this point,\nwe might write out a {K-map}, but instead let's rewrite our\nhigh-level table with the new information, as shown to the right.",
    "1551": "Looking at the table, notice the similarity \nto the high-level design for a single bit of an unsigned value.  The",
    "1552": "{cc|l}\nA_s& B_s& solution \n0& 0& pass result from less significant bits\n0& 1& A>B\n1& 0& A<B\n1& 1& pass result from less significant bits",
    "1553": "only difference is that the two A=B cases are reversed.  If we\nswap A_s and B_s, the function is identical.  We can simply use\nanother bit slice but swap these two inputs.",
    "1554": "Implementation of an {N-bit} 2's complement comparator based\non our bit slice comparator is shown below.  The blue circle highlights\nthe only change from the {N-bit} unsigned comparator, which is\nto swap the two inputs on the sign bit.",
    "1555": "{{file=part2/figs/integrated-2s.eps,width=5.5in}}",
    "1556": "Let's return to the topic of optimization.  To what extent \ndid the representation of the three outcomes affect our ability\nto develop a good bit slice design?  Although selecting a good \nrepresentation can be quite important, for this particular problem\nmost representations lead to similar implementations.",
    "1557": "Some representations, however, have interesting properties.  Consider",
    "1558": "{cc|l|l}\nC_1& C_0& original& alternate \n0& 0& A=B& A=B\n0& 1& A<B& A>B\n1& 0& A>B& not used\n1& 1& not used& A<B",
    "1559": "the alternate representation on the right, for example (a copy of the \noriginal representation is included for comparison).  Notice that \nin the alternate representation, C_0=1 whenever A=B.",
    "1560": "Once we have found the numbers to be different in some bit, the end\nresult can never be equality, so perhaps with the right \nrepresentation---the new one, for example---we might be able to\ncut delay in half?",
    "1561": "An implementation based on the alternate representation appears in the\ndiagram to the right.  As you can see, in terms of gate count,\nthis design replaces one {2-input} gate with an inverter and\na second {2-input} gate with a {3-input} gate.  The path\nlengths are the same, requiring 2N+2 gate delays for \nan {N-bit} comparator.\nOverall, it is about the same as our original design.",
    "1562": "{file=part2/figs/comparator-opt-alt.eps,width=4.1in}",
    "1563": "Why didn't it work?  Should we consider still other representations?\nIn fact, none of the possible representations that we might choose\nfor a bit slice can cut the delay down to one gate delay per bit.  \nThe problem is fundamental, and is related to the nature of CMOS.\nFor a single bit slice, we define the incoming and outgoing \nrepresentations to be the same.  We also need to have at least\none gate in the path to combine the C_1 and C_0 inputs with\ninformation from the bit slice's A and B inputs.  But all CMOS\ngates invert the sense of their inputs.  Our choices are limited\nto NAND and NOR.  Thus we need at least two gates in the path to\nmaintain the same representation.",
    "1564": "One simple answer is to use different representations for odd and\neven bits.  Instead, we optimize a logic circuit\nfor comparing two bits.  We base our design on the alternate \nrepresentation.  The implementation is shown below.  The left\nshows an implementation based on the algebra, and the right shows\na NAND/NOR implementation.  Estimating by gate count and number of\ninputs, the {two-bit} design doesn't save much over two\nsingle bit slices in terms of area.  In terms of delay, however,\nwe have only two gate delays from C_1 and C_0 to either output.\nThe longest path from the A and B inputs to the outputs is\nfive gate delays.  Thus, for an {N-bit} comparator built\nwith this design, the total delay is only N+3 gate delays.\nBut N has to be even.",
    "1565": "{file=part2/figs/comparator-two.eps,width=3.125in}\n{file=part2/figs/comparator-two-nn.eps,width=3.125in}",
    "1566": "As you can imagine, continuing to scale up the size of our logic\nblock gives us better performance at the expense of a more complex\ndesign.  Using the alternate representation may help you to see\nhow one can generalize the approach to larger groups of bits---for\nexample, you may have noticed the two bitwise \ncomparator blocks on the left of the implementations above.",
    "1567": "Boolean logic functions, truth tables, etc.\n \n overflow expression using logic functions (and difficulty with\n    binary overflow!)\n \n completeness\n \n implications of completeness: enable abstraction, and evaluate\n     future possible implementations of devices quickly: logically complete\n     or not?\n \n examples\n \n generalizing to operations on sets of bits",
    "1568": "This set of notes briefly describes a generalization to truth tables,\nthen introduces Boolean logic operations as well as \nnotational conventions and tools that we use to express general\nfunctions on bits.  We illustrate how logic operations enable \nus to express functions such as overflow conditions concisely,\nthen show by construction that a small number of logic operations\nsuffices to describe any operation on any number of bits.  \nWe close by discussing a few implications and examples.",
    "1569": "You have seen the basic form of truth tables in the textbook and in class.\nOver the semester, we will introduce several\nextensions to the basic concept, mostly with the\ngoal of reducing the amount of writing necessary when using truth\ntables.  For example, the truth table to the right uses two generalizations\nto show the carry out C (also the unsigned overflow indicator) and the \nsum S produced by\nadding two {2-bit} unsigned numbers.\nFirst, rather than writing each input bit separately, we have grouped\npairs of input bits into the numbers A and B.  \nSecond, we have defined multiple \noutput columns so as to include both bits of S as well as C in \nthe same table.\nFinally, we have \ngrouped the two bits of S into one column.",
    "1570": "Keep in mind as you write truth tables that only rarely does an operation\ncorrespond to a simple and familiar process such as addition of base 2\nnumbers.  We had to choose the unsigned and 2's complement representations\ncarefully to allow ourselves to take advantage of a familiar process.\nIn general, for each line of a truth table for an operation, you may\nneed to make use of the input representation to identify the input values,\ncalculate the operation's result as a value, and then translate the value\nback into the correct bit pattern using the output representation.\nSigned magnitude addition, for example, does not always correspond to\nbase 2 addition: when the",
    "1571": "{cc|cc}\n{c|}& \nA& B& C& S \n00& 00& 0& 00\n00& 01& 0& 01\n00& 10& 0& 10\n00& 11& 0& 11\n01& 00& 0& 01\n01& 01& 0& 10\n01& 10& 0& 11\n01& 11& 1& 00\n10& 00& 0& 10\n10& 01& 0& 11\n10& 10& 1& 00\n10& 11& 1& 01\n11& 00& 0& 11\n11& 01& 1& 00\n11& 10& 1& 01\n11& 11& 1& 10",
    "1572": "signs of the two input operands differ, \none should instead use base 2 subtraction.  For other operations or\nrepresentations, base 2 arithmetic may have no relevance at all.",
    "1573": "In the middle of the 19^ century, George Boole introduced a \nset of logic operations that are today known as { Boolean logic}\n(also as { Boolean algebra}).  These operations today form one of\nthe lowest abstraction levels in digital systems, and an understanding\nof their meaning and use is critical to the effective development of \nboth hardware and software.",
    "1574": "You have probably seen these functions many times already in your \neducation---perhaps first in set-theoretic form as Venn diagrams.  \nHowever, given the use of common\nEnglish words { with different meanings} to name some of the functions,\nand the sometimes confusing associations made even by engineering\neducators, we want to provide you with a concise set of\ndefinitions that generalizes correctly to more than two operands.\nYou may have learned these functions based on truth values \n(true and false), but we define them based on bits, \nwith 1 representing true and 0 representing false.",
    "1575": "Table  on the next page lists logic operations.",
    "1576": "The first column in the table lists the name of each function.  The second\nprovides a fairly complete set of the notations that you are likely\nto encounter for each function, including both the forms used in\nengineering and those used in mathematics.  The third column defines \nthe function's\nvalue for two or more input operands (except for NOT, which operates on\na single value).  The last column shows the form generally used in\nlogic schematics/diagrams and mentions the important features used\nin distinguishing each function (in pictorial form usually called\na { gate}, in reference to common physical implementations) from the \nothers.",
    "1577": "{\n{|c|c|c|c|}\n{ Function}& { Notation}& { Explanation}& { Schematic} \nAND& {A AND B}{AB}{A}{A}{A}& {the ``all'' function: result is 1 iff}{{ all} input operands are equal to 1}& {flat input, round output}  \nOR& {A OR B}{A+B}{A}& {the ``any'' function: result is 1 iff}{{ any} input operand is equal to 1}& {{-6pt}}{round input, pointed output} \nNOT& {NOT A}{A'}{}{}& {logical complement/negation:}{NOT 0 is 1, and NOT 1 is 0}& {triangle and circle} \n{exclusive OR}& {A XOR B}{A}& {the ``odd'' function: result is 1 iff an { odd}}{number of input operands are equal to 1}& {{-6pt}}{OR with two lines}{on input side} \n{``or''}& A, B, or C& {the ``one of'' function: result is 1 iff exactly}{{ one of} the input operands is equal to 1}& (not used)",
    "1578": "}\n{Boolean logic operations, notation, definitions, and symbols.}{-12pt}",
    "1579": "The first function of importance is { AND}.  Think of { AND} as the\n``all'' function: given a set of input values as operands, AND evaluates \nto 1 if and only if { all} of the input values are 1.  The first\nnotation line simply uses the name of the function.  In Boolean algebra,\nAND is typically represented as multiplication, and the middle three\nforms reflect various ways in which we write multiplication.  The last\nnotational variant is from mathematics, where the AND function is formally\ncalled { conjunction}.",
    "1580": "The next function of importance is { OR}.  Think of { OR} as the\n``any'' function: given a set of input values as operands, OR evaluates\nto 1 if and only if { any} of the input values is 1.  The actual\nnumber of input values equal to 1 only matters in the sense of whether\nit is at least one.  The notation for OR is organized in the same way\nas for AND, with the function name at the top, the algebraic variant that\nwe will use in class---in this case addition---in the middle, and\nthe mathematics variant, in this case called { disjunction}, at the\nbottom.",
    "1581": "{ The definition of Boolean OR is not the same as our use of \nthe word ``or'' \nin English.}  For example, if you are fortunate enough to enjoy\na meal on a plane, you might be offered several choices: ``Would you like\nthe chicken, the beef, or the vegetarian lasagna today?''  Unacceptable\nanswers to this English question include: ``Yes,'' ``Chicken and lasagna,''\nand any other combination that involves more than a single choice!",
    "1582": "You may have noticed that we might have instead mentioned that\nAND evaluates to 0 if any input value is 0, and that OR evaluates to 0\nif all input values are 0.  These relationships reflect a mathematical\nduality underlying Boolean logic that has important practical\nvalue in terms of making it easier for humans to digest complex logic\nexpressions.\nWe will talk more about duality later in the course, but you\nshould learn some of the practical\nvalue now: if you are trying to evaluate an AND function, look for an input\nwith value 0; if you are trying to evaluate an OR function, look for an \ninput with value 1.  If you find such an input, you know the function's\nvalue without calculating any other input values.",
    "1583": "We next consider the { logical complement} function, { NOT}.  The NOT\nfunction is also called { negation}.  Unlike our\nfirst two functions, NOT accepts only a single operand, and reverses\nits value, turning 0 into 1 and 1 into 0.  The notation follows the\nsame pattern: a version using the function name at the top, followed\nby two variants used in Boolean algebra, and finally the version\nfrequently used in mathematics.  For the NOT gate, or { inverter},\nthe circle is actually the important part: the triangle by itself\nmerely copies the input.  You will see the small circle added to other\ngates on both inputs and outputs; in both cases the circle implies a NOT\nfunction.",
    "1584": "Last among the Boolean logic functions, we have the\n{ XOR}, or { exclusive OR} function.\nThink of XOR as the ``odd'' function: given a set of input values as\noperands, XOR evaluates to 1 if and only if { an odd number} of\nthe input values are 1.  Only two variants of XOR notation are given:\nthe first using the function name, and the second used with Boolean\nalgebra.  Mathematics rarely uses this function.",
    "1585": "Finally, we have included the meaning of the word ``or'' in English as\na separate function entry to enable you to compare that meaning\nwith the Boolean logic functions easily.  Note that many people refer\nto English'",
    "1586": "use of the word ``or'' as ``exclusive'' because one\ntrue value excludes all others from being true.  Do not let this \nhuman language ambiguity confuse you about XOR!  For all logic design\npurposes, { XOR is the odd function}.",
    "1587": "The truth table to the right provides values illustrating these functions operating\non three inputs.  The AND, OR, and XOR functions are all \nassociative---(A  B)  C = A  (B  C)---and\ncommutative---A  B = B  A,\nas you may have already realized from their definitions.",
    "1588": "{ccc|cccc}\n{c|}& \nA& B& C& ABC& A+B+C& & A \n0& 0& 0& 0& 0& 1& 0\n0& 0& 1& 0& 1& 1& 1\n0& 1& 0& 0& 1& 1& 1\n0& 1& 1& 0& 1& 1& 0\n1& 0& 0& 0& 1& 0& 1\n1& 0& 1& 0& 1& 0& 0\n1& 1& 0& 0& 1& 0& 0\n1& 1& 1& 1& 1& 0& 1",
    "1589": "In the last set of notes, we discussed overflow conditions for unsigned\nand 2's complement representations.  Let's use Boolean logic to express\nthese conditions.",
    "1590": "We begin with addition of two {1-bit} unsigned numbers.  \nCall the two input bits A_0 and B_0.  If you write a truth table for\nthis operation, you'll notice that overflow occurs only when all (two)\nbits are 1.  If either bit is 0, the sum can't exceed 1, so overflow\ncannot occur.  In other words, overflow in this case can be written using\nan AND operation:",
    "1591": "{eqnarray*}\nA_0B_0\n{eqnarray*}",
    "1592": "The truth table for adding two {2-bit} unsigned numbers is four\ntimes as large, and seeing the structure may be difficult.  One way of\nwriting the expression for overflow of {2-bit} unsigned addition is\nas follows:",
    "1593": "{eqnarray*}\nA_1B_1 + (A_1+B_1)A_0B_0\n{eqnarray*}",
    "1594": "This expression is slightly trickier to understand.  Think about the\nplace value of the bits.  If both of the most significant bits---those\nwith place value 2---are 1,\nwe have an overflow, just as in the case of {1-bit} addition.\nThe A_1B_1 term represents this case.  We also have an overflow if\none or both (the OR) of the most significant bits are 1 and the\nsum of the two next significant bits---in this case those with place \nvalue 1---generates a carry.",
    "1595": "The truth table for adding two {3-bit} unsigned numbers is \nprobably not something that you want to write out.  Fortunately,\na pattern should start to become clear with the following expression:",
    "1596": "{eqnarray*}\nA_2B_2 + (A_2+B_2)A_1B_1 + (A_2+B_2)(A_1+B_1)A_0B_0\n{eqnarray*}",
    "1597": "In the {2-bit} case, we mentioned the ``most significant bit''\nand the ``next most significant bit'' to help you see the pattern.\nThe same reasoning describes the first two product terms in our\noverflow expression for {3-bit} unsigned addition (but the place\nvalues are 4 for the most significant bit and 2 for the next most\nsignificant bit).  The last term represents the overflow case in which\nthe two least significant bits generate a carry which then propagates\nup through all of the other bits because at least one of the two bits\nin every position is a 1.",
    "1598": "The overflow condition for addition of two {N-bit} 2's complement\nnumbers can be written fairly concisely in terms of the first bits\nof the two numbers and the first bit of the sum.  Recall that overflow\nin this case depends only on whether the three numbers are negative\nor non-negative, which is given by",
    "1599": "{ 0pt\n 0pt",
    "1600": "&A_{N-1}&A_{N-2}&&A_2&A_1&A_0\n+   &B_{N-1}&B_{N-2}&&B_2&B_1&B_0 \n&S_{N-1}&S_{N-2}&&S_2&S_1&S_0",
    "1601": "}",
    "1602": "the most significant bit.  Given\nthe bit names as shown to the right, we can write the overflow condition\nas follows:",
    "1603": "{eqnarray*}\nA_{N-1} B_{N-1} {S_{N-1}}+\n{A_{N-1}} {B_{N-1}} S_{N-1}\n{eqnarray*}",
    "1604": "The overflow condition does of course depend on all of the bits in\nthe two numbers being added.  In the expression above, we have simplified\nthe form by using S_{N-1}.  But S_{N-1} depends on the bits A_{N-1}\nand B_{N-1} as well as the carry out of bit (N-2).",
    "1605": "Later in this set of notes, we present a technique with which you can derive\nan expression for an arbitrary Boolean logic function.  As an exercise,\nafter you have finished reading these notes, try using that technique to\nderive an overflow expression for addition of \ntwo {N-bit} 2's complement numbers based on \nA_{N-1}, B_{N-1}, and the carry out of bit (N-2) (and into \nbit (N-1)), which we might\ncall C_{N-1}.  You might then calculate C_{N-1} in terms of the rest\nof the bits of A and B\nusing the expressions for\nunsigned overflow just discussed.",
    "1606": "In the next month or so, you will learn how to derive\nmore compact expressions yourself from truth tables or other representations\nof Boolean logic functions.",
    "1607": "Why do we feel that such a short list of functions is enough?  If you \nthink about the\nnumber of possible functions on N bits, you might think that we need\nmany more functions to be able to manipulate bits.  With 10 bits, for\nexample, there are 2^ such functions.  Obviously, some of them\nhave never been used in any computer system, but maybe we should define\nat least a few more logic operations?\nIn fact, we do not\neven need XOR.  The functions AND, OR, and NOT are sufficient, even if\nwe only allow two input operands for AND and OR!",
    "1608": "The theorem below captures this idea, called { logical completeness}.\nIn this case, we claim that the set of functions {AND, OR, NOT} is\nsufficient to express any operation on any finite number of variables, \nwhere each variable is a bit.",
    "1609": "{ Theorem:}",
    "1610": "Given enough {2-input} AND, {2-input} OR, and {1-input}\nNOT functions, one can express any Boolean logic function on any finite \nnumber of variables.",
    "1611": "{ Proof:}",
    "1612": "The proof of our theorem is { by construction}.  In other words, \nwe show a systematic\napproach for transforming an arbitrary Boolean logic function on an\narbitrary number of variables into a form that uses only AND, OR, and\nNOT functions on one or two operands.",
    "1613": "As a first step, we remove the restriction on the number of inputs for\nthe AND and OR functions.  For this purpose, we state and prove two\n{ lemmas}, which are simpler theorems used to support the proof of\na main theorem.",
    "1614": "{ Lemma 1:}",
    "1615": "Given enough {2-input} AND functions, one can express an AND function\non any finite number of variables.",
    "1616": "{ Proof:}",
    "1617": "We prove the Lemma { by induction}.{We assume that you have\nseen proof by induction previously.}  Denote the number of inputs to a\nparticular AND function by N.",
    "1618": "The base case is N=2.  Such an AND function is given.",
    "1619": "To complete the proof, we need only show that, given \nany number of AND functions with up to N inputs, we\ncan express an AND function with N+1 inputs.  To do so, we need\nmerely use one {2-input} AND function to join together\nthe result of an {N-input} AND function with an additional\ninput, as illustrated to the right.",
    "1620": "{ Lemma 2:}",
    "1621": "Given enough {2-input} OR functions, one can express an OR function\non any finite number of variables.",
    "1622": "{ Proof:}",
    "1623": "The proof of Lemma 2 is identical in structure to that of Lemma 1, but\nuses OR functions instead of AND functions.",
    "1624": "Let's now consider a small subset of functions on N variables.  \nFor any such function, you can write out the truth table for the\nfunction.  The output of a logic function is just a bit, \neither a 0 or a 1.  Let's consider the set of functions on N\nvariables that produce a 1 for exactly one combination of the N\nvariables.  In other words, if you were to write out the truth\ntable for such a function, exactly one row in the truth table would\nhave output value 1, while all other rows had output value 0.",
    "1625": "{ Lemma 3:}",
    "1626": "Given enough AND functions and {1-input}\nNOT functions, one can express any Boolean logic function \nthat produces a 1 for exactly one combination of\nany finite number of variables.",
    "1627": "{ Proof:}",
    "1628": "The proof of Lemma 3 is by construction.\nLet N be the number of variables on which the function operates.\nWe construct a { minterm} on these N variables,\nwhich is an AND operation on each variable or its complement.\nThe minterm is specified by looking at the unique combination of \nvariable values that produces a 1 result for the function.\nEach variable that must be a 1 is included as itself, while\neach variable that must be a 0 is included as the variable's complement\n(using a NOT function).  The resulting minterm produces the\ndesired function exactly.  When the variables all match\nthe values for which the function should produce 1, the\ninputs to the AND function are all 1, and the function produces 1.\nWhen any variable does not match the value for which the function\nshould produce 1, that variable (or its complement) acts as a 0\ninput to the AND function, and the function produces a 0, as desired.",
    "1629": "The table below shows all eight minterms for three variables.",
    "1630": "{\n{ccc|cccccccc}\n{c|}& \nA& B& C& \n  &\n  C&\n B &\n B C&\nA  &\nA  C&\nA B &\nA B C\n \n0& 0& 0& 1& 0& 0& 0& 0& 0& 0& 0\n0& 0& 1& 0& 1& 0& 0& 0& 0& 0& 0\n0& 1& 0& 0& 0& 1& 0& 0& 0& 0& 0\n0& 1& 1& 0& 0& 0& 1& 0& 0& 0& 0\n1& 0& 0& 0& 0& 0& 0& 1& 0& 0& 0\n1& 0& 1& 0& 0& 0& 0& 0& 1& 0& 0\n1& 1& 0& 0& 0& 0& 0& 0& 0& 1& 0\n1& 1& 1& 0& 0& 0& 0& 0& 0& 0& 1",
    "1631": "}",
    "1632": "We are now ready to prove our theorem.",
    "1633": "{ Proof (of Theorem):}",
    "1634": "Any given function on N variables\nproduces the value 1 for some set of combinations\nof inputs.  Let's say that M such combinations produce 1.  \nNote that M{2^N}.\nFor each\ncombination that produces 1, we can use\nLemma 1 to construct an {N-input} AND function.\nThen, using Lemma 3, we can use as many as M NOT functions\nand the {N-input} AND function to construct a minterm\nfor that input combination.\nFinally, using Lemma 2, we can construct an {M-input}\nOR function and OR together all of the minterms.\nThe result of the OR is the desired function. If the function\nshould produce a 1 for some combination of inputs, that combination's \nminterm provides a 1 input to the OR, which in turn produces a 1.\nIf a combination should produce a 0, its minterm does not appear in\nthe OR; all other minterms produce 0 for that combination, and thus\nall inputs to the OR are 0 in such cases, and the OR produces 0,\nas desired.",
    "1635": "The construction that we used to prove logical completeness does\nnot necessarily help with efficient design of logic functions.  Think\nabout some of the expressions that we discussed earlier in these\nnotes for overflow conditions.  How many minterms do you need \nfor {N-bit} unsigned overflow?  A single Boolean logic function\ncan be expressed in many different ways, and learning how to develop\nan efficient implementation of a function as well as how to determine\nwhether two logic expressions are identical without actually writing out\ntruth tables are important engineering skills that you will start to \nlearn in the coming months.",
    "1636": "If logical completeness doesn't really help us to engineer logic functions,\nwhy is the idea important?  Think back to the layers of abstraction\nand the implementation of bits from the first couple of lectures.  \nVoltages are real numbers, not bits.  { The device layer implementations\nof Boolean logic functions must abstract away the analog properties\nof the physical system.}  Without such abstraction, we must think\ncarefully about analog issues such as noise every time we make use\nof a bit!",
    "1637": "Logical completeness assures us that no matter what we want to do\nwith bits, implementating a handful of operations correctly is\nenough to guarantee that we never have to worry.",
    "1638": "A second important value of logical completeness is as a tool in\nscreening potential new technologies for computers.  If a new technology\ndoes not allow implementation of a logically complete set of functions,\nthe new technology is extremely unlikely\nto be successful in replacing the current one.",
    "1639": "That said, {AND, OR, and NOT} is not the only logically complete set of\nfunctions.  In fact, our current complementary metal-oxide semiconductor\n(CMOS) technology, on which most of the computer industry is now built,\ndoes not directly implement these functions, as you will see later in\nour class.",
    "1640": "The functions that are implemented directly in CMOS are NAND and NOR, which\nare abbreviations for AND followed by NOT and OR followed by NOT, \nrespectively.  Truth tables for the two are shown to the right.",
    "1641": "Either of these functions by itself forms a logically complete set.\nThat is, both the set  and the set  are logically\ncomplete.  For now, we leave the proof of this claim to you.  Re-",
    "1642": "{cc|cc}\n{c|}& \n& & & {A+B}\nA& B& A NAND B& A NOR B \n0& 0& 1& 1\n0& 1& 1& 0\n1& 0& 1& 0\n1& 1& 0& 0",
    "1643": "member that all\nyou need to show is that you can implement any set known to be\nlogically complete, so in order to prove that  is \nlogically complete (for example),\nyou need only show that you can implement AND, OR, and NOT using only\nNAND.",
    "1644": "Let's use our construction to solve a few examples.  We begin with\nthe functions that we illustrated with the first truth table from this\nset of notes,\nthe carry out C and sum S of two {2-bit}\nunsigned numbers.  Since each output bit requires a separate expression,\nwe now write S_1S_0 for the two bits of the sum.  We also need to\nbe able to make use of the individual bits of the input values, so we \nwrite these as A_1A_0 and B_1B_0, as shown on the left below.\nUsing our \nconstruction from the logical completeness theorem, we obtain the\nequations on the right.\nYou should verify these expressions yourself.",
    "1645": "{",
    "1646": "{cccc|ccc}\n{c|}& \nA_1& A_0& B_1& B_0& C& S_1& S_0 \n0&0& 0&0& 0& 0&0\n0&0& 0&1& 0& 0&1\n0&0& 1&0& 0& 1&0\n0&0& 1&1& 0& 1&1\n0&1& 0&0& 0& 0&1\n0&1& 0&1& 0& 1&0\n0&1& 1&0& 0& 1&1\n0&1& 1&1& 1& 0&0\n1&0& 0&0& 0& 1&0\n1&0& 0&1& 0& 1&1\n1&0& 1&0& 1& 0&0\n1&0& 1&1& 1& 0&1\n1&1& 0&0& 0& 1&1\n1&1& 0&1& 1& 0&0\n1&1& 1&0& 1& 0&1\n1&1& 1&1& 1& 1&0",
    "1647": "{eqnarray*}",
    "1648": "C &=& \n{A_1} A_0 B_1 B_0 +\nA_1 {A_0} B_1 {B_0} +\nA_1 {A_0} B_1 B_0 +\n&& A_1 A_0 {B_1} B_0 +\nA_1 A_0 B_1 {B_0} +\nA_1 A_0 B_1 B_0",
    "1649": "S_1 &=& \n{A_1} {A_0} B_1 {B_0} +\n{A_1} {A_0} B_1 B_0 +\n{A_1} A_0 {B_1} B_0 +\n&& {A_1} A_0 B_1 {B_0} +\nA_1 {A_0} {B_1} {B_0} +\nA_1 {A_0} {B_1} B_0 +\n&& A_1 A_0 {B_1} {B_0} +\nA_1 A_0 B_1 B_0",
    "1650": "S_0 &=& \n{A_1} {A_0} {B_1} B_0 +\n{A_1} {A_0} B_1 B_0 +\n{A_1} A_0 {B_1} {B_0} +\n&& {A_1} A_0 B_1 {B_0} +\nA_1 {A_0} {B_1} B_0 +\nA_1 {A_0} B_1 B_0 +\n&& A_1 A_0 {B_1} {B_0} +\nA_1 A_0 B_1 {B_0}\n{eqnarray*}",
    "1651": "}",
    "1652": "Now let's consider a new function.  Given\nan {8-bit} 2's complement number, A=A_7A_6A_5A_4A_3A_2A_1A_0,\nwe want to compare it with the value -1.  We know that\nwe can construct this function using AND, OR, and NOT, but how?\nWe start by writing the representation for -1, which is 11111111.\nIf the number A matches that representation, we want to produce a 1.\nIf the number A differs in any bit, we want to produce a 0.\nThe desired function has exactly one combination of inputs that\nproduces a 1, so in fact we need only one minterm!  In this case, we\ncan compare with -1 by calculating the expression:",
    "1653": "{eqnarray*}\nA_7  A_6  A_5  A_4  A_3  A_2  A_1  A_0\n{eqnarray*}",
    "1654": "Here we have explicitly included multiplication symbols to avoid\nconfusion with our notation for groups of bits, as we used when \nnaming the individual bits of A.",
    "1655": "In closing, we briefly introduce a generalization of logic\noperations to groups of bits.  Our representations for integers,\nreal numbers, and characters from human languages all use more than\none bit to represent a given value.  When we use computers, we often\nmake use of multiple bits in groups in this way.  A { byte}, for example,\ntoday means an ordered group of eight bits.",
    "1656": "We can extend our logic functions to operate on such groups by pairing\nbits from each of two groups and performing the logic operation on each\npair.  For example, given\nA=A_7A_6A_5A_4A_3A_2A_1A_0=01010101\nand\nB=B_7B_6B_5B_4B_3B_2B_1B_0=11110000, we calculate\nA AND B by computing the AND of each pair of bits, \nA_7 AND B_7,\nA_6 AND B_6,\nand so forth, to\nproduce the result 01010000, as shown to the right.\nIn the same way, we can extend other logic\noperations, such as OR, NOT, and XOR, to operate on bits of groups.",
    "1657": "{ 0pt\n 0pt",
    "1658": "&A  & 0&1& 0&1& 0&1& 0&1\n  AND   &B& 1&1&1&1&0&0&0&0 \n&& 0& 1&0&1&0&0&0&0",
    "1659": "}",
    "1660": "This set of notes explains the rationale for using the 2's complement\nrepresentation for signed integers and derives the representation \nbased on equivalence of the addition function to that of addition\nusing the unsigned representation with the same number of bits.",
    "1661": "In modern digital systems, we represent all types of information\nusing binary digits, or { bits}.  Logically, a bit is either 0 or 1.\nPhysically, a bit may be a voltage, a magnetic field, or even the\nelectrical resistance of a tiny sliver of glass.",
    "1662": "Any type of information can be represented with an ordered set of\nbits, provided that \n{ any given pattern of bits corresponds to only\none value} and that { we agree in advance on which pattern of bits\nrepresents which value}.",
    "1663": "For unsigned integers---that is, whole numbers greater or equal to\nzero---we chose to use the base 2 representation already familiar to\nus from mathematics.  We call this representation the { unsigned\nrepresentation}.  For example, in a {4-bit} unsigned representation,\nwe write the number 0 as 0000, the number 5 as 0101, and the number 12\nas 1100.  Note that we always write the same number of bits for any\npattern in the representation: { in a digital system, there is no \n``blank'' bit value}.",
    "1664": "In class, we discussed the question of what makes one representation\nbetter than another.  The value of the unsigned representation, for\nexample, is in part our existing familiarity with the base 2 analogues\nof arithmetic.  For base 2 arithmetic, we can use nearly identical\ntechniques to those that we learned in elementary school for adding, \nsubtracting, multiplying, and dividing base 10 numbers.",
    "1665": "Reasoning about the relative merits of representations from a\npractical engineering perspective is (probably) currently beyond \nyour ability.",
    "1666": "Saving energy, making the implementation simple, and allowing the\nimplementation to execute quickly probably all sound attractive, but \na quantitative comparison between two representations on any of these\nbases requires knowledge that you will acquire in the\nnext few years.",
    "1667": "We can sidestep such questions, however, by realizing that if a\ndigital system has hardware to perform operations such as addition\non unsigned values, using the same piece of hardware to operate\non other representations incurs little or no additional cost.\nIn this set of notes, we discuss the 2's complement representation,\nwhich allows reuse of the unsigned add unit (as well as a basis for\nperforming subtraction of either representation using an add unit!).\nIn discussion section and in your homework, you will \nuse the same idea to perform operations on other representations, such as\nchanging an upper case letter in ASCII to a lower case one, or converting\nfrom an ASCII digit to an unsigned representation of the same number.",
    "1668": "In order to define a representation for signed integers that allows\nus to reuse a piece of hardware designed for unsigned integers, we\nmust first understand what such a piece of hardware actually does (we\ndo not need to know how it works yet---we'll explore that question \nlater in our class).",
    "1669": "The unsigned representation using {N} bits is not closed\nunder addition.  In other words, for any value of N, we can easily\nfind two {N-bit} unsigned numbers that, when added together,\ncannot be represented as an {N-bit} unsigned number.  With N=4, \nfor example, we can add 12 (1100) and 6 (0110) to obtain 18.\nSince 18 is outside of the range [0,2^4-1] representable using\nthe {4-bit} unsigned representation, our representation breaks\nif we try to represent the sum using this representation.  We call\nthis failure an { overflow} condition: the representation cannot\nrepresent the result of the operation, in this case addition.",
    "1670": "Using more bits to represent the answer is not an attractive solution, \nsince we might then want to use more bits for the inputs, which in turn\nrequires more bits for the outputs, and so on.  We cannot build \nsomething supporting an infinite number of bits.  Instead, we \nchoose a value for N and build an add unit that adds two {N-bit}\nnumbers and produces an {N-bit} sum (and some overflow \nindicators, which we discuss in the next set of notes).  The diagram\nto the right shows how we might draw such a device, with two {N-bit}\nnumbers entering at from the top, and the {N-bit} sum coming out\nfrom the bottom.",
    "1671": "The function used for {N-bit} unsigned addition is addition \nmodulo 2^N.  In a practical sense, you can think of this function\nas simply keeping the last N bits of the answer; other bits \nare simply discarded.  In the example to the right,\nwe add 12 and 6 to obtain 18, but then discard the extra bit on the\nleft, so the add unit produces 2 (an overflow).",
    "1672": "{ Modular arithmetic} defines a way of performing arithmetic for\na finite number of possible values, usually integers.  \nAs a concrete example, let's use modulo 16, which corresponds to\nthe addition unit for our {4-bit} examples.",
    "1673": "Starting with the full range of integers, we break the number\nline into contiguous groups of 16 integers, as shown to the right.",
    "1674": "The numbers 0 to 15 form one group.  The numbers -16 to -1 form a\nsecond group, and the numbers from 16 to 31 form a third group. \nAn infinite number of groups are defined in this manner.",
    "1675": "We then define 16 { equivalence classes} consisting of the first numbers\nfrom all groups, the second numbers from all groups, and so forth.\nFor example, the numbers , -32, -16, 0, 16, 32,  form\none such equivalence class.",
    "1676": "Mathematically, we say that two numbers A and B are equivalent modulo 16,\nwhich we write as",
    "1677": "{eqnarray*}\n(A &=& B)  16, {or sometimes as}\nA && B {(mod 16)}\n{eqnarray*}",
    "1678": "if and only if A=B+16k for some integer k.",
    "1679": "Equivalence as defined by a particular modulus\ndistributes over addition and multiplication.  If, for example,\nwe want to find the equivalence class for (A + B)  16,\nwe can find the equivalence classes for A (call it C) and B \n(call it D) and then calculate the equivalence class \nof (C + D)  16.\nAs a concrete example of distribution over multiplication, \ngiven (A = 1,083,102,112  7,323,127)  10,\nfind A.",
    "1680": "For this problem, we note that the first number is equivalent \nto 2  10, while the second number is equivalent \nto 7  10.  We then write (A = 2  7)  10,\nand, since 2  7 = 14, we have (A = 4)  10.",
    "1681": "Given these equivalence classes, we might instead choose to draw a circle\nto identify the equivalence classes and to associate each class with one\nof the sixteen possible {4-bit} patterns, as shown to the right.\nUsing this circle representation, we can add by counting clockwise around\nthe circle, and we can subtract by counting in a counterclockwise direction\naround the circle.  With an unsigned representation, we choose to use the\ngroup from [0,15] (the middle group in the diagram markings to the right)\nas the number represented by each of the patterns.  Overflow occurs\nwith unsigned addition (or subtraction) because we can only choose one\nvalue for each binary pattern.",
    "1682": "In fact, we can choose any single value for each pattern to create a \nrepresentation, and our add unit will always produce results that\nare correct modulo 16.  Look back at our overflow example, where\nwe added 12 and 6 to obtain 2, and notice that (2=18)  16.\nNormally, only a contiguous sequence of integers makes a useful\nrepresentation, but we do not have to restrict ourselves to \nnon-negative numbers.",
    "1683": "The 2's complement representation can then be defined by choosing a \nset of integers balanced around zero from the groups.  In the circle \ndiagram, for example, we might choose to represent numbers\nin the range [-7,7] when using 4 bits.  What about the last pattern, 1000?\nWe could choose to represent either -8 or 8.  The number of arithmetic\noperations that overflow is the same with both choices (the choices\nare symmetric around 0, as are the combinations of input operands that \noverflow), so we gain nothing in that sense from either choice.\nIf we choose to represent -8, however, notice that all patterns starting\nwith a 1 bit then represent negative numbers.  No such simple check\narises with the opposite choice, and thus an {N-bit} 2's complement \nrepresentation is defined to represent the range [-2^{N-1},2^{N-1}-1],\nwith patterns chosen as shown in the circle.",
    "1684": "Some people prefer an algebraic approach to understanding the\ndefinition of 2's complement, so we present such an approach next.\nLet's start by writing f(A,B) for the result of our add unit:",
    "1685": "{eqnarray*}\nf(A,B) = (A + B)  2^N\n{eqnarray*}",
    "1686": "We assume that we want to represent a set of integers balanced around 0\nusing our signed representation, and that we will use the same binary\npatterns as we do with an unsigned representation to represent\nnon-negative numbers.  Thus, with an {N-bit} representation,\nthe patterns in the range [0,2^{N-1}-1] are the same as those\nused with an unsigned representation.  In this case, we are left with\nall patterns beginning with a 1 bit.",
    "1687": "The question then is this: given an integer k, 2^{N-1}>k>0, for which we \nwant to find a pattern to represent -k, and any integer m\nthat we might want to add to -k, \ncan we find another integer p>0\nsuch that",
    "1688": "(-k + m = p + m)  2^N   ?",
    "1689": "If we can, we can use p's representation to represent -k and our\nunsigned addition unit f(A,B) will work correctly.",
    "1690": "To find the value p, start by subtracting m from both sides of\nEquation () to obtain:",
    "1691": "(-k = p)  2^N",
    "1692": "Note that (2^N=0)  2^N, and add this equation to \nEquation () to obtain",
    "1693": "{eqnarray*}\n(2^N-k = p)  2^N\n{eqnarray*}",
    "1694": "Let p=2^N-k.",
    "1695": "For example, if N=4, k=3 gives p=16-3=13, which is the pattern 1101.\nWith N=4 and k=5, we obtain p=16-5=11, which is the pattern 1011.\nIn general, since\n2^{N-1}>k>0, \nwe have 2^{N-1}<p<2^N.  But these patterns are all unused---they all\nstart with a 1 bit!---so the patterns that we have defined for negative\nnumbers are disjoint from those that we used for positive numbers, and\nthe meaning of each pattern is unambiguous.",
    "1696": "The algebraic definition of bit patterns for negative numbers\nalso matches our circle diagram from the last\nsection exactly, of course.",
    "1697": "The algebraic approach makes understanding negation of an integer\nrepresented using 2's complement fairly straightforward, and gives \nus an easy procedure for doing so.\nRecall that given an integer k in an {N-bit} 2's complement\nrepresentation, the {N-bit} pattern for -k is given by 2^N-k \n(also true for k=0 if we keep only the low N bits of the result).  \nBut 2^N=(2^N-1)+1.  Note that 2^N-1 is the pattern of\nall 1 bits.  Subtracting any value k from this value is equivalent\nto simply flipping the bits, changing 0s to 1s and 1s to 0s.\n(This operation is called a { 1's complement}, by the way.)\nWe then add 1 to the result to find the pattern for -k.",
    "1698": "Negation can overflow, of course.  Try finding the negative pattern for -8 \nin {4-bit} 2's complement.",
    "1699": "Finally, be aware that people often overload the term 2's complement\nand use it to refer to the operation of negation in a 2's complement\nrepresentation.  In our class, we try avoid this confusion: 2's complement\nis a representation for signed integers, and negation is an operation\nthat one can apply to a signed integer (whether the representation used\nfor the integer is 2's complement or some other representation for signed\nintegers).",
    "1700": "For some of the topics in this course, we plan to cover the material\nmore deeply than does the textbook.  We will provide notes in this\nformat to supplement the textbook for this purpose.",
    "1701": "In order to make these notes more useful as a reference, definitions are\nhighlighted with boldface, and italicization emphasizes pitfalls or other\nimportant points.",
    "1702": "{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}",
    "1703": "These notes are broken up into four parts, corresponding to the three\nmidterm exams and the final exam.  Each part is covered by\none examination in our class.  { The last section of each of the four\nparts gives you\na summary of material that you are expected to know for the corresponding\nexam.}  Feel\nfree to read it in advance.",
    "1704": "As discussed in the textbook and in class, a { universal\ncomputational device} (or { computing machine}) is a device \nthat is capable of computing the\nsolution to any problem that can be computed, provided that the device\nis given enough storage and time for the computation to finish.",
    "1705": "One might ask whether we can describe problems that we cannot answer (other \nthan philosophical ones, such as the meaning of life).",
    "1706": "The answer is yes: there are problems that are provably { undecidable},\nfor which no amount of computation can solve the problem in general.\nThis set of notes describes the first problem known to be\nundecidable, the { halting problem}.  For our class, you need only recognize\nthe name and realize that one can, in fact, give examples of problems\nthat cannot be solved by computation.  In the future, you should be able\nto recognize this type of problem so as to avoid spending your time\ntrying to solve it.",
    "1707": "The things that we call computers today, whether we are talking about\na programmable microcontroller in a microwave oven or the Blue Waters\nsupercomputer sitting on the south end of our campus (the United\nStates' main resource to support computational science research), are\nall equivalent in the sense of what problems they can solve.",
    "1708": "These machines do, of course, have access to different amounts of memory, \nand compute at different speeds.",
    "1709": "The idea that a single model of computation could be described and proven\nto be equivalent to all other models came out of a 1936 paper by Alan Turing, \nand today we generally refer to these devices as { Turing machines}.",
    "1710": "All computers mentioned earlier, as well as all computers with which you \nare familiar in your daily life, are provably equivalent to Turing machines.",
    "1711": "Turing also conjectured that his definition of computable was\nidentical to the ``natural'' definition (today, this claim is known\nas the { Church-Turing conjecture}).  In other words, a problem that\ncannot be solved by a Turing machine cannot be solved in any\nsystematic manner, with any machine, or by any person.  This conjecture\nremains unproven!  However, neither has anyone been able to disprove\nthe conjecture, and it is widely believed to be true.  Disproving the\nconjecture requires that one demonstrate a systematic technique (or a\nmachine) capable of solving a problem that cannot be solved by a\nTuring machine.  No one has been able to do so to date.",
    "1712": "You might reasonably ask whether any problems can be shown to\nbe incomputable.  More common terms for such problems---those known\nto be insolvable by any computer---are { intractable} or \nundecidable.",
    "1713": "In the same 1936 paper in which he introduced the universal computing\nmachine, Alan Turing also provided an answer to this question\nby introducing (and proving) that there are in fact problems that cannot be\ncomputed by a universal computing machine.\nThe problem that\nhe proved undecidable, using proof techniques almost identical to those\ndeveloped for similar problems in the 1880s, is now known as { the\nhalting problem}.",
    "1714": "The halting problem is easy to state and easy to prove undecidable.\nThe problem is this: given a Turing machine and an input to the Turing\nmachine, does the Turing machine finish computing in a finite number\nof steps (a finite amount of time)?  In order to solve the problem, an\nanswer, either yes or no, must be given in a finite amount of time\nregardless of the machine or input in question.  Clearly some machines\nnever finish.  For example, we can write a Turing machine that counts\nupwards starting from one.",
    "1715": "You may find the proof structure for undecidability of the halting problem\neasier to understand if\nyou first think about a related problem with which you may\nalready be familiar, the Liar's paradox\n(which is at least 2,300 years old).  In its stengthened form, it is\nthe following sentence: ``This sentence is not true.''",
    "1716": "To see that no Turing machine can solve the halting problem, we begin\nby assuming that such a machine exists, and then show that its\nexistence is self-contradictory.  We call the machine the ``Halting\nMachine,'' or HM for short.  HM is a machine that operates on \nanother",
    "1717": "Turing machine and its inputs to produce a yes or no answer in finite time:\neither the machine in question finishes in finite time (HM returns\n``yes''), or it does not (HM returns ``no'').  The figure illustrates\nHM's operation.",
    "1718": "From HM, we construct a second machine that we call the HM Inverter,\nor HMI.  This machine inverts the sense of the answer given by HM.  In\nparticular, the inputs are fed directly into a copy of HM, and if HM\nanswers ``yes,'' HMI enters an infinite loop.  If HM answers ``no,'' HMI\nhalts.  A diagram appears to the right.",
    "1719": "The inconsistency can now be seen by asking HM whether HMI halts when\ngiven itself as an input (repeatedly), as",
    "1720": "shown below.  Two\ncopies of HM are thus\nbeing asked the same question.  One copy is the rightmost in the figure below\nand the second is embedded in the HMI machine that we are using as the\ninput to the rightmost HM.  As the two copies of HM operate on the same input\n(HMI operating on HMI), they should return the same answer: a Turing\nmachine either halts on an input, or it does not; they are\ndeterministic.",
    "1721": "Let's assume that the rightmost HM tells us that HMI operating on itself halts.\nThen the copy of HM in HMI (when HMI executes on itself, with itself\nas an input) must also say ``yes.''  But this answer implies that HMI\ndoesn't halt (see the figure above), so the answer should have been\nno!",
    "1722": "Alternatively, we can assume that the rightmost HM says that HMI operating on itself\ndoes not halt.  Again, the copy of HM in HMI must give the same\nanswer.  But in this case HMI halts, again contradicting our\nassumption.",
    "1723": "Since neither answer is consistent, no consistent answer can be given,\nand the original assumption that HM exists is incorrect.  Thus, no\nTuring machine can solve the halting problem.",
    "1724": "This set of notes introduces the C programming language and explains\nsome basic concepts in computer programming.  Our purpose in showing\nyou a high-level language at this early stage of the course is to give\nyou time to become familiar with the syntax and meaning of the language,\nnot to teach you how to program.  Throughout this semester, we will\nuse software written in C to demonstrate and validate the digital system design\nmaterial in our course.  Towards the end of the semester, you will\nlearn to program computers using instructions and assembly language.\nIn ECE 220, you will make use of the C language to write\nprograms, at which point already being familiar with the language will\nmake the material easier to master.",
    "1725": "These notes are meant to complement the\nintroduction provided by Patt and Patel.",
    "1726": "After a brief introduction to the history of C and the structure of\na program written in C, we connect the idea of representations developed \nin class to the data types used in high-level languages.",
    "1727": "We next discuss the use of variables in C, then describe some of the \noperators available to the programmer, including arithmetic and logic\noperators.  The notes next introduce C functions that support the ability to\nread user input from the keyboard and to print results to the monitor.",
    "1728": "A description of the structure of statements in C follows, explaining\nhow programs are executed and how a programmer can create statements\nfor conditional execution as well as loops to perform repetitive tasks.",
    "1729": "The main portion of the notes concludes with \nan example program, which is used to illustrate both the execution of \nC statements as well as the difference between variables in programs\nand variables in algebra.",
    "1730": "The remainder of the notes covers more advanced topics.  First, we \ndescribe how the compilation process works,\nillustrating how a program written in a high-level language is\ntransformed into instructions.  You will learn this process in much\nmore detail in ECE 220.",
    "1731": "Second, we briefly introduce the C preprocessor.",
    "1732": "Finally, we discuss implicit and explicit data type conversion in C.",
    "1733": "{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}",
    "1734": "Programming languages attempt to bridge the semantic gap between human\ndescriptions of problems and the relatively simple instructions that\ncan be provided by an instruction set architecture (ISA).  \nSince 1954, when the Fortran language\nfirst enabled scientists to enter FORmulae symbolically and to have\nthem TRANslated automatically into instructions, people have invented\nthousands of computer languages.",
    "1735": "The C programming language was developed by Dennis Ritchie at Bell Labs\nin order to simplify the task of writing the Unix operating system.\nThe C language provides a fairly transparent mapping to typical ISAs,\nwhich makes it a good choice both for system software such as operating\nsystems and for our class.",
    "1736": "The { syntax} used in C---that is, the rules that one must follow\nto write valid C programs---has also heavily influenced many other\nmore recent languages, such as C++, Java, and Perl.",
    "1737": "For our purposes, a C program consists of a set of { variable \ndeclarations} and a sequence of { statements}.",
    "1738": "{",
    "1739": "aaaa=/* =\nint\nmain ()\n{\n>  int answer = 42;        /* the Answer! */",
    "1740": ">  printf (\"The answer is d.n\", answer);",
    "1741": ">  /*> Our work here is done.\n>    > Let's get out of here! */\n>  return 0;\n}",
    "1742": "}",
    "1743": "Both of these parts are written into a single C function called { main},\nwhich executes when the program starts.",
    "1744": "A simple example appears to the right.  The program uses one variable\ncalled { answer}, which it initializes to the value 42.\nThe program prints a line of output to the monitor for the user,\nthen terminates using the { return} statement.  { Comments} for human\nreaders begin with the characters { /*} (a slash followed by an \nasterisk) and end with the characters { */} (an asterisk followed \nby a slash).",
    "1745": "The C language ignores white space in programs, so we encourage\nyou to use blank lines and extra spacing to make your programs\neasier to read.",
    "1746": "The variables defined in the { main} function allow a programmer\nto associate arbitrary { symbolic names} (sequences of English characters, \nsuch as ``sum'' or ``product'' or ``highScore'') with specific\ntypes of data, such as a {16-bit} unsigned integer or a\ndouble-precision floating-point number.",
    "1747": "In the example program above, the variable { answer} is declared\nto be a {32-bit} {2's} complement number.",
    "1748": "Those with no programming experience may at first find the difference\nbetween variables in algebra and variables in programs slightly \nconfusing.  { As a program executes, the values of variables can \nchange from step to step of execution.}",
    "1749": "The statements in the { main} function are executed one by one\nuntil the program terminates.",
    "1750": "Programs are not limited to simple sequences of statements, however.\nSome types of statements allow a programmer\nto specify conditional behavior.  For example, a program might only\nprint out secret information if the user's name is ``lUmeTTa.''\nOther types of statements allow a programmer to repeat the execution\nof a group of statements until a condition is met.  For example, a program\nmight print the numbers from 1 to 10, or ask for input until the user\ntypes a number between 1 and 10.",
    "1751": "The order of statement execution is well-defined in C, but the\nstatements in { main} do not necessarily make up an algorithm:\n{ we can easily write a C program that never terminates}.",
    "1752": "If a program terminates, the { main} function\nreturns an integer to the operating system, usually by executing\na { return} statement, as in the example program.",
    "1753": "By convention, returning the value 0 indicates successful completion\nof the program, while any non-zero value indicates a program-specific\nerror.",
    "1754": "However, { main} is not necessarily a function in the mathematical \nsense because { the value returned from { main} is not \nnecessarily unique for a given set of input values to the program}.",
    "1755": "For example, we can write a program that selects a number from 1 to 10 \nat random and returns the number to the operating system.",
    "1756": "As you know, modern digital computers represent all information with\nbinary digits (0s and 1s), or { bits}.  Whether you are representing \nsomething as simple as an integer or as complex as an undergraduate \nthesis, the data are simply a bunch of 0s and 1s inside a computer.",
    "1757": "For any given type of information, a human selects a data type for the\ninformation.  A { data type} (often called just a { type})\nconsists of both a size in bits and a representation, such as the\n2's complement representation for signed integers, or the ASCII\nrepresentation for English text.  A { representation} is a way of\nencoding the things being represented as a set of bits, with each bit\npattern corresponding to a unique object or thing.",
    "1758": "A typical ISA supports a handful of\ndata types in hardware in the sense that it provides hardware \nsupport for operations on those data types.",
    "1759": "The arithmetic logic units (ALUs) in most modern processors,\nfor example, support addition\nand subtraction of both unsigned and 2's complement representations, with\nthe specific data type (such as 16- or 64-bit 2's complement)\ndepending on the ISA.",
    "1760": "Data types and operations not supported by the ISA must be handled in\nsoftware using a small set of primitive operations, which form the\n{ instructions} available in the ISA.  Instructions usually\ninclude data movement instructions such as loads and stores\nand control instructions such as branches and subroutine calls in\naddition to arithmetic and logic operations.",
    "1761": "The last quarter of our class covers these concepts in more detail\nand explores their meaning using an example ISA from the textbook.",
    "1762": "In class, we emphasized the idea that digital systems such as computers\ndo not interpret the meaning of bits.  Rather, they do exactly what\nthey have been designed to do, even if that design is meaningless.",
    "1763": "If, for example, you store\na sequence of ASCII characters \nin a computer's memory as \nand\nthen write computer instructions to add consecutive groups of four characters\nas 2's complement integers and to print the result to the screen, the\ncomputer will not complain about the fact that your code produces\nmeaningless garbage.",
    "1764": "In contrast, high-level languages typically require that a programmer\nassociate a data type with each datum in order to reduce the chance \nthat the bits \nmaking up an individual datum are misused or misinterpreted accidentally.",
    "1765": "Attempts to interpret a set of bits differently usually generate at least\na warning message, since",
    "1766": "such re-interpretations of the\nbits are rarely intentional and thus rarely correct.  A compiler---a\nprogram that transforms code written in a high-level language into\ninstructions---can also generate the proper type conversion instructions \nautomatically when the \ntransformations are intentional, as is often the case with arithmetic.",
    "1767": "Some high-level languages, such as Java, \nprevent programmers from changing the type of a given datum.\nIf you define a type that represents one of your\nfavorite twenty colors, for example, you are not allowed to turn a\ncolor into an integer, despite the fact that the color is represented\nas a handful of bits.  Such languages are said to be { strongly\ntyped}.",
    "1768": "The C language is not strongly typed, and programmers are free to\ninterpret any bits in any manner they see fit.  Taking advantage of\nthis ability in any but a few exceptional cases, however, \nresults in arcane and non-portable code, and is thus considered to be\nbad programming practice.  We discuss conversion between types in more\ndetail later in these notes.",
    "1769": "Each high-level language defines a number of { primitive data\ntypes}, which are always available.  Most languages, including C,\nalso provide ways of defining new types in terms of primitive types,\nbut we leave that part of C for ECE 220.",
    "1770": "The primitive data types in C include signed and unsigned integers of various\nsizes as well as single- and double-precision IEEE floating-point numbers.",
    "1771": "The primitive integer types in C include both unsigned and 2's\ncomplement representations.  These types were originally defined so as\nto give reasonable performance when code was ported.  In particular,\nthe { int} type is intended to be the native integer type for the\ntarget ISA.  Using data types supported directly in hardware is faster \nthan using larger or smaller integer types.  When C was standardized in 1989,\nthese types were defined so as to include a range of existing\nC compilers rather than requiring all compilers to produce uniform\nresults.  At the",
    "1772": "{-8pt}{|r|l|l|}\n& {|c|}{2's complement}& {|c|}\n8 bits& { char}& { unsigned char} \n16 bits& { short}& { unsigned short}\n& { short int}& { unsigned short int} \n32 bits& { int}& { unsigned}\n&& { unsigned int} \n32 or & { long}& { unsigned long}\n64 bits& { long int}& { unsigned long int} \n64 bits& { long long}& { unsigned long long}\n& { long long int}& { unsigned long long int}",
    "1773": "{-14pt}",
    "1774": "time, most workstations and mainframes were 32-bit machines, while\nmost personal computers were 16-bit machines, thus flexibility was somewhat\ndesirable.  For the GCC compiler on Linux, the C integer data \ntypes are defined\nin the table above.  Although the { int} and { long}\ntypes are usually the same, there is a semantic difference in common\nusage.  In particular, on most architectures and most compilers, a\n{ long} has enough bits to identify a location in the computer's\nmemory, while an { int} may not.",
    "1775": "When in doubt, the { size in bytes} of any type or variable can be\nfound using the built-in C function { sizeof}.",
    "1776": "Over time, the flexibility of size in C types has become less\nimportant (except for the embedded markets, where one often wants even\nmore accurate bit-width control), and the fact that the size of an\n{ int} can vary from machine to machine and compiler to compiler\nhas become more a source of headaches than a helpful feature.  In the\nlate 1990s, a new set of fixed-size types were recommended for\ninclusion",
    "1777": "{-8pt}{|r|l|l|}\n& {|c|}{2's complement}& {|c|}\n 8 bits& {  int8_t}& {  uint8_t}\n16 bits& { int16_t}& { uint16_t}\n32 bits& { int32_t}& { uint32_t}\n64 bits& { int64_t}& { uint64_t}",
    "1778": "{-14pt}",
    "1779": "in the C library, reflecting the fact that many companies\nhad already developed and were using such definitions to make their\nprograms platform-independent.",
    "1780": "We encourage you to make use of these types, which are shown in \nthe table above.  In Linux, they can be made available by including \nthe { stdint.h} header file.",
    "1781": "Floating-point types in C include { float} and { double},\nwhich correspond respectively to single- and double-precision IEEE\nfloating-point values.  Although the {32-bit} { float} type\ncan save memory compared with use of {64-bit} { double}\nvalues, C's math library works with double-precision values, and\nsingle-precision data are uncommon in scientific and engineering\ncodes.  In contrast, single-precision floating-point operations\ndominated the\ngraphics industry until recently, and are still well-supported even\non today's graphics processing units.",
    "1782": "The function { main} executed by a program begins with a list\nof { variable declarations}.  Each declaration consists of two parts:\na data type specification and a comma-separated list of variable names.\nEach variable declared can also \nbe { initialized} by assigning an initial value.  A few examples \nappear below.  Notice that one can initialize a variable to have the same\nvalue as a second variable.",
    "1783": "{",
    "1784": "aaaaaaa=aa=aaaaaaaaaaaaaaaaaaaaa=/=* a third 2's complement variable with unknown initial value =\nint > x > = 42; >/>* a 2's complement variable, initially equal to 42 > */\nint > y > = x;  >/>* a second 2's complement variable, initially equal to x > */\nint > z;>       >/>* a third 2's complement variable with unknown initial value > */\ndouble> a, b, c, pi = 3.1416; > >/>*\n>>>>* four double-precision IEEE floating-point variables\n>>>>* a, b, and c are initially of unknown value, while pi is\n>>>>* initially 3.1416\n>>>>*/",
    "1785": "}",
    "1786": "What happens if a programmer declares a variable but does not \ninitialize it?",
    "1787": "Remember that bits can only be 0 or 1.",
    "1788": "An uninitialized variable does have a value, but its value is unpredictable.",
    "1789": "The compiler tries to detect uses of uninitialized variables, but sometimes\nit fails to do so, so { until you are more familiar with programming,\nyou should always initialize every variable}.",
    "1790": "Variable names, also called { identifiers}, can include both letters\nand digits in C.",
    "1791": "Good programming style requires that programmers select variable names\nthat are meaningful and are easy to distinguish from one another.",
    "1792": "Single letters are acceptable in some situations, but longer names with\nmeaning are likely to help people (including you!) understand your \nprogram.",
    "1793": "Variable names are also case-sensitive in C, which allows programmers\nto use capitalization to differentiate behavior and meaning, if desired.\nSome programs, for example, use identifiers with all capital letters\nto indicate variables with values that remain constant for the program's\nentire execution.",
    "1794": "However, the fact that identifiers are case-sensitive also means \nthat a programmer can declare distinct variables \nnamed { variable}, { Variable}, { vaRIable}, { vaRIabLe}, \nand { VARIABLE}.  We strongly discourage you from doing so.",
    "1795": "The { main} function also contains a sequence of statements.",
    "1796": "A statement is a complete specification of a single step\nin the program's execution.",
    "1797": "We explain the structure of\nstatements in the next section.",
    "1798": "Many statements in C include one or more { expressions},\nwhich represent calculations such as arithmetic, comparisons,\nand logic operations.",
    "1799": "Each expression is in turn composed of { operators} and { operands}.",
    "1800": "Here we give only a brief introduction to some of the operators available\nin the C language.  We deliberately omit operators with more\ncomplicated meanings, as well as operators for which the original\npurpose was to make writing common operations a little shorter.",
    "1801": "For the interested reader, both the textbook and ECE 220 give more \ndetailed introductions.",
    "1802": "The table to the right gives examples for the operators described \nhere.",
    "1803": "{",
    "1804": "{int i = 42, j = 1000;}\n{/* i = 0x0000002A, j = 0x000003E8 */}",
    "1805": "i + j &  1042\ni - 4 * j &  -3958\n-j &  -1000\nj / i &  23\nj  i &   42\ni & j &   40& /* 0x00000028 */\ni | j &   1002& /* 0x000003EA */\ni  j &   962& /* 0x000003C2 */\ni &   -43& /* 0xFFFFFFD5 */\n(i) >> 2 &   -11& /* 0xFFFFFFF5 */\n((i) >> 4) &   2& /* 0x00000002 */\nj >> 4 &  62& /* 0x0000003E */ \nj << 3 &  8000& /* 0x00001F40 */ \ni > j &   0\ni <= j &   1\ni == j &   0\nj = i &   42& /* ...and j is changed! */",
    "1806": "}",
    "1807": "{ Arithmetic operators} in C include addition ({ +}), \nsubtraction ({ -}), negation (a minus sign not \npreceded by another expression), multiplication ({ *}), \ndivision ({ /}), and modulus ({ }).  No exponentiation\noperator exists; instead, library routines are defined for this purpose\nas well as for a range of more complex mathematical functions.",
    "1808": "C also supports { bitwise operations} on integer types, including \nAND ({ &}), OR ({ |}), XOR ({ ^{ }}), NOT ({ }), \nand left ({ <<}) and right ({ >>}) bit shifts.\nRight shifting a signed integer results in an { arithmetic right shift}\n(the sign bit is copied), while right shifting an unsigned integer\nresults in a { logical right shift} (0 bits are inserted).",
    "1809": "A range of { relational} or { comparison operators} are \navailable, including equality ({ ==}),\ninequality ({ !=}), and relative order ({ <}, { <=},\n{ >=}, and { >}).",
    "1810": "All such operations evaluate to 1 to indicate a true relation\nand 0 to indicate a false relation.  Any non-zero value is considered\nto be true for the purposes of tests (for example, in an { if} statement\nor a { while} loop) in C---these statements are explained later in \nthese notes.",
    "1811": "{ Assignment} of a new value to a variable \nuses a single equal sign ({ =}) in C.",
    "1812": "For example, the expression { A = B} copies\nthe value of variable { B} into variable { A}, overwriting the\nbits representing the previous value of { A}.",
    "1813": "{ The use of two equal signs for an equality check and a single\nequal sign for assignment is a common source of errors,} although\nmodern compilers generally detect and warn about this type of mistake.",
    "1814": "Assignment in C does not solve equations, even simple equations.  \nWriting ``{ A-4=B}'', for example, generates a compiler error.\nYou must solve such equations yourself to calculate the desired\nnew value of a single variable, such as ``{  A=B+4}.''\nFor the purposes of our class, you must always write a single variable \non the left side of an assignment, and can write an arbitrary expression \non the right side.",
    "1815": "Many operators can be combined into a single expression.  When an\nexpression has more than one operator, which operator is executed first?\nThe answer depends on the operators' { precedence}, a well-defined order on\noperators that specifies how to resolve the ambiguity.  In the case\nof arithmetic, the C language's precedence specification matches the\none that you learned in elementary school.  For example, { 1+2*3}\nevaluates to 7, not to 9, because multiplication has precedence over\naddition.  For non-arithmetic operators, or for any case in which\nyou do not know the precedence specification for a language, {\ndo not look it up---other programmers will not remember the\nprecedence ordering, either!}  Instead, add parentheses to make your \nexpressions clear and easy to understand.",
    "1816": "The { main} function returns an integer to the operating system.\nAlthough we do not discuss how additional functions can be written\nin our class, we may sometimes make use of functions that have been\nwritten in advance by making { calls} to those functions.\nA { function call} is type of expression in C, but we leave \nfurther description for ECE 220.  In our class, we make use of only\ntwo additional functions to enable our programs to receive input\nfrom a user via the keyboard and to write output to the monitor for \na user to read.",
    "1817": "Let's start with output.  The { printf} function allows a program\nto print output to the monitor using a programmer-specific format.\nThe ``f'' in { printf} stands for ``formatted.''{The \noriginal, unformatted variant of printing was never available\nin the C language.  Go learn Fortran.}",
    "1818": "When we want to use { printf}, we write a expression with\nthe word { printf} followed by a parenthesized, comma-separated\nlist of expressions.  The expressions in this list are called\nthe { arguments} to the { printf} function.",
    "1819": "The first argument to the { printf} function is a format string---a \nsequence of ASCII characters between quotation marks---which tells \nthe function what kind of information we want printed to\nthe monitor as well as how to format that information.",
    "1820": "The remaining arguments are C expressions that give { printf}\na copy of any values that we want printed.",
    "1821": "How does the format string specify the format?",
    "1822": "Most of the characters in the format string are simply printed to \nthe monitor.",
    "1823": "In the first example shown to on the next page, we use { printf}\nto print a hello message followed by an ASCII newline character\nto move to the next line on the monitor.",
    "1824": "The percent sign---``''---is used \nas an { escape character} in the\n{ printf} function.  When ``'' appears in the format\nstring, the function examines the next character in the format string\nto determine which format to use, then takes\nthe next expression from the sequence\nof arguments and prints the value of that expression to the \nmonitor.  Evaluating an expression generates a bunch of bits, so it is up to\nthe programmer to ensure that those bits are not misinterpreted.\nIn other words, the programmer must make sure that the number and\ntypes of formatted values match the number and types of arguments passed\nto { printf} (not counting the format string itself).",
    "1825": "The { printf} function returns the number of characters printed\nto the monitor.",
    "1826": "output: =\n> { printf (\"Hello, world!n\");}\noutput: > { Hello, world!} [and a newline]",
    "1827": "> { printf (\"To x or not to d...n\", 190, 380 / 2);}\noutput: > { To be or not to 190...} [and a newline]",
    "1828": "> { printf (\"My favorite number is cc.n\", 0x34, '0'+2);}\noutput: > { My favorite number is 42.} [and a newline]",
    "1829": "> { printf (\"What is pi?  f or e?n\", 3.1416, 3.1416);}\noutput: > { What is pi?  3.141600 or 3.141600e+00?} [and a newline]",
    "1830": "{|c|l|}\nescape  &                         \nsequence& { printf} function's interpretation of expression bits \n{ c}& 2's complement integer printed as an ASCII character\n{ d}& 2's complement integer printed as decimal\n{ e}& double printed in decimal scientific notation\n{ f}& double printed in decimal\n{ u}& unsigned integer printed as decimal\n{ x}& integer printed as hexadecimal (lower case)\n{ X}& integer printed as hexadecimal (upper case)\n{ }& a single percent sign",
    "1831": "A program can read input from the user with the { scanf} function.\nThe user enters characters in ASCII using the keyboard, and the\n{ scanf} function converts the user's input into C primitive types,\nstoring the results into variables.  As with { printf}, the\n{ scanf} function takes a format string followed by a comma-separated\nlist of arguments.  Each argument after the format string provides\n{ scanf} with the memory address of a variable into which the\nfunction can store a result.",
    "1832": "How does { scanf} use the format string?",
    "1833": "For { scanf}, the format string is usually just a sequence\nof conversions, one for each variable to be typed in by the user.\nAs with { printf}, the conversions start with ``'' and\nare followed by characters specifying the type of conversion\nto be performed.  The first example shown to the right reads\ntwo integers.",
    "1834": "The conversions in the format string can be separated by spaces \nfor readability, as shown in the exam-",
    "1835": "effect: =unsigned =\n> { int     } > { a, b;  /* example variables */}\n> { char    } > { c;}\n> { unsigned} > { u;}\n> { double  } > { d;}\n> { float   } > { f;}",
    "1836": "> { scanf (\"dd\", &a, &b);   /* These have the */}\n> { scanf (\"d d\", &a, &b);  /* same effect.   */}\neffect: > try to convert two integers typed in decimal to\n> 2's complement and store the results in { a} and { b}",
    "1837": "> { scanf (\"cx lf\", &c, &u, &d);}\neffect: > try to read an ASCII character into { c}, a value\n> typed in hexadecimal into { u}, and a double-\n> precision > floating-point number into { d}",
    "1838": "> { scanf (\"lf f\", &d, &f);}\neffect: > try to read two real numbers typed as decimal,\n> convert the first to double-precision and store it \n> in { d}, and convert the second to single-precision \n> and store it in { f}",
    "1839": "ple.  The spaces are ignored\nby { scanf}.  However, { any non-space characters in the\nformat string must be typed exactly by the user!}",
    "1840": "The remaining arguments to { scanf} specify memory addresses\nwhere the function can store the converted values.",
    "1841": "The ampersand (``&'') in front of each variable name in the examples is an\noperator that returns the address of a variable in memory.",
    "1842": "For each con-",
    "1843": "{|c|l|}\nescape  &                         \nsequence& { scanf} function's conversion to bits \n{ c}& store one ASCII character (as { char})\n{ d}& convert decimal integer to 2's complement\n{ f}& convert decimal real number to float\n{ lf}& convert decimal real number to double\n{ u}& convert decimal integer to unsigned int\n{ x}& convert hexadecimal integer to unsigned int\n{ X}& (as above)",
    "1844": "version\nin the format string, the { scanf} function tries to convert\ninput from the user into the appropriate result, then stores the\nresult in memory at the address given by the next argument.",
    "1845": "The programmer is responsible for ensuring that the number of \nconversions in the format string\nmatches the number of arguments provided (not counting\nthe format string itself).  The programmer must also ensure that\nthe type of information produced by each conversion can be\nstored at the address passed for that conversion---in other words,\nthe address of a\nvariable with the correct type must be\nprovided.  Modern compilers often detect missing { &} operators\nand incorrect variable types, but many only give warnings to the\nprogrammer.  The { scanf} function itself cannot tell whether\nthe arguments given to it are valid or not.",
    "1846": "If a conversion fails---for example, if a user types ``hello'' when\n{ scanf} expects an integer---{ scanf} does not overwrite the\ncorresponding variable and immediately stops trying to convert input.",
    "1847": "The { scanf} function returns the number of successful \nconversions, allowing a programmer to check for bad input from\nthe user.",
    "1848": "Each statement in a C program specifies a complete operation.",
    "1849": "There are three types of statements, but two of these types can\nbe constructed from additional statements, which can in turn be\nconstructed from additional statements.  The C language specifies\nno bound on this type of recursive construction, but code \nreadability does impose a practical limit.",
    "1850": "The three types are shown to the right.\nThey are the { null statement}, \n{ simple statements}, \nand { compound statements}.",
    "1851": "A null statement is just a semicolon, and a compound statement \nis just a sequence of statements surrounded by braces.",
    "1852": "Simple statements can take several forms.  All of the examples\nshown to the right, including the call to { printf}, are\nsimple state-",
    "1853": "{",
    "1854": "aaaa=aaaaaaaaaaaaaa=/* =aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa=\n;   > > /* >a null statement (does nothing) >*/",
    "1855": "A = B; > > /*  >examples of simple statements >*/\nprintf (\"Hello, world!n\");",
    "1856": "{    > > /* > a compound statement >*/ \n>  C = D; > /* > (a sequence of statements >*/\n>  N = 4; > /* > between braces) >*/ \n>  L = D - N;\n}",
    "1857": "}\n{-2pt}",
    "1858": "ments consisting of a C expression followed by a \nsemicolon.",
    "1859": "Simple statements can also consist of conditionals or iterations, which\nwe introduce next.",
    "1860": "Remember that after variable declarations, the { main} function\ncontains a sequence of statements.  These statements are executed one\nat a time in the order given in the program, as shown to the right\nfor two statements.  We say that the statements are executed in\nsequential order.",
    "1861": "A program must also be able to execute statements only when \nsome condition holds.  In the C language, such a condition can be\nan arbitrary expression.  The expression is first evaluated.\nIf the result is 0, the condition\nis considered to be false.  Any result other than 0 is considered\nto be true.  The C statement\nfor conditional execution is called an { if}",
    "1862": "{file=part1/figs/part1-sd-sequential.eps,width=0.8in}",
    "1863": "statement.  Syntactically, we put the expression for the condition\nin parentheses after the keyword { if} and follow the parenthesized\nexpression with a compound statement containing the statements\nthat should be executed when the condition is true.  Optionally,\nwe can append the keyword { else} and a second compound\nstatement containing statements to be executed when the condition\nevaluates to false.  \nThe corresponding flow chart is shown to the right.",
    "1864": "{",
    "1865": "aaaa=aaaaaaaaaaaa=\n/* Set the variable y to the absolute value of variable x. */\nif (0 <= x) {> >/* Is x greater or equal to 0? */\n> y = x;     >/* Then block: assign x to y. */\n} else {\n> y = -x;    >/* Else block: assign negative x to y. */\n}",
    "1866": "}",
    "1867": "{file=part1/figs/part1-sd-conditional.eps,width=2in}",
    "1868": "If instead we chose to assign the absolute value of variable { x}\nto itself, we can do so without an { else} block:",
    "1869": "{",
    "1870": "aaaa=aaaaaaaaaaaa=\n/* Set the variable x to its absolute value. */\nif (0 > x) {> >/* Is x less than 0? */\n> x = -x;    >/* Then block: assign negative x to x. */\n}            >> /* No else block is given--no work is needed. */",
    "1871": "}",
    "1872": "Finally, we sometimes need to repeatedly execute a set of statements,\neither a fixed number of times or so long as some condition holds.\nWe refer to such repetition as an { iteration} or a { loop}.\nIn our class, we make use of C's { for} loop when we need to\nperform such a task.  A { for} loop is structured as follows:",
    "1873": "{",
    "1874": "for ([initialization] ; [condition] ; [update]) {\n    [subtask to be repeated]\n}",
    "1875": "}",
    "1876": "A flow chart corresponding to execution of a { for} loop appears\nto the right.  First, any initialization is performed.  Then the\ncondition---again an arbitrary C expression---is checked.  If the\ncondition evaluates to false (exactly 0), the loop is done.  Otherwise,\nif the condition evaluates to true (any non-zero value),\nthe statements in the compound statement, the subtask or { loop body},\nare executed.  The loop body can contain anything: a sequence of simple \nstatements, a conditional, another loop, or even just an empty list.\nOnce the loop body has finished executing, the { for} loop\nupdate rule is executed.  Execution then checks the condition again,\nand this process repeats until the condition evaluates to 0.\nThe { for} loop below, for example, prints the numbers \nfrom 1 to 42.",
    "1877": "{",
    "1878": "/* Print the numbers from 1 to 42. */\nfor (i = 1; 42 >= i; i = i + 1) {\n    printf (\"dn\", i);\n}",
    "1879": "}",
    "1880": "{file=part1/figs/part1-sd-iterative.eps,width=1.35in}",
    "1881": "We are now ready to consider the execution of a simple program,\nillustrating how variables change value from step to step and\ndetermine program behavior.",
    "1882": "Let's say that two numbers are ``friends'' if they have at least one\n1 bit in common when written in base 2.  So, for example, 100_2 and \n111_2 are friends because both numbers have a 1 in the bit with \nplace value 2^2=4.  Similarly, 101_2 and 010_2 are not friends,\nsince no bit position is 1 in both numbers.",
    "1883": "The program to the right prints all friendships between numbers\nin the interval [0,7].",
    "1884": "{",
    "1885": "aaaa=aaaa=aaaaaaaaaa=/* a second number to consider as check's friend =\nint\nmain ()\n{\n>  int > check;  > /* number to check for friends > */\n>  int > friend; > /* a second number to consider as check's friend > */\naaaa=aaaa=aaaa=aaaa=\n>  \n>  /* Consider values of check from 0 to 7. */\n>  for (check = 0; 8 > check; check = check + 1) {",
    "1886": ">  >  /* Consider values of friend from 0 to 7. */\n>  >  for (friend = 0; 8 > friend; friend = friend + 1) {",
    "1887": ">  >  >  /* Use bitwise AND to see if the two share a 1 bit. */\n>  >  >  if (0 != (check & friend)) {",
    "1888": ">  >  >  >  /* We have friendship! */\n>  >  >  >  printf (\"d and d are friends.n\", check, friend);\n>  >  >  }\n>  >  }\n>  }\n}",
    "1889": "}",
    "1890": "The program uses two\ninteger variables, one for each of the numbers that we consider.\nWe use a { for} loop to iterate over all values of our first\nnumber, which we call { check}.  The loop initializes { check}\nto 0, continues until check reaches 8, and adds 1 to check after\neach loop iteration.  We use a similar { for} loop to iterate\nover all possible values of our second number, which we call { friend}.\nFor each pair of numbers, we determine whether they are friends\nusing a bitwise AND operation.  If the result is non-zero, they\nare friends, and we print a message.  If the two numbers are not\nfriends, we do nothing, and the program moves on to consider the\nnext pair of numbers.",
    "1891": "Now let's think about what happens when this program executes.",
    "1892": "When the program starts, both variables are filled with random bits,\nso their values are unpredictable.",
    "1893": "The first step is the initialization of the first { for} loop, which\nsets { check} to 0.",
    "1894": "The condition for that loop is { 8 > check}, which is true, so\nexecution enters the loop body and starts to execute the first\nstatement, which is our second { for} loop.",
    "1895": "The next step is then the initialization code for the second { for}\nloop, which sets { friend} to 0.",
    "1896": "The condition for the second loop is { 8 > friend}, which is true, so\nexecution enters the loop body and starts to execute the first\nstatement, which",
    "1897": "after executing...& { check} is...& and { friend} is... \n(variable declarations)& unpredictable bits& unpredictable bits\n{ check = 0}& 0& unpredictable bits\n{ 8 > check}& 0& unpredictable bits\n{ friend = 0}& 0& 0\n{ 8 > friend}& 0& 0\n{ if (0 != (check & friend))}& 0& 0\n{ friend = friend + 1}& 0& 1\n{ 8 > friend}& 0& 1\n{ if (0 != (check & friend))}& 0& 1\n{ friend = friend + 1}& 0& 2\n{(repeat last three lines six more times; number 0 has no friends!)}\n{ 8 > friend}& 0& 8\n{ check = check + 1}& 1& 8\n{ 8 > check}& 1& 8\n{ friend = 0}& 1& 0\n{ 8 > friend}& 1& 0\n{ if (0 != (check & friend))}& 1& 0\n{ friend = friend + 1}& 1& 1\n{ 8 > friend}& 1& 1\n{ if (0 != (check & friend))}& 1& 1\n{ printf ...}& 1& 1\n{(our first friend!?)}",
    "1898": "is the { if} statement.",
    "1899": "Since both variables are 0, the { if} condition is false, and\nnothing is printed.",
    "1900": "Having finished the loop body for the inner loop (on { friend}),\nexecution continues with the update rule for that loop---{ friend = \nfriend + 1}---then returns to check the loop's condition again.",
    "1901": "This process repeats, always finding that the number 0 (in { check})\nis not\nfriends (0 has no friends!) until { friend} reaches 8, at which\npoint the inner loop condition becomes false.",
    "1902": "Execution then moves to the update rule for the first { for} loop,\nwhich increments { check}.  Check is then compared with 8 to\nsee if the loop is done.  Since it is not, we once again enter the\nloop body and start the second { for} loop over.  The initialization\ncode again sets { friend} to 0, and we move forward as before.\nAs you see above, the first time that we find our { if} condition\nto be true is when both { check} and { friend} are equal to 1.",
    "1903": "Is that result what you expected?  To learn that the number 1 is\nfriends with itself?  If so, the program works.  If you assumed that\nnumbers could not be friends with themselves, perhaps we should fix the \nbug?  We could, for example, add another { if} statement to \navoid printing anything when { check == friend}.",
    "1904": "Our program, you might also realize, prints each pair of friends twice.\nThe numbers 1 and 3, for example, are printed in both possible orders.  To\neliminate this redundancy, we can change the initialization in the \nsecond { for} loop, either to { friend = check} or to\n{ friend = check + 1}, depending on how we want to define friendship\n(the same question as before: can a number be friends with itself?).",
    "1905": "Many programming languages, including C, can be \n{ compiled}, which means that the program is converted into \ninstructions for a particular ISA before the program is run\non a processor that supports that ISA.\nThe figure to the right illustrates the compilation process for \nthe C language.",
    "1906": "In this type of figure, files and other data are represented as cylinders,\nwhile rectangles represent processes, which are usually implemented in \nsoftware.",
    "1907": "In the figure to the right, the outer dotted box represents the full \ncompilation\nprocess that typically occurs when one compiles a C program.\nThe inner dotted box represents the work performed by the { compiler}\nsoftware itself.",
    "1908": "The cylinders for data passed between the processes that compose\nthe full compilation process\nhave been left out of the figure; instead, we have written the type\nof data being passed next to the arrows that indicate the flow of information\nfrom one process to the next.",
    "1909": "The C preprocessor (described later in these notes) forms the\nfirst step in the compilation process.  The preprocessor\noperates on the program's { source code} along\nwith { header files} that describe data types and\noperations.  The preprocessor merges these together\ninto a single file of preprocessed source code.  The preprocessed\nsource code is then analyzed by the front end of the compiler based on the\nspecific programming language being used (in our case, the C language),\nthen converted by the back end of the compiler\ninto instructions for the desired ISA.  The output of a compiler\nis not binary instructions, however, but is instead\na human-readable form of instructions called { assembly code},\nwhich we cover in the last quarter of our class.  A tool called\nan assembler then converts these human-readable instructions into\nbits that a processor can understand.  If a program consists of\nmulti-",
    "1910": "{file=part1/figs/part1-compiler.eps,width=3in}",
    "1911": "ple source files, or needs to make use of additional \npre-programmed operations (such as math functions, graphics, or sound),\na tool called a linker merges the object code of the program with\nthose additional elements to form the final { executable image}\nfor the program.  The executable image is typically then stored on\na disk, from which it can later be read into memory in order to\nallow a processor to execute the program.",
    "1912": "Some languages are difficult or even impossible to compile.  Typically, the\nbehavior of these languages depends on input data that are only available \nwhen the program runs.  Such languages can be { interpreted}: each step \nof the algorithm described by a program is executed by a software interpreter\nfor the language.  Languages such as Java, Perl, and Python are usually\ninterpreted.  Similarly, when we use software to simulate one ISA using\nanother ISA, as we do at the end of our class with the {LC-3}\nISA described by the textbook, the simulator is a form of interpreter.\nIn the lab, you will use a simulator compiled into and executing as x86 \ninstructions in order to interpret {LC-3} instructions.",
    "1913": "While a program is executing in an interpreter, enough information\nis sometimes available to compile part or all of the program to\nthe processor's ISA as the program runs, \na technique known as { ``just in time'' ( JIT) compilation}.",
    "1914": "The C language uses a preprocessor to support inclusion of common\ninformation (stored in header files) into multiple source files.",
    "1915": "The most frequent use of the preprocessor is to enable the unique\ndefinition of new data types and operations within\nheader files that can then be included by reference within source\nfiles that make use of them.  This capability is based on the \n{ include directive}, { #include}, as shown here:",
    "1916": "{",
    "1917": "#include \"my_header.h\"    = /* search in current followed by standard directories =\n#include <stdio.h>      > /* search in standard directories > */\n#include \"my_header.h\" > /* search in current followed by standard directories > */",
    "1918": "}",
    "1919": "The preprocessor also supports integration of compile-time constants\ninto source files before compilation.  For example, many\nsoftware systems allow the definition of a symbol such as { NDEBUG}\n(no debug) to compile without additional debugging code included in\nthe sources.",
    "1920": "Two directives are necessary for this purpose: the { define directive},\n{ #define}, which\nprovides a text-replacement facility, and { conditional inclusion} (or\nexclusion) of parts of a file within { #if}/{ #else}/{\n#endif} directives.",
    "1921": "These directives are also useful in allowing",
    "1922": "a single header file to\nbe included multiple times without causing problems, as C does not\nallow redefinition of types, variables, and so forth, even if the\nredundant \ndefinitions are identical.  Most header files are thus wrapped as shown\nto the right.",
    "1923": "{",
    "1924": "#if !defined(MY_HEADER_H)\n#define MY_HEADER_H\n/* actual header file material goes here */\n#endif /* MY_HEADER_H */",
    "1925": "}",
    "1926": "The preprocessor performs a simple linear pass on the source and does\nnot parse or interpret any C syntax.",
    "1927": "Definitions for text replacement are valid as soon as they are defined\nand are performed until they are undefined or until the end of the\noriginal source file.",
    "1928": "The preprocessor does recognize spacing and will not replace part of a\nword, thus ``{ #define i 5}'' will not wreak havoc on your {\nif} statements, but will cause problems if you name any variable { i}.",
    "1929": "Using the text replacement capabilities of the preprocessor does have\ndrawbacks, most importantly in that almost none of the information is\npassed on for debugging purposes.",
    "1930": "Changing the type of a datum is necessary from time to time, but\nsometimes a compiler can do the work for you.",
    "1931": "The most common form of { implicit type conversion} occurs with binary\narithmetic operations.  Integer arithmetic in C always uses types of\nat least the size of { int}, and all floating-point arithmetic uses\n{ double}.",
    "1932": "If either or both operands have smaller integer types, or differ from\none another, the compiler implicitly converts them before performing\nthe operation, and the type of the result may be different from those of\nboth operands.",
    "1933": "In general, the compiler selects the final type according to some\npreferred ordering in which floating-point is preferred over integers,\nunsigned values are preferred over signed values, and more bits are\npreferred over fewer bits.",
    "1934": "The type of the result must be at least as large as either argument,\nbut is also at least as large as an { int} for integer operations\nand a { double} for floating-point operations.",
    "1935": "Modern C compilers always extend an integer type's bit width before\nconverting from signed to unsigned.  The original C specification\ninterleaved bit width extensions to { int} with sign changes, thus\n{ older compilers may not be consistent, and implicitly require\nboth types of conversion in a single operation may lead to portability\nbugs.}",
    "1936": "The implicit extension to { int} can also be confusing in the sense\nthat arithmetic that seems to work on smaller integers fails with\nlarger ones.  For example, multiplying two 16-bit integers set to 1000\nand printing the result works with most compilers because the 32-bit \n{ int} result is wide enough to hold the right answer.  In contrast,\nmultiplying two 32-bit integers set to 100,000 produces the wrong\nresult because the high bits of the result are discarded before it can\nbe converted to a larger type.  For this operation to produce the\ncorrect result, one of the integers must be converted explicitly (as\ndiscussed later) before the multiplication.",
    "1937": "Implicit type conversions also occur due to assignments.  Unlike\narithmetic conversions, the final type must match the left-hand side\nof the assignment (for example, a variable to which a result is assigned), and\nthe compiler simply performs any necessary conversion.",
    "1938": "{ Since the desired type may be smaller than the type of the value\nassigned, information can be lost.}  Floating-point values are\ntruncated when assigned to integers, and high bits of wider integer\ntypes are discarded when assigned to narrower integer types.  { Note\nthat a positive number may become a negative number when bits are\ndiscarded in this manner.}",
    "1939": "Passing arguments to functions can be viewed as a special case of\nassignment.  Given a function prototype, the compiler knows the type\nof each argument and can perform conversions as part of the code\ngenerated to pass the arguments to the function.  Without such a\nprototype, or for functions with variable numbers of arguments, the\ncompiler lacks type information and thus cannot perform necessary\nconversions, leading to unpredictable behavior.  By default, however,\nthe compiler extends any integer smaller than an { int}\nto the width of an { int} and converts { float} to\n{ double}.",
    "1940": "Occasionally it is convenient to use an { explicit type cast} to force\nconversion from one type to another.  { Such casts must be used\nwith caution, as they silence many of the warnings that a compiler\nmight otherwise generate when it detects potential problems.}  One\ncommon use is to promote integers to floating-point before an\narithmetic operation, as shown to the right.",
    "1941": "{",
    "1942": "aaaa=\nint\nmain ()\n{\n>  int numerator = 10;\n>  int denominator = 20;\n>\n>  printf (\"fn\", numerator / (double)denominator);\n>  return 0;\n}",
    "1943": "}\n{-14pt}",
    "1944": "The type to which a value is to be converted\nis placed in parentheses in front of the value.  In most cases,\nadditional parentheses should be used to avoid confusion about the\nprecedence of type conversion over other operations.",
    "1945": "This short summary provides a list of both terms that we expect you to\nknow and and skills that we expect you to have after our first few weeks\ntogether.  The first part of the course is shorter than the other three\nparts, so the amount of material is necessarily less.",
    "1946": "These notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.",
    "1947": "According to educational theory, the difficulty of learning depends on \nthe type of task involved.  Remembering new terminology is relatively \neasy, while applying the ideas underlying design decisions shown by \nexample to new problems posed as human tasks is relatively hard.",
    "1948": "In this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).",
    "1949": "This time, we'll list the skills first and leave the easy stuff for the \nnext page.",
    "1950": "We expect you to be able to exercise the following skills:",
    "1951": "{}{{}{}\n{}{}{}",
    "1952": "{Represent decimal numbers with unsigned, 2's complement, and IEEE\nfloating-point representations, and be able to calculate the decimal value\nrepresented by a bit pattern in any of these representations.}",
    "1953": "{Be able to negate a number represented in the 2's complement\nrepresentation.}",
    "1954": "{Perform simple arithmetic by hand on unsigned and 2's complement\nrepresentations, and identify when overflow occurs.}",
    "1955": "{Be able to write a truth table for a Boolean expression.}",
    "1956": "{Be able to write a Boolean expression as a sum of minterms.}",
    "1957": "MOVED TO PART 4",
    "1958": "{Be able to calculate the Hamming distance of a code/representation.}\n \n {Know the relationships between Hamming distance and the abilities\n to detect and to correct bit errors.}",
    "1959": "{Know how to declare and initialize C variables with one of the \nprimitive data types.}",
    "1960": "At a more abstract level, we expect you to be able to:",
    "1961": "{}{{}{}\n{}{}{}",
    "1962": "{Understand the value of using a common mathematical basis, such\nas modular arithmetic, in defining multiple representations (such as\nunsigned and 2's complement).}",
    "1963": "{Write Boolean expressions for the overflow conditions\non both unsigned and 2's complement addition.}",
    "1964": "MOVED TO PART 4",
    "1965": "{Be able to use parity for error detection, and Hamming codes for\n error correction.}",
    "1966": "{Be able to write single { if} statements and { for} loops\nin C in order to perform computation.}",
    "1967": "{Be able to use { scanf} and { printf} for basic input and \noutput in C.}",
    "1968": "And, at the highest level, we expect that you will be able to reason about\nand analyze problems in the following ways:",
    "1969": "{}{{}{}\n{}{}{}",
    "1970": "{Understand the tradeoffs between integer\n  FIXME?     not covered by book nor notes currently \n, fixed-point,    \nand floating-point representations for numbers.}",
    "1971": "{Understand logical completeness and be able to prove or disprove\nlogical completeness for sets of logic functions.}",
    "1972": "PARTIALLY MOVED TO PART 4",
    "1973": "{Understand the properties necessary in a representation, and understand\n basic tradeoffs in the sparsity of code words with error detection and\n correction capabilities.}\n{Understand the properties necessary in a representation: no ambiguity\nin meaning for any bit pattern, and agreement in advance on the meanings of \nall bit patterns.}",
    "1974": "{Analyze a simple, single-function C program and be able to explain its purpose.}",
    "1975": "You should recognize all of these terms\nand be able to explain what they mean.\n(the parentheses give page numbers,\nor ``P&P'' for Patt & Patel).",
    "1976": "Actually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  (You may skip the *'d terms in Fall 2012.)",
    "1977": "Note that we are not saying that you should, for example, be able to \nwrite down the ASCII representation from memory.  In that example, \nknowing that it is a {7-bit} representation used for English\ntext is sufficient.  You can always look up the detailed definition \nin practice.",
    "1978": "[t]\n{}{{}{}\n{}{}{}",
    "1979": "{universal computational devices /  computing machines ()\n{--}{{}{}\n{}{}{}\n undecidable ()\n the halting problem ()",
    "1980": "}",
    "1981": "pre-Fall 2015 version",
    "1982": "{Turing machines\n {--}{{}{}\n {}{}{}\n  universal computational device/ computing machine\n  intractable/undecidable\n  the halting problem\n \n }",
    "1983": "{information storage in computers\n{--}{{}{}\n{}{}{}\n bits ()\n representation (P&P)\n data type ()\n unsigned representation ()\n 2's complement representation",
    "1984": "FIXME?  not covered by book nor notes currently\n  fixed-point representation",
    "1985": "IEEE floating-point representation\n ASCII representation\n equivalence classes",
    "1986": "}",
    "1987": "{operations on bits\n{--}{{}{}\n{}{}{}\n 1's complement operation\n carry (from addition)\n overflow (on any operation) ()\n Boolean logic and algebra\n logic functions/gates\n truth table\n AND/conjunction\n OR/disjunction\n NOT/logical complement/ (logical) negation/inverter\n XOR\n logical completeness\n minterm",
    "1988": "}",
    "1989": "{mathematical terms\n{--}{{}{}\n{}{}{}\n modular arithmetic\n implication\n contrapositive\n proof approaches: by construction, by contradiction, by induction\n without loss of generality (w.l.o.g.)",
    "1990": "}",
    "1991": "MOVED TO PART 4",
    "1992": "{error detection and correction\n {--}{{}{}\n {}{}{}\n  code/sparse representation\n  code word\n  bit error\n  odd/even parity bit\n  Hamming distance between code words\n  Hamming distance of a code\n  Hamming code\n  SEC-DED\n \n }",
    "1993": "[t]\n{}{{}{}\n{}{}{}",
    "1994": "{high-level language concepts\n{--}{{}{}\n{}{}{}\n syntax\n{variables\n{--}{{}{}\n{}{}{}\n declaration\n primitive data types\n symbolic name/identifier\n initialization",
    "1995": "}\n FIXME?  really not necessary for them\n strongly typed languages\n expression\n statement",
    "1996": "}",
    "1997": "{C operators\n{--}{{}{}\n{}{}{}\n operands\n arithmetic\n bitwise\n comparison/relational\n assignment\n address\n arithmetic shift\n logical shift\n precedence",
    "1998": "}",
    "1999": "{functions in C\n{--}{{}{}\n{}{}{}\n { main}\n function call\n arguments\n {{ printf} and { scanf}\n{--}{{}{}\n{}{}{}\n format string\n escape character",
    "2000": "}\n { sizeof} (built-in)",
    "2001": "}",
    "2002": "{transforming tasks into programs\n{--}{{}{}\n{}{}{}\n flow chart\n sequential construct\n conditional construct\n iterative construct/iteration/loop\n loop body",
    "2003": "}",
    "2004": "{C statements\n{--}{{}{}\n{}{}{}\n statement: null, simple, compound\n { if} statement\n { for} loop\n { return} statement",
    "2005": "}",
    "2006": "THESE ARE NOT REQUIRED TOPICS",
    "2007": "{execution of C programs\n {--}{{}{}\n {}{}{}\n  compiler/interpreter\n  source code\n  header files\n  assembly code\n  instructions\n  executable image\n \n }\n \n {the C preprocessor\n {--}{{}{}\n {}{}{}\n  #include directive\n  #define directive\n \n }",
    "2008": "{   }   blank 3rd page",
    "2009": "This set of notes discusses the overflow conditions for unsigned and\n2's complement addition.  For both types, we formally prove that\nthe conditions that we state are correct.  Many of our faculty want our\nstudents to learn to construct formal proofs, so we plan to begin\nexposing you to this process in our classes.",
    "2010": "Prof. Lumetta is a fan of Prof. George Polya's educational theories\nwith regard to proof techniques, and in particular the idea that one\nbuilds up a repertoire of approaches by seeing the approaches used \nin practice.",
    "2011": ", but teaching proof\n techniques effectively is challenging, particularly when exercises\n are of the form, ``Prove that <insert true theorem> is true'' rather\n than the more open-ended form that we typically encounter in research,\n ``Prove that <insert some conjecture> is true, or find a \n counterexample.''",
    "2012": "Some of you may not have been exposed to basics of mathematical logic, so\nlet's start with a brief introduction to implication.  We'll use \nvariables p and q to represent statements that can be either true\nor false.  For example, p might represent the statement, ``Jan is\nan ECE student,'' while q might represent the statement, ``Jan\nworks hard.''  The { logical complement} or { negation} of \na statement p,\nwritten for example as ``not p,'' has the opposite truth value:\nif p is true, not p is false, and if p is false, not p is\ntrue.",
    "2013": "An { implication} is a logical relationship between two statements.\nThe implication itself is also a logical statement, and may be true or\nfalse.  In English, for example, we might say, ``If p, q.''\nIn mathematics, the same implication is usually written as either \n``q if p'' or ``p,'' and the latter is read \nas, ``p implies q.''",
    "2014": "Using our example values for p and q, we can see that\np is true: ``Jan is an ECE student'' does\nin fact imply that ``Jan works hard!''",
    "2015": "The implication p \nis only considered false if p is true and q is false.\nIn all other cases, the implication is true.\nThis definition can be a little confusing at first, so let's use\nanother example to see why.",
    "2016": "Let  p represent the statement\n``Entity X is a flying pig,'' and let q represent \nthe statement, ``Entity X obeys air traffic control regulations.''",
    "2017": "Here the implication p is again true: \nflying pigs do not exist, so p is false, and thus \n``p'' is true---for any value of statement q!",
    "2018": "Given an implication ``p,'' we say that the {\nconverse} of the implication is the statement \n``q,'' which is also an implication.\nIn mathematics, the converse of \np\nis sometimes written\nas ``q only if p.''  The converse of an implication may or may not have\nthe same truth value as the implication itself.  Finally,\nwe frequently use the shorthand notation, ``p if and only if q,''\n(or, even shorter, ``p iff q'') to mean \n``p { and}\nq.'' This last statement is true only when both\nimplications are true.",
    "2019": "Let's say that we add two {N-bit} unsigned numbers, A\nand B.  The {N-bit} unsigned representation \ncan represent integers in the range [0,2^N-1].",
    "2020": "Recall that we say that the addition operation has \noverflowed if the number represented by the {N-bit} pattern\nproduced for the sum does not actually represent the number A+B.",
    "2021": "For clarity, let's name the bits of A by writing the number\nas a_{N-1}a_{N-2}...a_1a_0.  Similarly, let's write B as\nb_{N-1}b_{N-2}...b_1b_0.  Name the sum C=A+B.  The sum that\ncomes out of the add unit has only N bits, but recall that\nwe claimed in class that the overflow condition for unsigned \naddition is given by the { carry} out of the most significant\nbit.  So let's write the sum as \nc_c_{N-1}c_{N-2}...c_1c_0, realizing that c_N is the\ncarry out and not actually part of the sum produced by the \nadd unit.",
    "2022": "{ Theorem:}",
    "2023": "Addition of two {N-bit} unsigned numbers\nA=a_{N-1}a_{N-2}...a_1a_0\nand\nB=b_{N-1}b_{N-2}...b_1b_0\nto produce sum\nC=A+B=c_c_{N-1}c_{N-2}...c_1c_0,\noverflows if and only if\nthe carry out c_N of the addition is a 1 bit.",
    "2024": "{ Proof:}",
    "2025": "Let's start with the ``if'' direction.  In other words, c_N=1 implies\noverflow.  Recall that unsigned addition is the same as base 2 addition,\nexcept that we discard bits beyond c_{N-1} from the sum C.\nThe bit c_N has place value 2^N, so, when c_N=1 we can write that \nthe correct sum C{2^N}.  But no value that large can be represented\nusing the {N-bit} unsigned representation, so we have an overflow.",
    "2026": "The other direction (``only if'') is slightly more complex: we need to\nshow that overflow implies that c_N=1.  We use a range-based argument\nfor this purpose.  Overflow means that the sum C is outside the\nrepresentable range [0,2^N-1].  Adding two non-negative numbers cannot\nproduce a negative number, so the sum can't be smaller than 0.  Overflow \nthus implies that C{2^N}.",
    "2027": "Does that argument complete the proof?  No, because some numbers, such \nas 2^{N+1}, are larger than 2^N, but do not have a 1 bit in the Nth\nposition when written in binary.  We need to make use of the constraints\non A and B implied by the possible range of the representation.",
    "2028": "In particular, given that A and B are represented as {N-bit}\nunsigned values, we can write",
    "2029": "{eqnarray*}\n0  & A &  2^N - 1\n0  & B &  2^N - 1\n{eqnarray*}",
    "2030": "We add these two inequalities and replace A+B with C to obtain",
    "2031": "{eqnarray*}\n0  & C &  2^{N + 1} - 2\n{eqnarray*}",
    "2032": "Combining the new inequality with the one implied by the overflow \ncondition, we obtain",
    "2033": "{eqnarray*}\n2^N  & C &  2^{N + 1} - 2\n{eqnarray*}",
    "2034": "All of the numbers in the range allowed by this inequality have c_N=1,\ncompleting our proof.",
    "2035": "Understanding overflow for 2's complement addition is somewhat trickier,\nwhich is why the problem is a good one for you to think about on your\nown first.",
    "2036": "Our operands, A and B, are now two {N-bit} 2's complement numbers.\nThe {N-bit} 2's complement representation \ncan represent integers in the range [-2^{N-1},2^{N-1}-1].",
    "2037": "Let's start by ruling out a case that we can show never leads to overflow.",
    "2038": "{ Lemma:}",
    "2039": "Addition of two {N-bit} 2's complement numbers A and B\ndoes not overflow if one of the numbers is negative and the other is\nnot.",
    "2040": "{ Proof:}",
    "2041": "We again make use of the constraints implied by the fact that A and B\nare represented as {N-bit} 2's complement values.  We can assume\n{ without loss of generality}{This common mathematical phrasing\nmeans that we are using a problem symmetry to cut down the length of the\nproof discussion.  In this case, the names A and B aren't particularly\nimportant, since addition is commutative (A+B=B+A).  Thus the proof\nfor the case in which A is negative (and B is not) is identical to the\ncase in which B is negative (and A is not), except that all of the \nnames are swapped.  The term ``without loss of generality'' means that\nwe consider the proof complete even with additional assumptions, in\nour case that A<0 and B.}, or { w.l.o.g.}, \nthat A<0 and B.",
    "2042": "Combining these constraints with the range representable \nby {N-bit} 2's complement, we obtain",
    "2043": "{eqnarray*}\n-2^{N-1}  & A & < 0\n0  & B & < 2^{N-1}\n{eqnarray*}",
    "2044": "We add these two inequalities and replace A+B with C to obtain",
    "2045": "{eqnarray*}\n-2^{N-1}  & C & < 2^{N-1}\n{eqnarray*}",
    "2046": "But anything in the range specified by this inequality can be represented\nwith {N-bit} 2's complement, and thus the addition does not overflow.",
    "2047": "We are now ready to state our main theorem.  For convenience, \nlet's use different names for the actual sum C=A+B and the sum S\nreturned from the add unit.  We define S as the number represented by\nthe bit pattern produced by the add unit.  When overflow \noccurs, S=C, but we always have (S=C)  2^N.",
    "2048": "{ Theorem:}",
    "2049": "Addition of two {N-bit} 2's complement numbers A and B\noverflows if and only if one of the following conditions holds:",
    "2050": "{A<0 and B<0 and S}\n{A and B and S<0}",
    "2051": "{ Proof:}",
    "2052": "We once again start with the ``if'' direction.  That is, if condition 1 \nor condition 2 holds, we have an overflow.  The proofs are straightforward.\nGiven condition 1, we can add the two inequalities A<0 and B<0 to \nobtain C=A+B<0.  But S, so clearly S=C, thus overflow \nhas occurred.",
    "2053": "Similarly, if condition 2 holds, we can add the inequalities A\nand B to obtain C=A+B.  Here we have S<0, so again\nS=C, and we have an overflow.",
    "2054": "We must now prove the ``only if'' direction, showing that any overflow\nimplies either condition 1 or condition 2.  By the \n{ contrapositive}{If we have a statement of the form\n(p implies q), its contrapositive is the \nstatement (not q implies not p).\nBoth statements have the same truth value.  In this case, we can turn\nour Lemma around as stated.} of our\nLemma, we know that if an overflow occurs, either both operands are \nnegative, or they are both positive.",
    "2055": "Let's start with the case in which both operands are negative, so A<0\nand B<0, and thus the real sum C<0 as well.  Given that A and B\nare represented as {N-bit} 2's complement, they must fall in\nthe representable range, so we can write",
    "2056": "{eqnarray*}\n-2^{N-1}  & A & < 0\n-2^{N-1}  & B & < 0\n{eqnarray*}",
    "2057": "We add these two inequalities and replace A+B with C to obtain",
    "2058": "{eqnarray*}\n-2^N  & C & < 0\n{eqnarray*}",
    "2059": "Given that an overflow has occurred, C must fall outside of the \nrepresentable range.  Given that C<0, it cannot be larger than the\nlargest possible number representable using {N-bit} 2's\ncomplement, so we can write",
    "2060": "{eqnarray*}\n-2^N  & C & < -2^{N-1}\n{eqnarray*}",
    "2061": "We now add 2^N to each part to obtain",
    "2062": "{eqnarray*}\n0  & C + 2^N & < 2^{N-1}\n{eqnarray*}",
    "2063": "This range of integers falls within the representable range \nfor {N-bit} 2's complement, so we can replace the middle\nexpression with S (equal to C modulo 2^N) to find that",
    "2064": "{eqnarray*}\n0  & S & < 2^{N-1}\n{eqnarray*}",
    "2065": "Thus, if we have an overflow and both A<0 and B<0, the resulting\nsum S, and condition 1 holds.",
    "2066": "The proof for the case in which we observe an overflow when \nboth operands are non-negative (A and B)\nis similar, and leads to condition 2.  We again begin with\ninequalities for A and B:",
    "2067": "{eqnarray*}\n0  & A & < 2^{N-1}\n0  & B & < 2^{N-1}\n{eqnarray*}",
    "2068": "We add these two inequalities and replace A+B with C to obtain",
    "2069": "{eqnarray*}\n0  & C < & 2^N\n{eqnarray*}",
    "2070": "Given that an overflow has occurred, C must fall outside of the \nrepresentable range.  Given that C{}0, it cannot be smaller than the\nsmallest possible number representable using {N-bit} 2's\ncomplement, so we can write",
    "2071": "{eqnarray*}\n2^{N-1}  & C & < 2^N\n{eqnarray*}",
    "2072": "We now subtract 2^N to each part to obtain",
    "2073": "{eqnarray*}\n-2^{N-1}  & C - 2^N & < 0\n{eqnarray*}",
    "2074": "This range of integers falls within the representable range \nfor {N-bit} 2's complement, so we can replace the middle\nexpression with S (equal to C modulo 2^N) to find that",
    "2075": "{eqnarray*}\n-2^{N-1}  & S & < 0\n{eqnarray*}",
    "2076": "Thus, if we have an overflow and both A and B, the resulting\nsum S<0, and condition 2 holds.",
    "2077": "Thus overflow implies either condition 1 or condition 2, completing our\nproof."
}