\documentclass{patt}
\usepackage[section]{placeins}
\graphicspath{{..//art/ch02/},{..//art/designelements/},{..//art/designelements/UnNumberedArt/}}

\setcounter{chapter}{1}
\setcounter{page}{25}
\setcounter{part}{1}

\begin{document}

\bgroup
  \makeatletter
\def\@makechapterhead#1{%
  \begingroup
  \parindent \z@%
  \vspace*{-8.5\p@}%
  \begin{picture}(0,0)
    \put(432,-577){\includegraphics[width=0.723333in,height=9.52667in]{PattChp.tif}}
    \put(428,-577){\rule{1\p@}{9.52667in}}
    \put(381.5,-181){\vbox{%
        \includegraphics{PattIcon1\ifnum\value{part}=1
            a\else b\fi.eps}\vspace{1pc}\par
        \includegraphics{PattIcon2\ifnum\value{part}=4
            a\else b\fi.eps}\vspace{1pc}\par
        \includegraphics{PattIcon3\ifnum\value{part}=4
            a\else b\fi.eps}\vspace{1pc}\par
        \includegraphics{PattIcon4\ifnum\value{part}=4
            a\else b\fi.eps}\vspace{1pc}\par
        \includegraphics{PattIcon5\ifnum\value{part}=4
            a\else b\fi.eps}\vspace{1pc}}}
  \end{picture}%
  \settowidth{\chapternumberwidth}{\fontsize{12}{12}\selectfont\industriasolid\trackonefifty{chapter}}%
  \hfill\parbox{\chapternumberwidth}{%
    \centering\industriasolid
    \centerline{\hss\fontsize{12}{12}\selectfont\trackonefifty{chapter}\hss}\par\vspace{1.5pc}
    \centerline{\sans\fontsize{72}{12}\selectfont\colour\thechapter}}
  \par
  \vspace{6.48pc}
  {\industriasolid\fontsize{30}{32}\selectfont\trackten{#1}\par}%
  \vspace{1.5pc}%
  {\colour\rule{36.4pc}{4\p@}}
  \vspace{4pt}
  \endgroup}
  \makeatother

 \egroup

\chapter{Bits, Data Types, and Operations}\label{chapt:bits}

%2.1
\section{Bits and Data Types}

%2.1.1
\subsection{The Bit as the Unit of Information}

We noted in Chapter~1 that the computer was organized as a system with
several levels of transformation.  A problem stated in a natural
language such as English is actually solved by the electrons moving
around inside the components of the computer.

Inside the computer, millions of very tiny, very fast devices control
the movement of those electrons.  These devices react to the presence
or absence of voltages in electronic circuits.  They could react to
the actual values of the voltages, rather than simply to the presence 
or absence of
voltages. However, this would make the control and detection circuits
more complex than they need to be.  It is much easier to detect simply
whether or not a voltage exists at a point in a circuit
than it is to measure exactly what that voltage is.

To understand this, consider any wall outlet in your home.  You could
measure the exact voltage it is carrying, whether 120 volts or 115
volts, or 118.6 volts, for example.  However, the detection circuitry
to determine {\em only} whether there is a voltage 
or whether there is no voltage is much simpler.  Your
finger casually inserted into the wall socket, for example, will
suffice.

We symbolically represent the presence of a voltage as ``1'' and the
absence of a voltage as ``0.''  We refer to each 0 and each 1 as a
``bit,'' which is a shortened\index{bit} \index{binary digit} form of
{\em binary digit}. Recall the digits you have been using since you were a
child---$0,1,2,3,\ldots,9$. There are 10 of them, and they are
referred to as decimal digits.  In the case of binary digits, there
are two of them, 0 and~1.

To be perfectly precise, it is not really the case that the computer
differentiates the {\em absolute} absence of a voltage (that is, 0)
from the {\em absolute} presence of a voltage (that is, 1).  Actually,
the electronic circuits in the computer differentiate voltages {\em
  close to} 0 from voltages {\em far from} 0.  So, for example, if the
computer expects either a voltage of 1.2 volts or a voltage of 0 volts (1.2
volts signifying 1 and 0 volts signifying 0), then a voltage of 1.0
volts will be taken as a 1 and 0.2 volts will be taken as a~0.

With one wire, one can differentiate only two things.  One of them can be 
assigned the value 0, the other can be assigned the value 1.  But to get 
useful work done by the computer, it is necessary to be able to diffentiate 
a large number of distinct values, and to assign each of them a unique 
representation.  We can accomplish this by combining many wires, i.e., many
bits.  \lightbulb[-18pt]
For example, if we use eight bits (corresponding to
the voltage present on each of eight wires), we can represent one particular
value as 01001110, and another value as 11100111.  In fact, if we are
limited to eight bits, we can differentiate at most only 256 (that is,
$2^8$) different things.  In general, with $k$ bits, we can
distinguish at most $2^k$ distinct items.  Each pattern of these $k$
bits is a code\index{code}; that is, it corresponds to a particular
item (or value).

%2.1.2
\subsection{Data Types}

\looseness=1
There are many ways to represent the same value.  For example, the
number five can be written as a 5.  This is the standard decimal
notation that you are used to.  The value five can also be represented
by someone holding up one hand, with all fingers and thumb extended.
The person is saying, ``The number I wish to communicate can be
determined by counting the number of fingers I am showing.''  A
written version of that scheme would be the value 11111.  This
notation has a name also---\textit{unary}. The Romans had yet another
notation for five---the character V.  We will see momentarily that a
fourth notation for five is the binary representation 00000101.

It is not enough simply to represent values; we must be able to
operate on those values.  We say a particular representation is a {\em
  data type}\index{data type} if there are operations in the computer
that can operate on information that is encoded in that representation. 
Each instruction set architecture (ISA) has its own set of data types and 
its own set of instructions that can operate on those data types. In this 
book, we will mainly use two data types: {\em 2's complement integers} for
\lightbulb[-23pt]
representing positive and negative integers that we wish to perform
arithmetic on, and {\em ASCII codes} for representing characters that 
we wish to input to a computer via the keyboard or output from the computer
via a monitor.  Both data types will be explained shortly.

\looseness=1
There are other representations of information that could be used, and
indeed that are present in most computers.  Recall the ``scientific
notation'' from high school chemistry where you were admonished to
represent the decimal number 621 as $6.21{\cdot} 10^2$. There are
computers that represent numbers in that form, and they provide
operations that can operate on numbers so represented.  That data type
is usually called {\em floating point}. We will examine its
representation in Section~2.7.1.

%2.2
\section{Integer Data Types}

%2.2.1
\subsection{Unsigned Integers}

The first representation of information, or data type, that we shall
look at is the unsigned integer.  As its name suggests, an unsigned integer
has no sign (plus or minus) associated with it.  An unsigned integer just has
a magnitude.  Unsigned integers have many uses in
a computer.  If we wish\index{unsigned integer} to perform a task some
specific number of times, unsigned integers enable us to keep track of
this number easily by simply counting how many times we have performed
the task.  Unsigned integers also provide a means for
identifying different memory locations in the computer in the same
way that house numbers differentiate 129 Main Street from 131 Main Street.
I don't recall ever seeing a house number with a minus sign in front of it.

We can represent unsigned integers as strings of binary digits.  To do
this, we use a positional notation much like the decimal system that
you have been using since you were three years old.

You are familiar with the decimal number 329, which also uses
positional notation.  The 3 is worth much more than the 9, even though
the absolute value of 3 standing alone is only worth 1/3 the value of
9 standing alone.  This is because, as you know, the 3 stands for 300
($3\cdot 10^2$) due to its position in the decimal string 329, while
the 9 stands for $9\cdot 10^0$.

Instead of using decimal digits we can represent unsigned integers using
just the the binary digits 0 and 1.  Here the base is 2, rather
than 10.  So, for example, if we have five bits (binary digits) available 
to represent our values, the number 5, which we menioned earlier, is 
represented as 00101, corresponding to
\begin{equation*}
0\cdot 2^4+0 \cdot 2^3+1\cdot 2^2+0\cdot 2^1+1\cdot 2^0
\end{equation*}
With $k$ bits, we can represent in this positional notation exactly
$2^k$ integers, ranging from 0 to $2^k-1$.  Figure~\ref{fig:integers}
shows the five-bit representations for the integers from 0~to~31.

%Figure 2.1
\begin{figure}
\vspace*{2pt}
\begin{center}
\small
\begin{tabular}{@{}lcccc@{}}\toprule
\tabhead{Representation} & & \tabhead{Value Represented} \\[-4pt]
\multicolumn{5}{@{}l@{}}{\hrulefill}\\
\rule{0pt}{10pt}       & \tabhead{Unsigned}  & \tabhead{Signed Magnitude}
&  \tabhead{1's Complement}  & \tabhead{2's Complement}\\\midrule
00000 &  0  &  0  &     0  &     0\\
00001 &  1  &  1  &     1  &     1 \\
00010 &  2  &  2  &     2  &     2 \\
00011 &  3  &  3  &     3  &     3 \\
00100 &  4  &  4  &     4  &     4 \\
00101 &  5  &  5  &     5  &     5 \\
00110 &  6  &  6  &     6  &     6 \\
00111 &  7  &  7  &     7  &     7 \\
01000 &  8  &  8  &     8  &     8 \\
01001 &  9  &  9  &     9  &     9 \\
01010 &  10 &  10 &     10 &     10 \\
01011 &  11 &  11 &     11 &     11 \\
01100 &  12 &  12 &     12 &     12 \\
01101 &  13 &  13 &     13 &     13 \\
01110 &  14 &  14 &     14 &     14 \\
01111 &  15 &  15 &     15 &     15 \\
10000 &  16 &  $-$0 &     $-$15 &     $-$16 \\
10001 &  17 &  $-$1 &     $-$14 &     $-$15 \\
10010 &  18 &  $-$2 &     $-$13 &     $-$14 \\
10011 &  19 &  $-$3 &     $-$12 &     $-$13 \\
10100 &  20 &  $-$4 &     $-$11 &     $-$12 \\
10101 &  21 &  $-$5 &     $-$10 &     $-$11 \\
10110 &  22 &  $-$6 &     $-$9 &     $-$10 \\
10111 &  23 &  $-$7 &     $-$8 &     $-$9  \\
11000 &  24 &  $-$8 &     $-$7 &     $-$8  \\
11001 &  25 &  $-$9 &     $-$6 &     $-$7  \\
11010 &  26 &  $-$10 &     $-$5 &     $-$6  \\
11011 &  27 &  $-$11 &     $-$4 &     $-$5  \\
11100 &  28 &  $-$12 &     $-$3 &     $-$4  \\
11101 &  29 &  $-$13 &     $-$2 &     $-$3  \\
11110 &  30 &  $-$14 &     $-$1 &     $-$2  \\
11111 &  31 &  $-$15 &     $-$0 &     $-$1 \\\botrule
\end{tabular}
\end{center}
\caption{Four representations of integers}
\label{fig:integers}
\vspace{-6pt}
\end{figure}

%2.2.2
\subsection{Signed Integers}

To do useful arithmetic, however, it is often (although not always)
necessary to \index{signed integers} be able to deal with negative
quantities as well as positive.  We could take our $2^k$ distinct
patterns of $k$ bits and separate them in half, half for positive
integers, and half for negative integers.  In this way, with five-bit
codes, instead of representing integers from 0 to ${+}31$, we could
choose to represent positive integers from ${+}1$ to ${+}15$ and
negative integers from ${-}1$ to ${-}15$.  There are 30 such integers.
Since $2^5$ is 32, we still have two 5-bit codes unassigned.  One of
them, 00000, we would presumably assign to the value 0, giving us the
full range of integer values from ${-}15$ to ${+}15$. That leaves one
five-bit code left over, and there are different ways to assign this code,
as we will see momentarily.

We are still left with the problem of determining what codes to assign
to what values. That is, we have 32 codes, but which value should go
with which code?

Positive integers are represented in the straightforward positional
scheme.  Since there are $k$ bits, and we wish to use exactly half of
the $2^k$ codes to represent the integers from 0 to $2^{k-1}-1$, all
positive integers will have a leading 0 in their representation.  In
our example of Figure~\ref{fig:integers}, (with $k=5$), the largest positive 
integer ${+}15$ is represented as 01111.

Note that in all three {\em signed} data types shown in 
Figure~\ref{fig:integers} , the representation for 0 and all the positive 
integers start with a
leading 0. What about the representations for the negative integers (in
our five-bit example, ${-}1$ to $-$15)?  The first thought that
usually comes to mind is: If a leading 0 signifies a {\em positive}\enlargethispage{-2\baselineskip}
integer, how about letting a leading 1 signify a {\em negative}
integer?  The result is the {\em signed-magnitude} data type shown in
Figure~\ref{fig:integers}.  A second thought (which was actually\index{data
  type!signed-magnitude} used on some early computers such as the
Control Data Corporation 6600) was the following: Let a negative
number be represented by taking the representation of the positive
number having the same magnitude, and ``flipping'' all the bits.  That is, if
the original representation had a 0, replace it with a 1; if it originally
had a 1, replace it with a 0.  For example, since $+$5 is represented as 00101,
we designate $-$5 as 11010.  This data type is referred to in the computer 
engineering community as {\em 1's complement}, \index{data type!1's complement}
and is also shown in Figure~\ref{fig:integers}.

At this point, you might think that a computer designer could assign
any bit pattern to represent any integer he or she wants.  And you
would be right!  Unfortunately, that could complicate matters when we
try to build an electronic circuit to add two integers.  In fact, the
signed-magnitude and 1's complement data types both require
unnecessarily cumbersome hardware to do addition.  Because computer
designers knew what it would take to design a circuit to add two
integers, they chose representations that simplified the 
circuit.  The result is the {\em 2's complement} data type, also shown
in Figure~\ref{fig:integers}.  \index{data type!2's complement} It is used 
on just about every computer manufactured today.

\enlargethispage{-2\baselineskip}
\vspace{-6pt}

%2.3
\section{2's Complement Integers}

We see in Figure~\ref{fig:integers} the representations of the integers 
from $-$16 to $+$15 for the 2's complement data type.  Why were those 
representations chosen?

The positive integers, we saw, are represented in the straightforward
positional scheme.  With five bits, we use exactly half of the $2^5$
codes to represent 0 and the positive integers from 1 to $2^4-1$.

The choice of representations for the negative integers was based, as
we said previously, on the wish to keep the logic circuits as simple
as possible.  Almost all computers use the same basic mechanism to perform
addition.  It is called an \textit{arithmetic and logic unit}, usually
known by its acronym ALU.  We will get into the actual structure of
the ALU in Chapters~3 and~4. What is relevant right now is that an ALU
has two inputs and one output.  It performs addition by adding the
binary bit patterns at its inputs, producing a bit pattern at its
output that is the sum of the two input bit patterns.

For example, if the ALU processed five-bit input patterns, and
the two inputs were 00110 and 00101, the result (output of
the ALU) would be 01011.  The addition is as follows:

\begin{cctable}
00110\\
00101\\
\seprule
01011
\end{cctable}

The addition of two binary strings is performed in the same way
addition of two decimal strings is performed, from right to left,
column by column.  If the addition in a column generates a carry, the
carry is added to the column immediately to its left.

What is particularly relevant is that the binary ALU does not know
(and does not care) what the two patterns it is adding represent.  It
simply adds the two binary patterns.  Since the binary ALU only ADDs
and does not CARE, it would be nice if our assignment of codes to the integers 
resulted in the ALU producing correct results when it added two integers.

For starters, it would be nice if, when the ALU adds the representation for 
an arbitrary integer to the representation of the integer having the same 
magnitude but opposite sign, the sum would be 0.  That is, if the inputs to
the ALU are the representations of non-zero integers $A$ and $-A$, the
output of the ALU should be 00000.

To accomplish that, the 2's complement data type specifies the
representation for each negative integer so that when the ALU adds it
to the representation of the positive integer of the same magnitude,
the result will be the representation for 0.  For example, since 00101
is the representation of $+$5, 11011 is chosen as the representation
for~$-$5.

Moreover, and actually more importantly, as we sequence through representations 
of $-$15 to $+$15, the ALU is adding 00001 to each successive
representation.

We can express this mathematically as:
\begin{align*}
&\text{REPRESENTATION(value${}+{}$1)}=\\[-\jot]
&\quad\text{REPRESENTATION(value)} + \text{REPRESENTATION(1).}
\end{align*}

This is sufficient to guarantee (as long as we do not get a result
larger than $+$15 or smaller than $-$15) that the binary ALU will
perform addition correctly.

Note in particular the representations for $-$1 and 0, that is, 11111
and 00000.  When we add 00001 to the representation for $-$1, we do
get 00000, but we also generate a carry.  That carry, however, does not
influence the result.  That is, the correct result of adding 00001 to
the representation for $-$1 is 0, not 100000.  Therefore, the carry is
ignored.  In fact, because the carry obtained by adding 00001 to 11111
is ignored, the carry can {\em always} be ignored when dealing with
2's complement arithmetic.

{\em Note:} If we know the representation for $A$, a shortcut for figuring 
out the representation for $-A(A\ne0)$, is as follows:
Flip all the bits of $A$ (the official term for ``flip'' is
{\em complement}), and add 1 to the complement of $A$. The
sum of A and the complement of A is 11111.  If we then add 00001 to 11111, the final
result is 00000.  Thus, the representation for $-A$ can be
easily obtained by adding 1 to the complement of~$A$.

\begin{example}
What is the 2's complement representation for ${-}13$?

\begin{enumerate}
\item Let $A$ be $+$13. Then the representation for $A$ is 01101 
since 13 = 8+4+1.
\item The complement of $A$ is 10010.
\item Adding 1 to 10010 gives us 10011, the 2's complement representation for $-$13.
\end{enumerate}
We can verify our result by adding the representations for $A$ and
$-A$,
\begin{cctable}
01101\\
10011\\
\seprule
00000\\[-8pt]
\end{cctable}
\end{example}

\enlargethispage{-\baselineskip}

You may have noticed that the addition of 01101 and 10011, in addition
to producing 00000, also produces a carry out of the five-bit ALU.\vadjust{\pagebreak}
That is, the binary addition of 01101 and 10011 is really 100000.
However, as we saw previously, this carry out can be ignored in the
case of the 2's complement data type.

At this point, we have identified in our five-bit scheme 15 positive
integers.  We have constructed 15 negative integers.  We also have a
representation for 0.  With $k=5$, we can uniquely identify 32
distinct quantities, and we have accounted for only 31 ($15+15+1$).
The remaining representation is 10000.  What value shall we assign to
it?

We note that $-$1 is 11111, $-$2 is 11110, $-$3 is 11101, and so on.
If we continue this, we note that $-$15 is 10001.  Note that, as in
the case of the positive representations, as we sequence backwards
from representations of $-$1 to $-$15, the ALU is subtracting 00001
from each successive representation.  Thus, it is convenient to assign
to 10000 the value $-$16; that is the value one gets by subtracting
00001 from 10001 (the representation for $-$15).

In Chapter~5 we will specify a computer that we affectionately have
named the LC-3 (for Little Computer~3). The LC-3 operates on 16-bit
values.  Therefore, the 2's complement integers that can be
represented in the LC-3 are the integers from $-$32{,}768 to
$+32{,}767$.

\enlargethispage{-\baselineskip}

%2.4
\section{Conversion between binary and decimal}

It is often useful to convert numbers between the 2's complement data type
the computer likes and the decimal representation that you have used all 
your life.

%2.4.1
\subsection{Binary to Decimal Conversion}

We convert a 2's complement representation of an integer 
\index{binary to decimal conversion} to
a decimal representation as follows: For purposes of illustration, we
will assume our number can be represented in eight bits,
corresponding to decimal integer values from $-$128 to $+$127.

Recall that an eight-bit 2's complement integer takes the form
\begin{equation*}
b_7\ b_6\ b_5\ b_4\ b_3\ b_2\ b_1\ b_0
\end{equation*}
where each of the bits $b_i$ is either 0 or 1.
\begin{enumerate}
\item Examine the leading bit $b_7$. If it is a 0, the integer is
  positive, and we can begin evaluating its magnitude.  If it is a 1,
  the integer is negative.  In that case, we need to first obtain the
  2's complement representation of the positive number having the same
  magnitude.  We do this by flipping all the bits and adding 1.

\item The magnitude is simply
\begin{equation*}
b_6\cdot 2^6+b_5\cdot 2^5+b_4\cdot 2^4+b_3\cdot 2^3+b_2\cdot 2^2+b_1\cdot 2^1+b_0\cdot 2^0
\end{equation*}
In either case, we obtain the decimal magnitude by simply adding 
the powers of 2 that have coefficients of~1.

\item Finally, if the original number is negative, we affix a minus
  sign in front. Done!
\end{enumerate}
\removelastskip
\pagebreak

\begin{example}
  Convert the 2's complement integer 11000111 to a decimal integer
  value.

  \begin{enumerate}
  \item Since the leading binary digit is a 1, the number is negative.
    We must first find the 2's complement representation of the
    positive number of the same magnitude.  This is 00111001.

  \item The magnitude can be represented as
    \begin{equation*}
      0\cdot 2^6+1\cdot 2^5+1\cdot 2^4+1\cdot 2^3+0\cdot 2^2+0\cdot 2^1+1\cdot 2^0
    \end{equation*}
    or,
    \begin{equation*}
      32+16+8+1.
    \end{equation*}

  \item The decimal integer value corresponding to 11000111 is $-$57.
\end{enumerate}
\end{example}

%2.4.2
\subsection{Decimal to Binary Conversion}

Converting from decimal to 2's complement is a little more
complicated.  The crux of the \index{decimal to binary conversion}
method is to note that a positive binary number is {\em odd} if the
rightmost digit is 1 and {\em even} if the rightmost digit is~0.

Consider again our generic eight-bit representation:
\begin{equation*}
b_7\cdot 2^7+b_6\cdot 2^6+b_5\cdot 2^5+b_4\cdot 2^4+b_3\cdot
2^3+b_2\cdot 2^2+b_1\cdot 2^1+b_0\cdot 2^0
\end{equation*}
We can illustrate the conversion best by first working
through an example.

Suppose we wish to convert the value $+$105 to a 2's complement binary
code.  We note that +105 is positive.  We first find values for $b_i$,
representing the magnitude 105.  Since the value is positive, we will
then obtain the 2's complement result by simply appending $b_7$, which
we know is~0.

Our first step is to find values for $b_i$ that satisfy the
following:
\begin{equation*}
105=b_6\cdot 2^6+b_5\cdot 2^5+b_4\cdot 2^4+b_3\cdot 2^3+b_2\cdot
2^2+b_1\cdot 2^1+b_0\cdot 2^0
\end{equation*}
Since 105 is odd, we know that $b_0$ is 1. We subtract
1 from both sides of the equation, yielding
\begin{equation*}
104=b_6\cdot 2^6+b_5\cdot 2^5+b_4\cdot 2^4+b_3\cdot 2^3+b_2\cdot
2^2+b_1\cdot 2^1
\end{equation*}
We next divide both sides of the equation by 2, yielding
\begin{equation*}
52=b_6\cdot 2^5+b_5\cdot 2^4+b_4\cdot 2^3+b_3\cdot 2^2+b_2\cdot
2^1+b_1\cdot 2^0
\end{equation*}
Since 52 is even, $b_1$, the only
coefficient not multiplied by a power of 2, must be equal to~0.

We iterate this process, each time subtracting the rightmost digit
from both sides of the equation, then dividing both sides by 2, and
finally noting whether the new decimal number on the left side is odd
or even.  Continuing where we left off, with
\begin{align*}
52&=b_6\cdot 2^5+j_5\cdot 2^4+b_4\cdot 2^3+b_3\cdot 2^2+b_2\cdot 2^1
\intertext{the process produces, in turn:}
26&=b_6\cdot 2^4+b_5\cdot 2^3+b_4\cdot 2^2+b_3\cdot 2^1+b_2\cdot 2^0
\intertext{Therefore, $b_2=0$.}
13&=b_6\cdot 2^3+b_5\cdot 2^2+b_4\cdot 2^1+b_3\cdot 2^0
\intertext{Therefore, $b_3=1$.}
6&=b_6\cdot 2^2+b_5\cdot 2^1+b_4\cdot 2^0
\intertext{Therefore, $b_4=0$.}
3&=b_6\cdot 2^1+b_5\cdot 2^0
\intertext{Therefore, $b_5=1$.}
1&=b_6\cdot 2^0
\end{align*}
Therefore, $b_6=1$, and we are done.  The binary
representation is 01101001.

Let's summarize the process. If we are given a decimal integer value
$N$,~we construct the 2's complement representation as follows:

\begin{enumerate}
\item We first obtain the binary representation of the magnitude of
$N$ by forming the equation
\begin{equation*}
N=b_6\cdot 2^6+b_5\cdot 2^5+b_4\cdot 2^4+b_3\cdot 2^3+b_2\cdot 2^2+b_1\cdot 2^1+b_0\cdot 2^0
\end{equation*}
and repeating the following, until the left side of
the equation is~0:
\begin{enumerate}[a.]
\item[a.] If $N$ is odd, the rightmost bit is 1.  If $N$ is even,
the rightmost bit is~0.
\item[b.] Subtract 1 or 0 (according to whether $N$ is odd or even)
  from $N$, remove the least significant term from the right side, and
  divide both sides of the equation by~2.
\end{enumerate}
Each iteration produces the value of one coefficient $b_i$.

\item If the original decimal number is positive, append a leading 0
  sign bit, and you are done.

\item If the original decimal number is negative, append a leading 0
  and then form the negative of this 2's complement representation,
  and then you\break are done.
\end{enumerate}

\vspace{-12pt}

%2.4.3
\subsection{Extending conversion to numbers with fractional parts}

What if the number we wish to convert is not an integer, but instead has
a fractional part.  How do we handle that wrinkle?
\paragraph{binary to decimal}

The binary to decimal case is straightforward.  In a positional notation
system, the number 

\noindent
$$0.b_{-1}b_{-2}b_{-3}b_{-4}$$

shows four bits to the right of the binary point, representing (when 
the corresponding $b_i$ = 1) the values 0.5,  0.25,  0.125, and  0.0625.  
To complete the
conversion to decimal, we simply add those values where the corresponding
$b_i$ = 1.  For example, if the fractional part of the binary representation is  
\begin{equation*}
.\ 1\ 0\ 1\ 1
\end{equation*}

\noindent
we would add 0.5 plus 0.125 plus 0.0625, or 0.6875.

\paragraph{decimal to binary}

The decimal to binary case requires a little more work.
Suppose we wanted to convert 0.421 to binary.  As we did for integer 
conversion, we first form the equation

$$0.421 = b_{-1} \times 2^{-1} + b_{-2} \times 2^{-2} + b_{-3} \times 2^{-3} + b_{-4} \times 2^{-4} + ...$$

\noindent
In the case of converting a decimal integer value to binary, we divided 
by 2 and 
assigned a 1 or 0 to the coefficient of $2^{0}$ depending on whether the number 
on the left of the equal sign is odd or even.  Here (i.e., in the case of 
converting a decimal fraction to binary), we multiply both sides of 
the equation by 2 and assign a 1 or a 0 to the coefficient of $2^{0}$ 
depending on whether the left side of the equation is greater than or 
equal to 1 or whether the left side is less than 1.  Do you see why?

Since 
$$0.842 = b_{-1} \times 2^{0} + b_{-2} \times 2^{-1} + b_{-3} \times 2^{-2} + b_{-4} \times 2^{-3} + ...$$,
\noindent
we assign $b_{-1} = 0$.  Continuing, 
$$1.684 = b_{-2} \times 2^{0} + b_{-3} \times 2^{-1} + b_{-4} \times 2^{-2} + ...$$,
\noindent
so we assign $b_{-2} = 1$, and subtract 1 from both sides of the equation,
yielding
$$0.684 = b_{-3} \times 2^{-1} + b_{-4} \times 2^{-2} + ...$$,
\noindent
Multiplying by 2, we get
$$1.368 = b_{-3} \times 2^{0} + b_{-4} \times 2^{-1} + ...$$,
\noindent
so we assign $b_{-3} = 1$, and subtract 1 from both sides of the equation,
yielding
$$0.368 = b_{-4} \times 2^{0} + ...$$, which assigns 0 to $b_{-4}$.
\noindent
We can continue this process indefinitely, until we are simply too tired to
go on, or until the left side = 0, in which case all bits to the right of where
we stop are all 0s.  In our case, stopping with four bits, we have 
converted 0.421 decimal to 0.0110 in binary.

%2.5
\section{Operations on Bits---Part I: Arithmetic}

%2.5.1
\subsection{Addition and Subtraction}

Arithmetic on 2's complement numbers is very much like the arithmetic
on decimal numbers that you have been doing for a long time.

Addition still proceeds from right to left, one digit at a time.  At
each point, we generate a sum digit and a carry.  Instead of
generating a carry after 9 (since 9 is the largest decimal digit), we
generate a carry after 1 (since 1 is the largest binary digit).

\pagebreak

\begin{example}
Using our five-bit notation, what is $11+3$?

\bigskip

\hspace*{-1.5pc}\begin{minipage}{27.5pc}
\color{seventyblack}%
\begin{Verbatim}[fontsize=\fontsize{9}{14}\selectfont]
The decimal value 11 is represented as 01011
The decimal value 3 is represented as  00011
The sum, which is the value 14, is     01110
\end{Verbatim}
\end{minipage}\vspace{-16pt}
\end{example}\vspace{-4pt}

\smallskip

\noindent
Subtraction is simply addition, preceded by determining the negative
of the number to be subtracted. That is, $A-B$ is simply $A+(-B)$.\vspace{-4pt}

\begin{example}
What is $14-9$?

\medskip

\hspace*{-1.5pc}\begin{minipage}{27.5pc}
\color{seventyblack}%
\begin{Verbatim}[fontsize=\fontsize{9}{14}\selectfont]
The decimal value 14 is represented as   01110
The decimal value 9 is represented as    01001

First we form the negative, that is, -9: 10111

Adding 14 to -9, we get                  01110
                                         10111

which results in the value 5.            00101
\end{Verbatim}
\end{minipage}

\bigskip

\noindent
Note again that the carry out is ignored.
\end{example}\vspace{-10pt}

\begin{example}
What happens when we add a number to itself (e.g., $x+x$)?

Let's assume for this example eight-bit codes, which would allow us to
represent\index{left shift} integers from $-$128 to 127.  Consider a
value for $x$, the integer 59, represented as 00111011.  If we add 59
to itself, we get the code 01110110.  Note that the bits have all
shifted to the left by one position.  Is that a curiosity, or will
that happen all the time as long as the sum $x+x$ is not too large to
represent with the available number of bits?

Using our positional notation, the number 59 is
\begin{equation*}
0\cdot 2^6+1\cdot 2^5+1\cdot 2^4+1\cdot 2^3+0\cdot 2^2+1\cdot 2^1+1\cdot 2^0
\end{equation*}
The sum $59+59$ is $2\cdot59$, which, in our representation, is
\begin{equation*}
2\cdot(0\cdot 2^6+1\cdot 2^5+1\cdot 2^4+1\cdot2^3+0\cdot2^2+1\cdot 2^1+1\cdot 2^0)
\end{equation*}
But that is nothing more than
\begin{equation*}
0\cdot 2^7+1\cdot 2^6+1\cdot 2^5+1\cdot 2^4+0\cdot 2^3+1\cdot 2^2+1\cdot 2^1
\end{equation*}
which shifts each digit one position to the left.  Thus, adding a
number to itself (provided there are enough bits to represent the
result) is equivalent to shifting the representation one bit position
to the left.
\end{example}\vspace{-10pt}

%2.5.2
\subsection{Sign-Extension}

It is often useful to represent a small number with fewer bits.  For
example, rather than\index{sign-extension} represent the value 5 as
0000000000000101, there are times when it\vadjust{\pagebreak} makes more sense
to use only six bits to represent the value 5: 000101.  There is little 
confusion, since we are all used to adding leading zeros without affecting the
value of a number.  A check for \$456.78 and a check for \$0000456.78
are checks having the same value.

\looseness=-1
What about negative representations?  We obtained the negative
representation from its positive counterpart by complementing the
positive representation and adding 1.  Thus, the representation  \lightbulb[-18pt]
for $-$5, given that 5 is represented as 000101, is 111011.
If 5 is represented as 0000000000000101, then the representation for
$-$5 is 1111111111111011.  In the same way that leading 0s do not
affect the value of a positive number, leading 1s do not affect the
value of a negative number.

In order to add representations of different lengths, it is first
necessary to represent them with the same number of bits.  For
example, suppose we wish to add the number 13 to $-$5, where 13 is
represented as 0000000000001101 and $-$5 is represented as 111011.  If
we do not represent the two values with the same number of bits, we
have

\begin{cctable}
~~0000000000001101\\
~~~~~~~~~~+~111011\\
\\[-17pt]\multicolumn{1}{@{}c@{}}{\phantom{+~}\hrulefill}\\[-2pt]
\end{cctable}

\noindent
When we attempt to perform the addition, what shall we do with the
missing bits in the representation for $-$5?  If we take the absence
of a bit to be a 0, then we are no longer adding $-$5 to 13.  On the
contrary, if we take the absence of bits to be 0s, we have changed the
$-$5 to the number represented as 0000000000111011, that is $+$59.
Not surprisingly, then, our result turns out to be the representation
for~72.

However, if we understand that a six-bit $-$5 and a 16-bit $-$5
differ only in the number of meaningless leading 1s, then we
first extend the value of $-$5 to 16 bits before we perform
the addition.  Thus, we have

\begin{cctable}
~~0000000000001101\\
+~1111111111111011\\
\\[-17pt]\multicolumn{1}{@{}c@{}}{\phantom{+~}\hrulefill}\\[-2pt]
~~0000000000001000
\end{cctable}

\noindent
and the result is ${+}8$, as we should expect.

The value of a positive number does not change if we
extend the sign bit 0 as many bit positions to the left as
desired.  Similarly, the value of a negative number does not
change by extending the sign bit 1 as many bit positions
to the left as desired. \lightbulb[-24pt] Since in both cases, it is the sign
bit that is extended, we refer to the operation as
{\em Sign-EXTension}, often abbreviated SEXT.  Sign-extension
is performed in order to be able to operate on representations
of different lengths.  It does not affect the values of the
numbers being represented.

%2.5.3
\subsection{Overflow}

Up to now, we have always insisted that the sum
of two integers be small enough\index{overflow}
to be represented by the available bits.  What happens if
such is not the case?

You are undoubtedly familiar with the odometer on the front dashboard
of your automobile.  It keeps track of how many miles your car has
been driven---but only up to a point.  In the old days, when the\vadjust{\pagebreak}
odometer registered 99992 and you drove it 100 miles, its new reading
became 00092.  A brand new car!  The problem, as you know, is that the
largest value the odometer could store was 99999, so the value 100092
showed up as 00092.  The carryout of the ten-thousands digit was lost.
(Of course, if you grew up in Boston, the carryout was not lost at
all---it was in full display in the rusted chrome all over the car.)

We say the odometer \textit{overflowed}. Representing 100092 as 00092
is unacceptable. As more and more cars lasted more than 100{,}000
miles, car makers felt the pressure to add a digit to the odometer.
Today, practically all cars overflow at 1,000,000 miles, rather than
100,000 miles.

The odometer provides an example of unsigned arithmetic.  The miles
you add are always positive miles.  The odometer reads 000129 and you
drive 50 miles.  The odometer now reads 000179.  Overflow is a
carry out of the leading digit.

In the case of signed arithmetic, or more particularly, 2's complement
arithmetic, overflow is a little more subtle.

Let's return to our five-bit 2's complement data type, which allowed
us to represent integers from $-$16 to $+$15.  Suppose we wish to add
$+$9 and $+$11.  Our arithmetic takes the following form:

\begin{cctable}
01001\\
01011\\
\seprule
10100
\end{cctable}

\noindent
Note that the sum is larger than $+$15, and therefore too large to
represent with our 2's complement scheme.  The fact that the number is
too large means that the number is larger than 01111, the largest
positive number we can represent with a five-bit 2's complement data
type.  Note that because our positive result was larger than $+$15, it
generated a carry into the leading bit position.  But this bit
position is used to indicate the sign of a value.  Thus detecting that
the result is too large is an easy matter.  Since we are adding two
positive numbers, the result must be positive.  Since the ALU has
produced a negative result, something must be wrong.  The thing that
is wrong is that the sum of the two positive numbers is too large to
be represented with the available bits.  We say that the result has
\textit{overflowed} the capacity of the representation.

Suppose instead, we had started with negative numbers, for example,
$-$12 and $-$6. In this case our arithmetic takes the following form:

\begin{cctable}
10100\\
11010\\
\seprule
01110
\end{cctable}

\enlargethispage{-2\baselineskip}

\noindent
Here, too, the result has overflowed the capacity of the machine,
since $-12+-6$ equals $-$18, which is ``more negative'' than $-$16,
the negative number with the largest allowable magnitude.  The ALU
obliges by producing a positive result.  Again, this is easy to detect
since the sum of two negative numbers cannot be positive.

\pagebreak

Note that the sum of a negative number and a positive number never
presents a problem.  Why is that? See Exercise 2.25.

%2.6
\section{Operations on Bits---Part II: Logical Operations}

We have seen that it is possible to perform arithmetic (e.g., add,
subtract) on values represented as binary patterns.  Another class of
operations useful to perform on binary patterns is the set
of {\em logical} operations.

%2.6.1
\subsection{A logical variable}

Logical operations operate on logical variables.  A logical variable
can have one of two values, 0 or 1.  The name {\em logical} is a
historical one; it comes from the fact that the two values 0 and 1 can
represent the two logical values {\em false} and {\em true}, but the
use of logical operations has traveled far from this original meaning.

There are several basic logic functions, and most ALUs perform all of them.

%2.6.2
\subsection{The AND Function}

AND is a {\em binary} logical function.  This means it requires two
pieces of input data.
\index{AND function}
Said another way, AND requires two source operands.  Each source
is a logical variable, taking the value 0 or 1.  The output of
AND is 1 only if both sources\index{logical variable}
have the value 1.  Otherwise, the output is 0.  We can think
of the AND operation as the ALL operation; that is, the
output is 1 only if ALL two inputs are 1. Otherwise, the
output is~0.

A convenient mechanism for representing the behavior of a logical
operation is the {\em truth table}.  A truth table consists of
$n+1$ columns and $2^n$ rows.  The first
\index{truth table}
$n$ columns correspond to the $n$ source operands.  Since each
source operand is a logical variable and can have one of two
values, there are $2^n$ unique values that these source operands
can have.  Each such set of values (sometimes called an {\em input
combination}) is represented as one row of the truth table.  The
final column in\index{input combination} the truth table
shows the output for each input combination.

In the case of a two-input AND function, the truth table has two
columns for source operands, and four ($2^2$) rows for unique
input combinations.

\bigskip

\begin{vtabular}{@{}cc|c@{}}
A & B & AND\\
\hline
0 & 0 & 0\\
0 & 1 & 0\\
1 & 0 & 0\\
1 & 1 & 1
\end{vtabular}

\bigskip

\noindent
We can apply the logical operation AND to two bit
patterns of $m$ bits each. This involves applying the operation
individually and independently to each pair of bits in the two source
operands.  For example, if $a$ and $b$ in Example~2.6 are 16-bit
patterns, then $c$ is the AND of $a$ and $b$. This operation
is often called a {\em bit-wise AND} because the operation is applied to each 
pair of bits individually and independently.

\newpage



\begin{example}
If $c$ is the AND of $a$ and $b$, where $a=0011101001101001$
and $b=0101100100100001$, what is~$c$?

We form the AND of $a$ and $b$ by bit-wise ANDing the two values.

That means individually ANDing each pair of bits $a_i$ and $b_i$ to form $c_i$. 
For example, since $a_0$=1 and $b_0$=1, $c_0$ is the AND of $a_0$ and $b_0$, 
which is~1.

Since $a_6$=1 and $b_6$=0, $c_6$ is the AND of $a_6$ and $b_6$, which is~0.

%% \pagebreak

\leftskip7.5pc

The complete solution for $c$ is

\begin{cctable}
a:  0011101001101001\\
b:  0101100100100001\\
\\[-17pt]\multicolumn{1}{@{}c@{}}{\phantom{a: }\hrulefill}\\[-2pt]
c:  0001100000100001\\[-8pt]
\end{cctable}
\end{example}
%% \endgroup

\medskip

\begin{example}
Suppose we have an eight-bit pattern, let's call it $A$, in
which the rightmost two bits have particular significance.
The computer could be asked to do one of four tasks depending
on the value stored in the two rightmost bits of $A$. Can we isolate those two bits?

Yes, we can, using a bit mask.  A {\em bit mask} is a binary
pattern that enables the bits of $A$ to be separated
into two parts---generally the part you care about and the part you
wish to ignore. In this case, the bit mask 00000011 ANDed with $A$
produces 0 in bit positions 7 through 2, and the original
values of bits 1 and 0 of $A$ in bit positions 1 and~0.  The bit mask is
said to {\em mask out} the values in bit positions 7 through~2.

If $A$ is 01010110, the AND of $A$ and the bit mask 00000011 is 00000010.
If $A$ is 11111100, the AND of $A$ and the bit mask 00000011 is 00000000.

That is, the result of ANDing any eight-bit pattern with the mask 00000011
is one of the four patterns 00000000, 00000001, 00000010, or 00000011.
The result of ANDing with the mask is to highlight the two bits that are relevant.
\end{example}
\lightbulb[-120pt]

\vspace{-6pt}

%2.6.3
\subsection{The OR Function}

OR is also a {\em binary} logical function.  It requires two source
operands, both of which are
\index{OR function}
logical variables.  The output of OR is 1 if any source has the
value 1.  Only if both sources are 0 is the output 0. We can
think of the OR operation as the ANY operation; that is, the
output is 1 if ANY of the two inputs are~1.

The truth table for a two-input OR function is

\begin{vtabular}{@{}ll|c@{}}
A & B & OR\\
\hline
0 & 0 & 0\\
0 & 1 & 1\\
1 & 0 & 1\\
1 & 1 & 1\\
\end{vtabular}

\noindent
In the same way that we applied the logical operation
AND to two $m$-bit patterns, we can apply the OR operation
bit-wise to two $m$-bit patterns.

\newpage

\bigskip

\begingroup
\makeatletter
\renewenvironment{example}{%
  \refstepcounter{example}%
  \begin{colourframed}%
    \def\@mathmargin{\z@}
    \let\enumargs\exenumargs
    \vspace*{-2\p@}%
    \rlap{\hspace*{29pc}\hbox{\hspace*{-\fboxsep}%
      \setlength{\fboxsep}{\z@}%
      {\colorbox{SPOThundred}{\vbox to 13\p@{%
          \vss\hbox to 7pc{\hss
          \color{white}\sansbold\fontsize{10}{12}\selectfont
          Example~\theexample\hss}\vss}}}}}%
  \normalfont\fontsize{9.5}{11}\selectfont
  \rightskip7.5pc\advance\rightskip by \fboxsep
  \leftskip3\p@
  \parindent1.5pc\@afterheading\@afterindentfalse
  \vspace*{-14.5\p@}}%
{\par\unskip\removelastskip\offinterlineskip\vspace{4\p@}\end{colourframed}}
\makeatother
\begin{example}
If $c$ is the OR of $a$ and $b$, where $a=0011101001101001$
and $b=0101100100100001$, as before, what is~$c$?

We form the OR of $a$ and $b$ by bit-wise ORing the two values.
That means individually ORing each pair of bits $a_i$ and $b_i$ to form $c_i$.
For example, since $a_0$=1 and $b_0$=1, $c_0$ is the OR of $a_0$ and $b_0$, 
which is~1.
Since $a_6$=1 and $b_6$=0, $c_6$ is the OR of $a_6$ and $b_6$, which is
also~1.

The complete solution for $c$ is

\begin{cctable}
a: 0011101001101001\\
b: 0101100100100001\\
\\[-17pt]\multicolumn{1}{@{}c@{}}{\phantom{a: }\hrulefill}\\[-2pt]
c: 0111101101101001
\end{cctable}

\noindent
Sometimes this OR operation is referred to as the
{\em inclusive-OR} in order to distinguish it from the
exclusive-OR function, which we will discuss momentarily.
\end{example}
\endgroup

\bigskip

%2.6.4
\subsection{The NOT Function}

NOT is a {\em unary} logical function.  This means it operates
on only one source operand.
\index{NOT function}
It is also known as the {\em complement} operation.  The output
is formed by complementing the input.  We sometimes say the
output is formed by {\em inverting} the input.  A 1 input results in
a 0 output.  A 0 input results in a 1 output.

The truth table for the NOT function is

\begin{vtabular}{@{}c|c@{}}
A & NOT\\
\hline
0 & 1\\
1 & 0
\end{vtabular}

\noindent
In the same way that we applied the logical operation
AND and OR to two $m$-bit patterns, we can apply the NOT
operation bit-wise to one $m$-bit pattern.  If $a$ is as
before, then $c$ is the NOT of~$a$.

\begin{cctable}
a:  0011101001101001\\
\\[-17pt]\multicolumn{1}{@{}c@{}}{\phantom{a: }\hrulefill}\\[-2pt]
c:  1100010110010110
\end{cctable}

\vspace{-12pt}

\enlargethispage{-3\baselineskip}

%2.6.5
\subsection{The Exclusive-OR Function}

Exclusive-OR, often abbreviated XOR, is a {\em binary} logical
function.  It, too, requires two
\index{exclusive-OR}
\index{XOR}
source operands, both of which are logical variables.
The output of XOR is 1 if one (but not both) of the two sources is 1.
The output of XOR is 0 if both sources are 1 or if neither source is 1.
In other words, the output of XOR is 1 if the two sources are different.
The output is 0 if the two sources are the same.

\pagebreak

The truth table for the XOR function is

\begin{vtabular}{@{}cc|c}
A & B & XOR\\
\hline
0 & 0 & 0\\
0 & 1 & 1\\
1 & 0 & 1\\
1 & 1 & 0
\end{vtabular}

\noindent
In the same way that we applied the logical operation
AND to two $m$-bit patterns, we can apply the XOR operation
bit-wise to two $m$-bit patterns.

\begin{example}
If $a$ and $b$ are 16-bit patterns as before, then $c$ (shown
here) is the XOR of $a$ and~$b$.

\begin{cctable}
a:  0011101001101001\\
b:  0101100100100001\\
\\[-17pt]\multicolumn{1}{@{}c@{}}{\phantom{a: }\hrulefill}\\[-2pt]
c:  0110001101001000
\end{cctable}

\noindent
Note the distinction between the truth table for
XOR shown here and the truth table for OR
\index{inclusive-OR}
shown earlier.  In the case of exclusive-OR, if both source
operands are 1, the output is 0.  That is, the output is 1
if the first operand is 1 but the second operand is not 1 or
if the second operand is 1 but the first operand is not 1.
The term {\em exclusive} is used because the output is 1
if {\em only} one of the two sources is 1.  The OR function,
on the other hand, produces an output 1 if only one of the two
sources is 1, or if both sources are 1.  Ergo, the name {\em inclusive-OR}.
\end{example}

\begin{example}
Suppose we wish to know if two patterns are identical.  Since the XOR
function produces a 0 only if the corresponding pair of bits
is identical, two patterns are identical if the output of
the XOR is all zeros.
\end{example}

%2.6.6
\subsection{DeMorgan's Laws}

There are two well-known relationships between AND functions and OR functions, 
known as DeMorgan's Laws.  One of them is illustrated in 
Figure~\ref{fig:demorgan}.  In Figure~\ref{fig:demorgan}a we complement each of
the two inputs A and B before using them as inputs to the AND function, and
also complement the result produced by the AND function. 
Figure~\ref{fig:demorgan}b shows the output of these functions if A=0 and B=1.
Figure~\ref{fig:demorgan}c summarizes by means of a truth table the behavior 
of the logic functions for all four input combinations of A and B.  Note that
the NOT of $A$ is represented as $\bar{A}$.
\index{DeMorgan's Law}

%Figure 2.x
\begin{figure}
\centerline{\includegraphics{demorgan.eps}}
\caption{DeMorgan's law}
\label{fig:demorgan}
\end{figure}

We can describe the behavior of these functions algebraically:
\begin{equation*}
\overline{\overline{A} \mbox{ AND } \overline{B}}=A \mbox{ OR } B
\end{equation*}
We can also state this behavior in English:

\medskip

``It is not the case that both $A$ and $B$ are false'' is equivalent
to saying ``At least one of $A$ and $B$ is true.''

\medskip

\noindent
This equivalence is known as one of two DeMorgan's Laws.  Question: Is there a 
similar result if one inverts both inputs to an OR function, and then 
inverts the output?

\vspace{-12pt}
%2.6.7
\subsection{The Bit Vector}

We have discussed above the AND, OR, and NOT functions performed on m-bit 
patterns, where each of the m bits is a logical value (0 or 1) and the 
operations are performed
bit-wise (i.e., individually and independently).  We have also discussed the
use of an m-bit bit mask, where our choice of 0 or 1 for each bit allows us to
isolate the bits we are interested in focusing on from the bits that don't 
matter.

An m-bit pattern, where each bit has a logical value (0 or 1) independent of 
the other bits is called a {\em bit vector}.  It is a convenient 
mechanism for identifying a property such that some of the bits identify the
presence of the property and other bits identify the absence of the property.

There are many uses for bit vectors.  The most common use is a bit mask,
as we saw in Example 2.7.  In that example, we had an 8 bit value,
and we wanted to focus on bit 1 and bit 0 of that value.  We did not care about 
the other bits.  Performing the AND of that value with the bit mask 00000011 
caused 
bit 7 through bit 2 to be ignored, resulting in the AND function producing 
00000000, 00000001, 00000010, or 00000011, depending on the values of bit 1
and bit 0.
The bit mask is a bit vector, where the property of each of the bits is
whether or not we care about that bit.  In Example 2.7, we only cared about
bit 1 and bit 0.  

Another use of a bit mask could involve a 16-bit 2's complement integer.  
Suppose the only thing we cared about was whether the integer was odd or even 
and whether it was positive or negative.  The bit vector 1000000000000001 has 
a 1 in bit 15 which is used to identify a number as positive or negative, and 
a 1 in bit 0 which is used to identify if the integer is odd or even.  If we 
perform the AND of 
this bit vector with a 16-bit 2's complement integer, we would get one of four 
results, depending on whether the integer was positive or negative and odd 
or even: 

\begin{colorverbatim}
		0000000000000000
		0000000000000001
		1000000000000000
		1000000000000001
\end{colorverbatim}

Another common use of bit vectors involves managing a complex system made 
up of several units, each of which is individually and independently either 
{\em busy} or {\em available}.  The system could be a manufacturing plant 
where each unit is a particular machine.  Or the system could be a taxicab 
network where each unit is a particular taxicab.  In both cases, it is 
important to identify which units are busy and which are available, so that 
work can be properly assigned.

Say we have $m$ such units.  We can keep track of these $m$ units with 
an $m$-bit
binary pattern we call a {\em bit vector}, where a bit is 1 if the unit is
free and 0 if the unit is busy.

\begingroup
\makeatletter
\renewenvironment{example}{%
  \refstepcounter{example}%
  \begin{colourframed}%
    \def\@mathmargin{\z@}
    \let\enumargs\exenumargs
    \vspace*{-2\p@}%
    \rlap{\hspace*{29pc}\hbox{\hspace*{-\fboxsep}%
      \setlength{\fboxsep}{\z@}%
      {\colorbox{SPOThundred}{\vbox to 13\p@{%
          \vss\hbox to 7pc{\hss
          \color{white}\sansbold\fontsize{10}{12}\selectfont
          Example~\theexample\hss}\vss}}}}}%
  \normalfont\fontsize{9.5}{11}\selectfont
  \rightskip7.5pc\advance\rightskip by \fboxsep
  \leftskip3\p@
  \parindent1.5pc\@afterheading\@afterindentfalse
  \vspace*{-14.5\p@}}%
{\par\unskip\removelastskip\offinterlineskip\vspace{4\p@}\end{colourframed}}
\makeatother
\begin{example}
Suppose we have eight machines that we want to monitor with respect to their
availability.  We can keep track of them with an eight-bit BUSYNESS bit vector,
where a bit is 1 if the unit is free and 0 if the unit is busy.  The bits are
labeled, from right to left, from 0~to~7.

The BUSYNESS bit vector 11000010  corresponds to the situation where only
units 7, 6, and 1 are free, and therefore available for work assignment.

Suppose work is assigned to unit 7.  We update our BUSYNESS bit vector
by performing the logical AND, where our two sources are the current
bit vector 11000010 and the bit mask 01111111.  The purpose of the bit
mask is to clear bit 7 of the BUSYNESS bit vector, while leaving alone the
values corresponding to all the other units.  The result is the
bit vector 01000010, indicating that unit 7 is now busy.

Suppose unit 5 finishes its task and becomes idle.  We can update the
BUSYNESS bit vector  by performing the logical OR of it with the bit mask
00100000.  The result is 01100010, indicating that unit 5 is now available.
\end{example}
\endgroup

%2.7
\section{Other Representations}

There are many other representations of information that are used in computers.
Two that are among the most useful are the floating point data type and ASCII
codes.  We will describe both in this section.  We will also
describe a notation called hexadecimal which, although not a data type,
is convenient for humans to use when dealing with long strings of 0s and 1s.

%2.7.1
\subsection{Floating Point Data Type (greater range, less precision)}
\label{sec:fp_data_type}

Most of the arithmetic in this book uses integer values.
The LC-3 computer, which you will start studying in Chapter~4, uses the 
16-bit, 2's complement integer data type.  That data type provides one bit 
to identify whether the number is positive or negative and 15 bits to 
represent the magnitude of the value.  With 16 bits used 
in this way, we can express integer values between $-$32,768 and $+$32,767, 
that is, between $-2^{15}$ and $+2^{15}-1$.  We say the {\em precision} of
our value is 15 bits, and the {\em range} is $2^{16}$.  As you learned
in high school chemistry class, sometimes we need to express much
larger numbers, but we do not require so many digits of precision.  In
fact, recall the value \mbox{$6.022 {\cdot} 10^{23}$}, which you may have been
required to memorize back then. The range needed to express the value 
$10^{23}$ is far greater than the largest value $2^{15} -1$ that is available 
with 16-bit 2's complement integers.  On the other hand, the 15 bits of 
precision available with 16-bit 2's complement integers is overkill.  We need
only enough bits to express four significant decimal digits (6022).

So we have a problem.  We have more bits than we need for precision.
But we don't have enough bits to represent the range.

The {\em floating point}\index{data type!floating point} data type solves
the problem.  Instead of using all the bits 
to represent the precision of a value, the floating
point data type allocates some of the bits to the range of values
(i.e., how big or how small) that can be expressed. The rest of the bits
(except for the sign bit) are used for precision.

Most ISAs today specify more than one floating point data type.
One of them, usually called {\em float}, consists of 32 bits,
allocated as follows:

\begin{Verbatim}[fontsize=\fontsize{9}{14}\selectfont]
         1 bit for the sign (positive or negative)
         8 bits for the range (the exponent field)
        23 bits for precision (the fraction field)
\end{Verbatim}
In most computers manufactured today, the format of the 32 bit floating point
data type is as shown in Figure~\ref{fig:floating_point}. 

%Figure 2
\begin{figure}
\centerline{\includegraphics{float.eps}}
\caption{The 32 bit floating point data type\label{fig:floating_point}}
\vspace{-12pt}
\end{figure}

\FloatBarrier

%2.7.1.1
\subsubsection{Normalized form}

Like Avogadro's Number that you learned years ago, the floating point data
type represents numbers expressed in Scientific Notation, and mostly  
in normalized form:

\vspace{.25 in}
$N = (-1)^S \times 1.fraction \times 2 ^ {exponent - 127}  ,   1 \leq exponent
\leq 254 $
\vspace{.25 in}

\noindent
where {\em S}, {\em fraction}, and {\em exponent} are the binary numbers 
in the fields of Figure~\ref{fig:floating_point}.


We say {\em mostly} in normalized form because (as noted in the above 
equation) the data type represents a floating point number in normalized form 
{\bf only} if the 8 bit exponent is restricted to the 254 unsigned integer 
values, 1~(00000001) through 254~(11111110).  

As you know,
with 8 bits, one can represent 256 values uniquely.  For the other two 
integer values 
0~(00000000) and 255~(11111111), the floating point data type does not 
represent normalized numbers.  We will explain what it does represent 
in Section~\ref{sec:infinity} and Section~\ref{sec:subnormal} below.

Recall again Avogadro's Number: (a) an implied $+$ sign (often left out when
the value is positive), (b) four decimal digits $6.022$ in normalized
form (one non-zero decimal digit $6$ before the decimal point) times (c) the 
radix $10$ raised to the power $23$.  The computer's 32-bit floating point 
data type, on the other hand, 
consists of (a) a sign bit (positive or negative), (b) $24$ binary digits in 
normalized form (one non-zero binary digit to the left of the binary point) 
times (c) the radix $2$ raised to an exponent expressed in $8$ bits.   

\enlargethispage{-2\baselineskip}

We determine the value of the 32-bit floating point representation shown in
Figure~\ref{fig:floating_point} by examining its three parts. 

The sign bit S is just a single binary digit, 0 for positive numbers, 1
for negative numbers. The formula contains the factor
$-1^S$, which evaluates to $+$1 if $S=0$, and $-1$ if~$S=1$.

The 23 fraction bits form the 24 bit quantity 1.fraction, where 
{\em normalized form} demands exactly one non-zero binary digit to the left of 
the binary point.  Since there exists only one non-zero binary digit (i.e., the
value 1), it is unnecessary to explicitly store that bit in our 32-bit floating 
point format.  In fact that is how we get 24 bits of precision, the $1$ to the
left of the binary point which is always present in normalized numbers and so
is unnecessary to store, and the 23 bits of fraction that are actually part of 
the 32-bit data type.

The 8 exponent bits are encoded in what we call an excess code, named for the
notion that one can get the *real* exponent by treating the code as an unsigned
integer and subracting the excess (sometimes called the {\em bias}.  In the 
case of the IEEE Floating Point that almost everyone uses, that 
excess (or bias) is $127$ for 32-bit floating point numbers.  Thus, an 
exponent field containing 10000110 corresponds to the exponent +7 (since 
10000110 represents the unsigned integer 134, from which we subtract 127, 
yielding +7).  An exponent field containing 00000111 corresponds to the 
exponent -120 (since 00000111 represents the unsigned integer 7, from which 
we subtract 127, yielding -120.  The exponent field gives us numbers as large
as $2^{+127}$ for an exponent field containing 254~(11111110) and as small as
$2^{-126}$ for an exponent field containing 1~(00000001). 

\enlargethispage{-3\baselineskip}

\pagebreak

\begingroup
\makeatletter
\renewenvironment{example}{%
  \refstepcounter{example}%
  \begin{colourframed}%
    \def\@mathmargin{\z@}
    \let\enumargs\exenumargs
    \vspace*{-2\p@}%
    \rlap{\hspace*{29pc}\hbox{\hspace*{-\fboxsep}%
      \setlength{\fboxsep}{\z@}%
      {\colorbox{SPOThundred}{\vbox to 13\p@{%
          \vss\hbox to 7pc{\hss
          \color{white}\sansbold\fontsize{10}{12}\selectfont
          Example~\theexample\hss}\vss}}}}}%
  \normalfont\fontsize{9.5}{11}\selectfont
  \rightskip7.5pc\advance\rightskip by \fboxsep
  \leftskip3\p@
  \parindent1.5pc\@afterheading\@afterindentfalse
  \vspace*{-14.5\p@}}%
{\par\unskip\removelastskip\offinterlineskip\vspace{4\p@}\end{colourframed}}
\makeatother
\begin{example}
\advance\abovedisplayskip by 4pt
\advance\belowdisplayskip by 4pt
What does the floating point data type
\begin{equation*}
\text{\color{seventyblack}\texttt{00111101100000000000000000000000}}
\end{equation*}
represent?

The leading bit is a 0.  This signifies a positive number.
The next eight bits represent the unsigned number 123.  If
we subtract 127, we get the actual exponent $-4$.  The last
23 bits are all 0.  Therefore the number being represented
is ${+}1.000000000000000000000000\cdot 2^{-4}$, which is
$\tfrac{1}{16}$.
\end{example}

\bigskip

\begin{example}
\advance\abovedisplayskip by 4pt
\advance\belowdisplayskip by 4pt
How is the number $-6\tfrac{5}{8}$ represented in the floating point data type?

First, we express $-6\tfrac{5}{8}$ as a binary number: $-110.101$.
\begin{equation*}
-(1\cdot 2^2+1\cdot 2^1+0\cdot 2^0 +1\cdot 2^{-1}+0\cdot 2^{-2}+1\cdot 2^{-3})
\end{equation*}
Then we normalize the value, yielding $-1.10101 \cdot 2^{2}$.

The sign bit is 1, reflecting the fact that $-6\tfrac{5}{8}$ is a negative
number. The exponent field contains 10000001, the unsigned number
129, reflecting the fact that the real exponent is $+$2 ($129 - 127 = +2$).
The fraction is the 23 bits of precision, after removing the leading
1.  That is, the fraction is 10101000000000000000000.  The result is
the number $-6\tfrac{5}{8}$, expressed as a floating point number:
\begin{equation*}
\text{\color{seventyblack}\texttt{1 10000001 10101000000000000000000}}
\end{equation*}
\end{example}

\begin{example}
The following three examples provide further illustrations of the
interpretation of the 32-bit floating point data type
according to the rules of the IEEE standard.
\begin{equation*}
\text{\texttt{0 10000011 00101000000000000000000} is }1.00101\cdot 2^4=18.5
\end{equation*}
The exponent field contains the unsigned number 131.
Since $131 - 127$ is 4, the exponent is $+$4.
Combining a 1 to the left of the binary point with the
fraction field to the right of the binary point yields
1.00101.  If we move the binary point four positions to
the right, we get 10010.1, which is 18.5.
\begin{equation*}
\text{\texttt{1 10000010 00101000000000000000000} is } {-}1 \cdot 1.00101
\cdot 2^3 = -9.25
\end{equation*}
The sign bit is 1, signifying a negative number.
The exponent is 130, signifying an exponent of $130 - 127$,
or $+$3.  Combining a 1 to the left of the binary point with
the fraction field to the right of the binary point
yields 1.00101.  Moving the binary point three positions to
the right, we get 1001.01, which is $-9.25$.
\begin{equation*}
\text{\texttt{0 11111110 11111111111111111111111} is }{\sim}2^{128}
\end{equation*}
The sign is $+$.  The exponent is $254 -127$, or $+$127.
Combining a 1 to the left of the binary point with the fraction
field to the right of the binary point yields
$1.11111111\ldots1$, which is approximately 2.
Therefore, the result is approximately $2^{128}$.
\end{example}

%2.7.1.2
\subsubsection{Infinities}\label{sec:infinity}

We noted above that the floating point data type represented numbers expressed
in Scientific Notation in normalized form provided the exponent field does
not contain 00000000 or 11111111.

If the exponent field contains 11111111, we use the floating point data type
to represent various things, among them the notion of infinity.  {\em Infinity}
is represented by the exponent field containing all 1s and the fraction field 
containing all 0s.  We represent positive infinity if the sign bit is 0, and 
negative infinity if the sign bit is 1.

%2.7.1.3
\subsubsection{Subnormal numbers}\label{sec:subnormal}

The smallest number that can be represented in normalized form is

\vspace{.25 in}
$N = 1.00000000000000000000000 \times 2 ^{- 126}$
\vspace{.25 in}

What about numbers smaller than $2 ^{-126}$ but larger than $0$?  We call
such numbers {\em subnormal numbers} because they can not be represented in
normalized form.  The largest subnormal number is

\vspace{.25 in}
$N = 0.11111111111111111111111 \times 2 ^{- 126}$
\vspace{.25 in}

The smallest subnormal number is

\vspace{.25 in}
$N = 0.00000000000000000000001 \times 2 ^{- 126}$, i.e., $2^{-23} \times 
2^{-126}$ which is $2^{-149}$.
\vspace{.25 in}

Note that the largest subnormal number is $2^{-126}$ minus $2^{-149}$.
Do you see why that is the case?

Subnormal numbers are numbers of the form:

\vspace{.25 in}
$N = (-1)^S \times 0.fraction \times 2 ^ {- 126}$
\vspace{.25 in}

We represent them with an exponent field of 00000000.  The fraction field
is represented in the same way as with normalized numbers.

That is, if the exponent field contains 00000000, the exponent is $-126$,
and the significant digits are obtained by starting with a
leading 0, followed by a binary point, followed by the 23
bits of the fraction field.  

\begin{example}
What number corresponds to the following floating point representation?

\begin{equation*}
\text{\color{seventyblack}\texttt{0 00000000 00001000000000000000000}}
\end{equation*}
Answer: The leading 0 means the number is positive. The next eight bits, 
a zero exponent, means the exponent is $-126$, and the bit to the left of the
binary point is $0$.  The last 23 bits form the number
0.00001000000000000000000, which equals $2^{-5}$. Thus, the
number represented is \allowbreak $2^{-5}\cdot 2^{-126}$, which is $2^{-131}$.
\end{example}

Including subnormal numbers allows very, very tiny numbers to be represented.

\endgroup


\bigskip
\bigskip

A detailed understanding of IEEE Floating Point Arithmetic is
well beyond what should be expected in this first course.  Our purpose in
including this section in the textbook is to at least let
you know that there is, in addition to 2's complement integers,
another very important data type available in almost all ISAs.
This data type is called {\em floating point}; it allows very
large and very tiny numbers to be expressed at the cost of
reducing the number of binary digits of precision.

\smallskip
\enlargethispage{-2\baselineskip}

%2.7.2
\subsection{ASCII Codes}

Another representation of information is the standard code that almost
all \index{ASCII codes} computer equipment manufacturers have agreed
to use for transferring characters between the main computer
processing unit and the input and output devices. That code is an
eight-bit code referred to as {\em ASCII}.  ASCII stands for American
Standard Code for Information Interchange.  It (ASCII) greatly
simplifies the interface between a keyboard manufactured by one
company, a computer made by another company, and a monitor made by a
third company.

Each key on the keyboard is identified by its unique ASCII code.  So,
for example, the digit 3 is represented as 00110011, the digit 2 is 00110010, 
the lowercase $e$ is 01100101, and
the ENTER key is 00001101.  The entire set of eight-bit ASCII
codes is listed in Figure~E.2 of Appendix~E. When you type a key on
the keyboard, the corresponding eight-bit code is stored and made
available to the computer.  Where it is stored and how it gets into
the computer is discussed in Chapter~9.

Most keys are associated with more than one code.  For example, the
ASCII code for the letter $E$ is 01000101, and the ASCII code for the
letter $e$ is 01100101.  Both are associated with the same key,
although in one case the Shift key is also depressed while in the
other case, it is not.

In order to display a particular character on the monitor,
the computer must transfer the ASCII code for that character
to the electronics associated with the monitor.  That, too,
is discussed in Chapter~9.

%\enlargethispage{-2\baselineskip}

%2.7.3
\subsection{Hexadecimal Notation}

We have seen that information can be represented as
2's complement integers, as bit vectors, in
\index{hexadecimal notation}
floating point format, or as an ASCII code.  There are other
representations also, but we will leave them for another book.
However, before we leave this topic, we would like to introduce
you to a representation that is used more as a convenience for
humans than as a data type to support operations being performed
by the computer.  This is the {\em hexadecimal} notation.  As
we will see, it evolves nicely from the positional binary notation
and is useful for dealing with long strings of binary digits
without making errors.

It will be particularly useful in dealing with the LC-3 where
16-bit binary strings will be encountered often.

An example of such a binary string is

\begin{cctable}
0011110101101110
\end{cctable}

\noindent
\looseness=1
Let's try an experiment. Cover the preceding 16-bit binary string
of 0s and 1s with one hand, and try to write it down from
memory. How did you do?  Hexadecimal notation is about being
able to do this without making mistakes.  We shall see~how.

In general, a 16-bit binary string takes the form
\begin{equation*}
a_{15}\ a_{14}\ a_{13}\ a_{12}\ a_{11}\ a_{10}\
  a_9\ a_8\ a_7\ a_6\ a_5\ a_4\ a_3\ a_2\ a_1\ a_0
\end{equation*}
where each of the bits $a_i$ is either 0 or~1.

If we think of this binary string as an unsigned integer, its
value can be computed as
\begin{align*}
&a_{15}\cdot 2^{15}+a_{14}\cdot 2^{14}+a_{13}\cdot 2^{13}+
  a_{12}\cdot 2^{12}+a_{11}\cdot 2^{11}+a_{10}\cdot 2^{10}\\
&\quad +\; a_9\cdot 2^9+a_8\cdot 2^8+a_7\cdot 2^7+a_6\cdot 2^6+a_5\cdot 2^5+a_4\cdot 2^4+a_3\cdot 2^3\\
&\quad +\; a_2\cdot 2^2+a_1\cdot 2^1+a_0\cdot 2^0
\end{align*}

We can factor $2^{12}$ from the first four terms,
$2^8$ from the second four terms, $2^4$ from the third set
of four terms, and $2^0$ from the last four terms, yielding
\begin{align*}
&2^{12}(a_{15}\cdot 2^3+a_{14}\cdot 2^2+a_{13}\cdot 2^1+a_{12}\cdot 2^0)\\
&\quad + 2^8(a_{11}\cdot 2^3+a_{10}\cdot 2^2 + a_9\cdot 2^1+a_8\cdot 2^0)\\
&\quad+ 2^4(a_7\cdot 2^3+a_6\cdot 2^2+ a_5\cdot 2^1+a_4\cdot 2^0)\\
&\quad + 2^0(a_3\cdot 2^3+a_2\cdot 2^2+a_1\cdot 2^1+a_0\cdot 2^0)
\end{align*}
Note that the largest value inside a set of parentheses
is 15, which would be the case if each of the four bits
is 1.  If we replace what is inside each square bracket by a
symbol representing its value (from 0 to 15), and we replace
$2^{12}$ by its equivalent $16^3$, $2^8$ by $16^2$, $2^4$ by
$16^1$, and $2^0$ by $16^0$, we have
\begin{equation*}
h_3\cdot 16^3+h_2\cdot 16^2+h_1\cdot 16^1+h_0\cdot 16^0
\end{equation*}
where $h_3$, for example, is a symbol representing
\begin{equation*}
a_{15}\cdot 2^3+a_{14}\cdot 2^2+a_{13}\cdot 2^1+a_{12}\cdot 2^0
\end{equation*}
Since the symbols must represent values from 0 to 15,
we assign symbols to these values as follows: 0, 1, 2, 3, 4, 5,
6, 7, 8, 9, A, B, C, D, E, F.  That is, we represent 0000 with
the symbol 0, 0001 with the symbol $1,\ldots\,1001$ with 9, 1010
with A, 1011 with B, $\ldots$ 1111 with F. The resulting notation
is called hexadecimal, or\break base~16.

So, for example, if the hex digits E92F represent a 16-bit 2's
complement integer, is the value of that integer positive or
negative?  How do you know?

Now, then, what is this hexadecimal representation good for,
anyway?  It seems like just another way to represent a number
without adding any benefit.  Let's return to the exercise where
you tried to write from memory the string

\begin{cctable}
0011110101101110
\end{cctable}
If we had first broken the string at four-bit boundaries

\begin{cctable}
0011\quad 1101\quad  0110\quad 1110
\end{cctable}
\noindent
and then converted each four-bit string to its equivalent hex digit
\begin{cctable}
3\quad D\quad 6\quad E
\end{cctable}
\noindent
it would have been no problem to jot down (with the string covered)
3D6E.

\enlargethispage{-4\baselineskip}

In summary, although hexadecimal notation can be used to perform base-16 
arithmetic, it is mainly used as a convenience
for humans.  It can be used to represent binary strings that are
integers or floating point numbers or sequences of ASCII codes,
or bit vectors.  It simply reduces the number of digits by a
factor of 4, where each digit is in hex ($0,1,2,\ldots$ F)
instead of binary $(0,1)$. The usual result is far fewer
copying errors due to too many 0s and 1s.

\begin{exercises}
\vspace{-12pt}
\item[2.1] Given $n$ bits, how many distinct combinations of the $n$
  bits exist?

\item[2.2] There are 26 characters in the alphabet we use for writing
  English. What is the least number of bits needed to give each
  character a unique bit pattern? How many bits would we need to
  distinguish between upper- and lowercase versions of all 26
  characters?

\item[2.3]
  \begin{enumerate}
  \item[a.] Assume that there are about 400 students in your class.
    If every student is to be assigned a unique bit pattern, what is
    the minimum number of bits required to do this?

  \item[b.] How many more students can be admitted to the class
    without requiring additional bits for each student's unique bit
    pattern?
  \end{enumerate}

\item[2.4] Given $n$ bits, how many unsigned integers can be
  represented with the $n$ bits? What is the range of these integers?

\item[2.5] Using 5 bits to represent each number, write the representations
of 7 and $-7$ in 1's complement,
signed magnitude, and 2's complement integers.

\item[2.6] Write the 6-bit 2's complement representation of~$-32$.

\item[2.7] Create a table showing the decimal values of all
4-bit 2's complement numbers.

\item[2.8]
\begin{enumerate}
\item[a.] What is the largest positive number one can represent in an 8-bit
2's complement code? Write your result in binary and decimal.

\item[b.] What is the greatest magnitude negative number one can represent
in an 8-bit 2's complement code? Write your result in binary
and decimal.

\item[c.] What is the largest positive number one can
represent in $n$-bit 2's complement code?

\item[d.] What is the greatest magnitude negative
number one can represent in $n$-bit 2's complement code?
\end{enumerate}

\item[2.9] How many bits are needed to represent Avogadro's
number $(6.02\cdot 10^{23})$ in 2's complement binary representation?

\item[2.10] Convert the following 2's complement binary numbers to decimal.
\begin{enumerate}[a.]
\item[a.] {\tt 1010}
\item[b.] {\tt 01011010}
\item[c.] {\tt 11111110}
\item[d.] {\tt 0011100111010011}
\end{enumerate}

\item[2.11] Convert these decimal numbers to 8-bit 2's
complement binary numbers.

\begin{enumerate}[d.]
\item[a.] 102
\item[b.] 64
\item[c.] 33
\item[d.] $-$128
\item[e.] 127
\end{enumerate}

\newpage

\item[2.12] If the last digit of a 2's complement binary number is 0, then
the number is even. If the last two digits of a 2's complement
binary number are 00 (e.g., the binary number 01100), what
does that tell you about the number?

\item[2.13] Without changing their values, convert the following 2's complement
binary numbers into 8-bit 2's complement numbers.
\begin{enumerate}[d.]
\item[a.] {\tt 1010}\qquad\quad\quad\qquad \textit{c.}\quad {\tt 1111111000}
\item[b.] {\tt 011001}\qquad\quad\quad\qquad\hspace*{-1pc}\textit{d.}\quad  {\tt 01}
\end{enumerate}

\item[2.14] Add the following bit patterns. Leave
your results in binary form.
\begin{enumerate}[d.]
\item[a.] {\tt 1011 + 0001}
\item[b.] {\tt 0000 + 1010}
\item[c.] {\tt 1100 + 0011}
\item[d.] {\tt 0101 + 0110}
\item[e.] {\tt 1111 + 0001}
\end{enumerate}

\item[2.15] It was demonstrated in Example~2.5 that shifting a binary number
one bit to the left is equivalent to multiplying the number by~2.
What operation is performed when
a binary number is shifted one bit to the right?

\item[2.16] Write the results of the following additions as both
8-bit binary and decimal numbers. For each part, use standard
binary addition as described in Section~2.5.1.
\begin{enumerate}[c.]
\item[a.] Add the 1's complement representation of 7 to the
1's complement representation of ${-}7.$
\item[b.] Add the signed magnitude representation of 7 to the signed
  magnitude representation of ${-}7$.
\item[c.] Add the 2's complement representation of 7 to the 2's
  complement representation of ${-}7$.
\end{enumerate}

\item[2.17] Add the following 2's complement binary numbers.
Also express the answer in decimal.
\begin{enumerate}[d.]
\item[a.] {\tt 01 + 1011}
\item[b.] {\tt 11 + 01010101}
\item[c.] {\tt 0101 + 110}
\item[d.] {\tt 01 + 10}
\end{enumerate}

\item[2.18] Add the following unsigned binary numbers. Also,
express the answer in decimal.
\begin{enumerate}[d.]
\item[a.] {\tt 01 + 1011}
\item[b.] {\tt 11 + 01010101}
\item[c.] {\tt 0101 + 110}
\item[d.] {\tt 01 + 10}
\end{enumerate}

\item[2.19] Express the negative value $-$27 as a 2's complement integer,
using eight bits. Repeat, using 16 bits. Repeat, using 32 bits. What does
this illustrate with respect to the properties of sign
extension as they pertain to 2's complement representation?

\item[2.20] The following binary numbers are 4-bit 2's complement binary numbers.
Which of the following operations generate overflow? Justify
your answer by translating the operands and results into decimal.
\begin{enumerate}[d.]
\item[a.] {\tt 1100 + 0011}\qquad\qquad\quad \textit{d.}\quad {\tt 1000 - 0001}
\item[b.] {\tt 1100 + 0100}\qquad\qquad\quad \textit{e.}\quad {\tt 0111 + 1001}
\item[c.] {\tt 0111 + 0001}
\end{enumerate}

\item[2.21] Describe what conditions indicate overflow has occurred
when two 2's complement numbers are added.

\item[2.22] Create two 16-bit 2's complement integers such
that their sum causes an overflow.

\item[2.23] Describe what conditions indicate overflow has
occurred when two unsigned numbers are added.

\item[2.24] Create two 16-bit unsigned integers such that their sum causes an overflow.

\item[2.25] Why does the sum of a negative 2's complement number and a
positive 2's complement number never generate an overflow?

\item[2.26] You wish to express ${-}64$ as a 2's complement number.
\begin{enumerate}[c.]
\item[a.] How many bits do you need (the minimum number)?
\item[b.] With this number of bits, what is the largest positive
  number you can represent? (Please give answer in both decimal and
  binary).
\item[c.] With this number of bits, what is the largest unsigned
  number you can represent? (Please give answer in both decimal and
  binary).
\end{enumerate}

\item[2.27] The LC-3, a 16-bit machine adds the two 2's
complement numbers 0101010101010101 and 0011100111001111,
producing 1000111100100100. Is there a problem here? If
yes, what is the problem? If no, why not?

\item[2.28] When is the output of an AND operation equal to~1?

\item[2.29] Fill in the following truth table for a one-bit AND operation.

\begin{center}
\medskip
{\fontsize{9}{13pt}\selectfont%
\begin{tabular}{cc|c}
\hline
\rule{0pt}{10pt}X\rule{0pt}{10pt} & Y & X AND Y\\
\hline
\rule{0pt}{10pt}0\rule{0pt}{10pt} & 0\\
0 & 1\\
1 & 0\\
1 & 1\\
\hline
\end{tabular}}
\bigskip
\end{center}

\item[2.30] Compute the following. Write your results in binary.
\begin{enumerate}[d.]
\item[a.] {\tt 01010111 AND 11010111}
\item[b.] {\tt 101 AND 110}
\item[c.] {\tt 11100000 AND 10110100}
\item[d.] {\tt 00011111 AND 10110100}
\item[e.] {\tt (0011 AND 0110) AND 1101}
\item[f.] {\tt 0011 AND (0110 AND 1101)}
\end{enumerate}

\item[2.31] When is the output of an OR operation equal to~1?

\item[2.32] Fill in the following truth table for a one-bit OR operation.

\begin{center}
\vspace{-6pt}
{\fontsize{9}{13pt}\selectfont%
\begin{tabular}{cc|c}
\hline
\rule{0pt}{10pt}X\rule{0pt}{10pt} & Y & X OR Y\\
\hline
\rule{0pt}{10pt}0\rule{0pt}{10pt} & 0\\
0 & 1\\
1 & 0\\
1 & 1\\
\hline
\end{tabular}}
\vspace{-6pt}
\end{center}

\item[2.33] Compute the following:
\begin{enumerate}[f.]
\item[a.] {\tt 01010111 OR 11010111}
\item[b.] {\tt 101 OR 110}
\item[c.] {\tt 11100000 OR 10110100}
\item[d.] {\tt 00011111 OR 10110100}
\item[e.] {\tt (0101 OR 1100) OR 1101}
\item[f.] {\tt 0101 OR (1100 OR 1101)}
\end{enumerate}

\item[2.34] Compute the following:
\begin{enumerate}[d.]
\item[a.] {\tt NOT(1011) OR NOT(1100)}
\item[b.] {\tt NOT(1000 AND (1100 OR 0101))}
\item[c.] {\tt NOT(NOT(1101))}
\item[d.] {\tt (0110 OR 0000) AND 1111}
\end{enumerate}

\item[2.35] In Example 2.11, what are the masks used for?

\item[2.36] Refer to Example 2.11 for the following questions.

\begin{enumerate}[a.]
\item[a.] What mask value and what operation would one use to indicate
  that machine 2 is busy?
\item[b.] What mask value and what operation would one use to indicate
  that machines 2 and 6 are no longer busy? (Note: This can be done
  with only one operation.)
\item[c.] What mask value and what operation would one use to indicate
  that all machines are busy?
\item[d.] What mask value and what operation would one use to indicate
  that all machines are idle?
\item[e.] Using the operations discussed in this chapter, develop a 
  procedure to isolate the status bit of machine 2 as the sign bit. 
  For example, if the BUSYNESS pattern is 01011100, then the output 
  of this procedure is 10000000.  If the BUSYNESS pattern is 01110011, 
  then the output is 00000000. In general, if the BUSYNESS pattern is:
\smallskip

\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
  \rule{0pt}{10pt}b7\rule{0pt}{10pt} & b6 & b5 & b4 & b3 & b2 & b1 & b0 \\ \hline
\end{tabular}

\smallskip

\noindent
the output is:

\smallskip

\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
  \rule{0pt}{10pt}b2\rule{0pt}{10pt} & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
\end{tabular}.

\smallskip

\noindent
{\em Hint:} What happens when you ADD a bit pattern to itself?
\end{enumerate}

\removelastskip
\pagebreak


\item[2.37] %2.34
If $n$ and $m$ are both 4-bit 2's complement numbers, and $s$
is the 4-bit result of adding them together, how can we
determine, using only the logical operations described in Section
2.6, if an overflow occurred during the addition? Develop
a ``procedure'' for doing so. The inputs\break to the procedure
are $n$, $m$, and $s$, and the output will be a bit pattern\break of
all zeros (0000) if no overflow occurred and 1000 if an overflow\break
did occur.

\smallskip

\item[2.38] %2.35
If $n$ and $m$ are both 4-bit unsigned numbers, and $s$ is the
4-bit result of adding them together, how can we determine,
using only the logical operations described in Section 2.6, if
an overflow occurred during the\break
addition? Develop a ``procedure''
for doing so. The inputs to the procedure are $n$, $m$,
and $s$, and the output will be a bit pattern of\break
all zeros (0000) if no overflow occurred and 1000 if an overflow\break
did occur.

\smallskip

\item[2.39] %2.36
Write IEEE floating point representation of the following
decimal~numbers.

\begin{enumerate}[a.]
\item[a.] $3.75$
\item[b.] $-55 \frac{23}{64}$
\item[c.] $3.1415927$
\item[d.] $64{,}000$
\end{enumerate}

\smallskip

\item[2.40] Write the decimal equivalents for these IEEE floating point\break numbers.
\begin{enumerate}[d.]
\item[a.] {\tt 0 10000000 00000000000000000000000}
\item[b.] {\tt 1 10000011 00010000000000000000000}
\item[c.] {\tt 0 11111111 00000000000000000000000}
\item[d.] {\tt 1 10000000 10010000000000000000000}
\end{enumerate}

\smallskip

\item[2.41]
\begin{enumerate}
\item[a.] What is the largest exponent the IEEE standard allows for
a 32-bit floating point number?
\item[b.] What is the smallest exponent the IEEE standard allows for
a 32-bit floating point number?
\end{enumerate}

\smallskip

\item[2.42] A computer programmer wrote a program that adds
two numbers. The programmer ran the program and observed that
when 5 is added to 8,\break
the result is the character $m$.
Explain why this program is behaving erroneously.

\smallskip

\item[2.43] Translate the following ASCII codes into strings of characters by
interpreting each group of eight bits as an ASCII character.
\begin{enumerate}[d.]
\item[a.] x48656c6c6f21
\item[b.] x68454c4c4f21
\item[c.] x436f6d70757465727321
\item[d.] x4c432d32
\end{enumerate}

\pagebreak

\item[2.44] What operation(s) can be used to convert the binary representation
for 3 (i.e., 0000 0011) into the ASCII representation for 3 (i.e.,
0011 0011)? What about the binary 4 into the ASCII 4? What about any
digit?

\smallskip

\item[2.45] Convert the following unsigned binary numbers to hexadecimal.
\begin{enumerate}[d.]
\item[a.] {\tt 1101 0001 1010 1111}
\item[b.] {\tt 001 1111}
\item[c.] {\tt 1}
\item[d.] {\tt 1110 1101 1011 0010}
\end{enumerate}

\smallskip

\item[2.46] Convert the following hexadecimal numbers to binary.
\begin{enumerate}[e.]
\item[a.] x10
\item[b.] x801
\item[c.] xF731
\item[d.] x0F1E2D
\item[e.] xBCAD
\end{enumerate}

\smallskip

\item[2.47] Convert the following hexadecimal representations of 2's complement
binary numbers to decimal numbers.
\begin{enumerate}[d.]
\item[a.] xF0
\item[b.] x7FF
\item[c.] x16
\item[d.] x8000
\end{enumerate}

\smallskip

\item[2.48] Convert the following decimal numbers to hexadecimal
representations of 2's complement numbers.
\begin{enumerate}[d.]
\item[a.] 256
\item[b.] 111
\item[c.] 123,456,789
\item[d.] $-$44
\end{enumerate}

\smallskip

\item[2.49] Perform the following additions. The corresponding 16-bit
binary numbers are in 2's complement notation. Provide your
answers in hexadecimal.
\begin{enumerate}[e.]
\item[a.] x025B $+$ x26DE
\item[b.] x7D96 $+$ xF0A0
\item[c.] xA397 $+$ xA35D
\item[d.] x7D96 $+$ x7412
\item[e.] What else can you say about the answers to parts~$c$ and~$d$?
\end{enumerate}

\smallskip

\item[2.50] Perform the following logical operations. Express
your answers in hexadecimal notation.
\begin{enumerate}
\item[a.] x5478 AND xFDEA
\item[b.] xABCD OR x1234
\item[c.] NOT((NOT(xDEFA)) AND (NOT(xFFFF)))
\item[d.] x00FF XOR x325C
\end{enumerate}

\pagebreak

\item[2.51] What is the hexadecimal representation of the following numbers?
\begin{enumerate}[c.]
\item[a.] 25,675
\item[b.] 675.625 (that is, $675\tfrac{5}{8}$), in the IEEE
754 floating point standard
\item[c.] The ASCII string: Hello
\end{enumerate}

\item[2.52] Consider two hexadecimal numbers: x434F4D50 and x55544552.
  What values do they represent for each of the five data types shown?

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\rule{0pt}{10pt}& x434F4D50 & x55544552\\\hline
Unsigned binary & \rule{0pt}{10pt} & \\\hline
1's complement & \rule{0pt}{10pt} &\\\hline
2's complement & \rule{0pt}{10pt} &\\\hline
IEEE 754 floating point & \rule{0pt}{10pt} &\\\hline
ASCII string & \rule{0pt}{10pt} & \\
\hline
\end{tabular}
\smallskip
\end{center}

\item[2.53] Fill in the truth table for the equations given.
The first line is done as an example.
\begin{align*}
Q_{1} &= \text{NOT(A AND B)}\\
Q_2 &= \text{NOT(NOT(A) AND NOT(B))}
\end{align*}

\begin{center}
\fontsize{9}{13}\selectfont
\begin{tabular}{cc|cc}
\hline
\rule{0pt}{10pt}A\rule{0pt}{10pt} & B & $Q_1$ & $Q_2$\\
\hline
\rule{0pt}{10pt}0\rule{0pt}{10pt} & 0 & 1 & 0\\ & & & \\ & & & \\ & &
& \\ & & & \\ & & & \\ & & & \\
\hline
\end{tabular}
\smallskip
\end{center}

Express $Q_2$ another way.

\item[2.54] Fill in the truth table for the equations given.
The first line is done as an example.
\begin{align*}
Q_1 &=\text{NOT(NOT(X) OR (X AND Y AND Z))}\\
Q_2 &=\text{NOT((Y OR Z) AND (X AND Y AND Z))}
\end{align*}

\begin{center}
\fontsize{9}{13}\selectfont
\begin{tabular}{ccc|cc}
\rule{0pt}{10pt}X\rule{0pt}{10pt} & Y & Z & $Q_1$ & $Q_2$\\
\hline
\rule{0pt}{10pt}0\rule{0pt}{10pt} & 0 & 0 & 0 & 1\\ & & & & \\ & & & &
\\ & & & & \\ & & & & \\ & & & & \\ & & & & \\ & & & & \\
\hline
\end{tabular}
\vspace*{1pt}
\end{center}

\item[2.55] We have represented numbers in base-2 (binary)
and in base-16 (hex). We are now ready for unsigned base-4,
which we will call quad numbers. A quad digit can be 0, 1, 2, or~3.
\begin{enumerate}[g.]
\item[a.] What is the maximum unsigned decimal value that one
can represent with 3 quad digits?
\item[b.] What is the maximum unsigned decimal value that one
can represent with $n$ quad digits (Hint: your answer should be a function of~$n$)?
\item[c.] Add the two unsigned quad numbers: 023 and 221.
\item[d.] What is the quad representation of the decimal
number~42?
\item[e.] What is the binary representation of the unsigned quad number 123.3?
\item[f.] Express the unsigned quad number 123.3 in IEEE
floating point format.
\item[g.] Given a black box which takes $m$ quad digits as
input and produces one quad digit for output, what is the
maximum number of unique functions this black box can implement?
\end{enumerate}

\item[2.56] Define a new 8-bit floating point format with 1
sign bit, 4 bits of exponent, using an excess-7 code (that is,
the bias is 7), and 3 bits of fraction. If xE5 is the bit
pattern for a number in this 8-bit floating point format, what value
does it have? (Express as a decimal number.)
\end{exercises}
\end{document}
