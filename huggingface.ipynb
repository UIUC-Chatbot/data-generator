{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ijosh\\PycharmProjects\\UIUC_Chatbot\\huggingface.ipynb Cell 2\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelWithLMHead, AutoTokenizer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmrm8488/t5-base-finetuned-question-generation-ap\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelWithLMHead\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmrm8488/t5-base-finetuned-question-generation-ap\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_question\u001b[39m(answer, context, max_length\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:609\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    607\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[0;32m    608\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 609\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1805\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1802\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1803\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1805\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[0;32m   1806\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   1807\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   1808\u001b[0m     init_configuration,\n\u001b[0;32m   1809\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[0;32m   1810\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1811\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1812\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1813\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1950\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1950\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[0;32m   1951\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   1952\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   1953\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1954\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1955\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:134\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m extra_tokens \u001b[39m!=\u001b[39m extra_ids:\n\u001b[0;32m    128\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    129\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBoth extra_ids (\u001b[39m\u001b[39m{\u001b[39;00mextra_ids\u001b[39m}\u001b[39;00m\u001b[39m) and additional_special_tokens (\u001b[39m\u001b[39m{\u001b[39;00madditional_special_tokens\u001b[39m}\u001b[39;00m\u001b[39m) are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m provided to T5Tokenizer. In this case the additional_special_tokens must include the extra_ids\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m         )\n\u001b[1;32m--> 134\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    135\u001b[0m     vocab_file,\n\u001b[0;32m    136\u001b[0m     tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[0;32m    137\u001b[0m     eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[0;32m    138\u001b[0m     unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[0;32m    139\u001b[0m     pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[0;32m    140\u001b[0m     extra_ids\u001b[39m=\u001b[39;49mextra_ids,\n\u001b[0;32m    141\u001b[0m     additional_special_tokens\u001b[39m=\u001b[39;49madditional_special_tokens,\n\u001b[0;32m    142\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    143\u001b[0m )\n\u001b[0;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_fast.py:119\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    120\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "\n",
    "def get_question(answer, context, max_length=64):\n",
    "  input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
    "  features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "  output = model.generate(input_ids=features['input_ids'], \n",
    "               attention_mask=features['attention_mask'],\n",
    "               max_length=max_length)\n",
    "\n",
    "  return tokenizer.decode(output[0])\n",
    "\n",
    "context = \"Manuel has created RuPERTa-base with the support of HF-Transformers and Google\"\n",
    "answer = \"Manuel\"\n",
    "\n",
    "get_question(answer, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ijosh\\PycharmProjects\\UIUC_Chatbot\\huggingface.ipynb Cell 3\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m T5ForConditionalGeneration, T5TokenizerFast\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m hfmodel \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mThomasSimonini/t5-end2end-question-generation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmrm8488/t5-base-finetuned-question-generation-ap\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m text\u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\u001b[39mLet\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms begin by recalling that the control unit consists of three parts: a \u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mhigh-level FSM that controls instruction processing, a program counter~(PC)\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mregister that holds the address of the next instruction to be executed,\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mand an instruction register~(IR) that holds the current instruction as\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mit executes.\u001b[39m\u001b[39m'''\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ijosh/PycharmProjects/UIUC_Chatbot/huggingface.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_model\u001b[39m(input_string, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgenerator_args):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:609\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    607\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[0;32m    608\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 609\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1805\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1802\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1803\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1805\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[0;32m   1806\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   1807\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   1808\u001b[0m     init_configuration,\n\u001b[0;32m   1809\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[0;32m   1810\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1811\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1812\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1813\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1950\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1950\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[0;32m   1951\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   1952\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m   1953\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1954\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1955\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:134\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m extra_tokens \u001b[39m!=\u001b[39m extra_ids:\n\u001b[0;32m    128\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    129\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBoth extra_ids (\u001b[39m\u001b[39m{\u001b[39;00mextra_ids\u001b[39m}\u001b[39;00m\u001b[39m) and additional_special_tokens (\u001b[39m\u001b[39m{\u001b[39;00madditional_special_tokens\u001b[39m}\u001b[39;00m\u001b[39m) are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    130\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m provided to T5Tokenizer. In this case the additional_special_tokens must include the extra_ids\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m         )\n\u001b[1;32m--> 134\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    135\u001b[0m     vocab_file,\n\u001b[0;32m    136\u001b[0m     tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[0;32m    137\u001b[0m     eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[0;32m    138\u001b[0m     unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[0;32m    139\u001b[0m     pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[0;32m    140\u001b[0m     extra_ids\u001b[39m=\u001b[39;49mextra_ids,\n\u001b[0;32m    141\u001b[0m     additional_special_tokens\u001b[39m=\u001b[39;49madditional_special_tokens,\n\u001b[0;32m    142\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    143\u001b[0m )\n\u001b[0;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_fast.py:119\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    120\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "hfmodel = T5ForConditionalGeneration.from_pretrained(\"ThomasSimonini/t5-end2end-question-generation\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "\n",
    "text= '''Let's begin by recalling that the control unit consists of three parts: a \n",
    "high-level FSM that controls instruction processing, a program counter~(PC)\n",
    "register that holds the address of the next instruction to be executed,\n",
    "and an instruction register~(IR) that holds the current instruction as\n",
    "it executes.'''\n",
    "\n",
    "def run_model(input_string, **generator_args):\n",
    "  generator_args = {\n",
    "  \"max_length\": 256,\n",
    "  \"num_beams\": 4,\n",
    "  \"length_penalty\": 1.5,\n",
    "  \"no_repeat_ngram_size\": 3,\n",
    "  \"early_stopping\": True,\n",
    "  }\n",
    "  input_string = \"generate questions: \" + input_string + \" </s>\"\n",
    "  input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "  res = hfmodel.generate(input_ids, **generator_args)\n",
    "  output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "  output = [item.split(\"<sep>\") for item in output]\n",
    "  return output\n",
    "  \n",
    "run_model(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
