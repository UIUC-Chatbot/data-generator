"eval_batch_size":  32
"learning_rate":    2e-05
"model_name":       cross-encoder/ms-marco-MiniLM-L-6-v2
"train_batch_size": 32
"warmup_steps":     1000
"weight_decay":     0.01

Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████| 90.9M/90.9M [00:04<00:00, 19.6MB/s]
Using 16bit native Automatic Mixed Precision (AMP)
/Users/joshuamin/miniconda3/envs/fine/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:118: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn("torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.")
GPU available: True (mps), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/Users/joshuamin/miniconda3/envs/fine/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /Users/joshuamin/Desktop/Internships/UIUC_chatbot_data_generator/output/cross-encoder-ms-marco-MiniLM-L-6-v2-2022-11-26_16-24-18 exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Traceback (most recent call last):
  File "/Users/joshuamin/Desktop/Internships/UIUC_chatbot_data_generator/filtering/train_script.py", line 378, in <module>
    main(args)
  File "/Users/joshuamin/Desktop/Internships/UIUC_chatbot_data_generator/filtering/train_script.py", line 266, in main
    trainer.fit(model, datamodule=dm)
  File "/Users/joshuamin/miniconda3/envs/fine/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 582, in fit
    call._call_and_handle_interrupt(
  File "/Users/joshuamin/miniconda3/envs/fine/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/Users/joshuamin/miniconda3/envs/fine/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 624, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/Users/joshuamin/miniconda3/envs/fine/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1000, in _run
    self._call_setup_hook()  # allow user to setup lightning_module in accelerator environment
  File "/Users/joshuamin/miniconda3/envs/fine/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1249, in _call_setup_hook
    self._call_lightning_module_hook("setup", stage=fn)
  File "/Users/joshuamin/miniconda3/envs/fine/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1305, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/Users/joshuamin/Desktop/Internships/UIUC_chatbot_data_generator/filtering/train_script.py", line 183, in setup
    tb_size = self.hparams.train_batch_size * max(1, self.trainer.gpus)
  File "/Users/joshuamin/miniconda3/envs/fine/lib/python3.10/site-packages/pytorch_lightning/_graveyard/trainer.py", line 45, in _gpus
    raise AttributeError(
AttributeError: `Trainer.gpus` was deprecated in v1.6 and is no longer accessible as of v1.8. Please use `Trainer.num_devices` or `Trainer.device_ids` to get device information instead.