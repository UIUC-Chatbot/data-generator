{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO :\n",
    "1. Alter prompt (use 4 or 5 shot prompting to make it follow the pattern)\n",
    "2. Final output formatting - first question and final answer QA pair\n",
    "3. Metaprompting techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "import backoff\n",
    "from openai.error import RateLimitError\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "        return infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key =  os.getenv('API_KEY')\n",
    "openai.api_key =api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt3_embedding(content, model='text-similarity-ada-001'):\n",
    "    try:\n",
    "        response = openai.Embedding.create(input=content, model=model)\n",
    "    except openai.error.APIConnectionError:\n",
    "        print(\"Failed\") \n",
    "    return response['data'][0]['embedding'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute cosine similarity\n",
    "def get_similarity(v1, v2):\n",
    "    cosine = np.dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching through textbook \n",
    "def search_index(query, data, count=1):\n",
    "    question_vector = gpt3_embedding(query)\n",
    "    scores = []\n",
    "    for i in data:\n",
    "        score = get_similarity(question_vector, i['vector'])\n",
    "        scores.append({'content' : i['content'], 'score' : score})\n",
    "    most_relevant= sorted(scores, key=lambda d: d['score'], reverse=True)\n",
    "    return most_relevant[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(backoff.expo, RateLimitError)\n",
    "def response_API(prompt, myKwargs = {}):\n",
    "\n",
    "  #default arguments to send the API, unless changed in function\n",
    "  kwargs = {\"model\" :\"text-davinci-002\",\n",
    "            \"temperature\" :0.7,\n",
    "            \"max_tokens\": 300,\n",
    "            \"best_of\" :5,\n",
    "            \"n\" :3,\n",
    "            \"top_p\" : 1,\n",
    "            \"stop\" : '\\n\\n\\n',\n",
    "            \"presence_penalty\" : 0}\n",
    "\n",
    "\n",
    "  for kwarg in myKwargs:\n",
    "    kwargs[kwarg] = myKwargs[kwarg]\n",
    "\n",
    "  r = openai.Completion.create(prompt=prompt, **kwargs)\n",
    "  return r['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_completions_with_backoff(passages): \n",
    "       \n",
    "    question_prompts = ['''Generate 5 interactive and coherent questions about this context. The questions should not be repeated from the previous step. The questions should consist of reasoning and procedural steps. \\n\n",
    "                        The questions should be precise and factual. Start the question with a '[Q]' ''',\n",
    "                        \n",
    "                        '''Generate 5 objective, concise and firm questions about this context. The questions should not be repeated from the previous step. \\n\n",
    "                        The questions should begin with any of Why/How/Where/Who/When. Start the question with a '[Q]' ''' ,\n",
    "                        \n",
    "                        '''Generate 5 thoughtful and compelling, steps-based procedural questions about this context that start with Why or How. The questions should not be repeated from the previous step. \\n\n",
    "                        The questions should be unique and creative with an abstract and subjective aspect. Start the question with a '[Q]' ''' ]\n",
    "    \n",
    "    n=len(question_prompts)\n",
    "    questions = []\n",
    "    for p in passages:\n",
    "        for j in question_prompts:\n",
    "                #prompt_tokens = calculate_tokens(j)\n",
    "                #context_tokens = calculate_tokens(p)\n",
    "                #max_tokens = 300\n",
    "                \n",
    "                #while(max_tokens+prompt_tokens+context_tokens < 4096):\n",
    "                prompt= \"%s \\n %s\" % (j, p)\n",
    "    \n",
    "                response = response_API(prompt)\n",
    "                \n",
    "                questions.append(response)\n",
    "                print(response)\n",
    "                      \n",
    "    question_list = [questions[i:i + n] for i in range(0, len(questions), n)]\n",
    "    \n",
    "    return question_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question, data):\n",
    "    #most relevant passages\n",
    "    result = search_index(question, data) #get most relevant passages where answer could be\n",
    "    prompt = \"PASSAGE - %s \\n QUESTION - %s \\nAnswer this question in 2-3 concise sentences based on the passage. Be objective in the answer given and explain in a few lines only.\\n\" % (result['content'], question)\n",
    "    answer = response_API(prompt)\n",
    "    print(answer)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(generated):\n",
    "    if '\\n' not in generated:\n",
    "        last_line =  generated\n",
    "    else: \n",
    "        last_line = generated.split('\\n')[-1]\n",
    "\n",
    "    if ':' not in last_line:\n",
    "        after_colon = last_line\n",
    "    else:\n",
    "        after_colon = generated.split(':')[-1]\n",
    "    \n",
    "    if ' ' == after_colon[0]:\n",
    "        after_colon = after_colon[1:]\n",
    "    if '.' == after_colon[-1]:\n",
    "        after_colon = after_colon[:-1]\n",
    "\n",
    "    return after_colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(generated):\n",
    "    if '\\n' not in generated:\n",
    "        last_line =  generated\n",
    "    else: \n",
    "        last_line = generated.split('\\n')[-1]\n",
    "\n",
    "    if 'Follow up:' not in last_line:\n",
    "      print('we probably should never get here...' + generated)\n",
    "\n",
    "    if ':' not in last_line:\n",
    "        after_colon = last_line\n",
    "    else:\n",
    "        after_colon = generated.split(':')[-1]\n",
    "    \n",
    "    if ' ' == after_colon[0]:\n",
    "        after_colon = after_colon[1:]\n",
    "    if '?' != after_colon[-1]:\n",
    "        print('we probably should never get here...' + generated)\n",
    "        \n",
    "    return after_colon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_line(generated):\n",
    "    if '\\n' not in generated:\n",
    "        last_line =  generated\n",
    "    else: \n",
    "        last_line = generated.split('\\n')[-1]\n",
    "\n",
    "\n",
    "    return last_line\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []\n",
    "directory = '../../notes'\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if not os.path.isfile(f):\n",
    "        for fn in os.listdir(f):\n",
    "            notes.append(f+'/'+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../notes/part3/ece120-set-3-2-fsm-examples-part-i.tex',\n",
       " '../../notes/part3/ece120-set-3-6-memory.tex',\n",
       " '../../notes/part3/ece120-set-3-3-lab.tex',\n",
       " '../../notes/part3/ece120-set-3-5-fsm-examples-part-ii.tex',\n",
       " '../../notes/part3/ece120-set-3-7-fsm-to-computer.tex',\n",
       " '../../notes/part3/ece120-set-3-4-keyless-extension.tex',\n",
       " '../../notes/part3/ece120-set-3-1-serialize.tex',\n",
       " '../../notes/part3/ece120-set-3-8-summary.tex',\n",
       " '../../notes/part4/ece120-set-4-3-isa-design.tex',\n",
       " '../../notes/part4/ece120-set-4-1-control-unit.tex',\n",
       " '../../notes/part4/ece120-set-4-2-coding.tex',\n",
       " '../../notes/part4/ece120-set-4-4-summary.tex',\n",
       " '../../notes/part2/ece120-set-2-3-adder.tex',\n",
       " '../../notes/part2/ece120-set-2-8-summary.tex',\n",
       " '../../notes/part2/ece120-set-2-7-registers.tex',\n",
       " '../../notes/part2/ece120-set-2-2-dontcare.tex',\n",
       " '../../notes/part2/ece120-set-2-1-goodforms.tex',\n",
       " '../../notes/part2/ece120-set-2-6-sequential.tex',\n",
       " '../../notes/part2/ece120-set-2-5-abstraction.tex',\n",
       " '../../notes/part2/ece120-set-2-4-comparator.tex',\n",
       " '../../notes/part1/ece120-set-1-4-logic.tex',\n",
       " '../../notes/part1/ece120-set-1-2-twos-complement.tex',\n",
       " '../../notes/part1/ece120-set-1-1-halting-problem.tex',\n",
       " '../../notes/part1/ece120-set-1-5-programming.tex',\n",
       " '../../notes/part1/ece120-set-1-6-summary.tex',\n",
       " '../../notes/part1/ece120-set-1-3-overflow.tex']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/gpt-3/GPT-3_section_level.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/07/58sc206x78d9p6tm35y898h00000gn/T/ipykernel_22934/1386050566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sections data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/gpt-3/GPT-3_section_level.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msections_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#full textbook embeddings - vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/gpt-3/GPT-3_section_level.json'"
     ]
    }
   ],
   "source": [
    "#sections data\n",
    "'../../split_textbook/sections.json'\n",
    "s = open('../../split_textbook/sections.json')\n",
    "sections_data = json.load(s)\n",
    "\n",
    "#full textbook embeddings - vectors\n",
    "with open(\"/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/prompt engineering/embeddings/index.json\") as input_file:\n",
    "    data = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of only texts from the json file\n",
    "sections_list = []\n",
    "for p, item in enumerate(sections_data):\n",
    "    subtext = item['positive_ctxs']['text']\n",
    "    sections_list.append(subtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = open(\"/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/prompt engineering/self-ask /GPT-3_semantic_search.json\")\n",
    "semantic_search_data =  json.load(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for s in range(len(semantic_search_data)):\n",
    "    q = semantic_search_data[s]['GPT-3-Semantic-Search-Generations']['question']\n",
    "    questions.append(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ['''Question: What is the output of a Gray code counter?\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: What is the output of a three-bit Gray code counter?\n",
    "Intermediate answer: The output of a three-bit Gray code counter is a sequence of three-bit values that differ by only one bit.\n",
    "Follow up: What is the output of a two-bit Gray code counter?\n",
    "Intermediate answer: The output of a two-bit Gray code counter is a sequence of two-bit values that differ by only one bit.\n",
    "So the final answer is: a sequence of values that differ by only one bit.\n",
    "\n",
    "Question: What is the design process for a digital FSM?\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: What are the steps in the design process?\n",
    "Intermediate answer: The steps in the design process are: develop an abstract model, specify I/O behavior, complete the specification, choose a state representation, calculate logic expressions, and implement with flip-flops and gates.\n",
    "So the final answer is: develop an abstract model, specify I/O behavior, complete the specification, choose a state representation, calculate logic expressions, and implement with flip-flops and gates.\n",
    "\n",
    "Question: How many transistors does an N-input gate require?\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How many inputs does a 2-input gate have?\n",
    "Intermediate answer: A 2-input gate has 2 inputs.\n",
    "Follow up: How many inputs does a 10-input gate have?\n",
    "Intermediate answer: A 10-input gate has 10 inputs.\n",
    "So the final answer is: a 2-input gate requires roughly 2 transistors, and a 10-input gate requires roughly 10 transistors.\n",
    "\n",
    "Question: ''', \n",
    "'''\n",
    "Are follow up questions needed here:''', ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate = \"\\nIntermediate answer:\"\n",
    "followup = \"Follow up:\"\n",
    "finalans= '\\nSo the final answer is:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_prompt = prompt[0] +  questions[10] + prompt[1]\n",
    "\n",
    "print(cur_prompt, end ='')\n",
    "\n",
    "ret_text = response_API(cur_prompt, myKwargs = {\"stop\" : intermediate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while followup in get_last_line(ret_text):\n",
    "      \n",
    "      cur_prompt += ret_text\n",
    "      question = extract_question(ret_text)\n",
    "      external_answer = get_answer(question, data)\n",
    "\n",
    "      if external_answer is not None:\n",
    "        cur_prompt += intermediate + ' ' + str(external_answer) + '.'\n",
    "        print(intermediate + ' ' + external_answer + '.', end='' )\n",
    "        ret_text = response_API(cur_prompt, myKwargs = {\"stop\" : intermediate})\n",
    "      else:\n",
    "        #We only get here in the very rare case that Google returns no answer.\n",
    "        cur_prompt += intermediate\n",
    "        print(intermediate + ' ')\n",
    "        gpt_answer = response_API(cur_prompt, myKwargs = {\"stop\" : '\\n'+followup})\n",
    "        cur_prompt += gpt_answer\n",
    "    \n",
    "if finalans not in ret_text:\n",
    "  cur_prompt += finalans\n",
    "  print(finalans, end = '')\n",
    "  ret_text = response_API(cur_prompt, myKwargs = {\"stop\" : '\\n'})\n",
    "\n",
    "  print(cur_prompt + ret_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28596858067060ee176f7bab50a17c769bfd8a96306468e7ae0695529617abaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
