{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversational Data \n",
    "\n",
    "To do :\n",
    "-Alter prompts to add more questions per conversation \n",
    "-Format questions as per Q0,A0 - Q1,A1, etc.\n",
    "-Generate different types of conversations - interactive/talkative, formal/academic, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer\n",
    "import backoff\n",
    "from openai.error import RateLimitError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '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'\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API response for any prompt - change settings : temperature to be low, best_of=5, n=3 -> captures completions response\n",
    "@backoff.on_exception(backoff.expo, RateLimitError)\n",
    "def response_API(prompt, myKwargs = {}):\n",
    "    \n",
    "    kwargs = {\"model\" :\"text-davinci-002\",\n",
    "            \"temperature\" :0.46,\n",
    "            \"max_tokens\": 600,\n",
    "            \"frequency_penalty\":1,\n",
    "            \"presence_penalty\":0}\n",
    "    #assign changed values and keep everything else the same\n",
    "    for kwarg in myKwargs:\n",
    "        kwargs[kwarg] = myKwargs[kwarg]\n",
    "        \n",
    "    try:\n",
    "        response = openai.Completion.create(prompt=prompt, **kwargs)\n",
    "    except openai.error.APIConnectionError:\n",
    "        print(\"Failed\")\n",
    "    \n",
    "    return response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check number of tokens - to add, function takes too long. - not being used currently \n",
    "def calculate_tokens(context):\n",
    "    features = tokenizer(\"%s\", context)['input_ids']\n",
    "\n",
    "    return len(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = open(\"/Users/nehasheth/Desktop/Research - AI Chatbot TA/github/data-generator/gpt-3/GPT-3_section_level.json\")\n",
    "sections_data = json.load(s)\n",
    "\n",
    "sections_list = []\n",
    "for p, item in enumerate(sections_data):\n",
    "    subtext = item['positive_ctxs']['text']\n",
    "    sections_list.append(subtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = []\n",
    "for section in sections_list:\n",
    "    prompt = '''Generate an interactive and objective conversation between a teaching assistant and a student. \\n\n",
    "    The student approaches the teaching assistant with some questions and the teaching assistant has to answer using thorough, academically correct responses.\\n\n",
    "    The teaching assistant has to ask follow up questions or suggestions to spur curiosity. The TA maintains a professional, formal tone and talks in continuation with the previous topic the student refers to. \\n\n",
    "    The context for the following conversation is : %s \\n''' % section\n",
    "    \n",
    "    response = response_API(prompt)\n",
    "    print(response)\n",
    "    conversations.append(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting data \n",
    "all_data = []\n",
    "for i, passage in enumerate(sections_list):\n",
    "    data = {}\n",
    "    data['textbook-paragraph'] = passage\n",
    "    data['GPT-3_conversations'] = conversations[i]\n",
    "    #data['GPT-3_conversations']['conversations'] = conversations[i]\n",
    "    all_data.append(data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GPT-3_conversations.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=4) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75437bf4ff09ccf37c1a7b89ec6af0194e10f783ac143972fe56d1183a93e4f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
