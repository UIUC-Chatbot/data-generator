[
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What are the concrete aspects of our first few examples?\\n",
      "answer": "\nThe first few examples in the passage are of two-level logic, DeMorgan's law, and SOP form. Two-level logic is a popular way of expressing logic functions, and DeMorgan's law is a way to simplify expressions. SOP form is a way of representing a logic function."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": "How does the choice of representation for the FSM's internal state affect the complexity of the implementation?\", '\\n",
      "answer": "\nThe choice of representation for the FSM's internal state affects the complexity of the implementation in terms of the number of states, the number of inputs, the number of outputs, and the number of transition rules."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What are the concrete aspects of the design process?\\n",
      "answer": "\nThe design process involves the manipulation of logic expressions in order to optimize them for specific metrics, such as cost, power, or performance. Today, this manipulation is performed almost entirely by computers, with human engineers supplying the logic functions of interest and imposing constraints on the process."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is an abstract model?\\n",
      "answer": "\nAn abstract model is a mathematical model that is used to describe a system. It is usually simplified and does not include all the details of the system."
    }
  },
  {
    "textbook-paragraph": "level state, to simplify the implementation.} {Use counters to insert time-based transitions between states (such  as timeouts).} {Implement an FSM using logic components such as registers,  counters, comparators, and adders as building blocks.} {Explain the basic organization of a computer's microarchitecture as well as the role played by elements of a von Neumann design in the processing of instructions.} {Identify the stages of processing an instruction (such as fetch, decode, getting operands, execution, and writing back results) in a  processor control unit state machine diagram.}   And, at the highest level, we expect that you will be able to do the following:  {}{{}{} {}{}{}  {Explain the difference between the Moore and Mealy machine models,  as well as why you might find each of them useful when designing an FSM.} {Understand the need for initialization of an FSM, be able to analyze  and identify potential problems arising from lack of initialization, and  be able to extend an implementation to include initialization to an  appropriate state when necessary.} {Understand how the choice of internal state bits for an FSM can  affect the complexity of the implementation of next-state and output  logic, and be able to select a reasonable state assignment.} {Identify and fix design flaws in simple FSMs by analyzing an existing  implementation, comparing it with the specification, and removing any  differences by making any necessary changes to the implementation.}      {   }  empty 3rd page      {Instruction Set Architecture*}  This set of notes discusses  tradeoffs and design elements of instruction set architectures (ISAs). { The material is beyond the scope of our class, and is provided purely for your interest.}  Those who find these topics interesting may also want to read the ECE391 notes, which describe similar material with a focus on the x86 ISA.  As you know, the ISA defines the interface between software and hardware, abstracting the capabilities of a computer's datapath and standardizing the format of instructions to utilize those capabilities.  Successful ISAs are rarely discarded, as success implies the existence of large amounts of software built to use the ISA.  Rather, they are extended, and their original forms must be supported for decades (consider, for example, the IBM 360 and the Intel x86).  Employing sound design principles is thus imperative in an ISA.  {Formats and Fields*}  The LC-3 ISA  employs fixed-length instructions and a load-store architecture, two aspects that help to reduce the design space to a manageable set of choices.  In a general ISA design, many other options exist for instruction formats.  Recall the idea of separating the bits of an instruction into (possibly non-contiguous) fields.  One of the fields must contain an opcode, which specifies the type of operation to be performed by the instruction.  In the {LC-3} ISA, most opcodes specify both the type of operation and the types of arguments to the operation.   More generally, many addressing modes are possible for each operand, and we can think of the bits that specify the addressing mode as a separate field, known as the { mode} field.    As a simple example, the {LC-3's} ADD and AND instructions  contain a {1-bit} mode field that specifies whether the second operand of the ADD/AND comes from a register or is an immediate value.  Several questions must be answered in order to define the possible instruction formats for an ISA.  First, are instructions fixed-length or variable-length?  Second, how many addresses are needed for each instruction, and how many of the addresses can be memory addresses? Finally, what forms of addresses are possible for each operand? For example, can one use full memory addresses or only limited offsets relative to a register?  The answer to the first question depends on many factors, but several clear advantages exist for both answers.  { Fixed-length instructions} are easy to fetch and decode.  A",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is the need for FSM initialization?\\n",
      "answer": "\nAn FSM must be initialized to ensure that it starts in a known state. This allows the FSM to function correctly and avoid any potential problems that could arise from an unknown starting state."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How is a digital FSM designed from an abstract model?\\n",
      "answer": "\nFrom the passage, it seems that a digital FSM is designed from an abstract model by first identifying a finite set of states, a set of possible inputs, a set of possible outputs, and a set of transition rules. Then, the FSM is implemented as a digital system by representing all states as patterns using a fixed number of bits, translating all inputs into bits, and translating all outputs into bits. Finally, transition rules are defined so that given any state of the FSM and any pattern of input bits, a transition is defined from that state to another state."
    }
  },
  {
    "textbook-paragraph": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the design process for a counter with a number of states that is not a power of two?\\n",
      "answer": "\nThe design process for a counter with a number of states that is not a power of two is to choose a representation for the states that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How does the choice of representation for the FSM's internal state affect the complexity of the implementation?\"]",
      "answer": "\nThe choice of representation for the FSM's internal state affects the complexity of the implementation. If the states are represented as bit patterns, the transition rules must be complete. If the states are represented as abstract states, the outputs for each state must be specified."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How does the choice of state representation affect the implementation of an FSM?\\n",
      "answer": "\nThe choice of state representation affects the implementation of an FSM in terms of the number of gates and inverters required, as well as the delay of the overall design. A serial comparator design based on the 2-bit slice variant, for example, requires 24 gates and 10 inverters, which is not much larger than the earlier serial design. In terms of gate delays, however, the new design is identical, meaning that comparisons are finished in half the time. More realistic area and timing metrics show slightly more difference between the two designs. These differences can dominate the results if the design is not carefully considered."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " Why is it important for the components of a digital system to be compatible?\"]",
      "answer": "\nOne important reason for the compatibility of components in a digital system is that it helps to ensure that the system will function correctly. If the components are not compatible, the system may not work correctly, or may not work at all. Additionally, compatibility among components can help to reduce the cost of manufacturing the system, as well as the cost of maintaining and repairing the system."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the inputs to a Gray code counter?\\n",
      "answer": "\nThe inputs to a Gray code counter are the current state of the counter and the clock signal. The clock signal triggers the state change of the counter."
    }
  },
  {
    "textbook-paragraph": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the inputs to the counter?\\n",
      "answer": "\nThe inputs to the counter are the coins that are inserted into the machine. The machine will track the value of the coins in terms of nickels and will reject any coins that are inserted that would cause the value to exceed the maximum value that the machine can track."
    }
  },
  {
    "textbook-paragraph": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the difference between a Gray code counter and a regular counter?\\n",
      "answer": "\nA Gray code counter is a type of counter that uses a Gray code, which is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit. A regular counter is a type of counter that does not use a Gray code."
    }
  },
  {
    "textbook-paragraph": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the internal state value for the third bit gray code counter?\\n",
      "answer": "\nThe internal state value for the third bit gray code counter is S_2S_1S_0=001. This is because when HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired."
    }
  },
  {
    "textbook-paragraph": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the output for the third bit gray code counter?\\n",
      "answer": "\nThe output for the third bit gray code counter would be either 0 or 1. If the input is 0, the output is 1. If the input is 1, the output is 0."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the next state value for S2?\\n",
      "answer": "\nThe next state value for S2 is 0. This is because the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes. The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1."
    }
  },
  {
    "textbook-paragraph": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the internal state value for the third bit gray code counter?\\n",
      "answer": "\nThe internal state value for the third bit gray code counter is S_2S_1S_0=001. This is because when HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired."
    }
  },
  {
    "textbook-paragraph": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the output for the third bit gray code counter?\\n",
      "answer": "\nThe output for the third bit gray code counter would be either 0 or 1. If the input is 0, the output is 1. If the input is 1, the output is 0."
    }
  },
  {
    "textbook-paragraph": "abstraction.  When HOLD=1, the next-state logic for S_1^+ and S_0^+  reduces to S_1^+=S_1 and S_0^+=S_0; in other words, the counter stops counting and simply stays in its  current state.  When HOLD=0, these equations become S_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating sequence for S_1S_0 of 00, 01, 11, 10, as desired. You may want to look back at our two-bit Gray code counter design to compare the next-state equations.  We can now verify that the implementation produces the correct transition behavior.  In the counting states, S_2=0, and the HOLD value simplifies to HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and and the counter continues to count in the correct sequence. When H=1, HOLD=1, and the counter stops at its current value (S_2^+S_1^+S_0^+=1S_1S_0,  which is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).  In any of the halted states, S_2=1, and we can reduce HOLD to HOLD=H+.  Here, so long as we press the ``halt'' button or do not press the ``go'' button, the counter stays in its current state, because HOLD=1.  If we release ``halt'' and press ``go,'' we have HOLD=0, and the counter resumes counting (S_2^+S_1^+S_0^+=0S_0{{S_1}}, which is shorthand for S_2^+=0, S_1^+=S_0, and  S_0^+={{S_1}}).  We have now verified the implementation.  What if you wanted to build a three-bit Gray code counter with the same controls for starting and stopping?  You could go back to basics and struggle  with six-variable {K-maps}.  Or you could simply copy the HOLD  mechanism from the two-bit design above, insert muxes between the next  state logic and the flip-flops of the three-bit Gray code counter that  we designed earlier, and control the muxes with the HOLD bit.   Abstraction is a powerful tool.    {Impact of the State Representation}  What happens if we choose a bad representation?  For the same FSM---the two-bit Gray code counter with start and stop inputs---the  table below shows a poorly chosen mapping from states to internal  state representation.  Below the table is a diagram of an implementation using that representation.  Verifying that the implementation's behavior is correct is left as an exercise for the determined reader.  { {|c|c|c|c|c|}{1-2}{4-5} state& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5} { COUNT A}& 000& & { HALT A}& 111  { COUNT B}& 101& & { HALT B}& 110  { COUNT C}& 011& & { HALT C}& 100  { COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}  }         {Random Access Memories}  This set of notes describes random access memories (RAMs), providing slightly more detail than is available in the textbook.  We begin with a discussion of the memory abstraction and the types of memory most commonly used in digital systems, then examine how one can build memories (static RAMs)  using logic.  We next introduce tri-state buffers as a way of simplifying ouput connections, and illustrate how memory chips can be combined to provide larger and wider memories.  A more detailed description of dynamic  RAMs finishes this set.  { Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes.}    A computer { memory} is a group of storage elements and the logic necessary to move data in and out of the elements.  The size of the elements in a memory---called the { addressability} of the  memory---varies from a single binary digit, or { bit}, to a { byte} (8 bits) or more.  Typically, we refer to data elements larger than a byte as { words}, but the size of a word depends on context.   Each element in a memory is assigned a unique name, called an { address}, that allows an external circuit to identify the particular element of interest.  These addresses are not unlike the street addresses that you use when you send a letter.  Unlike street addresses, however, memory addresses usually have little or no redundancy; each possible combination of bits in an address identifies a distinct set of bits in the memory.  The figure on the right below",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the next-state values for a three-bit Gray code counter?\\n",
      "answer": "\nThe next-state values for a three-bit Gray code counter are as follows: S_2^+=0, S_1^+=S_0, and S_0^+={{S_1}}."
    }
  },
  {
    "textbook-paragraph": "This process is no different than for combinational logic, and should already be fairly  familiar to you.    Finally, in Step {step-gates}, we translate our logic expressions into gates and use flip-flops (or registers) to hold the internal state bits of the FSM.  In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic.   {Example: A Two-Bit Gray Code Counter}  Let's begin with a two-bit Gray code counter with no inputs. As we mentioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit.  For simplicity, our first few examples are based on counters and use the internal state of the FSM as the output values.  You should already know how to design combinational logic for the outputs if it were necessary.  The inputs to a counter, if any, are typically limited to functions such as starting and stopping the counter, controlling the counting  direction, and resetting the counter to a particular state.  A fully-specified transition diagram for  a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_1S_0  (before the ``/'') and the output Z_1Z_0 (after the ``/''), which are  always equal for this counter.  Based on the transition diagram, we can fill in the K-maps for the  next-state values S_1^+ and S_0^+ as shown to the right of the transition diagram, then  derive algebraic expressions in the usual way to obtain S_1^+=S_0 and S_0^+={{S_1}}.  We then use the next-state logic to develop the implementation shown on the far right, completing our first counter design.             {Example: A Three-Bit Gray Code Counter}   Now we'll add a third bit to our counter, but again use a Gray code as the basis for the state sequence.  A fully-specified transition diagram for such a counter appears to  the right.  As before, with no inputs, the states simply form a loop,  with the counter moving from one state to the next each cycle.  Each state in the diagram is marked with the internal state value S_2S_1S_0  (before ``/'') and the output Z_2Z_1Z_0 (after ``/'').        Based on the transition diagram, we can fill in the K-maps for the  next-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then  derive algebraic expressions.  The results are more complex this  time.          For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_0 + S_1 {{S_0}}  S_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}}  S_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1 {eqnarray*}   Notice that the equations for S_2^+ and S_1^+ share a common term, S_1{{S_0}}.  This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce  the design complexity by explicitly identifying and making use of  common algebraic terms and sub-expressions for different outputs. In modern design processes, identifying such opportunities is generally performed by a computer program, but it's important to understand how they arise.  Note that the common term becomes a single AND gate in the implementation of our counter, as shown to the right.  Looking at the counter's implementation diagram, notice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify understanding the diagram.  In particular, they are ordered from left to right (on the left side of the figure) as  {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read  and check the correctness of the logic.       {Example: A Color Sequencer}   Early graphics systems",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How are the vertical lines carrying the current state values and their inverses back to the next state logic inputs ordered in a three-bit Gray code counter?', \"\\n",
      "answer": "\nThe vertical lines carrying the current state values and their inverses back to the next state logic inputs are ordered from left to right (on the left side of the figure) as {{S_0}}S_0{{S_1}}S_1{{S_2}}S_2."
    }
  },
  {
    "textbook-paragraph": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How does adding a third bit to a Gray code counter increase design complexity?\\n\\n",
      "answer": "\nAdding a third bit to a Gray code counter increases design complexity because it requires an additional carry bit. This carry bit must be propagated through the adder bit slice logic, which increases the number of logic gates required and the overall complexity of the design."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How can you reduce design complexity by making use of common algebraic terms and sub-expressions?\\n\\n",
      "answer": "\nOne way to reduce design complexity is to make use of common algebraic terms and sub-expressions. This can help to simplify the design and make it easier to understand. Another way to reduce complexity is to use DeMorgan's laws to simplify the expression. This can help to reduce the number of gates needed and make the design more efficient."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " Why is it important to have a reasonable order when designing any logic diagram?\\n\\n",
      "answer": "\nIt is important to have a reasonable order when designing any logic diagram because it can help simplify the design process and make it easier to understand the function of the logic diagram. Additionally, it can help to ensure that the logic diagram is compatible with other products that may be used in the same system."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How does the counter's implementation diagram simplify understanding the diagram?\\n\\n",
      "answer": "\nThe counter's implementation diagram simplifies understanding the diagram by breaking it down into smaller, more manageable pieces. By doing this, it is easier to see how the various parts of the diagram interact with each other. Additionally, it makes it easier to identify potential problems or areas for improvement."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the purpose of the vertical lines carrying the current state values and their inverses back to the next state logic inputs?\"]",
      "answer": "\nThe purpose of the vertical lines carrying the current state values and their inverses back to the next state logic inputs is to ensure that the state of the FSM is always consistent. This is necessary for safety reasons, as the FSM must always ensure that the lights on one or both roads are red at all times."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the next-state values for S_2, S_1, and S_0?\\n\\n",
      "answer": "\nThe next-state values for S_2, S_1, and S_0 are given in the table to the right. The states of S act like a counter, and by examining the connections, we can derive equations for the next state and draw a transition diagram. There are six states in the loop defined by the next-state logic, with the two remaining states converging into the loop after a single cycle."
    }
  },
  {
    "textbook-paragraph": "unit.  We define S as the number represented by the bit pattern produced by the add unit.  When overflow  occurs, S=C, but we always have (S=C)  2^N.  { Theorem:}   Addition of two {N-bit} 2's complement numbers A and B overflows if and only if one of the following conditions holds:  {A<0 and B<0 and S} {A and B and S<0}   { Proof:}  We once again start with the ``if'' direction.  That is, if condition 1  or condition 2 holds, we have an overflow.  The proofs are straightforward. Given condition 1, we can add the two inequalities A<0 and B<0 to  obtain C=A+B<0.  But S, so clearly S=C, thus overflow  has occurred.  Similarly, if condition 2 holds, we can add the inequalities A and B to obtain C=A+B.  Here we have S<0, so again S=C, and we have an overflow.  We must now prove the ``only if'' direction, showing that any overflow implies either condition 1 or condition 2.  By the  { contrapositive}{If we have a statement of the form (p implies q), its contrapositive is the  statement (not q implies not p). Both statements have the same truth value.  In this case, we can turn our Lemma around as stated.} of our Lemma, we know that if an overflow occurs, either both operands are  negative, or they are both positive.      Let's start with the case in which both operands are negative, so A<0 and B<0, and thus the real sum C<0 as well.  Given that A and B are represented as {N-bit} 2's complement, they must fall in the representable range, so we can write  {eqnarray*} -2^{N-1}  & A & < 0 -2^{N-1}  & B & < 0 {eqnarray*}  We add these two inequalities and replace A+B with C to obtain  {eqnarray*} -2^N  & C & < 0 {eqnarray*}  Given that an overflow has occurred, C must fall outside of the  representable range.  Given that C<0, it cannot be larger than the largest possible number representable using {N-bit} 2's complement, so we can write  {eqnarray*} -2^N  & C & < -2^{N-1} {eqnarray*}  We now add 2^N to each part to obtain  {eqnarray*} 0  & C + 2^N & < 2^{N-1} {eqnarray*}  This range of integers falls within the representable range  for {N-bit} 2's complement, so we can replace the middle expression with S (equal to C modulo 2^N) to find that  {eqnarray*} 0  & S & < 2^{N-1} {eqnarray*}  Thus, if we have an overflow and both A<0 and B<0, the resulting sum S, and condition 1 holds.  The proof for the case in which we observe an overflow when  both operands are non-negative (A and B) is similar, and leads to condition 2.  We again begin with inequalities for A and B:  {eqnarray*} 0  & A & < 2^{N-1} 0  & B & < 2^{N-1} {eqnarray*}  We add these two inequalities and replace A+B with C to obtain  {eqnarray*} 0  & C < & 2^N {eqnarray*}  Given that an overflow has occurred, C must fall outside of the  representable range.  Given that C{}0, it cannot be smaller than the smallest possible number representable using {N-bit} 2's complement, so we can write  {eqnarray*} 2^{N-1}  & C & < 2^N {eqnarray*}  We now subtract 2^N to each part to obtain  {eqnarray*} -2^{N-1}  & C - 2^N & < 0 {eqnarray*}  This range of integers falls within the representable range  for {N-bit} 2's complement, so we can replace the middle expression with S (equal to C modulo 2^N) to find that  {eqnarray*} -2^{N-1}  & S & < 0 {eqnarray*}  Thus, if we have an overflow and both A and B, the resulting sum S<0, and condition 2 holds.  Thus overflow implies either condition 1 or condition 2, completing our proof.",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the equation for S_2^+?', \"\\n\\n",
      "answer": "\nThe equation for S_2^+ is as follows: S_2^+ = A + B + C, where A is the carry bit, B is the first bit being added, and C is the second bit being added. If an overflow occurs, then S_2^+ = C."
    }
  },
  {
    "textbook-paragraph": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM",
    "GPT-3-Semantic-Search-Generations": {
      "question": " Given that we need three flip-flops, can we choose to use the counter's internal state directly as our output values?\\n",
      "answer": "\nYes, we can choose to use the counter's internal state directly as our output values. This is because the colors we need to produce as outputs are all unique bit patterns. By using the counter's internal state directly as our output values, we can simplify our design."
    }
  },
  {
    "textbook-paragraph": "least a controllable number of cycles. If we cannot specify clearly when transitions occur from an FSM state, we may not be able to implement the state.    Now let's go through the flow chart and identify states.  Initialization of { min} and { idx} need not occur serially, and the result of the first comparison between { idx} and the constant 10 is known in advance, so we can merge all three operations into a single state, which we  call { INIT}.  We can also merge the updates of { min} and { idx} into a second FSM state, which we call { COPY}.  However, the update to { min}  occurs only when the comparison ({ min > value[idx]}) is true.   We can use logic to predicate execution of the update.  In other words, we  can use the output of the comparator, which is available after the comparator  has finished comparing the two values (in a high-level FSM state that we  have yet to define), to determine whether or not the register holding  { min} loads a new value in the { COPY} state.  Our model of use for this FSM involves external logic filling the memory (the array of integer values), executing the FSM ``code,'' and then checking the answer.  To support this use model, we create a FSM state  called { WAIT} for cycles in which the FSM has no work to do. Later, we also make use of an external input signal { START}  to start the FSM execution.  The { WAIT} state logically corresponds to the ``START''  bubble in the flow chart.   Only the test for the { if} statement remains.  Using a serial comparator to compare two {32-bit} values requires 32 cycles. However, we need an additional cycle to move values into our shift  registers so that the comparator can see the first bit.  Thus our single comparison operation breaks into two high-level states.  In the first state, which we call { PREP}, we copy { min} to one of the shift registers, copy { values[idx]} to the other shift register, and reset the counter that measures the cycles needed for our serial comparator.  We then move to a second high-level state, which we call { COMPARE}, in which we feed one bit per cycle from each shift register to the serial comparator.  The { COMPARE}  state   {{file=part3/figs/part3-min-flow-chart-states.eps,width=3.96in}}   executes for 32 cycles, after which the comparator produces the one-bit answer that we need, and we can move to the { COPY} state.  The association between the flow chart and the high-level FSM states is illustrated in the figure shown to the right above.   We can now also draw an abstract state diagram for our FSM, as shown to the right.  The FSM begins in the { WAIT} state.  After external logic fills the { values} array, it signals the FSM to begin by raising the { START} signal.  The FSM transitions into the  { INIT} state, and in the next cycle into the { PREP} state. From { PREP}, the FSM always moves to { COMPARE}, where it remains for 32 cycles while the serial comparator executes a comparison. After { COMPARE}, the FSM moves to the { COPY}   {{file=part3/figs/part3-min-state-diag.eps,width=3in}}   state, where it remains for one cycle.  The transition from { COPY} depends on how many loop iterations have executed.  If more loop iterations remain, the FSM moves to { PREP} to execute the next iteration.  If the loop is done, the FSM returns to { WAIT} to allow external logic to read the result of the computation.    {Laying Out Components}   Our high-level FSM design tells us what our components need to be able to do in any given cycle.  For example, when we load new values into the shift registers that provide bits to the serial comparator, we always copy  { min} into one shift register and { values[idx]} into the second. Using this information, we can put together our components and simplify our design by fixing the way in which bits flow between them.  The figure at the right shows how we can organize our components. Again, in practice, one goes back and forth thinking about states, components, and flow from state to",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is a fully-specified transition diagram?\\n",
      "answer": "\nA fully-specified transition diagram is a diagram that specifies all of the states that a system can be in, as well as all of the transitions between those states. This allows for a clear understanding of the behavior of the system."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the next-state logic equations?\", '\\n\\n",
      "answer": "\nThe next-state logic equations are the functions that describe the relationship between the current state of the system and the next state of the system. These functions can be represented in either SOP or POS form. DeMorgan's law can be used to simplify these functions by replacing OR gates with NAND gates, or AND gates with NOR gates."
    }
  },
  {
    "textbook-paragraph": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the color mapping for a three-bit RGB encoding system?\\n",
      "answer": "\nThe color mapping for a three-bit RGB encoding system is that the colors are produced by unique bit patterns. The states form a loop, and are marked with the internal state value S_2S_1S_0 and the output RGB."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the next-state values for S_2, S_1, and S_0?']",
      "answer": "\nThe next-state values for S_2, S_1, and S_0 are all 1."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What happens in the next cycle?\\n",
      "answer": "\nThe timer is set to 0, and the state of the FSM is changed to the next state in the sequence. The new state of the FSM determines the duration of the next state, and the timer is set accordingly."
    }
  },
  {
    "textbook-paragraph": "range corresponds to systems in which when a processor's power consumption in a digital  system is extremely low relative to the power consumption of the system. For example, the processor in a computer might use less than 1  of the total used by  the system including the disk drive, the monitor, the power  supply, and so forth.  One power consumption value in this range  is just as good as any another, and no one cares about the power consumption of the processor  in such cases.  In the second range, power consumption of the processor makes a difference.  Cell phones use most of their energy in radio operation, for example, but if you own a phone with a powerful processor, you may have noticed that you can turn off the phone and  drain the battery fairly quickly by playing a game.  Designing a processor that uses half as much power lengthens the battery life in such cases.  Finally, the third region of power consumption measurements is impossible: if you use so much power, your chip will overheat or even burst into flames.  Consumers get unhappy when such things happen.  As a first step, you can remove any metrics for which all solutions are effectively equivalent.  Until a little less than a decade ago, for example, the power  consumption of a desktop processor actually was in the first range that we discussed.  Power was simply not a concern to engineers: all designs of  interest consumed so little power that no one cared.  Unfortunately, at that point, power consumption jumped into the third range rather quickly.  Processors hit a wall, and  products had to be cancelled.  Given that the time spent designing a processor has historically been about five years, a lot of engineering effort was wasted because people had not thought carefully enough about power (since it had never mattered in the past).  Today, power is an important metric that engineers must take into account in their designs.   However, in some areas, such as desktop and high-end server processors, other metrics (such as performance) may be so  important that we always want to operate at the edge of the interesting range.  In such cases, we might choose to treat  a metric such as power consumption as a { threshold}: stay below 150 Watts for a desktop processor, for example.  One still has to make a coordinated effort to ensure that the system as a whole does not exceed the threshold, but reasoning about  threshold values, a form of constraint, is easier than trying to think about multiple metrics at once.  Some metrics may only allow discrete quantification.  For example, one could choose to define compatibility with previous processor generations as binary: either an existing piece of software (or operating system) runs out of the box on your new processor, or it does not.  If you  want people who own that software to make use of your new processor, you must ensure that the value of this binary metric is 1, which can also be viewed as a threshold.  In some cases, two metrics may be strongly { correlated}, meaning that a design that is good for one of the metrics is frequently  good for the other metric as well.  Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider them separately.  A design that requires a larger chip is probably more complex, and thus takes more engineering time to get right (engineering time costs money).    Each silicon wafer costs money to fabricate, and fewer copies of a  large design fit on one wafer, so large chips mean more fabrication cost.  Physical defects in silicon can cause some chips not to work.  A large chip uses more silicon than a small one, and is thus more likely to suffer from defects (and not work).  Cost thus goes up again for large chips relative to small ones.  Finally, large chips usually require more careful testing to ensure that they work properly (even ignoring the cost of getting the design right, we have to test for the presence of defects), which",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What could be going wrong?\\n",
      "answer": "\nThe passage states that power consumption is an important metric that engineers must take into account in their designs. However, in some cases, such as desktop and high-end server processors, other metrics (such as performance) may be so important that we always want to operate at the edge of the interesting range. This could be what is going wrong, as engineers are not taking power consumption into account as much as they should be."
    }
  },
  {
    "textbook-paragraph": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM",
    "GPT-3-Semantic-Search-Generations": {
      "question": "How can the FSM ever get into the unexpected states?\\n",
      "answer": "\nOne possibility is that the FSM is not resetting properly, so it starts in an unexpected state. Another possibility is that there is a race condition in the FSM, so that it can get into an unexpected state if the inputs change too quickly."
    }
  },
  {
    "textbook-paragraph": "{3-input} NAND gates and allows set/reset of the flip-flop (using two extra inputs).    The { timing diagram} to the right illustrates the operation of our flip-flop.  In a timing diagram, the horizontal axis represents (continuous) increasing time, and the individual lines represent voltages for logic signals.  The relatively simple version shown here uses only binary values for each signal.  One can also draw  transitions more realistically (as taking finite time).  The dashed vertical lines here represent the times at which the clock rises. To make the      example interesting, we have varied D over two clock cycles.  Notice that even though D rises and falls during the second clock cycle, its value is not copied to the output of our flip-flop. One can build flip-flops that ``catch'' this kind of behavior (and change to output 1), but we leave such designs for later in your career.  Circuits such as latches and flip-flops are called { sequential feedback} circuits, and the process by which they are designed  is beyond the scope of our course.  The ``feedback'' part of the name refers to the fact that the outputs of some gates are fed back  into the inputs of others.  Each cycle in a sequential feedback  circuit can store one bit.  Circuits that merely use latches and flip-flops as building blocks are called { clocked synchronous sequential circuits}.  Such designs are still sequential: their behavior depends on the bits currently stored in the latches and flip-flops.  However, their behavior is substantially simplified by the use of a clock signal (the ``clocked'' part of the name) in a way that all elements change at the same time (``synchronously'').  The value of using flip-flops and assuming a square-wave clock signal with uniform timing may not be clear to you yet, but it bears emphasis.  With such assumptions, { we can treat time as having  discrete values.}  In other words, time ``ticks'' along discretely, like integers instead of real numbers.  We can look at the state of the system, calculate the inputs to our flip-flops through the combinational logic that drives their D inputs, and be confident that, when time moves to the next discrete value, we will know the new bit values stored in our flip-flops, allowing us to repeat the process for the next clock cycle without worrying about exactly when things change.  Values change only on the rising edge of the clock!  Real systems, of course, are not so simple, and we do not have one clock to drive the universe, so engineers must also design systems that interact even though each has its own private clock signal (usually with different periods).    {Static Hazards: Causes and Cures*}  Before we forget about the fact that real designs do not provide perfect clocks, let's explore some of the issues that engineers must sometimes face.    We discuss these primarily to ensure that you appreciate the power of the abstraction that we use in the rest of our course. In later classes (probably our 298, which will absorb material  from 385), you may be required to master this material. { For now, we provide it simply for your interest.}  Consider the circuit shown below, for which the output is given by  the equation S=AB+.   {{file=part2/figs/lec15-1.eps,width=4in}}  The timing diagram on the right shows a { glitch} in the output when the input shifts from ABC=110 to 100, that is, when B falls. The problem lies in the possibility that the upper AND gate, driven  by B, might go low before the lower AND gate, driven by , goes high.  In such a case, the OR gate output S falls until the second AND gate rises, and the output exhibits a glitch.  A circuit that might exhibit a glitch in an output that functionally remains stable at 1 is said to have a { static-1 hazard}.  The qualifier ``static'' here refers to the fact that we expect the output to remain static, while the ``1'' refers to the expected value of the output.    The presence of hazards in circuits can be problematic in",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is the issue with the FSM getting into unexpected states?\\n",
      "answer": "\nThe issue with the FSM getting into unexpected states is that it can cause the output of the circuit to glitch. This can be problematic in some applications where a stable output is required."
    }
  },
  {
    "textbook-paragraph": "used a three-bit red-green-blue (RGB)  encoding for colors.  The color mapping for such a system is shown to the right.  Imagine that you are charged with creating a counter to drive a light through a sequence of colors.  The light takes an RGB input as just described, and the desired pattern is  {off (black)     yellow     violet     green     blue}  You immediately recognize that you merely need a counter with five states.  How many flip-flops will we need?  At least three, since _2 (5)=3.  Given that we need three flip-flops,  and that the colors we need to produce as   {c|l} RGB& color  000& black 001& blue 010& green 011& cyan 100& red 101& violet 110& yellow 111& white    outputs are all unique bit patterns, we can again choose to use the counter's internal  state directly as our output values.   A fully-specified transition diagram for our color sequencer appears to the right.  The states again form a loop, and are marked with the internal state value S_2S_1S_0  and the output RGB.       As before, we can use the transition diagram to fill in K-maps for the  next-state values S_2^+, S_1^+, and S_0^+, as shown to the right. For each of the three states not included in our transition diagram, we have inserted x's          into the K-maps to indicate ``don't care.''  As you know, we can treat each x as either a 0 or a 1, whichever produces better results (where ``better'' usually means simpler  equations).  The terms that we have chosen for our algebraic  equations are illustrated in the K-maps.  The x's within the ellipses become 1s in the implementation, and the x's outside of the ellipses become 0s.   For our next-state logic, we obtain: {eqnarray*} S_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}}  S_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}}  S_0^+ &=& S_1 {eqnarray*}  Again our equations for S_2^+ and S_1^+ share a common term, which becomes a single AND gate in the implementation shown to the right.      {Identifying an Initial State}  Let's say that you go the lab and build the implementation above,  hook it up to the light, and turn it on.  Does it work?  Sometimes. Sometimes it works perfectly, but sometimes the light glows cyan or red briefly first. At other times, the light is an unchanging white.   What could be going wrong?  Let's try to understand.  We begin by deriving K-maps for the implementation, as shown to the right.  In these K-maps, each of the x's in our design has been replaced by either a 0 or a 1.  These entries are highlighted with green italics.   {file=part3/figs/colS2-bad.eps,width=1.00in}  {file=part3/figs/colS1-bad.eps,width=1.00in}  {file=part3/figs/colS0-bad.eps,width=1.00in}   Now let's imagine what might happen if somehow our FSM got into the S_2S_1S_0=111 state.  In such a state, the light would appear white, since RGB=S_2S_1S_0=111.  What happens in the next cycle?  Plugging into the equations or looking into the K-maps gives (of course) the same answer: the next state is the S_2^+S_1^+S_0^+=111 state. In other words, the light stays white indefinitely!  As an exercise, you should check what happens  if the light is red or cyan.  We can extend the transition diagram that we developed for our design with the extra states possible in the implementation, as shown below. As with the five states in the design, the extra states are named with the color of light that they produce.  {{file=part3/figs/colors-full.eps,width=5.8in}}  Notice that the FSM does not move out of the WHITE state (ever).    You may at this point wonder whether more careful decisions  in selecting our next-state expressions might address this issue. To some extent, yes.  For example, if we replace the  S_2S_1 term in the equation for S_2^+ with S_2{{S_0}},  a decision allowed by the ``don't care'' boxes in the K-map for our design, the resulting transition diagram does not suffer from the problem that we've found.  However, even if we do change our implementation slightly, we need to address another aspect of the problem:  how can the FSM",
    "GPT-3-Semantic-Search-Generations": {
      "question": "How can the FSM ever get into the unexpected states?\\n",
      "answer": "\nThe FSM can get into unexpected states if the inputs are not set properly. For example, if the input is set to 111, the FSM will stay in the WHITE state indefinitely."
    }
  },
  {
    "textbook-paragraph": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is the initial state of the three flip-flops in our implementation?\\n",
      "answer": "\nThe initial state of the three flip-flops in our implementation is the 0 state. This is because we have an explicit mechanism for initialization that forces each flip-flop into the 0 state. Alternatively, we could add some extra logic to our design that would allow us to force the FSM into any specific state in the next cycle."
    }
  },
  {
    "textbook-paragraph": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What is the first approach to making use of the flip-flop design?\\n",
      "answer": "\nThe first approach is to use the flip-flop design to add a single bit and pass any necessary information into another copy of itself. By using copies of this bit-sliced adder circuit, we can mimic our approach as humans and build adders of any size."
    }
  },
  {
    "textbook-paragraph": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the initial state of the three flip-flops in our implementation?\\n",
      "answer": "\nThe initial state of the three flip-flops is unknown unless we have an explicit mechanism for initialization. Initialization can work in two ways. The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input. Alternatively, we can add some extra logic to our design."
    }
  },
  {
    "textbook-paragraph": "enable input (we called it WE) to control when the latch copies its input into the stored bit. A symbol for a gated D latch appears to the right.  Note that we have dropped the name P in favor of , since P= in a gated D latch.   {The Clock Abstraction}  High-speed logic designs often use latches directly.  Engineers specify the number of latches as well as combinational logic  functions needed to connect one latch to the next, and the CAD tools optimize the combinational logic. The enable inputs of successive groups of latches are then driven by what we call a clock signal, a single bit line distributed across most of the chip that alternates between 0 and 1 with a regular period.  While the clock is 0, one set of latches holds its bit values fixed, and combinational logic uses those latches as inputs to produce  bits that are copied into a second set of latches.  When the clock switches to 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches. Of course, some of the latches in the first and third sets may be the same.  The timing of signals in such designs plays a critical role in their correct operation.  Fortunately, we have developed powerful abstractions  that allow engineers to ignore much of the complexity while thinking about the Boolean logic needed for a given design.  Towards that end, we make a simplifying assumption for the rest of our class, and for most of your career as an undergraduate: the clock signal is a { square wave} delivered uniformly across a chip.  For example, if the period of a clock is 0.5 nanoseconds (2 GHz), the clock signal is a 1 for 0.25 nanoseconds, then a 0 for 0.25 nanoseconds.  We assume that the clock signal changes instantaneously and at the same time across the chip.  Such a signal can never exist in the real world: voltages do not change instantaneously, and the  phrase ``at the same time'' may not even make sense at these scales. However, circuit designers can usually provide a clock signal that is close enough, allowing us to forget for now that no physical signal can meet our abstract definition.    SSL altered terminology on 3 Dec 21     The device shown to the right is a { master-slave} implementation of a   The device shown to the right is a { dual-latch} implementation of a  { positive edge-triggered} D flip-flop.  As you can see, we have  constructed it from two gated D latches with opposite senses of write enable.  The ``D'' part of the name has the same meaning as with a gated D latch: the bit stored is the same as the one delivered   {file=part2/figs/latch-step-8.eps,width=2.75in}   {file=part2/figs/latch-step-9.eps,width=0.95in}   to the input.  Other variants of flip-flops have also been built, but this  type dominates designs today.  Most are actually generated automatically from hardware ``design'' languages (that is, computer programming languages for hardware design).  When the clock is low (0), the first latch copies its value from the flip-flop's D input to the midpoint (marked X in our figure, but not usually given a name).  When the clock is high (1), the second latch copies its value from X to the flip-flop's output Q. Since X can not change when the clock is high, the result is that the output changes each time the clock changes from 0 to 1, which is called the { rising edge} or { positive edge} (the derivative) of the clock signal.  Hence the qualifier  ``positive edge-triggered,'' which describes the flip-flop's behavior.  The ``master-slave'' implementation refers to the use of two latches.  The ``dual-latch'' implementation refers to the use of two  latches.{Historically, this implementation was called ``master-slave,'' but ECE Illinois has decided to eliminate use of such terminology.} latches. In practice, flip-flops are almost never built this way.  To see a  commercial design, look up 74LS74, which uses six",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How can we make use of the internal reset lines on the latches to force each flip-flop into the 0 state?\\n",
      "answer": "\nOne way to make use of the internal reset lines on the latches is to force each flip-flop into the 0 state. This can be done by connecting the reset lines of the latches to a 0 voltage source."
    }
  },
  {
    "textbook-paragraph": "Notes Set 2.4, for example, we showed a design for a logic slice that compares  two bits at a time.  In general, we can optimize logic for any  number of bits and then apply multiple copies of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generalization of the serialization approach), or in a combination of the two.  Sometimes these tradeoffs may happen at a higher level. As mentioned in Notes Set 2.3, computer software uses  the carry out of an adder to perform addition of larger  groups of bits (over multiple clock cycles) than is supported by the processor's adder hardware.  In computer system design, engineers often design hardware elements that are general enough to support this kind of extension in software.  As a concrete example of the possible tradeoffs, consider a serial comparator design based on the {2-bit} slice variant.  This approach leads to a serial design with 24 gates and 10 inverters,  which is not much larger than our earlier serial design.  In terms of gate delays, however, the new design is identical, meaning that we finish a comparison in half the time.  More realistic area and timing metrics show slightly more difference between the two designs.  These differences can  dominate the  results if we blindly scale the idea to handle more bits without  thinking carefully about the design.  Neither many-input gates nor gates driving many outputs work well in practice.    {Finite State Machines}  A { finite state machine} (or { FSM}) is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs.  In any given state, a particular input may cause the FSM to move to another state; this  combination is called a { transition rule}.  An FSM comprises five parts: a finite set of states, a set of  possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs.    When an FSM is implemented as a digital system, all states must be represented as patterns using a fixed number of bits, all inputs must be translated into bits, and all outputs must be translated into bits.  For a digital FSM, transition rules must be { complete}; in other words, given any state of the FSM, and any pattern of input bits, a transition must be defined from that state to another state  (transitions from a state to itself, called { self-loops},  are acceptable).  And, of course, calculation of outputs for a digital FSM reduces to Boolean logic expressions.  In this class, we focus on clocked synchronous FSM implementations, in which the FSM's internal state bits are stored in flip-flops.  In this section, we introduce the tools used to describe, develop, and analyze implementations of FSMs with digital logic.  In the next few weeks, we will show you how an FSM can serve as the central control logic in a computer.  At the same time, we will illustrate connections between FSMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital  control systems.  The table below gives a { list of abstract states} for a typical  keyless entry system for a car.  In this case, we have merely named the states rather than specifying the bit patterns to be used for each state---for this reason, we refer to them as abstract states. The description of the states in the first column is an optional element often included in the early design stages for an FSM, when identifying the states needed for the design. A list may also include the outputs for each state.  Again, in the list below, we have specified these outputs abstractly.  By including outputs for each state, we implicitly assume that outputs depend only on the state of the FSM.  We discuss this assumption in more detail later in these notes (see ``Machine Models''),  but will make the assumption throughout",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How can we add multiplexers to each of our flip-flop inputs to choose between normal operation and forcing the FSM into a specific state?']",
      "answer": "\nThe passage discusses the design of a serial comparator using a bit-sliced approach. This approach leads to a design with 24 gates and 10 inverters, which is not much larger than the earlier serial design. However, the new design is identical in terms of gate delays, meaning that a comparison can be finished in half the time."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What does the second version of the table imply about the buttons?\\n",
      "answer": "\nThe second version of the table implies that the buttons are not placed in the most ergonomic way possible. The buttons are too close together and there is not enough space between the buttons and the edge of the table."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the two states in the first column of the table?\\n",
      "answer": "\nThe two states in the first column of the table are the states of the FSM when F=0. In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the four counting states?\\n",
      "answer": "\nThe four counting states are:\n\n1) State A: The timer is set to A when S_2=0. The duration of this state is A.\n\n2) State B: The timer is set to B when S_0=1. The duration of this state is B.\n\n3) State C: The timer is set to C when S_0=0. The duration of this state is C.\n\n4) State D: The timer is set to D when S_2=1. The duration of this state is D."
    }
  },
  {
    "textbook-paragraph": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the four halted states?\\n",
      "answer": "\nThere are four halted states: Halt A, Halt B, Halt C, and Halt D. In each of these states, the counter retains its current value until the user presses the \"go\" button."
    }
  },
  {
    "textbook-paragraph": "away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically.   Our machine will not accept pennies, so let's have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows five types of coins, their value in  dollars, and their value in terms of nickels.  The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value.  Let's   {l|c|c} {c|}{coin type}& value& # of nickels  nickel&      0.05& 1 dime&        0.10& 2 quarter&     0.25& 5 half dollar& 0.50& 10 dollar&      1.00& 20    decide to use six bits to record the number of nickels, which allows the machine to keep track of up to 3.15 (63 nickels).  We call the abstract states { STATE00} through { STATE63}, and refer to a state with an inserted value of N nickels as { STATE{<}N{>}}.  Let's now create a next-state table, as shown at the top of the next page. The user can insert one of the five coin types, or can pick one of the  three items.  What should happen if the user inserts more money than the  FSM can track?  Let's make the FSM reject such coins.  Similarly, if the  user tries to buy an item without inserting enough money first, the FSM  must reject the request.  For each of the possible input events, we add a  condition to separate the FSM states that allow the input event to  be processed as the user desires from those states that do not.  For example, if the user inserts a quarter, those states with N<59 transition to states with value N+5 and accept the quarter.  Those states with N reject the coin and remain in { STATE{<}N{>}}.   {c|l|c|l|c|c} &&& {|c}{final state} &&& {|c}{}& & release  initial state& {|c|}{input event}& condition& {|c}& & product  { STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none { STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none { STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none { STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none { STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none { STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none { STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1 { STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2 { STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none { STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3 { STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none    We can now begin to formalize the I/O for our machine.  Inputs include  insertion of coins and selection of items for purchase.  Outputs include a signal to accept or reject an inserted coin as well as signals to release each of the three items.   For input to the FSM, we assume that a coin inserted in any given cycle  is classified and delivered to our FSM using the three-bit representation  shown to the right.  For item selection, we assume that the user has access to three buttons, B_1, B_2, and B_3, that indicate a desire to purchase the  corresponding item.  For output, the FSM must produce a signal A indicating whether a coin should be accepted.  To control the release of items that have been purchased, the FSM must produce the signals R_1, R_2, and R_3, corresponding to the re-   {l|c} {c|}{coin type}& C_2C_1C_0  none&        110 nickel&      010 dime&        000 quarter&     011 half dollar& 001 dollar&      111    lease of each item.  Since outputs in our class depend only on state, we",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the input behavior?\\n",
      "answer": "\nThe input behavior of the machine is based on the type of coin inserted and the selection of an item for purchase. If the user inserts a coin that is not a nickel, the machine will reject the coin and remain in the same state. If the user inserts a nickel, the machine will transition to the next state and accept the coin. If the user selects an item for purchase, the machine will release the item if the user has inserted enough money. Otherwise, the machine will remain in the same state."
    }
  },
  {
    "textbook-paragraph": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the output behavior?\\n",
      "answer": "\nThe output behavior is that the add3 procedure takes three integers as arguments, adds them together, and returns the sum. The procedure is called with the constants 10, 20, and 30, and the result is printed. By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30."
    }
  },
  {
    "textbook-paragraph": "to implement a procedure and a call of that procedure.   {file=part4/figs/lec23-2.eps,width=1.25in}   [t] { [t]  int =add3 (int n1, int n2, int n3) { >return (n1 + n2 + n3); }  printf (``d'', add3 (10, 20, 30));    by convention: = n1 is in R0 >n2 is in R1 >n3 is in R2 >return value is in R6    [t]  add3:  = WWWW= WWWWW=  add3:>ADD>R0,R0,R1 >ADD>R6,R0,R2 >RETN > >PUSH>R1>; save the value in R1 >LDI>R0,#10>; marshal arguments >LDI>R1,#20 >LDI>R2,#30 >CALL>add3 >MOV>R1,R6>; return value becomes 2nd argument >LDI>R0,``d''>; load a pointer to the string >CALL>printf >POP>R1>; restore R1    The add3 procedure takes three integers as arguments, adds them together, and returns the sum.  The procedure is called with the constants 10, 20, and 30, and the result is printed.  By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, and R2 the value 30.  We assume that the caller wants to preserve the value of R1, but does not care about R3 or R5.  In the assembly language version on the right, R1 is first saved to the stack, then the arguments are marshaled into position, and finally the call is made.  The procedure itself needs no local storage and does not change any callee-saved registers, thus must simply add the numbers together and place the result in R6.  After add3 returns, its return value is moved from R6 to R1 in preparation for the call to printf.  After loading a pointer to the format string into R0, the second call is made, and R1 is restored, completing the translation.  { System calls} are almost identical to procedure calls.  As with procedure calls, a calling convention is used: before invoking a system call, arguments are marshaled into the appropriate registers or locations in the stack; after a system call returns, any result appears in a pre-specified register.  The calling convention used for system calls need not be the same as that used for procedure calls. Rather than a call instruction, system calls are usually initiated with a { trap} instruction, and system calls are also known as traps.  With many architectures, a system call places the processor in privileged or kernel mode, and the instructions that implement the call are considered to be part of the operating system.  The term system call arises from this fact.      {Interrupts and Exceptions*}  Unexpected processor interruptions arise both from interactions between a processor and external devices and from errors or unexpected behavior in the program being executed.  The term { interrupt} is reserved for asynchronous interruptions generated by other devices, including disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities.  { Exceptions} occur when a processor encounters an unexpected opcode or operand.  An undefined instruction, for example, gives rise to an exception, as does an attempt to divide by zero.  Exceptions usually cause the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intelligently.  The table below summarizes the characteristics of the two types and compares them to system calls.  { {|l|l|l|c|c|} {|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected  interrupt& external device& packet arrived at network card& yes& yes exception& invalid opcode or operand& divide by zero& no& yes trap/system call& deliberate, via trap instruction& print character to console& no& no   }  Interrupts occur asynchronously with respect to the program.  Most designs only recognize interrupts between instructions.  In other words,  the presence of interrupts is checked only after completing an instruction rather than in every cycle.  In pipelined designs, however, instructions execute simultaneously, and the decision as to which instructions occur ``before'' an interrupt and which occur ``after'' must be made by the processor.  Exceptions are not asynchronous in the sense that they occur",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What does the ``go'' button do?\\n",
      "answer": "\nThe \"go\" button initiates a system call that causes the processor to enter kernel mode and execute the instructions associated with the system call. These instructions are part of the operating system and are used to perform the desired action, such as printing a character to the console."
    }
  },
  {
    "textbook-paragraph": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the purpose of a Gray code?\\n",
      "answer": "\nA Gray code is a sequence of patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit. You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns."
    }
  },
  {
    "textbook-paragraph": "and lower-case checker, we can make use of two groups of seven {2-to-1} muxes, all controlled by a single bit select signal S, to choose between the inputs needed for an upper-case checker and those needed for a lower-case checker.  Specific configurations of multiplexers are often referred to as { N-to-M multiplexers}.  Here the value N refers to the number of inputs, and M refers to the number of outputs.  The number of select bits can then be calculated as _2(N/M)---N/M  is generally a power of two---and one way to build such a  multiplexer is to use M copies of an  (N/M)-to-1 multiplexer.  Let's extend our upper- and lower-case checker to check for four different ranges of ASCII characters, as shown below. This design uses two {28-to-7} muxes to create a single checker for the four ranges.  Each of the muxes in the figure logically  represents seven {4-to-1} muxes.  {{file=part2/figs/ascii-four-range.eps,width=3.75in}}   The table to the right describes the behavior of the checker.  When the select input S is set to 00, the left mux selects the value 0x00, and the right mux selects the value 0x1F, which checks whether the ASCII character represented by C is a control character.  When the select input S=01, the muxes produce the values needed to check whether C is an upper-case letter.  Similarly, when the select input S=10,   {c|c|c|c} & left& right&  & comparator& comparator&  S_1S_0& input& input& R(C) produced   00& 0x00& 0x1F& control character? 01& 0x41& 0x5A& upper-case letter? 10& 0x61& 0x7A& lower-case letter? 11& 0x30& 0x39& numeric digit?     the muxes produce the values  needed to check whether C is a lower-case letter.  Finally, when the select input S=11, the left mux selects the value 0x30, and the right mux selects the value 0x39, which checks whether the ASCII character represented by C is a digit (0 to 9).     {Example: Bit-Sliced Comparison}  This set of notes develops comparators for unsigned and 2's complement  numbers using the bit-sliced approach that we introduced in Notes Set 2.3.   We then use algebraic manipulation and variation of the internal  representation to illustrate design tradeoffs.   {Comparing Two Numbers}   Let's begin by thinking about how we as humans compare two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation, so we can just think of them as binary numbers with leading 0s.  We handle 2's complement values later in these notes.  As humans, we typically start comparing at the most significant bit. After all, if we find a difference in that bit, we are done, saving ourselves some time.  In the example to the right, we know that A<B  as soon as we reach bit 4 and observe that A_4<B_4.  If we instead start from the least significant bit, we must always look at all of the bits.  When building hardware to compare all of the bits at once, however, hardware for comparing each bit must exist, and the final result  must be able to consider      all of the bits.  Our choice of direction should thus instead depend on  how effectively we can build the  corresponding functions.  For a single bit slice, the two directions  are almost identical.  Let's develop a bit slice for comparing from least to most significant.  { NOTE TO SELF: We should either do the bit-by-bit comparator  state machine in the notes or as an FSM homework problem.  Probably the lattter.}  {An Abstract Model}  Comparison of two numbers, A and B, can produce three possible answers: A<B, A=B, or A>B (one can also build an equality comparator that combines the A<B and A>B cases into a single  answer).  As we move from bit to bit in our design, how much information needs  to pass from one bit to the next?  Here you may want to think about how you perform the task yourself. And perhaps to focus on the calculation for the most significant bit. You need to know the values of the two bits that you are comparing. If those two",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How many input bits are there?', \" ",
      "answer": "\nThere are two input bits."
    }
  },
  {
    "textbook-paragraph": "1&0       {eqnarray*} C&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0  S_1&=& A_1 {B_1} {B_0}+ A_1 {A_0} {B_1}+ {A_1} {A_0} B_1+ {A_1} B_1 {B_0}+ &&{A_1} A_0 {B_1} B_0+ A_1 A_0 B_1 B_0  S_0&=&A_0 {B_0}+{A_0} B_0 {eqnarray*}   In theory, {K-maps} extend to an arbitrary number of variables. Certainly Gray codes can be extended.  An { {N-bit} Gray code}  is a sequence of {N-bit} patterns that includes all possible patterns such that any two adjacent patterns differ in only one bit. The code is actually a cycle: the first and last patterns also differ in only one bit.  You can construct a Gray code recursively as follows: for an {(N+1)-bit} Gray code, write the sequence for an {N-bit} Gray code, then add a 0 in front of all patterns. After this sequence, append a second copy of the {N-bit} Gray code in reverse order, then put a 1 in front of all patterns in the second copy.  The result is an {(N+1)-bit} Gray code. For example, the following are Gray codes:   1-bit& 0, 1 2-bit& 00, 01, 11, 10 3-bit& 000, 001, 011, 010, 110, 111, 101, 100 4-bit&  0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000   Unfortunately, some of the beneficial properties of {K-maps} do not extend beyond two variables in a dimension.  { Once you have three variables in one dimension}, as is necessary if a function operates on five or more variables, { not all product terms are contiguous in the grid}.  The terms still require a total number of rows and columns equal to a power of two, but they don't all need to be a contiguous group.  Furthermore, { some contiguous groups of appropriate size do not correspond to product terms}.  So you can still make use of {K-maps} if you have more variables, but their use is a little trickier.  {Canonical Forms}  What if we want to compare two expressions to determine whether they represent the same logic function?  Such a comparison is a test of { logical equivalence}, and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem.  You know that any given function can be expressed in many ways, and that two expressions that look quite different may in fact represent the same function (look back at Equations () to () for an example).  But what if we rewrite the function using only  prime implicants?  Is the result unique?  Unfortunately, no.   In general, { a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants}.  For example, consensus terms may or may not be included in our expressions.  (They are necessary for reliable design of  certain types of systems, as you will learn in a later ECE class.)  The green ellipse in the K-map to the right represents the consensus  term BC. {eqnarray*} Z &=& A C +  B + B C Z &=& A C +  B {eqnarray*}       Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms.   The K-maps shown to the right, for example, illustrate  how one function might be written in either of the following ways:  {eqnarray*} Z &=&   D +  C  + A B C + B  D Z &=&   C + B C  + A B D +   D {eqnarray*}         When we need to compare two things (such as functions), we need to transform them into what in mathematics is known as a { canonical form}, which simply means a form that is defined so as to be unique  for each thing of the given type. What can we use for logic functions?  You already know two answers!    The { canonical sum} of a function (sometimes called the { canonical SOP form}) is the sum of minterms.    The { canonical product} of a function (sometimes called the { canonical POS form}) is the product of maxterms.    These forms technically only meet the mathematical definition of canonical if we agree on an order for the min/maxterms, but that problem is solvable.  However, as you already know, the forms are not particularly  convenient to use.    In practice, people and tools in the industry use more",
    "GPT-3-Semantic-Search-Generations": {
      "question": "What would happen if we didn't use a Gray code?\"]",
      "answer": "\nIf we didn't use a Gray code, we would have to compare two expressions to determine whether they represent the same logic function. This would be a test of logical equivalence, and is an important part of hardware design. However, tools today provide help with this problem, so it is not necessary to use a Gray code."
    }
  },
  {
    "textbook-paragraph": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What should happen if the user presses the ``halt'' button while in state { COUNT A}?\\n",
      "answer": "\nThere is no explicit answer to this question in the passage. However, based on the information given, it seems that if the user presses the ``halt'' button while in state { COUNT A}, the system should reset to state { HALT A}."
    }
  },
  {
    "textbook-paragraph": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What should happen if the user presses the ``go'' button while in state { HALT A}?\\n",
      "answer": "\nThere are two possible outcomes when the user presses the ``go'' button while in state { HALT A}. The first outcome is that the FSM moves to state { COUNT B}, which is the next state in the sequence. The second outcome is that the FSM resets to state { COUNT A}, which is the initial state."
    }
  },
  {
    "textbook-paragraph": "transitions specified so far are that when we press ``go'' (HG=01), the counter moves to  the { COUNT B} state, and that the counter remains halted in  state { HALT A} if no buttons are pressed (HG=00). What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?  For consistency, we decide that ``halt'' overrides ``go,'' but does nothing special if it alone is pressed while the counter is halted.  Thus, input patterns HG=10 and HG=11 also  take state { HALT A} back to itself. Here the arc could be labeled HG=00,10,11 or, equivalently, HG=00,1x or HG=x0,11.   To complete our design, we apply the same decisions that we made for  the { COUNT A} state to all of the other counting states, and the  decisions that we made for the { HALT A} state to all of the other  halted states.  If we had chosen not to specify an answer, an implementation could produce different behavior from the different counting and/or halted states, which might confuse a user.  The resulting design appears to the right.        {Choosing a State Representation}  Now we need to select a representation for the states.  Since our counter has eight states, we need at least three (_2 (8)=3) state bits S_2S_1S_0 to keep track of the current state.  As we show later, { the choice of representation for an FSM's states can dramatically affect the design complexity}.  For a design as simple as  our counter, you could just let a computer implement all possible  representations (there aren't more than 840, if we consider simple  symmetries) and select one according to whatever metrics are interesting.  For bigger designs, however, the number of possibilities quickly becomes impossible to explore completely.  Fortunately, { use of abstraction in selecting a representation  also tends to produce better designs} for a wide variety of metrics (such as design complexity, area, power consumption, and performance).  The right strategy is thus often to start by selecting a representation  that makes sense to a human, even if it requires more bits than are strictly necessary.  The resulting implementation will be easier to design and to debug than an implementation in which only the global  behavior has any meaning.   Let's return to our specific example, the counter.  We can use one bit,  S_2, to record whether or not our counter is counting (S_2=0) or halted (S_2=1).  The other two bits can then record the counter state in terms of the desired output.  Choosing this representation implies that only wires will be necessary to compute outputs Z_1  and Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting design, in which states are now labeled with both internal state and outputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version, we have changed the arc labeling to use logical expressions, which can sometimes help us to think about the implementation.      The equivalent state listing and state table appear below.  We have ordered the rows of the state table in Gray code order to simplify transcription of K-maps.    & S_2S_1S_0&  { COUNT A}& 000& counting, output Z_1Z_0=00 { COUNT B}& 001& counting, output Z_1Z_0=01 { COUNT C}& 011& counting, output Z_1Z_0=11 { COUNT D}& 010& counting, output Z_1Z_0=10  { HALT A}& 100& halted, output Z_1Z_0=00  { HALT B}& 101& halted, output Z_1Z_0=01  { HALT C}& 111& halted, output Z_1Z_0=11  { HALT D}& 110& halted, output Z_1Z_0=10   {rc|cccc} &&{HG} &S_2S_1S_0& 00& 01& 11& 10  { COUNT A}&000& 001& 001& 100& 100 { COUNT B}&001& 011& 011& 101& 101 { COUNT C}&011& 010& 010& 111& 111 { COUNT D}&010& 000& 000& 110& 110  { HALT D}&110& 110& 000& 110& 110  { HALT C}&111& 111& 010& 111& 111  { HALT B}&101& 101& 011& 101& 101  { HALT A}&100& 100& 001& 100& 100    Having chosen a representation, we can go ahead and implement our design in the usual way.  As shown to the right, K-maps for the  next-state logic are complicated, since we have five variables and must consider implicants that are not contiguous in",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What if the ``halt'' button is pressed (HG=10), or both buttons are pressed (HG=11)?\"]",
      "answer": "\nIf the ``halt'' button is pressed (HG=10), the counter remains in the halted state. If both buttons are pressed (HG=11), the counter also remains in the halted state."
    }
  },
  {
    "textbook-paragraph": "0& 0  EXTRA1& 011& 000& 100& x& x EXTRA2& 111& 000& 100& x& x    states, but left the output bits as don't cares.  A state transition diagram appears at the bottom of this page.  {Testing the Design}  Having a complete design on paper is a good step forward, but humans make mistakes at all stages.  How can we know that a circuit that we build in the lab correctly implements the FSM that we have outlined  in these notes?  For the lab design, we have two problems to solve.  First, we have not specified an initialization scheme for the FSM. We may want the FSM to start in one of the PAID states, but adding initialization logic to the design may mean requiring you to wire together significantly more chips.  Second, we need a sequence of inputs that manages to test that all of the next-state and output logic implementations are correct.  Testing sequential logic, including FSMs, is in general extremely difficult. In fact, large sequential systems today are generally converted into  combinational logic by using shift registers to fill the  flip-flops with a particular pattern,  executing the logic for one clock cycle, and checking that the resulting  pattern of bits in the flip-flops is correct.  This approach is called  { scan-based testing}, and is discussed in ECE 543.  You  will make use of a similar approach when you test your combinational logic in the second week of the lab, before wiring up the flip-flops.  We have designed our FSM to be easy to test (even small FSMs may be challenging) with a brute force approach.  In particular, we  identify two input sequences that together serve both to initialize and  to test a correctly implemented variant of our FSM.  Our initialization sequence forces the FSM into a specific state regardless of its initial state.  And our test sequence crosses every transition arc leaving the six valid states.    In terms of T, the coin type, we initialize the FSM with the input sequence 001.  Notice that such a sequence takes any initial  state into PAID2.  For testing, we use the input sequence 111010010001.  You should trace  this sequence, starting from PAID2, on the diagram below to see how the test sequence covers all of the possible arcs.  As we test, we need also to observe the A and P outputs in each state to check the output logic.  {{file=part3/figs/lab-diag-notes.eps,width=4.25in}}    {Finite State Machine Design Examples, Part II}  This set of notes provides several additional examples of FSM design. We first design an FSM to control a vending machine, introducing encoders and decoders as components that help us to implement our design.  We then design a game controller for a logic puzzle implemented as a children's game.  Finally, we analyze a digital FSM designed to control the stoplights at the intersection of two roads.   {Design of a Vending Machine}  For the next example, we design an FSM to control a simple vending machine.   The machine accepts {U.S. coins}{Most countries have small  bills or coins in demoninations suitable for vending machine prices, so think  about some other currency if you prefer.} as payment and offers a choice of three items for sale.  What states does such an FSM need?  The FSM needs to keep track of how much money has been inserted in order to decide whether a user can  purchase one of the items.  That information alone is enough for the simplest machine, but let's create a machine with adjustable item prices.  We can use registers to hold the item prices, which  we denote P_1, P_2, and P_3.  Technically, the item prices are also part of the internal state of the  FSM.  However,  we leave out discussion (and, indeed, methods) for setting the item prices, so no state with a given combination of prices has any  transition to a state with a different set of item prices. In other words, any given combination of item prices induces a subset  of states that operate independently of the subset induced by a distinct  combination of item prices.  By abstracting",
    "GPT-3-Semantic-Search-Generations": {
      "question": " How many states does an FSM need in order to have eight states?\\nA: 3 \\n\\n",
      "answer": "\nThe FSM needs at least three states in order to have eight states. This is because there are three possible items that can be purchased, and each item can be purchased in one of two ways (with or without enough money)."
    }
  },
  {
    "textbook-paragraph": "tradeoffs in the sparsity of code words with error  detection and correction capabilities.}      {   }   blank 3rd page     {Example: Bit-Sliced Addition}  In this set of notes, we illustrate basic logic design using integer addition as an example.  By recognizing and mimicking the structured  approach used by humans to perform addition, we introduce an important  abstraction for logic design.  We follow this approach to design an adder known as a ripple-carry adder, then discuss some of the  implications of the approach and highlight how the same approach can  be used in software.  In the next set of notes, we use the same technique to design a comparator for two integers.  {One Bit at a Time}  Many of the operations that we want to perform on groups of bits can  be broken down into repeated operations on individual bits.  When we add two binary numbers, for example, we first add the least significant bits, then move to the second least significant, and so on. As we go, we may need to carry from lower bits into higher bits.  When we compare two (unsigned) binary numbers with the same number of bits, we usually start with the most significant bits and move downward in significance until we find a difference or reach the end of the two  numbers.  In the latter case, the two numbers are equal.  When we build combinational logic to implement this kind of calculation, our approach as humans can be leveraged as an abstraction technique. Rather than building and optimizing a different Boolean function for  an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size that we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this { bit-sliced} adder circuit, we can mimic our approach as humans and build adders of any size, just as we expect that a human could add two binary numbers of any size.  The resulting designs are, of course, slightly less efficient than designs that are optimized for their specific purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the tradeoff an interesting one.   {Abstracting the Human Process}   Think about how we as humans add two {N-bit} numbers, A and B.  An illustration appears to the right, using N=8.  For now, let's assume that our numbers are stored in an unsigned  representation.  As you know, addition for 2's complement is identical except for the calculation of overflow.  We start adding from the least significant bit and move to the left. Since adding two 1s can overflow a single bit, we carry a 1 when necessary into the next column.  Thus, in general, we are actually adding three input bits.  The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank.  Focus now on the addition of a single column.  Except for the first and last bits, which we might choose to handle slightly  differently, the addition process is identical  for any column.  We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column.  Column addition is the task that our bit slice logic must perform.  The diagram to the right shows an abstract model of our  adder bit slice.  The inputs from the next least significant bit come in from the right.  We include arrowheads because  figures are usually drawn with inputs coming from the top or left  and outputs going to the bottom or right.  Outside of the bit slice logic, we index the carry bits using the    {{file=part2/figs/add-abs.eps,width=1.90in}}     bit number.  The bit slice has C^M provided as an input and  produces C^{M+1} as an output.  Internally, we use C_ to denote the carry input, and C_ to denote the carry output.  Similarly, the bits A_M and B_M from the numbers A and B are represented internally as A and B, and the bit S_M produced for the sum S is",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the right strategy for choosing a representation?\\nA: The right strategy is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary.', '\\n",
      "answer": "\nThe right strategy for choosing a representation is to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. This will make the design process simpler and more efficient in the long run."
    }
  },
  {
    "textbook-paragraph": "compact  approaches when comparing functions, but those solutions are a subject for a later class (such as ECE 462).     {Two-Level Logic}   { Two-level logic} is a popular way of expressing logic functions. The two levels refer simply to the number of functions through which an input passes to reach an output, and both the SOP and POS forms are  examples of two-level logic.  In this section, we illustrate one of the  reasons for this popularity and  show you how to graphically manipulate expressions, which can sometimes help when trying to understand gate diagrams.  We begin with one of DeMorgan's laws, which we can illustrate both  algebraically and graphically: C  =  B+A  =      {file=part2/figs/demorgan-nand.eps,width=0.95in}     Let's say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ.  The diagram on the left below shows the function constructed from three AND gates and an OR gate.  Using DeMorgan's law, we can replace the OR gate with a NAND with inverted inputs. But the bubbles that correspond to inversion do not need to sit at the input to the gate.  We can invert at any point along the wire, so we slide each bubble down the wire to the output of the first column of AND gates.  { Be careful: if the wire splits, which does not happen in our example, you have to replicate the inverter onto the other output paths as you slide past the split point!}  The end result is shown on the right: we have not changed the  function, but now we use only NAND gates.  Since CMOS technology only supports NAND and NOR directly, using two-level logic makes it simple to map our expression into CMOS gates.  {file=part2/figs/SOP-equiv.eps,width=6.5in}   You may want to make use of DeMorgan's other law, illustrated graphically to the right, to perform the same transformation on a POS expression.  What do you get?   {file=part2/figs/demorgan-nor.eps,width=0.95in}    {Multi-Metric Optimization}  As engineers, almost every real problem that you encounter will admit  multiple metrics for evaluating possible designs.  Becoming a good engineer thus requires not only that you be able to solve problems creatively so as to improve the quality of your solutions, but also that you are aware of how people might evaluate those solutions and are able both to identify the most important metrics and to balance  your design effectively according to them.  In this section, we introduce some general ideas and methods that may be of use to you in this regard.  { We will not test you on the concepts in this section.}  When you start thinking about a new problem, your first step should be to think carefully about metrics of possible interest.  Some important metrics may not be easy to quantify.    For example, compatibility of a design with other products already  owned by a customer has frequently defined the success or failure of computer hardware and software solutions.  But how can you compute the compability of your approach as a number?  Humans---including engineers---are not good at comparing multiple metrics simultaneously.  Thus, once you have a set of metrics that you feel is complete,  your next step is to get rid of as many as you can.  Towards this end, you may identify metrics that have no practical  impact in current technology, set threshold values for other metrics to simplify reasoning about them, eliminate redundant metrics, calculate linear sums to reduce the count of metrics, and, finally, make use of the notion of Pareto optimality.  All of these ideas are described in the rest of this section.  Let's start by considering metrics that we can quantify as real numbers.  For a given metric, we can divide possible measurement values into three ranges.  In the first range, all measurement values are equivalently useful. In the second range,  possible values are ordered and interesting with respect to one another. Values in the third range are all impossible to use in practice. Using power consumption as our example, the first",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What are the benefits of using abstraction when selecting a state representation?\\n",
      "answer": "\nAbstraction is a powerful tool that can help simplify complex problems. When selecting a state representation, abstraction can help identify the most important aspects of the problem and ignore irrelevant details. This can make it easier to find a solution that is both effective and efficient."
    }
  },
  {
    "textbook-paragraph": "if a chip finishes its work, the chip can turn itself off, saving energy.  How do such high-level metrics relate to the problem at hand?  Only indirectly in practice.  There are too many factors involved to make direct calculations of cost, power, or performance at the level of logic expressions.    Finding an { optimal} solution---the best formulation of a specific logic function for a given metric---is often impossible using the  computational resources and algorithms available to us.  Instead, tools typically use heuristic approaches to find solutions that strike a balance between these metrics. A { heuristic} approach is one that is believed to yield fairly good solutions to a problem, but does not necessarily find an optimal solution.  A human engineer can typically impose { constraints}, such as limits on the chip area or limits on the minimum performance, in order to guide the process.  Human engineers may also restructure the implementation of a  larger design, such as a design to perform floating-point arithmetic, so as to change the logic functions used in the design.  { Today, manipulation of logic expressions for the purposes of  optimization is performed almost entirely by computers.}  Humans must supply the logic functions of interest, and must program the acceptable  transformations between equivalent forms, but computers do the grunt work of comparing alternative formulations and deciding which one is best to use in context.  Although we believe that hand optimization of Boolean expressions is no longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization.  The rationale for retaining this exposure is  threefold.   First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and logical equivalence checking (answering the question, ``Do two  expressions represent the same function?'').  Second, the complexity of the problem is a good way to introduce you to real engineering.  Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today.  Towards that end, we conclude this introduction by discussing two metrics that engineers traditionally used to optimize  logic expressions.  These metrics are now embedded in { computer-aided  design} ({ CAD}) tools and tuned to specific underlying technologies, but the reasons for their use are still interesting.  The first metric of interest is a heuristic for the area needed for a design.  The measurement is simple: count the number of variable occurrences in an expression.  Simply go through and add up how many variables you see. Using our example function C,  Equation () gives a count of 6, Equation () gives a count of 8, and Equation () gives a count of 24. Smaller numbers represent better expressions, so  Equation () is the best choice by this metric.  Why is this metric interesting? Recall how gates are built from transistors. An {N-input} gate requires roughly 2N transistors, so if you  count up the number of variables in the expression, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design.  A variation on variable counting is to add the number of operations, since each gate also takes space for wiring (within as well as between gates).  Note that we ignore the number of inputs to the operations, so a {2-input} AND counts as 1, but a {10-input} AND also counts as 1.  We do not usually count complementing variables as an operation for this metric because the complements of variables are sometimes  available at no extra cost in gates or wires.  If we add the number of operations in our example, we get a count of 10 for Equation ()---two ANDs, two ORs, and 6 variables,  a count of 12 for Equation ()---three",
    "GPT-3-Semantic-Search-Generations": {
      "question": " Why is it important to start with a representation that makes sense to a human?\\n",
      "answer": "\nOne reason it is important to start with a representation that makes sense to a human is that humans must supply the logic functions of interest, and must program the acceptable transformations between equivalent forms. A second reason is that the complexity of the problem is a good way to introduce you to real engineering. Finally, the contextual information will help you to develop a better understanding of finite state machines and higher-level abstractions that form the core of digital systems and are still defined directly by humans today."
    }
  },
  {
    "textbook-paragraph": "the K-maps. The S_2^+ logic is easy enough: we only need two terms,  as shown.  Notice that we have used color and line style to distinguish different          implicants in the K-maps.  Furthermore, the symmetry of the design produces symmetry in the S_1^+ and S_0^+ formula, so we have used the same color and line style for analogous terms in these two K-maps.  For S_1^+, we need four terms.  The green  ellipses in the HG=01 column are part of the same term, as are the two halves of the dashed blue circle.  In S_0^+, we still need four terms, but three of them are split into two pieces  in the K-map.  As you can see, the utility of the K-map is starting to break down with five variables.   {Abstracting Design Symmetries}  Rather than implementing the design as two-level logic, let's try to take advantage of our design's symmetry to further simplify the logic (we reduce gate count at the expense of longer, slower paths).  Looking back to the last transition diagram, in which the arcs were labeled with logical expressions, let's calculate an expression for when the counter should retain its current value in the next cycle.  We call  this variable HOLD.  In the counting states, when S_2=0,  the counter stops (moves into a halted state without changing value)  when H is true. In the halted states, when S_2=1, the counter stops (stays in  a halted state) when H+ is true.  We can thus write  {eqnarray*} HOLD &=& {S_2}  H + S_2  ( H +  ) HOLD &=& {S_2} H + S_2 H + S_2  HOLD &=& H + S_2  {eqnarray*}  In other words, the counter should hold its current  value (stop counting) if we press the ``halt'' button or if the counter was already halted and we didn't press the ``go'' button.  As desired, the current value of the counter (S_1S_0) has no impact on this  decision.  You may have noticed that the expression we derived for HOLD also matches S_2^+, the next-state value of S_2 in the  K-map on the previous page.  Now let's re-write our state transition table in terms of HOLD.  The left version uses state names for clarity; the right uses state values to help us transcribe K-maps.  { {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& { COUNT B}& { HALT A} { COUNT B}&001& { COUNT C}& { HALT B} { COUNT C}&011& { COUNT D}& { HALT C} { COUNT D}&010& { COUNT A}& { HALT D}  { HALT A}&100& { COUNT B}& { HALT A}  { HALT B}&101& { COUNT C}& { HALT B}  { HALT C}&111& { COUNT D}& { HALT C}  { HALT D}&110& { COUNT A}& { HALT D}  {rc|cc} &&{HOLD} &S_2S_1S_0& 0& 1  { COUNT A}&000& 001& 100 { COUNT B}&001& 011& 101 { COUNT C}&011& 010& 111 { COUNT D}&010& 000& 110  { HALT A}&100& 001& 100  { HALT B}&101& 011& 101  { HALT C}&111& 010& 111  { HALT D}&110& 000& 110    The K-maps based on the HOLD abstraction are shown to the right. As you can see, the necessary logic has been simplified substantially, requiring only two terms each for both S_1^+ and S_0^+.  Writing the next-state logic algebraically, we obtain  {eqnarray*} S_2^+ &=& HOLD S_1^+ &=&   S_0 + HOLD  S_1 S_0^+ &=&   {{S_1}} + HOLD  S_0 {eqnarray*}          Notice the similarity between the equations for S_1^+S_0^+ and the  equations for a {2-to-1} mux: when HOLD=1, the counter retains  its state, and when HOLD=0, it counts.      An implementation appears below.  By using semantic meaning in our choice of representation---in particular the use of S_2 to record whether the counter is currently halted (S_2=1) or counting (S_2=0)---we have enabled ourselves to  separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLD bit in the diagram is used to determine whether or not the counter should advance in the current cycle.  Let's check that the implementation matches our original design.  Start by verifying that the HOLD variable is calculated correctly, HOLD=H+S_2, then look back at the K-map for S_2^+ in the low-level design to verify that the expression we used does indeed match.    Next, check the mux",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the value of S_2 when the counter is in the HALT A state and HOLD=1?\\n",
      "answer": "\nS_2 is 1 when the counter is in the HALT A state and HOLD=1. This is because when S_2=1, the counter is halted, and when HOLD=1, the counter should retain its current value."
    }
  },
  {
    "textbook-paragraph": "these outputs is given in the table to the right.   {c|c} L& light color  0x& red 10& yellow 11& green    Let's think about the basic operation of the controller.  For safety reasons, the controller must ensure that the lights on one or both roads are red at all times.    Similarly, if a road has a green light, the controller should  show a yellow light before showing a red light to give drivers some warning and allow them to slow down.  Finally, for fairness, the controller should alternate green lights between the two roads.  Now take a look at the logic diagram below.  The state of the FSM has been split into two pieces: a {3-bit}  register S and a {6-bit} timer.  The timer is simply a binary  counter that counts downward and produces an output of Z=1 when it  reaches 0.  Notice that the register S only takes a new value when the timer reaches 0, and that the Z signal from the timer also forces a new value to be loaded into the timer in the next  cycle.  We can thus think of transitions in the FSM on a cycle by  cycle basis as consisting of two types.  The first type simply counts downward for a number of cycles while holding the register S constant, while the second changes the value of S and sets the timer in order to maintain the new value of S  for some number of cycles.    3.45   Let's look at the next-state logic for S, which feeds into the IN inputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice  that none of the inputs to the FSM directly affect these values.  The states of S thus act like a counter.  By examining the connections, we can derive equations for the next state and draw a transition diagram, as shown to the right.  As the figure shows, there are six states in the loop defined by the  next-state logic, with the two remaining states converging into the loop after a single cycle.  Let's now examine the outputs for each state in order to understand how the stoplight sequencing works.  We derive equations for the outputs that control the lights, as shown to the right, then calculate values and colors for each state, as shown to the far right.  For completeness, the table  includes the states outside of the desired loop.  The  lights are all red in both of these states, which is necessary for safety.   {eqnarray*}  S_2^+ &=& {S_2} + S_0 S_1^+ &=& {S_2}  S_1 S_0^+ &=& {S_2} {eqnarray*}  {eqnarray*} L_1^ &=& S_2 S_1 L_0^ &=& S_0 L_1^ &=& S_2 {S_1} L_0^ &=& S_0 {eqnarray*}    {c|cc|cc} &&& EW& NS &&& light& light S& L^& L^& color& color  000& 00& 00&    red&    red 111& 11& 01&  green&    red 110& 10& 00& yellow&    red 010& 00& 00&    red&    red 101& 01& 11&    red&  green 100& 00& 10&    red& yellow  001& 01& 01&    red&    red 011& 01& 01&    red&    red     Now let's think about how the timer works.  As we already noted, the timer value is set whenever S enters a new state, but it can also be set under other conditions---in particular, by the signal F calculated at the bottom of the FSM logic diagram.     For now, assume that F=0.  In this case, the timer is set only when the state S changes, and we can find the duration of each state by analyzing the muxes.  The bottom mux selects A when S_2=0, and  selects the output of the top mux when S_2=1.  The top mux selects B when S_0=1, and selects C when S_0=0.  Combining these results, we can calculate the duration of the next states of S when F=0,  as shown in the table to the right.  We can then combine the next state duration with our previous calculation of the state sequencing  (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.   {c|cc|cc} & EW& NS& next& current & light& light& state& state S& color& color& duration& duration  000&    red&    red& A& C 111&  green&    red& B& A 110& yellow&    red& C& B 010&    red&    red& A& C 101&    red&  green& B& A 100&    red& yellow& C& B  001&    red&    red& A& --- 011&    red&    red& A& ---    What does F do?  Analyzing the gates that",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the value of S_1S_0 when the counter is in the COUNT A state and HOLD=1?', '\\n\\n",
      "answer": "\nThe value of S_1S_0 when the counter is in the COUNT A state and HOLD=1 is 01."
    }
  },
  {
    "textbook-paragraph": "ever get into the unexpected states?   What is the initial state of the three flip-flops in our implementation?  { The initial state may not even be 0s and 1s unless we have an  explicit mechanism for initialization.}   Initialization can work in two ways.    The first approach makes use of the flip-flop design. As you know, a flip-flop is built from a pair of latches, and we can  make use of the internal reset lines on these latches to force each flip-flop into the 0 state (or the 1 state) using an additional input.   Alternatively, we can add some extra logic to our design.  Consider adding a few AND gates and a  input (active low), as shown in the dashed box in the figure to the right. In this case, when we assert  by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state.  The approach taken here is for clarity; one can optimize the design, if desired.  For example, we could simply connect  as an extra input into the three AND gates on the left rather than adding new ones, with the same effect.  We may sometimes want a more powerful initialization mechanism---one that allows us to force the FSM into any specific state in the next cycle.  In such a case, we can add multiplexers to each of our  flip-flop inputs, allowing us to use the INIT input to choose between normal operation (INIT=0) of the FSM and forcing the FSM into the next state given by I_2I_1I_0 (when INIT=1).         {Developing an Abstract Model}   We are now ready to discuss the design process for an FSM from start to finish.  For this first abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We now want a counter that allows us to start and stop the   {c|ccc} state&    no input&  halt button& go button  counting& counting&      halted&  halted&   halted&              & counting    count.  What is the mechanism for stopping and starting?  To begin our design, we could sketch out an abstract next-state table such as the one shown to the right above.  In this form of the table, the first column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination.   The table contains two states, counting and halted, and specifies that the design uses two distinct buttons to move between the states. The table further implies that if the counter is halted, the ``halt'' button has no additional effect, and if the counter is counting, the ``go'' button has no additional effect.   A counter with a single counting state, of course, does not provide much value.  We extend the table with four counting states and four halted states, as shown to the right.  This version of the table also introduces more formal state names, for which these notes  use all capital letters.  The upper four states represent uninterrupted counting, in which  the counter cycles through these states indefinitely.  A user can stop the counter in any state by pressing the ``halt'' button, causing the counter to retain its current value until the user presses the ``go'' button.  Below the state table is an abstract transition diagram, which provides exactly the same information in graphical form.  Here circles represent states (as labeled) and arcs represent transitions from one state to another based on an input combination (which is used to label the arc).  We have already implicitly made a few choices about our counter design.  First, the counter   {c|ccc} state&    no input&  halt button& go button  { COUNT A}& { COUNT B}& { HALT A}&  { COUNT B}& { COUNT C}& { HALT B}&  { COUNT C}& { COUNT D}& { HALT C}&  { COUNT D}& { COUNT A}& { HALT D}&  { HALT A}&  { HALT A}&              & { COUNT B} { HALT B}&  { HALT B}&              & { COUNT C} { HALT C}&  { HALT C}&              & { COUNT D} { HALT D}&  { HALT D}&              & { COUNT A}     shown retains the current state of the system when ``halt'' is pressed. We could instead reset",
    "GPT-3-Semantic-Search-Generations": {
      "question": " What is the value of HOLD when the counter is counting?\\n",
      "answer": "\nThe value of HOLD when the counter is counting is 0. The value of HOLD when the counter is halted is 1."
    }
  }
]