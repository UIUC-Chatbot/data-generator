[
    {
        "Lec 12 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-xMWcIb6XGVA.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. Hello, welcome. Today we're going to talk about one last new topic, which has to do with search. So as you remember, we're working on our last topic. The last topic was probability and planning. Last lecture we talked about probability. The idea was mostly we focused on Bayes' theorem, Bayes' rule, that was a way of updating our belief about some situation based on new information. And this week in Design Lab 12, you'll get a chance to use that in a robot application. The idea for Design Lab 12 is going to be that a robot is pedaling along a corridor with some obstacles off to its left. And the idea will be, you don't know where the robot is, but the robot will be able to estimate where it is by the signals that it receives from its left-facing sonars. So this is a very realistic type of state estimation problem. The idea is going to be that at any given time t, you'll have access to your previous belief at time t minus 1, and you'll have access to a new observation, which is the sonar to your left. And based on those two bits of information, you will update your belief, which means that when you start out, you have no idea where you are, but that situation should improve with time. So that's what we're going to do in Design Lab 12. Today, we're going to blast ahead and think about the other important topic, which is search. So we're going to think about planning. We're going to be planning ahead. We're not going to just react to the situation that's given to us, we're going to try to figure out what's the right thing to do in the future. And to do that, we're going to think about all the things that we could possibly do, search through that space, and figure out the one that's, quote, best. And we'll have to define best somehow. Just to get going, I want to show you a very simple kind of a search problem. The search problem, this is called the eight puzzle. The idea is to make a plan to go from this configuration, which we'll call a state, to this configuration, which we'll call the goal state. And on each move, you get to move one of the tiles into the free space. So I could move the eight to the right or the sixth down. Those are the only two things that I could do in the start state. So I have to make up my mind which of those I would like to do. And I would like to believe that ultimately, after a series of moves, I'm going to be able to perturb this state into that state. And you can imagine sort of guessing, and we'll estimate in a moment how big is the guess space. I mean, if the guess space only has like four elements in it, guessing is a fine strategy. If the guess space has a lot more elements than that, guessing is probably not a good idea. So I previously ran our search algorithms, the ones that we'll develop during lecture, on this problem. And here's the solution that our search algorithm came up with. It's not exactly what you might do the first time you touched it. OK, I made it. If you were counting, I made 22 moves. The question is, how difficult was that problem, and how good was that solution? Was that a good solution or a bad solution? Is there a better solution? How much work did I have to do in order to calculate that solution? So to get a handle on that, let's start by asking a simple question, how many configurations are there? I mean, I got there in 22. What was the space of things I had to look through? How many different board configurations exist? So think about that for 20 seconds, talk to your neighbor, and figure out whether it is 8 squared, 9 squared, 8 factorial, 9 factorial, or none of those. This will be 10. So how many board configurations do you see? Raise your hand, show me a number of fingers so I can figure out roughly how many were there. That's excellent. So very good participation and nearly 100% correct. So the answer is number four. You can think about this. What if you took all the tiles out and threw them on the floor and put them in one at the time? Well, you would have nine possibilities for where you wanted to put the first one. Then you would have eight possibilities for where you wanted to put the second one. Then you'd have seven possibilities for where you put the third one, et cetera, et cetera. So even though the space doesn't have a number on it, it still sort of counts. And so you end up with 9 factorial. And the point is, 9 factorial is a big number. 9 factorial is 362880. So if you thought about simply guessing, that's probably not going to work all that well. Even if you guessed, you have on each, so there's a third of a million different configurations that you have to look at. And that's if you didn't lose track of things. If you lost track of it, oh my, it's coming up. Almost, anyway. Looks like it's chopped off at the top. So ignore that for now. Look over here. Even if you didn't confuse yourself, there's sort of a space of a third of a million things to look at. And if you confuse yourself, there's even more. So it's not a huge problem by computer science standards. But it's certainly not a trivial problem. It's not something that you can just guess and get right. So what we want to do today is figure out an algorithm for conducting a search like that. We'd like to figure out the algorithm, analyze how well it works, optimize it, and try to find out a way to find the best solution where best for this particular problem would mean minimum path length. So figure out the best solution by considering as few cases as possible. Obviously, if you enumerate all the cases, that should work. The problem is that we'll be interested to solve problems where that enumeration is quite large. Even here, the enumeration is quite large. So let's think about the algorithm. And I'll think about the algorithm by way of an even simpler, more finite problem. What if I thought about a grid of possible locations where I could be? Maybe this is the intersections of streets in Manhattan. I want to go from point A to point I. What's the minimum distance path? I hope you probably can all figure that out. But what I want to do is write an algorithm that can figure that out. And then if we write the algorithm well, we'll be able to use it for the tile problem, which is not quite so easy to do. The way we're going to think about doing that is to organize all of our possible paths through that maze in a tree. So if I started at A, I have a decision to make. I could either go to B or D. Then if I went to, say, B, I could either then go to A or C or E. I've organized them alphabetically for no particularly good reason. I needed some order. So then if I went from A to B, to A, say, then I could either go from A to B or D. That's illustrated here. So the idea then, ah, it works. OK, so now it looks like they're all three working. So the idea is, think about the original problem. The original problem is find the shortest path through some grid. I want to go from A to I, and I'll think about all the possible paths on a tree. Then the problem is that for the kinds of problems we're going to look at, that tree could be infinite in length. Oh, that's a bummer. That means that the strategy of building the tree and then searching it is probably not a good strategy. So what we'll do instead is we'll try to write the algorithm in such a way that we construct the tree and search for the best solution all in one pass. Then hopefully, if we find the solution in some finite number of steps, we'll only have built part of the tree, but we'll have built the part that has the answer. So the idea then is going to be, think about what is the path we want to take by thinking about the tree of all possible paths. But what we want to do is write code that will construct the tree on the fly while it's considering how good were all the different nodes. OK, so how are we going to do that? So we'll be working in Python, not surprisingly. We'll represent all the possible locations. We'll call those states. So the problem will have states A, B, C, D, and we'll just represent those by strings. And that makes it flexible. That makes it arbitrary. Then we'll think about transitions not by enumerating them. Remember, we don't want to enumerate them because there could be infinitely many of them. So how's the other way we could do it? Well, we'll embody that information in a program. We'll write a procedure called successor that will, given the current state and action, figure out the next state. So that's a way that we can incrementally build the tree. So imagine here, if I told you A and I took action 0 or 1, if I started in A and I executed action 0 or 1, I would end up in B or D, respectively. So I tell you the current state and the current action, and the successor program then will return to you the new state. That's all we need to construct the tree on the fly. Then to specify the particular problem of interest, I'll have to tell you where you start. So I'll have to define initial state. And I'll have to tell you where to end. I could just tell you the final state, but in some of the problems of the type that we will want to do, there could be multiple acceptable answers. So I don't want to just give you the final state. I'll give you a test. I'll give you another procedure called goal test. And that goal test, when passed an input which is a state, will tell you whether or not you reached the goal. That way, for example, all the even numbered squares could satisfy the goal if that were the problem of interest. Or all the states on the right could satisfy the goal. It's just a little bit more flexible. So the idea then is that in order to represent that tree, we'll do it by specifying a procedure called successor and specifying the start state and the goal test. So here's how I might set that up for the simple Manhattan problem that I started with. So I want ultimately to have something called successor that eats a state and an action. I've built the structure of Manhattan into a dictionary. The dictionary lists for every state, A, B, C, D, E, F, G, H, I. For every state, it associates that state with a list of possible next states. So if I'm in A, I could next be in B or D. I could next be in B or D. I've organized these arbitrarily in alphabetical order so I can remember what's going on. So the next states are all in alphabetical order. The number of next states depends on the state. I'm not going to worry about that too much. I'm just going to specify the action as an integer, 0, 1, 2, 3, however many I need. So the possible actions are taken from that list. The possible action might be do action 0, do action 1, do action 2. So if I did action 2 starting on state E, I would go to, so action start at 0. So 0, 1, 2. I would go to state F. Is that all clear? The initial state is A, and the goal state is return S equal to I. So if S is equal to I, it returns true. If S is not equal to I, it returns false. I'm not quite done. That's enough to completely specify the tree, but now I have to build the tree in Python. Not surprisingly, from our object-oriented past, we will use an object-oriented representation for that tree. So we'll specify every node in the tree as an instance of the class searchNode. searchNode is trivial. searchNode simply knows what was the action that got me here, who's my parent, and what's my current state. So when you make a new node, you have to tell the node, you have to tell the constructor those three things. So what was the action that got me here, what's my current state, and who is my parent? Knowing the node, you're supposed to know the entire path that got you here. So we'll also add a method which reports the path. So if I happen to be in node E, my path ought to be I started in A, I took action 0 and got to B, and then I took action 2 and got to E. And so this subroutine is intended to do that. If my parent is none, which will happen for the initial state, simply report that the path to me is none A. However, if I'm anybody other than the initial node, then figure out the description of the path to my parent and add the action that got me here and my state. So that's what this is. OK, so what are we doing? So we specify a problem by telling you the successor function, the start state, and the goal test. And then we provide a class by which you can build nodes to construct on the fly the search tree. OK, now we're ready to write the algorithm. So here's the pseudo code for the algorithm. What do we do? We initialize. So we're going to be doing a search. This is very confusing. I'm trying to solve a search problem. To solve the search problem, I'm going to search through the tree. So I'm going to think about the state of my search through the tree by way of something we'll call the agenda. Very jargony word. I completely apologize for it. I didn't invent it. It's what everybody calls it. Sorry. So we'll call it the agenda is the state. The agenda is the set of nodes that I'm currently thinking about. So I'll initialize that to contain the starting node. Then I'll just systematically just keep repeating the same thing over and over again. Take one of the nodes out of the agenda, think about it, replace that node by its children. While I'm doing that, two things are supposed to happen. I'm supposed to construct the search tree, but I'm also going to be looking over my shoulder to see if I just constructed a child who was the answer. Because if I just constructed the answer, I'm done. OK? So initialize the agenda to contain just the starting node, then repeat the following steps. Remove one node from the agenda, add that node's children to the agenda, and keep going until one of two things happens. Either you found it, goal test true, return true, or the agenda got empty, in which case it must not be a solution. OK? If I've removed all of my options and still haven't found anything, then there's no solution. OK, so what's the program look like? It's actually remarkably simple, especially when you think about just how hard the problem is. Imagine if you wanted to do that tiles problem with a very simple-minded if this, then this, if this, then this. We're talking about a third of a million different ifs. That's probably not the right way to do it. So this program's going to end up being about this long. It'll fit on this page. And it's going to be able to handle that case or even harder cases. So define the search procedure. The search procedure is something that's going to take the initial state, the goal test, the possible actions, and the successor subroutine, the successor procedure. That's everything you need to specify the problem. And it's going to return to me the optimal path. So first, step one, initialize the agenda to contain the start node. OK, I want to put the start node in. Well, there's a chance, I want this procedure to be general purpose, right? There's a chance that that start node is the answer. OK, so take care of that first. So if you're already there, return the answer. The path to the answer is me. So I'm trying to create the agenda. I'm trying to put the first node into the agenda. There's a chance that first node is the answer. If that first node is the answer, return the path to me, which is take no action, you're here. OK, but that's not likely to be the case for the kinds of questions we ask, in which case we will create a list that contains one node, which is the node that represents the start node. Then, repeat, remove a node, which we'll call the parent, from the agenda, and substitute, replace that node that we pulled out of the agenda, replace that with the children. So while not empty of agenda, empty is some kind of a pseudo routine that I'm going to fill in in a minute. While the agenda is not empty, get an element out of the agenda, which we'll call the parent. Then, I want to think about all the children. Well, there's a list of possible actions. So for A in actions, do the following things. Each parent can have multiple children, one for every possible action. So for A in action, figure out what would be the new state. Well, the new state is just the successor of the parent state. Remember, the parent is a node. Right? The parent is a node. We're constructing nodes in the search tree. But nodes know their state. So figure out the new state, which is the successor of my parent, the guy that I pulled out of the agenda. Make a new node, which corresponds to this child. Then ask the question, did the new state satisfy the goal test? If it did, the answer is the path to the new node. So create a new state, which is the successor of the parent under the given action A. Create a new node. See if it's the end. If it is, just return. I'm done. Return from search. Otherwise, add it, again, one of these pseudo procedures, we'll fill that in in a minute, add the new node into the agenda. There's several things that could happen when I run this loop. So if the node has no children, I will take out the parent and not put anything back in. If the node has multiple children, I could take out one node and put in more nodes than I took out, so the agenda could get longer. So the agenda could either increase in length or decrease in length as a result of spinning around this loop. Also, we could either identify a goal or fail to identify a goal. So as it's increasing and decreasing, we either will or won't find an answer. OK? Now the trick, the only thing that makes this complicated, is that order matters. So those pseudo operations, whatever they were, so get element and add, exactly how I get element and exactly how I add it to the agenda affects the way I conduct the search. Let's think of something very simple. Let's always remove the first item, or remove the first node from the agenda, and replace it by its children. So pull out the first node and put back into the beginning of the agenda the children of the first node. OK, so how? I would start out in step 0. I would put the start node into the agenda. So now there's one element in the agenda, the start node. Then on the first pass through the loop, I would pull out that, that's the first node in the agenda. There's only one node in the agenda, so that's the first one. Pull out the first one and replace it by its children. Its children are AB and AD. I'm representing the nodes in this notation by the path, because the same state can appear multiple times in the tree. Notice that I could walk A, B, A, B, which would correspond to the same state being repeated in the tree. So I can't, when I'm writing it down here, represent the node by a state. But I can represent a node by a path. So on the first pass through the loop, pull out the first item in the agenda, which is A, and push back that A's children. Well, A's children are AB and AD. So now on the second pass, the rule is pull out the first guy and replace it by the children. So now the first guy is AB. So I'm here. So pull that guy out and replace him by his children. His children are A, B, A, A, B, C, and A, B, E. AD is left over. The number of elements in the agenda got bigger. Next step, pull out the first item in the agenda, replace it by its children. The first item in the agenda is A, B, A. The children of A, B, A are A, B, A, B, and A, B, A, D. OK? So notice the structure of what's going on. So ignore the stuff in the bottom and just watch the picture on the top. So I start by putting A in the agenda, then its children, then its children, then its children. So when I implemented the algorithm, take out the first and replace it by its children, I'm searching along the depth first. I'm going deeper and deeper into the tree without fully exploring all the horizontal spaces. So I'm tracing a line down that way. So if you imagine this tree, I've only represented the first three layers of nodes here. This tree goes on forever. It's an infinite tree because you can walk around in that Manhattan grid forever. There's no limit to how long you can walk around. So although I'm only listing the first three, the tree in principle goes on forever. And this algorithm will have the feature that it walks along the left edge. We call that depth first search because we're exploring depth first as opposed to breadth. OK? That results because of our rule. The rule was replace the first node by its children. Let's think about a different rule. Let's replace the last node by its children. So we start by initializing the agenda to the node that represents the start state. So that's the path A. Then pull out the last node in the agenda, well that's A, and stick in and replace it by its children. Its children are still AB and AD just like before. Now the answer differs from the previous answer because when I pull out the last node, I'm pulling out AD now instead. And now I replace AD by its children, which are ADA, ADE, ADG. Repeat. And what I've got is a different but still depth first search. So I've looked at two different orderings. Pull out the first node from the agenda and replace it by its children. Pull out the last node and replace it by its children. Both of those algorithms give an exploration of the decision tree, searching out depth first. So it's going to try to exhaustively go through the entire depth before it tries to explore the width. As an alternative, think about a slightly more complicated rule, remove the first element from the agenda and add its children to the end of the agenda. So initialize it with the start state, so A. Pull out the first element from the agenda, that's A, and replace it by its children, which is AB, AD. Now pull out the first guy. Well, the first guy is AB. And put its children at the end. Its children are ABA, ABC, ABE, ABA, ABC, ABE. And they are now put at the end, so that on the next step I'll pick up AD, the guy at the beginning, and put AD's children at the end, et cetera, et cetera, et cetera, et cetera, et cetera, et cetera. The idea being, and now pay no attention to the bottom for a moment and just think about the pattern that you see at the top. So in this order, where we remove the first node and put its children at the end of the agenda, has the effect of exploring breadth first. So we call that a breadth first. So the idea is, we've got this generic set of tools that let us construct search trees. But the order by which we manipulate the agenda plays a critical role in how the search is conducted. And the two that epitomize the two extreme cases are, what would happen if I replaced the last node by its children? Or what would happen if I removed the first node and put its children at the end? Those two structures have names because they happen so often. We'll call the first one a stack and the second one a queue. The stack-based is going to give us depth first. The queue-based is going to give us breadth first. So stack, how do you think about a stack? You think about a stack by saying, OK, I've got a stack. A stack is like a stack of dishes. So here's my table, and I'm going to rack my dishes up. I'm going to put them on a stack. So I make a stack. OK, I made the stack. Push a 1, push a 9, push a 3. Push a 1, push a 9, push a 3. That's how I do a stack. Then pop. OK, when I pop, the 3 comes out. Then pop, then the 9 comes out. Then push it minus 2. Then pop, now the minus 2 comes out. OK, it's stack-based. So the last in becomes the first out. That was the rule that we wanted to have for the depth first search. And it's very easy to implement this. We can implement it as a list. And all we need to do is be careful about how we implement the push and pop operators. So if we set up the push operator to simply append to the end, and then pop ordinarily pops from the end, and so we'll get the behavior of a stack. So that gives me then the rules that I would want to use for those procedures that did get element and add. I will use these stack-based operators. The other alternative is a queue. So a queue is different. A queue is like when you're waiting in line at a stop and shop. So the queue is I've got this queue here, and I've got the server over here. So the first person who comes into the queue, so say I push one, so now one goes into the queue, then another person walks up. The second person lines up behind the first person. Then I push a three. But the way the queue works is that when I pop the next person off the queue, I take the head of the line. So the one comes out. If I pop again, the nine comes out. If I then push a minus two and pop, then the next person in the queue comes out, and it's like that. So it's queue-based versus stack-based. And the queue-based is the one that we want to do for a breadth-first organization. And the implementation of a queue is very trivially different from the implementation for a stack. The only difference is that I'll manipulate the list by popping off from the head of the queue. So pop takes an optional argument, which when zero tells you the argument tells you which element to pop. So when you specify the zero with one, it takes it from the head of the queue. That makes it very easy now to replace the pseudo procedures with real procedures. If I wanted to implement a depth-first search, I would replace the create a list that contains the agenda with create a stack that will contain the agenda. So create a new stack. The agenda is a stack. And then rather than simply sticking the node, the start node, into a list, I will push it into the stack. So agenda is a stack. Agenda.push the initial node. And then every time I want to get a new element out, I'll agenda.pop it. And every time I want to put something into it, I'll agenda.push it. Other than that, it looks just the same as the pseudo code that I showed earlier. So there is an implementation then for a depth-first search. If I wanted instead to do breadth, it's trivial. Change the word stack to the word queue. Now create an agenda that is a queue, but queues have the same operations, push and pop and empty, that stacks have. So nothing else in the program changed. All I needed to do is change the structure of the thing that's holding the agenda. Everything else just follows. OK. You may have noticed. So that's everything we need. Now what I want to do is think through examples and think about the advantages and disadvantages of different kinds of searches. And I want to go on to the second step that I raised in the first slide. I want to think about how do I optimize the search. As I said, even that simple little tile problem, even the eight puzzle, eight sounds easy, right? Even the eight puzzle had a third of a million different states. I don't necessarily want to look through all of them. I want to think now about these different search strategies and how optimal are they relative to each other, and are there ways to improve that. Now some of you may have noticed that all of these paths don't seem equally good. So take a minute, think about it. Remember the problem. The problem was this walk around Manhattan problem. I wanted to go from A to I. This was the tree of all possible paths from A to I. What I'd like you to do is think about whether all of those paths are important. Could we get rid of some of them? So the question is, could I throw away some of the terminal nodes? Notice that I'm using the word terminal kind of funny here. The tree keeps going. The tree is actually infinite in length. So by terminal I just mean this row 3. So could I throw away some of the nodes in row 3? And in particular, how many of them could I throw away? 0, 2, 4, 6, or 8. Raise your hand with the funny coding. And the answer is, come on, come on. Raise your hands. Blame it on your neighbor, right? That's the whole point. OK. It's 2 thirds 5 and 1 third 4. So how'd you get 5 and 4? Yes? So if you're walking around A at n's and you're trying to go from A to I, and if you spun around in this loop and came back to the start place, I'm just starting over. So if I think about those, I can identify by red all the places where I'm repeating. So if I went back to the start place, I'm just starting over. So if I think about those, I can identify by red all the places where I'm repeating. So I would like to identify instances where I go back to where I started. So for example, that A. That A is bad. I'm repeating. So A, B, A. Don't really care what happens after that. A, B, C, B. Well, that's B again. So that's just brain dead, right? So I can actually remove a fair amount of the tree by simply getting rid of silliness. Don't start the path over again. Whereover means, if you come to a place you've been before, stop looking there. That's not the right answer. And so you can see there that I actually deleted half of the tree. So the number of nodes on the third line was 16. And eight of them had the property that they repeated. Yes? So what about the G and the D? So this B and D. So there's no reason to consider this D even if you know the D equal to D. ABED. ABC. ABED. ABED. Yeah, the end is in the seventh. ABED. So I didn't, in this path, ever hit D before. No, what I didn't do to the path, then you'll get the site from ABED. I guess I don't understand. Yeah, so what I'm saying is you could have hit the site with ABED. And I still did. Yes. So this D seems clearly inferior to that D. Yes, that's absolutely true. So this is a very simple rule for removing things. You're thinking of a more advanced rule. So if there's a shorter path to a particular place, don't look at the longer path. You're absolutely right. So in fact, there might be more severe pruning that you could do. So there might have been an answer that was bigger than 8. So you're absolutely right. Let me ignore that for the moment and come back to it in about four slides. You're absolutely right. So what we want to do now is take that idea of throwing away silly paths and formalize it so that we can put it into the algorithm. And we'll think about that as pruning rules. So the first pruning rule is the easy one. Don't consider any path that visits the same state twice. So that doesn't pick up your case, but it does pick up eight cases here. So that's easy to implement. All we need to do is, down here where we're thinking about whether this is a good state to add or not, we just ask, is it in the path? So if the state that I'm about to put in the path is already in the path, don't put it there. If you don't shove it back into the agenda, it'll get forgotten. So before you shove it into the agenda, ask yourself the question, is it already in the path? And so I do that here. Keep in mind, I popped out an element called the parent. I'm looking at the children. The children's state is called new state. So I ask, is new state in the parent's path? So parent.inPath of new state. So that means I have to write inPath. inPath is easy. It's especially easy if we use recursion. So inPath says, if my state is state, then return true. I'm in the path. If that's not true, and I don't have a parent, then that means I'm the start state. That means it wasn't in the path. And if neither of those is true, ask the same question of my parent. So that makes it recursive, right? So consider two cases that could either make it true or false. It would be true if I'm currently sitting on a node that happens to be the same state. It would be false if I recurse the whole way back to the start state and hadn't found it yet. So there are two termination states. I landed on a state in the path that was the same as new state, or I ran the whole way back to the start state and didn't find it. Those two terminate by doing returns. Return true and return false. The other option is that I don't know the answer. Ask my parent. So just recurse on inPath and ask my parent to do the same thing. So that's the way I can implement pruning rule one. Now pruning rule two, if multiple actions lead to the same state, only think about one of them. That actually doesn't happen on the Manhattan grid problem, but you can imagine search cases where there are three different things that you could do. In fact, you saw some of those when you were coding the robot last week. So there were multiple ways you could end up at the state at the end of the hall. So you could get there by being there and moving left, which you hit the wall, or you could get there by being here and moving left. Both of them left you in the same place. So if you're planning a search, you don't need to distinguish among those because they take the same amount of steps. So since they take the same amount of steps, we don't need to search further so we can collapse them. That's called pruning rule two. That's also easy to implement. What we do is we keep track of for every parent, what are all of its children? If the parent already has a child at that place, throw away the excess children. That doesn't sound good. So keep track of how many child states I have. Make a list. And if the new state didn't satisfy the goal, ask if it's already in the list of children. If it's already there, pass. Don't do anything. Otherwise, do pruning rule one. And then, before you push it into the agenda, also push it in to the list of new children. That's a way of making sure that if there's multiple ways to get the same state, you only keep track of one. So that's an additional pruning rule. So now, let's think about how we would implement these. Let's think about the solution to a problem where we want to apply depth-first search on this Manhattan problem to get from A to I. So let's think about how do I apply depth-first search to that problem. So think about the agenda. So the agenda, I initialize it with the node that corresponds to the start state. So that's A. I'm doing depth-first. What's the rule for depth-first? Pop the last guy, replace it by its children. So pop the last guy. What's the last guy? The last guy is A. Replace it by its children. What's the children of A? Well, there's two of them, AB and AD. OK. So I'm done with the loop for the first level. So pop the last guy. That's AD. Replace it by its children. What are the children of AD? Well, what could D do? D could go to A or E or G. A's brain dead, so I don't want that one. So I'll think about E and G. So ADE, ADG. By the way, stop me if I make a mistake. It's really embarrassing. OK, pop the end, ADG. Who's the possible children of ADG? ADG. Well, it could go back to D, but that's stupid. So ADGH seems to be the only good one. Pop the last one, ADGH. And who's his children? ADGH. ADGH has children E and I. H. E, G, and I, but I don't want G because that's brain dead. So AD, AD, G, H, E, or I. And that one won, right, because I got to A. Everyone see what I did? I tried to work out the algorithm manually. So the idea then was that I visited. So how much work did I do? I visited 1, 2, 3, 4, 5, 6, 7, and then I found it. So I did seven visits. And I got the right answer. So both of those are good. Seven is a small number. And getting the right answer, both of those are good things. And in general, if you think about the way depth for search works, here's a transcript of what I just did. So this will be posted on the online version so that you can see it even though it's not handed out to you now. So you can look this up on the web. So in general, depth for search won't work for every problem. It happened to work for this problem. In fact, it happened to be very good for this problem. But it won't work for every problem because it could get stuck. It could run forever in a problem with infinite domain. This problem has infinite domain. So if I were to choose my start and end state judiciously, I could get it stuck in an infinite loop. That's a property of depth for search. Even when it finds a path, it doesn't necessarily find the shortest path. Well, that's a bummer. But it's very efficient in its use of memory. So it's not a completely brain-dead search strategy, but it's usually brain-dead. So let's think about breadth for search as an alternative. Again, all we need to do is switch the idea of thinking about stacks versus queues. Take off the beginning, add to the end. That's the way queues work. So now let's do the same problem with a breadth first search. So I start with the agenda. I put in A. I pop off the head of the queue and stuff the children at the end. OK, so I pop off the beginning and stuff the children, A, B, A, D. That looks right. That's the end of pass 1. Now I pop off the beginning and stick in the children. What are the children of A, B? Well, A, B could go to A, C, E. A is brain-dead. So A, B, C. A, B, C or A, B, E. A, B, C or A, B, E. That looks right. Now pop off this guy, A, D, and put his children at the end. A, D. That's A, D, E and A, D, G. I don't think I made a mistake yet. Pop off the first guy, A, B, C. Stick in his children, A, B, C. A, B, C. Could go to B or F. It looks like F is the only one that makes any sense. A, B, C. That looks right. A, B, E. Put his children. A, B, E. E could go to B. That's brain-dead. D, F or H. D, F. Wait. A, B. Thank you. OK. I'm supposed to be doing A, B, E followed by something. A, B, E followed by something. I don't want B. D is fine. F is fine. And H is fine. D, F, and H. OK so far? Oh no, it's not right. OK, what did I do wrong? AUDIENCE MEMBER 2, INAUDIBLE OK, next. Pop off A, D, E. This is why we have computers, right? We don't normally do this by hand. OK. So A, D, E. OK, if I had A, D, E, I could do B. That seems OK. D seems bad. F or H. So it looks like B, F, H. OK. A, D, G. OK, A, D, G. Looks like H is my only option. A, B, C, F. It looks like I could do E or I. Finally. OK. Now the only question is whether I got the right number of states. Let's assume I did. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16. OK. Which happens to be the right answer, or at least happens to be the answer I got this morning when I was at breakfast. OK. So what did I just do? So I just did breadth for a search. Here's a transcript. 16 matches, good. OK. Breadth for a search has a different set of properties. Notice that it took me longer. But because it's breadth first, and because each row corresponds to an increasing path length, it's always guaranteed to give you the shortest answer. That's good. So it always gives you the shortest answer. It requires more space. I mean, you can see that just on the chalkboard. Right? And also, it still didn't take care of your problem. So this still seems like there's too much work. So I looked at 16 different places. I did 16 visits. There's just something completely wrong about that, because there's only nine states. How could it take more visits than there are states? So that just doesn't sound right. And it's for exactly your point. So there's another idea that we can use, which is called dynamic programming. The idea in dynamic programming, the principle is if you think about a path that goes from x to z through y, the best path from x to z through y is the sum of two paths, the best path from x to y and the best path from y to z. OK, if you think about that, that has to be the case. And if we further assume that we're going to do breadth first, then the first time that we see a state is the best way to get there. So what we can do then, in order to take care of your case, is keep track of the states we've already visited. If we've already visited a state, it appears earlier in the tree, there's no point to thinking about it further. That's the idea of dynamic programming. And that's also easy to implement. All we need to do is keep track of all those places we've already visited. So we make a dictionary called visited. So in visited, so I initialize before I start looking at the children, I initialize right after I set up the initial contents of the agenda. I create this dictionary called visited. And every time I visit a new state, I put that state in the visit list. Then, before I add the child to the agenda, I ask, is the child already in the visit list? If the child's already there, well, forget it. I don't need it. Otherwise, just before you push the new state, remember now that that's an element that's been visited. So the idea then is that by keeping track of who you've already looked at, you can avoid looking. So if there's a depth 3 way to get to D and a depth 2, I don't need to worry about the previous ones, because it's already in the visit list. Yes? AUDIENCE MEMBER 2 Why do we still need the new child states up there? PROFESSOR 1 Why do we still have to do child states? The placement with visited eliminates me. PROFESSOR 1 I think you're right. I should think about that. I think you're right. I think when I was modifying the code for the different places, I slipped and could have removed that line. I think you're right. I'll have to think about it, but I think you're right. So if that line magically disappears from the online version, he's right. OK, so now one last problem. I want to see if I can figure out what would happen with dynamic programming. So I want to do breadth first. And just as a warning, I'm hypoglycemic at this point. So there may be more errors than usual. So I need to keep track of two things. I need to keep track of the visit list, and I need to keep track of the agenda. So there's two lists I have to keep track of. OK, let's start out by saying that the agenda contains the start element. That's A. That means we visited A. It's breadth first. So I want to take the first guy out of the queue and add his children to the end of the queue. Take the first guy out of the queue, add his children. A's children are B and D, which means that I've now visited B and D. Now I want to take out the first guy from the queue, AB, and I want to put his children at the end of the queue. AB's children are A, that's been visited, C, not visited, and E, not visited. So A, B, C, E. But that visits C and E. Now I want to take out AD and put its children at the end. AD is A, E, G. A is visited, E is visited, the treaty is just G. So ADG. And that visits G. Then I want to take out ABC, put in its children, ABC. Oh dear. I'm looking up there. I said I'm a hypoglycemic. ABC, ABC, could be B or F. B is no good, which leaves F, but that visits F. So now A, B, E. So that could be B, D, F, H. B, visited, D, visited, F, visited, H. OK. That visits H. Now take out ADG. Children of ADG, ADG, two children, D and H, D and H. They're both there. That didn't work. There are no children of ADG. ABCF, ABCF, three children, CEI, C visited, E visited, I, done. Found the right answer. One, two, three, four, five, six, seven, eight. Eight visits. So I've got the same answer, and it's optimal, and I did fewer than nine visits. Nine was the number of states. So this algorithm will always have those properties. So the dynamic programming with breadth-first search will always find the best. It'll never take longer than the number of states. So in this problem that had a finite number of states, even though it had an infinite number of paths, because you could go around in circles, it'll still never take more than the number of states. And all that it requires to implement is to maintain two lists instead of one. So the point then is that today we looked at two real different kinds of search algorithms. Depth-first search and breadth-first search. And we looked at a number of different pruning rules. Pruning rule one, don't go to some place that you've already visited. Pruning rule two, if you have two children that go to the same place, only think about one of them. You can consider dynamic programming to be a third pruning rule, because that's the effect of it. And the final announcement, don't forget about Wednesday. So have a good week."
    },
    {
        "Rec 9 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-qGZy1CRoZdE.mp3": " Hi. Today I'd like to talk to you about circuits. Last time we finished up the LTIs and signals and systems where we learned how to both model existing systems and predict their long-term behavior. But we haven't forayed into how to actually create systems in the physical world, right? We've created some amount of systems and software and made some brains for our robots. But if we want to make something in the physical world, then we probably have to come up with ways to model physical systems or use physical components. That starts our new module on circuits. Circuits are going to be our first foray into designing systems in the physical world, also designing systems using physical components. It's worth mentioning now that the information that you learn about circuits is good for more things than even circuits. You can use basic circuit diagrams and properties of circuits to model all sorts of kinds of systems, especially ones in the human body, right? Circuitory system, neurological system, different kinds of fluid flow, that kind of thing. In the next few videos, we'll go over how to represent circuits and also cover some of the basic methods by which people solve circuits. We'll also introduce an element called an op-amp and use that element in order to enable us to do things like modularity and abstraction from our circuits. First, let's talk about representation. In the general sense, when you come across a circuit diagram, you're going to see at the very broad level a bunch of elements and a bunch of connections between the elements. Those things will form loops and nodes. If you don't actually specify the elements, then your circuit diagram actually looks a whole lot like a block diagram. And in fact, block diagrams and circuit diagrams are very closely related in part because block diagrams are used to model feedback systems, which frequently are implemented using circuits. In this course, we're going to be focusing on independent sources and resistors as the two major kinds of elements that we'll use in our circuits. We'll also use things like potentiometers, which are resistors that you can adjust, and op-amps. And we'll look at op-amps specifically in a later video, but I have one drawn up here just so you recognize it when you see it written. Note that it looks a whole lot like the block diagram symbol for a gain, and that's intentional, and we'll cover that later. But in the meantime, the other sources that we're going to be using are independent current and voltage sources. We're going to use resistors to adjust the amount of voltage and current that we're actually dealing with, and then sample either the current or the voltage at a particular point in our circuit to get the desired values that we're after. On a circuit diagram, when you're interested in the voltage drop across a particular element, you'll indicate it by putting a plus and minus sign. This also indicates the directionality of the voltage drop. Likewise, when you're interested in the current flowing through a particular element, you'll usually see an indication of it by labeling the current i, and then maybe i with some sort of subscript. And an arrow indicating the direction of current flow through that element so that you avoid making sign errors with the person that might be reading or writing your diagram. A quick note here. This is the reason that electrical engineers use j to symbolize values in the complex plane. It's because i is used in particular for values of current. Let's review Kirchhoff's voltage laws and Kirchhoff's current laws. You've probably covered this in 802, Electricity and Magnetism, or possibly in an AP Physics class, but we're going to go over it really fast right now. Kirchhoff's voltage laws is that the voltage drop around a loop is equal to 0. Or if you take the voltage drop across a particular loop in your circuit, the sum of those voltage drops is going to be 0. Let's demonstrate on this diagram, or I'll demonstrate on this diagram. Say the voltage drop across this element is equal to v, right? It doesn't matter what it is. We're going to stick with that. The voltage drop across these elements, if I were to move around this loop, is going to sum to 0. Note that if I'm tracing out my voltage drop across this loop, I'm actually moving through this voltage source in the direction opposite of its indicated potential. So when I move through this voltage source, I'm going to account for its value as negative v. As I work my way around the rest of the circuit, the voltage drop across these elements is going to sum to v. This is true for all loops in my circuit. So any loop that includes v, the elements I encounter as a consequence of moving around that loop, are going to have voltage drop equal and opposite to the value I get by moving through v in this direction. This loop counts too, but it doesn't include v. All this loop tells me is that the voltage drop across this element is equivalent to the voltage drop across this element. Or the voltage drop in this direction across that element is equal to the voltage drop in this direction across this element. That's Kirchhoff's voltage law. Kirchhoff's current law is that the current flow into a particular node is equal to 0. Or if you take all of the current flows in and out of a particular node and sum them, they should sum to 0. I've actually got the same setup here. I'm not going to use a current divider. I'm interested in the current flowing over this element. It's actually the same as the current flowing over this element, because resistance doesn't change current. Or flowing through a resistor should not change the current. So this is still the same I. Here's my node. The current flowing in this direction and in this direction, if I took the linear combination of these two currents, they would be equal in value to the current flowing into this node. What I'm looking at the current flowing through a particular node, I pick a direction. It's usually arbitrary. I pick a direction. It's arbitrary which direction I pick. Typically, you pick currents flowing into the node as being positive. I sum up all the currents and I set that equal to 0. So in this case, or pretty simple. Let's practice on this particular circuit. One thing to note is that when you're solving circuits in the general sense, both when you want TA help and when you're solving for a midterm and want partial credit, you want to label all of your nodes, all of your elements, and all of the currents that you're interested in solving. See, I've got my voltage drop across this resistor, this resistor, and this resistor labeled, as well as these currents, which I'll also be solving for. The first thing that I would do when approaching this problem is attempt to reduce this circuit to something that is a little bit simpler. The first thing that I'm going to do is try to figure out how to change these two resistors in parallel into a single resistor and still have an equivalent circuit. That'll allow me to solve for I1 because there'll be zero nodes in my system. I'll just have one single loop. And the current through the system will just be V over R. So if I'm just looking at these two resistors, I have resistors in parallel. In the general sense, the way to solve for resistors in parallel is to take the inverse of the sum of their inverses. When you only have two resistors, you can typically cheat by saying that this is equal to their product over their sum. I'm going to redraw my current understanding of the circuit. The other step that I've saved myself is that because these resistors are in parallel, they're a current divider. They take the current in and divide it two ways, determined by the ratio between these two values. The thing I'm actually interested in expressing is that V2 and V3 are the same value. When you have a current divider, the voltage drop across all elements in the current divider are the same. So the value of V here is going to be both V2 and V3. V2R plus 6 fifths R. I'm going to go with 16 fifths R for now. I've solved for I. At this point, I have a voltage divider, which means that the current flowing through this part of the system is going to be the same. But the voltage drop across this element versus this element is going to be proportional to the ratio between these two values. V1 is going to be the amount of the total resistance in this simple circuit that this resistor contributes over the entire resistance in the system, or 10 fifths R over 16 fifths R, which is 10 sixteenths R or 5 eighths V. Same thing happens with V2. Note that these two values should sum to V in order to maintain Kirchhoff's voltage law. We've also found V3. So the two things that we have left to find are I2 and I3. Here, I've just done Kirchhoff's current law for this node. Because I'm working with a current divider, I can break up the total current flowing into that node into the number of parts equal to the sum of these values, and then distribute them in the method appropriate given that less resistance means more current. What do I mean by that? Well, I mean that here my current is equal to 5 sixteenths V over R. I2 is going to be equal to this value over the sum of these two values times I1. Likewise. And just to simplify. That concludes my tutorial on circuits. Next time we'll talk about other ways we can solve this circuit, and then we'll end up talking about op amps."
    },
    {
        "Rec 6 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-l0tUtVRhmDs.mp3": " Hi. Today I'd like to talk about signals and systems again. At this point, you're probably familiar with the motivation for why we're talking about discrete linear time invariant systems, and also with a few of the representations that we're going to end up using in this course. But you're still not sure what it is that we're trying to accomplish, or where's the part where we get to predict the future based on the fact that we're capable of manipulating these systems. Well, we actually have to be capable of manipulating these systems. And at this point, we can describe a system as we see it, but we can't also manipulate its representation in ways that make sense to us. So the thing that I'm going to do today is talk about different system equivalences and how to take a system and solve for an expression that represents a complex system, and also how if you know that some things in your system are equivalent, how you can convert between them. At that point, we should be able to talk about poles, which is how we're going to actually predict the future. So different equivalences that I'd like to talk about. I'm first going to briefly review the fact that last time we discovered the notion of a system function. We can take a representation of a system and abstract it away into some sort of function where we take the input as it's given to us, and then multiply it by the system function, and then get the output that we're interested in. How do we deal with something more complex? I mean, y is all the way over here, and we've got multiple system functions, and I don't even know what happens here, but it doesn't have to be that scary. Let's break it down. One of the easiest ways to approach something like this is to identify each position where you have a new signal, or if you were to sample here, you would have a new signal, and label those values appropriately. You can then start with your final output, and then back solve for the values that you're interested in as a consequence of that final output. In this particular example, y is going to be y2 plus y3. y2 is going to be y1 times h2, where h2 is some system function. And it probably is abstracting away some combination of gains, delays, and adders like this one here. y1 is going to be x times h1, and y3 is going to be x times h3. Now I've got all my expressions in terms of either y or something for which I have an equivalent expression for x. So I can do my substitutions, come up for an expression for y over x in terms of h1, h2, and h3. Here, I've just made the substitutions of the equations above. And factored out the x. If I wanted the system function, I would then just divide by x, and then I would have y over x is equal to this expression. The thing I wanted to indicate is that if I wanted to abstract this away into its own box, maybe I wanted like a big H or an H0 or something like that, and it represented what was happening in this top line, cascading two system functions is the functional equivalent of multiplying them together. So if I have an expression for h1 and I have an expression for h2, and I want the expression that is equal to cascading h1 and h2, I just multiply them together. Likewise, if I want an expression for the linear combination of two system functions applied to an input individually, like the combination h1, h2, and h3, it's a summation of those two values, which is expressed here. This is the same as the relationship that we reviewed in a very basic sense when we were originally doing the accumulator. The only thing I'm attempting to indicate is that that relationship scales to an arbitrary level of complexity. So if you need to, you could shift around these values if you can find some sort of equivalence. Let's see what happens when h2 is equal to h3. I'm going to take my operator equation. What this means is that if I wanted to rewrite this block diagram, I could do so by doing this. This is really similar to bubble pushing if you've done 6.004 or 6.002 and have experience with logic gates. I just wanted to indicate that it's also a thing that you can do for block diagrams and system functions. There's one more type of equivalence that I want to talk about, and I call it feedback equivalence. Here's our normal accumulator. If I wanted to represent this feedback system as a feed-forward system, what would I have to do? Well, the first time that I sampled from x, it would just be y. So right now this diagram matches for the first time step. On the second time step, if I had an input from x from the previous time step, I would also want to account for it by putting in a delay and then summing it with the current value of x in order to get y. At the second time step, I would want access to the starting value, the value from the previous time step, and the value from the current time step. And one more time to exhaust the example, at the third time step, my output would be a linear combination of the starting value, the value from the first time step, the value from the second time step, and the value from the current third time step. We'd end up doing this ad nauseum to model our feedback system. So it's difficult to do on paper. But it turns out there's a great relationship between these two equivalences and things that we already know from, I want to say high school calculus, or possibly 18.01, 18.02. Geometric sequences. When we solved for the system function, we found an expression for our feedback system. If I wanted to find an equivalent expression using this feed-forward system, I would look at this infinite summation of x terms. So if I wanted to know something about the long-term behavior of the system in terms of the system function, I would solve for this expression and then use my knowledge of geometric sequences in order to express the long-term behavior. In the general sense, in this course, we're going to be looking at the unit sample response of a system. What that means is, if the only thing I ever do for input is a single value of 1 at time 0, then what does my output look like? The reason we're looking at the unit sample response is because it's the simplest way to look at the long-term behavior of a discrete linear time invariant system. But the other reason is, once we have this, we can also use it to do things like make predictions about the long-term step response and other more complicated input signals. In the case of the accumulator, if I input 1 at time 0, my output is going to be 1 forever more. That's reflected in the coefficient of my geometric sequence. If I want to know what my long-term response is going to look like, I can look at the coefficient of r and make a decision about whether or not I'm going to diverge or converge or do neither. If I put a coefficient on r, whatever p0 converges to is what my system is going to converge to. So, using my knowledge of p0, I can make long-term predictions about the behavior of the system. Next time, I'm going to go over some general classifications of those behaviors for the system and how to more effectively use our knowledge of p0 and how to deal with things like second-order systems."
    },
    {
        "Lec 6 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-oTNwGuI7Wic.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. Today I want to talk a little bit about designing control systems. This will finish up our discussions of signals and systems. Let me then just briefly review where we are. Hopefully this might help you also for perspective with regard to thinking about the exam tonight. We've looked at a bunch of different kinds of representations for discrete time systems. The easiest, most concise method we looked at was the representation using a difference equation. That's mathematically as concise as you can get. But it doesn't tell you important things like who's the input and who's the output, and what are all the different ways that you can get through the system from input to output. So for that question, block diagrams are nice. Block diagrams are graphical. It makes it very easy to see when there is, for example, a cyclic path through the network. But they're graphic. They're not nearly so concise as difference equations. So then we went on to operators. Operators are just as concise as difference equations, but they contain additional information because the operators have an implicit argument. So there's an input, which is the argument to the operator, and there's an output, which is the output of the operator. So you can tell who is the input and who is the output. So that's good. That sort of combines the strengths of difference equations in block diagrams. You end up with a concise representation that has complete information about the signal flow paths. Furthermore, you can analyze the operators by using polynomial mathematics. And that gives rise to the notion of a system functional. And that's a very nice closure because that represents an abstraction that lets us think about a whole system as though it were just one part, one thing, one operator. So we use that structure then, all of those representations, to try to learn about feedback. And first off, in the block diagram, it's very easy to see that any time you have feedback. Feedback's so enormously powerful that we want to use it in design. But you can see immediately from the structure of the block diagram that if you have feedback, then you have cycles. Why is that interesting? Well, that's interesting because if you have cycles, then even transient inputs can generate persistent outputs. So that's a kind of behavior that we would like to understand. So from the nature of feedback, it generates cycles. From the nature of cycles, it generates persistent responses, even if there's no input. And we saw that we could characterize those by thinking about those responses for one part at a time and those parts we thought of as poles. And the responses to a single pole we called modes. So we thought through a way of decomposing the response of a complicated system in terms of a number of additive components that are based on poles. Poles are just the base of a geometric sequence. So today then, what I want to do is use that framework to think about design. How would you optimize the design of a controller? So looking back to where we've been, way back in lab 4, ancient history, we looked at how you could program the robot to approach a wall. And we saw that depending on how you set up that system, you could get very different performances. And what we'd like to do is have a way that we can design for performance without actually building it. The kind of thing that we built in the lab, lab 4, was so easy that building it to determine its behaviors was not a bad problem. But in general, if you were building a 777, there's more than one pole. And you wouldn't necessarily want to test drive all of the bad configurations. So we'd like to be able to understand that kind of a problem analytically. We'd like to be able to analyze it. So using the different representations, you can generate a very concise representation just thinking in terms of difference equations. You all did this in lab 4. And you get a single difference equation that tells you in principle everything there is that you could know, but not in a form that's very easy to analyze. It's a bit better if you translate the difference equation into a block diagram, because now you can see that this system of equations has, in fact, two feedback loops in it. Two cycles. Two things that might potentially generate persistent responses to transient signals, which could then degrade performance. If the transient signal lasts 10 years, it might be a bad controller. If the 777 hits turbulence and never stabilizes, that would be bad. If small disturbances got bigger with time, that might be bad. So we can see that this simple controller described by these difference equations has the potential to do that sort of stuff. And we'd like to understand when does it. The easiest way to think about analyzing this is to think about focus first on the inner loop and ask the question, what's the functional representation for that box, which we would call an accumulator? This box, this thing, accumulates at its output the sum of all the things that ever came in. So we call it an accumulator. So what's the functional representation of an accumulator? Well, we just do polynomial math. Easy. So we can recognize from the block diagram that the signal y could be constructed by applying r to w. But we can also see in the block diagram that w is the sum of x and y. And then if we take the left-hand side and the right-hand side of this double equation, we get something that involves just x and y, which we can solve for the ratio y over x, which then says that the ratio is r over 1 minus r. That's a functional representation for the effect of the accumulation. That's also something that comes up so frequently in the design of control systems that we give it a name. We call this Black's equation. And it's especially useful to avoid these little trivial steps in algebra to just jump to the answer. So let's see that everybody's following me. Figure out the equation for this box is the thing that we will call Black's equation. It's not mysterious. It's something that you could derive. So derive it. Figure out the functional form for the system that goes from x to y, and figure out which of these forms is correct. 1 through 4 or 5, if none of the above applies. So take 30 seconds. Figure out the answer. I'm going to ask you to raise your hand. You're free to talk to your neighbors. Starts with a parenthesis. OK, so everybody raise your hands. Tell me what the right answer is. OK, wonderful. It's about 95%. No, it's about 100% correct out of about 95% participation. So the answer, you can form just like we did before, no particular tricks. Using simple algebra, simple algebra, you get f over 1 minus fg. The thing I want you to recognize is kind of a graphical way of thinking about that. And you could just memorize f over 1 minus fg, and that's fine, but there's some interesting things that a designer thinks about. This functional form is f. That's the forward gain. That's the gain through the system, starting at the input and going directly to the output. So this form says that the closed loop gain, the loop that happens, the system functional that results when the loop is closed is just the forward gain, f, divided by 1 minus the loop gain. The loop gain is the product around the loop once. So that's just a convenient way of thinking about it. Any time you have a feedback system of this type, you can think about the closed loop response as being the forward path f divided by 1 minus the loop gain, fg. So the answer was 1. And generally, we'll see two different ways of representing the system. Sometimes we'll represent it with a plus, as we did with the accumulator. Many times we'll represent it with a minus. That's the way we think about control problems. We put in the minus because we like to think about the controller as trying to make something go to 0. So when you take the difference, that gives us an error signal. And then we can think about the controller being the thing that drives that error to 0. But however you think about it, there's two forms that are very closely related. They really differ by just the minus sign, which you could think of as just multiplying g. So it's sort of like the right-hand side is just a minus g plugged into the left-hand side. So then the way you use this idea, you think about the block diagram, and you say, OK, I can replace this thing with an equivalent system, which is r over 1 minus r, and then repeat. So the r over 1 minus r means that, if you think about Black's equation now for this loop, you should think about the forward gain, k minus t r over 1 minus r, that's this, divided by 1 plus the loop gain. So 1 plus and the loop gain. Well, the wire down here just has a gain of 1. So the loop gain and the forward gain are the same thing. So you get this kind of expression, which simplifies to this. There's two things I want you to see about this. First off, I want you to see that even though the simple-minded way of plugging in said that we should have got a quotient of quotients, n over d over n over d, it's simplified to a single ratio. If you design a system out of just adders, gains, and delays, that will always be true. There's a closure. It will always be the case that the functional that represents a system of that form will always have the property that it's a polynomial in r divided by another polynomial in r. That's just the way polynomials work. The other thing is that you can now start to interpret what the behavior of this could represent in a simpler form than thinking about this. So this kind of a representation that leads to intuition about what the behavior should be is very helpful when you're thinking about design. And in particular, this particular thing says that if we think about a simpler system that could generate that same response, we can generate some intuition for where we would like to set the parameter k. So in particular, this system, this system functional, which we generated for this system, could equally apply to that system. Is that clear? So there's a numerator, which I've represented here. There's a numerator which has an r in it, and it has a minus kt in it. For reasons that you'll see in a minute, I'm going to call something p0, because it's the pole. So the numerator I've represented here, and this denominator, has this form. And I wrote it that way because this is the canonical form for the way of thinking about a pole. So what I've showed is that even though this was a more complicated system, you can think about it as the cascade of a delay, a gain, and a pole. The pole can be calculated from the gain. The pole is the multiplier for r. So the pole is 1 plus kt. So if I were to choose kt to be minus 0.2, then the pole would be at 0.8. If the pole is at 0.8, then the mode, the natural response to the pole, would have the form p to the n. They always have the form geometric p to the n. So look, I point a to the n. Because of the pre-multiplier of 1 minus p0, the whole thing gets multiplied by 0.2. And because of the delay, the whole thing gets shifted to the right. The important thing is that by thinking about manipulating this as an operator, we can recognize and simplify the form of the behavior. That gives us an intuitive grasp over how to best choose kt. Now, the behaviors that we're interested in are not always unit sample responses. We do unit sample responses because they're the easiest possible thing we could think of. A unit sample is the simplest non-zero signal. A unit sample is the signal that is different from 0 in exactly one place. The easiest possible place, 0. And it has its easiest possible non-zero value, 1. So we focus on the unit sample signal because it's the easiest possible signal we could think about. But when we're thinking about behaviors, we're often thinking about other things. Often we'll think about the step response. Here I've illustrated the way you would measure a step response. A step response is what would happen if the output were initially 0, if we were at rest, and suddenly we turned on a signal that was constant at 1. So that would happen in the robot case if we started the robot close to the wall at rest, near 0, so that the output signal is close to 0. But the desired input was a meter behind. Then that would be an input signal that started at time equals 0 equal to 1 and persisted forever at 1. And the result then would be what we call the step response. The step response is typically easier to measure in the lab than is the unit sample response. So we use the unit sample response when we're thinking analytically, when we're doing calculations, and we use the step response when we're in the lab trying to measure something. And the whole theory wouldn't be very useful if there weren't a close relationship between those two things. And this diagram illustrates the relationship. If we think about a system H for which we would like to find the step response, the step response of that system is what would the system do if you put the unit step into the system? I've represented the unit step here as u of n. u of n, the signal that is 0 for n less than 0 and 1 for n bigger than or equal to 0, is just the accumulation of the delta function, the unit sample. So this system, the cascade of an accumulator with H, would measure the step response of H if it were driven with the unit sample signal. Because of the properties of polynomials and because block diagrams follow the rules for polynomials, we can flip these whenever the systems both start at rest. And if we flip those, we get a different interpretation. What this says is that if you were to take H and stimulate it with a unit sample, you would get h, little h, which we would call the unit sample response because it's the response of the system when the input is the unit sample. So if you measured H with the unit sample rather than with the unit step, you would get the unit sample response from which you could generate the step response by running it through an accumulator. So what that says is there's a close association. There's a close relationship between the unit sample response and the unit step response. One is the accumulation of the other. The unit step response is the accumulated response to the unit sample response. So that means that in that previous example where we saw that setting kt equal to minus 0.2 resulted in this unit sample response, that would correspond to this unit step response. All you do is for every sample you calculate, for this response, you calculate the sum of, say, n equals 0. You would take the sum of all of the previous answers in H of n. It's the accumulation. So it starts at 0 since the sum of all those numbers is 0. Then at time 1, it becomes the sum from here back. So it becomes 0.2. Then here, it's the sum from here back. And if you add these all up, it becomes a number that approaches 1, not surprisingly. If you've got a feedback system, and if you started the robot up against the wall and the desired position was about one meter behind, it would monotonically approach 1. And what you can see is that if you change the value of the pole, here I've changed the kt from minus 0.2, which is what the previous answer was, to minus 0.8. I've changed the value of the pole. The unit sample response got faster. And the unit step response also got faster. The point is, there are different kinds of performance metrics that we might want to use. Unit sample response, unit step response. But there are responses of all of them. You can tell something about the response to all of them from the response of the unit sample signal. That's why we focus so much on the unit sample signal. It's not because it's the most popular thing to use in the lab, it's because it's the easiest thing to calculate with, and it gives us insight into things that we would like to measure in the lab. So for this very simple system, what you can show is that there's only a few possible behaviors, a few categories of behaviors. If you were to choose kt to be between 0 and 1, then the pole, which is 1 plus kt, would also be between 0 and 1. And since the system has a single pole, you can say a lot about the response from the numerical value of the pole. If the pole is between 0 and 1, then the response is going to be monotonic and converging. That results because the unit sample response was positive only and decayed towards 0. Because it's positive only, it makes the step response converge. Positive only means monotonic. Goes to 0 makes it converge. So you can infer properties about the unit steps response from properties of the pole, just like we could infer properties of the unit sample response. If you changed kt to be between minus 2 and 1, you would get a p0 that's between minus 1 and 0. Again, that's just that equation. That says that the response will be alternating. So the sign of the unit sample response goes positive then negative. It still converges in the sense that the unit sample response approaches 0. And what that means for the unit step response is that the unit step response will converge toward 1. The sign won't alternate. Well, I shouldn't say it that way. The sign doesn't alternate around 0. The sign alternates around 1. So again, you can infer the properties of the unit step response from the properties of the unit sample response. And if you have kt that is less than minus 2, then you get a p0 that's less than minus 1, and that's a divergent response. So the point is that you can infer properties about the control system by thinking about the poles of the system, where here I've illustrated it for a simple system that only has one pole. OK. I told you a bunch of facts. Now you figure out something. How would I choose k for this system to get the quote best performance? OK. OK. OK. OK. OK. So which value of kt would give the fastest convergence for the unit sample signal? OK. Participation is down. But the hit rate is still good. So virtually everybody who volunteered in the answer got the right answer. So the most popular answer was 2. Why is the answer 2? What's the range of possibilities that we could get? We could choose k or kt. We could choose kt to be what's the range of kt that we could use? Minus 2 to 0. In principle, you could use kt any real number. We could use imaginary numbers because that doesn't sort of make sense for a real system. But we could choose sort of any real number. The real number is mapped according to this chart. The real number is mapped to a different real number. If you choose kt, you can figure out where is the pole by that mapping. Where would you put the pole to get the fastest response? If you have your choice of putting the pole anywhere on this red line, that red line, or that red line, where would you put it? And why? Just inside the unit circle. So putting it inside the unit circle would probably be a good idea because if you didn't put it inside the unit circle, it wouldn't converge. That's right. So you'd like the pole to be inside, given the choice of anywhere here and anywhere here, which would you choose? How would you choose it? Yeah. D-Rox. So how do you get that? AUDIENCE MEMBER 2.0 What would happen if it was close to 0 but not 0? It converges quite quickly, right? Poles always converge geometrically. The basis of the geometric is the pole value. So you'd like the pole to be as small as possible to get the convergence as fast as possible. Does that make sense to everybody? So in particular, for this example, if you chose k t to be minus 1 in that limit, then this entire factor goes away. So the entire response degenerates to R. And R is not instantaneous, but it's pretty fast. What that says is that you get to the final value in one step. So if the input consisted of a unit sample, which has non-zero value only at 0, the output would have non-zero value only at 1, right? So thinking about the way that works in practice, so think about the robot. Think about we're trying to drive toward the wall. If we made k t be minus 1, and just for the sake of being concrete, let me say that t is about 1 tenth. That's what the sampling period is for the robots we use in the lab. If t were 1 tenth, then the best k would be minus 10. And what that says is that if we were one meter away from where we want to be, we would set the velocity to 10 meters per second. What that says is that if we started one meter away from where we want to be, so this dot is intended to represent position on the same axis that this is showed. So if we started here and we wanted to be here, time is plotted down. If we use the rule that we just specified, then we would set the velocity given this condition, which is one meter away from where we want to be, we would set the velocity to be 10. If we set the velocity to be 10, then after one unit of time, after a tenth of a second, we are one meter to the right, which just happens to be exactly where we want to be. Had we chosen k to be bigger, we would have overshot. Had we chosen k to be smaller, we would have undershot. k equals 10 gave us precisely the right answer so that we get there in one fell swoop. Then on the very next step, we would compute a velocity of 0, because we are at where we want to be. So we would stay there. And that condition would persist forever. So the idea would be this simple system provides a way that we could set the gain so we could get to where we want to be in one step. It's hard to beat that. The problem that results, and the reason you didn't see that good of behavior in the lab, was that the sensors in the robot don't work instantaneously. They introduce delay. And as an idealization of that delay, I want to think through the same problem, but now let's say that the sensor delays the input to the sensor, which is the output of the system. Let's say that the sensor introduces a delay of 1. So now, instead of reporting d sensed, which was d0 of n, it reports d0 of n minus 1. So now what would happen? Now with the delay, if I started here, and if by some mysterious process I was here at time n equals 1, then I would calculate my new velocity. What would be my new velocity here? I'm right where I want to be. What would be my new velocity if I assume that the sensor has a delay of 1? 10. Because of the delay, the sensor is reporting that I'm a meter away from where I want to be. So the controller calculates, oh, I need to go forward a meter, I'll set the velocity to 10. So having set the velocity to 10, and then one step goes by, now we're completely on the wrong side. That's what happens when you put delay into the system. So because we're basing this decision on where we were last time, we go to the wrong place. So now we're here. What would the controller say next? Stay here. You're at a great place. So I'm really one meter too close. In fact, I banged into the wall. But the sensor is telling me I'm exactly where I want to be. So stay here. Say I didn't kill myself. What will the velocity next be? Minus 10. So now I tell myself to go back. That's probably a good move. But now I still think I'm too close to the wall, so I tell myself to continue to back up. So the idea is that I get poor performance. The delay had a devastating effect on the way that the controller worked. Even though it's a tiny change to the way the system works, it has a devastating effect on behavior. So we'd like to be able to predict that without having to measure it. So here's the same equations, except that I've put a delay in the sensor. Here's the same block diagram, but I've represented a delay in the sensor path. So now the question is, what's the new functional representation for that control system? OK. OK. OK. OK. So what's the answer? Can you write the functional form for this system as one of one, two, three, or four, or is it none of the above? It's about a third participation and about 100% correct. So the answer is four. So you can use black equation or hardly black equation. You can think about reducing the inner loop the same as we did before, and then think about this as forward over 1 plus loop gain. But now the loop gain has R squared in it instead of R. So we get this form. How does this form differ when the R wasn't here? What's the difference between R not there and R is there? What changes? The square term. So this term in the three-dism form was just an R, and in this form is a square. And so what's that do? What's the importance of the fact that there's a squared there? Two poles. We now have a polynomial in the denominator that is quadratic in R. And what that's going to do is it's going to give us two poles instead of one. The importance of that is that now we're going to have to think through. We previously categorized what were all the behaviors you could get from one pole. The behaviors you could get from one pole were monotonic divergence, non-monotonic alternating divergence, monotonic convergence, alternating convergence. So there were four behaviors that were possible with one pole. Now we have to think through what are all the possible behaviors that we could get with two poles. Different problem. Hopefully they're related. So here the way we would find out what the poles are is take this expression, substitute for every R, 1 over z. Turn the ratio of polynomials in R into a ratio of polynomials in z. To do that in this case, I had to multiply numerator and denominator by z squared. Having done that, I get a second order polynomial in z in the bottom. So there's two poles, which are the roots of that polynomial. And that's just the quadratic equation. So the interesting thing now is to map out what are all the possible behaviors that that system can give us. So it's important to realize that's a simple generalization of what we saw before. It will be the case that any system that we construct out of adders, gains, and delays will have the property that we can write the system functional as a ratio of polynomials in R. By the factor theorem, we will always be able to factor the denominator. And by the notion of partial fractions, we'll always be able to write some complicated expression like that in terms of a sum of parts, each part being first order. So the intuition we get from this is that what we ought to do is factor the denominator, find the poles, and associate a behavior with each of those poles. So here's what the problem looks like for the two pole problem if we have the general form given here. And if we start by thinking about KT having a small magnitude, if KT has a small magnitude, then we have 1 by 1 plus or minus the square root of 1 by 1 squared. So that's 1 by 1 plus or minus 1 by 1. That's 0 or 1. So the poles for this system, if you make K be very small, the poles are near 0 and near 1. Is that a good system response or a bad system response? Bad, why? Well, we're trying to think through the behavior of the second order system by thinking about the separate behaviors of each of the poles. So is this a good pole or a bad pole? Why? The response is always pole to the end. The mode associated with the response at a pole near 1 is something near 1 to the end, that never converges. If you start with some error, the error persists forever. Well, that's not good. If wind turbulence knocks you into a decline in your airplane and it persists forever, that's not good. You would like those things to damp out. So this pole is bad. How about that pole? That one's good. That one has a response that decays quickly. But the problem is that when you add the two pieces together, that was the reason I showed you this decomposition, you can think about the polynomial being factored and being broken into a number of parts. The part that's associated with the pole near 1 has a response that goes for a long time. So that will asymptotically dominate your response. So we refer to this as a dominant pole. This pole dominates the response. That's a way of inferring behavior of two poles from the sum of single poles. In this particular case, there's one pole that matters more than the other one. So we call that pole the dominant pole. If you were to make kt more negative, so here's the general form. If you make kt negative, you can make the thing under the radical sign go towards 0. If you made the thing under the radical sign go to 0, then you would get two poles at 1 by 2. That would happen. So here we would see that if kt were minus 1 by 2, we would have 1 by 2, which is plus 1 by 2. Minus 1 by 2 would give us 0 under the radical. So we would get two poles at 1 by 2. Is that good or bad? Well, it's better than the previous example. Because each of those poles is associated with a response where the error gets half what it used to be on every step. So it converges. If you were to continue to make k, if going from 0 to minus 1 by 2 is good, well then going to minus 1 by 2 might be better. If you continue that trend, say you make kt be minus 1, if kt is minus 1, then you get a half squared minus 1. So a half squared is a quarter. Minus 1 would be minus 3 quarters. That gives us a complex pole pair. So we get two poles that are right on the unit circle. So what's that mean? That means oscillations. Oscillations is something you can't get with one pole, with a real system. Oscillations result from a pole that has an imaginary component. If the system is real, you can only get such poles in pairs. So it's this pair that makes sense for a real valued system. And that gives rise to oscillations. And that's exactly what we saw here. So we can associate the oscillations that we saw in the simulated lab experiment with poles that have imaginary components. So what would be the period of the oscillation in the system given by a half plus j root 3 over 2? Excuse me? If you substitute minus 1. So what's the period of the oscillation? OK, so the period is represented by five fingers, which is six. So how do you get six? The easiest way to think about that is to think about the poles being expressed in polar notation. The poles we previously said were half plus or minus j root 3 over 2. That's the same as e plus or minus j pi over 3. It's easier to use that form, because if you take that form, so if you think about e to the j, what was it, 2 pi over 3? Pi over 3. So if you think about that form, that's the pole. If we associate, we can write that that way. Then the inside has a magnitude of 1. So we can think about that just being a magnitude of 1 and an angle of pi over 3. So when you raise that to the n, the magnitude to the n, 1 to the n is always 1. And the angle raised to the n just increases linearly with n. So the angle goes from pi over 3 to 2 pi over 3 to pi to 4 pi over 3, et cetera. So you can think about this going from pi over 3, 2 pi over 3, pi, 4, 5, 6. It takes n equals 6 to get around to where it started, so the period is 6. If you were to further change the gain, if you were to make it even more negative, the poles would go outside the unit circle. And then what would happen? Then it diverges. So that's completely unacceptable. So the point is that by changing the gain, you can get any behavior on this figure, which is called the root locus. So root meaning the root of a polynomial, locus meaning the acceptable values of points. So the root locus shows you all the possible behaviors that could result from this system. So given that root locus, how would you choose k to make your system respond as fast as you could? OK. So what value of kt would you want? Everyone reaching ahead? OK, sorry. So the most popular answer is number 2. So why would the answer be number 2? What do you look at? Yeah. So remember what we're trying to do. We're trying to infer properties of the behavior of this second order system from the pole locations. We know that there's an expansion that lets us expand the system in terms of the sum of two first order responses. The slowest of the first order responses will dominate eventually. So what we need to look at is the slowest of the two responses. So we would like to know of the two poles, which one is the slowest. The slowest is the one that's closest to the unit circle. So we would get the fastest response when the slowest one is as fast as possible. So as the poles initially go toward each other from 0 to 1, this one is getting faster, this one is getting slower. So the slowest one is this one. So the slowest one is fastest when they meet. And then when they diverge, does the slowest one get faster or slower? It's slower when it gets slower because it gets closer to the unit circle. So you get the fastest response whenever you get the two poles both colliding at a half. And that was the case that happened when kT was minus a quarter from two or three slides ago. So the idea then is to try to infer what would be the behavior of this higher order system by thinking about the behaviors of the individual components. Here are the poles. And what we saw was something that's in fact a very important general trend. What we saw was that we first analyzed the wall finder system, assuming there was no delay in the sensor. And we found that that system was characterized by a single pole. And we had the design freedom of putting that pole anywhere we wanted to on the real axis. And that allowed us to choose the pole to be at 0, which gave terrific performance. The interesting thing that happened when you add just one more pole by putting a delay in the sensor, you make the system more complicated. And now you can't possibly get nearly so good behavior. The behavior is a lot worse than it was before. And in fact, if you were to do the same kind of analysis by putting yet another delay in the sensor, you would find even worse behavior. So the idea then, the generalization of the way the behaviors is working, generally speaking, adding delays inside a feedback loop is a destabilizing thing. Generally, as the number of delays increases, you end up having to back off on the maximum gain that you can use because the system becomes less stable. So the overall moral is that delays are bad, generally. I mean, you can concoct some kind of a weird scheme where that wouldn't be true. But it's actually hard to concoct such a weird scheme. In general, and in virtually every physical system that you're running into, adding delays makes the system harder to stabilize. And that's kind of the big message. And the system that we looked at in the lab, the wall finder was actually quite hard because the number of delays was large. If you try to track where delays can enter the robot system, they get in at very many different places. In the physical sensor, in the microprocessor, in the conversion from analog to digital, there's a number of delays in that system. And that's why it becomes hard to stabilize. OK, so that's the main content for today. What I want to do is give you one more practice question to think about the big problem that I want you to think about from today is, how do you characterize performance? When we had a single pole, performance was easy to talk about because performance was diverging, monotonically, diverging, alternating, converging, monotonically, converging, alternating. There were four kinds of behaviors. When we went to second order, we saw some new behaviors. It could become oscillatory. What I'd like you to do now is think not just about those properties, but many other properties. So here's some questions. Think about the system on the top. And I'd like you to infer properties about that system. In particular, does this system have three poles? Is the unit sample response, is there a way to write that as the sum of three geometric sequences? What's the unit sample response? And is one of the poles at z equals 1? So think about the system. Think about five ways of characterizing it. And tell me how many of those five characterizations is correct. One, two, three, four. There. So how many of the properties are true? How many poles? How do you get three? How do I find the poles? So a little more formally, we would take this thing and we would rewrite that with r goes to 1 over z. So we get 1 over z cubed, 1 over 1 minus z cubed, which is then clearing the z cubed, we get 1 over z cubed minus 1. How many poles? Three. What are the poles of z cubed minus 1? Three poles and one. So let's take a look. There's three poles and z equals 1. Yes? No? Why not? So there's three poles. So we make a z-plane. Where's the poles? Well, you could factor it, right? If you factored it, you would find that there is a pole at 1, right? But then there's two more poles like that, right? So the poles are the three roots of 1, right? Which can be written like 1 e to the j 2 pi over 3 and e to the j minus 2 pi over 3. Which pole is the dominant pole? OK. Bad question. What's a better question? How many dominant poles are there? That's a much better question. Yes, there's sort of three poles that are equally dominant, right? They all have the same magnitude. Why do we talk about dominant poles? What are dominant poles good for? So if I told you that I had a pole at 3 and a pole at minus 1, which one is the dominant pole? Why? Greater magnitude. Greater magnitude. Why do you care? You don't care, right? What's good about the dominant pole? Well, we can write this response as something that looks like 3 to the n plus minus 1 to the n. If you let n get big enough, the only one that matters is 3 to the n. So if all you care about is exactly how the plane was flying the instant before it hit the ground, then you would only need to worry about long time. And if you only worry about long time, you only need to worry about the pole that's worst behaved. That's where the concept comes from. So none of these poles are particularly worse behaved than the others. What's the unit sample response associated with that pole? We have a name for that signal, right? What's the unit sample response associated with this pole? Well, it's got a complex value, right? So the unit sample response associated with that pole is e to the j 2 pi over 3 n. So that's a complex number. So that's 1 at time 0, and e to the j 2 pi over 3 n at time 1, and e to the j 4 pi over 3 n at time 2. So it goes from here at 0 to here at 1 to here at 2, 3, 4, 5, 6, 7, 8. What's the period of this pole? What's the period of the unit sample response associated with that pole? 3, because it takes 3 to get around to where you started again. What's the period of this pole? 3, just spin it around backwards. What's the period of the response associated with that pole? Bad question. What's a better question? Is there a period associated with that? So the period is one of those kinds of problems, that's the period. What's the period of this pole? Double question. So period implies repeat. If the response repeats itself after some time, then we would say the response is periodic. That response is neither of those poles. Well, no, the minus 1 is. Is the minus 1 pole periodic? Yes. What's the difference between periodic and alternation? Is the minus 1 pole alternate? Does it oscillate? Bad question. So alternate is a word that we invented for one pole, because the response alternated in sign. The unit sample response of one pole, where the pole is a negative number, alternated in sign. So we gave that a name. Alternation is not necessarily something that we would like to associate with a higher order system. Periodic is perfectly reasonable to talk about for a higher order system. Periodic merely means that if y of n were a periodic signal, then I could express y of n plus n as y of n. That would be periodic. If the thing repeats itself, we would say it's periodic. So the point of going over this stuff is just to give you some exercise in thinking about how to think about properties of systems. We developed properties initially thinking about one pole. Those properties were easy. Converging, diverging, monotonic, alternating. When we try to think about corresponding properties of higher order systems, we can't simply map the simple properties of first order systems into the other. We have to think about more complicated things. Then we think about things like dominant. If one of the poles has a bigger magnitude than the other, then for large times we can ignore the smaller one. So what happens for short time? Is it the case? Does this response monotonically increase with time for all time? Now, since the response associated with minus 1 alternates inside, for short time, for times with n close to 0, that can be just as important as this one. So the dominant pole idea tells you how things work when you have large times. It doesn't necessarily tell you how things work when you have small times. How about the unit sample response is the sum of three geometrics, yes or no? What are the three geometrics? The answer is yes. That's very important, right? The three geometrics over here are this pole to the n plus this pole to the n plus something that goes with this other pole to the n. Now, it's a weighted sum, but the weights are not necessarily 0. So the slide that I showed you for the partial fraction decomposition, you can always write a higher order system as a sum of first order factors. That's the partial fraction expansion. That doesn't mean the weights are all unity. So number two, can you write the unit sample response as a sum of three geometric signals? Yes, there it is. And if you're really good at complex math, you can find out a, b, and c. And that would tell you the unit sample response. And that would tell you the answer to three and four. Is the unit sample response 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, or 1, 0, 0, 1, 1, whatever? Is it one of those two or something different? And how do you figure that out? How do you figure out the units? Is the unit sample response, is number two correct? Is the unit sample response 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, yes? How do you know that? You could solve that equation. Is there an easier way? Yeah? I wrote it as a difference equation. Just write the difference equation, exactly. So even though I've had enough difference equations a lot, here it's easy. And in fact, you can see it in the network. Thinking about the difference equation would be easy. Thinking about the network would be easy. If we think about the unit sample response of this thing started at rest, rest means this is 0, this is 0, and this is 0 initially. Unit sample response means this becomes 1 at time 0. At time 0, this is 1. This is 0. This is 0. This is 0. So if I think about what's the time response look like, and I did plus, so this is 1, 0, 0, 0. So the first answer is 0. Clock ticks. What happens? So this is 1. This becomes 1. This doesn't change. That doesn't change. This goes to 0. The 0 comes around here. That goes to 0. So that's the answer at time 1. What happens at time 2? Just keeps rotating. The clock ticks. This goes to 1. This goes to 0. These stay 0. This stays 0. That's the answer at time equals 2. Now the clock ticks. Now this comes over to here. That means it comes back here. This is still 0. That comes to 1. That's the answer for time 3. Now the clock ticks. And you can see the whole thing will just repeat itself now. Is the response periodic? Yes. The response is periodic. What's a period? 3. In fact, you can show that if there's only a periodic response over there, it's going to have to be related to the periods over here. These periods are not the same. This period is 3. This period is 3. This period is 1, if you want to call that a period. But they are related. So the point of this exercise is to illustrate two things. We inferred properties of first order systems by looking at a single pole, which for a real system could only behave in one of four different ways. Second order system introduced new behaviors. Now we can oscillate, which we couldn't do before. Having got to oscillation, oscillation came about because of complex numbers. If we go to higher order systems, nothing new happens in algebra. Complex numbers, there's not such thing as meta-complex numbers, right? Complex is as bad as it gets. So you can have complex numbers. The higher order behaviors can still have complex numbers. But you have to think when we ask you what's the property of a higher order system. You can think about it in terms of the individual parts, but it requires some thinking. OK. Good luck tonight. See you then."
    },
    {
        "Rec 10 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-dAZ-i9MsbRM.mp3": " Today, I'd like to talk to you about a new method for solving circuits. Last time we reviewed using KVL, KCL, and Ohm's law in order to solve circuits in the general sense. This produced a lot of equations, and in particular a lot of redundant equations, or a lot of dependent equations that we don't necessarily need for solving our circuit. Today, I'm going to review the node voltage component current method, which is very similar to node analysis. So if you hear one versus the other, know that you're talking about approximately the same thing, and I'll review the difference in a second. Once we know how to use the NVCC method for solving circuits, we can express the relationships between components in our circuit more concisely and more effectively, and possibly solve our equations faster, which is relevant when we're working on the midterm or in the general sense just trying to save time. Let's take a look at NVCC. As I said before, NVCC stands for node voltage component current. What that means is if you have a very simple circuit, let's say just a voltage source and a couple resistors. Previously with KVL, we were interested in the voltage drop around a loop being equivalent to 0. In this case, we're actually going to look at the voltages associated with particular nodes. To do that, we're going to label our nodes, which are anywhere our components connect. We're also going to go after the component current. When we're doing KCL, we typically look at the flow in and out of a particular node. At this point, we're going to look at the flow through a given component. We're also going to label all of the currents associated with our circuit. But when we approach NVCC method, we're going to think about the currents flowing through a particular component as opposed to in and out of a particular node. NVCC is sort of the opposite of KVL and KCL in that sense, even though you're still going to end up using the same relationships. At this point, I've labeled one individual current for every component in my circuit. The next step would be to identify what I'm going to call ground, or in particular, one of my nodes I'm going to assign to ground or relative voltage 0. At that point, I'm going to write out the relationships between the voltage drop across particular components, the current flowing through that particular component, and whatever relationship the component requires the voltage and the current to have to one another. That's a whirlwind review of NVCC method. Node analysis is very similar. The main difference between node analysis and NVCC method is when your component is a voltage source and there are multiple currents flowing into that voltage source, you can treat this as a single voltage node, where this voltage node has value 0, and actually write your KCL equations as though this point were collapsed. So current flowing in and current flowing out, or vice versa, have to sum to 0. That's the major difference. Let's look at an example. You can find this example repeated in 6.4.3 of the readings. And I'm going to walk through these directions. So the first thing I'm going to do is label my nodes and my currents. We'll interchangeably use E or N. It doesn't really matter. I guess N in particular could refer to the node, while E in particular could refer to the voltage associated with that node. Now I'm going to specify the voltage drop across a particular component in terms of the node voltages. I'm also going to assign N0 to 0 as my ground. As a consequence, I know that N1 is going to be 15 volts. My voltage drop is typically specified in the same direction as the current that I've also decided. So this convention is arbitrary. But if you want to be consistent in your work, make it easier to get partial credit or get help in office hours, et cetera, then assume that the voltage drop occurs in the same direction as the current. That's it for our relationships associated with voltage drop. Now I'm going to go over KCL for the relevant nodes. And the last step, I'm going to combine the two into the equation that you're certainly allowed to use to express your work on midterms. Or if you can skip to the third step immediately, then that's OK. Just check your signs. Here's the second step. Flowing into N1 is I0, and flowing out is I1. So I0 is going to be equal to I1. Flowing into N2 is I1 and I3, and flowing out of N2 is I2. Flowing into I2 and flowing out of N0 is I0 and I3. We're almost certainly not going to end up using that equation, because it's the last of our KCL equations and is dependent upon the other equations that we've already written out for KCL. I3 is equal to 10 amperes. So we can go ahead and make that substitution. I still have to work with I1 and I2, though. And I can go after expressions for them in terms of my node voltages and components by using the equations for voltage drop I made earlier. This is the equation that you can jump straight to. If you understand where this expression comes from as a consequence of our KCL and also our component voltages. I'm going to substitute in 15 volts for N1 here. And now I have an expression that only contains N2 and known values. So I can solve for N2. And I'll do that real quickly right now. So first I'm just going to copy this over. I'm going to multiply through by 6 ohms. And I've solved for the voltage associated with N2. At this point, I can solve for I2 and I1. Sorry about that. What do I have left? I3 I know is 10 amperes. I0 is equal to I1, which is negative 1 amperes. There's the voltage drop associated with the 3 ohm resistor, which is 15 minus 18, negative 3 volts. And the voltage drop associated with the 2 ohm resistor is 18 volts, since N0 is ground. This concludes my introduction to node voltage component current method, or node analysis without the ability to collapse voltage sources for KCL."
    },
    {
        "Rec 11 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-sNLB6_ZIfX0.mp3": " Hi. Last time we talked about NVCC method and how to reduce the number of equations we had to deal with to solve a particular circuit. At this point, we're pretty well equipped to solve circuits in the general sense. But we really haven't talked about how to use that information or possibly use circuits in a particular way. Before we jump into both that and abstraction of circuits, we need to talk about op amps. Op amps is short for operational amplifier. And it's a tool that we can use in order to sample particular voltages from a subsection of the circuit without affecting it. Another thing we can use op amps to do is modify our signal, or if we're going to sample a voltage from a particular subsection of the circuit, we can then do stuff to that voltage without affecting the circuit all within the op amp or within the op amps special subset of circuitry. So first of all, what is an operational amplifier? Well, an operational amplifier is a giant web of transistors. But what an operational amplifier does is act as a voltage-dependent voltage source. It can effectively sample voltages from an existing circuit and then use them to power some other object, for instance, a light bulb. If you set up this kind of circuit, you will not actually be powering this light bulb with 5 volts, because the light bulb itself acts as a resistor. And so the voltage drop across this part of the circuit is going to be different from just 5 volts. If you want to enable a voltage drop of 5 volts across this light bulb, then you have to use an op amp to sample the voltage drop at this component and put it in between the light bulb and the rest of the circuit. When you see an op amp on a schematic diagram, it will frequently look like this. You'll have a positive input voltage, a negative input voltage, power rails, which are actually the thing that determine the range of expressivity that the op amp has, and an output voltage. In reality, the relationship between the output voltage and the input voltages is something like this, where k is a very large number. The effect that this has is that V out is going to be whatever V out needs to be, such that V plus is equal to V minus. That's the basic rule you want to use when you're interacting with op amps. So in this case, if we wanted to power this light bulb with 5 volts, we would do something like this. Excuse the sloppiness of the second diagram. We still have our 10 volt voltage source. We still have our voltage divider. This point samples 5 volts from this sub-circuit and isolates this part of the circuit from the light bulb. V out has to be whatever value is necessary, such that this sample point and this sample point are equal. Since this value is 5 volts, this value will also be driven to 5 volts by the op amp, which means that this value is 5 volts. And we've successfully managed to power our light bulb with 5 volts. The other thing you might be asked to do is to take an existing circuit diagram and figure out what the operational amplifier does to a given signal, or possibly what V out is, or possibly what V out is in terms of the input signal. So let's practice using this diagram. Here's what we're after. I can figure out what V plus is going to be. This is another voltage divider. I'm now interested in V minus in terms of V out, which is another voltage divider. I can set these two equations equal to one another and solve for V out. I found an expression for V out in the particular case, where V is 10 volts. If my input voltage were previously unspecified, or if this voltage source were not specified, or just V in, then I would be after this expression. Some things I'd like to mention while we're talking about op amps. All the operational amplifiers we've been working with so far deal with V out in terms of V in, where V in is driven through the positive terminal and the negative terminal is typically connected to ground. You can do the opposite and end up with some interesting effects, but it comes at a cost. It is entirely possible that you will end up driving your op amp to an unstable equilibrium. What you need to look at is this relationship. There may be a particular point in which case your system is stable, but if you get any sort of minor perturbations, you'll actually end up with divergence. If this is the case, then you'll probably burn out your op amp. You can do this by hooking it up in this way. This is expensive and could possibly burn you. The other thing to note is that the power rails on your op amp limit its range of expressivity. And I think I've said this before, but it's worth mentioning again. If your op amp is only powered by 10 volts, it cannot amplify your input signal to a final value greater than 10 volts. Likewise, if your input value is a negative voltage and you're working with a non-inverting amplifier, if your ground is truly ground or if your ground is higher relative than your input voltage, you cannot actually express a negative voltage. The third thing I'd like to quickly mention is that there are some terms associated with op amps that you might hear used by the staff or online, that sort of thing. A buffer and a voltage follower are the same thing. And that's explicitly when you want to sample a signal or you want to sample a particular voltage and you don't want to multiply it or add it to something or do any kind of LTI operations that we might be able to do using op amps in this course. You can work with amplifiers. And the thing we worked with was using a amplifier for a value less than 1. You can also use op amps to sum signals. And if you look for a voltage summer amplifier on the internet, you should be able to find some information. In any case, op amps are really powerful. They allow us to both isolate a particular section of a circuit and sample a particular voltage value from that circuit without affecting that circuit, and also allow us to modify that particular voltage value before using it in another part of the circuit. Before using it in another part of our overall circuit. Therefore, we're enabled to design more complicated and powerful things. Next time, I'll talk about superposition and Thevenin and Norton equivalents, which will further enable modularity and abstraction in our circuit design."
    },
    {
        "Rec 4 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-8FWfmvj3HYw.mp3": " Hi. Today I'd like to talk to you about state machines. State machines are an incredibly important concept both in 6.01 and in the general sense when you're going to be doing things like control theory or artificial intelligence or any further work you do in terms of computability theory. State machines are really important. So I'm going to review what we've done so far and talk about why we now need state machines to add additional complexity to the kinds of models that we're going to build and then talk a little bit about how state machines are represented in different domains and then also talk about how we're going to use state machines in the 6.01 software. First of all, let's review what we've learned so far. We've talked so far about functional imperative and object-oriented programming paradigms. In functional programming, everything is a function. In imperative programming, we're allowed to use functions, but they're also allowed to have side effects. And in object-oriented programming, everything is an object. We can actually use the first two to implement the last one and use the first one plus an idea of maintaining assignments to variables, that sort of thing, to implement this one. So you can sort of trace the progression of different kinds of computer science languages along this paradigm. But the one thing that none of these languages on their own allow us to do is keep a notion of internal state. And what I mean by that is that if we have a system that we want to model in terms of the passage of time or keep track of the evolution of that system or keep track of some of the data that has accumulated over time in that system, then we certainly can't do it with functional programming. Functional programming takes one input, generates one output. And you could generate a function that you could generate a list of code that took in every possible situation and then generated the logical output. But that would be a lot of code. Same thing for imperative and object-oriented programming. Them alone do not address everything that we want to address, which is the ability to look at everything that has happened as a consequence of the passage of time and all of the data that we've looked at as a consequence of the passage of time, synthesize it in some way, and then generate whatever is supposed to come out of that situation. That is the notion of internal state. State machines have been around for a while. You might see them referred to as discrete finite automata. There are also such things as continuous state machines, but we're not going to talk about those in this course so much. So we're only talking about discrete state machines. And when you see them referred to as state machines or discrete finite automata in literature, especially in mathematics, you'll be looking for this five set of things. One is a set of the possible states you could be in given a particular state machine. One is the set of inputs you could possibly encounter while in that state machine. One is the set of outputs that that state machine could possibly generate. One is a transition function that looks at pairs of these values and tells you, based on which state you're currently in, what state you will end up in as a consequence of the current input, and then also specify what the output will be as a consequence of that transition. And finally, it'll tell you where you start out. That's a lot to absorb. I'm going to show you a state transition diagram, which many of you have probably seen before, and map these values to the state transition diagram in hopes of making this a little bit more concrete. Hopefully at this point, all of you have interacted with an MBTA turnstile. This is the thing that opens and closes, and you stick your RFID card on it, and then Richard Stallman yells at you and possibly hands you more Charlie tickets or something like that. It has four states. One of them is that this turnstile could be closed, and it's waiting for people to interact with it in some way. It could be open and anthropomorphize the turnstile as being happy as a consequence of you putting money in it. It could be open and quiet as a consequence of usually some sort of other previous interaction with another person, or it could be open and angry as a consequence of people interacting with it when they should not. The vertices in this directed graph are my set of states. So when you see something like this and you're asked to map it to the mathematical construct, just grab the names and say this is my set of states. Let's say I start off enclosed, and actually it occurs to me that the other thing that should be specified is a start state, and it's not here. So let's say the turnstile starts off as closed. Usually this is an arrow coming out of nowhere that directs into one of the states. Sometimes it will be explicitly indicated by saying start state, but typically you'll just see an arrow from nowhere. At this point, I don't have any inputs or outputs. If I put money in the turnstile, it constitutes input to my system. It's going to end up in my set of inputs for math. My transition function looks at the state I'm in and looks at the current input and generates the output and the next state. So all of these arrows, in addition to whatever information is contained in the receiving vertex, specifies my transition function. Any transition that's not specified is not considered in the function. This was not part of our original drawing, so if I was fed open angry, there would be no way to get to open happy. Likewise, if I was in open angry and fed money for this simple system, we're going to say nothing would happen. I think at this point, it's become clear how to transform a state transition diagram into the mathematical constructs. It's good to have the mathematical constructs because they end up being used in software, which I'll talk about in a second. The first thing I'm going to do is just walk around the state transition diagram and indicate what would be inputs, what would be outputs, that sort of thing. Let's say I walk up to the turnstile and somebody else interacts with the turnstile by exiting. In this case, exit is the input, none is the output, the turnstile doesn't make any noise, and the turnstile is open. If at that point I interact with the turnstile by entering, that's going to make noise, which is the output. And then the new state is going to be that the turnstile is open and angry. At that point, you and I know that the turnstile is going to close. So this edge indicates that the only available input at that point is to do nothing or independent of anything else, it's going to swap again and close. One more time. Here are the states. Inputs are the first of these two pairs. Outputs as a consequence of the transition is the second of these two pairs. And the transition function is represented by the directed edge and the new state. Once you have all those sets figured out, you can start talking about how to implement state machines in software. We've actually abstracted this away from you. You don't have to deal with it. But as a consequence, you should know how to interact with the 601 library. Let's look at an example of the state machine class. I want to build an accumulator, which at every time step will look at the input, add it to every other example of input and output, and then output the new value and retain it as the new state. The first thing I need to do is initialize the accumulator with a value. This is our start state, which is the same as our start state from the MBTA's turnstile and also the same as our start state from the mathematical construct. I also want something called get next values, which is the functional equivalent of the transitions. Here's ourself again from object-oriented programming, but we don't care about that. We're going to look at the current state and the current input. Some get next values will do some internal data munging, possibly multiplication by 2 or comparing to the previous input and then having some conditionals, that sort of thing. But there'll be some sort of very simple function under get next values, at least for the first couple weeks. And then we'll return the new state and the output in a tuple. In this case, the new state is going to be the linear combination of the current state and the current input, and the output is going to be the linear combination of the current state and the current input. If I were to draw this accumulator as a state transition diagram, I would do this. My start state is the initial value. If I pass in a new input, we'll call it input 0, both my output and the new state. Are going to be the linear combination of these two values. If I made another transition, I would take whatever my next input was and add it to the current state value and return it out as the output, and so on and so forth. I encourage you to try this in Python, or I guess in idle, and munge around with it and see what you can get it to do. You might have to type add lib 601 in order to get the state machine class, or initialize using lib 601 in order to get the state machine class. But otherwise, this should be enough to get you started with the introduction to state machines. If you're having trouble, I highly recommend going through all the examples in the readings. They're pretty comprehensive, and it also includes the accumulator. That's all for now. Next time, we'll talk about linear time invariant systems."
    },
    {
        "Rec 13 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-UGdXwvB6K-w.mp3": " Hi. Today I'd like to introduce a new module. In previous modules we've talked about how to model particular systems, how to make predictions about certain kinds and classifications of systems, and also how to design both theoretical and physical systems. If our systems are operating in a deterministic universe, then we're all set. We've accounted for all the things we can possibly account for. But if we're making systems that are going to operate in the real world, then we need to be able to deal with some level of uncertainty. Today I'm going to talk about probability, which is the method by which we're going to model uncertainty in our world. And later we're going to talk about different strategies we can take to deal with that uncertainty to hopefully increase the level of autonomy of our systems as they operate in the world. The first thing I need to talk about is how to properly model probability or how to properly talk about probability such that we can use it to talk about uncertainty. When you're talking about probability, typically you'll end up talking about the sample space or u. This refers to your entire universe, where your universe is the values that you care about, or the possible assigned values of the variables that you care about. I'm already talking about variables, but what I really mean to say is events. There are different states that your universe can take, and those states can be sort of exhaustively enumerated or be atomic, or they can be factored into different variables. Let me elaborate on this a little. If I were talking about four coin flips, if those events were specified atomically, then I would have to exhaustively specify all possible sequences of four coin flips, right? Four heads, three heads and one tail, two heads and two tails, that sort of exercise. Or flipping four heads would be one event. Flipping three heads and then one tail would be a separate event. Flipping two heads, one tail and another head would be a third distinct event. If we're talking about a factored state space, then we'll have a different variable representing each one of these coin flips. And each one of those variables will say, here's the value associated with that particular sub-event, and the accumulation of all those values, or the specification for all those values, will end up referring to the same states that were addressed by the atomic events. Let me talk about variables, because they're the thing that allow us to not exhaustively enumerate every possible event in the universe and talk about our universe in meaningful ways or in ways that can be effectively communicated. And why am I talking about random variables? Why aren't they just random variables? Well, random variables is the way that you specify the fact that you're talking about probabilistic variables. When you're just dealing with regular algebra, variables have some sort of assigned value, and you're not forced to be in the space of 0 to 1. When you're talking about from 0 to 1, when you're talking about probabilities, you're forced to be in the space of 0 to 1 and forced to remain within the reals. If you want to talk about all the possible assigned values of that random variable, then you're talking about the distribution over that random variable. This means that A could be anything that A could be. You're talking about the function that says, give me a value of A, and I will tell you a probability associated with that value of A. If you're talking about the probability of A being assigned to a particular value, then you'll return out the probability. If A represents the color of the shirt I'm wearing and the probability that A is some value, I want to know the probability that A is some value, then I look at the color of the shirt I'm wearing and try to determine whether or not it's 1 or 0, or possibly look at the colors of shirts that I've worn over the past year and make some sort of ratio of the number of pink shirts I've worn over the past year. If I'm dealing with a factored state space, I'm going to end up talking about more than one random variable. There are two main ways to talk about more than one random variable at the same time. One is joint probabilities, where all the random variables are collectively specified at the same time, and the other is conditional probabilities, where you've already decided that you've specified some value for one or more random variables, and then within that scope you're going to talk about the probabilities associated with other random variables. I want to demonstrate this graphically, but there are two more things that I need to mention right now. One is the difference between the frequentist and Bayesian interpretations of probability. Right now they don't seem particularly relevant, but people will use these words, and it's good to know approximately what they're talking about. The frequentist interpretation of probability is more relevant when you're talking about actions that happen a lot of different times. For instance, how frequently it rains. If I say that today is Wednesday, and I want to know the probability that it rains on Wednesdays, then that probability is open to frequentist interpretation because there are a lot of Wednesdays, and it's rained a lot in the universe of Wednesdays, or the space of possible Wednesdays. So thinking about the fact that there's a 70% chance of rain, or a 30% chance of rain, or whatever probability of rain there is on Wednesdays makes sense, or is open to frequentist interpretation. The other interpretation that you'll hear talked about with respect to probability is the Bayesian interpretation. Bayesian interpretation tends to be more relevant when you are talking about spaces that are more atomic as opposed to factored, or represent events that are specified to the point that it does not make sense to talk about them in the frequentist sense. When we talk about probabilities in the Bayesian interpretation, we frequently use the term likelihood. For instance, if I'm talking about whether or not it's likely to rain on August 24, 2011, in the afternoon, the specificity of that event is so high that at that point it doesn't make sense to talk about the frequency of Wednesday, August 24, 2011 in the afternoons, at least for now. At that point we're talking more about likelihood and less about frequency. That event is more conducive to Bayesian interpretation. No world-one tour of probability would be complete without a mention of the three axioms of probability. The first axiom is that the likelihood of the universe happening is 1, or all random variables are going to be specified at some point. Relatedly, the likelihood of nothing happening is 0. What these two really do is establish the boundaries of the graphical representation up here. The other axiom of probability is that if you're going to be talking about the union of two events, or the probability associated with one or the other event having an assignment, then you're talking about the probability of one of those variables and the probability of the other variable, and then removing the section that you've double counted. So if I were to attempt to demonstrate this on this graph, I would be talking about the probability of A, added the probability of B. At this point I've double counted the section in the middle where they're both 1, so I would subtract this exactly once. And then I'm talking about the size of this space. If I'm talking about the probability of A being equal to 1, then I'm talking about the space in which A is specified to be equal to 1 divided by the area associated with my universe. When I'm talking about joint distributions, I'm going to find the space in which both of these things are true, scoped to the size of the universe, or the entire sample space. So in this space, both A and B are true, and I'm looking at it relative to the size of the universe. In contrast, when I'm talking about conditional probability, or the probability of A given to B, I'm going to look at where my specifications are true, scoped to where my givens are true. So if I'm already dealing in the space restricted to B, I'm just looking at this size, or this space, but because I'm scoped to B, I'm going to end up dividing by the area of B instead of the area of U. This is the main difference between the joint and conditional distributions, and a lot of people get hung up on it, which is why I'm exhaustively walking through it. All right, a couple other things before we talk about the first way in which we can use our models for uncertainty to do some amount of addressing of the fact that we're going to have to deal with uncertainty in the future. If we start off with a joint distribution, and we want to reduce the number of variables that we're actually talking about, we can do so by exhaustively walking through all the possible assigned values for the variable that we want to disregard, and then summing up the values appropriately. An easy example for this is if we had the joint probabilities of all the colors of shirts that I wear, and all the colors of pants that I wear, and we only wanted to talk about all the colors of shirts that I wear, then we could exhaustively cover all the different colors of pants that I wear, and accumulate all the different values of shirts that I wear simultaneously, and then collect that distribution. Accumulation is the concept of total probability, which is the same kind of accumulation. It just operates in the conditional space as opposed to in the joint space. So in this case, if I'm already operating in an A given B land, I have to scope myself back out to the space of the universe by then accounting for the fact that I've only been operating in the scope of B. Then exhaustively enumerate all the possible values of B, sum the probabilities associated with those values, and then I've reduced the number of dimensions that I'm talking about. The final thing we have to talk about before we move on to state estimation is called Bayes evidence, or you've probably seen this demonstrated as Bayes rule. If I want to talk about B, scope to A, and all I have is A scoped to B, B, and A, when we walk through total probability, we saw that the conditional probability multiplied by the probability associated with the variable that we're conditioning on is equal to the joint probability. When we multiply A given B by P of B, or P of A given B by P of B, we're going to end up with the joint probability associated with the two variables. When we then divide back out by A, or scope our joint probability to A, we end up talking about conditional probabilities again, which is where B given A comes from. Bayes evidence, or Bayes rule, is the basis for inference, which is going to be really important for state estimation, which we'll cover next time."
    },
    {
        "Rec 3 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-Y9r9dO7KQj4.mp3": " Hi. Today I'd like to talk to you about some notable aspects of Python that you'll encounter in the 6.01 software as well as in the general sense when working with Python. A lot of these little tidbits of Python have some interesting history associated with them, especially related to the history of computer science. And also I want to indicate some things that tend to mess up, especially first-time programmers, but also especially people that are new to Python. First, I'd like to bring us back to last week. We were talking about object-oriented programming and inheritance. Object-oriented programming is a programming paradigm. It's in the same category as imperative programming or functional programming. And you might say, I have a good sense of the fact that in object-oriented programming everything is an object, but I don't really have a good sense of what constitutes imperative programming or functional programming. I'm going to focus on functional programming right now because it has more of a historic root in the academic development of computer science. Functional programming is like object-oriented programming in that everything is a function. It refers to the idea that you want to write as much of your code as possible in a purely functional manner. And in particular, you're looking for the ability to avoid things like side effects or enable many different kinds of evaluation. Those kinds of things are not the subject of this course, but I think they're worth noting to figure out why we're bothering learning about things like lambdas and list comprehensions in the first place. Speaking of which, the first thing we need to talk about before we even talk about things like lambdas and list comprehensions is the concept that in Python everything is an object, but because functions are first-class objects, we can also treat Python to a large extent like a functional programming language. What do I mean when I say functions are first-class objects? I mean that a function can be the return value of another function. I mean that a function can be passed in as an argument to a function. And I mean that functions can be assigned variable names and manipulated the same way that we manipulate any other piece of data structure. This is important because if you want higher order functions or functions that actually modify or use other functions as a part of whatever it is that they do, then you need to be able to interact with a function like it's any other object, in part pass it in or return it out. Let's look at an example. So let's say I start off with a very basic function. If you want to square some sort of value, probably a numeric in this case, then all you have to do is multiply it by itself. Pretty simple. Good place to start. As a consequence of the idea that functions are first-class objects, I can write a function that takes in a function as an argument and then develops a return function that uses the function that I passed in on itself. So any arguments that I would pass to some function, I would pass into some function, take the return value of this function call, and then pass that into some function again. That's what return function does. And down here, I return return function. Note that the object type of this return value is a function. So once I have return function, I can actually pass it arguments, have them run through some function, not once, but twice, and then get out a value that's of the return type of some function. Note that I made some assumptions while writing this function. First, I've assumed that some function returns out the same number of arguments and either accepts an arbitrary number of arguments or accepts the same number of arguments as it puts out. The other assumption that I've made is that the data type that some function returns is the same as the data type that some function accepts. Or the things that some function does to its arguments can be done to multiple kinds of arguments. But that's a whole different argument. Right now, we're just focusing on the fact that we pass in some arbitrary number of arguments, call some function on it, call some function again on the return value of this, and return that as something you can do to whatever it is some function operates on. OK. Let's review an example because I promise it will be more clear. Let's say f is going to be the composition of square on itself. If I do that, I end up with a function that operates on an arbitrary number of arguments. That's not true. I end up with a copy of square that takes in the same number of arguments as square called on itself. So here, square is substituted for some function. Here, square is called on that call of square. And here is what f is assigned to. So when I call f of 2, I'm going to substitute 2 in for args. I'm going to call square on args. If I call square on args, I get out 4. And if I call square on args again, or if I call square on args where args is defined as the return value of this, then I'm going to call square on 4. I'll square 4, and I'll get 16. Take a second to type it into idle and make it make sense to yourself. All this code should compile. Mess around with the parameters if you're having trouble convincing yourself that it does. OK. Now I think we're ready to talk about lambdas. If you can pass in functions as arguments or return them as values or assign them to variable names and just treat them like any other data type, then you should be able to treat them as some sort of raw value. And that's where lambdas come in. Lambdas are lambda is a keyword in Python that tells you you're about to use a function that you have not given any sort of name or defined in any place in your code or environment beforehand. You're just going to start talking about something that you would like to do to a given number of arguments and then what you want to return out. Lambda has roots in lambda calculus, which if you're familiar with the history of computer science you've probably heard of, now might be a good time to look up lambda calculus if you've never heard it before or possibly Alonzo Church. But it's still available in code today, which is really cool and speaks to the continuing power or at least recognition of importance of functional programming. As I said before, the idea with lambda is that you can write an anonymous function. Over here, in order to write a function that's squared, I had to write out a defined line and because in Python indentation carries meaning, I'd have to enter and return over to the next line. I'm actually not sure if that's strictly true. I think you can do def square x, return x, but I'm not positive. I've only seen it written like this and I think it's because in the general sense people try to respect things like convention and readability in Python. If you're using lambda, people already know that you want to describe a function really quickly or you want to describe a function without assigning it a name. Therefore, the two most common uses you'll see of lambda is when you want a really fast function and you don't want to spend extra time or lines writing out that function, it's pretty clear what this is going to do. Or if in the particular sense you need an anonymous function, you don't want to assign a name or memory space associated with that function. Note that instead of using square, I can just write out this. So I save two lines of code, defining square, and then I don't have to refer back to some earlier part of the code in order to understand what this line does. Pretty cool. All right. So there's functions as first class objects. There's lambda. Let's talk about how to use lambdas on lists. In Python you'll end up doing a lot of list manipulation. One of the best uses of anonymized functions in any functional programming language is the ability to manipulate lists in line. If I've defined just a pretty straightforward list right here, I can use functions like map, filter, and reduce to, in one line, take a list, apply a function to every element in that list, and then return a copy of the result of that list. I didn't have to write a separate function to do the list handling. I didn't have to write a separate function to multiply the list by two. And I've communicated what I'm doing effectively to people that are used to functional programming. So if you type this line in, you should end up with a return in interactive mode of every element in this list multiplied by two. Now's a good time to try. Once you've done that, I also recommend looking at the original copy demo list. Note that this is unaltered. Map actually returned a new data structure that represents performing this function on this list. This becomes important later when I explain aliasing, but I will do that in about two minutes. If we want to get even more elegant, we can use list comprehensions. List comprehensions, if you ask somebody where list comprehensions are from, they'll probably say Haskell, because that is the most popular use of list comprehensions in terms of pure lazy functional programming. But it actually goes back further than that and is in small talk, and then something from the 60s, I can't remember the name of it right now. They're really nice because they borrow syntactic approach from mathematicians. This looks a lot like set notation. And when you read this, you can probably tell that the list comprehension is going to describe a set of points from one to four. But you accomplish that in possibly one line of code. I've written it on three here because I wanted to keep it in this space, but it's really concise, right? If you type this into idle, you should see the kind of list that it returns and also what list comprehension is now, what now constitutes list comprehension. You can use list comprehensions with functions like map and filter and reduce. And along with anonymized functions, all of these tools provide you with a lot of functionality and a very short amount of space. It's also good to be able to recognize what's going on when you see something like this or something like this because you'll probably run into it in particular a lot of Lisp code, but also in the artificial intelligence community in particular. We talked about all that. Let's take a second to talk about the fact that lists are actually mutable and what that means and what things you have to be careful about if you're going to be working with mutable objects. If you're new to programming or you're new to Python, you've probably already worked with some of these data types, right? Any numbers, any strings, any tuples are going to be immutable. What that means is if you have a variable and you have a second variable that has a different assignment line associated with it, right? I haven't assigned g to h here. I've just assigned g to hello and h to hello. If you look at the space in memory that Python associates with both objects, they point to the same place. This is the definition of an immutable object. When g points to hello and h points to hello, they both point to the same place. If x points to 5 and y points to 5, they're both pointing to the same place. If you point x at 6, it now points to a different memory address. This gets confounded when you're talking about mutable objects. The problem with mutable objects is that you select a memory space to contain the object or memory ID in Python to contain the object and then that object is changed in place. You can use this to your advantage but it can also mess with you and here's how. Let's say I assigned a to a small list and I also assigned b to a. b and a now point at the same place. If I manipulate b and I'm still working with an immutable object, the object in that memory address has been altered and b and a still point to the same place in memory. So if I look at a, it's going to look like b, which is going to look like 1, 2, 3, 4, which is not what I assigned a to originally. Again, can be powerful but you have to keep it in mind and it might start to mess with, in particular, the 601 software when you're dealing with state machines. In order to get around this, you can create a copy of the list and then modify the copy. This is actually what map does. If you want to do it, the easiest way to do it on the first layer is to specify the index into the list with no bounds. It'll copy the whole thing. There's also a Python library copy or deep copy if you want to copy lists of lists of lists of lists of lists. And just to clarify, you'll note that after you assign c to a copy of a, c and a occupy different memory places. So if you modify c, a will retain its original value. I think that's all the notable things I have to say about Python and gives us enough power to do some really powerful list manipulation or array manipulation and covers what we want to say about functional programming before we get into the notion of state, which I'll talk about next time."
    },
    {
        "Lec 7 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-5sLFTc10kg8.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So today I want to start a new topic, circuits. Right. No, no, no, that's good. That's good. Circuits are good. Thank you. Thank you. Much better. So just to provide some perspective, I want to remind you where we are, how we got here, and where we're going. So at the beginning of the course, we promised that there were several intellectual themes that we would talk about. Probably the most important one there is designing complex systems. That's what we're really about. We would like you to be able to make very complicated systems. How do you think about parts? How do you think about connecting them? How do you think about things when you want to make something that's very complicated? Part of that is modeling. We just finished a module on modeling. So in order to make a complex system, we'd like to be able to predict how it will behave before we completely build it. Sometimes it's impossible to build the entire system. Sometimes it's impossible to build prototypes. Sometimes you're stuck with going with the design at launch and figuring out how it works. In those cases in specific, it's very important to be able to model it to have some confidence that the thing is going to work. We're going to talk about augmenting physical systems with computation. That's the module we're about to begin. And we'll conclude by talking about how to build systems that are robust to change. So we started with the idea of how do you make complicated systems. And we introduced this notion of primitives, combination, abstraction, and pattern in terms of software engineering. We did that because that's the simplest possible way of getting started. It provided a very good illustration of PCAP at the low level by thinking about Python, by thinking about the primitive structures that Python gives you, how those can be combined, how you can abstract, how you can recognize patterns. But then we also built a higher level abstraction, which was the state machine idea. There the idea was you didn't have to have state machines in order to build the brain for a robot. But it actually turns out to be easy if you do, because there's a modularity there. You can figure out if each part works independent of the other parts. And then you can be pretty sure when you put it together the whole thing's going to work. So that was kind of our introduction to this notion of PCAP. Then we went on to think about signals and systems. And that was kind of our introduction to modeling. How do you make a model of something that predicts behavior? So we transitioned from thinking about how do you structure a design to how do you think about behavior. Today we're going to start to think about circuits. Circuits are really going to a more primitive physical layer. How do you think about actually making a device? So the device that we'll think about is a thing to augment the capabilities, the sensor capabilities of the robot. We'll think about making a light tracking system. And the idea is going to be that you'll build a head. The head has a neck. So you have to control the neck. The head has eyes. It will mount on top of the robot. And that will let you drive the robot around looking for a light. And you'll do that by designing a circuit. So that's kind of the game plan for the next three weeks. So today what I want to do is introduce the notion of circuits. Introduce the theory for how we think about circuits. And then in the two labs this week, the idea will be to become familiar with the practice of circuits. How do you actually build something? How do you actually make something work? So today's the theory. So the theory, the idea in circuits is to think about a physical system as the interconnection of parts and rules that connect them. In fact, the rules fall into two categories. We're going to think about the currents that go through parts and the voltage that develops across parts. And we'll see that there's a way of thinking about the behavior of the entire circuit that integrates all of those three pieces. How does the part work? How do the currents that go through the part work? And how do the voltages that are produced across the part, how do they work? So I'll just start with two very simple examples. The first is the most trivial example you can think about. You can think about a flashlight as a circuit. You close the switch, current flows. Very simple idea. We will make a model of that that looks like this. We'll think about the battery being a source, in this case a voltage source. We'll think about the light bulb being a resistor. We'll have two parts. We'll have to know the current voltage relationships for both of the parts. And we'll have to know the ramifications for those currents and voltages when you put them together. Very, very simple. The other simple example that I want to illustrate is this idea of a leaky tank. Here the idea that I want to get across is that the circuit idea is quite general. When we talk about circuits, we almost always talk about electronic circuits. But the theory is by no means limited to electronics. So for example, if we think about a leaky tank, we think about a pipe spewing water into a reservoir. Maybe that's the Cambridge reservoir. Maybe that's the water coming out of the Woburn reservoir. Maybe that's the demand put on by the Cambridge people trying to take showers in the morning. I don't know. So we think about flow into a tank, a reservoir, and flow out, and we can make a model for that in terms of a circuit. In the circuit, there are through variables and across variables. In an electronic circuit, the through variable is current. Here the through variable is the flow rate. So it's the flow of water in and the flow of water out, represented here by these things that look like currents. And the across variable for an electronic device is voltage. The across variable for this kind of a fluidic device, the across variable is pressure. So we think of as this thing gets ahead of that thing, as there's more stuff coming in than going out, the height goes up and the pressure builds. Same idea. So the point is that we'll develop the theory for circuits in terms of electronics, but you should keep in the back of your mind that it's a lot more general idea than just electronics. In fact, there are two completely distinct reasons why we even bother with circuits. One is that they're very important to physical systems. If you're designing a power network, of course you have to think about the way the power grid works as a circuit. That's obvious. In electronics, of course, if you're going to build a cell phone, you have to know how the parts interconnect electronically. That's obvious. But probably the biggest use for circuits these days is not those applications, although those are very important. Circuits are also used as models of things. So many models for complex behaviors are, in fact, circuit models. So in terms of electronics, the idea is that we want to get on top of electronics. We want to understand how circuits work so we can understand things like that. If you look at how complex processors have got over my professional life, we start with my professional life down at about 1,000 transistors per processor. And today we're up at about a billion. That's enormous. Even in the Stone Ages, when we were designing things that had 1,000 parts, we still had trouble thinking about those 1,000 parts all at once. We still need PCAP. We still needed ways of combining the activities of many things into a conceptual unit that was bigger. Here, it's just impossible if you don't have that. So that's one of the reasons we study circuits. And the other reason is here. So here I'm showing a model for the way a nerve cell works. This model is taken from 6.021. The idea, this comes from the study of the Hodgkin-Huxley model for neural conduction, arguably the most successful mathematical theory in biophysics, which explains the completely nontrivial relationship between how the parts from biology works and the behavior in terms of propagated action potentials works. So the idea is that we understand how this biological system works because we think about it in terms of a circuit. That's the only successful way we have to think about that. So what I want to do then is spend today figuring out circuits at the very most primitive level. The level that I'm going to talk about in terms of circuits is roughly analogous to the level that we talked about with Python when we were thinking about how Python provides utilities for primitives, combinations, abstractions, and patterns. So I'm going to start at the very lowest level and think about what are the basic primitives, the smallest units we'll ever think about in terms of circuits, and what are the rules by which we combine them? So I'll start with the very simplest elements. We will oversimplify things and think about the very simplest kind of electronic elements as resistors that obey Ohm's law, V equals IR, voltage sources, things that maintain a constant voltage regardless of what you do, and current sources, things that maintain a constant current regardless of what you do. These things are, as I said, analogous to the primitive things that we looked at in Python. They're also analogous to the primitive things that we looked at in system functions. Can somebody think of when we were doing difference equations, what were the primitives that we started with when we started to study difference equations? What's the most primitive elements that we thought about? AUDIENCE MEMBER 2, INAUDIBLE PROFESSOR 1. Delay, yeah. So we thought about things like, so we had delay. Anything else? Gain. Anything else? Add. So we had exactly three primitives, and we got pretty far with those three primitives. We learned the rules for interconnection. We didn't really make a big deal out of it. We didn't formalize it. But the rules for interconnection were something like every node has to have exactly one generator. You can't connect the output of this to this. That's illegal. Every node has to have one source, and every node can source lots of inputs. That was kind of the rules of the interconnect. The interconnects here will be a little bit more complicated. So those are the elements that we'll think about. And the first step's going to be to think about how do they interconnect? The simplest possible interconnections are trivial. In the case of the battery, you hook up the voltage source to the resistor. The voltage source makes the voltage across the resistor 1 volt. If we say the resistor is 1 ohm, then there's 1 amp current, period. Done. Easy. Similarly, if we were to hook up the resistor to a current source, we would get something equally easy, except now the current source would guarantee that the current through the resistor is an amp. Therefore, the voltage across the resistor, by Ohm's law, would be a volt. So we would end up with the same solution for a completely different reason. Here the voltage is constrained. Here's the current's constrained. And just to make sure everybody's with me, figure out what's the current, I, that goes through this resistor, slightly more complicated system. Take 20 seconds, talk to your neighbor, figure out a number between 1 and 5. OK. OK. OK. So what's the answer? Everybody raise your hand with a number 1 through 5. Come on, come on, come on. Everybody vote. Come on, come on, come on. You can blame it on your neighbor. That's the rules, right? You talk to your neighbor, then you can blame dumb answers on your neighbor. OK, about 80% correct, I'd say. So how do you think about this? What's going to be the current? How would you calculate the current? What do I do first? Shout. If you shout, and especially if my head's turned away from you, then you are. Well, the Kirchhoff's law. Kirchhoff's law. Wonderful. Which one? The two. Loop. Loop. Loop. Loop. Loop. Loop line. What loop do you want to use? Left side. So if we use the left side loop, we would conclude that there's a volt across the resistor. So the current would be an amp. Where'd the current come from? The voltage source, just like before, right? So not quite. So the voltage source establishes this voltage would be 1. That makes this current be 1. That would be consistent with the current coming out of here, except we have to also think about that 1 amp source. So the question is, what's a 1 amp source do? Nothing. It's just there for decoration or for off-view stations so that we can make an interesting question to ask in lecture. Oh, baby. So where's the current, where's the 1 amp that goes through the resistor come from? It comes from the right. It comes from the current over here. So the idea is that if this current, ignore the voltage over here for the moment, if this current flowed through the resistor, then you'd have 1 amp going through there, and you'd have 1 volt generated by that current, which just happens to be exactly the right voltage to match the voltage from the voltage source. So if you think about this, the voltage guarantees that this is 1 volt, but so does the current. In order to simultaneously satisfy everything, all you need to do is have all of this current go around and come down through that resistor. That'll generate the volt, so there's no propensity for more current to flow out of the source, because the source is 1 volt, and it's facing a circuit that's already 1 volt. So the idea was to try to give you something that's relatively simple that you can think through on your own, but not trivial. So the answer was 1 amp, but the 1 amp was not for the trivial reason. The 1 amp is because the current from the right flows through the resistor and makes the voltage be 1. So the right answer is 1, but for the reason that you might not have originally thought. But more importantly, I wanted to use that as a motivation for thinking about how do we think about bigger circuits. So when the simple circuit, like two parts, is no problem figuring out what the answer's going to be, but when the circuit has even three parts, it may require more thinking, and you may want to have a more structured way of thinking about the solution. Yes? AUDIENCE MEMBER 2 What would have occurred if the current provider on the right side puts 2 amps? PROFESSOR 1 Great question. Had this been 2 amps, you can't violate this voltage. So that would have been 1 volt. So that would have been 1 amp through the resistor. So then you're left with the problem that this guy's pushing 2, and that guy's only eating 1. But the rules for the voltage source say, eat or source however much current is necessary in order to make the voltage equal to 1. So the excess amp goes through the voltage source. So the voltage source is in fact being supplied power rather than supplying power itself. Had this been 2 amps, some of the power from this source would have gone into the resistor, and some of the power from this source would have actually gone into the voltage source. So if the voltage source were, for example, a model for a rechargeable battery, that rechargeable battery would be charging. Does that make sense? So if there had been a mismatch in the conditions, you still have to satisfy all the relationships from all the sources. AUDIENCE MEMBER 2 What if the voltage was larger? PROFESSOR 1 It would have happened if the power was going in the opposite direction. Let's say that if the voltage here had been 2 volts, then the voltage would have required that there was 2 amps flowing here. One amp would come from here, but another amp would come from here. This voltage source will supply whatever current is necessary to make its voltage law real. In fact, what we'll do now is turn toward a discussion of more complicated systems that will let you go back and in retrospect analyze all those cases that we just did, and you'll be able to see trivially how that has to be the right answer. So what I want to do now is generate a formal structure for how you would solve circuits. Yes? AUDIENCE MEMBER 2 Do we know of anything that could generate a current without generating a voltage? Like in real life? PROFESSOR 1 Can anything generate a current without generating a voltage? That's a tricky question. If you think about something as generating a current, then the voltage is not necessarily determined by that part. So that's kind of illustrated here. If this guy is generating a current, this guy is not actually the element that is controlling its own voltage. In general, if you want to speak simultaneously about the current and voltage across the device, you have to know what it was connected to. Each part, we'll get to this in a moment in case some of you are worried about launching ahead. We will cover this. This is very good motivation for figuring out what's going to happen in the next three slides. So each part gets to tell you one relationship between voltage and current. Generally speaking, that's not enough to solve for voltage and current. Voltage and current is like two unknowns. Each element relationship is one equation. So the current source gets to say current equals x. Current equals 1 amp. Doesn't get to tell you what the voltage is. So being a little more physical to try to address your question more physically, there are processes that can be extremely well modeled as current generators. In fact, many electronic semiconductor parts, like transistors, work more like a current source than like anything else. So there are devices that behave as though they were current sources, but they don't simultaneously get to tell you what is their current and what is their voltage. They only get to tell you what is their current. OK. So let's think about now, if you had a more complicated system, how could you systematically go about finding the solution? As was mentioned earlier, there's something called Kirchhoff's law. And in fact, there's two of them. Kirchhoff's voltage law and Kirchhoff's current law. Kirchhoff's voltage law, in its most elementary form, says that if you trace the path around any closed path in a circuit, regardless of what the path is, every closed path, the sum of the voltages going around a closed path is 0. So for example, in this circuit, the red path illustrates one closed path through the circuit. It goes up through the voltage source, down through this resistor, and then down through that resistor. Kirchhoff's voltage law says the sum of the voltages around that loop is 0. That's written mathematically here. Minus V1 for here, plus V2 for here, plus V4 for here is 0. OK, where do the signs come from? The signs came from the reference directions that we assigned arbitrarily to the elements. Before I ever do a circuits question, I always assign a reference direction. Every voltage has a positive terminal and a negative terminal, and I must be consistent in order to apply these rules. These rules only work if I declare a reference direction and stick with it. If midway through a problem I flip it, I'll get the wrong answer. So the minus sign has to do with the fact that as I trace this path, I enter the minus part of this guy, but the plus part of that guy and that guy, so the sign of V1 is negated relative to the others. A different way to think about that is here. We can think that V1 is the sum of V2 and V4. That's sometimes more intuitive, because if you started here, going through this path you would end up with a voltage that is V1 higher than where you started. Whereas starting here, you would end up with a voltage here that's V4 higher than where you started. And then by the time you got to here, it would be V2 plus V4 higher than where you started. You start one place and on one route you end up V1 higher, and in the other route you get V4 plus V2 higher. So it must be the case that V1 is the same as V2 plus V4. Those are absolutely equivalent ways of thinking about it. So those laws are equivalent. If you think about it as a path, you think about some of the paths coinciding with the negative direction of some of the elements and the positive direction of others. How many other paths are there? Take 20 seconds, talk to your neighbor, figure out all the possible paths for which KVL has to apply. OK, so everybody raise your hand and show a number of fingers equal to the number of KVL equations less 2. Very good. Virtually 100% correct. Why do you all say 5? Which is to say 7. Why do you all say 7? So there's three obvious ones. I was expecting a couple of 3's. This was supposed to be, OK, yeah, I do plot against you. I was expecting some 3's. So there's three obvious paths that are analogous to the first one we looked at. If I call the first path A, then there's B and C, which are the excursions around here. And you can write the equations just the same. They each involve three voltages, and they each go through some starting at the negative side and some starting at the positive side. So those are, in some sense, the obvious ones. But there are others too. So one way to think about it, what I'd like you to do is enumerate all the paths through the circuit. I should have said all the paths through the circuit that go through each element one or fewer times. I don't want you to go through the same element twice. So here's another path that would go through elements at most one time. So up through here, over through here, which didn't go through any elements, down through that element, across that line, down through here, et cetera. And you get an equation for that. Here's another, here's another, here's another. And if you try to think about a general rule, a general rule of something like, how many of those panels can you make and paste together where the loop goes through the perimeter? You're not allowed to go through an inner place, because if you want through an inner node, you'd have to go through it twice. If you wanted the path to go through an inner element, you'd have to go through that element twice. So in fact, the answer is seven. There are seven different ways, seven different paths, all of which, according to Kirchhoff's voltage law, all of which, the sum of the voltages around those paths, has to be 0. The problem is, of course, that those equations are not all linearly independent. So if you just had a general purpose equation solver, and by the way, we'll write one of those in week 8 for solving circuits, if you just passed those seven equations into a general purpose equation solver, it would tell you there's something awry with your equations, because they're not linearly independent. So you can, however, think about linearly independent in particularly simple cases. This network is a particular kind of network that we call a planar network. A planar network is one that I can draw on a sheet of paper without crossing wires. So I can draw this network without crossing wires. I'll call it planar. And it turns out that the Kirchhoff's voltage laws for the innermost loops are always independent of each other. That's kind of obvious, because as you go to a, so each loop contains at least one element that some other loop didn't have. So that's kind of the reasoning for why it works. So if you think about this particular loop, which we included in the 7, you can think about that as being the sum of the loops this way, the A loop and the B loop. Because if you write KVL for the A loop and KVL for the B loop and add them, you end up deriving KVL for the more complicated path. And if you think about what's going on, it's not anything terribly magic. This path is the same as the A path added to that path, where I went through this element down when I did the A path and up when I did the B path. So those parts canceled out. That was the rule that I was talking about, how I don't really want to go through the same element twice when I'm applying KVL. So the idea then is that there's a systematic way, an easy way, to figure out all the KVL loops. You just think about all the possible paths through the circuit. You do have to worry about linearly independent. In the case of planar networks, that's pretty straightforward. In the planar networks, you can always figure out the linearly independent KVL equations by looking at the smallest possible loops, the loops with smallest area. OK, so that's half of it. That's KVL. The other Kirchhoff's law is KCL, Kirchhoff's current law. There we're thinking about the flow of current. So the flow of current is analogous to the flow of incompressible fluid, water for example. If you trace the amount of water that flows through a pipe that goes into a Y, then the sum of the flows out has to equal the flow in. If that weren't true, the water would be building up. So we think about pipes as transporting the flow of water without allowing it to build up anywhere. That's precisely how we think about wires in electrical circuits. The wires allow the transport of electrons, but don't allow the buildup of electrons. OK, do electrons build up? Sure. In our idealized world, we say they don't build up in the wires, they build up in a part. And we'll have a special part that allows the electrons to build up. So we're not excluding the possibility that they build up, we're just saying that in this formalism, we don't allow the electrons to build up in the wires. So for the purpose of the wires, current in is equal to the current out. The net current in is 0. So we will think then about the circuit having nodes. The nodes are the places where more than one element meets, two or more elements meet. And we will apply KCL at each node. So for example, in this simple circuit where I would have three parts connected in what we would call parallel, they share a node at the top and they share a node at the bottom. So even though it looks like there's multiple interconnects up here, we say that's one node. And we would say that the sum of the currents into the node is equal to the sum of the currents out. So if I labeled all the possible currents that come out of that node, I would have I1, I2, I3. I1 goes to the first one, the second one, the third one. And so I would conclude from Kirchhoff's current law that the sum of I1, I2, and I3 is 0. Easy, right? As I said, we're going to make an abstraction where the electrons don't build up in the wires. They don't even build up in the parts. They do get stored in the parts. It's a little confusing, we'll come back to that. If they don't build up in the parts, then the current that goes in this leg has to come out that leg. If that's true, then I1 is I4, I2 is I5, I3 is I6. And we end up with another equation down here, which turns out to be precisely the same as the one at the top. OK, everybody's happy with that? So we're thinking about this just the way we would think about water flow. If there's water flow into a part, it better be coming out. If there's water flow in a pipe, the water that goes into the pipe better come out of the pipe someplace. So here is an arbitrary network made out of four parts. How many linearly independent KCL equations are there? So how many linearly independent KCL equations are in that network? Raise your hand. Some number of KCL equations. OK, I'm seeing a bigger variety. I see ones, twos, and threes. I don't see any fours. That's probably good. So how do you think about the number of linearly independent KCL equations? So first thing to do is to label things. So you have to have reference directions before you can think about things. So we have four elements. We would be expecting to see four element currents. The same current that goes into an element has to come out of it. So there's element current one, two, three, and four. There are three nodes. So we might be expecting three KCL equations. Here's one node from which you would conclude that the sum of I1 and I2 better be 0. Here's a node from which you would conclude that the current in I2 better be I3 plus I4. And here is a node from which you would conclude that I1 plus 3 plus 4 is 0. So I can write one KCL equation for every node. That's not surprising. But if you look at those equations, you'll see that they're not linearly independent. In fact, if you solve this one for I2, it's already solved for I2, stick that answer up here, you get I3 plus I4 added to I1 is 0, which is just the same as that equation. So of those three equations, only two of them are linearly independent. So the answer to that problem was 2. And there's a pattern. So think about the pattern in terms of figuring out the number of linearly independent KCL equations are in a slightly more complicated network. So what's the answer here? How many KCL equations are in this network? Wow. Wow. Not getting any of the answers I would have said. What does that mean? Ah, I'm forgetting to add 2. That's my problem. OK. Now I'm getting some of the answers that I would expect to get. OK, got it. I confused myself. OK, the vast majority say 1. How do you get that? Which is 3. So again, you think about in this circuit there are four nodes, A, B, C, D. So we can think about writing a KCL equation for each one. If we go to A, A has three currents coming out of it, 1, 2, 3. So the sum of those has to be 0, et cetera. And if you think about those equations, they're not linearly independent either. If you work through the math, you see that there's exactly one of those equations that you can eliminate. So you're left with three linearly independent KCL equations. And so there's a pattern emerging here. Does somebody see the pattern? 1 minus. Can somebody prove the pattern? So there's a pattern here. The pattern is take the number of nodes and the number of independent KCL equations is 1 less. So the challenge is, can you prove it? And by the theory of lectures? Yes? Yes. And by a corollary of the theory of lectures, the way you would prove it is? On the next slide. Exactly. So how do I prove it? Yeah? So there's something special about the last one. Because the circuit's closed. So that's right. So the idea is to sort of generalize the way we think about KCL. So we start with the circuit. We think about having four nodes here. It's certainly the case that KCL holds for each node. So here's KCL for that node. But now if you think about KCL for this node and then add them, that looks like a KCL equation. But it applies to a supernode. Imagine the node defined by the black box. And think about the net currents into or out of the black node. This current I2, which leaves the red node, enters the green node, but doesn't go through the surface of the black node at all. That's exactly the current that subtracted out when we added the red equation to the green equation. Does that make sense? So KCL says, oh, if all of the currents at a node have to sum to 0, and if elements have the same current coming out and going in, then if you draw a box around an element, what goes into the element is the same as what comes out of the element. It doesn't change the net current through the surface. So the generalization of the KCL equation, KCL says the sum of the currents into a node is 0. The generalization says, take any closed path in a circuit, the sum of the currents going across that closed path is 0. So if we apply that rule again, think about node 3. If we add the result of node 3 to the black node, which was the sum of 1 and 2, we get the new green curve. We get the new green equation. And what that says is the sum of the currents going across the green supernode. So what's going on? The i1's coming out of it. i4's coming out of it. i5's coming out of it. So the sum of i1, 4, and 5 has to be 0. So the generalization of KCL is that, well, KCL says the sum of the currents coming out of a node must be 0. The superKCL says the sum of the currents coming out of any closed region is also 0. But the interesting thing about this closed region is that it encloses all but one of the nodes. That's always true. Regardless of the system, regardless of the circuit, you can always draw a line that will isolate one node from all the others. So what that proves is that you can always write KCL for this node in terms of KCL for those nodes. OK? So there's a generalization then that says that you can always write KCL for every node. They will always be linearly dependent, so you can always throw away one. So in some sense now, we're done. We've just finished circuit theory. We talked about how every element has to have a law. A resistor is Ohm's law. A voltage source says that the voltage across the terminals is always a constant. A current source says that the current through the current source is always a constant. So every element tells you one law. We know how to think about KVL, so we know the rule for how the across variables behave. What's the aggregate behavior of all the across variables? Well, KVL has to be satisfied for every possible loop. The loops don't have to be independent. You have to worry about whether they're independent. The only simple rule we came up with, we'll come up with another one in a moment, the only simple rule that we came up with was for planar circuits, where the innermost loops were linearly independent of each other. And you have to write KCL for all the nodes, except one. One of them never matters. So in some sense, we're done. What we would do to solve the circuit, think about every element, for every element assign a reference voltage. For every element assign a current. Make sure they go in the right direction. We always define currents to go down the potential gradient. They always go in the direction through the element from the positive to the negative. So for every element assign a current and a voltage, we have six elements, that's 12 unknowns. Now we dig and refine 12 equations. In this particular circuit, we found those 12 equations. There were three KCL equations, one for each of the inner loops. There were three KCL equations, one for each node except one. There were five Ohm's law equations, one for each one of the resistors. There was one source equation for the voltage source. 12 equations, 12 unknowns were done. The only problem is a lot of equations. It's not a very complicated circuit. We've only got six elements. I tried to motivate this in terms of studying networks that had 10 to the 9th elements. This technique is not particularly great at 10 to the 9th. It would probably work. But we would probably be interested in finding simpler ways. So there are simpler ways, you might imagine. And we'll discuss two of them just very briefly. The dumb way that I just talked about is what we call primitive variables, element variables. If you write all the element variables, V1, V2, V3, V4, V5, V6, all the element currents, I1, I2, I3, I4, I5, I6, write all the equations, you can solve it. However, if you're judicious, you can figure out a smaller number of unknowns and a correspondingly smaller number of equations. One method is called the node method. When we're thinking about the individual element, the thing that matters is the voltage across the element. However, that's not the easy way to write the circuit equations. A much easier way is not to tell me the voltage across an element, but instead tell me the voltage associated with each of the nodes. If I tell you the voltage associated with every node, the important thing about that way of defining the variables is that you're guaranteed that from those variables you can tell me the voltage across every part. So for example, in this circuit, this voltage source, so if I call this one ground, we'll always have a magic node called ground. It is not special in the least. It's just the reference voltage. I'll come back to that. I'll say words in a minute about what the reference is. We always get to declare one node to be ground. We get one free node. It's a node whose voltage we don't care about, because it's the reference for all voltages. It's a node whose current we don't care about, because we get to throw away one node when we do current equations. So we have one special node called ground, about which we don't care too much. Except that it's the most important node in the circuit. Except for that, we don't care about it. So this guy's ground. We think about its voltage being 0. Then this voltage supply makes that node be V0. I don't know what that is, so I'll call it E1. And I don't know what that is, so I'll call it E2. So if I tell you the voltage on all of those nodes, ground voltage is 0, the top voltage is V0, the left voltage is E1, the right voltage is E2. From those four numbers, 0 and three non-trivial numbers, you can find all of the component voltages. So for example, the voltage V6, the voltage across R6 is E2 minus E1. The voltage V4, the voltage across the R4 resistor, is E1 minus 0. So if I tell you all the node voltages, you can tell me all of the element voltages. And in general, there's fewer nodes than there are components. OK, that's good. So instead of naming the volts across the elements, we'll name the voltages at the nodes because there's fewer of them. Then all we need to do in the node method is write the minimum number of KCL equations. We know we only have two unknowns, E1 and E2. And it turns out, and you can prove this, but I won't prove it today, it turns out that you need two KCL equations. Two unknowns, E1, E2, two KCL equations. And it turns out those two KCL equations are exactly the KCL equations associated with the two nodes. So the current leaving E1, so KCL at E1, well, there's a current that goes that way. Well, that's the voltage drop in going from E1 to V0, E1 minus V0, divided by R2. That's Ohm's law. So this term represents the current going up that leg, plus the current that goes through this leg, which is E1 minus E2 over R6, plus the current going in that leg, which is E1 minus 0 over R4. The sum of those three currents better be 0. Analogously, the sum of the currents at this node must be 0, and the equation looks virtually the same. Because V0 is known, so it didn't add an unknown. V0 was set by the voltage source. So I have two equations, two unknowns, solved, done. So rather than solving 12 equations and 12 unknowns, I can do it with two equations and two unknowns. That's called the node method. One of the most interesting theories about circuits is that every simplification that you can think about for voltage has an analogous simplification that you can think about in current. That's called duality. We won't do that, because it's kind of complicated. But it's kind of a cute result. If you can think of a simplification that works in voltage, then there is an analogous one, and you can prove it. In fact, you can formally derive what it must have been. So this is a rule for how you can simplify things by thinking about voltages in aggregate, rather than thinking about the element voltages, think about the node voltages. The analogous current law is, rather than thinking about the currents through the elements, the element currents, think about loop currents. OK, that's a little bizarre. So we name the loop, the current that flows in this loop, Ia, the current that flows in this loop, Ib, and the current that flows in this loop, Ic. What on earth is he doing? Well, the element voltages are some linear combination of those loop currents. And in fact, the coefficients in the linear combination are 1 and minus 1. So the element current I4, the current that flows through the R4 resistor, is the sum of Ia coming down minus Ic, which is going up. So there's a way of thinking about each element current as a sum or difference of the loop currents. Everybody get that? So instead of thinking about the individual element currents, I think about the loop currents. And now I need to write three KVL equations. So in the node method, I named the nodes and had to write two KCL equations. Here I named the loop currents, and I have to write three KVL equations, one for each loop. It's completely analogous. If you write out a sentence, what did you do? I assigned a voltage to every node, and I wrote KCL at all the nodes. Then if you turn the word current into the word voltage, the word node into the word loop, you derive this new method. So this says that if I write KVL at the A loop, think about spinning around this loop, as I go up through the voltage source. So I go in the negative terminal here, so that's minus V0. As I go down through this resistor, I have to use Ohm's law, so that's R2 times the down current. Well, the down current is IA down minus IB up. So I went up through here, down through here, now I go down through this one. When I go down through that one, that's the R4. According to Ohm's law, that's R4 times the current through that element. That current, well, it's IA down, and it's IC up. So this is the KVL equation for that loop. I write two more of them, and I end up with three equations and three unknowns. Both the node method and the loop method resulted in a lot fewer equations than the primitives did. I had 12 primitive unknowns, six voltages and six currents. In the node method, I get the number of independent nodes as the number of equations and unknowns, which is less than the number of primitive variables. In the loop method, I have the number of independent loops, which is again smaller. So the idea then is that we have a couple of ways to think about solving circuits. Fundamentally, all we have are the element relationships and the rules for combination. Oh, this is starting to sound like PCAT. Primitives and combinations. So the primitives are, how does the element constrain the voltages and currents? We know three of those, Ohm's law, voltage source, current source, and what are the rules for combination? Well, the currents add to the node, and the voltages add around loops. Just to make sure you've absorbed all that, figure out the current I for this circuit. OK. What's a good way to start? What should I do to start thinking about calculating I? OK, bad way. Assign voltages and currents to everything, four elements. That's four voltages, four currents. That's eight unknowns. Find eight equations, solve. That'll work. Bad way. What's a better way? OK. Where is the theory of vectors? It was on the previous sheet because it was checked itself. KDL for where? Which loop? So do KDL on the left loop? OK, that's good. But you have to tell me how to assign variables. Do you want eight primitive variables? Eight primitive variables are V1, I1, V2, I3, V3, I3, V4, I4. So that's what I mean by primitive variables, right? Or element variables is another word for it. What's a better way than using element variables? Yeah? AUDIENCE MEMBER 2. I1 is going around here. So if you do I1 going around here, then I1 is actually I. And if you do I2 going around here, what's I2? So if I think about I2 spinning around this loop, so the sum of I1 and I2 goes through that box. But the only current that goes through this box is, so the suggestion is that I think about, so if I have I1 here, but I know that's I, then I can see immediately since the only current that goes through here, so if I have I1 and I2, that was a very clever idea. If you have I1 and I2, the only current that goes through here is I, so I1 must have been I. The only current that goes over here must have been this guy. So this must be minus 10, right? So I could redo that this way. I could say I've got 10 going that way. That make sense? So now I only have one unknown, which is I. So that's a very clever way of doing it. So what I could do is showed here. I have I going around one loop, and I have 10 going around that loop. That completely specifies all the currents, so now all I need to do is write KVL for these different cases. So if I write KVL for the left loop, then I get going up through here, that's minus 15, and going down through here, going to the right through this guy is 3I. Going down through this guy is 2 times I plus 10. Both of these are going down, so you have to add them. So I get one equation and one unknown, and when I solve it, I get minus 1. That make sense? There's an analogous way you could have done it with one node. You could have said that the circuit has a single node and figured out KCL for that one node. KCL would be the sum of the currents here. There's a current that goes that way, that way, and that way, and again, you end up with one equation and one unknown. Yes? AUDIENCE MEMBER 2. I think you get an A to S. PROFESSOR. Correct. So if I thought about this current going this way, it would be minus 10. If I flip the direction, then it's plus 10. So the loop current has the property that it's the only current through this element, so that has to match. It's one of two currents that go through this element. AUDIENCE MEMBER 2. You said that everything in that loop goes to be 10x. PROFESSOR. This loop is 10x. Yes? So why is the loop in the system, there's the i and the 10? Correct. I'm sorry, I want to have this picture now. So if I'm doing it with loops, I have two loops. The current through this element is just i. The current through this element is just i. The current through this element is just 10. The current through this element, well, the sum of these two currents go through that element. Does that make sense? Yes. The current, you know, if they make that whole loop turn. This loop current is just a fraction of the current in the whole system. So this loop current goes through this element and contributes to this element, but so does that one. OK, if you're still confused, you should try to get it straightened out in one of the software labs or the hardware lab or talk to me after lecture. But the idea is to decompose, in the case of the node voltages, think about the element voltages in terms of differences in the node voltages. In the case of the loop currents, think about the element currents in terms of a sum of loop currents. OK, so the answer is minus 1, regardless of how you do it. OK, the remaining thing I want to do today is think about abstraction. We've talked about the primitives, the resistors, voltage sources and current sources, means of combinations, that's KVL and KCL. Now we want to think about abstraction. And the first abstraction that we'll talk about is how do you think about one element that represents more than one element? This is the same thing that we did when we thought about linear systems, when we did signals and systems. We started with Rs and Ks and pluses. We started with Rs and Ks and pluses, and we made single boxes that had lots of Rs and pluses and gains in them. What was the name of the thing that was inside the box? If we combined lots of Rs, gains and pluses into a single box, what would we call the thing that's in the box? Shout again? System function. So we started with boxes that only had things like Rs in them, but eventually we got boxes that looked like much more complicated things like that. We thought about a system function which was a generalized box that could have lots of Rs or lots of gains or lots of pluses in it. And that was a way of abstracting complicated systems so they looked like simple systems. What we want to do here is the same thing for circuits. We want to have a single element, a single circuit element, that represents many circuit elements. And the simplest case of that is for series and parallel combinations of resistors. It's very simple to think about how if you had two ohms law devices connected in series, you could replace those two with a single resistor. And the voltage current relationships measured at the outside of the box would be the same. That's how we think about an abstraction in circuits. When is it that you can draw a box around the piece of a circuit and think about that as one element? The very simplest case is the series combination of two resistors, same sort of thing happens for the parallel combination. And that simple abstraction makes some things very easy. What would be the equivalent resistance for a complicated system like that? Well, that's easy. All you need to do is think about successively reducing the pieces. Here I'm thinking about that having four resistors. I can just successively apply series and parallel in order to reduce that, make it less complicated. So I can think about combining these two in series to get, instead of two 1-ohm resistors, one 2-ohm resistor. Then I can think about these two 2-ohm resistors being equivalently one parallel 1-ohm resistor. And so this whole thing looks as though it's just 2 ohms from the outside world. That's what we mean by an abstraction. What we're trying to do, and what we will do over the next two weeks, is we'll think about ways of combining circuits so that we can reduce the complexity this way. Another convenient way of thinking about reducing the work that you need to do is to think about common patterns that result, PCAP, primitives, combinations, abstractions. So the series and parallel idea was an abstraction. Here's a common pattern. If you've got two resistors in series, if the same current flows through two resistors, then there's a way of very simply calculating the voltage that falls across each. So you can think about the sum resistor, R1 plus R2, since they're in series. So that allows you then to compute the current from the voltage. Then the voltage that falls across this guy is, by Ohm's law, just the current times its resistor, which is like that, and similarly with this one. So you can see that some fraction of this voltage, V, occurs across the V1 terminal, and some different fraction appears across the V2 terminal, such that some of the fractions is, of course, V. That's what has to happen for the two. And there's a proportional drop. The bigger R1, the bigger is the proportion of the voltage that falls across R1. So it's a simple way of thinking about how voltage drops across two resistors. There's a completely analogous way of thinking about how current splits between two resistors. Here the result looks virtually the same, except it has kind of the unintuitive property that most of the current goes through the resistor that is the smallest. So you get a bigger current in I1 in proportion to the R2. So it works very much like the voltage case, except that it has this inversion in it, that the current likes to go through the smaller resistor. So last problem. Using those kinds of ideas, think about how you could compute the voltage V0 and determine what's the answer. So what's the easy way to think about this answer? What do I do first? So superposition, that's one thing. How simple is that? Simplify. What's a good simplification? Collapse. You can put the two, one in a series and you can create a new one. You can treat this as a series combination. And you can replace the series of one and three with a four. So this can be replaced by four resistors. How about that? So you can replace the parallel of a six and a 12 with a four. Amazingly, a four. So there's a four there and there's a four there. And the answer is half of whatever was by voltage divider relationship. So you think about this becoming that. You think about the parallel becoming that. You get a simple divide by two voltage divider. So the answer is 7 and 1 half, which was the middle answer. And so what we did today was basically a whirlwind tour of the theory of circuits. And the goal for the rest of the week is to go to the lab and do the same sort of thing with practical, where you build a circuit and try to use some of these ideas to understand what it does."
    },
    {
        "Rec 2 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-abW4cppRABM.mp3": " Hi. I'd like to talk to you today about inheritance as a fundamental concept in object-oriented programming, its use in Python, and also tips and tricks for using inheritance in Python in object-oriented programming in the object-oriented programming paradigm in 6.01. First thing I'm going to do is give a really quick crash course on inheritance to catch you up to speed and also so that you're clear on what I mean when I say something like parent class. And also I'm going to address the sort of little nuances of using inheritance while programming in an object-oriented fashion in Python. A lot of the code in 6.01 uses the object-oriented programming paradigm, and a lot of the code in 6.01 will inherit from parent classes, so this is part of the motivation for going through a quick review and then also indicating the most common slip-ups and also the most significant things that you may or may not have seen from other languages. All right. First, let's crash course on inheritance. Let's look at the board. This is the idea that you can arrange objects in a hierarchical fashion such that very generalized or basic things that are true of a whole group of objects can be specified at a higher level and then you can work your way down to progressively more specific levels. The most formal encounter you've probably had with this thing is that that approach is the biological taxonomy, right? Kingdom phylum class order family genus species. Every species has all the properties of that particular genus. All the genuses of a particular family have the properties of that family and so on and so forth. That's a very concrete example, but I find it a little boring, so I'm going to talk about dog breeds instead. You're probably familiar with the fact that golden retrievers are a type of retriever and that retrievers are a particular kind of dog. You can make generalizations about goldens based on what you know about dogs, right? All dogs bark. All dogs have four legs if they aren't injured or have some sort of congenital defect, that kind of thing. Goldens also have all the properties of retrievers, right? They are capable of going and catching a game that you've either shot or possibly chased it down and bring it back to you. So they're bred to have very particular properties. Goldens are also bred to have very particular properties. Those that are very specific to goldens define the difference between a golden versus a retriever in the general sense. Likewise, when we want to make objects that have very particular properties but also share general properties with other objects, we're going to create a new category of object and put the specifics in that very specific category and then take the things that we can generalize and put them in more general categories so we don't end up rewriting a lot of code or we end up reusing code but not copying and pasting it everywhere because that's annoying. The other major advantage of using inheritance is that code is more intuitive. You can make references to the same piece of code all over the place but it's not as intuitively accessible to do that over and over and over again, right? It's really convenient to think of the fact that golden could be a subclass or subtype of retriever and that retriever could be a subclass or subtype of dog. When I talk about this relationship in terms of object oriented programming, when I talk about these categories in terms of object oriented programming and when you're actually looking at code, goldens are a subclass or child class of retrievers and retrievers are a parent class or superclass of goldens. Likewise, dogs are a parent class of retrievers. So now I've defined my terminology and also hopefully given you a very, very, very quick review of inheritance. Now I'm going to talk about the specifics in Python. If I turn over here, I've written up a very short class definition for a dog, right? Every dog has the class attribute cry. Every dog has an initialization method that gives every dog a very specific name that is passed in when you initialize the dog. And every dog has access to the class method greeting, which returns a string that says I'm whatever the name of the dog is and also the specific cry, which in this case is actually the class cry. If you're unfamiliar with using the plus in terms of strings, it's just a concatenator. So play around with that in idle if you're confused. I would recommend copying all of this into idle and then playing around with a particular instantiation of dogs, in this case Lassie. If you look at lassie.name, you'll end up going after self.name, which is specified when you initialize the object. So Lassie's name is Lassie. Likewise, if you were to type in lassie.greeting, open paren, close paren, and hit Enter, you should get a string return that says I'm lassie, bark. Mostly this is to familiarize you with the object-oriented programming in Python in the general sense. Now we're going to look at what happens when you want to set up a subclass. If I set up class retriever and I want to inherit from the superclass dog, I'm going to pass in dog in the same syntax that I would use if it were a function and I wanted to pass in a parameter. If I wanted to inherit from multiple things or multiple classes, I would put multiple classes here. Right now, we're just going to inherit from dog. Note that I have no code here. This is pretty much meant to explicitly specify the fact that retriever is not actually going to introduce any new properties to dogs. Their types are going to be different. So if I create something that's a retriever, it will be of object type retriever, versus if I create something and say dog, open paren, close paren, it's going to be of type dog. But what happens when I create a retriever, and as an aside, if you know who Benji is, I know he's not a retriever, but bear with me here. If I create a retriever, it's first going to look for an initialization and any other methods or attributes in the retriever class definition. Run any code that's here, and then go to the parent class and run all the code here. So even though retriever did not have any explicit code underneath it, I can still interact with the object Benji the same way that I interacted with the object Lassie. It has all the same methods and all the same attributes. So there's basic inheritance. And I will make another aside that if you're doing this, you probably don't need to create a subclass in the first place. If you're designing your own code and you're trying to think about what the best way to organize things is, if you have to create a subtype or a subclass and there are no new methods or attributes or no different ways of addressing those methods or attributes, then this category is probably actually just this category. You may want to make a difference so that you can do interesting things with type checking. I think that's the only thing I can think of that would justify it. And I might be wrong, Python gurus out there should correct me. But a thing to keep in mind. So we've done the first half of our inheritance. We're going to inherit one more time and create a class for golden retrievers. Once again, I've got my class definition and my indication that I'm going to inherit from retriever. I don't have any initialization or attribute assignments. I only have a definition for greeting. So what happens here? Well, the first thing we always do is look for an initialization method. Golden doesn't have one, so it's going to check the retriever class. Golden retriever doesn't have one, so it's going to check the dog class. The initialization method is here. So when it runs the initialization method, it's going to run this code. The first thing that's going to happen is any code or any attribute assignments or method assignments here are going to be considered the canon or the first thing that any golden is going to reference. So greeting is going to be executed before greeting used in any other place. You notice the only difference between this greeting and the dog greeting is that oh hi has been prepended to the phrase. And the way that we end up doing that is we refer to, we concatenate and then refer to the superclass. And once again, we have to pass in the explicit argument self when we're talking about a class definition. Later, when you actually instantiate an object and use your friends, you're not going to have to put self as an argument. It'll get confused. We'll go over that in a second. So let's say I create a golden retriever, Sydney. I'm going to pass in one argument, which is the name. We're going to consider all the definitions here first, which means that goldens are going to have a method for greeting that is specified here. It's going to use the method for greeting from retriever. And we could put in anything here. We could put dog.greeting. We could put in some other function that is in the same environment as class golden. But here we can explicitly access the superclass that we defined here. We're going to head over to retriever to see if there are any additional methods or attributes that are a consequence of being a subclass of retriever that we need to add to our definition. And we just hit the pass. On the other hand, retriever inherits from dog. So once again, we have to jump over to a superclass and grab any attributes or methods that are defined there as well. So all the way back over to Sydney. When I call Sydney.greeting, the first thing that happens is that I look in the most specific subclass or whatever my object type is and see if there's a definition for the method. Because there is, I'm not going to use dog.greeting. I'm going to use golden.greeting. Golden.greeting says return a string that says oh hi and also append it to whatever retriever.greeting returns. I go over to retriever. It's not here, but I still have a reference to dog. I go over to dog. It has a method for greeting. And it says I'm Sydney.bark. So the final return type should be oh hi, I'm Sydney.bark. This concludes my basic overview of inheritance of object-oriented programming in Python for 6.01. Next time, I'll review some interesting features in Python that are actually originated in earlier languages and also particular things in aliasing that people that are new to Python or people that are new to programming find especially confusing."
    },
    {
        "Rec 8 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-QleELaAfTd4.mp3": " Last time, we introduced poles. And in particular, we introduced how to move from the manipulation of feed-forward and feedback systems and the geometric sequence that fell out into using the base of that geometric sequence to attempt to predict the long-term behavior of the system. When we're solving for poles and we're only interested in long-term behavior, one of the easiest ways to do so is to solve for the roots of z, where z is a substitution for 1 over r in the denominator of the system function. Once we've done that, we have a list of poles. From that list of poles, we would like to select the dominant pole, or the pole with the greatest magnitude. And then based on the magnitude and period of that pole, we can determine what the long-term behavior of our system looks like. Today, I'd like to mention to you some notable things about poles. If you are interested in this information or feedback and controls in the general sense, I highly recommend 6.003. But here's some information you should at least be aware of as a consequence of 6.01. The other thing I would like to do is just walk through a couple of pole problems to familiarize you or get you more comfortable with the idea of solving for the poles of a system function, or looking at the unit sample response of a system function, and then graphing the poles. The first thing that I want to mention is pole-zero cancellation. And what do I mean when I say that? I mean that if both the numerator and the denominator have a degree of r in them, then you're going to have both a zero and a pole. If the zero and the pole have the same value associated with them, you may be tempted to cancel them out. Unless both the zero and the pole are equal to zero, don't do it. The reason why is that when you get to an implementation of a real system, it is highly unlikely that both the zero and the pole will be implemented to a degree of accuracy that you will actually see those two things cancel out. The only exception to this is when both the pole and the zero are equal to zero, or in this case. This you should feel free to convert to this. And almost any other situation, don't factor. The other thing I want to talk about is repeated roots. If you have a repeated root, you'll have repeated poles. This does get tricky when you're talking about how to add the unit response of those poles. But the long-term behavior of your system is going to look the same. So if both of these poles are the dominant pole, then the characteristics of both, which are the same, are going to determine what your long-term behavior looks like. If they're not, then the dominant pole is going to determine what your long-term behavior looks like. The last thing I want to mention is superposition. So far we've only talked about the unit sample response of a system function and how we use poles to determine what the long-term behavior of our system's going to be. We can look at the response to more complicated inputs than the unit sample response, or the delta. In fact, one of the things we'll probably end up looking at at some point is the step function. The thing that you need to know to go from talking about unit sample response to any other sort of response is that we're still working with an LTI system. What that means is if you take the summation of your inputs, apply the system function, and apply the system function to that summation, it is the same as the output that would result from inputting all those values at once. The best way I would like to explain it is by referring again to if your function was a system function, the same property applies. Now let's walk through a pole problem. Here I have a second order system set up. We've got 2 degrees of r. I have feedback. And I can solve for an expression of y in terms of x. In fact, let's do that right now. y is the result of the summation or a linear combination of x plus delayed signal of y scaled by 1.6 in a linear combination with a delayed value of the delayed value of y scaled by negative 0.63. There's my first degree. For consistency's sake, there's my second degree. Let's first solve for the system function. If you're confused, I recommend doing the algebra from here to this expression. You should get this fraction out. Our second step is to solve for the roots of z. Remember that z is equal to 1 over r in the denominator of the system function. In this case, we'll be working with. All I've done there is taken every degree of r, substitute it in for 1 over z, and then multiply it out so that I'm not working with z in the denominator anymore. I'm actually just working with everything in the numerator. If I fold this back out, I get this expression. And my poles are going to be 0.7 and 0.9. All right. Based on my poles, what are the properties of the unit sample response in the long term? First thing I'm going to do is look for the dominant pole among the poles that I found. In this case, I don't even have to worry about finding the length of the distance from the origin for poles in the complex plane. All I have to worry about is the magnitude of poles on the real axis. 0.9 is my dominant pole because it's the largest pole. 0.9 is less than 1, so I'm going to end up with convergence. Eventually, my system is going to converge, or tend towards 0. The other interesting property of my system is what is this period and how does that relate to what my function is going to look like. In this case, we're only working on the positive real axis. So the angle associated with graphing this pole on the complex plane is 0. So there is no period for our system. This means that our system is going to converge monotonically. Now let's walk through some unit sample responses and then graph through the poles that generated those unit sample responses on the unit circle, where this is the complex plane. Let's look at this graph first. The first thing that I notice about this graph is that, like in the previous example, we have monotonic convergence. We're tending towards 0, and we're not alternating or oscillating about the x-axis. So I know I'm going to be working somewhere along this line before the edge of the unit circle, because at the edge of the unit circle, the distance from the origin is equal to 1. If you made me guess, then I would look at the distance here and compare it to the distance at the next time step. I realize this is blackboard. It's not entirely to scale. But for the purposes of this dimension, I'm going to use entirely to scale, but for the purposes of this demonstration, I'd like to say that the signal at this time step is 0.5 the signal from the previous time step. Likewise, at the next time step, I would like to say that this signal is 0.5 the signal from the previous time step, and so on and so forth. Therefore, I'm going to graph my pole right here. Let's take a look at this graph. I've drawn these squiggles to indicate that the unit sample response exceeds the bounds of the space that I gave for this graph. So just assume that these values are much larger than I've drawn them. The first thing that I notice about this unit sample response graph is the fact that not only am I increasing in a way that does not seem to change in any way, we're going to end up diverging, is that I'm actually alternating about the x-axis. And particularly, that if I were to call this an oscillation, then I would say it's an oscillation with period 2. This means that I'm working with a negative real pole. The fact that I'm diverging means that I'm working with a negative real pole that is greater than 1, or has magnitude greater than 1. If you had to make the guess, I would look at the distance associated with this time step. Compare it to the distance associated with this time step. And if you had to ask me, I would say this is about 1.3, the value at the previous time step. Likewise, if I were to look at the next time step, I would say that this increase is about 30% of the previous value. I'm not even going to try that one. But what I'm trying to get at is that you can use comparisons of previous and future time steps in order to attempt to determine the magnitude of your dominant pole if you're working with a first order system. If you're working with a second order system, then it's possible that you'll see some really interesting initialization effects. And you should probably ask one of us, what's up? But for this example, we're going to put our pole over here. Here's the last graph I want to talk about. The first thing that I notice is that it doesn't seem to be diverging, but it doesn't really seem to be converging either. If this is the case, then I'm going to put it on the unit circle. The second thing that I notice is that it's not monotonic, and it's not alternating. This is oscillating. So in order to determine what angle I'm going to assign to my unit sample response, I'm going to count out the time steps that it takes to cycle through an entire period, and then from there, figure out what the angle would have to be in order to determine a period of that length. So if I start here, I'm just going to count 1, 2, 3, 4, 5, 6, 7, 8 to complete one full oscillation. This means that my period is 8. If I have to divide 2 pi by a particular angle in order to get out 8, I want to divide by pi over 4. So at this point, I'm working with a magnitude of about 1, and I want this angle to be about pi over 4. This concludes my tutorial on solving poles. Next time, we'll end up talking about circuits."
    },
    {
        "Rec 14 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-SpS3ud58yTI.mp3": " Last time we talked about probability as an introduction on how to model uncertainty. State estimation is one of the ways that we can deal with uncertainty in our system. We can take a model that we don't completely understand and attempt to infer information about it based on the things that we can observe about that particular system. In particular, we're going to look at a set of observations and actions that either our system takes or that we observe about the system and that we take on that particular system. If we continue this process for multiple time steps, then we can continue to attempt to learn things about the particular system. The process of completing that behavior over multiple time steps while making those inferences is what we refer to when we talk about state estimation. First off, state estimation is a process that's completed as a consequence of wanting to understand a stochastic state machine. State estimation itself is not a stochastic state machine. State estimation attempts to take a stochastic state machine, make a model of that stochastic state machine, and then run state estimation on it iteratively to attempt to figure out, or recursively, an attempt to figure out what's going on inside that stochastic state machine. When you build a stochastic state machine model, there are three components that need to be specified. The first is the starting distribution over states. For instance, let's say that I believe that I'm sick, and I'm trying to figure out what it is that I'm sick with. And I could be sick with three things, as far as I'm concerned. I could be sick with strep, or I could be sick with some other more boring virus, or I could be sick with mononucleosis. The starting distribution refers to my starting belief as to those systems. And if I'm generically sick, then in the general sense, one of the assumptions that's frequently made with respect to starting distributions is that they're uniform. It could be equally any of these things. The second thing you need to specify when you're talking about modeling a stochastic state machine is your observation distribution. Or what is the likelihood associated with making a particular observation given that you're in a current state? For instance, if I had mononucleosis, how likely would it be that I'd observe a bunch of white, ugly patches on the back of my throat? Or if I had strep, what is the likelihood associated with that kind of thing? Typically, this observation variable is factored into a couple different phenomena. In the sick example, the best thing to talk about is symptoms. Am I lethargic? Do I have the white spots on the back of my throat? Do I have a fever? That sort of thing. The last thing that you need to specify when you're talking about modeling a stochastic state machine is your transition distribution. You assume that your state machine is going to change over time, or it is likely that I will get more or less sick. And there are things that I can do to induce that kind of change, or there are things that I can do that effectively model the passage of time. Your actions for a stochastic state machine model can either be actions that the model takes, and you are exclusively doing observations. But one of the particular observations that you do also qualifies as an action, or something that indicates the passage of time. Or actions can be something that you do to a particular state. In the sick example, things I could do to myself to try to make myself feel better, or at least figure out better what is going on, or what caused my distribution to sway towards one particular state. I could take antibiotics, or sleep in and drink a lot of orange juice, or continue my day as normal. Given a particular action, any particular state that you're starting from, your transition distribution tells you the likelihood associated with being in a new particular state. So as a consequence of making those actions, does the distribution of likelihood of a particular illness change? At this point, I'm going to walk through a step of state estimation. Each step of state estimation is the same. In fact, if you complete multiple steps based on the information that you gained from the previous step, that's referred to as recursive state estimation. And I'll keep walking through the sick example. So when you're doing state estimation, you're trying to figure out something about a system that you cannot perfectly model. For instance, either your own immune system, or your own susceptibility to a particular disease. And you have all the components that you have for your stochastic state machine model. As a consequence of the passage of time, or as a consequence of making an observation and either observing an action taken by your stochastic state machine, or performing an action upon your stochastic state machine, you're going to make a new estimation of what you believe the current state of that unknown system, or system that is not completely observable to you. So you're going to make a new estimate of your belief of the state of that system. In short, you're going to solve for the probability distribution over S of t plus 1. There are two steps. The first step is referred to as the Bayesian reasoning step, and it involves performing Bayes' evidence or Bayes' rule upon the current state distribution given a particular observation. So at this point, I've made some sort of observation about myself, if I'm talking about the sick model. I spent all day coughing, or I have a fever, or my throat is sore, or I feel extremely lethargic. Given that observation, I can take the P of O given S from my observation distribution, multiply it by my current understanding of the state distribution, and then divide out by P of O. The slowest way to complete this action is to build the joint distribution and then condition on a particular column. It's very proper, but you can save yourself some cycles by doing this. Let's say I started off with the uniform distribution. It could be equally likely that I have strep or a normal virus or a mono. As a consequence of making the observation that I don't have white spots on the back of my throat, I could say, oh, the likelihood of me just having a normal virus is higher, and the likelihood of me having either strep throat or mono is lower. So this step takes P of S and multiplies it by P of O given S. Once I have these values, I have to scope back out to the universe, or I have to normalize these values such that they sum to 1. That's where I get my P S of T given O. At this point, I've accounted for the observation that I've made, but I haven't accounted for the action on the system. That's the next step. We're going to take our results of Bayesian reasoning, which are sometimes referred to as B prime S of T, and take the action and find the distribution over states as a consequence of a single time step or a single iteration of state estimation. The second step is referred to as a transition update. We've got our updated belief. We're going to take our transition distribution or our specification for what happens given that we're in a current state and an action has been taken. At that point, we'll have a probability distribution over the new states. And here are my values from the first step. As an example, let's say that I sleep in and drink a lot of orange juice. As a consequence of sleeping in and drinking a lot of orange juice, there's some amount of likelihood that I will either continue to be sick with strep, or it's possible that I actually have just a normal virus. If I have a normal virus and I sleep in and drink a lot of orange juice, this causality sounds backwards, but it's as a consequence of not being able to make perfect observations on the system. If I have an amount of belief that says that I think I have strep and I sleep in and drink a lot of orange juice, then it's equally likely that I will have either strep or a normal virus after completing that step. It doesn't differentiate between the two. If I'm sick with a virus and I sleep in and drink a lot of orange juice, then the state that I'm going to encourage myself to be in is I have a virus. If I have mono and I sleep in and drink a lot of orange juice, then there's some likelihood on the next day that I will still be in a state that looks like I have mono, but there's also some likelihood associated with it that I will be in some state that looks like I have strep. That's what happens when you run the transition update. When you run the transition update, you end up accumulating all the probabilities associated with being in a particular new state as a consequence of being in a particular previous state and entering that new state based on the transition distribution. Once you accumulate all these values, you end up with your new distribution over a new state. This represents one step of state estimation. If I wanted to run multiple, I would take the value that I got here for s of t plus 1, replace it in the value for s of t, and run the same process of Bayesian reasoning and transition update. This concludes my review of state estimation. Next time, we'll talk about search."
    },
    {
        "Lec 1 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-3S4cNfl0YF0.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. MIT.edu. Hello and welcome to 6.01. I'm Danny Freeman. I'm the lecturer. One thing you should know about today is that there's a single handout. You should have picked it up on your way in. It's available at either of the two doors. What I want to do today in this first lecture is mostly focus on content. But before I do that, since 6.01 is a little bit of an unusual course, I want to give you a little bit of an overview and tell you a little bit about the administration of the course. 6.01 is mostly about modes of reasoning. What we would like you to get out of this course is ways to think about engineering. We want to talk about how do you design, how do you build, how do you construct, how do you debug complicated systems. That's what engineers do. And we're very good at it. And we want to make you very good at it. We're very good at it. And you know that from your common everyday experience. Laptops are incredible. As we go through the course, you're going to see that laptops incorporate things from the tiniest, tiniest level, things so small that you can't see them. They're microscopic. The individual transistors are not things that you can see. We develop special tools for you even to be able to visualize them. And yet we conglomerate billions of them into a system that works relatively reliable. Now I realize I'm going out on a limb, because when you say things like that, then things always fail. But I'll go out on a limb and say, for the most part, the systems that we construct are very reliable. We'd like you to know how you think about making such a complicated system and making it reliable. We want to tell you about how you would model things. How do you gain insight? How do you get predictability? How do you figure out how something will work before you've built it? If you're limited to try out how things work by actually constructing it, you spend a lot of time constructing things that never make it. We want to avoid that by, where we can, making a model, analyzing the model, making a prediction from the model, and using that prediction to build a better system on the first try. We want to tell you about how to augment the physical behavior of a system by putting computation in it. That's a very powerful technique that is increasingly common in anything from a microwave to a refrigerator. We'd like you to know the principles by which you do that, and we'd like you to be able to build systems that are robust to failure. That's a newer idea. It's something that people are very good at. If we try to do something and we make a mistake, we know how to fix it, and often the fix works. We're less good at doing that in constructing artificial systems, in engineering systems, and we'd like to talk about principles by which we can do that. So the goal of 6.01 is then really to convey a distinct perspective about how we engineer systems. Now having said that, this is not a philosophy course. We are not going to make lists of things to do if you want it to be robust. We're going to learn to do things by actually making systems. This is an introductory engineering course, and so you're going to build things. The idea is going to be that in constructing those things, we've rigged the exercises so that some of those important themes become transparent. So the idea is this is introductory engineering. You'll all make things. You'll all get things to work. And in the process of doing that, learn something about the bigger view of how quality engineering happens. So despite the fact that we're really about modes of reasoning, that will be grounded in content. We selected the content very broadly from across EECS. EECS is an enormous endeavor. We can't possibly introduce everything about EECS in one subject. That's ridiculous. However, we wanted to give you a sense of the variety of tasks that you can apply the same techniques to. So we want to introduce modes of reasoning and then show you explicitly how you can use those modes of reasoning in a variety of contexts. So we've chosen four, and we've organized the course around four modules. First module is software engineering, then signals and systems, then circuits, then probability and planning. Even so, even having chosen just four out of the vast number of things we could have chosen, there's no way we can tell you adequately. We can't give you an adequate introduction to any of those things either. What we've chosen to do instead is focus on key concepts represented by the asterisks. The idea is going to be we choose one or two things and really focus on those deeply so you get a thorough understanding, not only of how that fits within, for example, the context of software engineering, but also how that concept ramifies into other areas. Notice that I tried to choose the stars so they hit multiple circles. That's what we're trying to do. We're trying to not only introduce an idea to you, but also show you how it connects to other ideas. So the idea then is to focus on a few, we hope very well chosen, applications that will demonstrate a variety of powerful techniques. Our mantra, the way we intend to go about teaching this stuff, is practice, theory, practice. There's an enormous educational literature that says, whether you like it or not, people learn better when they're doing things. You have a lot of experience with that. You have a lot of experience on the other side too. I'll try to forget the other side, or at least try to wipe it from your brain momentarily, to focus on your more fundamental modes of learning. When you were a kid and you were learning your first language, you didn't learn all the rules of grammar first. You didn't learn all the letters of the alphabet first. You didn't learn about conjugating verbs first. You learned a little bit about language. You started to use it. You ran into problems. You learned a little more about language. You learned to go from words like feed me to higher level concepts, like hey, what's for dinner? So the idea is that you learned it in an iterative process, where you learned some stuff, tried it out, learned some more stuff, tried it out, and it built up. There's an enormous literature in education that says that's exactly how we always learn everything. And so that's the way this course is focused. What we will do is, for example, for today, we'll learn a little bit about software engineering. Then we'll do two lab sessions where you actually try to use the things that we talk about. Then we'll come back to lecture and we'll have some more theory about how you would do programming. And then you go back to the lab and do some more stuff. And the hope is that by this tangible context, you'll have a deeper appreciation of the ideas that we're trying to convey. So let me tell you a little bit about the four modules that we've chosen. The course is going to be organized on four modules. Each module will take about one fourth of the course. First thing we'll look at is software engineering. As I said, we don't have time to focus on or even survey all of the big ideas in software engineering. It's far too big. So we're going to focus narrowly on one or two things. We'd like you to know about abstraction and modularity because that's such an important idea in the construction of big systems. So that's going to be our focus. We'll begin in today's lecture, we'll begin talking about modularity and abstraction at the small scale. How does it affect the things you type as instructions to a computer? But by next week, we're going to be talking about a whole bigger scale. By next week, we're going to talk about constructing software modules at a much higher level. In particular, we'll talk about something that we'll call a state machine. A state machine is a thing that works in steps. On every step, the state machine gets a new input. Then based on that input and its memory of what's come before, the state machine decides to do something. It generates an output. And then the process repeats. We will see that that kind of an abstraction, state machines, there's a way to think about state machines that is compositional, that you can think of as a hierarchy just as you can think of low-level hierarchies within a language. I'll say a lot more about that today. So the idea will be that once you've composed a state machine, you'll be able to join two state machines and have its behavior look just like one state machine. That's a way to get a more complicated behavior by constructing two simpler behaviors. That's what we want. We want to learn tools that lets us compose complex behaviors out of simple behaviors. And the tangible model of that will be the robot. We will see how to write a program that controls a robot as a state machine. That's certainly not the only way you could control a robot. And it's probably not the way you would first think of it if you took one course in programming and somebody said to you, go program the robot to do something. What we will see is that it's a very powerful way to think about it for exactly this reason of modularity. The bigger point that we will make in thinking about this first module is the idea of how do you make systems modular, how do you use abstraction to simplify the design task. And in particular, we will focus on something that we'll call PCAP. When you think about a system, we will always think about it in terms of what are the primitives, how do you combine them, how do you abstract a bigger behavior from those smaller behaviors, and what are the patterns that are important to capture. So the bigger point is this idea of PCAP, which we will then revisit in every subsequent module. Second module is on signals and systems. That's also an enormous area. So we only have time to do one thing. The thing that we will do is we will think about discrete time feedback. How do you make a system that's cognizant of what it's done so that it, in the future, can do things with awareness of how it got there? A good example is robotic steering. So the idea is going to be, OK, think about what you do when you're driving a car, and think about how you would tell a robot to do that same thing. Here's a naive driving algorithm. I don't recommend it, but it's widely used in Boston, apparently. I find myself to the right of where I would like to be. So what should I do? Turn left. I'm still to the right of where I'd like to be. What should I do? Turn left. Oh, I'm exactly where I should be. What should I do? Go straight ahead. Oh, that's a bad idea. And what we'll see is that perfectly innocent-looking algorithms can have horrendous performance. What we'll do is try to make an abstraction of that. We'll try to make a model. We'll try to capture that in math so that we don't need to build it to see the bad behavior. We'll make a model. We'll use the model to predict that that algorithm stinks. But more importantly, we'll use the model to figure out an algorithm that will work better. In fact, we'll even be able to come up with bounds on how well such a controller could possibly work. So the focus in this module is going to be, how do you make a model to predict behavior? How do you analyze the model so that you can design a better system? And then how do you use the model and the analysis to make a well-behaved system? The third module is on circuits. Again, circuits is huge. We don't have time to talk about all of circuits. We'll do very simple things. We'll focus our attention on how you would add a sensory capability to an already complicated system. The idea is going to be to start with a robot. I guess this is brighter. Start with our robots and design a head for the robot. The robot comes from the factory with sonar sensors. The sonar sensors are these things. There's eight of them. They tell you how far away something that reflects the ultrasonic wave is. As they come from the factory, the robots can't sense light. What you'll do is add light sensors. The goal is to make a system to modify the robot so that the robot tracks light. That's a very simple goal. And the way we'll do that is to augment the robot with a simple sensor here, showed a little more magnified here. The idea is that this is a LEGO motor. The LEGO motor will turn this relative to the attachment. That's the robot head's neck. So the robot will be able to do this. And the robot will have eyes. These are photo sensors, photo resistors actually. So the idea is going to be that there's information available in those sensors for figuring out where a light is so that you could track it. Your job will be to build a circuit, that's this thing, that connects via cables, these red cables and yellow cables, connects via cables over to this head. We'll give you the head. Your job will be to make the circuit that converts the signal from the photo resistor, which is in proportion to light, and figures out how to turn the motor to get the head to face the light. And then ship that information down to the robot to let the robot turn its wheels to get the body. So it's kind of like the light comes on right over here, the robot looks at it and says, oh yeah, that's where I want to be. So that's the idea in the third module, is to incorporate new sensing capabilities into the robot. The final module is on probability and planning. And the idea there is to learn about how you make systems that are robust to uncertainty, and that can implement complicated plans that they too are robust to uncertainty. So there's a number of things that we will do, including creating maps of spaces that the robot doesn't understand, telling the robot how to localize itself, how if it woke up suddenly in an environment it could figure out where it is. How to make a plan. And as an example, I'll show you the kind of system that we will construct. Here the idea is that we have a robot. The robot knows where it is. Imagine there's a GPS in it. There isn't, but imagine there is. So the robot knows where it is, and it knows where it wants to go. That's the star. But it has no idea what kind of obstacles are in the way. So if you were a robotic driver in Boston, you know that you started out at home, and you want to end up in MIT. But there's these annoying obstacles. They're called people that you should in principle at least miss. OK, so that's kind of the idea. So I know where I am. I'm the robot. I know where I am. I know where I want to be, and I'm going to summarize that information here. Where I am is purple. Where I want to be is gold. And I have a plan that's blue. My plan's very simple. I don't know anything about anything other than I'm in Waltham and I want to go to Cambridge. So blast east. So I imagine that the best way to do there is a straight line. OK, so now what I'm going to do is turn on the robot. The robot has now made one step. And I told you before about these sonar sensors. From the sonar sensors, the robot has learned now that there seems to be something reflecting at each of these black dots. It got a reflection from the black dots, from the sonar sensors. That means there's probably a wall there, or a person, or something that in principle I should avoid. And the red dots represent, OK, the obstacle is so close I really can't get there. So I'm excluded from the red spots because I'm too big. The black spots seem to be an obstacle. The red spots seem to be where I can't fit. I'd still want to go from where I am, purple, to where I want to be, gold. So what I do is I compute a new plan. OK, then I start to take a step along that plan. And as I'm stepping along, OK, so now I think that I can't go from where I started over to here. I have to go around this wall that I didn't know about initially. So now I just start driving. It looks fine, right? I'm getting there. So now I know I can go straight down here. Oh, wait a minute. There's another wall. OK, what do I do now? So as the robot goes along, it didn't know when it started what kinds of obstacles it would encounter. But as it's driving, it learned, oh, that didn't work. Start over. So the idea is that this robot is executing a very complicated plan. The plan has, in fact, many subplans. And the subplans all involve uncertainty. It didn't know where the walls were when it started. And when it's all done, it's going to have figured out where the walls were and, provided there's a way, presumably find the way to negotiate the maze and get to the destination. So the idea then is that if you were asked to write a conventional kind of program for solving that, it might be kind of hard because of the number of contingencies involved. What we will do is break down the problem and figure out simple and elegant ways to deal not only with uncertainty, but how do you make complex plans. So as I said, our primary pedagogy is going to be practice, theory, practice. And so that ramifies in how the course is organized. So this is a quick map of some of the aspects of the course. So we'll have weekly lectures. It's lecture unintensive. In total, there's only 13 lectures. We'll meet once a week here for lecture. There's readings. There's voluminous readings. There's readings about every topic that we will talk about. And the readings were specifically designed for this course. I highly recommend that you become familiar with the readings. If you have a question after lecture, it's probably there. It's probably explained. We will do online tutor problems. We sent you an email if you pre-registered for the course so you may already know about this. The idea is going to be that there's ways that you can prepare for the course by doing computer exercises. And we will also use those same kinds of exercises in all of the class sessions. We will have two kinds of lab experiences. Besides lecture, the other two events that you have to attend are a software lab and a design lab. That's the practice part. So after you learn a little bit about the theory by going to lecture, by doing the reading, then you go to the lab and try some things out. We call the first lab a software lab. It's a short lab. It's an hour and a half. You work individually. You try things out. You write little programs. Courseware can check the program to see if it's OK. And primarily, the exercises in the software lab are due during the software lab. But on occasion, there will be extra things due a day or two later. The due dates are very clearly written in the tutor exercises. Once a week, there's a design lab. That's a three-hour session in which you work with a partner. The reason for the partner is that the intent, the difference between the design labs and the software labs, is that the design labs ask you to solve slightly more open-ended questions, the kind of question that you might have no clue what we're asking. Open-ended, the kind of thing that you will be asked to do after you graduate. Design the system. What do you mean, design the system? So the idea is that working with a partner will give you a second immediate source of help and a little more confidence if neither of you knows the solution, so that you raise your hand and say, I don't have a clue what's going on here. So the idea is that once a week we do a software lab individually. Once a week we do a design lab a little more open-ended with partners. There's a little bit of written homework. For total, it's not much compared to other subjects. It's mostly practice. There's a nano quiz just to help you keep pace, to make sure that you don't get too far behind. The first 15 minutes of every design lab starts with a nano quiz. The nano quiz are intended to be simple if you've caught up, if you're up to date. So the idea is that you go to design lab. The first thing you do is a little 15 minute nano quiz. The nano quiz uses a tutor, much like the homework tutor, much like the Python tutor. And it's intended to be simple. But it does mean, please get to the design lab on time. The nano quizzes are administered by the software. It starts when the design lab starts. It times out 15 minutes later. So if you come 10 minutes late, you'll have five minutes to do something that we plan to give you 15 minutes for. We will also have exams and interviews. The interviews are intended to be a one-on-one conversation about how the labs went. And we will have two midterms and a final. So that's kind of the logistics. The idea behind the logistics is practice theory practice. Come to the labs, try things out, make sure you understand, develop a little code, type it in, see if it works. If it works, you're on top of things. You're ready to get the next batch of information from the lecture and readings. OK, let's go on and let's talk about the technical material in the first module of the course, in the software module. We kick the course off talking about software engineering for two reasons. We'd like you to know about software engineering. It's an incredibly important part of our department. It's an incredibly important part of the engineering of absolutely any system, any modern system. But we'd also like you to know about it because it provides a very convenient way to think about it. It's a convenient language to think about the design issues, the engineering issues, and all the other parts of the class. So it's a very good place to start. So what I will do today is talk about some of the very simplest ideas about abstraction and modularity in what I think of as the lowest level of granularity. How do you think about abstraction and modularity at the micro scale, at the individual lines of code scale? As I said earlier, we will, as we progress, look at modularity and abstraction at the higher scale. But we have to start somewhere, and we're going to start by thinking about how do you think about abstraction and modularity at the micro scale? Special note about programming. So what we are trying to do is in the first two weeks, ramp everybody up to some level of software security where you feel comfortable. So the first two weeks of this course is intended to make you comfortable with programming. We don't assume you've done extensive programming before. We want you to become comfortable that you're not behind, and that's the focus of the first two weeks exercises. If you have little or no previous background, if you are uncomfortable, please do the Python tutor exercises. If you do not have a lot of experience programming, if you're uncomfortable with the expectation that you can do programming, do that first. That takes priority over all the other assignments during the first two weeks. In particular, if you're uncomfortable, we will run a special Python help session on Sunday. And if you attend that, you can get a free extension. The idea is completing the tutor exercises is intended to make you feel comfortable that you have the software background to finish the rest of the course. Do that first. We will forgive falling behind in other things so that you feel comfortable with programming. If at the end of two weeks, you still feel uncomfortable we have a deal with 600, the Python programming class, that they will allow you to switch your registration from 601 to 600. But that expires Valentine's Day. So you have to make up your mind before Valentine's Day if you'd like to use that option. So the idea is to make up your mind before Valentine's Day if you'd like to use that option. The idea is we'd like you to be comfortable with programming. If you haven't programmed before, do the Python tutor exercises, go to software lab, go to design lab, but work on the tutor exercises. The staff will help you with them. You can go to office hours. There's office hours listed on the home page. You should try to become comfortable. And you should try to set as your goal, I'm going to be comfortable before Valentine's Day. And if you're not, talk to a staff member about that. OK, so what do I want you to know about programming? Well, we're going to use Python. We selected Python because it's very simple and because it lets us illustrate some very important ideas in software engineering in a very simple context. That's the reason. One of the reasons that it's simple is that it's an interpreter. After some initialization, the behavior of Python is to fall into an interpreter loop. The interpreter loop is ask the user what he would like me to do, read what the user types, figure out what they're talking about, and print the result, repeat. Very simple. What that means is that you can learn by doing. That's one of the points of today's software lab. You can simply walk up to a computer, type the word Python, what you type is in red. Type the word Python. It will prompt you, so this is Chevron, that says, I'd like you to tell me something to do. I have nothing to do. If you type 2, Python tries to interpret that. In this particular case, Python says, oh, I see. That's a primitive data item. That's an integer. This person wants me to understand an integer. And so it will echo 2, indicating that it thinks you wanted to understand a simple integer. Similarly, if you type 5.7, it says, oh, I got that. That's a float. The person wants me to remember a floating point number, and it will similarly echo the float. Now, of course, the floats don't have a, so there's no exact representation for floats. There's too many of them. There's a lot of them. There's even more floats than there are ints. So it has an approximation. So it will print its approximation to the float that it thinks you are interested in. If you type a string, hello, it'll say, oh, primitive data structure, string. And it'll print out that string. So the idea is, one of the features of Python that makes it easy to learn is the fact that it's interpreter-based. You can play around, you can learn by doing. Now, of course, if the only thing it did was simple data structures, it would not be very useful. So the next more complex thing that it can do is think about combinations. If you type 2 plus 3, it says, oh, I got it. This person's interested in a combination. I should combine by the plus operator two ints, 2 and 3. Oh, and if I do that, if I combine by the plus operator 2 and 3, I'll get 5. So it prints 5. So that's a way you know that it interprets 2 plus 3 as 5. Similarly here, except I've mixed types, 5.7 plus 3, it says, oh, this user wants me to apply the plus operator on a float and an int. OK, well, I'll upgrade the int to a float. I'll do the float version, and I'll get this, which is its representation of 8.7. So the idea is that it will first try to interpret what you're saying as a simple data type. If that works, it prints the result to tell you what it thinks is going on. It then will try to interpret it as an expression. And sometimes the expressions won't make sense. In particular, if you try to add an int to a string, it's going to say, huh? And over the course of the first two weeks, we hope that you get familiar with interpreting this kind of mess. That's Python's attempt to tell you what it was trying to do on your behalf and can't figure out what you're talking about. OK, so that was simple, but it already illustrates something that's very important. And that's the idea of a composition. So the way Python works, the fact that when you added 3 to 2, it came out 5. What we were doing was composing complicated, well, potentially complicated, that was pretty simple, potentially complicated expressions and reducing them to a single data structure. And so that means that in some sense, this operation 3 times 8 can be thought of as exactly the same as if the user had typed in 24. Whenever you can substitute for a complex expression a simpler thing, we say that the system is compositional. That's a very powerful idea. Even though it's simple, it's a very powerful idea. And it's an idea that you all know. You've seen it before in algebra, in arithmetic. So in arithmetic expressions, you can think about how the sum of two integers is an int. That's a closure, that's a kind of a combination that makes the system compositional, and that provides a layer of hierarchical thinking. So that in your head, even though it says 3 times 8, you don't need to remember that anymore. You can say, oh, for any purposes that follow, I might just as well think of 3 times 8 as being a single integer, 24. It's part of many other kinds of systems. For example, natural language. The simplest example in natural language is that you can think about apples are good as snacks. Apples is a noun. It's a plural noun. Or you could substitute apples and oranges, and it makes complete sense within that same structure. So apples and oranges are good as snacks. The combination of apples and oranges works in every way from the point of view of the grammar, in the same way that a simple noun, apples, worked. What we would like to do is use that idea as the starting point for a more general compositional system. And a good way to think about that is by way of names. What if we had some sequence of operations that we think is particularly important, so that we would like to somehow canonize that, so that subsequently we can use that sequence of operations easily? Python provides a very simple way to do it. Every programming language does. It's not unique to Python. But the idea is, so here's an example. 2 times 2, I'm squaring 2 and get 4. 3 times 3, I'm squaring 3 and I'm getting 9. 8 plus 4 times 8 plus 4, I'm squaring 8 plus 4. 8 plus 4, well, I can think of that as 12. I'm squaring 12, I'm getting 144. The thing I'm trying to illustrate there is the notion of squaring. Squaring is a sequence of operations that I would like to be able to canonize as a single entity, so that in subsequent programs, I can think of the squaring operation as a single operation. Just like I think of times. The way we say that in Python is define square of x to be return x squared. Then, having made that definition, I can say square of 6 and the answer is 36. OK, this is a very small step, but it illustrates a very important point. The idea being that Python provides a compositional facility. And it's hierarchical. Having defined square, I can use square just as though it were a primitive operator. And I can use square to define higher level operations. So for example, what if I were interested in doing lots of sums of squares? Say I'm Pythagoreas. So I might want to add the square of 2 and the square of 4 to get 20, or the square of 3 with the square of 4 to get 25. Using that simple idea of composition, we can write a new program, sum of squares. Sum of squares takes two arguments, x and y, and it returns the square of x and the square of y. Sum of squares doesn't care about how you compute the square. It trusts that square knows how to do that. So the work is smaller. The idea is that square takes care of squaring single numbers. Sum of squares doesn't have to know how to square numbers. It just needs to know how to make a sum of squares. So what we've done is we've broken a task, which was not very complicated, but the whole idea is hierarchical. We've taken a problem and broken it into two pieces. We factored the problem into how do you do a square and how do you sum squares. And the idea then is that this hierarchical structure is a way of building complex systems out of simpler parts. So that's the idea of how you would build programs that are compositional. Python also provides a utility for making data structures that are compositional. The most primitive is a list. So in Python, you can specify a list. Here's a list of integers. So the list says beginning list, end of list, elements of list. So there's five elements in the list. The integer is 1, 2, 3, 4, 5. Python doesn't care what the elements of a list are. We'll see in a minute that that's really important. But for the time being, the simplest thing that you can imagine is a heterogeneous list. It's not critical that the list contain just integers. Here's a list that contains an int, a string, an int, and a string. Python doesn't care. It's a list that has four elements. The first element's an int. The second element's a string, et cetera. Here's an even more complex example. Here's a list of lists. How many elements are in that list? Three. How many elements are in that list? So the idea is that you can build more complex data structures out of simple ones. That's the idea of compositional factoring applied to data. Just like it was important when we were thinking about procedures to associate names with procedures, that's what DEF did, we can also think about associating names with data structures. And that's what we use something that Python calls a variable for. So I can say b is 3, and that associates the data item 3 with the label b. I can say x is 5 times 2.2. Python will figure out what I mean by the expression on the right. It'll figure out that I'm composing by using the star operator, which is multiply, an integer and a float, which will give me a float. The answer to that's going to be a floating point number. And it will assign a label x to that floating point number. You can have a more complicated list, a data structure, and associate the name y with it. Then having associated the name y, you get many of the same benefits of associating a name with a data structure that we got previously and associating a name with an operation. So we can say y of 0, and what that means is, what's the 0th element of the data structure y? So the 0th element of the data structure y is a list, 1, 2, 3. Python has some funky notations. The minus 1 element is the last one. So the minus 1 element of y is 7, 8, 9. And it's completely hierarchical. If I ask for the minus 1 element of y, I get 7, 8, 9. But then if I ask for the first element of that result, I get 8. Everything's clear? Just make sure everything's clear. I want to ask you a question. But to kick off the idea of working together, I'd like you to think about this question with your neighbor. So before thinking about this question, everybody stand up. Introduce yourself to your neighbor. All right. So now I'd like you to each discuss with your neighbor the list that is best represented by which of the following figures? 1, 2, 3, 4, or 5? None of the above. And in 30 seconds, I'm going to ask everybody to raise a hand with a number of fingers indicating the right answer. You're allowed to talk. That's the whole point of having a partner. That's the whole point of having a partner. OK. I'd like everybody now to raise their hand, put up the number of fingers that show the answer, and I want to tally. Fantastic. Everybody gets it. OK. So which one do you like? 3. Why do you like 3? Somebody explain this to me. It just looks good. It's pattern recognition. What's good about 3? Compositional. What is the compositional element in the pictures? What represents what? OK, a represents a. That's pretty easy, right? So that takes care of the bulk of the figures. What's the blue lines represent? Someone else? I don't quite understand. The angles represent a list. They represent a list. Where is the list on the figures? The vertex. The vertices are lists. So in 3, at the highest level, we have a list that's composed of how many elements? 2. The first element of that list is? And the second element of that list is? Another list. That's the hierarchical part. That second list has how many elements? 5. Reverse. You got it. What is the list represented by number 2? A single list with 5 elements. Square bracket, a comma b comma c comma d comma e. Square bracket. What is the list represented by that one? It's not a list. What is it? Who knows? You have a variable. A is a final list that contains b and a variable c that contains b and c. So we could make that a variable. If we said a is a variable that comprises b and c, then we have the problem of how we're going to associate variables and elements into this list. So the weird thing about this one and, let's see, that one's weird. This one's also kind of weird. This one's weird because we're giving names to lists in a fashion that's not showed up here. That's not to say you couldn't invent a meaning. It's just that it doesn't map very well to that representation. Similarly over here, we seem to be giving the name b to the element a, and then the name c to the element b. It's not clear what we're doing there either. So the point is to get you thinking about the abstract representation of lists and how that maps into a complex data structure. That was the whole point. So we've talked about, then, four things so far. How do you think about operations in a hierarchical fashion? And the idea was composition. We think about composing simple operations to make bigger compound operations. That's a way of saying there's this set of operations that I want to call foo. So every time I do this complicated thing that has three pages of code, that's one foo. And that's a way that we can then combine foos in some other horribly complicated way to make big foos. So the idea is composition. That's the first idea. The second is associating a name with that composition. That's what def does, define name, name of a subroutine. So we thought about composing operations, associating names with them. We compose data in terms of lists, and we associated names with those lists in terms of variables. The next thing we want to think about is a higher order construct where we would like to conglomerate into one data structure, both data and procedures. Python has a concept called a class that lets us do that. In Python, you make a new class by saying to the Python prompt, I want a new class called student. And then under student, there is this thing, which we will call an attribute, an attribute to a class is simply a data item associated with the class, and a method. A method is just a procedure that is associated with a class. So there's this single item class called student that has one piece of data, its attribute school, and one procedure, which is the method calculate final grade. So then this is the kind of data structure you might imagine that a registrar would have. It's a way to associate. So the idea here is that everybody here is a student. They all have a school, and they all have a way of calculating their final grade. That's a very narrow view that maybe a registrar would have. So classes, having defined them, we can then use the class to define an instance. So an instance is a data structure that inherits all of the structure from the class, but also provides a mechanism for having specific data associated with the instance. So in Python, I say Mary is a student. By mentioning the name of the class and putting parentheses on it, I say give me an instance of the student. So now Mary is a name associated with an instance of the class student. John is similarly an instance of the class student. So both Mary and John have schools. In fact, they're both the same. The school of Mary and the school of John are both MIT. But I can extend the instance of Mary to include a new attribute, the section number, so that Mary's section number is three and John's section number is four. So this provides a way, it's a higher order concept. We thought of a way to aggregate operations into complicated operations, data into complicated data. Classes aggregate data and operations. Classes allow us to create a structure and then generate instances, and then the instances have access to those features that were defined in the class, but also have the ability to define their own unique attributes and methods. You can also use a class to define a subclass. So here I'm defining the subclass student 601. All student 601s are members of the class student. The reverse is not true. So all student 601 entities inherit everything that a student has, but all 601 students share some other things besides having a school which all students have. 601 students also have a lecture day, a lecture time, and a method for calculating tutor scores. Not all students have a method for calculating tutor scores, but members of the class student 601 do. So this, again, represents a way of organizing data and operations in a way that makes it easier to compose bigger, more complex structures. The final thing that I want to talk about today is the specific gory details for how Python manages the association between names and entities. We've already seen two of those. Naming operations is via def, and it gives rise to the name of a procedure. Variables are ways of naming data structures. Now we've seen a way of naming classes. And in fact, it's helpful if you understand. So Python associates names and entities in a very simple, straightforward fashion. And if you know the ground rules, it makes it very easy to deal with. And if you don't know the ground rules, it makes it very hard to deal with. So what's the ground rules? Here's the gory details. So Python associates names with values in what Python calls a binding environment. An environment is just a list that associates a name and an entity. So if you were to type b equals 3, what Python is actually doing is it's building this environment. When you type b equals 3, it adds to the environment a name b, and associates that name with the integer 3. When you type x equals 2.2, it adds a name x and associates it with the float 2.2. When you say foo is minus 506 times 2, it makes the name foo and associates it with an int, minus 10, 12. Then if you ask Python about b, the rule is look it up in the environment and type the thing that b refers to. So when you type b, what Python really does is it goes to the environment. It says, do I have some entity called b? Well, yes I do. It happens to be an int 3, so it prints 3. If you ask what is a, Python says, OK, in my environment, do I have some name a? It doesn't find it, so it prints out this cryptic message that basically says, sorry guys, I can't find something called a in the current environment. That's the key to the way Python does all name bindings. So in general, there's a global environment. You start typing to Python. It just starts adding and modifying the bindings in the binding environment. So if you type a equals 3 and then type a, it'll find 3. If you then type b equals a plus 2, it evaluates the right-hand side relative to the current environment. So it first looks here, and it says, do I have something called a? Yes. It's an integer 3. Substitute that. Do I know what 2 is? Oh yeah, that's just an int. Do I know what plus is? Oh yeah, that's the thing that combines 2 ints. So it decides that a plus 2, it evaluates a plus 2 in the current environment. It gets 5. And it says, oh, I'm trying to do a new equals, a new association, a new variable. Make the name b point to this evaluated in the current environment. So b gets associated with the int 5. Then if I do this line, it evaluates b plus 1 in the current environment. b is 5 in the current environment. It adds 1. It gets 6. And then it says, associate this thing, 6, with b. So it overwrites the b, which had been bound to 5. And b is now bound to 6. So the whole thing, the way it treats variables, the way Python associates a name with a value in a variable is evaluate the right-hand side according to the current environment, then change the current environment to reflect the new binding. What it does in the case of subroutines is very similar. So here's an illustration of the local environment that is generated by this piece of code. When I say a equals 2, it generates a name in the local environment, a. It evaluates the right-hand side and finds 2. So it makes a binding in the local environment where the name a is associated with the integer 2. Then I say define square of x to be return x squared. That's more complicated. Python says, oh, I'm defining a new operation. It's a procedure. The procedure has a formal argument x. It has a body return x times x. I'm going to have to remember all of that stuff. So I'm trying to define a new procedure called square. It's going to make a binding for square. So in the future, if somebody says the word square, it'll find out, oh, square, I remember that one. Square, it's a procedure. Just like the binding for a variable might be an int, the binding for a procedure is the name of the procedure. Then in the procedure, which is some other data structure outside the environment, it's got to remember the formal parameters, in this case x, and the body. And for the purpose of resolving what do the variables mean, it needs to remember what was the binding environment in which this subroutine was defined. So that's this arrow. So this sequence says, make a new binding square, points to a procedure. The procedure has the formal argument x. It has the body return x times x. And it has the binding, it came from the environment, e1, the current environment. OK, is everybody clear? So the idea is that the environment associates names with things. The thing could be a data item, or it could be a procedure. Then when you call a procedure, it makes a new environment. So what happens then when I try to evaluate a form, square of a plus 2? What Python does is it says, OK, I need to figure out what square is. So it looks it up in the environment, and it finds out that square is a procedure. Fine, I know how to deal with procedures. So then it figures out this procedure has a formal argument x. Oh, OK, if I'm going to run this procedure, I'm going to have to know what x means. So Python makes a new environment, here it's labeled e2, separate from the global environment, e1. It makes a new environment that will associate x with something. Doesn't know what it is yet. It just knows that this square is a procedure that takes a formal argument x. So Python makes a new environment, e2, with x pointing to something. Then Python evaluates the argument a plus 2 in the environment e1. You called square of a plus 2 in the environment e1, so it figures out what did you mean by a plus 3. Well, you were in the environment e1, so it means whatever a plus 3 would have meant if he had just typed a plus 3 in that environment. So you evaluate a plus 3 in the environment e1, and you get 5. So then this new environment, e2, that is set up for this procedure, square, associates 5 with x. Now it's ready to run the body. So now it runs this procedure, return x times x. But now when it's trying to resolve its variables, it looks it up in e2. So it says, I want to do the procedure, the body, x times x. I need to know what x is, and I need to know it twice. Look up what x means. But I will look it up in my e2 environment that was built specifically for this procedure. And fortunately, there's an x there. So it finds out that x is 5. It multiplies 5 times 5. It gets the answer is 25. It returns 25. And then it destroys this environment e2, because it was only necessary for the time when it was running the procedure body. Is that clear? OK, so a slightly more difficult example illustrates what happens whenever everything is not defined in the current local environment. What if I type define biz of a? Well, I create a new name in the local environment that points to a procedure. The procedure has a formal parameter a and a body that returns a plus b. The procedure also was defined within the environment e1, which I'll keep track of. Then if I say b equals x, that makes a new binding in the global environment, b equals x. Then if I try to run biz of 2, look up biz. Oh, that's a procedure. Formal parameter a, make an environment, has an a in it. What should I put in a? Evaluate the argument 2. OK, a is 2. Put 2 here. Now I'm ready to run the body. Run the body in the environment e2. When I run return a plus b in e2, I have to first figure out a. Well, that's easy. a is 2. Then I have to figure out b. What's b? 6. How do you get 6? So this local environment that was created for the formal parameter has as its parent e1, because that's where the procedure was defined. So it doesn't find b in this local environment, so it goes to the parent. Do you have a b? And it could, in principle, propagate up a chain of environments. So you could construct this hierarchically. So it will resolve bindings in the most recent environment that has that binding. So the answer then is that when you run biz of 2, this b gets associated with that b. OK? So that's how the environments work for simple procedures and simple data structures. It's very similar for the way it works with classes. So imagine that I had this data, and I wanted to represent that in Python. What I might do is look at the common features. The courses are all the same. The rooms are all the same. The buildings are all the same. The ages are highly variable. So I might want to create a class that has the common data. So I might do this. Class staff, 601. The course is 601. The building is 34. The room is this. The way Python implements a class is as an environment. Executing this set of statements builds the class environment. This is it. It's a list of bindings. Here I'm binding the name course to the string, 601, et cetera. If there were a method, I would do the same thing, except it would look like a procedure then. So this creates the staff, 601 environment. Staff, 601, because I executed this class statement, that creates a binding in the local environment, staff, 601, which points to the new environment. So now in the future, when Python encounters the name staff, 601, it will discover that that's an environment. Python implements classes as environments. So now when I want to access elements within a class, I use a special notation. It's a dot notation. Python regards dots as ways of navigating an environment. When Python parses staff point room, it looks up staff, 601 in the current environment. If it finds an environment, it then says, oh, I know about this dot room thing. All I do is I look up the room name in the environment, staff, 601. And when it does that, it gets the answer, 501. And the same sort of thing happens here. It looks up staff, 601. It finds an environment. It looks up coolness. It finds out there is no such thing. Well, no, that's not true. So it creates coolness within 601 and assigns an integer 11 to it. So then the way Python treats methods is completely analogous. Instances. So I'm doing instances first. If I make Pat be an instance of staff, 601, Pat is an instance of the class, staff, 601. Pat is implemented as an environment. So when I make Pat, Pat points to a new environment, here E3. The parent of E3 is the class that Pat belongs to, which is here E2. And when I make the instance, it's empty. But now if I ask, what is Pat.course? Well, Pat points to this environment. Does this environment have something called a course? No. Does the parent? Yes. Course is bound to the string 601, so Pat.course is 601. Just the same as staff601.course had been 601. Pat is an instance. It's a new environment with the class environment as its parent. You can add attributes to instances. You can add attributes to instances. And all that does is populate the environment associated with the instance. You can add methods to classes. And that does the same thing. So here I've got the class staff601, which has a method salutation and instance variables, course, building, and room. So when I build that structure, staff601 points to an environment that contains salutation, which is a procedure, in addition to a bunch of instance variables. So now all the rules that we've talked about with regard to environments apply now to this class. So in particular, I can say staff601 salutation of Pat. When Python parses staff601, it finds an environment. It says dot salutation. Oh, I know how to do that. Within the environment staff601, look for a binding for the name salutation. Do I find one? Well, yeah, there it is. It points to a procedure. So staff.salutation is a procedure. Do just the same things that we would have done with a simple procedure. The only difference here is that the procedure came from a class. In this particular case, the subroutine that I define has a formal parameter self. So then that's going to have to build, when I try to evaluate it, that has to build a binding for self, which is set to Pat. Pat was an environment, so self gets pointed to Pat. So now when I run staff.601 salutation on Pat, it behaves as though that generic method was applied to the instance Pat. We'll do that a lot. It's a little bit of redundancy. We know that Pat is a member of staff.601. So we will define a special form, or I should say Python defines a special form that makes that easier to say. This is the way we will usually say the instance Pat should run the class method salutation on itself. This is simply a simplified notation that means precisely that. OK. So what we covered today then was supposed to be the most elementary ideas in how you construct modular programs, modularity at the small scale. How do you make operations that are hierarchical, data structures, and classes? What we will do for the rest of the week is practice those activities."
    },
    {
        "Lec 2 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-cQntMUMQyRw.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. MIT.edu. Hello. Welcome to the second lecture in 6.01. I hope you had a good time last week. Last lecture, we looked at what is probably the most important theme in this course, which is how do you organize your thoughts, your design, the things that you do in order to manage complexity when you're trying to build a complicated system. The mantra for this class is PCAP. Primitives, combinations, abstractions, and patterns. And last time, we saw how at the very most elementary level, Python provides some tools with which we can achieve this goal of defining primitives, combining them into more complicated ideas, abstracting the important information, and generating and capturing new patterns. And so for example, we saw that Python has a defined statement that lets you associate a sequence of operations with a name. Both of those things are important. The sequence represents a combination. The name represents a way that we can abstract the behavior of the combination and treat it as though it were a primitive operation. We saw that we could do the same sort of thing for data structures. And in particular, the list structure in Python allows us to generate hierarchical heterogeneous structures in much the same way. Then variables allow us to associate names with those structures. And finally, we saw that classes allow us to combine not only data, but also procedures all into one object of related things. So that's PCAP at the most primitive level. What I want to do today is talk about PCAP at higher levels. How do you build upon that framework to continue this idea of building with abstraction and modularity? How do you combine primitive operations into powerful operations? How do you combine primitive data into powerful data? And what I want to do is think about the next level above the most rudimentary level in Python. So we'll look first at some programming styles and how that affects your ability to define useful abstractions. And then I'll look at something much higher level, which is state machines, which is the way that we will think about the construction of robot control. So I'll start with just a few words about the different ways that you could structure a program. The reason for doing this is that the basic structure that you use can have an important effect on your ability to abstract. We'll look at three different methodologies for constructing a program. I'll refer to the first one as imperative, also procedural. That's kind of the most basic way you could think about writing a program. It's kind of a recipe. It's kind of like cooking with a recipe. Take this, add this, stir this, bake for 30 minutes, that kind of thing. So if you define a procedure, if you organize the way you think about it in terms of step-by-step instructions for what should I do next, we refer to that kind of an approach as imperative. We'll look at functional programming. There, even though you implement precisely the same algorithm, the perspective is a little bit different. There, rather than focusing quite so narrowly on the step-by-step how do you get from A to B, the idea is going to be think about the structure of the problem in terms of functions in a mathematical sense. By which I mean functions that eat inputs, generate outputs, and don't have side effects. Side effects are things like setting variables that you can later look at. Then I'll look at object-oriented programming. Again, you could be implementing precisely the same algorithm by using an object-oriented approach. But here, the focus will be on building collections of data with procedures that are related and organizing the solution to your problem in terms of a hierarchy of such structures. So what I'd like to start off today with is to look at an example problem and how you could program it using any of these three approaches. So the example program is going to be find a sequence of operations by which I mean an operation is either increment or square. The idea is that the operations are things that we will do to integers. Find a sequence of operations, either increment or square, that transforms one integer, which is the initial value i, into a goal, which I'll call g. So I want to think about the problem of finding such sequences. So for example, the sequence increment, increment, increment, square, when applied to 1 would give 16. So I'm thinking about the first increment, increments 1 to give you 2. The second increment, increments 2 to give you 3. The third increment, increments 3 to give you 4. Then square, squares 4 to give you 16. So I'll refer to this as having found the sequence of operations, increment, increment, increment, square, that transforms 1 into 16. Everybody with me? OK, I'll be the judge of that. So to prove that you're with me, what's the minimum length sequence of increment and square operations needed to transform 1 into 100? You've got 30 seconds. Discuss it with your neighbor. Come to agreement. And I'm going to ask you to raise your hands with a number of fingers, 1, 2, 3, 4, or 5, indicating the right answer. OK. OK. Everybody raise your hand. Tell me a number of fingers equal to the right answer. Raise your hand. Show me a number of fingers. OK. So keep in mind the number of fingers is the thing before the colon. That avoids the awkward way of saying less than 4. So I want the number before the colon. So what is the minimum length sequence? Raise your hand, indicate a number of fingers. OK, the answers are improving. Higher so I can see you. OK, it's about 90% correct, I think. OK, most people said 3, which is another name for 5. OK. 4, no, no. OK, so how do you get the answer 5? What's the answer? Somebody explain that answer? Yeah. So increment, increment square, increment square? Since I have two operators, increment and square, and since I'm trying to cover a big distance, 1 to 100, square increases faster, at least for bigger numbers, than increment does. So what you'd like to do is figure out a way of coercing as many squares as possible. So a good thing to do is to start at the end. And you can take the square root of the first one evenly, and that gives you 10. But then you can't take the square root of 10, so you back off and you get 9. And then you can take the square root of 9, et cetera. So there's any number of ways you could solve this problem. The point is that there's a simple solution, which is the answer 3, which is a pseudonym for 5. OK, so 5 operations will get you from 1 to 100. OK, so what I want to do now, now that you know the answer to the question, I want to write a program to find that answer. The most straightforward approach that you could use is what we would call imperative or procedural. The idea is to solve the problem by walking your way through it in some premeditated fashion. A reasonable premeditated fashion would be, think about the sequences and order them by length. Think about all the sequences of length 1, see if they work. Think about all the sequences of length 2, see if they work. 3, see if they work. And just keep going until you find one that works. That's a very reasonable procedure. Start by thinking about short sequences and proceed by making them longer and longer until you run into one that happens to solve your problem. So that's what's going on here. First I give a name to the operator increment, and I give a name to the operator square. That's just for convenience. Then what I want to do is write a program, find sequence, that will start at some initial value, say 1, and go to some goal, say 100. And the way I'll do that is to enumerate over the lengths of sequences that are possible, 1, 2, 3, et cetera. I'll represent each one of those sequences of operations by this kind of a representation. I'll make a tuple that has a string that tells me in English what did I do, and an integer that tells me what the answer is after I've done that. So looking ahead, this is the idea. This is the program that I'm trying to construct. I would like the output of the program to start by thinking about all sequences of length 1, then sequences of length 2, then sequences of length 3, then sequences of length 4. For each sequence of length 1, I'd like to think about what are all the possible sequences. Well, I could start with 1 and increment it to get 2, or I could start with 1 and I could square it to get 1. That's all the possible sequences of length 1. Then I go on to length 2, 3, 4. By the time I'm down here to length 4, I could start with 1, increment, square, square, increment, and that would give me 17. That's the structure of the program that I'm trying to make. Everybody's with it? I'm going to define find sequence. I'm going to loop over all of those lengths. I'm going to keep track of all the different sequences I found as a list of tuples. After last week, you're supposed to be very comfortable with those kinds of words. Each one of the sequences is a tuple, a string and a final value, and the list is all possible ones. And I'm going to try the ones of length 1, then I'm going to append to each one to make sequences of 2, and then I'm going to append to that to make sequences of 3, 4, 5, and keep on going. So the point is that this is a very simple-minded, easy to conceive recipe for how to search through a large number of sequences and find the one with the minimum length. So when you write that program, it iterates down until it finds number 5. The 5-length sequence here, it came up with the answer 100, so that's the answer. OK, the point is that it was an easy way to write the program, we just think about telling somebody with a pencil and paper what would you do, and we tell Python, rather than the person with the piece of paper, what to do. The approach is straightforward. The only ugliness is that it ended up with three levels of loops, and the most common kind of error in this kind of programming is you just botch the indices. Because it's got three nested layers of loops, it's very easy to lose track when you're thinking of layer 3 about what's happening in layer 2. So that's the only difficulty in this approach. The challenge is just to keep all the indices consistent. But it works. There's nothing wrong with this approach. It doesn't necessarily lead to the most modular designs, but you can write functional programs that work this way. A different way to structure the program, here's a different version of the same program, the same algorithm, but I'm going to refer to this one as functional program. Here the idea is to focus on procedures that implement functions of a mathematical type. I want to recast the problem this time, thinking about how would I divide it up into functions that calculate things. So rather than focusing on what's the set of nested loops that I have to make, I want to ask what would be a meaningful calculation that I would want to do to solve this problem. So the first thing I might do, focusing on this part, I might write a function apply, where apply, I'm using the methodology of functional programming, apply is a pure function. It's going to eat some inputs, generate an output, and have no side effects. So what I want it to do, I want to feed it a list of functions and ask it what's the answer. So apply is going to have as its first argument a list, but the list is going to be a list of operations. It's not a list of strings. It's not a list of integers. It's a list of functions. And what the procedure apply is going to do is step through that list, applying the functions one by one to argument. So the final goal is written down here. If I apply nothing to 7, the answer ought to be 7. If I apply increment to 7, the answer ought to be 8. If I apply square to 7, the answer ought to be 49. And if I apply increment square to 7, the answer ought to be 64. There's a couple things you're supposed to see here. First off, apply is a pure function. It has no side effects. It eats its input. It does a calculation. It tells you an answer, and it's done. There's no extra things that's going on behind your back. It's not setting a variable or creating a list or doing anything like that. It has inputs. From those inputs, it generates an output. And that's the end of the story. Another thing that you should see is that I'm treating procedures as first class objects. That's another tenet of functional programming. Rather than sending the input to apply as a list of text strings or as a list of integers, I'm giving it a list of function names. I'm treating functions just as though it were any other kind of data. I'm making a list of functions just the same as I could make a list of integers. Or if I made a list of strings, I'm just making a list of functions. So another important feature of this program is the idea that the functions are being treated as first class objects. The next procedure is addLevel. addLevel is going to be a second pure function. The idea there is that I'm going to use addLevel to construct all the different sequences of operations that are possible. Each sequence is going to be a list. So the entire, all possible sequences will be a list of lists. So the idea in addLevel is going to be that you take the list of lists and you generate from that list of lists a new list of lists that has one more element. So if the first list of lists had four elements in it, how many lists are in the second list of lists? Five. Four, five. The first list of lists represents some number of different sequences of operations. My second list of lists, what I want to do is think about all the different ways that could be extended from a length four sequence to a length five sequence. How many ways can you extend a length four sequence to a length five sequence? Eight. So four becomes eight. Why does four become eight? Because there's two possibilities for the way that you could extend it. I could extend the length four sequence by adding on an increment operator, or I could extend it by adding on a square operator. So every time I addLevel, it's going to create a new list of lists with double as many elements in it as the old list of lists. Does that make sense? Other than that, the program works very much the same. I should illustrate that. So addLevel applied to the list of lists increment. I'm only considering one sequence of length one. So how many ways can I change that if I'm willing to add procedures increment or square? Well, I could end up with increment, increment by taking that one and that one, or I could end up with increment square by taking that one and that one. So if I were to extend the sequence increment two possible ways by either incrementing again or by squaring again, I end up with two possibilities represented by this list of two different lists. Is that clear? The important point is that I'm treating functions as any other kind of data element that could be added to a list. And when you run that program, you get an answer that's very similar to the program that we looked at previously, the imperative program. Except that the answer now is a list of functions. So rather than generating a text string like the previous example did, this program generates a list as the answer, this program generates a list, which is a list of functions. The first element in the list is increment. The second element in the list is the function increment. The third element is square, increment square. So it gave you the same answer, it just represented it differently. Here I'm representing the answer as a list of functions. Does that make sense? So there's a number of advantages to this approach. One of them is that it kind of automatically coerces you to think about modules. By specifically focusing my attention on how could I break up that problem by defining functions, I end up with a bunch of functions, here apply and add level. And being functions, they're modules. They are things that are easy to manipulate in other contexts. And in particular, it's very easy to debug them, especially in an interpretive environment like Python. It's very easy to figure out if your program works by figuring out if each module works. That's one of the important features of modular programming. When you have a module, it means that you can use that module in multiple different ways. It's easy to break apart the debugging problem so that rather than trying to debug the one monolithic structure, which was the procedural program that we wrote in the first part, here we can debug the individual components, which happen to be functions. So I can ask, what happens when I apply the empty list to 7? Well, my answer better be 7. So that provides a way of checking. So one of the features of thinking about the algorithm as being broken up into a number of functions is the greater modularity that allows you, for example, easier debugging. A much more important reason for thinking this way, though, is the clarity of thought. And that can be derived from another feature of the definition of apply. The particular way I define apply is what we call recursive. It's recursive in the sense that the definition of apply calls apply. OK, that sounds like a bad idea. How do I define something in terms of itself? The idea is that each application, every time apply calls itself, the idea is to structure the procedure so that the new incarnation is in some sense simpler than the previous one. If you can guarantee that, it will reduce the complexity of the problem from something that you don't know how to solve to something that you do know how to solve. And for that reason, it represents a powerful way to think about structuring programs. So as an example of that, as an example of structuring programs as recursions, think about raising a number to a non-negative integer power. So if I'm trying to raise b to the n, if n is 0 and if n is a non-negative integer, well, if n is 0, b to the n is 1. And if n is a non-negative integer, then b to the n, it can be reduced to b times b to the n minus 1. Rewriting that functionally, if I say that my function b to the n can be represented by f of n, this statement is precisely equivalent to saying that f of n is 1 if n is 0 or b times f of n minus 1 if n is bigger than 0. So the idea then is that I may not know how to raise 2 to the 10th, but I can use the rule to say 2 to the 10th, oh, that must be 2 times 2 to the 9th. Great, I don't know how to do 2 to the 9th either. But 2 to the 9th is 2 times 2 to the 8th. And eventually, I boil it down to a case that I do know the answer to. In particular, b to the 0 is 1. I would express that idea in Python this way. Define exponent of b comma n. If n is 0, return 1. Otherwise, return b times exponent of b n minus 1. So what that does then when you invoke it, invoking exponent of 2 comma 6, in fact, invokes exponent an additional 5 times, 6 times. If I ask Python to evaluate exponent of 2 comma 6, it will end up calling exponent of 2 comma 5. But to evaluate that, it'll call exponent of 2 comma 4. 3, 2, 1, 0. Then finally, it gets to the base case. When it gets to the call exponent of 2, 0, it falls into this case, which returns 1 always. So now, exponent of 2, 0 returns 1. But then that can pick up where call exponent of 2, 1 left off. When I did 2, 1, it fell into this case, where it was trying to take 2 times exponent of 2 comma 0. Now it knows the answer to 2 comma 0 is 1. So I can multiply by 2 and get the answer is 2. And it backs out then all of the other answers. So that's an example of how I could use recursion to reduce a complicated case to a base case, a simple case that I know the answer to. Here, the recursion with a power of n causes a linear increase in the number of invocations. So we would call that a linear process. So the idea then is that recursion is a good way to reduce a complicated problem to a simpler problem. Now that algorithm wasn't particularly fast. If I imagine doing a big number like 1024, raising a number to the 1024 power, that could take a while. How long will it take? 1024. It will make 1024 calls to reduce it to the base case. Here's a way I could speed it up. Here, I'm trying to make something called fast exponentiation, where I'm going to take advantage of another rule for the way exponentiation works. Not only is it true that I can write b to the n as b times b to the n minus 1, but if n happens to be even, there's another rule that I can use. If b is even, then I can raise b to the n over 2 and square the answer. That's more knowledge about the way exponents work. The point is that when I think about the problem recursively, when I think about the problem as a function, there's a natural way to take advantage of that additional information. So what I can do then is implement in Python in a scheme that looks very similar to the one that I used for the simple exponentiation, but it has a new rule embedded in it. So I can say if n is 0, that was my original base case. If n is 0, the answer is 1. If the modulus of n, when divided by 2, is 1, if it's odd, use the old rule, which says b times the exponent of b n minus 1. However, if it's neither 0 nor odd, then use this new rule. So if I use that procedure compared to the previous program exponent, how many invocations of FASTA exponent is generated by a call to FASTA exponent 210? So talk to your neighbor, figure out an answer, raise your hand, use the number before the dot just to keep you alert. Thank you. So how many invocations of FASTA exponent? Is generated when you call it with the arguments 2, 10. Everybody raise your hand, tell me a number of fingers. OK, it's about 90%, something like that. The most common answer is 5. How do you get 5? So how do you think of reducing FASTA exponent of 210? What happens to the first call? AUDIENCE MEMBER 2 AND 3 It goes to L, it goes to N, and then that's even. So it goes to L, it goes to 5, it calls itself again. So the first thing that happens is that it realizes that the 10 is even. So the 10 is not 0, it's not odd, it goes into this case. So it tries to run it with 5. Then when it tries to run FASTA exponent with 5, what happens? So 5 is not 0, so it's not the base case. It is odd. So 5 gets reduced to 4. 4 gets reduced to 2. 2 gets reduced to that, et cetera. So the idea is that we get a faster convergence, because just like in the very first example we did, we can do two different kinds of operations, either decrement or, in this case, half. And half works faster when the numbers are bigger than decrement does. So it's the same idea as in that first program. So the idea then is that this requires 5, where we would have expected in the previous exponent procedure, it would have required 10. And that difference just gets bigger and bigger as we make n bigger and bigger. Much more importantly, though, than the fact that we can make something fast is the idea that this is expressive. The idea is that by structuring the program this way, it was perfectly obvious how to incorporate more knowledge about the way exponents work. I could have done it by writing a procedure. I didn't say that right. I could have done it by using a procedural approach. Do this, then do this, then do this. But then I have very complicated indices. Check if it's squared, check if it's squared, and it's inside the loop. So I complicate my looping structure, where here, because I'm using the functional approach, it's very easy to understand that it just introduces another case. It's nothing complicated. So it's much more important. The reason we think about recursion isn't so much that it's fast, it's because it's expressive. It's a way to incorporate knowledge about an answer into a solution. And that's what we'd like to do. We want to have a way of making complicated things simply. Here's a way to incorporate knowledge about a procedure in a very straightforward fashion. All we have is a second case. Rather than making the loops more complicated, all we have is a simple new case in this statement. And that's especially important when you think about harder problems. Here's a much harder problem. So Tower of Hanoi, you probably played with this as a kid. I want to transfer all the disks from post A to post B. I have a typo on your handout, by the way. I typed C on the handout, and then I wrote the program the other way. That should have been a B. Transfer a stack of disks from post A to post B by moving the disk one at a time, never placing a bigger disk on top of a smaller disk. OK, that's kind of a challenging problem. It's a very challenging problem if you try to structure it as loops. If you try to structure it as loop, I would recommend that you do this gazillions of times until you get the pattern stuck in your head. Because the pattern is not obvious, but the pattern is obvious if you think about it as a recursion. OK, I don't know how to move n from post A to post B. How about this algorithm? Take the top n minus 1, move those to C. Get them out of the way. I don't know how you're going to do that, but just trust me. Just do it. So move n minus 1 of them off the top, put them over on C. That exposes the bottom one. Move the bottom one to B, which is where I'd like it to be, and then by this mysterious process of moving n minus 1, bring the n minus 1 back from C back on top of B. OK, what did I just do? I just started with a problem that I don't know the answer to. How do I move n disks from this post to that post? And I broke it into a sequence of three operations, two of which I don't know how to do, but one of which I know. The one will fortuitously be the base case, or at least similar to the base case. So the idea is, if I want to transfer the n, do two versions of the n minus 1 problem and glue them together with this middle step. OK, well, I don't know how to do the n minus 1 problem either, just recurse. How do I do the n minus 1 one? Well, I'll do the n minus 2 one. I don't know how to do that either. Well, then do the n minus 3 one. Keep going till I get it down to 1, because when I get down to 1, I do know how to move 1. Does that make sense? This is supposed to illustrate that the power of recursion is in thinking. Some algorithms are easy to describe recursively. That set is the set when the hard problem can be reduced to a problem of the same type that is, in some sense, simpler. Here, simpler means taking the index n and turning it into n minus 1. If you write this simple little Python snippet and run it, you get this procedure. And so the procedure means take the top disk off A, put it on B. Take the top disk off A, put it on C. Take the top disk off B, put it on C. If you do that procedure, it will magically do exactly the right thing. If you try to see the pattern here so that you were to write this in a procedural approach, the pattern's very peculiar. But the algorithm for doing it is very simple. So the idea, one of the reasons that we like recursion, is this idea of expressibility. It makes it easy to express what we're trying to do. So that's a quick view of the first two ways to do the first problem that I talked about. We started thinking about a sequence of operations, either increment or square, that transforms an integer to i into g. The first thing we looked at was a procedural approach, where I just gave you a recipe. Do this, do this, do this. The issue there is that I ended up with nested loops three deep. The issue there is keeping track of the indices for all the different loops. We just talked about a functional approach that can have an advantage, especially whenever the functional approach has a very simple implementation. The last approach I want to talk about very briefly is object-oriented approach. Here, I'm doing the same problem again. Find the sequence of operations that transforms i into g, 1 into 100. Here I'm doing the same algorithm again. I haven't really changed the algorithm in going from the procedural approach to the functional approach to here, the object-oriented approach. I have changed the representation of the program. And that's the point. Here, the representation of the program is in terms of objects. Here what I'm going to do is think about organizing all the sequences of operations that are possible in a tree. So the sequences that I could do, I could do no operation followed by increment, increment, increment. Or I could do nothing, increment, increment, square. Or I could do nothing, square, increment, square. So I'm thinking about all the possible sequences. But this time, I'm not thinking about it as a text string. I'm not thinking about it as a list of functions that I need to call. I'm thinking about it as this tree, this tree of objects. And in the object-oriented approach, what I'm going to do is think about the solution in terms of building up the tree until I find the answer. So the key in the object-oriented approach is to think of a useful object. What would I like to remember when I'm constructing the solution as this kind of a tree? Well, if I call every circle a node, I can define a class that will keep track of all the important things about each node. And then I can structure my program as creating nodes, connecting them up into that tree, and looking for the answer. So each one of these nodes, each one of these circles, ought to have a parent. They ought to have an action. And they ought to have an answer. So for example, let's say that I started with the initial value i equals 1. Then this node right here might say, whatever your answer was at this point, the answer here was 1. So the answer here is going to be 1 after it's been incremented, so the answer should be 2. So associated with each one of these nodes is going to be a parent. The parent of this guy is this guy. The parent of this guy is this guy. The parent of this guy is that guy. Associated with each node is going to be an action. When I'm here, my action is to increment. When I'm here, my action is to square. And associated with each node is going to be the answer. So if I started at 1, by the time I get here, the answer is going to be 2, 4, 16. So the idea is that I'm structuring the problem as a tree. Each node has a couple things it wants to remember. It wants to know who its parent was. It wants to know what's the operation associated with this node. And it wants to know the answer at this node. I also associated with each node a method. And the method will be to print out the answer if this node is the answer. So the print routine is going to have to say, OK, I ended up here and that happens to be the answer. Well, how did I get here? Well, I have a parent who has a parent who has a parent who was none. So this routine is supposed to figure out the sequence of operations that led me from the initial value up at the top to the final answer that just happens to be goal. Is that clear? So then I structure the program in terms of building those node structures. I start off by making a node, which is the top node. It has no parent. It has no action. And the answer to the top node is 1. And then I just think through loops that are very similar to the loops that I was using before. They're structured similar to the way they were structured in the case of the imperative program. Except now the iterations are generating new nodes. The iterations are simpler because each one just generates a node. It's not like in the imperative case where I treated separately what was the ASCII string that represented the answer and what was the cumulative answer. Here I don't have two pieces of things that I need to do inside my loop. I just make a node. So the idea is that when I solve the problem using this approach, what I'm trying to do is build a representation for the solution out of pieces that make sense for this problem. OK. So the important idea that I wanted to get across was to think about modularity at the next higher level. We started last time with modularity at the most microscopic level in Python. And we saw how Python had primitives that allowed us to build more complicated structures and simplified our task of understanding complex designs. We looked at the most primitive level in the first lecture. What I just went over was how even within the structure of Python, you can structure programs that are more or less modular. And the important point is that the way you structure the program influences the degree of modularity that you get. So the structure of the program has a significant effect on how modular your solution is. What I want to do for the rest of the time today is jump one higher level yet. OK. So last time, primitives within Python. First half of this hour, imperative programming, functional programming, object-oriented programming approaches that affect the modularity of your solution. Now I want to go up one more level and think about abstraction and modularity at a much higher level. And there, I'm going to think about programs that control the evolution of processes. What's a process? A process is some set of procedures that evolve over time. An example is a bank account. OK. With the problems that we've been thinking about so far, there was a clear answer. What's the minimum length sequence that blah, blah, blah, right? There was a clear answer to that problem. Here, there isn't a clear answer to the problem, what's the answer to the bank account? The bank represents a process. It's something that changes with time. There are transactions. Every time you make a deposit or a withdrawal, you do a transaction. That causes some action to happen in the bank. So there isn't an answer. There's an evolving answer. So those are the kinds of problems I want to think about now. Graphical user interfaces. Again, there's no answer to Excel. You can't say calculate the answer to Excel. So the right answer for Excel depends on what's the sequence of buttons that you're pressing and what's the sequence of pull-down menus that you're doing and all that sort of stuff. So graphical user interfaces are processes. They're things that evolve over time. So they take a sequence of inputs and they generate a sequence of outputs, just like a bank does. Controllers. How do you make a robotic steerer? That's a process. There are inputs. So for example, where are you in the middle of the lane? What time is it? How many pedestrians are you about to hit? There's all those kinds of inputs and then there are things that you output, which is like keep going. Some steering rules are particularly simple. Other steering rules are avoid the guy. So the idea is that when you were controlling processes, we're going to want to think about other kinds of programming structures. And even though the programming structures are going to be different, we're still going to want to have this idea of modularity. We're going to want to be able to think about what are the primitives that we have? How can we combine them? What are the important abstractions and how do we capture patterns? We're still going to want to do PCAP, but we're going to want to do PCAP at a much higher level. And the programming module that I want to introduce is the idea of a state machine. A state machine is a very convenient structure to capture the behavior of a process. A state machine has an input, a state, and an output. The idea is that it's a process. It's operated upon in steps. Every step there's a new input. From that input, the state machine can calculate an output and the next state. By keeping track of the state, the state machine can remember everything it needs to know so that it can continue the process into the future. So the idea is that this is going to represent a fundamental programming element that we will be able to use to construct things that are like processes. And in particular, we will use this to represent the calculations that are done within our robot. We'll think about the robot as being a process. It's not got an answer. You can't ask the question, what's the answer to the robot? But you can answer the question, if I were trying to steer and the sonar said x, y, z, how fast should you turn the wheels? You can think about that as a state machine. There's some state of the robot. There are some inputs, which is what the sonars are currently telling us. And there's some output, which is how fast we're driving the wheels. So we want to think about this kind of a representation for processes that evolve over time. OK, here's a very simple example, a turnstile. So we can think about the turnstile as a state machine. The turnstile has two states. Think about getting onto the T. Either it's locked or it's not locked. Those are the two states of the turnstile. There are some inputs. The inputs are, I put a coin in it, or I walk through it and the turnstile turns, or I don't do anything. And based on those inputs and based on the current state, the turnstile has some outputs. So the turnstile can either allow you to enter or ask you to pay. So the output might be an LED sign that says, please enter. Or it could have an LED sign that says, please pay. So those are all of the elements of a state machine. And so we can think about a graphical representation for the turnstile in terms of states. Here I'm representing the states as circles. There are two states in the turnstile. The turnstile is either locked or unlocked. You move between states by getting inputs. So the inputs are represented by the arcs. It's the top identifier on each arc is the input. The bottom identifier is the output. And the special arrow says, I start here. So the turnstile start out locked in the morning. The turnstile is locked. If you drop a coin in it, you'll move into the unlocked state, and it will output enter. It'll tell you that it's OK to enter because you've put a coin in it now. Then as long as you stay there and do nothing, it'll continue to say enter, none enter. Or if you keep dropping coins in it, it'll say, hey, yeah, keep going. And it'll continue to eat your coins and continue to say enter, being very nice. So in those cases, it remains unlocked. It continues to tell you that you can enter. And that state persists until you turn the turnstile. When you turn the turnstile, this particular turnstile forgets how many coins you gave it. And it simply locks. And it tells you, you've got to pay more. So the idea is that we can capture the desired behavior of the turnstile. The turnstile is a process. It's a thing that evolves over time. But we can capture the essence of what we would like the turnstile to do in terms of this diagram. We'll call this a state diagram. And then we can think about behaviors in terms of a set of outputs as a function of time. So imagine that as a function of time, say I started in the state locked, and the input is nothing. So you wake up in the morning, you go to the tea. Overnight, the turnstiles have all been locked. They all say that they're locked. They all say pay as their output. If I don't do anything, it just sits there and it says pay. If I drop a coin, it says enter and it moves into the unlocked state. If I don't do anything, enter persists and it stays unlocked. If I walk through it, it eats the coin and it says OK, you got to pay now and it locks. So you can think about the evolution of this process, the evolution of the turnstile in terms of the diagram with regard to time. And the idea then is that this is a kind of a representation that allows us to succinctly capture the important information about how processes work. One of the important features of this representation is that it removes you one level away from the looping structure. Somewhere somebody's keeping track of the loop. Something's going to be there. And something's going from time 0 to time 1 to time 2 to time 3. We'll talk about that later. But for the time being, this representation focuses on what do you do at each instance in time. So it's a way of tearing apart the time part from the state transition part. So separate out the loop from the essence. It's very much like the functional example that I did. The functions divorced themselves from the looping structure that we saw in the imperative approach. And you could define the functions independent of the looping. We're doing the same thing here. We can define the state machine representation independent of the looping. And most importantly, this idea of state machines is modular. Let me show you how that plays out when we think about the robot. Think about the problem that I showed you last time, where we're trying to have the robot go from point A to point B, where, for example, it knows where it is. Say it has GPS or something. It knows from a map, say, where it wants to be. But it has no idea of the obstacles between where it is and where it wants to be. So we can think about that kind of a problem. We looked at this last time. It takes a step. It uses its sonars, which are depicted by these lines, to infer that there are some walls. That's the black dots. So it gets a reflection off the sonar. That tells it that there's some obstacle in the way. That means it's not a good idea to try to plow straightforward. So if I back up, the original plan, it had no idea what was between it and its destination. So its plan was just buzz straight through. At the end of its first step, it has got back sonar reports that tell it that there's a wall here and here and here and here and here and here. And based on where the wall is and the size of the robot, it can't get into any of these red squares. It's too big. So it knows its original path, which went from here to here, isn't going to work because it can't fit through. So it computes a map of the path. So it computes a new path. And then repeat. So the idea is that we're solving a very complicated problem. The robot's solving a very complicated problem. And what I'd like to do now is think through a solution to that problem. As the robot's moving along, on every step it's getting some new input. It's behaving just like a state machine. On every step it gets new input from the sonars. Based on that new input, it knows where there's new obstacles. Based on that knowledge, it has a better idea of what the map is. Based on that knowledge, it has a better idea of finding a plan that might work. And then based on that knowledge, it knows how to tell the wheels to move forward. So think about where it is right now. Now, it has done logically three different things. It has figured out where the walls are. That's the thing represented in black and red. We'll call that it made a map. Given the map, it made a plan. That's the blue thing. Given the plan, it figured out how to turn the wheels. It's going straight ahead. So there are logically three things going on, all at the same time. You can imagine writing an imperative program to capture that behavior, but that's complicated. Not impossible. You would structure it as a bunch of loops. Unless you're a crack programmer, and I'm sure you are, you'll end up being an ugly program. The idea is that by using the state machine representation, we'll be able to think about how to pull apart those three pieces. So in particular, we can think about the state machine for the robot, which would take the sensor input from the sonars and turn it into an action, which is turning the wheels. We can think about that as having three modules. There's the map maker module, which just looks at the sensor input, doesn't care what the plan is, doesn't care how it's turning the wheels right now. All it cares about is what was the map, what's my current state, and what new information can I infer about the map from the sonars. So given the sonars and my current state, what's the most recent version of the map likely to look like? Then there's a planner that says, given the sonars and the map, how would I construct this blue line? Given my current understanding of the map of the world, how would I get from where I am to where I want to be? That's a plan. Then from the plan, say the robot is here, from the plan, you can see that the thing the robot should do is move straight ahead. So the mover can take the heading. The heading is just simply the coordinates of the first destination point in the plan. So the mover can look at the heading, the first destination point, and figure out how to make the wheels turn. So the idea is that the state machine representation will let us parse a very complicated problem, a process, the process by which the robot gets from here to there, a very complicated process. We're going to be able to break it up into pieces. The pieces are going to be much easier. They're going to be modular. We will be able to write this piece and debug it and make sure that it all works, independent of these other pieces, because we can just have it do any arbitrary walk-around task or no walk-around at all, and ask whether the set of sensor inputs generates the correct map. Similarly, this will be a module. This will be something that we can debug as well, without having the other pieces working. We'll be able to say, OK, well, if I fed a map that I hand-constructed to the robot, ignore this. Just forget this. Assume there is none of these. I can hand-make a map, feed it to this guy, give it sensor input, and see if it comes up with the right heading. It's just like in that functional approach example that I did. Having made the individual functions, we were able to test them individually because they were modules. Having made these individual state machines, we'll be able to test them individually because they're modules. Then at the end, we'll be able to paste them all together and get a very complicated behavior, much more complicated than you would expect from the sum of the three programming activities. That's what we want. That's how we want to structure things. We want to be able to combine simple things and get something that seems more powerful than the linear combination, because it is. So that's the goal. To do that, we're going to invent some new Python structures. We're going to think about the representation of a state machine in Python. And we will use objects. In fact, we'll use objects at three levels. We'll think about the highest level of abstraction of this problem as a state machine. So there's going to be a state machine class. The state machine class is going to have all the things in it that all state machines have to know about. All state machines, as we define them, are going to have to know how to start, how to make one step, and how to combine single steps into a sequence of steps. And we'll call that operation a transduction. So if you give me a list of inputs, transduce will make a list of outputs. So all state machines are going to have to know how to start, step, and transduce. Then, for particular types of state machines, like the turnstile, we will generate a subclass. The subclass of turnstiles are going to have to know all of those are going to have to do some other things. All turnstiles are going to have to have a rule for what happens when you're in state A and you get input I. What should you do? Well, there's going to be a general pattern that all turnstiles will subscribe to. We'll put those in the subclass turnstile. Then, individual turnstiles, the first one in the Kendall stop, the second one in the Kendall stop, the first one in the Central stop, the third one in the Harvard stop, particular turnstiles will be instances of the turnstile class. So the way this plays out, we'll have a generic state machine class defined here. The state machine class will have a method start. Start will create the instance variable state. Every instance of a turnstile has to remember where it is. That's an instance variable. That instance variable gets created by the start routine. So the very first thing you do if you want to talk about a new turnstile is you have to instantiate the turnstile. Then you have to make an instance, and you do that by calling start. Every state machine has to have a start routine. Every start routine will do this. It'll create an instance. It'll create an instance variable. Every state machine has to know how to take a step. We will make a generic method called getNextValues, which will include all of the rules for how the class turnstile generates a new output and a new state from the input. And every subclass of state machine will have to define what one of those looks like. The important thing here is that every state machine has to have a step routine. And the way we've decided to implement the step routine is to call the getNextValues that is particular to the subclass of interest. So for example, the getNextValues for a turnstile might be different from the getNextValues for a planner or for a mapper. Then we will subclass state machine according to the kind of state machine we're making, a planner, a mapper, a mover, a turnstile. So a turnstile will be a class which is a subclass of state machine, and that's where we'll define how do you do getNextValues. If you're a turnstile, here's the way you ought to do getNextValues. When I call getNextValues, I'll tell you your current state and the current input. And from those two, you ought to be able to figure out the next state and the output. So the getNextValues method for the turnstile class ought to take state and input, the input and the current state, and it generates a tuple, which is the new state and the output. And this is the rule by which it happens. All turnstiles also have the same start state. So if you look back again at state machine, state machine created the instance variable state by looking at start state, all turnstiles have the same start state. They all start locked. And then finally, the way we'll use this is we will instantiate a turnstile. We will make an instance of turnstile, put it into ts turnstile, so this could be central square of 16, for example. And then that instance will be able to do things like start, step, and transduce. So if I transduced this input, none, coin, none, turn, turn, coin, coin, on a particular turnstile, I'll generate some sequence of outputs like so. OK. So that's a complicated example using turnstiles. Let me do something simpler just to make sure you get the idea. Let's think about a simpler class, a subclass of state machines, a subclass called accumulator. That subclass always has a start state of 0 and has a getNext values that returns the same value for the next state and for the output. It always adds the current state to the input, hence the name accumulator. It accumulates the input by taking the current state adding the current input to generate a new state. That's just like a bank account. The bank account would accumulate transactions. So what's going to happen, so if the input to the accumulator is 7, then the state gets incremented by 7. And the output is the value of the new state. So that's what this says. The starting state is always 0. The getNext value always returns for the new state, state plus input, and for the current output, the same thing as the current state. Everybody's clear? OK. So the question is, what would happen if I did this sequence of operations? Let's say that I have the state machine class. I have the accumulator subclass. I make an instance A. I do some stuff to A. I make an instance B. I do some stuff to B. I do some stuff to A. And I type this. What gets printed? Take 30 seconds. Talk to your neighbor. Figure out what the answer is. OK. OK. OK. OK. What will get printed by the print statement? Answer 1, 2, 3, 4, or 5. Everybody raise your hand with an answer. Excellent, excellent. Almost 100%. I don't think I see a single wrong answer. So everybody seems to be saying 2. OK, so what's going on? So A is an accumulator. We start it. We step it. B is an accumulator. We start it. We step it. We step. So what is going to be A? How did you get 5? Well, A started at 0. That's what the start method does. Right? Stepping it, stepped it to 7. B made a new one. We can ignore that because it's a separate instance. The idea is that the state is associated with the instance. So when we perturb the state of B by doing B.step, we aren't touching the state of A because they're instance variables. So that means when we decrement it, we're decrementing the 7 to get 5. OK, seems reasonable. So the answer is either 1 or 2 or none. Why is it the same, 21, 21, 21, 21, when we called the A dot getNextValues and here we're calling the B? Why are they the same? Yeah? AUDIENCE MEMBER 2 So it says the state and then we figure it. So getNextValues is a pure function, so it gets passed in the state. The first number is the state, the second number is the input, so it doesn't pay any attention to the instance variable state. It uses the thing that got passed in. Furthermore, the getNextValues associated with A is precisely the same as the getNextValues associated with B because getNextValues is something that's in the accumulator subclass. So they're exactly the same because it's the same method. So what should be in your head is a picture that looks like this. When we said create a state machine, that was something we put in our library and that makes this environment. When we said subclass accumulator, subclass state machine to make accumulator, that made a new class, a new environment, and then when we made A and B by saying A equals accumulator and B equals accumulator, that made instances. The instances were initially empty, but when I said A dot start, that created the state variable in the A instance and in the B instance and associated values with them. So the picture that you're supposed to have in mind is that there's an environment associated with state machine. There's a separate environment associated with accumulator, but just one of those. But there's two instances. So there's two sets of instance variables, one associated with A and B. So the answer was number two. Now, the robot example was supposed to motivate this idea that what we're going to do with state machines is put them together modularly. So what we're going to do, and you'll start to do that this week and also continuing into next week, we'll figure out how to compose state machine M1 and M2 in a cascade. That means the output of the first one becomes the input to the second one. We'll put things together in parallel. We'll put things together with feedback. So what we'll do is we'll figure out operators, PCAP, primitives, means of combination, abstraction patterns. The primitives are going to be the state machines. The ways we combine them are going to be these kinds of structures, cascade them, put them in parallel, that kind of stuff. And that's going to allow us to make complicated brains out of simple ones. Here's a very simple example. What if I had a state machine A, which is an accumulator. So A is an instance of accumulator. B is an instance of accumulator. There's two separate instances. And C is a new state machine composed by cascading A and B. What would happen if I typed out C.transduce of 7, 3, 4? What's the answer to C.transduce 7, 3, 4? What's the answer to C.transduce 7, 3, 4? So what happens when I transduce C? Well, C is a composition of A and B. In particular, it's this composition. C is just the cascade of A and B. What that means is that if I put some sequence of inputs into C, it's the same as putting it into A. The output of the composite machine is the output of B. And the input to B is the same as the output of A. So 7, 3, 4 goes through A, which is an accumulator, and comes out 7, 10, 14. It accumulates. That's what accumulators do. That clear? Then 7, 10, 14 goes into B and comes out 7, 17, 31. So C, which is the cascade of two accumulators, ends up transforming 7, 3, 4 into 7, 17, 31. So that's the idea for how we're going to compose complicated behaviors out of simple ones. And that's what the assignment is for this week."
    },
    {
        "Rec 1 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-hdjWA3YcDII.mp3": " Hi. Let's talk about object-oriented programming in Python. First, object-oriented programming is a programming paradigm. It's a lot like, it's in the same category as things like functional programming and imperative programming, but object-oriented programming is going to be the programming paradigm that describes most of the code that you're going to interact with in 6.01. So it's important to understand how it works and how, in particular, you want to be able to code in an object-oriented programming paradigm in Python. So today I'm going to go over a quick crash course on object-oriented programming and also indicate all the little tips and tricks you need in order to program in Python. Let's look at what I have written up. So the most important thing to remember when you're learning about the object-oriented programming paradigm is that everything is an object. And what I mean when I say that, and what people mean when people say that, is that the ideals behind the paradigm are that you interact with your code in the same way that you would interact with objects in the physical world. There's a particular piece of paper on the desk in front of me, and it is a kind of piece of paper, so I know how to interact with it the way that you would interact with any other piece of paper. If you want to codify this in an object-oriented programming paradigm, you write up classes. Classes are your basic unit of code block. They describe what a thing can do and what a thing has or what attributes a thing has. And in frequently object-oriented programming and object-oriented programming in Python, we refer to those things as methods or functions that a particular object may have and attributes or particular variables that an object may have. Once you've codified what a particular object of a particular class can do, you can then use the code that you've written to instantiate an object. An object is the functional unit in the object-oriented programming paradigm. It's the thing that you interact with and tell what to do and produce results for you. It's the thing that makes up the, you know, in classes are sort of the two things that you need to think about, but you also have to think about how they're different. I have a particular sheet of paper in front of me. It has all the properties of a sheet of paper. And when I think about all the things I can do to a sheet of paper, that constitutes a class. But the particular piece of paper that I have is an instance of that class. It's a particular piece of paper. That's the gist of object-oriented programming and the things that you need to know. Now that I've covered them, I'm going to go over the most basic class I could come up with in terms of object-oriented programming in Python. This is a class that specifies what a 601 staff member has in terms of an attribute or a method in Python. If I want every instance of a particular class or every staff member of the class, staff member 601, to have a particular attribute, I can specify it like this. Every instance of staff 601 is going to have an attribute room, and that attribute room is going to be set to the string describing 34501, the 601 lab. If I want every 601 staff member to be able to do a particular thing or have a particular method or call a particular function, then I specify it like this. This is the beginning of a method in the class staff 601. It's called sayHi. I'll talk about self in a second. Don't worry about it. Act as though, you know, if this is your introduction to Python programming, then pretty much pretend it's not there. It's kind of like this. We'll cover that in a second. And if any instance of the staff 601 class calls sayHi, then hello will be printed to standard out. I have a couple examples up on the board behind me, and if you type them into idle and see what their return is like, you'll be able to, or after you've typed in this, you'll be able to interact a little bit better with what Python considers classes and objects and that sort of thing. If you look at type staff 601, it'll tell you about a class, which is an object in itself, but it's a specification for instances of an object. If you want to instantiate an object that is of type staff 601, you need to use the parens on the end. This treats staff 601 like a call and creates an object. If you just type staff 601, you're just reassigning the staff 601 class to the name KPU, and that's not useful. Every staff 601 member should not be considered a Kendra. Right. Once you have instantiated a particular object of type staff 601, you can look at the type of that object, right? Now you've got one object, myself, and that is a class of staff 601. Likewise, now that you have this object, KPU, you can look at its attributes and methods. If you look at KPU.room, then it should print to the screen 34501. That's because that's the attribute associated with this instance. If you call KPU.sayHi, it will use the method in the class type of this object. So when you call.sayHi, it looks at KPU, looks at the type, says that's a type staff 601, goes to class staff 601, finds the definition for sayHi, and then executes this code. Hopefully that all made sense. Now I want to talk about self. And you might say, Kendra, I don't understand where that comes into play, and you didn't even use it over here. If you're familiar with C++ or Java, self is a lot like this. Self is an implicit argument passed in here. Even though you specified zero arguments, it's considered the first argument. And you'll probably see a lot of type errors when you're first programming an object-oriented programming that say that you've either passed in too many or too few arguments. It has to do with this definition with self. Self says, I am talking about myself. That's not particularly intuitive, but I'll try to explain a little bit more. When KPU calls.sayHi, sayHi always has an implicit reference to whatever called it. When you look at this code, other instances can call this code. If I had an instance of Adam Hartz or an instance of Ike Schwang, they would also have access to the method sayHi. And when they called sayHi, sayHi would point back to the class definition, but also have a reference to whatever instance called it. So when you substitute in this self, you substitute in whatever instance called the method. That doesn't seem particularly useful right now, because that class definition does not actually make any use of the self or the ability to use unique instances of an object as sort of unique storage containers. I'm KPU. I'm different from Adam Hartz. Therefore, I should be able to have different attributes or different methods or things that act slightly differently from the way they do for Adam. I'm going to look at a revised definition of class staff 601, and that definition will use self in a way that indicates that you can have different functionality for different instances. Class staff 601. It looks really similar. In fact, the first two lines are exactly the same. We've got a class attribute. That means that every instance of this class is going to have this attribute, because it's a class attribute. If I want different instantiations of my class to have different properties, then I need to explicitly address the initialization of those properties. In Python, when we want to do that, we define the method init. Init is a very special method. It's got these underscores. I think it's a protected keyword. And it always has the format self and then whatever arguments you want to pass in when you're instantiating an object. Init is not exactly a constructor, but for those of you that are familiar with C++ and Java, it acts like a constructor. Immediately after the object is constructed, init is called and all the setup that is required to set up the object happens. So any time you instantiate an object, all of these things are going to be executed, or all the things under init is going to be executed. In particular, we're going to set the attribute greeting to whatever argument we passed in when we instantiated the object. So every instance that we create of this class is going to have the class attribute room. They're all going to be in the same room. But they're also going to have a greeting, and you have the option of specifying the greeting to be whatever you want. We're going to make use of this in the method sayHi, which still only takes the argument self or the reference to whatever object called the method in the first place. That reference is going to get substituted in here. So no matter which object calls the method, you will have access to its particular greeting using this syntax. Let's walk through an example. Let's say I make an example of Adam Hartz, and he is a Staff 601 member, and his greeting is going to be hi. Likewise, let's make another instance of me using the new Staff 601 definition. Make sure you type this in, because it's not going to work otherwise. And make my greeting hello, as opposed to just hi. If you call the sayHi method using Hartz, then you should get a different result than when you call the sayHi method using KPU. But if you call the room method, or if you are after the room attribute of both instances, then you should get the same result, because this is the class attribute definition, whereas this attribute is specific to each instance. That's all I have to say for now about object-oriented programming. In my next video, I'll start to talk about inheritance, which is another really important property in 601, and also object-oriented programming in Python, and also has some slip-ups. I'd like to talk to you about those next."
    },
    {
        "Rec 16 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-J09o6QRVsfw.mp3": " Today, I'd like to talk to you about some techniques that you can add to basic search to enable your systems to make more intelligent decisions and save computation time. Last time we introduced basic search and the basic idea of how to encode search so that our system can use search when encountering an unknown territory or state space. Today we're going to review some things you can do in terms of using the information that you know and also using estimations of the information that you would like to know to improve your chances of discovering the path that you're interested in the fastest. The first thing that we can do in order to improve our search is use dynamic programming. Dynamic programming refers to the idea that once you've done a computation for a particular kind of problem, you can save that computation and use it later as opposed to having to engage in that computation a second time. The way this manifests in search is that once you've visited a particular state, you don't have to visit that state again because the way your agenda is set up, you've found the fastest way to that particular state as far as you're concerned. So the general idea is that once you've visited a particular state, you don't have to visit it again. If you're building a normal search tree, it can be difficult to keep track of where you've been, especially if you're doing something like F-th for search. To get around this and to enable dynamic programming, the easiest thing to do is just keep a list of the states that you visited as a consequence of this particular run of search. I'm going to demonstrate dynamic programming by running depth-first search on our state transition diagram from last time. The first two steps are the same, except for the fact that we're going to keep track of the fact that we've visited both a, b, and c as a consequence of the first two iterations of search. If I'm running depth-first search, my agenda acts as a stack, which means I'm going to take the partial path that I added most recently to the agenda, pop it off, and expand the last node in the partial path. When I expand c, I'm going to visit b and d. However, b is already in my list of states that I visited, because it's already in one of the partial paths in my agenda. So I'm not going to visit b again. Instead, I'm only going to add one new partial path to my agenda, a, c, d. I'm also going to add d to my list of visited states. If I run another iteration of depth-first search, I find my goal. For completeness, e is added to the list of visited states. This took even fewer iterations than the original depth-first search. We didn't waste time expanding different nodes. We also use less space. In the general sense, the concept of dynamic programming is great to use whenever you can. And in search, it can save you a lot of time and energy. Another way we can intelligently improve our search technique is by making considerations for costs that we know that are associated with particular transitions. In this state transition diagram, I've indicated some costs associated with the transitions between particular states. If we know the costs associated with transitions, then we can use a cumulation of the weights accumulated through traversing a particular partial path to prioritize which paths we're going to explore first in an effort to reduce the amount of cost associated with our final actions. In order to do that, we can keep track of the value associated with that cumulation and sort the agenda at every iteration based on that cost. If we're using dynamic programming while making considerations for both costs and heuristics, and also when we're running the goal test when making considerations for cost and heuristics, we're actually going to make our considerations when we expand the node as opposed to when we visit the node. This difference is very important because it provides us with the most optimal solution. If it's possible for us to visit the goal node, but it's 100 cost units away, it may be worth our while to search for alternatives that provide a much shorter path to the goal node. That's why we switch from considering when we visit to considering when we expand. Let's look at uniform cost search run with dynamic programming. In my first step, I expand A and add two partial paths to my queue. I'm also going to keep track of the cumulative cost associated with that partial path. When I expand A, I add A to the agenda. Note that I haven't talked about stacks or queues or depth first or breadth first or anything. We're working with a priority queue. And what that means is that things with a higher priority float to the top, or things with the lowest cost associated with them we're going to consider first. This means that I'm going to expand B in the partial path AB first. I'll add it to my list. When I expand B, B has two child nodes, D and C. A, B, C has partial path cost 3 associated with it. And A, B, D has partial path cost 11 associated with it. That means when I reprioritize my queue, I'm going to end up sorting everything such that AC comes first, A, B, C comes second, and A, B, D comes third. Previously, with our strategy for dynamic programming, C would not have been added to this partial path because we've already visited it with path AC. At this step, we're going to expand the path AC, add C to the list, and any other time that we end up visiting C, we will not add it to our paths. If I expand C, I have a transition to B, which is already in my list, and a transition to D that has cost 7. A, B, C is going to float to the top of my priority queue. A, C, B is not going to be considered because B is already part of my list. And A, B, D is going to remain with cost 11. At this point, you might say, but Kendra, why am I considering partial path A, B, C when C is in my list, or C should be in my list, as a consequence of expanding the partial path AC? Even though we expanded the partial path AC, if we have not made any considerations to weed out our list at every iteration, this is still going to float to the top, and we're still going to have to deal with it even though we've already expanded C. Since we've already expanded C, we're going to ignore this partial path and just move ACD 7 and ABD 11 up to the front. D is not in our expanded list. When we expand D, we have one child node E. Because we're working with costs and heuristics, we do not actually evaluate the goal test when we visit a node. We evaluate it when we expand a node. So I am going to add ACDE to my agenda. It's going to have cost 8. And ABD 11 is still going to hang out here at the back of the priority queue. At this point, I get to expand E. I skipped adding D to the list, expanded list. When I expanded it from ACD to ACDE. At this point, I'm going to expand E. And the first thing I'm going to do is test and see whether or not it passes the goal test. At that point, I stop search, return that I successfully completed the search, and that my partial path is going to be ACDE. That covers uniform cost search. At this point, you might say, Kendra, this is bearing a lot of similarity to things like maps. And I would really like to be able to use my knowledge of things like Euclidean distance in order to make even more intelligent decisions about where it is that I go with myself. And I would say, yes, you should be able to. In fact, people do. In fact, people do all the time. They say, well, something's this far away as the crow flies, so I know that if I've gone further than that at any point, then I've wasted some amount of time. But it represents a good underestimate of the distance that I'm going to cover. This is the basic concept of heuristics. If you're attempting to find a goal and you have an estimate for the remaining cost, but you know it's not exactly right, you can still use that information to attempt to save you an amount of computation or amount of search. In particular, you probably shouldn't be using a heuristic if you know it's perfect. Because if you know the heuristic is perfect, then you should be using the heuristic to solve your problems instead of doing search in the first place. Or if the heuristic already tells you how long it's going to take to find something, then it probably also has the path that represents that amount of cost. If you want to use a heuristic effectively, you have to make sure that your heuristic represents a non-strict underestimate of the amount of cost that is left over. And what do I mean by that? I mean that if you have a heuristic and you're using it as a thing to tell you whether or not you're wasting your time, if your heuristic represents information that is bogus or says this particular path has more cost associated with it than it actually does, then it will lead you astray. Or you don't want to use a heuristic that will prevent you from using a path that actually costs less than the heuristic advertises. This is what's known as an admissible heuristic. An admissible heuristic always underestimates if it makes an error in estimation about your total distance to the goal. So at this point, I've covered dynamic programming. I've covered costs. I've covered heuristics. And it turns out that you can use all of these techniques at the same time. When you use both costs and heuristics in combination while evaluating your priority queue, that's known as an A-star search. And you'll see a decent amount of literature on A-star. This covers all of the intelligent improvements to basic search that I will talk about in this course. We hope you enjoyed 6.01."
    },
    {
        "Lec 13 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-vcDBNyKvLcs.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So hello. Today I want to talk about, and I want to finish talking about, search algorithms. OK. So last time we started to think about a framework within which to think about search. And the important thing was to figure out a way to be systematic about it. So we figured out a way to organize the way we think about searching by way of a tree. Put all the possible places where we could be in the search, consider all the possible actions that we could take. So if we started at A, there are two different actions we could have taken. We could have gone to B or D. Think of the actions as the edges of the graph. And then think about where we land. Then, by way of that graph, think about the shortest path to the goal. So that was the idea. And the big outcome was order matters. So if we were to construct an agenda, which is the list of nodes that we are currently considering, if we started at A, we would start by putting A on the agenda. Then we would pop A out of the agenda and replace it with its children. Its children are AB and AD. If the algorithm was replaced the first node in the agenda by the children, the first node is AB. So then we would pop AB and replace it with its children. AB has children A, C, E, et cetera. And the result would be something that we call a depth-first search. Because what's happening is we're following lines deeply before we look crosswise. And there's a million varieties of this that you could think of. You would get the same kind of algorithm if you replaced the last node by its children. Then you'd start with A, replace it by its children, which is still BD. But now expand D in terms of its children. Then take the last child, expand it, and we would get another depth search. So the idea is whatever order you choose affects the solution that you find. Generally, we're interested in finding a search that locates the best, shortest path. And so a good method is to remove the first node from the agenda and add the children to the end. That's a queue-based ordering, where the first out is the first in. Then if you imagine starting at A, replacing it by its children, BD, take the first guy out, that's AB. Consider its children and put them at the end of the list. Then we go back and expand AD before we think about the children of AB. And so the result, if you just follow the red up here, the result of that search is what we call breadth-first. So we systematically propagate down the search tree, looking at short paths first. So that's the idea. The idea is that search is easy. You organize it around one of these graphs and just systematically run through the search by keeping track of what we call the agenda. The node's under consideration. And the only trick is that order matters. We found two useful orders last time, first in, first out, and last in, first out. One of those giving depth-first, which is generally not a good idea. The other giving breadth-first, which is generally a much better idea. Today what I want to do is generalize that structure to take into account a much more flexible group of problems. That'll give rise to something we'll call uniform cost search. And the other thing that I want to think about is, again, the order. By thinking about the order, we can drastically improve the efficiency of a search. And that idea gives rise to something that we'll call heuristics. So the idea then that I want to look at first in terms of uniform cost search is the idea that so far we've only looked at problems where the cost of each child is the same. We were only looking at how many children in total, how many generations did we have to go through to get from the starting point to the goal. That's an important class of problems, but it's by no means the only important class of problems. And in fact, the most trivial problems you can think of don't fall into that class. So for example, imagine that what we were doing, so I motivated the entire problem last time in terms of searching for a path on a Manhattan grid. So if I wanted to go from A to I, where all the distances are equal, then minimizing the number of generations, minimizing the number of intersections that I go through will give rise to the best search. However, imagine that one particular node is way off the map. Then the number of generations that I go through is obviously not the right answer. Because there's a penalty for taking a path that goes through C, because the distance between B and C is so much bigger than the distance, for example, from D to G. So the first thing that I want to look at is how do we incorporate that kind of new information into the search algorithm? So before I do that, think about how the breadth-first search, the thing that we found last time that was the best, breadth-first search with dynamic programming, think about how it would approach the problem, and think about as we go through step by step, what is it doing wrong? So I'm going to go through an algorithm that is known not to work with the idea that you're supposed to identify as I'm doing that what step was wrong. So imagine that I'm doing this, and then I'm going to compare it in a moment to C being off the map. So if I do this problem with dynamic programming, I have to keep track of how many nodes I already visited, and I have to keep track of all the nodes under consideration. The nodes under consideration is what we call the agenda, and I will call the list of nodes that we're keeping track of for dynamic programming, I'll call that the visited list, because we'll add states to the list as we visit them. So we start out the algorithm with node A being on the agenda, we're trying to go to node I, and by the time I've started, I've already visited A. So the starting point is that the visited list has one element in it A, the agenda has one element in it A. So the algorithm is going to be pop the first guy out of the agenda and add the children to the end, paying attention to the visited list. So pop A out of the agenda, the children of A are B and D. As I visit them, they get added to the visited list, B and D. So now, as I pop the first person out, that's AB, then I want to think about the children of AB, that's ACE. Well, A is already visited, so I don't need to add that again, but C and E are not. So I add them and add them to the visited list. Then I pop AD out, AD, well the children are AEG. A and E are in the list already, G is not, so I end up adding G to the list. Next is to pop out C, the children of C are B and F. B was in the list, F was not, so I add the child that has F at the end. Then the next one is E, E has children B, D, F, H. B, D, F, the only new one is H. Then G, ADG, take G out, the children of G are D and H, but they're already in the visited list, so I don't need to worry about them. Then ABCF, so F is this guy, CEI. CEI is not in the list, and that's my answer. Yes? AUDIENCE MEMBER 2. If you keep drawing and you get it once more, then you would get another path that's equally short. So why is that not the correct answer? PROFESSOR 1. A slightly different algorithm might give me that solution. All that I'm tracing here, I'm trying to be consistent with the algorithm that we discussed last time. But at a very good point, so the breadth-first search with dynamic programming is guaranteed to give you a solution that has minimum length. It's not guaranteed to give you a particular solution of minimum length. So when there exists multiple solutions with the same length, the search algorithm might give you any of them. And that's an important thing to keep in mind. So with regard to the problem, I'm trying to think ahead. I'm trying to think ahead to where the C is off the map. It's over here someplace. So what I want to do is stop thinking about how many hops there were and start thinking about how many miles there are. The first thing I want you to notice is that this search pattern that we did created a visitation list. We visited the states in the order of increasing number of hops. That's obviously a good thing. If we can always keep in the agenda the smallest number of hops to the next place, and if we faithfully visit states starting at the minimum number and proceeding up, so we started with A, there's no hops in getting to A. So that's 0. In going from A to B, there's one hop. In going from A to D, there's one hop. In going from A to B to C, A, B, C, there's two hops. So what the algorithm that we described last time, breadth first with dynamic programming, does is it visits the states in the order of increasing number of hops. That's obviously a good thing. OK, so what's my next slide? I want to think about C being off the map. So what I'd like to do is think about what order would this algorithm visit number of miles. So if I think about replacing the metric in the bottom with the number of miles whenever the different actions that can occur incur different costs, think about what I'm saying. So I'm saying that if I were at B and I think about my children, A, C, E, which action I take, go from B to A, or go from B to C, or go from B to E, which action I take incurs different costs. So what I want to do now is think about, change the focus from thinking about how many hops is it to thinking about how many miles is it. So now, if I replace the metric, so over here the metric was how many hops, replace how many hops with how many miles. And what you see is that the algorithm is not picking up, it's not visiting the states in order of increasing path length. That's what's wrong. So what we'd like to do is modify the algorithm somehow so that it proceeds through the paths shortest to longest. So that's the goal. And that's pretty easy to do. The first thing we have to do is put that new information somewhere. And I've already alluded to the fact that the way to think about the new information is that it's associated with actions. It's not associated with states. States are where we go to in the diagram, like state E. The extra cost is not summarized in the state. The extra cost is summarized in the exact path that we took. And the way we'll think about that is incrementally. So we create the path by doing actions. And each action has a different cost. So the first thing we do is associate this additional cost with the actions. Then we're looking for a search procedure that will enumerate the paths in the order of path cost. And the way to do that is to think about the basic ordering schemes that we had before, which were the stack, last in first out, versus the queue, first in first out. What we'll do is we'll make a trivial modification of the idea of a queue, and we'll call it a priority queue. So a priority queue is basically like a queue, except the things that are queued have priorities. So the idea will be that when you push a possible action, say I had actions A, B, or C, in addition to pushing them onto the queue, which is how we would have done breadth for search, in addition to pushing them on the queue, I'll also associate with them a cost. So when I push A on the queue, the priority queue, I'll associate with that a cost, which I've arbitrarily said here, the cost is 3. When I push B, I'll associate a cost 6. When I push C, I'll associate a cost 1. So that when I do the first pop, what will come out will be the element that I pushed that has the smallest associated cost. That'll be a way that I can order then my search through the search tree in terms of the minimum cost. Is that clear? So the first time I do a pop, the element that pops out is the one with the least cost, which is this one. So I get C. C is then removed from the list. And the second time I do it, when I do a pop, I pick out the one with the cost 3, which is A. That's an easy modification to the schemes that we used before. Just like before, we can implement a priority queue with a list. Here, we've put all the complication into popping. So we just push like we did before, just jam it in. So just add it to the end of the list. And we've put the complication into pop. Pop looks through the list and finds the one that has the biggest negative cost. That's just because we had a utility routine that gave us the biggest. We want the smallest. We know that the costs are non-negative. So we implement the find the smallest by calling the routine find the biggest with a negative cost. So notice that all the new complication is in this pop routine. And if you think about it, pop's doing far too much work. The way this routine works, every time you do a pop, it goes through the whole list looking for something that is small. Obviously, it ought to be able to keep track of progress that is made in that previously. There are much better algorithms. But for the purpose of this illustration, we have to implement, but for the purpose of this illustration, we didn't bother with it. So this is not a very clever implementation. We're not trying to be clever. We're trying to illustrate the point. So if you were seriously trying to do a big search, if you were writing code for Google, you would never do it this way. But the idea, again, is in abstraction. We're going to bury those details in the way the queue is implemented. And then at the next higher level, we don't need to worry about those details. We'll abstract them away. Is that all clear? OK, so this then is the way we end up doing the search. So when we do the search, when we create a new node, the idea is going to be that we can generate a cost. Remember, nodes in the search tree summarize paths. Nodes are different from states, right? States are places that we can visit. Nodes are paths. So the thing that we need to keep track of now, in addition to what we did before, before we had the idea that nodes had states, they have a place where you are currently in the search. They have actions, which is which direction do you go next. And they have parents. And that was enough information to create and maintain a search tree. Now what we need to do is also keep track of cost. So we do that by adding instantiation time when we create a new node. We have to also pass it what is the action cost. So in the previous example, the action cost would be the distance from B to C, for example, being 5, which is different from the distance between B to E, which is 1. So we have to associate, at the time we create a node, what is the action cost associated with this new node. And then the node keeps track of the total path cost. So that's what the red stuff is doing. So it's a very small change to the code that we used for creating nodes in the previous two searches. And then we have to also change the way we do the basic search algorithm. And here, too, the idea is pretty simple. It's almost exactly the same thing that we did before with the idea that we substitute, keep track of the agenda with a priority queue rather than with a queue or a stack. There's one more complication, and that is that in the past, we knew that all children added the same penalty because we were only keeping track of how many generations, how many hops are there to the current node. All children were, in some sense, created equal because we knew they all incurred one more hop. Here, because the children can have different associated action costs, we don't know at the time we've picked up the parent, we don't know at that time which child is going to end up being the shortest one. So previously, the goal test was performed when the children were pushed onto the agenda. Here, we have to wait until we look at all of the children before we will know which child has the shortest length. And that just means that we take the goal test, which had been inside the for A in actions loop. We have to factor that out and defer it until the next time. So that's the only change to the algorithm that we need to make. So the idea is it's a pretty simple modification of the algorithm that we have so far, but it gives rise to a much more versatile kind of search. So last time, we saw that there was really no good point for not doing the breadth-first search with dynamic programming. We have the same thing here. So when you're doing the priority search, when you're doing the uniform cost search, there's no real good reason for not implementing that with dynamic programming. So we also want to think about the dynamic programming algorithm. So you remember in breadth-first search and in depth-first search, dynamic programming referred to the principle that the shortest path from x to z through y, so if you have a path from x to z and you don't go through y, the shortest way you can get from x to z through y is to add the shortest path from x to y to the shortest path from y to z. Sounds trivial, but it has a tremendous impact on the number of states that can be omitted from the search. Here, when we do the uniform cost search, we can do the same sort of thing. Except that, we run into the same sort of problem with not all paths from x to y are created equally. We don't want to remember any random path from x to y. We want to remember the best one, which means that in general, we're going to have to expand all of the children before we'll know which child gives the minimum length path. So that means that the dynamic programming principle that we'll use is to remember the state when it gets expanded, because it only gets expanded after it's already been compared to its siblings. So wait until a path has been compared to all the siblings of that particular path before you consider it for dynamic programming. So we'll do that by not keeping track of states as they are visited, but instead keeping track of states as they are expanded. That's the same idea that we had to do when we had to reorder goal tests. Defer the goal tests until after you think about all the children, defer putting it in the dynamic programming list until after you've looked at all the children. And so that means that we think about expansions instead of visits. And so that looks like this. So just like we took the goal test outside the action list, in the previous Brecht-Furzerst, we did the test goal state in this loop. Here we defer it until we've looked at all the children and fetched the parent. So we defer it into the higher loop. We take it out of this loop and into this. And similarly, we take the dynamic programming memory. We now call it expanded to remind ourselves that what we're doing is keeping track of states after they were expanded, not after they were visited. And updating it, again, not when we look at the individual actions, but only after we've decided which child is the winner. That's probably confusing. That's not important at this point, because I've written an example, and hopefully by thinking through the example, you'll be able to see what the code is supposed to be doing. And then a good exercise is to go through the example, which will be posted on the web with all the worry details, and make sure that you can match up the search, the partial results of the search to the way the algorithm is written. Now I want to do the problem of interest. Think about what if one of the states, state C, is very distant relative to all the rest? How will the new search, the uniform cost search, work with this problem? So we do the same sort of thing we did before. Now we have an agenda to keep track of the nodes under consideration. We have a dynamic programming list that is going to be called expanded, because we don't add to it until we expand states. It had previously been called visited. And we also keep track of the metric, which is the path cost. So if we start by putting node A on the agenda, its path cost is 0, because it doesn't cost anything to get there, because that's where we started. And we haven't expanded anybody yet. So notice that the starting state is a little bit different. Here, because we're keeping track of expanded, the expanded list started out with 0 elements in it. Previously, where we were keeping track of visits, it started out knowing already what was going on with A. So now we expand A. Think about A's children, B and D. Put them on the agenda and add A to the expanded list. We expanded A, so it's time to add it to the dynamic programming list. Then also keep track of the costs of these paths. AB costs 1. AD costs 1. That's the end of the first pass. Now pop the first guy off the queue. Expand it. That means we're expanding the B state. We go from A to B, so we're going to expand B. B has children A, C, and E. A has already been expanded, so we don't need to think about A anymore. C and E, so it was A, C, and E. C and E have not been expanded. So I expand B to get these two. And I associate their total path costs. So the path ABC has length 6, and the path ABE has length 2. Then I take the first one off the agenda again. That's AD. I expand AD, which is expanding D. Put that on the expanded list, on the dynamic programming list. D has children A, E, G. A is already on the expanded list, so that means I have to worry about E and G. Those paths have length 2 and 2. That was the same so far as the previous search. Now I see something different. Because I'm using a priority queue, I skip ABC because that's just not the right one to do next. So ABC, I'm keeping track of with the priority queue. I know that that path is already length 6. It could end up being the optimum path, but at this phase of the search, it's not the one I should think of next. The one I should think of next is the shortest path. So what I'm trying to do is search the search tree in order of shortest path. So I skip the ABC because its path is long. And the minimum length, so going back up one. So in the priority queue idea, I want to extract from the queue the first item with the minimum length. So that's A, B, E. Everyone's with that? So then I want to expand E. E has children B, D, F, A. H. So B and D are here. F and H are not. So I add A, B, E, F, H to the agenda. Each of those have length 3. What's the next guy out of the agenda? AB. I need the first one with the smallest path, the smallest cost possible. The smallest cost possible is 2, so ADE is the next guy. So I expand ADE, but I've already expanded E. Don't need to do anything. The dynamic expansion list is keeping track of the nodes I've already expanded. I already did E. There's nothing new to be learned. So that doesn't do anything. So the next one is G. Well, I didn't do G yet, so add it to the expanded list. G. G's children are D and H. D was already there. H was not, so add H. The next one is F. I haven't expanded F yet, so let's do F. So F's children are C, E, I. C is not there. E is there. I is not there. So I add those. So next is this guy, H. H wasn't expanded, so I add H to the list. H's children are E, G, I. E, G are already there. I is not, so I add that. Try expanding H, but I already did expand H, so that doesn't count. Try expanding C. That's fine. Expand C. I haven't expanded C before. Add it to the list. C's children are B and F. B and F are both there. Didn't add any new children. AUDIENCE MEMBER 2 Why did you expand that C? It's got an H. When you expand C, it's B. PROFESSOR 1 Oh, thank you very much. My slides are all. PROFESSOR 1 Ignore that. Absolutely correct. Thank you. So let's see, I guess this proves that 200 people watching the lecturer have greater insight. Well, anyway. Yes, that's wrong. Don't bother with that because it's got the wrong priority. Jump straight to that. I'll fix this slide on the web. You're absolutely right. I should have gone straight to there. And when I try to expand I, I realize that that's my answer. So I'm done. OK? Questions? OK, it's a little tedious. The point is that it really wasn't very much different from doing breadth-first search. The same ideas still work. The same idea of organizing the search on a tree, looking for the minimum cost answer on the tree. That's the big picture. That's what we want you to know about. So the details, I mean, we may ask you a quiz question about breadth-first search, or depth-first search, or dynamic programming, or whatever. But the big picture, the thing we really want you to know, is how to think about a search. The way to think about a search is as a sequence of states organized in a tree that you can then systematically search. That's the idea. And the point of going through the uniform cost search is to see, first and foremost, that it fits the same structure, it's the same kind of problem. Secondly, there are very tiny details. And so I've tried to go over those details. The details have to do with keeping track of the action costs and keeping track of which one to do next by way of a priority queue. But the big picture is the same idea works. States, nodes, trees, search, queues. Questions, comments? OK. The other important thing that I want to talk about today is again this idea of trying to minimize the length of your search. The other point that you're supposed to get from these two lectures is, depending on exactly how you set up the search, you can do a lot of work or less work. And we're always interested to do less. Especially because if you can do a lot less, you can do a lot harder problem. The other thing that I want to talk about next is the idea that our searches so far have been starting state centric. What do I mean by that? Every search has a starting state and a goal. And the ordering of our searches so far have been go to the starting state, think of all the steps you can make from the starting state, and just keep widening your search wider and wider and wider until you stumble onto the goal. OK, can you all see that that's what we've been doing? So nowhere in our code was the code aware of the goal other than, am I there yet? So the search algorithm so far has been start at the beginning state, ask if I'm there yet. Try the closest place I can go. Am I there yet? Am I at the goal yet? Try the next closest place I can go from the start. Am I at the goal yet? Try the next closest place I can start from. Am I there yet? Everything has been starting state centric. Obviously, that's wrong. Somehow, when you do a search, if you were asked to find the shortest route through the interstate highway system from Kansas to Boston, there's a good chance you wouldn't be looking at Wyoming. OK, everybody knows enough geography to do that one, right? So the idea is that in the searches we've done so far, you would look at Wyoming before you'd look at Massachusetts. OK, that's stupid, right? Everybody sort of see that? So the searches we have done so far have been starting state centric. So what I'd like to do is think about a way to undo that. But again, to set things up, let's think about what I mean by that. Let's imagine a search very much like what we did before, except let's go from E to I, from Kansas to Boston. I guess it's Kansas to Tallahassee. But you get the idea. So we start at E. And let's think about how our search proceeds. So if you start at E, so think about we push E in the agenda. So I'm doing the uniform cost search with dynamic programming. I'm keeping track of the expanded states as my dynamic programming state. I'm starting at E. The cost of being at E is 0, because that's where I started. And now I think about expanding E. So E goes on the expanded list. I think about all the children of E. The children of E are B, D, F, H. All of those children have distance 1. So when I look among them to figure out the next one that I should do, I do the first guy, EB. B's children are A and C. So I take off EB from the beginning of the agenda, stick on A and C. No, wait. Start again. B. B's children are A, C, E. E is already expanded. Push A and C. Pop D. D's children are A, E, G. Push A and G, because E's already there. Expand F. F's children are C, E, I. E's already there. Push C, I. Expand H. H's children are G, I. E, G, I. E's already there. Push G, I. Expand A. A's children are B and D. They're already there. Don't need to do anything. Expand C. C's children are B, F. B, F are there. I don't need to do anything. Expand A. B, D are already there. I don't need to do anything. Expand G. G's children are D, H. D, H are both there. I don't need to do anything. Expand C. B, F. B, F. Nothing. Expand I. I'm there. Yes? AUDIENCE MEMBER 2. OK. Thank you. I'll fix this one, too. So the question was, when I expanded A, when I did this one, for example, when I tried to expand A, I should have added it to the list here so I don't try to do it again. It wouldn't have affected the outcome, but I should have added it to that list. Thank you. OK. Class 2, Freeman 0. OK. Yes? AUDIENCE MEMBER 3. It's correct, right. So what I really should do is go back and read the code and figure out what it should do. What I'm trying to do is emulate the code. So your point is, it's a technical definition about whether I want to think about whether I actually expanded it or not. If I don't add any children, was that a real expansion? And that's a question that is determined by where was the if statement in the code? And frankly, I don't remember. It does not expand it. So I have the right answer. It does expand. OK, so I have the wrong answer. OK, so class 2 and 1.5. OK. So the point of this is that the search was symmetric around E, even though the goal is not. OK. And the question is, how could we fix it so the search is not symmetric around E, the starting point? How do I fix it so the search is biased toward going toward the answer? And so the way we think about this is with something we call heuristics. So if you think about the searches we've been doing, either breadth-first and depth-first from last time, where we counted the number of hops, or today, where we counted the lengths of paths, each of those considered what thing to do next based on the path from the starting point to the point under consideration. The idea of a heuristic is to add something that informs the search about how much distance we're expecting to add from the point under consideration to the goal. So the idea of the heuristic is to put in the second part of the path. The problem with a heuristic is that finding the second part of the path is just as hard as finding the first part of the path. And it would be a terrible idea to, for every point in the search tree, run a new search to find the best answer from that point to the goal. Because that would increase the length of the search time enormously. The problems are of equal complexity. The problem of getting from the start point to the place of interest, and the problem of going from the place of interest to the goal, are problems of equal complexity. We don't want to try to solve the problem of making the search better informed by increasing the complexity of the search drastically. So that's the issue. So a heuristic is going to be a way of approximating the amount of work we have to do yet, where what we would like it to be is not all that terribly difficult to compute. So one way we could think about that would be to consider as an approximation to how much work we have to do the Manhattan distance, for example, to complete the path. Manhattan distance is the sum of the x and y distance. Generally speaking, Manhattan distance is not a good idea for map-like problems, because generally you can cut across corners in such searches. In this particular search, since I've excluded cutting across diagonals, since the search space is already Manhattan, thinking about a heuristic that's based on Manhattan distance is probably OK. So the idea is to develop a heuristic, and here what I'm going to think about is, what if I complete the path by adding the Manhattan distance from the point under consideration to the goal? OK, so now if I started at e like before, and I'm going toward i like before, then I start with the agenda having just e and having expanded nothing. However, the cost associated with e is no longer 0. The cost of going from e, the starting point, to e, the point under consideration, is still 0. But I'm estimating the cost of going from e to i by the Manhattan distance between e and i, which is 2. The Manhattan distance between e and i is you have to increment x by 1, and you have to decrement y by 1. So instead of saying that the cost of state e is 0, I'm saying that it's 2. That clear, why I'm doing that? So now I expand e. I think about the children, BDFH, and those children now are not the same cost. Even though it costs the same amount to go from e to each of its children, going from each child to the goal does not cost the same amount. If I were to go from e to b, that's a cost of 1, but the Manhattan distance from b to i is 3. So I'm estimating then that the cost of making the decision go from e to b is 4. The real cost of going from e to b plus the estimated cost of going from b to i. Similarly, if I go from e to d, the Manhattan distance is 3. So that's a length 4. If I go from e to f, the Manhattan distance from f to i is just 1. So the total cost is just 2. So now, rather than circling out from the starting point, my next step is biased toward the goal. So my next step is biased toward, so in the remaining items, the smallest cost is 2. So the next place that I expand is e, f. F's children are c, e, i. e's already here, so I only think of c, i. c and i also have different costs. So the cost of going e, f, c, e, f, c, the direct cost is 2 and the estimated distance, the Manhattan distance between c and i is also 2, so that's a cost 4. Whereas the cost of e, f, i, e, f, i has a direct cost of 2, and the Manhattan distance from i to i is 0. So now, I look for the minimum distance that's remaining. So that's going to be h. I expand h, the children are e, g, i. So e is already here, I think about g, i. Same sort of deal, some of them are short, some of them are long, and as I proceed through the search, I very quickly find i without having ever looked in the wrong direction, or at least having looked minimally in the wrong direction. Is that clear? So the idea in a heuristic is to add an estimate of how much it will cost to complete the path so that you bias the search toward the goal. Rather than making circles that spiral out from the starting point, make the spirals biased toward the goal. And here's the way you do that, it's very easy. All you do is, every place we would have looked at cost before, add the heuristic function. So you have to add a heuristic function. That's the part that's hard. The code, given the heuristic function is easy, the really hard part is actually figuring out what a reasonable heuristic is. The reason that's hard is that you have to be careful not to miss the solution. So the heuristic function can't be bigger than the actual distance. OK, why is that? The agenda is trying to keep track of all the possible places that you could look next. If your heuristic function makes the next step look too big, it will be taken out of the agenda and never appear again, and you'll never find that path. So when you're making a heuristic function, you have to be very careful not to ever overestimate the distance from where you are to the goal. If you ever overestimate it, then you can exclude forevermore. So think about the agenda as a pruning operation. We did this starting last time. When we think about pruning, we lop off things from the agenda. We say, don't ever need to look there. Don't need to look there. We pruned it. If you have a heuristic that is too big, you can prune a path that was the answer. So you have to be careful never to do that. So it's asymmetrical. If you were to put in a heuristic that is too small, that causes you to underestimate the penalty of going in the wrong direction. So if you said, I'm trying to go from Kansas to Boston, and you inadvertently said that Wyoming really didn't cost anything, then you would not necessarily exclude the correct answer. But you would include and cause the search to consider unnecessary paths. So the idea is that you would like the heuristic to be the same as the real cost or smaller. You don't want to end up solving another search problem in order to calculate it, because that will increase the cost of doing the search too much. So you would like some number that's easy to calculate that has the guarantee that it will always be less than or equal to the actual cost. And that's the art of doing a heuristic. If you satisfy those, then that previous algorithm that we looked at, which is in the literature it's called the A star algorithm for historical reasons, that algorithm, if you obey these rules for finding an admissible heuristic, if you find an admissible heuristic, then the A star search will be a lot faster and still find a solution with the same length. OK, so now just to see if you're following what I'm saying, here's a question to ask yourself. Remember the tiles problem. The tiles problem is the first problem that I did last time. The idea was, imagine starting in this configuration, 1, 2, 3, 4, 5, 6, 7, 8, move one tile at a time by moving the tile into the free spot. So I could move 8 to the right or 6 down. Keep doing that until you perturb this configuration into this configuration. We saw last time that there are a large number of states. There's a third of a million states. So this is a big search problem, even though it's something that you've almost certainly all solved as a child. In fact, my version, it was the same as yours, I'm sure, was a 4 by 4. I did the 15 puzzle, not the 8 puzzle, but it's the same idea. So what I want to do is consider heuristics. Consider three heuristics. Heuristic A is 0. It always returns 0. That's an easy heuristic to calculate, right? Heuristic B is sum the number of tiles that are in their wrong position. What is heuristic B for this state? There's eight tiles that are out of position. So heuristic C is the sum of the Manhattan distances required to move each tile to their respective locations. And the question is, and calculate two partial sums, consider MI to be the number of moves in the best solution if you use heuristic I, and EI is the number of states that are expanded while you're doing the search if you use heuristic I. Which of the following statements are true? Take a minute, talk to your neighbor, figure out which of these are true. Well, if it takes longer than that, then that's what we expect. OK, so what's the smallest numbered correct answer? The smallest numbered correct answer. Oh, come on. Follow through, explain it to your neighbor. OK, very good. The smallest numbered correct answer is 1. MA equals MB equals MC. Why is that? How would that prove that A equals MB equals MC would have to do? Yeah? They're all running the same search, so they'll have to reach the shortest possible solution first. So under what conditions will they all reach the same length solution? They're all doing breadth first. They're all doing breadth first. So they're all doing breadth first. There's another condition, yes? They're all either exactly correct or 100% correct. So the heuristics, all three heuristics have to be admissible, which means that they have to be non-negative numbers. To be admissible, a heuristic has to be non-negative. And it has to generate an answer that's smaller than the actual number of moves necessary to complete the path. So we have to prove that these are all admissible. So are they all non-negative? Yes. Are they all less than or equal to the number? Let's do that. Let's first solve that for a minute. We'll do that after we do the second part. What's the second smallest correct statement? Does that mean you see if I get the answer or does it? The answer is 4, number 2. So how do you know that EA is bigger than EB is bigger than EC? So the number of states expanded has to do with the size of the heuristic. If the size of the heuristic is 0, that's the same as not using a heuristic. That'll search all possible states in a breadth-first search fashion. If the heuristic is anything that's bigger, the number of searches will go down. What's the greater than or equal to thing doing here? Yes? Well, number 3 is technically also true. Because it's a plus, and 3 has to be bigger. If what the number 3 is technically true, then is bigger than or equal to the heuristic itself. OK, OK, OK. OK, class, three plus Freeman. Oh well. Yes, you're right. Yes, I agree. Did you raise your hand for 3? OK. So 3 is technically the second. Yes, that's right. That's right. OK. So moving on. Number 5, the same best solution will result for all the heuristics, true or false? Come on, things can only go downhill for me. So the same best solution will result for all the heuristics. How could it possibly be false? Didn't we already say MA equals MB equals MC? Yeah? Yeah? STUDENT 1 Well, all of them have the same amount of moves, but maybe not exactly the same solutions. PROFESSOR 1 So if there are multiple solutions of the same length, the different heuristics don't have to give you the same solution exactly. They have to give you solutions with the same length. Right? So what happens with the heuristics is that you perturb the order of search. If you perturb the order of search, the only thing that is proved is that you get a minimum length solution, not the same minimum length solution. This particular problem has lots of solutions, and so you don't necessarily get the same solution when you use different heuristics. OK? OK. So the final point is that the addition of the heuristics can be extremely effective. If you run this problem with our search algorithm, with heuristic A, so you always get solutions with 22 moves in them, that's good because all the heuristics were admissible. So you always get a right answer, a shortest answer. But the number of visited and expanded are drastically different when you add the heuristics. So if you use heuristic A, which is equivalent to no heuristic, you end up visiting 170,000 states to find this answer, where if you use the Manhattan distance to the goal, the sum of the Manhattan distances, you do a very small fraction of that. So the point is that this stuff matters, especially when you do a higher dimension search, which is all of the searches that we will be interested in. So the idea is that the order really does matter. And with that, I'll conclude with a reminder that tomorrow evening is the make-up retake day for NanoQuizzes. Please come to the lab if you'd like to make up a retake for NanoQuizzes."
    },
    {
        "Lec 5 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-qB5wq5L6EL4.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. Last time, which is now two weeks ago, we started to talk about the signals and systems approach, by which I mean, think about a system by the way it transforms its input signal into an output signal. That's kind of a bizarre way of thinking about systems. I demonstrated that last time by thinking about a mass and a spring, something that you have a lot of experience with, but you probably didn't use this kind of approach for thinking about it. Over this lecture and over the next lecture, what I'd like to do is show you the advantages of this kind of approach. And so for today, what I'd like to do is talk about how to think about feedback within this structure. And I'd like to also think about how you can use this structure to characterize the performance of a system in a quantitative fashion. So first off, I want to just think about feedback. Feedback is so pervasive that you don't notice it most of the time. You use feedback in virtually everything that you do. Here's a very simple example of driving a car. If you want to keep the car in the center of your lane, something that many people outside of Boston at least think is a good idea, then you are mentally doing feedback. You're constantly comparing where you are to where you'd like to be and making small adjustments to the system based on that. When you think of even the most simplest systems, like the thermostat in a house. I'm not talking about a cheap motel. The cheap motels don't do this. But in a real house, there's a thermostat which regulates the temperature. That's important because if the temperature suddenly drops, it would never do that, of course. But if the temperature ever dropped, it could compensate for it. Here's one of my favorite examples. Feedback is enormously pervasive in biology. There's no general rules in biology, but to a first cut, everything is regulated. And in many cases, the regulation is amazing. Here's an example from 6.021 where it's illustrating the system that your body uses to regulate glucose, glucose delivery from food sources to every cell in your body, which is crucial to your being cognizant and mobile. So the idea is that it does that with amazing precision, despite the fact that eating and exercise are enormously episodic. In order for you to remain healthy and functional, you need to have something between approximately 2 and 10 millimoles per liter of glucose in your blood at all times. Were it to go higher than that systematically, you would develop cardiac problems and could lead to even congestive heart failure. If you were to have lower than 2 millimoles per liter, you would go comatose. That's a very narrow range, two to five, especially because what we eat is so episodic. What we exercise is so episodic. It's amazing. And just to dramatize how amazing it is, how much sugar do you think circulates in your blood right now? Well, if you convert 5 millimoles per liter, and if you assume the average person has about 3 liters of blood, which is true, and if you calculate it, that comes out to 2.7 grams. That's this much. This is 2.7 grams of table sugar. By comparison, so this is how much sugar is in your blood now, this is what's keeping you healthy, this is what's keeping you from becoming comatose. Or not. How much sugar do you think is in this? STUDENT 1. Exactly. We call that the theory of lectures. I don't want to look like an idiot. So my props make sense. This is the amount of sugar in a can of soda. Numerically, this is 39 grams. OK, that's a lot more. That's more than 10 times, that's more than 13 times, this much. So when you down one of these, it's very important that the sugar gets taken out of the blood quickly, and it does. And that happens by way of a feedback system. And the feedback system is illustrated here. Basically, the feedback involves the hormone insulin. And that's why insulin deficiency is such a devastating disease. Finally, everything we do has feedback in it. Think about how your life would be different if it didn't. Even a simple task like removing a light bulb would be virtually impossible, except for the fact that you get feedback from everything. Your hand's amazing. You have touch sensors, you have proprioceptive sensors, you have stress sensors on the muscles and ligaments, and they all coordinate to tell you when to stop squeezing on the light bulb so you don't break it. That's all completely amazing. And what we'd like to do then is think about that kind of a system, a feedback system, within the signals and systems construct. As an example, I want to think through the wall finder problem that you did last week in Design Lab. I'm sure you all remember that in that problem, we were trying to move the robot to a fixed distance away from the wall. And we thought about that as a feedback system comprised of three parts, a controller, a plant, and a sensor. We wrote difference equations to characterize each of those parts. And then we figured out how to solve those difference equations to make some meaningful prediction about how the robot would work. So just to make sure you're all on board with me, here's a question for you. These are the equations that describe the wall finder problem. How many equations and how many unknowns are there? Take 20 seconds, talk to your neighbor, figure out an answer between 1 and 5. Unlike during the exam next week, you aren't allowed to talk. T and K are no. OK, so what's the answer? Number 1, 2, 3, 4, or 5? Everybody raise your hands. Let me see if I got the answer. Come on, raise your hands. Come on, everybody vote. If you're wrong, just blame it on your neighbor. You had a poor partner? That's the idea, right? So you can all vote, and you don't need to worry about being wrong. And you're all wrong. So it all works out well. OK, so the predominant answer is number 2. I don't like number 2. Can somebody think of a reason? Now that you know the answer by the theory of lectures, the answer is not 2. Why isn't the answer 2? Yeah? So the question is, you count before or after doing substitution and simplification. And I meant to count before you do simplification. Any other issues? Yeah? AUDIENCE MEMBER 2, INSTRUCTOR 1, PART 2 Is d0 n different from d0 n minus 1? You say it. Is d0 n different from d0 n minus 1? Is d0 n different or the same from d0 n minus 1? That's the key question. So I want to think about this as a system of algebraic equations. And if I do that, then there's a lot of them. And in particular, if I think about, it looks like there's three equations. The problem with that approach is that there's actually three equations for every value of n. If you think about a system of equations that you could solve with an algebra solver, you would have to treat all the n separately. That's what we call the samples approach. So here's a way you could solve them. You could think about what if k and t are parameters, so they're known, they're given. What if my input signal is known, say it's a unit sample signal, for example. What would I need to solve this system? Well, I'd need to tell you the initial conditions. So in some sense, I want to consider those to be known. So my knowns kind of comprise t and k, the initial conditions for the output of the robot, and the sensor, all of the input signals, because I'm telling you the input and asking you to calculate the output. My unknowns are all of the different velocities for all values of n bigger than or equal to 0, because I didn't tell you those. All of the values of the robot's output at samples n bigger than 0, all the values of the sensor output for values n bigger than 0. So I get a lot of unknowns, infinitely many, and I get a lot of equations, also infinitely many. So the thing I wanted you to think about is if you're thinking about solving difference equations using algebra, that's a big system of equations. By contrast, what if you were to try to solve the system using operators? Now how many equations and unknowns do you see? By the theory of lectures. OK, raise your hand. Or talk to your neighbor so you can blame your neighbor. Talk to your neighbor. Get a good alibi. So the idea here is that if you think about operators instead, so that you look at a whole signal at a time, then the equations, each equation only specifies one relationship among signals, and there's a small number of signals. So if I think about the knowns being kt, the parameters, and the signal di, and if I think about the unknowns being the velocity, the output, and the sensor signal, then I get three equations and three unknowns. So one of the values of thinking about the operator approach is that it just simply reduces the amount of things you need to think about. It reduces complexity. That's what we're trying to do in this course. We're trying to think of methods that allow you to solve problems by reducing complexity. We would like ultimately to solve very complicated problems. And this operator approach is an approach that lets you do that. It does a lot more than that, too. It also generates new kinds of insights. So it lets you focus on the relations. But the relation is now not quite the same as we would have expected from algebra. Now the relation between the input signal and the output signal is an operator. We're going to represent the operation that transforms the input to the output by this symbol h. We'll call that the system functional. It's an operator. It's a thing that when operated on x gives you y. It's also convenient, and this is one of the main purposes of today's lecture, it's also convenient to think about h as a ratio. We like to think of it that way because, as we'll see, there's a way of thinking about h as a ratio of polynomials in R. So two ways of thinking about it. We're trying to develop a signals and systems approach for thinking about feedback. We want to think about the input goes into a box. The box represents an operation. We will characterize that by a functional. We'll call the functional h. The functional, when applied to the input, produces the output. And what we'd like to do is infer what is the nature of that functional and what are the properties of the system that that functional represents. OK, to see that you're all with me, think about the wall finder system. Think about the equations for the various components of that system when expressed in an operator form. And figure out the system functional for that system. Figure out the ratio of polynomials that can be in R that is represented by h. Take 30 seconds, talk to your neighbor, figure out whether the answer is 1, 2, 3, 4, or 5. So what's the answer? 1, 2, 3, 4, or 5? Come on, more voter participation, right? Blame it on your partner, right? OK, virtually 100% correct. So the idea is algebra. You solve the operator equations exactly as though they were algebraic. Here I've started with the second equation and just done substitutions until I got rid of everything other than do and di. So I express v in terms of ke, then I express e in terms of di minus rdo. Then I'm left with one equation that relates do and di, which I can solve for the ratio. And the answer comes out there, which was number 3. Point is that you can treat the operator just as though it were algebra. So that results in an enormous simplification. But what we want to understand is what's the relationship between that functional, that thing that we just calculated, and the behaviors? These are the kinds of behaviors that you observed with the wall finder system. When you built the wall finder system, depending on what you made ke, you could get behaviors that were monotonic and slow, faster and oscillatory, or even faster and even more oscillatory. And what we'd like to know is, before we build it, how should we have constructed the system so that it has a desirable behavior? And incidentally, are these the best you can do? Or is there some other set of parameters that's lurking behind some door that we just don't know about? And if we could discover it, it would work a lot better. So the question is, given the structure of our problem, what's the most general kind of answer that we can expect? And how do we choose the best behavior out of that set of possible behaviors? So that's what I want to think about for the rest of the hour and a half. And I want to begin by taking a step backwards and look at something simpler. The idea is going to be the same as the idea that we used when we studied Python. I want to find simple behaviors, think about that as a primitive, and combine primitives to get a more complicated behavior. So I want to use an approach that's very much PCAP. Find the most simple behavior, and then see if I can leverage that simple behavior to somehow understand more complicated things. So let's think about this very simple system that has a feedback loop, that has a delay in it, and a gain of p0. What I want to do is think about what would be the response of that very simple system if the input were a unit sample. So find y, given that the input x is delta. In order to do that, I have to start the system somehow. I will start it at rest. You've all seen already, I'm sure, that rest is the simplest assumption I can make. I'll say something at the end of the hour about how you deal with things that are not at rest. For the time being, we'll just assume rest, because that's simple. Assume that the system is at rest. That means that the output of every delay box is 0. That's what rest means. If the output of this starts at 0, then the output of the scale by p0 is also 0. And if that's 0, and if the input is 0, because I'm at time before 0, then the output is 0, indicated here. So now, if I step, then the input becomes 1, because delta of 0 is 1. The output of the delay box is still 0. So the first answer is 1. The 1 just propagates straight through the add box. Then I step, and the 1 that was here goes through the r and becomes 1. The 1 goes through p0 and becomes p0. But at the same time, the 1 that was at the input goes to 0, because the input is 1 only at time equals 0. So the result, then, is that after one step, the output has become p0. This propagated to 1, that became p0, added to 0, and it became p0. So now the answer, which had been 1, is p0. On the next step, a very similar thing happens. The p0 that was here becomes the output of the delay, gets multiplied by p0 to give you p0 squared, gets added to 0 to give you p0 squared, et cetera. The thing that I want you to see is that the output was in some sense simple. The value simply increased as p0 to the n, geometrically. There's another way we can think about that. I just did the sample by sample approach, but the whole theme of this part of the course is the signals approach. If I think about the whole signal at one failed swoop, then I can develop an operator expression to characterize the system. The operator expression says the signal y is constructed by adding the signal x to the signal p0 r y. If I solve that for the ratio of the output to input, I get 1 over 1 minus p0 r. Again, going back to the idea that I started with, that we're going to get ratios of polynomials in r. Now the r is in the bottom. And now I can expand that just as though it were an algebraic expression. I can expand 1 over p0 r in a power series by using synthetic division. The result is very similar in structure to the result we saw in sample by sample. It consists of an ascending series in r, which means an ascending number of delays. Every time you increase the number of delays by 1, you also multiply the amplitude by p0. So this is, in fact, the same kind of result but viewed from a signal point of view. Finally, I want to think about it in terms of block diagrams. Same idea. I've got the same feedback system, but now I want to take advantage of this ascending series expansion that I did and think about each of the terms in that series as a signal flow path through the feedback system. So 1, the first term in the ascending series, represents the path that goes directly from the input to the output passing through no delays. The second term in the series, p0 r, represents the path that goes to the output, loops around, comes back through the adder, and then comes out. In traversing that more complicated path, it picked up one delay and one multiplied by p0. Second term, two loops. Third term, three loops. Fourth term, four loops. The idea is that the block diagram gives us a way to visualize how the answer came about. It came about by all the possible paths that lead from the input to the output. Those possible paths all differed by a delay, and that's why the decomposition was so simple. Each path corresponded to a different number of delays through the system. That won't always be true for more complicated systems, but it is true for this one. This flow diagram also lets you see something that's extremely interesting. The cyclical flow paths, which are characteristic of feedback, feedback means the signal comes back. Cyclical flow paths require that transient inputs generate persistent outputs. They generate persistent outputs because the output at time n is not triggered by the input at time n. It's triggered by the output at time n minus 1. It keeps going on itself. That's fundamental to feedback. There's no way of getting around that. That's what feedback is. And it also shows why you got that funny oscillatory behavior in wall finder. There wasn't any way around that. Feedback meant that you were looping back. That meant that there was a cycle in the signal flow paths. That means that even transient signals, signals that go away very quickly, like the unit sample, generate responses that go on forever. So that's a fundamental way of thinking about systems. Systems are either feed forward or feed back. Feed forward means that there are no cyclic paths in the system. Every path that takes you from the input to the output, no path in the system that takes you from the input to the output has a cycle in it. That's what acyclic means. That's what feed forward means. Acyclic, feed forward, those all have responses to transient inputs that are transient. That contrasts with cyclic systems. Acyclic system has feedback and will have the property that transient signals can generate outputs that go on forever. OK. How many of these systems are cyclic? Easy questions. 15 seconds. Talk to your neighbor. OK, so what's the answer? How many? OK, virtually 100% correct. The answer is three. I've illustrated the cycles in red. So there's a cycle here. There's two cycles in this one. And there's a cycle here. So the idea is that when you see a block diagram, one of the first things you want to characterize, because it's such a big difference between systems, is whether or not there's a cycle in it. If there's a cycle, then you know there's feedback. If there's feedback, then you know you have the potential to have a persistent response to even a transient signal. OK, so if you only have one loop of the type that I started with, where we had just one loop with an R and a P0, then the question is, as you go around the loop, do the samples get bigger or smaller, or do they stay the same? That's a fundamental characterization of how the simple feedback system works. So here, if on every cycle the amplitude of the signal diminishes by multiplication by half, that means that the response ultimately decays. Mathematically, it goes on forever, just like I said previously, but the amplitude is decaying. So practically, it stops after a while. It becomes small enough that you lose track of it. By contrast, if every time you go around the loop, you pick up amplitude, if the amplitude here were multiplied by 1.2, then it gets bigger. So the idea then is that you can characterize this kind of a feedback by one number. We call that number the pole. Very mysterious word. I won't go into the origins of the word. For our purposes, it just simply means the base of the geometric sequence that characterizes the response of a system to the unit sample signal. So here I've showed an illustration of what can happen if p is 1.5, p is 1, p is 1.2. What you can see is decay, persistence, divergence. Can you characterize this system by p0? And if so, what is p0? Yes, no. Yeah, so virtually everybody's getting the right answer. The right answer is 2. So we like algebra. We like negative numbers. So we're allowed to think about poles being negative. In fact, by the end of the hour, we'll even think about poles having imaginary parts. But for the time being, this is fine. If the pole were negative, what that means is the consecutive terms in the unit sample response, the response of the system to a unit sample signal, the unit sample response, the unit sample response can alternate in sign. OK, so this then represents all the possible behaviors that you could get from a feedback system with a single pole. If a feedback system has a single pole, the only behaviors that you can get are represented by these three cartoons. If p0 is bigger than 1, so here this z-axis contains all possible values of p0. If p0 is bigger than 1, then the magnitude diverges and the signal grows monotonically. If the pole is between 0 and 1, the response is also monotonic, but now it converges towards 0. If you flip the sign, the relations are still the same, except that you now get sign alternation. So if the p0 is between 0 and minus 1, which is here, the output still converges because the magnitude of the pole is less than 1. But now the sign flips. And if the pole is below minus 1, then you get alternation, but you also get divergence. The important thing is we started with a simple system, and we ended up with an absolutely complete characterization of it. This is everything that can happen. That's a powerful statement. When I can analyze a system, even if it's simple, and find all the possible behaviors, I have something. So if you have a simple system with a single pole, the this is all that can happen. There might be offsets. There might be delays. The signal may not start until the fifth sample, but the persistent signal will either grow without bounds, decay to 0, or do one of those two with alternating sign. That's the only things that can happen. Which, of course, begs the question, well, what if the system is more complicated? OK, so here's a more complicated system. This system cannot be represented by just one pole. In fact, this system's complicated enough you should think through how you would solve it. You should all be very comfortable with this sort of thing. So if you were to think about what if I had a system like so, and I want it to be 1.6 at minus 0.63, what would be the output signal at time 2 if the input were a unisample signal? As with all systems that we're going to think about, we have to specify initial conditions. The simplest kind of initial conditions we could think about would be rest. If I thought about this system at rest, then the initial outputs of the r's would be 0. That's at rest. For times less than 0, the input would be 0. 0 times 1.6 plus 0 times minus 0.63 plus 0 would give me 0. Now the clock ticks. When the clock ticks, it becomes time 0. At time 0, the input is 1. This 0 just propagated down to here, but this was 0, so nothing interesting happened at the r's. But now my output is 1. Now the clock ticks. What happens? When the clock ticks, this 1 propagates down here. This 0 propagates down here, but that was 0. This 1 goes to 0 because the input's only 1 at time 0. So what's the output? 1.6. Now the clock ticks. What happens? Well, this 1 comes down here. This 1.6 comes down here. This 0 becomes another 0 because the input has an infinite stream of 0's after the initial time. So what's the output? Well, it's 1.6 times 1.6 plus 1 times minus 0.63. So the answer is number 3. Yep, OK. I forgot to write it up there. So the answer is in red down here. 1.6 squared minus 0.63. The point is that it's slightly more complicated to think about than the case with a single pole. And in fact, if you use that logic to simply step through all the responses, you get a response that doesn't look geometric. The geometric sequences that we looked at previously either monotonically increased, monotonically decreased towards 0, or give one of those two things an alternated. This does none of those behaviors. So the point is, Freeman's an idiot. He spent all that time telling us what one pole does, and now two poles does something completely different. So the response is not geometric. The response grows and then decays. It never changes sign. It does something completely different from what we would have expected from a single pole system. As you might expect from the theory of lectures, that's not the end of the story. So the idea is to now capitalize on this notion that we can think about operators as algebra. If our expressions behave like I told you they did last lecture, if they behave as entities upon which, if they are isomorphic with polynomials, as I said, then there's a very cute thing we can do with this system to make it a lot simpler. The thing we can do is factor. If we think about the operator expression to characterize this system, the thing that's different is that there's an R squared. But if our operators work just like polynomials, you can factor polynomials. That's the factor theorem from algebra. And if I factor it, I get two things that look like first order systems. Well, that's good. The factored form means that I can think about this more complicated system as the cascade of two first order systems. Well, that's pretty good. In fact, it doesn't even matter what order I put them in, because as we've seen previously, if the system started at initial rest, then you can swap things because they obey all the principles of polynomials, which include commutation. So what we've done then is transformed this more complicated system into the cascade of two simple systems. And that's very good. And even better, we can think about the complicated system as the sum of simpler parts. And that uses more intuition from polynomials. If we have one over a second order polynomial, we can write it in a factored form here. But we can expand it in what we call partial fractions. We can expand this thing in this sum. And if you think about putting this over a common denominator and working out the relationship, this difference, 4.5 over 1 minus 0.9r minus 3.5 over 1 minus 0.7r, that's precisely the same using the normal rules for polynomials. That's precisely the same as that expression. But the difference from the point of view of thinking about systems is enormous. We know the answer to that one. That's the sum of the responses to two first order systems. So we can write that symbolically this way. We can think about having a sum system that generates this term. This term is a simple system of the type that we looked at previously that then gets multiplied by 4.5. I'm just factoring again. I'm saying I've got something over something, which means that I can put the something in each of two different parts of two things that I multiply together. And I can think about this as having been generated by this system, and you just add them together. The amazing thing is that that says that despite the fact that the response looked complicated, it was in fact the sum of two geometrics. So it wasn't very different from the answer for a single pole. What I've just done is amazing. I've just taken something that had you studied the difference equations, and if you had studied the block diagrams, it would have been very hard for you to conclude that something this complicated has a response that can be written as the sum of two geometrics. By thinking about the system as a polynomial in R, it's completely trivial. It's a simple application of the rules for polynomials that you all know. So what we've shown then is that this complicated system has a way of thinking about it as just two of the simpler systems. The complicated response that grew and decayed, that's just the sum difference, really. 4.5 minus 3.5. It's the weighted difference of a part that goes like 0.7 to the n and a different part that goes like 0.9 to the n. So what we've done is we've showed that in fact, so far we've got two results. The n equals 1 case, the first order polynomial in our case, the one pole case, that's trivial. It's just a geometric sequence. The response is just this geometric sequence. If it happens to be second order, this is second order because when you write the operator expression, the thing in the bottom, the polynomial in the bottom, is second order, second order polynomial in R. The second order system has a response that looks like two pieces. Each piece looks like a piece that was from a first order system, and in fact, that idea generalizes. If we have a system that can be represented by linear difference equation with constant coefficients, that will always be true if the system was constructed out of the parts that we talked about, adders, gains, delays. If the system is constructed out of adders, gains, and delays, then it will be possible to express the system in terms of one difference equation. General form is showed here. y of n can be constructed out of parts that are delayed versions of y and delayed versions of x. If you do that, then you can always write the operator that expresses the ratio between the output and the input as the ratio of two polynomials. That will always be true. So this now is the generalization step. We did the n equals 1 case. We did the n equals 2 case. And now we're generalizing. We will always get, for any system that can be represented by a linear difference equation with constant coefficients, we can always represent the system functional in this form. Then, just like we did in the second order case, we can use the factor theorem to break this polynomial in the denominator into factors. That comes from the factor theorem in algebra. Then we can re-express that in terms of partial fractions. And what I've just showed is that, in the general case, regardless of how many delays are in the system, if the system only has adders, gains, and delays, I can always express the answer as a sum of geometrics. That's interesting. That means that if I knew the bases for all of those geometric sequences, I know something about the response. The bases are things we call poles. If you knew all the poles, you know something very powerful about the system. For every pole, so every one of the factors corresponds to a pole. And by partial fractions, you'll get one response for each pole. The response for each pole goes like pole to the n. You know the basic shape. You don't know the constants, but you know the basic shape of the response just by knowing the poles. We can go one more step, which makes the computation somewhat simpler. I use the factor theorem. Here I'm using the fundamental theorem of algebra, which says that if I have a polynomial of order n, I have n roots. The poles are related to the roots of the R polynomial. The relationship is take the functional, substitute for R 1 over z, re-express the functional as a ratio of polynomials in z. The poles are the roots of the denominator. So, recapping. I started with a first order system. I showed you how to get a second order system. I showed that in general, you can use the factor theorem to break down the response of a higher order system into a sum of responses of first order systems. Now I've showed that you can use the fundamental theorem of algebra to find the poles directly. And then by knowing the poles, you know each of the behaviors, the monotonic divergence, monotonic convergence, or alternating signs. And so here's the same example that I started with, worked out by thinking about what are the poles. The poles are 0.7 and 0.9, which we see by a simple application of the fundamental theorem of algebra. We got a long way by just thinking about operators as polynomials. We haven't done anything that you hadn't done in high school. Polynomials are very familiar. And we've made an isomorphism between systems and polynomials. OK, make sure you're all with me. Here's a higher order system. How many of these statements are true? 0, 1, 2, 3, 4, or 5? Talk to your neighbor, get an answer. OK. So how many are true? 0, 1, 2, 3, 4, or 5? Oh, come on. Blame it on your neighbor. You weren't talking, but I didn't hear you not talk. How many are true? 0, 1, 2, 3, 4, or 5? Raise your hands. They can't all be true. Are they mutually contradictory? None of the above. That sounds like they're. OK, so you've eliminated one. Which one's true? How many statements are true? Looks like about 75% correct. What should I do? How do I figure it out? What's my first step? What do I do? Operators. Operators. Absolutely. So turn it into operators. So take the difference equation, turn it into operators. The important thing to see is that there are three y terms. Take them all to the same side, and I get an operator expression like that. The ones that depend on x, there are two of them. That's represented here. The thing that is critical for determining poles is figuring out the denominator. The poles are going to come from this one. After I get the ratio of two polynomials in R, I substitute 1 over z for each R. So for this R, I get 1 over z, for this R squared, I get 1 over z squared. Then I want to turn it back into a ratio of polynomials in z. So I have to multiply top and bottom by z squared. And when I do that, I get this ratio of polynomials in z. So the poles are the roots of the denominator polynomial in z. The poles are minus 1 by 2 and plus 1 by 2. So the unit sample response converges to 0. What would be the condition that that represents? Something about the polynomial in the bottom. Would all second order systems have that property, that the unit sample response would converge to 0? AUDIENCE MEMBER 2<|es|><|translate|> Louder? Absolute value of the poles has to be less than 1. If the magnitude of the poles is less than 1, then the response magnitude will decay with time. So that's true here, and it would be true so long as none of the poles have a magnitude exceeding 1. There are poles at 1 by 2. No, that's not right. It's minus 1 by 2. There's a pole at 1 by 2. No, that's not right. There's a pole at minus 1 by 2. There are two poles. Yes, that's true. None of the above. No, that's not true. So the answer was 2. Everybody's comfortable? We've done something very astonishing. We took an arbitrary system and we figured out a rule that lets us break it into the sum of geometric sequences. The response we can always write, the response to a unit sample signal, we can always write as a weighted sum of geometric sequences. And the number of geometric sequences in the sum is the number of poles, which is the order of the operator that operates on y. So we've done something very powerful. There's one more thing that we have to think about, and then we have a complete picture of what's going on. Think about when you learn polynomials. One of the big shocks was that roots can be complex. What would that mean? What would it mean if we had a system whose roots, whose poles, were complex valued? So first off, does such a system exist? Well, here's one. I just pulled that out of the air. If I think about the functional 1 over 1 minus r plus r squared, if I convert that into a ratio of polynomials in z and then find the roots, I find that the roots have a complex part. The roots are half plus or minus root 3 over 2j. So there's a complex part. There's an imaginary part. So the question is, what would that mean? Or is perhaps that system just meaningless? Well, complex numbers work in algebra, and complex numbers work here, too. So the fact that a pole has a complex value in the context of signals and systems simply means that the pole is complex, that the base of the geometric sequence, that base is complex. So that means that we can still rewrite the denominator, which was 1 minus r plus r squared. We can rewrite that denominator in terms of a product of two first-order r polynomials. The coefficients are now complex. But it still works. The algebra still works. That has to work, because that's just polynomials. That's the way polynomials behave. So we can still factor it. We can still use the factor theorem. In fact, we can still use the fundamental theorem of algebra to find the poles by the z-trick. That's fine. We can still use partial fractions. All of these numbers are complex. But the math still works. The funny thing is that it implies that the fundamental mode, I mean the simple geometric for the case of a first-order system, more complex behaviors for higher order systems, the mode is the time response associated with a pole. So the modes are, in this case, complex sequences. So if I think of the, so in general, the modes look like p0 to the n. Here my modes are simply complex, have a complex value. So what did I say they were? So the poles were 1 by 2 plus or minus. So my modes simply look like that. Same thing. The strange thing that happened was that those modes, those geometric sequences, now have complex values. The first one up here, if I just look at the denominator, these coefficients mean that it's proportionate to the mode associated with this, which is that, which has a real part, which is the blue part, and the imaginary part, which is the red part. The other pole, there were two poles, plus and minus. The other pole just flips the minus sign, the imaginary part, sorry. So if I have imaginary poles, all I get is complex modes, complex geometric sequences. And a way of thinking about that, an easier way of thinking about that, is thinking about, so when we had a simple real pole, we just had p0 to the n. That's easy to visualize, because we just think about each time you go from 0 to 1 to 2 to 3, it goes from 1 to p0 to p0 squared to p0 cubed. Here, when you're multiplying complex numbers, it's easier to imagine that on the complex plane. Think about the location of the point 1. Think about the location of the point p0. Think about the location of the point p0 squared. And in this particular case, where the pole was half plus or minus square root of 3 over 2 times j, this would be pole to the 0. This is pole to the 1. This is pole squared, pole cubed. As you can see, when you have a complex number, the trajectory in complex space can be complicated. In this case, it's circular. The circular trajectory in the complex plane corresponds to the sinusoidal behavior in time. So there's a correlation between the way you think about the modes evolving on the complex plane and the way you think about the real and imaginary parts evolving in time. It seems a little weird that the response should be complex. We're studying this kind of system theory primarily because we're trying to gain insight into real systems. We want to know how things like robots work. How's the wall finder work? What would it mean if the wall finder went to position 1 plus the square root of 3 over 2j? That doesn't make sense. So there's a little bit of a strange thing going on here. How is it that we need complex numbers to model real things? That doesn't seem to sound right. Well, the answer is that if the difference equation had real coefficients as they will for a real system, if you think about a real system like a bank account, the coefficients in the difference equation are real numbers, not complex numbers. If you think about the wall finder system, the coefficients in the wall finder system, the coefficients of the difference equations, the coefficients of the difference equations that describe the wall finder behavior, we're all real numbers. If you have a polynomial, here I'm thinking about the denominator polynomial. If we try to find the roots of a polynomial, and if we find a complex root, if the coefficients were all real, it follows that the complex conjugate of the original root is also a root. That's pretty simple. If you think about what it means to be a polynomial, if you think about a polynomial is whatever. So I've got 1 plus z plus z squared plus blah, blah, blah. If all of the coefficients 2, 3, minus 16, if all of those coefficients were real, then the only way that if z were a root of this polynomial, then p star would have to be a root 2, because the complex conjugate, if you complex conjugate each of the z's, it's the same thing as complex conjugating the whole thing, because the coefficients are real valued. So the idea then is that if I happen to get a complex root for my system that can be described by real value coefficients, it must also be true that its complex conjugate is a root. If that happens, the two roots co-conspire so that the modes have canceling imaginary parts. You can prove that. I'm not worried about you being able to prove that. I just want you to understand that if you have two roots that are complex conjugates, they can conspire to have their imaginary parts cancel. And that's exactly what happens. Here in this example, the example that I started with, where the system was 1 over 1 minus r plus r squared, the unit sample responses showed here. You can write that as the sum of sinusoidal and cosinusoidal signals. And the sum that falls out has the property that the imaginary parts cancel. It's still useful to look at the imaginary parts, the same as it is when you're trying to solve polynomials. It's useful because the period of all of these signals is the same. If I think about the period of the individual modes, if I think about the period of p0 to the n, 1 over root 3 over 2j to the n, the period of that signal I can see in the complex plane, this is the n equals 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12. The period is 6. If I were to take the negative one, I would go around the circle the other way. This would be the 0 term, 1, 2, 3, 4, 5, 6, et cetera. Both of the modes, the geometric sequence associated with the two complex conjugate poles, both of those modes have the same period, they're both 6, as does the response to the real part. So you can deduce the period of the real signal by looking at the periods of the two complex signals. So think about this system. Here I've got a system whose response is showed here. I'm going to tell you that the response was generated by a second order system, that is to say a system whose polynomial was second order, whose polynomial in R was second order. Which of these statements is true? I want to think about the pole as being a complex number. Here I'm showing you the complex number in polar form. It's got a magnitude and an angle. And I'd like you to figure out what must the magnitude have been and what must the angle have been. So what's the utility? Why did I tell you the pole in terms of its magnitude and angle rather than telling it to you in its Cartesian form as a real and imaginary part? What's good about thinking about magnitude and angle? Well, if I break the pole into magnitude and angle, and if I think about the modes, the modes are always of the form p0 to the n. If I think about modes as having a magnitude and an angle, when I raise it to the n, something very special happens. What happens? You can separate it. What is Re to the j omega raised to the n? That's the same as r to the n, e to the j n omega. It's the product of a real thing times a very simple complex thing. What's simple about the complex thing? The magnitude is everywhere 1. e to the j, the magnitude of that term. So all of the magnitude is here. None of the magnitude is here. All of the angle is here. None of the angle is here. I've separated the magnitude and the angle. So it's very insightful to think about poles in terms of magnitude and angle, because it decouples the parts of the mode. So I can think then of this complicated signal that I gave you as being the product of a magnitude part and a pure angle part. From the magnitude part, I can infer something about r. r is the ratio of the nth one to the n minus 1th one. So r is a lot bigger than 1.5. In fact, r in this case is 0.97. And this is pure angle. This lets me infer something about the oscillations. In fact, I can say something about the period. The period is, here's a peak, here's a peak. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12. The period is 12. If the period is 12, what is omega? Well, omega is a number that by the time I've gone to n times it, I got up to omega is a number such that by the time I got up to 12 times it, I got up to 2 pi. What's omega? 2 pi over 12. So it's about 1.5. So that's a way that I can infer the form of the answer. I ask you what pole corresponds to this behavior. Well, the decay, that comes from the real part. That comes from r to the n. The oscillation, that comes from the imaginary part. And I can figure that out by thinking about the period and thinking about that relationship. So I know then that the answer is, oops. So the answer then is this one. r is between 1 and 1, and omega is about 1.5. OK, one last example. Whoops, wrong button. One last example. So what we've seen is a very powerful way of decomposing systems so that we can always think about them in terms of poles and complex geometrics, which we will call poles and modes. And that behavior works for any system in an enormous class of systems. So I want to think about one last one, which is Fibonacci sequence. You all know the Fibonacci sequence. You've all programmed it. We started with that when we were doing Python. We made some illustrations about recursion and all that sort of thing by thinking about it. Now I'm going to use signals and systems to do the same problem. So Fibonacci was interested in population growth, how many pairs of rabbits can be produced from a single pair in a year. If it is supposed that every month each pair begets a new pair from which the second month it becomes productive. OK, it's not quite the same English I would have used. From this statement, you can infer a difference equation. I've written it in terms of x. What do you think x is? x is the input signal. And here I'm thinking about x as something that specifies the initial condition. This is the thing I alluded to earlier. One trick that we use to make it easy to think about initial conditions is that we embed them in the input. So in this particular case, I'll think about the initial condition arising from a delta function. If I think about x as a delta, then the sequence of results, y0, y1, y2, y3 from this difference equation, is the conventional Fibonacci sequence. It would correspond to what if you had a baby rabbit, one baby rabbit, that's the 1, in generation 0, that's the delta. So the input is a way that I can specify initial conditions, and that's a very powerful way of thinking about initial conditions. So here's the problem. I've got one set of baby rabbits at time 0. They grow up. They have baby rabbits, which grow up. At the same time, the parents had more baby rabbits, at which point more babies grow into bigger rabbits, and big rabbits have more babies, et cetera, et cetera, et cetera, et cetera, et cetera. So the question is, so you all know that, right? You all know about Fibonacci sequence. It blows up very quickly. What are the poles of the Fibonacci sequence? The difference equation looks just like the difference equations we've looked at throughout this hour and a half. We can do it just like we did all of the other problems. We write the difference equation in terms of r. We rewrite the system functional in terms of a ratio of two polynomials in r. We substitute r goes to 1 over z, deduce a ratio of polynomials in z, factor the denominator, find the roots of the denominator, and find that there are two poles. The poles for the Fibonacci sequence are 1 plus or minus the root of 5 over 2. Now that's curious. There's no recursion there. It's a different way of thinking about things. There are two poles. The first pole, the plus 1, 1 plus the root of 5 over 2, corresponds to a pole whose magnitude is bigger than 1. It explodes. There it is. The second one is a negative number. So the first one is the golden ratio, 1.618... The second one is the negative reciprocal of the golden ratio, which is minus 0.618... And those two numbers amazingly conspire so that their sum, horrendous as they are, is an integer. And in fact, that's the integer that we computed here. So we've used Fibonacci before to think about the way you structure programs, iteration, recursion, that sort of thing. Here, by thinking about signals and systems, we can think about exactly the same problem as poles. There's no complexity in this problem. It doesn't take n or n squared or n log n or anything to compute, it's closed form. The answer is pole 1 to the n plus pole 2 to the n. That's it. That's the answer. So what we've done is we found a whole new way of thinking about the Fibonacci sequence in terms of poles. And more than that, we found that that way of thinking about poles works for any difference equation of this type. And we found that poles, this way of thinking about systems in terms of polynomials, is a powerful abstraction that's exactly the same kind of PCAP abstraction that we used for Python."
    },
    {
        "Rec 5 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-O6HHjiNKsco.mp3": " Hi. Today I'd like to talk to you about a new module called Signals and Systems. Our previous module on programming, we focused on object-oriented programming and state machines as different methods by which we could model the physical world. In this module, we're going to talk about a new way to model the physical world called the discrete linear time invariant system. You might say, Kendra, why do I need to do that? Or why would I move away from state machines? I seem to be able to model everything using state machines. The short answer is, you want to be able to move away from state machines because you want to be able to predict the future. But I'm not really going to be able to get to how you're going to be able to predict the future until two videos from now. So for now, we'll introduce discrete linear time invariant systems and also talk about different knowledge representations you might encounter them in so that you recognize them when you see them and can manipulate them as needed to talk to other people about them. Let's go to the board. Last time we talked about state machines. We're able to use them to model pretty much any system. We can model the evolution of a particular process over time using a record of all previous inputs and outputs and possibly previous states if we included that in our output. But the problem with state machines is that they, like most programming, encounter the halting problem. At some point, your program can reach a level of complexity at which you cannot determine what it's going to do without actually just running it and seeing what happens. This is great for researchers. This is the method by which people go out and have to find something or discover something instead of being able to simulate it using a machine. But if you want to be able to promise things to other people or predict what's going to happen, then it helps if you can model something that has a high level of complexity by abstracting away the complexity and interacting with it as though it has features that indicate that it's going to behave in a particular way. So if you want to make a control system that does a particular thing or look at an existing system and say, well, I know that your power plant's going to blow up in five years because of this, it helps if you have some understanding of particular classes of systems and what those particular classes of systems, what kind of long-term behaviors those particular classes of systems generate. And that's why we look at discrete LTI. It's a particular class of system. In particular, discrete LTI is what we're going to look at in 6.01 because it's fairly simple. But once you've learned how to deal with the system in that manner, you can use the skills that you've learned there to approach more complex feedback problems or more complex control problems. First, an incredibly brief review of linear time and variance systems. When you're talking about a linear time and variance system, both the inputs and the outputs are going to be real, we're not going to deal with a complex plane at all. If you were to model your LTI system using a state machine, that state machine would be dependent on a fixed amount of previous inputs, outputs, or states. You give some amount of leeway for start state or the fixed number of states before you get to your fixed amount of data structure that represents your current state. But if you're running a linear time and variance system, the amount of information that you need to figure out your state in a long term sense is always finite and fixed. In terms of functional expressions, you've probably seen these associated with linear time and variance systems. Note that this also means that you can represent linear time and variance systems as additions or scalar multiplications of your existing function. That's out of the way. These are all true of LTI. We're going to focus on discrete LTI. One, because we can use a discrete state machine to model it. And two, because it makes it easier to represent using computers. Once you've got that digital abstraction, you don't have to worry about having access to a truly continuous function. All right. Now that you know what we're going to talk about, how do you talk about it to other people? Here are the different representations that you might see a discrete linear time and variance system in. One of them is called a difference equation. And you've probably encountered this in one of your math classes. It says that you're going to take a sample from a signal y at some sort of given time step. Typically n. And when you're writing out a difference equation that represents an entire system, you'll usually see y of n on one side on the left. And then everything that represents the functional component on the system on the right, which in this case determines what determines the output at every time step. In this very particular case, we're talking about an accumulator, which means that at every time step, the output is determined by the input plus the previous output. So at every time step, we take the input and whatever it is we were outputting before and output it again. Pretty straightforward. If somebody described to you a difference equation in words, you could probably turn it into an equation at this point. Note that even though there are variables associated with these samples, they're still very particular samples. And the thing that you're interested in is probably not even the samples at all. It's the relationship between the samples, when they're sampled relative to one another, and how that relates to the output at a given time step. If you then want to talk about an entire signal, then you can use an operator equation. And you'll probably see these things when you do any sort of research on control theory or feedback. Instead of representing the sample of the signal y at a given time n, we're just going to talk about the overall signal, capital Y. Likewise, instead of talking about x at a given time n, we're going to talk about the signal, capital X. Remember that I said that the most important part of this equation is the fact that there's a different relationship between these two signals and when you're sampling from them. When we want to represent this relative delay, we represent it using an r. And in particular, I want to note the fact that the degree of r represents the amount of delay associated with the sample from a particular signal. So if I wanted to make this n minus 2, I would reflect that change in my operator equation by changing the degree of r. And because we're working with a linear time invariant system, if we wanted to scale this by 2, it'd be the same as scaling this by 2, et cetera. Now I can talk about signals. What if I want to talk about a physical manifestation of the relationship between these signals? And or what if I want to use something kind of like circuit diagrams to talk about what kind of signal manipulation I'm going to do? This is where block diagrams come in. Block diagrams are incredibly PowerPoint-friendly. Block diagrams mean that everybody is on the same page because all you have to do is trace out the arrows. Block diagrams mean that you're going to do a combination of gains, delays, and adders to visually represent the relationship between your output signal and any input signals that you have. Up here I have actually drawn an accumulator. And it's got a box around it, which will be relevant in two minutes, but right now you can ignore it. I've got my input signal, or my input signals are typically on the left. And the progression of gains, delays, and adders associated with the block diagram represent the relationship between the input signals and the output signal. And then I'll have my output signal on the right. Most people will indicate flow with arrows, in part to make things easier to read. But typically you only need the arrow indicating where you're sampling from and what your output's going to look like. In this case, in order to get y, and in the general sense, I can backtrace from the signal that I'm interested in through my diagram and figure out what values I'm actually interested in. So in this particular case, y is a linear combination of x and whatever this represents, which is Ry. If I did want to put a 2 here, I've been talking about gains, delays, and adders, but this diagram doesn't contain a gain yet. So let me show you how you would include a gain. Gains are typically represented by the value of the gain inside an arrow. It looks like an op amp from schematic diagrams. We'll get to those later in circuits. It's not essential that the arrow point in the direction of flow, but people might look at you funny if you don't put it in the direction of flow. And the other thing that I think I want to note here is that if you see a minus sign here, it means x minus 2 Ry, which is the same as putting a negative value on your gain. So right now, we're back to x plus 2 Ry. OK. We got to block diagrams. You might be saying at this point, Kendra, what are block diagrams good for other than PowerPoint presentations and possibly arguing with my friends over what's going on? And I say, they're really good for abstraction. I can draw a box around this and abstract it away into a function and say, if I put something into this box and I want to get something out where the action that happens in here is identical to this schematic, then I can find an equation that actually represents that operation. And the way I do that is I'm going to take my operator equation and solve in particular for y over x. In this case, with the 2, y over x is going to be 1 over 1 minus 2 R. Note that if h is equal to the expression y over x and I multiply h by x, I get out y. This concludes my coverage of motivations for learning about discrete LTI systems and also the different representations that we'll want to use when talking to other people about discrete LTI. At this point, we can also start talking about how we get to the point where we can start predicting the future. And then in the video after that, I'll actually start talking about poles."
    },
    {
        "Lec 8 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-e7Ptvu5Vu8k.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. Hello, welcome. Welcome back from Spring Break. Today what I want to do is talk about op amps. And in particular, what I want to do is talk about modularity in circuit design. How do you make circuits that are modular? We'll see that there's special problems when you think about modularity applied to circuits. And op amps are one solution for helping us think about those problems. Before launching straight into something new, though, I'd like to recap where we are under the assumption that you may not have been thinking about this exactly currently. So last time, we took our first look at circuits. Circuits are very different from the kinds of things we thought about before. Before, in programming, in linear systems theory, we thought about blocks that had well-defined inputs and well-defined outputs. Circuits are different. Circuits are all connected together. And circuit theory is thinking about how do you organize your thoughts about complicated interactions among things. The parts interact because they touch each other and they share voltages. Because they touch each other, they share currents. And you have to somehow think about that whole complicated system. We figured out that the way you think about that, you can think about it as three separate enterprises. How do you think about voltages? How do you think about currents? And how do you think about the elements? How do you think about voltages? Well, the sum of the voltages around any closed loop is 0. Done. KVL, Kirchhoff's voltage law. How do you think about currents? Draw any surface. The net current that leaves the surface is 0. KCL, Kirchhoff's current law. How do you think about the elements? Well, it depends on the element. If you're thinking about a linear resistor, it's Ohm's law, V equals IR. If you're thinking about a source, it could be a voltage source, then V equals V0, some fixed number. If you're thinking about a current source, then current is fixed at some fixed number. So it depends when you're thinking about the element laws, it depends on what element you are thinking about. And then you just combine all of those, and you can solve the circuit. The only problem that arises is that those equations are highly redundant. There's a lot more KVL equations than you need. There's a lot more KCL equations than you need. And so in fact, the trick in analyzing a circuit is to figure out the smallest number of equations that are adequate, the number of equations that are necessary and sufficient to find a solution. And we went last time. Last time we went through three different ways to do that. We thought about what I will think of as primitive or element voltages and currents. The idea in that technique is you think about every element and assign to that element its voltage and its corresponding current, then that gives you, as in this example, if you've got six elements, that gives you six unknowns, one voltage across every element, one current through every element. So then you've got to dig up six other relationships. Sorry, start again. Six elements, six voltages, six currents, 12 unknowns. You need to come up with 12 equations. Six of them are element equations. So six are easy, then you have to come up with six more. And for this particular circuit, it turns out that there is three independent Kirchhoff's voltage law equations, and there are three independent KCL, Kirchhoff's current law equations. 12 equations, 12 unknowns, done. That's actually more work than you need to do. So we looked at two other techniques for solving the same kind of circuit. One's called node voltages and another's called loop currents. They are duals of each other. In the node voltage method, you figure out exactly the minimum number of voltages that I would have to tell you in order to specify all of the element voltages. So for example, if I told you this voltage here and that voltage there and this voltage there and that voltage there, four of them, that would let you calculate all of the element voltages. But now I only have four instead of six. Four nodes instead of six element voltages. So it's smaller. And we talked about last time how you could find the correct equations that go with those nodes, node voltages. In the loop current method, you've specified the minimum number of currents that would be sufficient to account for all of the element currents. And here, in this circuit, it required three. Three is smaller than six. There were six element currents. So again, we got a reduction in the number of unknowns. 12, 4, 3. So that's kind of the idea. And just to make sure that you're all with it, think about this problem. How many of the following expressions are correct? Feel free to talk to your neighbor. At the end of about 45 seconds, I'll ask you to raise your hand with a number of fingers, one through five, indicating which is the correct answer. Dead silence. You're allowed to talk. OK, so how many of the relations are true? Everybody raise your hand. Indicate a number of fingers that tells me how many of the answers are correct. Blame it on your neighbor. You can say anything, right? It's always your neighbor's fault. OK, that's very good. It's about 95% correct, almost all correct. This first equation, what is this? Give that a name. KVL. Is it correct? Sure. You need to figure out which path it corresponds to. The only V's are over here, right? So therefore, it must be something that has to do with this circuit. So this says that there's 1, 2, 6, and 5. That's path 1, 2, 6, and 5. So all you really need to do is check the polarities. So if we took all of the variables to the same side, then we'd have minus V1 plus V2 plus V6 plus V5. If we think about going a loop that goes through the minus V1, that would be this way, plus V2 plus 6 plus 5. So that's right. Easy. What's the point of this equation? V6 equals E1 minus E2. What's that say? Why'd I ask that? Yeah. Yeah. STUDENT 1. It's saying more to the class than it was to equals the difference between the two. PROFESSOR 1. Exactly. It's intended to make you think through the relationship between the primitive variables, the element voltages, and the node voltages. If I told you the node voltages, E1 and E2, you could trivially compute V6. You could also compute any of the other V's. If I told you to find V4, that would be E1 minus ground. We assign the number 0 to ground. So that would be E1. So the idea, the reason I asked number 2, is to make you think about how you relate the node voltages to the element voltages. How about number 3? What's that? Well, it's highly related to number 2, right? But it's different from number 2 because? Ohm's law. So equation 3 is the way Ohm's law looks when you use node equations. Ohm's law is a little bit uglier when you use node equations than when you use primitive variables. When you use primitive variables, it would have simply said, the same relationship would have simply said that V6 is R6 times I6. Because I'm using node voltages instead, the voltage across resistor 6 shows up as a difference. How about this one? What's that? True or false? This is KC out. True. This is KC out. True. OK. This is KC out. False. OK, well, let me say it's more numerous. That's not sort of 100% participation. But this is not KCL. What is that? KCL says some of the currents out of some closed path. This is a mixed thing, right? I6 is one of these things. And IB and IC is one of these things. So what is equation 4? Incorrect. Incorrect, yes. What would be the correct way of saying equation 4? The answer to such questions, according to the theory of lecture, is, go to the next slide. Here's the correct expression. Why is this correct and why is that not correct? Equation 4 is intended to be, how do you relate the new currents to the element currents? So I6 is an element current. It's the current that goes through R6. When we do loop currents, we have two currents going through R6. So in the loop current view, the total current that goes through R6 is a sum or difference of the two loop currents that go through R6. So the element current through R6, which is I6, is a sum or difference of loop currents. There are two loop currents. We have to worry about which one goes through in the correct direction. I6 goes from left to right. So when we do the loop currents, we need to take positive as the direction from left to right. Well, that's the direction of IC. And it's opposite the direction of IB. So if I want to use loop currents to specify I6, it would be IC minus IB. Everybody clear on that? So equation 4 is the relation between element currents and loop currents. So what's equation 5? Equation 5 is Ohm's law for loop currents. Ohm's law, again, if I were thinking about Ohm's law for R6, I would have V6 equals I6R6. That's the way you say it in element voltages and currents. Over here, the current through R6 is IC minus IB. So Ohm's law looks a little bit more complicated. So the point is, these three methods represent ways of figuring out a linearly independent set of unknowns and equations. They differ. The left-hand one is probably the easiest to think about, especially when you're thinking about things like Ohm's law. It's the natural way to specify the element relationships. It's the relation between the voltage and current through that part. That generally gives me a large number of equations and unknowns. You can reduce the number of unknowns by using something like node voltages or loop currents. And that gives you fewer equations to solve. They're completely equivalent. They look a little different. And the reason for talking about them is that when we think about writing a program to solve circuits automatically, which, by the way, will be the exercise in software lab this week, when we think about writing a program, writing a program is yet a different kind of challenge. What's the easiest system to automate? So the system that is easiest for you may or may not be the easiest system to automate. So that's the point of this week's software lab. We'll do a method that's closely related to the node voltage method. It's not quite node voltages. It's a little simpler than node voltages for a computer. It's a little simpler to automate. So we use a method that's called node voltages with component currents. Now I'm going to start new stuff. That was review. What I want to think about today is what is it about circuit design that makes it hard? What are the issues that make it difficult to be modular when we're thinking about the analysis and design of circuits? And one of the hardest things to deal with is the idea that in a circuit, unlike in a linear time invariant system of the type we talked about in the previous module, in a circuit, the presence of every element affects the currents and voltages through, in principle, every other element. So if you change one thing, you change everything. So first off, I want to just give an example of that. Think about what would happen if I were trying to make a circuit to control the brightness of a light bulb. So imagine that I've got this circuit, and I close the switch. Closing the switch is equivalent to adding a component. So before I close the switch, when the switch was open, I have three elements, a voltage source and two resistors. After I close the switch, I have four elements, the original three plus the bulb. So the question is, how would closing the switch affect V0 and I0? Take a minute, talk to your neighbor, figure out how V0 and I0 change. Do you guys get it? OK. So what's the answer? Everybody raise your hand, show a number of fingers equal to the answer. That's very good. It's 95% correct at least, maybe 100. OK, so the answer is two. Why is the answer two? How do I figure that out? Yeah? So the torr resistance of the circuit is going to decrease because you're adding it in parallel. And then so V0 is going to decrease because it's going to have a torr component of resistance, and because that decreases, you have I0. Yeah, so I think that's exactly correct. So the idea was that when you add a component, let's think about the light bulb being a resistor. OK, that's kind of pulled out of thin air, but I sort of suggested that you might do that here. Think about representing the light bulb as a resistor. Then when you close the switch, these two resistors go in parallel. When you combine two things in parallel, the result is the same as a resistance that has a smaller value. And then think about how that smaller value would interact with this resistor and that source. And you can sort of figure out that the presence of this bulb would reduce this voltage and increase that current. That's kind of a high level of reasoning given where we are. If you wanted to think through this a little more step by step, it's easy. You could think about figuring out what are the voltages and currents before and after you close the switch. Before you close the switch, you can just ignore this. And you can calculate V0 just from voltage divider relationship. Right, that's clear. So you can see that when the switch is open, V0 is going to be 8 volts. And when the switch is open, you can figure out the I0 by lumping these two resistors together to make a 3 ohm resistor. And you see that I0 is 4 amps. Then to figure out what happens when you close the switch, just repeat. And the algebra is a little more tedious. I won't try to go through it. By the way, the slides that I show in lecture are always posted on the web. So these slides that have the answers, they are there. So on the web, there are two handouts for lecture. There's an electronic version of the thing that we handed out, there's also an electronic version of my slides, so everything that I showed is there. I'm not going to go through the tedious algebra, it's just tedious algebra. But if you go through the tedious algebra, you find that if you represent this bulb by a resistor R, V0 becomes an expression that looks like this. If you think about physical resistors, if you think about light bulbs being represented by a physical resistor, then the physical resistor has to have a resistance between 0 and infinity. And if you think about that expression, what could the value be if R varied between 0 and infinity? That expression is always less than or equal to 8 volts, showing that V0 went down when you closed the switch. Similar tedious algebra leads to an expression like this in R. And if you think about how this would change as R goes from 0 to infinity, you see that that's always bigger than 4 amps. So that means I0 goes up. So the point is, you can think through this in a more sophisticated way, and we hope by the end of the course you all be able to do that. Or you can think through it in terms of just solving the circuit. Solve it in two cases, when the bulb is there and when the bulb is not. Either way, the answer is 2. That V0 went down and I0 went up. The point is that when I added the element, currents that were far away from the element still changed. And that's a general way circuits interact. They're all connected. So the idea, the point of doing this, is that the addition of a new element changed the voltages and currents through the other elements. And that's a drag if what I was trying to do, for example, was design a brightness controller for the flashlight bulb. Imagine that what I really wanted to do was use a voltage divider to make 8 volts. And what was in my head was, I'd like that 8 volts to be across the light bulb. That's a good idea. It just won't work. At least it won't work the way this circuit worked. If I just build it like so, there's an interaction between the bulb and the voltage divider circuit. So the voltage divider circuit is no longer a voltage divider. After I've in this circuit, when I close the switch, current flows in this wire. The rule in a voltage divider is the same current has to flow through both resistors. If the same current flows through two resistors, then you can use the voltage divider relationship to see how voltage partitions between the two resistors. If current gets siphoned off the node between the two resistors, you can't use the voltage divider relation anymore. That violates the premise of the voltage divider relation. So when I close the switch, this is no longer a voltage divider, and it no longer works like a voltage divider. Is that clear? So what I'd really like is some magic circuit that I can put here that isolates the effect of the bulb on the effect of the rest of the circuit. And that's exactly what an op-amp does. That's what we're going to talk about next. So this magic circuit is something that we'll call a buffer. A buffer is a thing that isolates the left from the right. So the buffer is going to be something that measures the voltage on this side and magically generates that voltage over here without changing this side. So what we want to do now is develop some thought tools for how you think about op-amps. Op-amps are different. And if you just look at the picture, op-amp has to be different because there's too many terminals. It's not like a resistor that has two legs. It's not like a V source which has two legs. It's not like an I source which has two legs. It has three legs. In fact, I haven't drawn all the legs. There's more than three. There's at least five. So they're different. And the way we think about them are different. The key to thinking about the way an op-amp works is to think about a new class of elements called controlled elements. So a controlled element is an element whose voltage current relationship depends somehow on a voltage and current measured someplace else in the circuit. As an example, think about a current controlled current source. That's depicted here. This current source, I would normally write a current source with a circle. That means it's an independent current source. That means that the current is fixed. The little diamond thing is my way of representing the idea that the amount of current that comes out of this current source depends on something else. In this case, it depends on Ib. And Ib happens to be a current that flows in that circuit. So the idea is that the current in the current source depends on some other current. It's a current controlled current source. It's a current source whose current is controlled by another current. Got it? So we'll see. Figure out for this current controlled current source circuit, what's the ratio of v out over v in? So what's the answer? What's the ratio of v out over v in? 100%. Wonderful. The answer is 4. Easy to get. It's easy to get because I rigged this question to be easy. I rigged it to be easy because you can sort of figure out everything is going on the left. And then you can figure out everything is going on the right. And so there's no sort of complicated coupling between the two. So what's going on on the left? Well, you can solve for Ib. Ib is just vi over 1,000 ohms. Then you can take that value of Ib and use that to solve for what's going on over here. Over here, v out is this current, 100Ib times this resistance, 5 ohms. But we just found out that Ib is vi over 1,000 ohms. Substitute it in, you get half vi. So the answer is number 4. The ratio of v out to v in is 1.5. So the point is these are a different kind of element, controlled sources, dependent sources. But they're not too hard to work with. They sort of look different structurally. So think about what's going on here. The controlled current source, the current controlled current source, I have to think of that as a box. Because the current source has to know the value of Ib. So somehow, this box, this thing that's doing this current controlled current source, it knows about Ib. And it knows about the current source. So they're somehow linked, so that's why I put a box around it. And then think about the equations that characterize that component. So now we've got a component that's got four wires coming out of it. The components we had before had two wires coming out of them, resistors, current sources, voltage sources. This kind of a component has four wires coming out of them. We call this kind of a component a two-port. Because we think of the left port and the right port. That's compared to a resistor, which we would call a one-port. So we think about this being a two-port. And there's now two equations. So now I've got, for that two-port, I've got two voltages and two currents. There's the voltage across the left part, and there's the voltage across the right part, and there's the current through the left part, and there's the current through the right part. So with the elements we thought about before, I had one voltage across it and one current through it. Now I've got two voltages across it and two currents, kind of twice as big. Not surprisingly, it takes twice as many equations. Ohm's law was a single equation for one resistor, a one-port. V equals V0 was one equation for one component, a voltage source. Here I've got one component that has four wires, four unknowns, and I get two equations. So for this particular dependent source, I know that the voltage across this pair of terminals is 0, because it's essentially a short circuit between the two. Short circuit just means connected with a wire. And I know that the current, I2, is related to the current over there this way. So the idea then is that this current-controlled current source can be represented by a two-port, two voltages, two currents, related by two equations. It's kind of structurally twice as difficult to think about as a one-port. Functionally, it's different from having two one-ports, because the two one-ports are coupled. And that's the important part, it's the coupling between the two, that you can't model with a simple resistor and a simple constant source. OK, so when we think about an op-amp, a good first model for an op-amp is to think about it as a voltage-controlled voltage source. And so that's depicted here. I want to think about the op-amp, which I'll symbolically write this way. This means something that has two inputs, a plus input and a minus input, and a single output, V out. I can think about it as a voltage-controlled voltage source. So this I mean to be the element representation. This is how I'll draw it when I make a schematic diagram of a circuit. This is the functional form. This is the way I'll think about it when I'm analyzing it. I'll think about the op-amp as being a voltage-controlled voltage source. This voltage source adopts a voltage that is some number k times the difference voltage, V plus minus V minus. And the trick in op-amps is that k is typically a very big number, typically bigger than 10 to the fifth. We'll see in a minute why that's a frightfully useful way to think about, a frightfully useful component. So let's just walk through an example to see how you would solve a circuit that has this kind of a voltage-controlled voltage source in it. So think about this circuit where I'm applying a voltage to the plus lead of an op-amp. And I'm wrapping the output back through the minus lead through two resistors. And what I want to do is analyze that circuit by thinking about the op-amp as a voltage-controlled voltage source. So I can see just by the way it's wired up that the voltage at the plus lead, so I'm going to be thinking node voltages. Node voltages tend to be easy. I'm thinking node voltages, so I'm going to define all my voltages referenced to a ground. So I'll call this node ground. That's what the funny symbol means. Then this node voltage, the voltage at the plus terminal, is just the same as the input voltage. This voltage at the minus terminal looks like a voltage divider relationship. If you look at my model, it's very clear that I1 is 0, because there's no connection here between the V plus and the V minus. So I1 is 0. That means the total current that flows into the plus lead of the op-amp is 0. The total current that flows into the minus lead of the op-amp is 0. So because there's no current flowing in the plus and minus leads, I can calculate the voltage at this minus terminal as a voltage divider. And it's just R1 over the sum of R1 and R2 times V9. Then according to the voltage-controlled voltage source model, V out should be K times the difference between the V plus and V minus. So I just substitute in for V plus this guy, V i, and for V minus this guy. Do some algebra. I get this expression after some algebra. And then I say, yeah, but I know that K is really big. So if K is really big, Kr1 is big compared to R1. So I can ignore that. In fact, K is so big that for any reasonable choice of R1 and R2, Kr1 is even bigger than R2. So that means this reduces to this kind of a fraction. So the response of this circuit, this op-amp circuit, so what did I do? I just took the op-amp circuit, and I modeled it as a voltage-controlled voltage source, plugged through the equations, and found out that the ratio of the output voltage to the input voltage is R1 plus R2 divided by R1. So the idea then is that this simple circuit works like an amplifier. It's an amplifier in the sense that I can make the output voltage bigger than the input voltage. That's a very useful thing. In fact, you'll find useful ways to use that when you do the design live this week. So this is an amplifier. So here's a question. Make sure you follow what I just did. How could I choose the components R1 and R2 so that I make V out equal to Vi? OK. OK. There's no wire connecting. So there's nowhere for current to go. OK, so how would I choose the components R1 and R2 in order to make the output voltage equal to the input voltage? Wonderful. So the idea is that all of these manipulations have the same effect. All you need to do is look at the expression that we developed. The expression was R1 plus R2 over R1. If you substitute R1 goes to infinity, R2 equals 0, or the two at the same time, you get 1 in all of those cases. So that's a way that you can turn this amplifier circuit that, in general, would make the output bigger than the input into something that makes the output equal to the input. OK, now I want to turn to a simplification. I just dragged you through the math, thinking about the op amp as a voltage control voltage source. There's actually a shortcut. The shortcut is something we call the ideal op amp. So what I want to do in this slide is drag you through the math one more time, but then we're done. The idea is that if you have an ideal op amp, if you have an op amp, a voltage control voltage source, if you represent an op amp as a voltage control voltage source, and if k is very big, the effect will be to make the difference between the positive and negative terminals of the op amp quite small. You can see that here by way of a simple example. So let me think about this case with, again, the voltage control voltage source model. So here I've got V out according to the voltage control voltage source model. V out should be k times the difference between the two inputs. The positive input is clearly V i. The negative input is V o. One equation, two unknowns, solve for the ratio. The ratio of V out to V i is k over 1 plus k. That's the answer. Take that answer and back substitute to figure out how big was the difference between V plus and V minus. That's just algebra. And what you see is that if this is the answer, then V plus minus V minus can be written as a fraction of V i or a similar fraction of V o. k is big. k is essentially the same as k plus 1. k is in the denominator of both. What that says is the difference between the plus and the minus leads, the voltage between the plus and the minus leads, is very small if k is very large. So we call that the ideal op-amp relationship. The utility of that is that it makes solving the op-amp circuits much, much easier than what we've just been doing. I've been solving the circuits by thinking about the op-amp as a voltage controlled voltage source. That's fine. But if I additionally know that k is very big, I can shortcut it. I can say, look, the effect of the op-amp's going to be to make the positive and negative inputs the same. If it didn't do that, think of what it would mean if k is a big number and if the output voltage is k times the difference, if the difference is anything other than epsilon, V out has to be infinity. I mean, if k is very big, right? If k is very big, the only way the output could be some reasonable number, like a volt, would be if the difference between V plus and V minus is very small. OK, well, let's work backwards. Let's start with the assumption that V plus minus V minus is very small. And that lets us solve the circuits very quickly. So for example, the same circuit that took previously a few lines to get the answer to, if I just take as a rule that V plus has to be V minus, it's a one step. V plus equals V minus, OK, V i equals V o, period, done. It's a very simple way to think about the answer to an op-amp circuit. So if the op-amp can be represented by a voltage controlled voltage source and if k is very large, then V plus is roughly V minus. Shortcut, ideal op-amp assumption. So use that or ignore it, depending on what your mood is, and figure out the voltage relationship for this slightly more complicated circuit. So what's the answer? Yes? No. How many are done? How many are not done? OK, take a minute. This is supposed to be easy. Think ideal op-amp. OK. OK. OK, what's the output? Everybody raise your hand. What's the output? More hands, more hands, more hands, more hands. OK, tiny number of hands, but those who showed hands are about 100% correct. I don't know how to grade that, right? Small participation, 100% among those who did participate. So how do I think about this? What's step one? OK, according to the theory of lectures, what is step one? Look at the previous slide, yes? What was the previous slide? Previous slide, Henry with ideal op-amps. What's that in your author's day? V plus equals V minus. OK, V plus equals V minus. What happens here if V plus is equal to V minus? Well, what's V plus? So what's V minus? Zero. So if V minus is zero, what do I do now? That's hard, huh? Well, I've got a union here in which three different currents can flow. How big is the current that can come into this node? What's the sum of all the currents that can come into that node? Well, one could come in this way. One could come in that way. One could come in that way. How much current flows in the minus lead of the op-amp? None. No current goes into the minus. No current enters the op-amp through the minus lead. So it's the sum of three currents. How big is this current? How big is the current that flows in that lead? V1, V1 over 1, right? Ohm's law. So this node is zero. That node is V1. The voltage across this resistor is V1 minus zero. The voltage across is V1. The current is V1 over R. R is 1. So the current that flows in this leg is V1. How much current flows in this leg? How much current flows in this leg? V0. So the idea is that the total current that flows into this node is V1 plus V2 plus V0. So for V0, V0 is minus V1 minus V2. Right? Got it? So the idea was that this is really easy to solve if you use the ideal op-amp approximation. You can see that this is zero. Therefore, this is zero. So you have a single KCL equation. And the result is that this circuit looks like an inverting summer. It computes the sum of V1 and V2 and then takes the negative of that and presents that at the output. So what I'm trying to motivate is that there's a whole different level of reasoning that you can do when you have this element, which is an op-amp. Here what we've done is we've made something that performs a numerical operation. It presents at the output the negative sum of the two inputs. OK, another problem. Determine R so that V0 is twice V1 minus V2. OK. OK. OK. OK. OK. OK. OK. OK. So how should I choose R? Not very many ands. This is perfect nano quiz practice, right? This looks like a perfect nano quiz question, right? Smile. So what's the answer? OK, we're down to about half correct. This is harder. Basically, you do exactly the same thing. It's just that it's a little algebraically messier. So not surprisingly, the first idea is to think about the ideal op-amp. So we'll think about what was the voltage here, what was the voltage there, and then we'll equate them. So what's the voltage at the plus lead? Well, that's easy. That's just a voltage divider here. So since no current flows in here, I can compute the voltage relative to this ground as R over 1 plus R, that's showed here, times V1. This one's a little bit trickier because I've got two sources, V2 and V0, each pumping current into this place. The way I thought about it was start with V2 and then add to it this component, which can be thought of as a voltage divider here. So how big is the voltage at V minus? Well, it's V2 plus a voltage divider here, which is given by this. That's a little tricky. If you're not comfortable with that step, you could also solve for that voltage using the node method. The node method will give you the same answer. The answer is that the voltage at the V minus port is 2 thirds of V2 and 1 third of V0. That sort of looks right because the resistors are in a ratio of 2 to 1. And then using the ideal op-amp assumption, we equate the two, do some more algebra, and we get some relationship and figure out that R is 2. The point is that this is a relatively complicated circuit. You could have done it if I had asked you to do it with the voltage controlled voltage source model. But the algebra is already hard here with the ideal op-amp assumption, and with the voltage controlled voltage source, it's even harder. So the idea is that the ideal op-amp assumption, the idea that V plus is equal to V minus, makes the work of calculating these responses significantly easier. Everybody's with the bottom line? OK. So we started with the idea that when you add an element to a circuit, in general, adding an element changes voltages and currents throughout the circuit. We wanted a way to make the design more modular, a way of adding a component without changing everything else. We thought about this op-amp thing. The model for the op-amp was voltage controlled voltage source. We inferred this ideal op-amp model. The ideal op-amp model is great for calculating the response. It has this one problem. It seems to say these circuits are identical. If I literally believe the ideal op-amp assumption, that all the op-amp does is magically make the plus and minus terminals the same, I would conclude that this circuit where the input comes in the plus and the output wraps around to the minus, generates precisely the same input-output relationship as this one, where the input comes in the minus and the output wraps around the plus. The ideal op-amp assumption simply says for both of those, Vi equals Vo. So I would assume from the ideal op-amp model, I get the result that these two circuits generally work exactly the same way. Somehow that sounds wrong. I've got this part, and I can wire it up the right way, and it works, or I can flip the two wires, and it still works. Conservation of badness doesn't let that happen. If you do something bad, it should break. So the ideal op-amp assumption seems to lead to a bogus result. It seems to say these two are the same. So what I'd like to do is think about that for a moment. We want to be comfortable with the assumption that we make. The ideal op-amp assumption makes analysis really easy, but we'd like to understand exactly what we're assuming. This just doesn't sound right. OK, so let's back up. Let's not do the ideal op-amp assumption. Let's instead say that we use the voltage control voltage source model. So what I've done here is substitute into the left circuit. So this circuit is showed here, and the other over here. All I've done is connected the input either to the plus or to the minus port, and wrapped the other one around. But now I'm analyzing it using the voltage control voltage source model. And I've done the tedious algebra, and I don't think there's any mistakes in the tedious algebra. In one case, I get k over 1 plus k, which for large k is about 1. In the other case, I get minus k over 1 minus k, which for k large is about 1. OK, so the ideal op-amp assumption says that it doesn't matter. The voltage control voltage source model says it doesn't matter. And Freeman thinks this doesn't matter. And Freeman thinks this doesn't make any sense. OK, so what's going on here? What's going on is that we've actually made an enormous leap in thinking about the op-amp, even as a voltage control voltage source. The two models that we talked about, the voltage control voltage source and the ideal op-amp model, are great for calculating answers. They are not so good at providing a mechanism for how the op-amp's actually working. Think about just the ideal op-amp assumption. It's great to think, OK, the op-amp is going to do whatever magic is necessary to make these two leads the same. OK, well, how does it do that? The thing about the ideal op-amp assumption is that it doesn't tell you the mechanism by which that happens. What's the op-amp actually do in order to get the V plus equal to V minus? What's actually going on inside the op-amp? The ideal op-amp assumption is very good for analysis and not very good with mechanism. What's the mechanism? What's the op-amp actually doing? What's the op-amp actually doing? What it's really doing is moving charge around. 802, charge. So if you're going to have a change in voltage, there has to be motion of charge. And that's what's missing from the voltage control voltage model and from the ideal op-amp model. How do you think about moving charge around? Well, it's the same as moving water around. Somehow I think it's all those years of playing with water when I was a little kid. I find my intuition about water is better than my intuition about charge. So let me start by thinking about the intuition for water. The way a water tank works is if the flow in is different from the flow out, the height changes. Right? That's continuity. So if the water is conserved, if there's not significant evaporation over the duration of this experiment, or if molecules of water do not spontaneously disappear or appear. Under either of those two assumptions, then the change in height is proportional to the difference between the rate at which it's coming in and the rate at which it's going out. If the rate at which it's going out is equal to the rate at which it's going in, there's no change in height. If it's coming in faster, it's going up. If it's coming in slower, it's going down. All easy, right? Charge works the same way. The thing that accumulates charge in an electronic circuit is a capacitor. And there's a direct analogy between thinking about the way the flux of water generates height and the way flux of charge generates volts. So we can think about the flux of charge that's current. If there's a net flux, if there's a bigger current into a capacitor than there is out, the voltage on that capacitor goes up. If there's a smaller current in than goes out, the voltage on the capacitor goes down. Just like the height in a tank of water. We can make a much more realistic model for the way an op-amp works by explicitly making a representation for how the charge flows. And that's showed here. I'm going to take the voltage control voltage source model, but explicitly make a representation for the output charging up. What's the op-amp do? The op-amp senses the voltage at the input and the output, and does something to change the voltage at the output. The thing it does is if the positive voltage is greater than the negative voltage, it pumps charge into the output node. And if the opposite occurs, it sucks charge out. Now it's important, this is not an accurate depiction of what's inside an op-amp. This is a model for an op-amp. I don't want to lead any of you to think that this is literally what's in an op-amp. This is not literally what's in an op-amp. Here is much more literally what's in an op-amp. This is the schematic diagram of a 709, which is a Whittler circuit. It's a complicated transistor circuit. It's ingenious. This is how you build a circuit that has the remarkable property of the ideal op-amp circuit. It's not perfectly obvious from here. And even here, this doesn't give Whittler enough credit. Here's what he really did. He designed masks to shield semiconductor materials so that you could turn them into transistors so that it would turn into an op-amp. We're not going to worry about this. This is two levels of abstraction more complex than we're going to worry about. We're just going to say, OK, I don't know what's in there, but this is the way it behaves. This is intended to be a behavioral model for how an op-amp works. And in fact, that's an important idea. I use circuits on a daily basis not because I design semiconductor devices, but because I work on biological issues. I actually study hearing. And we make circuit models for the way biological parts work. And that helps us to understand the way the biological part works. So for example, here is a model taken from 602.1 where we try to understand how a nerve propagates an action potential by making a circuit model. That's the same thing we're doing here. We're making a circuit model for how the op-amp works. It's not intended to be literally what's inside the op-amp. It's intended to be a model that lets us think about the behavior of the op-amp. This lets us understand now why it's different when you flip the wires. Let's start with the original configuration where I put the voltage in the plus lead, and I wrap the output back around to the minus lead. What's going to happen? What's the op-amp actually do? Imagine that things had been stable. This was 0. The output was 0. Everybody was happy. And now all of a sudden, the input voltage steps up. What happens? OK, the input voltage steps up. The voltage source suddenly generates a huge positive voltage, and that starts to put current into the capacitor that represents the voltage at the output of the op-amp. So as a result of this voltage being higher than where it was, current is being dumped by the op-amp into this capacitor, and the output voltage starts to increase. That's showed in red. As the capacitor voltage gets bigger, as the output voltage increases, the difference between V plus and V minus gets smaller, and the rate at which charge is flowing into the capacitor slows. And in fact, as the voltage at the output gets very close to the voltage at the input, the rate of current into the capacitor goes to 0. And the output voltage stabilizes at the input voltage. The same thing happens in reverse if I were to change the input voltage and make it go negative. If the input voltage went negative, then K times V plus minus V minus would be a big negative number, and it would suck current out. The voltage control voltage source would suck current out of the capacitor. And make the voltage fall. The voltage would fall. The absolute difference between the plus and the minus port would get smaller. The current flowing would become less. And it would again stabilize when the output is equal to the input. Contrast that to what would happen if I put the input into the minus port. If I put the input into the minus port and the input goes through a step, then the input going positive makes the voltage control voltage source generate a big negative voltage. It sucks current out. So the input went positive, and the output goes negative. It goes the wrong way. That's bad. So here, if I put the input into the minus lead, a positive transition of the input leads to the output going the wrong way. And as it goes the wrong way, the drive to make it go the wrong way gets bigger. It's a runaway system. It's positive feedback. So the idea is that by squirting the input into the negative lead instead of into the positive lead leads to a positive feedback situation in which a small change at the input makes the output go the wrong way, convinces the op amp that things are getting worse, so it makes even more current, which makes it go even more the wrong way. And the same thing happens with the flip situation. So the idea then is that by thinking about the flow of current, what's the op amp actually doing? The op amp's actually sensing the difference between the voltage at the positive port and the minus port, and it's sourcing current that changes the output voltage to go up or down. You need to wire the op amp so that ultimately the output equals the input. Otherwise, the ideal op amp condition, the V plus equals V minus, will never be attained. So the left one is what we would refer to as a stable feedback situation. You can think about that as analogous to thinking about a ball in a valley. So we have a valley and we have a ball. The ball's going to roll down here eventually. It's stable. Bop it a little bit to the right, it'll roll back into the valley. Bop it a little to the left, it'll roll back into the valley. That's as opposed to when we've wired up the wrong way, it's like we have an unstable equilibrium. It's as though we're trying to put the ball there. Bop it a little to the right, it runs away to the right. Bop it a little to the left, it runs away to the left. That's unstable. There is a meta-stability point. If you actually balanced it exactly, exactly, exactly at the right place, it would stay there. That's what the solution is telling you. That was the minus k over 1 minus k. The fact that the equations had a solution is correct. It's just an unstable solution. The tiniest little disturbance will make it go awry. So the idea then is that the ideal op-amp assumption is fine, it's a good thing to use, it doesn't tell you about the mechanism. If you think a little bit more about the way the physics works, you have to wire the circuit up so that it has negative feedback in order to get a stable equilibrium. So if you understand that and you just make sure that you've hooked it up so that it has negative feedback, then you can use the ideal op-amp assumption. Everything's just fine. And in the interest of time, I'm going to just skip over this because we've already talked about all of this. It's just another example. The last thing I want to mention is just that in order to work this magic, the op-amp has to get power from somewhere. So that means that it's not a three terminal device. It's not the kind of device I've been drawing so far that has got power pins too. And the way the op-amp works is it takes power from those power pins to be able to force the output to a voltage that will make the input the v plus equal to v minus. That's the mechanism by which it works. And that will have important consequences when we build the robot head. Because what it means is that we're not going to be able to generate arbitrary voltages at the output of an op-amp. We're going to be limited to generating voltages that are between the power rails. So if we were to supply plus or minus 10 volts to the power pins, we would not be expecting to be able to generate 20 volts at the output. So that's an important implication when we do the design lab. So in summary, what we've done is we've showed how circuits can be a pain to make modular. Because in principle, adding one component changes voltages and currents everywhere. But there's a way using op-amps to have this buffering idea that lets us logically separate parts of circuits so that one can control the other. Have a good day."
    },
    {
        "Rec 15 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-yWQYXEjxAnk.mp3": " Hi. Today I'd like to talk to you about search. Previously we've talked about ways to model uncertainty and also how to make estimations about a particular system when we're observing it from the outside or when we don't have perfect information. At this point, we've almost got enough components to attempt to make an autonomous system, but at this point we've also never enabled our autonomous systems to make complex decisions for itself. This is why we want to be able to encode search or we want to be able to enable a system to make an evaluation of a succession of decisions or a succession of actions when there are multiple choices and possibly multiple choices at every level. So as a consequence of being able to do search, our autonomous system should be able to complete a successive grouping of actions. In 6.01 we're going to be searching state spaces. And searching state spaces borrows a lot of ideas from state transition diagrams or what we already know about state machines. When we're searching a state space, we want to know what states we're searching, what the transitions between them are going to look like or have access to the transitions from a given state to all the states that are its neighbors. We're going to want the start state to be specified so we know where to begin. We want a goal test which actually specifies what we're looking for as a consequence of the search. Or if we get to a state and we want to know whether or not we're done, we use our goal test and it could look at the output of the state or actually just the state name, that sort of thing. The other thing that we're going to have while we're searching is a legal action list. The searches that we're going to do today, you and I are going to be able to see the entire state transition diagram at once. But if we're encoding how to search on our robot, our robot can't see the entire state transition diagram at once because if it could, then it wouldn't have to do search. It would know how to get there from here. Therefore, we give our system a legal action list or all the actions that it should attempt to do at a given state. It's entirely possible that there aren't legal transitions at every state for every legal action, but it's good to have an exhaustive list of things to try and see if they succeed or fail. And as a consequence, what states you end up visiting every time you try to do one of these. Being able to do search is great, but we also want to be able to keep track of where we searched and how so that once we're done searching, we can actually execute the best thing. We're going to use a search tree to keep track of where we've been and how we got there. Search tree is going to be comprised of nodes. It's otherwise going to look like a directed acyclic graph and it's going to have a lot of similarity to any particular state transition diagram that we end up searching, but it's going to have nodes instead of states. Nodes are different. Nodes represent both the state that you've expanded as a consequence of being in that state. You've visited as a consequence of expanding its parent node. The parent node or the place that you came from as a consequence of getting to that node and the transition that you made in order to get there or the action that happened that got you from the parent node to the child node. Keeping track of a list of nodes is known as a path or it specifies where you've been and how you've got there. And if you're at a given node, you can actually use the reference to the parent node and the action to trace back from whatever node you're at currently to its parent node, to its parent node, to its parent node, and then finally get back to the start state. At that point, you'll know what path to take. So the only thing left to do is how do you figure out which paths to follow first? That's where the agenda comes in. The agenda is going to be the collection of all partial paths you've ever created as a consequence of expanding nodes and then putting its child nodes on a partial path meant for future expansion. The order in which you add and remove things to the agenda is going to determine what your search tree looks like. That's a lot of information. At this point, I'm going to go over an example. We're going to search the state transition diagram. We're going to start at A. And our goal test would be whether or not our state was equal to E. Today we're going to try two different kinds of basic search. One is referred to as breadth-first search or BFS. And one is referred to as depth-first search or DFS. Depth-first search refers to the idea that as you build your search tree, you're going to exhaustively expand all the nodes at a given level before advancing to the next level or all the given nodes at a given depth before expanding to the next depth. This means you're being very thorough. It also means that you're guaranteed to find the shortest path from your start node to the goal if it exists. Depth-first search is the opposite. As a consequence of depth-first search, you're going to expand all the nodes in a given branch as far down the tree as you possibly can before advancing to the next branch. It takes up a lot less space than breadth-first search, but it's not guaranteed to find the optimal path. Another way to think about these two types of search is that if you're doing breadth-first search, then your agenda acts as a queue. First items in or first partial paths that you discover are the first items out or the first partial paths that you end up expanding. Depth-first search is when the agenda is used as a stack or the first partial paths that you visit are the first partial paths. Or the most recent partial paths that you visited are going to be the partial paths that you first expand. First in, last out. Or last in, first out. Let me walk through a couple iterations on this state transition diagram, and hopefully it'll be clear what's going on. The first thing that happens is that you end up visiting and expanding the start node. That's pretty straightforward. So the path A is going to be added to both agendas, and the node A is going to be visited first on both search trees. If in the general sense I say that I'm going to make a transition to states in alphabetical order, and that's the order in which I'm going to add them to my agenda, that's going to be reflected in what I write up here. Let's say that I'm going to visit new nodes in alphabetical order. So nodes I would visit are B and C, and I'm going to add A, B, and A, C to my agenda. Here's where the difference between breadth-first search and depth-first search comes in. In breadth-first search, because I'm following the convention first in, first out, if I place the partial path AB in my agenda first, then I'm going to expand B as a consequence of the partial path AB first. So when I go to B, I'm actually going to expand B now. I'm going to look at the nodes that I can visit as a consequence of expanding B. The ones that I can visit are C and D, and I'm going to add the partial paths A, B, C, and A, B, D to my agenda. So A, C, I'm just going to move to the front of the queue, or the agenda. And I'm going to add A, B, C, and A, B, D. And I got there through B, so I'm going to add C and D here. Depth-first search grabs from the opposite end of the agenda. So the first thing I'm going to look at is A, C. I'm going to expand to C, look at the nodes that I can reach as a consequence of expanding C, and visit B and D. AB is still hanging out here. I popped off AC to use it in order to expand C's children, and I'm going to add ACB first, and ACD second. Note that our search trees already look different, and we'll actually end up reaching the goal using one of these search strategies first, as opposed to the other. If I go back to breadth-first search, I'm going to pop the partial path AC off the front of my agenda. I'm going to expand C. Expanding C gets me B and D. I'm going to move over my existing partial paths and add ACB and ACD. I've staggered these in order to indicate that there are consequence of a third iteration of breadth-first search, but they're actually considered to be at the same depth, since their parents are considered to be at the same depth, since their parents are parents of the start node. That's the defining feature of breadth-first search, is the fact that we're going to exhaustively search a given depth in our search tree before advancing to the next depth level. If I run one more iteration of depth-first search, again I'm popping partial paths off this end of the agenda. I'm going to expand D. D has one transition available to a node, and in plain old-fashioned breadth-first search and depth-first search, I run my goal test when I visit a node. So at this point, I would test whether or not E was my goal test. I would discover it's my goal test. My search would return successfully and return the path found. So A, B, and A, C, B remain on the agenda. I popped off A, C, D in order to expand D, and I found this path. At this point, I've concluded depth-first search. I'm going to do one more round of breadth-first search to demonstrate an important rule. If I pop A, B, C off the agenda and move all these over, if I'm looking at A, B, C, and I look at the children of C, the two children of C are B and D. So the first partial path that I would end up adding to the agenda as a consequence of expanding C in this case would be A, B, C, B. And it would look like this. You'll notice that we're going to create an infinite loop. And there are two basic rules of basic search that I need to emphasize now to prevent you from doing things like creating an infinite loop. If you look in the textbook, they're called how to not be completely stupid. If at any point you are visiting a node in your partial path that it already exists in your partial path, don't add it to that partial path. You'll prevent yourself from creating a cycle, because if you visit the same node more than once, you've actually done more work than you need to. The second rule, and it's not demonstrated well on this state transition diagram, but for instance, if I had two arrows from B to D, there's no particular reason to consider both of these actions. And if you have a state transition diagram that allows multiple transitions from one state to another based on different actions, then you need to come up with some sort of rule to decide between the two actions. That's the second rule of how to not be completely stupid, is if you have more than one transition from one state to another as a consequence of doing search, pick one and come up with a rule to pick one. This covers basic search. Next week I'm going to talk about dynamic programming, costs, and heuristics."
    },
    {
        "Lec 10 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-u_x67-kaedM.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. Today I'm starting a new topic, and that's always the occasion for putting things into perspective. Keep in mind what we were trying to do in this subject. We were trying to introduce several intellectual themes. The first and absolutely the most important is how do you design a complex system? We think that's very important because there's absolutely no way this department could exist the way it does, making things like that, hooking up internets and so forth. Those are truly complex systems. And if you didn't have an organized way of thinking about complexity, they're hopeless. So the kinds of things we're interested to teach you about are just hopeless if you can't get a handle on complexity. So that's by far the most important thing that we've been thinking about. We've been interested in modeling and controlling physical systems. I hope you remember the way we chased the robot around the lab. And that was the point there. We've thought about augmenting physical systems by adding computation. I hope you've got a feel for that. And we're going to start today thinking about how do you build systems that are robust? So just in review, so far, you've already seen most of this. So far, we've taught you about abstraction, hierarchy, and controlling complexity, starting primarily by thinking about software engineering, because that's such a good pedagogical place to start. We introduced the idea of PCAP, and that has continued throughout the rest of the subject. Then we worried about how do you control things. We developed ways of modeling so that you could predict the outcome before you actually built the system. That's crucial. You can't afford to build prototypes for everything. It's just not economical. And so this was an exercise in making models, figuring out how behaviors relate to the models, and trying to get the design done in the modeling stage rather than in the prototyping stage. And you built circuits. And this had to do with how you augment a system with new capabilities, either hardware or software. Today, what I want to start to think about is how do you deal with uncertainty, and how do you deal with things that are much more complicated to plan? So the things that we will do in this segment are things like mapping. What if we gave you a maze, you the robot? What if we gave the robot a maze and didn't tell them the structure of the maze? How would it discover the structure? How would it make a map? How would it localize? What if you had a maze? To make it simple, let's say that I tell you what the maze is, but you wake up, you're the robot. You wake up, you have no idea where you are. What do you do? How do you figure out where you are? That's a problem we call localization. And then planning, what if you have a really complicated objective? What's the step-by-step things that you could do to get there? Those are the kinds of things we're going to do. And here's a typical kind of problem. Let's say that the robot starts someplace and say that it has something in it that lets it know where it is, like GPS, and it knows where it wants to go. Making a plan is not very difficult. I'm here. I want to go there. Connect with a straight line. And that's what I've done here. The problem is that unbeknownst to the robot, that path doesn't really work. So on the first step, he thinks he's going to go from here to here in a straight line. The blue represents the path that the robot would like to take. But then on the first step, the sonars report that they hit walls. And those show up as the black marks over here. So already, it can see that it's not going to be able to do what it wants to do. So it starts to turn, and it finds even more places that don't work. Try again. Try again. Notice that the plan now is, well, I don't know what's going on here, but I certainly can't go through there. So I'm going to have to go around it. Keep trying. Keep trying. Notice the plan. So he's always making a plan that sort of makes sense. He's using for each plan the information about the walls that he's already figured out. And now he's figured out, well, that didn't work. So now backtrack, try to get out of here. Well, he's going forward. He's making a forward plan. He's saying, OK, now I know all these walls are here, and I'm way down in this corner. How do I get on the other side of that wall? Well, given the information that I know, I'm going to have to go around the known walls. So my point of showing you this is several fold. First off, it's uncertain. You didn't know at the outset just how bad the problem was. So there's no way to kind of pre-plan for all of this. Secondly, it's a really hard problem. If you were to think about structuring a program to solve that problem in a kind of high school kind of programming sense, if this happens, then do this, if this happens, then do this, you would have a lot of if statements. That's just not the way to do this. So what we're going to learn to do in this module is think through much more complicated plans. We're going to be looking at the kind of plans, like showed here, that just are not practical for do this until this happens, and then do this until this happens, and then while that's going on, do this. It's just not going to be practical. That's the idea. So the very first element, the thing that we have to get on top of first, is how to think about uncertainty. And there's a theory for that. And the theory is actually trivial. The theory is actually simple. Except that it's so mind-bogglingly weird that nobody can get their head around it the first time they see it. It's called probability theory. As you'll see in a minute, the rules are completely trivial. You have no trouble with the basic rules, which you will have trouble with unless you're a lot different from most people. The first time you see this theory, it's very hard to imagine exactly what's going on. And it's extremely difficult to have an intuition for what's going on. So the theory is going to give us a framework, then, for thinking about uncertainty. And in particular, uncertainty sounds uncertain. What we would like to do is make precise statements about uncertain situations. Sounds contradictory, but we'll do several examples in lecture, and then you'll do a lot more examples in the next week so that you learn exactly what that means. We would like to draw reliable inferences from unreliable observations. OK, you have a lot of experience with unreliable observations, right? The sonars don't tell you the same thing each time. That's what we'd like to deal with. We would like to be able to take a bunch of different, individually not all that reliable observations, and come up with a conclusion that's a lot more reliable than any particular observation. And when we're all done with that, what we'd like to do is use this theory to help us design robust systems, systems that are not fragile, systems that are not thrown off track by having a small feature that was not part of the original formulation of the problem. So that's the goal. And what I'd like to do is start by motivating it with a kind of practical thing to get you thinking. So here's the game. Let's make a deal. I'm going to put four LEGO bricks in a bag. LEGO bricks, you've seen those probably. Bag. The LEGO bricks are white or red. There's only going to be four, and you're not going to know how many of each there is. Then you get to pull one LEGO brick out. And if you pull a red one out, I'll give you 20 bucks. OK, the hitch is you have to pay me to play this game. So the question is, how much are you willing to pay me to play the game? So I need a volunteer, I need somebody to take four LEGOs and not let me see. OK, please, please. I want you to put four LEGOs, only four. They can be white or red. If you have LEGOs in your pockets that are a different color, don't use them. So you're allowed to know what the answer is, but you're not allowed to tell me or them. So OK, so we'll come over here. So bag, LEGOs, hide, put some number in. No, no, no, wait, wait, wait. Put them back, put them back. I'm not supposed to see either. OK, I'll go away. OK, four. OK, so we'll close the bag. And I'll call you back later, but it'll be nearer the end of the hour. So here's four LEGOs. Sort of sounds like four LEGOs, right? It's more than one. OK, so how much would you be willing to pay me to play the game? Yeah? $5. Can I get more? I want to make money. Can I get a higher bid? More than $5. $9.90. How much? $9.90. $9.90. Very interesting. Can I get more than $9.90? $9.99 and a half. $9.99 and a half? Wow, a magic number it would seem. $10. $10. Can I hear even a penny more? No, no. A penny more. I don't think a penny more. You just have to go to the back. No, no, no. I thought we were being very careful and letting you not know. No, no. Aren't you going to put four white blocks in all the time? I didn't do it. That person did it. It wasn't me. I'm innocent. I'm completely fair. Yeah? Are we imagining that you are equally as likely to put any number of blocks in? So are we able to say that she's more likely to put it all white? Because that just changes how you get a capital. OK, that's an interesting question. We need a model of a person. That's tricky. OK, I have another idea. Two more volunteers. OK, volunteer, volunteer. Here's the experiment. One person will hold the bag up high so that the other person can't see it. And the other person, I didn't look in. Notice that I'm being very careful. I'm very honest, right? Except for the x-ray vision, which you don't know about. Everything's completely fair. And the little window in the back, but you don't know about that either. So one person holds it up so the other person can't see in. The other person grabs a Lego and pulls it out, and lets everybody see that Lego. It was intended to make it hard to see it. OK, a red one. OK, that's fine. So we're done. No, no, no. This was a different part of the bet. No, no, no, no. Thank you. Thank you. Now how much would you pay me to play the game? With that one out? No, we'll put that one back. OK, so this one came out. It was red. Now without looking, I'm going to stick it back in. OK, so we pulled it out. So what do we know? We know there's at least one red one. OK, now what are you willing to pay to play the game? $5. $5. Yes? $5? Wait a minute. Should you be willing to pay more or less? I got it up to $10. Should you be willing to pay more or less now? More, why? The same. More. The same. You're insured that there's at least one red one. I know there's at least one, but didn't I know that before? No. The person could have been evil. That first person could have loaded it, because I was giving her a cut on the, I didn't talk about this beforehand, right? This is not a set up, right? OK, so I want to vote. How many people would give me less than $10? Wait, wait, wait. I'm going to give you the top ones first. 10 to 12. Let's see, 13 to 15, 16 to 18, more than 18. So how many people would give me, you're only allowed to vote once. Keep in mind that I'm more likely to choose you if you vote high. Right? Vote high. So how many people would give me less than $10 to play the game? A lot. I would say 20%. How many people would give me between $10 and $12? A lot smaller, 5%. How many people would give me between 13 and 15? Even smaller, 2%. How many people would give me between 16 and 18? Wait, these numbers are not going to add up to 100%. OK, we'll learn a theory for how to normalize things in a minute. OK, so we're down to about 1%. How many people would give me more than $18? One person, thank you. Thank you. So that's 1 in 200.05%. OK, so what I'd like to do now is go through the theory that's going to let us make a precise calculation for how much a rational person, not to say that you're not rational, but how much a rational person might be willing to pay. OK, so that was the setup. Then we'll do the theory. Then we'll come back at the end of the hour and see how many people I would have made money on. Well, whatever. OK, so we're going to think about probability. And the first idea that we need is set theory. Because we're going to think about experiments having outcomes. And we're going to talk about the outcomes being an event. An event is any describable outcome from an experiment. So for example, what if the experiment were to flip three coins in sequence? An event could be head, head, head. And you could talk about, was the outcome head, head, head. The event could be head, tail, head. The event could be one head and two tails. The event could be the first toss was a head. So the idea is there are sets that we're thinking about. And we're going to think about events as possible outcomes being members of sets. There's going to be a special kind of event that we're especially interested in. And that is an atomic event, by which we mean finest grain. Finest grain is a kind of amorphous idea. What it really means is for the experiment at hand, it doesn't seem to make sense to try to slice the outcome into two smaller units. You keep slicing them down until slicing them into a smaller unit won't affect the outcome. So for example, in the coin toss experiment, I might think that there are eight atomic events. Head, head, head, head, head, tail, head, tail, head, head, tail, tail, blah, blah, blah. So I've ignored some things like it took three minutes to do the first flip, and it took two minutes to do the second one. That's the art of figuring out what atomic units are. So for the class of problems that I'm thinking about, those things can be ignored, so I'm not counting them. But that's an art. That's not really a science. So you sort of have to use good judgment when you try to figure out what are the atomic events for a particular experiment. Atomic events always have several properties. They are always mutually exclusive. If I know the outcome was atomic event three, then I know for sure that it was not atomic event four. And you can see that these events up here don't have those properties. So the first toss here, here's an event, head, head, head, which is not mutually exclusive with the first toss was a head. So atomic events have to be mutually exclusive. Furthermore, if you list all of the atomic events, that set has to be collectively exhaustive. Collectively exhaustive, what buzzwords. That means that you've exhausted all possibilities when you've accounted for the collective behaviors of all the atomic events. And we have a very special name for that because it comes up over and over and over again. The set of atomic events, the maximum set of atomic events, is called the sample space. So the first thing we need to know when we're thinking about probability theory is how to chunk outcomes into a sample space. Second thing we need to know are the rules of probability. These are the things that are so absurdly simple that everybody who sees these immediately comes to the conclusion that probability theory is trivial. They then don't do anything until the next exam. And then they don't have a clue what we're asking. Because it's subtle. It's more subtle than you might think. Here's the rules. Probabilities are real numbers that are not negative. Pretty easy. Probabilities have the feature that the probability of the sample space is 1. That's really just scaling. That's really just telling me how big all the numbers are. So if I enumerate all the possible atomic events, the probability of having one of those as the outcome of an experiment, that probability is 1. Doesn't seem like I've said much. And I've already two-thirds of the way through the list. Yes? AUDIENCE MEMBER 2. Doesn't that just mean that something happened? PROFESSOR 1. Something happened. Yes. And we are going to say that the certain event has probability 1. All probabilities are real. All probabilities are bigger than 0. And the probability of the certain event, written here as the universe, the sample space, the probability of some element in the sample space is 1. The only one that's terribly interesting is additivity. If the intersection between A and B is empty, the probability of the union is the sum of the individual events. Astonishingly, I'm done. And this doesn't alter the fact that people are still, to this day, doing fundamental research in probability theory. There are many subjects in probability theory, including many highly advanced graduate subjects, all of which derive from these three rules. It's absurd how unintuitive things can be given such simple beginnings. So you can prove all of the interesting results from probability theory. You can prove all results from probability theory with these three rules. And here's just one example. If the intersection of A and B were not empty, you can still compute the probability of the union. It's just more complicated than if the intersection were empty. Generally speaking, the probability of the union of A and B is the probability of A plus the probability of B minus the probability of the intersection. And you can sort of see why that ought to be true if you think about an event diagram. If you think about the odds of having A in the universe, the universe is the sample space, the probability of having some event A, the probability of having some event V, the probability of their intersection. If you were to just add the probability of A and B, you doubly count the intersection. You don't want to double count it. You want to count it once. So you have to subtract 1 off. So that's sort of what's going on. As I said, the theory is very simple. But let's make sure that you've got the basics first. So experiment. I'm going to roll a fair six-sided die. And I'm going to count as the outcome the number of dots on the top surface, not surprisingly. Find the probability that the result is odd and greater than 3. You have 10 seconds. OK, 10 seconds are up. What's the answer? 1, 2, 3, 4, or 5? Raise your hands. Excellent. Wonderful. The answer is 1. The way I want you to think about that is in terms of the theory that we just generated, because it's useful for developing the answers to more complicated questions. In terms of the theory, what we will always do, the process that always works is enumerate the sample space. What's that mean? That means identify all of the atomic events. The atomic events here are the faces that show our 1, 2, 3, 4, 5, 6. Enumerate the sample space, and then find the event of interest. So here, the event was a compound event. The result is odd and greater than 3. Odd, well, that's 1, 3, 5, showed by the check marks. Bigger than 3, that's the bottom three check marks. If it's going to be both, then you have to look where those overlap, and that only happens for the outcome 5. Since there's only one, so fair meant that these probabilities were the same. If you think through the fundamental axioms of probability, if they're equal, they're all non-negative real numbers, and they sum to 1, then they are all 1 sixth. So the answer is 1 sixth. OK, that was easy. The rule that is most interesting for us happens, not surprisingly, to also be the one that people have the most trouble with, not excluding the people who originally invented the theory. The theory goes back to Laplace, a bunch of people back then who were absolutely brilliant mathematicians. And still, it took a while to formulate this rule. It was formulated by a guy named Bayes. Bayes' theorem gives us a way to think about conditional probability. What if I tell you in some sample space B happened? How should you relabel the probabilities to take that into account? Bayes' rule is trivial. It says that if I know B happened, what is the probability that A occurs, given that I know B happens? And the rule is, you find the probability of the intersection. What do you do then? We'll go over that. We'll do some examples. So we need to find the probability of the intersection, and then we have to find the probability of B occurring, and then we normalize, a word I used before, and that's exactly what we need to do to that distribution. We normalize the intersection by the probability of B. That's an interesting rule. It's the kind of thing we're going to want to know about. We're going to want to know, OK, I'm a robot. I'm in a space. I don't know where I am. I have some a priori probability idea about where I am. So I think I'm 1,120th likely to be here, and 1,120th likely to be there, et cetera, et cetera. And then I find out the sonars told me that I'm 0.03 meters no, it can't be that small, 0.72 meters from a wall. Well, how do I take into account this new information to update my probabilities for where I might be? That's what this rule is good for. So here's a picture. The way to think about the rule is if I condition on B, if I tell you B happened, that's equivalent to shrinking the universe. The universe U, the square, that's everything that can happen. Inside the universe, there's this event A, and it does not occupy the entire universe. There is a fraction of outcomes that belong logically in not A. OK, that's the part that's in U, but not in A. Similarly with B. Similarly, there's some region, there's some part of the universe where both A and B occurred, the intersection of the two occurred. So what Bayes' theorem says is if I tell you B occurred, all this part of the universe outside of B is irrelevant. As far as you're concerned, B's the new universe. Notice that if B is the new universe, then the intersection, which is the part where A occurred, is bigger after the conditioning than it was before the conditioning. Before the conditioning, the universe was this big. Now the universe is this big. The universe is smaller. So this region of overlap occupies a greater part of the new universe. Is that clear? So when you condition, you're really making the universe smaller, and the relative likelihood of things that are still in the universe seem bigger. So what's the conditional probability of getting a die roll greater than 3, given that it was odd? Calculate, you have 30 seconds. This is three times harder. OK. What's the probability of getting a die roll greater than 3, given that the die roll was odd? Everybody raise your hands. And it's a landslide. The answer is 2. You roughly do the same thing we did before, except now the math is incrementally harder, because you have to do a divide. So we think about the same two events. The event that it is odd, and the event that is bigger than 3, and now we ask the question, if it were odd, what's the likelihood that it's greater than 3? Before I did the conditioning, what was the likelihood that it was bigger than 3? 1 half. No. 1 half. So bigger than 3 is 4, 5, or 6. There are three atomic units there. There are six atomic units to start with. They're equally likely. So before I did the conditioning, the event of interest had a probability of a half. After I do the conditioning, I know that half of the possible samples didn't happen. The universe shrank. Instead of having a sample space with 6, I now have a sample space with 3. Similarly, the probability law changed. Similarly, the probability law changed. So now the event of interest is bigger than 3, but bigger than 3 now only happens once. So what I need to do is rescale my probabilities. Remember, the scaling rule, one of the fundamental properties of probability, the scaling rule said the sum of the probabilities must be 1. After I've conditioned, the sum of the probabilities is 1 half. That's not good. I've got to fix it. So the way to think about Bayes' rule is if all I know is that the universe got smaller, how should I redo the scaling? Well, if all I've told you is that the answer is odd, then there are three possibilities. Before I told you that the answer was odd, they were equally likely. After I tell you that they're odd, has it changed the fact that they're equally likely? No. They're still equally likely, even under that new condition. I haven't changed their individual probabilities. So they started out equally likely. They're still equally likely. They just don't sum to 1 anymore. Bayes' rule says make them sum to 1. So the way I make the sum sum to 1 is to divide by 1 half. If you divide a 6 by 1 half, you get 1 third. Notice that the probability that it's bigger than 3 went from 1 half to 1 third. It got smaller. So it could have gone either way. Think about what happens when the world shrinks, when the universe gets smaller, when I tell you that B happened. Well, when I tell you that B happened, then I ask you whether A happened. Here I'm showing a picture that in the original universe, A and B sort of covered the same amount of area, by which I mean they're about equally likely. Before I did the conditioning, the probability of A was about the same size as the probability of B. What happens when I condition? Well, when I condition, now the universe is B. But notice the way I've drawn them. There's very little overlap. So now when I condition on B, the odds that I'm in A seems to have got smaller. Rather than being of equal probability, as I showed here, after the conditioning, the relative likelihood of being event A is smaller than it used to be. But that's entirely because of the way I rigged the circles. I could have rigged the circles to have a large amount of overlap. Then when I condition, it seems as though it's relatively more likely that I'm in the event A. That's what we mean by the conditioning. The conditioning can give you unintuitive insight. Because when you condition, probabilities can get bigger or littler. And that's something that sort of at a gut level, we all have trouble dealing with. So that's the fundamental ideas. We've talked about events. Three axioms of probability that are completely trivial. One not quite so trivial rule, which is Bayes' rule. In order to apply it, there's two more things we need to talk about. The first is notation. We could do the entire rest of the course using the notation that I've showed so far, drawing circles on the blackboard. It would work. It would not be very convenient. So to better take advantage of math, which is a very concise way to write things down, we will define a new notion, which is a random variable. Random variable is just like a variable, except shockingly, it's random. So where we would normally think about a variable represents a number, a random variable represents a distribution. So we could, for example, in the die rolling case, we could say the sample space has six atomic events. And I could think about it as six circles. Circles wouldn't pack all that well. Six squares inside the universe, because they are mutually exclusive and collectively exhaustive. So if I started with a universe that looked like that, I would have this one would be the probability that the number of dots was one, two, three, and it has to fill up by the time I've put six of them in there. And they have to nod over that. A more convenient notation is to say, OK, let's let X represent that outcome. Then I can think about X taking on, so I can label the events with math. I can say there's the event camp X equals one, the event camp X equals two, the event camp X equals three. And it just makes it much easier to write down the possibilities than to try to draw pictures with Venn diagrams all the time. So all we're doing here is introducing a mathematical representation for the same thing we've talked about before. So among the things that you can do, after you've formalized this so that you can have a random variable, then it's a very small jump to say you can have a multidimensional random variable. That lets us, for example, have a two-space, X and Y, for example. So now we can talk very conveniently about situations that factor. So for example, when I think about flipping three coins, I can think about that as a multivariate random variable in three dimensions. One dimension represents the outcome of the first coin toss. Another dimension is the second. The third dimension is the third. So there's a very convenient way of talking about it. And we have a more concise notation. We say, OK, let V be the outcome of the first die roll or whatever. Let W be the second one. And then we can think about the joint probability distribution in terms of the multidimensional random variable. So we have the random variable defined by V and W. We will, generally, to try to make things easy for you to know what we're trying to talk about, we'll try to remember to capitalize things when we're talking about random variables. And then we'll use the small numbers to talk about events. So this notation would represent the probability that V took on the value little v and W took on the value little w. We'll see examples of this in a minute. So the idea is you don't need to do this. It's just a convenient notation to write more complicated things concisely. Now, a concept that's very easy to talk about now that we have random variables is reducing dimensionality. And in fact, we will constantly reduce dimensionality of complicated problems that are represented by multiple dimensions to smaller dimensional problems. And we'll talk about two ways of doing that. The first is what we will call marginalizing. Marginalizing means I don't care what happened in the other dimensions. So if I have a probability rule that told me, for example, about the outcome of one toss of a fair die and a second toss of a fair die, and if I tell you the joint probability space for that, so I would have six outcomes on one dimension, six outcomes on another dimension. Let's say they're all equally likely. I have 36 points altogether. If they're all equally likely, then my probability law is a joint distribution. The joint distribution has 32 non-zero points, and each point has height of? Right, I said the right thing, did I? 36 is what I meant to say. My brain is telling me that I might not have said that. I meant 36. So if I have 36 equally likely events, how high is each one? 1 over 36. OK, so the joint probability space for two tosses of a fair six-sided die is this 6 by 6 space. And I may be interested in marginalizing. Marginalizing would mean I don't care what the second one was. OK, well how do you infer the rule for the first one from the joint if I don't care what the second one was? Well, you sum out the second one. So if I have this two space that represented the first and the second, so say it's x and y, for example. So I've got six points that represent 1, 2, 3, 4, 5, 6, and then 6 this way, that sort of thing, except now I have to draw in tediously all of the others, right? So you get the idea. So each one of the x's represents a point with probability 1 over 36. And imagine that they're all in straight lines. Now, if I didn't care what is the second one, how would I find the rule for the first one? Well, I just sum over the second one. So say I'm only interested in what happened in the first one. Well, I would describe all of the probabilities here to that point. So I would sum out the one that I don't care about. That's obvious, right? Because if I marginalized, these x's that all represent the number 1 over 36 have to turn into a single dimension axis, which is just x. And they have to be six numbers that are each how high? 1 sixth, right? So the way I get six numbers that are each 1 over 6, when I started with 36 numbers that were each 1 over 36 is you sum. So that's called marginalization. The other thing that I can do is condition. I can tell you something about the sample space and ask you to figure out a conditional probability. So I might tell you, what's the probability rule conditioned on the first one? What's the probability rule for y conditioned on the first one being 3? Mathematically, that's a different problem. That's a rescale problem, because that's Bayes' rule. So generally, if I carved out by conditioning some fraction of the sample space, the way you would compute the new probabilities would be to rescale. So there's two operations that we will do. We will marginalize, which means summing out, and we will condition, which means rescale. So to give some practice at that, let's think about a tangible problem. Example, prevalence and testing for AIDS. Consider the effectiveness of a test for AIDS. This is real data, data from the United States. So imagine that we take a population, representative of the population in the United States, and classify every individual as having AIDS or not, and being diagnosed, according to some test, as positive or negative. Two-dimensional. The two dimensions are, what was the value of AIDS, true or false, and what's the value of the test, positive or negative. So we've divided the population into four pieces. And by using the idea of relative frequency, I've written probabilities here. So what's the probability of choosing by random choice an individual that has AIDS and tested positive? So that's 0.003648, et cetera. So I've divided the population into four groups. Multidimensional. Multidimensional random variable. The question is, what's the probability that the test is positive, given that the subject has AIDS? I want to know how good the test is. So the first question I'm going to ask is, given that the person has AIDS, what's the probability that the test gives a true answer? You've got 60 seconds. This is harder. Some people don't think it's harder. So what's the probability that the test is positive, given that the subject has AIDS? Is it bigger than 90% between 50 and 90, less than 50, or you can't tell from the data? Everybody votes, and the answer is 100% correct. Wonderful. So let me make it harder. Is it between 90 and 95, or between 95 and 100? 95 and 100. Is it between 95 and 97, or 97 and 100? 97, is it between? OK, so sorry. This is called marginalization. I told you something about the population that lets you eliminate some of the numbers. So if I told you that the person has AIDS, then I know I'm in the first column. That's marginalization. I gave you new information. I'm saying the other cases didn't happen. I've shrunk the universe. It used to have four groups of people. Now it has two groups of people. I use Bayes' rule. I need to rescale the numbers so that they add to 1. So these two numbers, the only two possibilities that can occur after I've done the conditioning, no longer add to 1. I've got to make them add to 1. I do that by dividing by the probability of the event that I'm using to normalize. So the sum of these two probabilities is something, whatever it is, 0.003700. So I divide each of those probabilities by that sum. That's just Bayes' rule. And I find out that the answer is the probability that the test is positive is, given that the person has AIDS, the probability that the test is positive is 0.986. Good test? Good test? Good test? Go on, go on. 98%. How many? I won't say that. 98 is a good test, right? Not that today is an appropriate day to talk about outcomes of tests and 98%. But I won't mention that. So good test. The accuracy of the test is greater than 98%. It's quite good. What's the probability that the test is positive? Everybody vote. 1, 2, 3, 4, 5. 1, 2, 3, 4? Looks 100%. OK, the answer is less than 50%. Why is that? Well, that's another marginalization problem, but now we're marginalizing on a different population. This is how you can go awry thinking about probability. The two numbers seem kind of contradictory. Here, I'm saying that the test came out positive, and I'm asking, does the subject have AIDS? It's still marginalization. I'm still throwing away two of the conditions, and I'm a two fraction of the population, and I'm only thinking about two. I still have to normalize so that the sums come out one. But the numbers are different. Yes? Why is marginalization so important when everything is in base? Thank you. Because my brain's not working. OK, I've been saying marginalization, and I meant uniformly over the last five minutes to be saying conditioning. OK, so I skipped breakfast this morning. My blood sugar is low. Sorry. Thank you very much. I should have been saying conditioning. Sorry. OK, so backing up. OK, I conditioned on the fact that the person had AIDS, and then I conditioned on the fact that the test came out positive. In both cases, I was conditioning. In both cases, I was doing Bayes' rule. Please ignore the person who can't connect his brain to his mouth. So here, because the conditioning event has a very different set of numbers from these numbers, the relative likelihood that the subject has AIDS is small. So even though the test is very effective in identifying cases that are known to be true, it is not very effective in taking a random person from the population and saying the test was positive, you have it. OK, those are very different things. And the probability theory gives us a way to say exactly how different those are. Why are they so different? The reason they're different is that other word, because the marginal probabilities are so different. And that is because the population is skewed. So the fact that the test came out positive is offset at least somewhat by the skew in the population. So the point here is actually marginalizing. If I think about how many people in the population have AIDS, that means I'm summing on the columns rather than conditioning. And what you see is a very skewed population. And that's the reason you can't conclude from the test whether or not this particular subject has the disease or not, because the population is so skewed. So this was intended to be an example of conditioning versus marginalization and how you think about that in a multi-dimensional random variable. Yes? AUDIENCE MEMBER 2 So how are they different? One of them has a divide in it and the other one has a divide in it, so when we did Bayes rule, we did the marginalization here. But then we used that summed number to normalize the individual probabilities by scaling, by dividing, so that the new sum over the new smaller sample space is still 1. So your point's right. Regardless of whether we're conditioning or marginalizing, we still end up computing the marginals. It's just that in one case we're done, in the other case we use that marginal to rescale. OK. So I said we could just use set theory and we're done. We'll in fact use random variables because it's simpler. That's one of the two other things we need to do, which are non-essential. It just makes our life easier. And the other non-essential thing that we will do is represented in some sort of a Python structure. So we would like to be able to conveniently represent probabilities in Python. The way we'll do that is a little obscure the first time you look at it, but again, once you've done it a few times, it's a very natural way of doing it. Otherwise we wouldn't do it this way. How are we going to represent probability laws in Python? The way we'll do it, since the labels for random variables can be lots of different things. So for example, the label in the previous one was, in the case of the subject having AIDS or not, the label was true or false. The label for the test was positive or negative. So in order to allow you to give symbolic and human meaningful names to events, we will use a dictionary as the fundamental way of associating probabilities with events. So we'll represent a probability distribution by a class, what a surprise, by a Python class that we will call D-dist, which means discrete distribution. D-dist want to associate the name of an atomic event, which we will let you use any string. Or in fact, I should generalize that. You can use any Python data structure to identify an atomic event, and then we will associate that using a Python dictionary with a probability. So what we will do when you instantiate a new discrete distribution, the instantiation rule, you must call it with a dictionary. A dictionary is a thing in Python that associates one thing with another thing. I'll give an example in a minute. And the utility of this is that you'll be able to use as your atomic event a string, like true or false, or a string, like positive or negative, or something more complicated, like a tuple. And I'll show you an example of where you would want to do that in just a second. So the idea is going to be you establish a discrete distribution by the init method called the dictionary. The dictionary is just a list of keys which tell you which event that you're trying to name the probability of, associated with a number, and that number is the probability. And this shows you that there's one extremely interesting method, which is the prob method. The idea is that prob will tell you what is the probability associated with that key. If it doesn't find the key in the dictionary, it'll tell you the answer is 0. We do that for a specific reason, too, because a lot of the probability spaces that we will talk about have lots of zeros in them. So instead of having to enumerate all the cases that are 0, we will assume that if you didn't tell us the probability, the answer was 0. So this is the idea. I could say use the dist module in lib 601 to create the outcome of a coin toss experiment, and I have a syntax error. This should have had a squiggle brace. A dictionary is something that in Python, so I should have said something like dist.ddist of squiggle. Sorry about that. That should have said squiggle, I'll fix it, and put the answer on the website. Head should be associated with the probability 0.5, and tail should be associated with the probability 0.5. End of dictionary, end of call. Sorry, I missed the squiggle. Actually, what happened was I put the squiggle in and LaTeX ate it, because that's the LaTeX. Anyway, it's sort of my fault. So the dog ate my homework, LaTeX ate my squiggle. It's sort of the same thing. So having defined a distribution, then I can ask what's the probability of the event head? The answer is 1.5. The probability of the event tail? The answer is 1.5. The probability of the event capital H? There is no capital H. The answer is 0. That's what I meant by sparsity. If I didn't tell you what the probability is, we assume the answer is 0. Conditional probabilities are a little more obscure. What's the conditional probability that the test gives me some outcome, given that I tell you the status of whether the patient has or doesn't have AIDS? OK, well conditionals, you're going to have to tell me which case I want to condition on. So in order for me to tell you the right probability law, you have to tell me, does the person have AIDS or not? So that becomes an argument. So we're going to represent conditional probabilities as procedures. That's a little weird. So the input to the procedure specifies the condition. So if I want to call the procedure and find out what's the distribution for the test, given that the person has AIDS, then I would call test given AIDS of true. So if AIDS is true, return this D dist. Otherwise, return this D dist. So it's a little bizarre, but think about what it has to do. If I want to specify a conditional probability, I have to tell you an answer. And that's what the parameter is for. So the way that would work is illustrated here. Having defined this as the conditional distribution, I could call it by saying, what is the distribution on test given that AIDS was true? And the answer to that is a D dist. Or if I had that D dist, which would be this phrase, I could say, what's then the probability in that new distribution that the answer is negative? Then I would look up the dot prob method within the resulting conditional distribution and look up the condition negative. And finally, the way that I would think about a joint probability distribution is to use a tuple. Joint probability distributions are multi-dimensional. Tupes are multi-dimensional. So for example, if I wanted to represent this multi-dimensional data, I might want the joint distribution of AIDS and tests. That's a two by two. AIDS can take on two different values, true or false. And tests can take on two different values, positive or negative. So there's four cases. The way I would specify a joint distribution would be, create a joint distribution starting with the marginal distribution for AIDS. And then, using Bayes' rule, tell me the two different conditional probabilities given AIDS. And that then will create a new joint distribution whose D dist is a tuple. So in this new joint distribution, AIDS and tests, if AIDS is false and test is negative, so false negative is this number, the probability associated with this tuple is that number. OK, is that clear? So I'm going to construct joint distributions by thinking about conditional probabilities. So I have simple distributions which are defined with dictionaries. I have conditional probabilities which are defined by procedures. And I have joint probabilities which are defined by tuples. OK, so that's the Python magic that we will use. And a lot of the exercises for week 10 have to do with getting that nomenclature straight. It's a little confusing at first. I assure you that by the time you've practiced with it, it is a reasonable notation. It just takes a little bit of practice to get onto it, much like other notations. OK, where are we going with this? What we would like to do is solve that problem that I showed at the beginning of the hour. So we would like to know things like, where am I? So the kind of thing that we're going to do is think about, where am I based on my current velocity and where I think I am, odometry, which is uncertain, it's unreliable, versus, for example, where I think I am based on noisy sensors. So that's like two independent noisy things. The odometry, you can't completely rely on it. You've probably run into that by now. The sonars are not completely reliable. So they're two kind of noisy things. How do you optimally combine them? That's where we're heading. So the idea is going to be, here I am. I think I'm a robot. I think I'm heading toward a wall. I'd like to know where am I? So the kinds of data that we're going to look at are things like, I think I know where I started out. Now, my thinking could be pretty vague. I have no clue, so I'm going to assume that I'm equally likely anywhere in space. So I have a small probability of being many places. That just means that my initial distribution is very broad. But then I will refine where I think I am by taking into account where I think I will be after my next step. So I think I'm moving at some speed. If I were here, and if I'm going at some speed, I'll be there. So we will formalize that by thinking about a transition. I think that if I am here at time t, I will be there at time t plus 1. And I'll also think about, what do I think the sonar should have told me? If I think I'm here, what would the sonars have said? If I think I'm here, what would the sonars have said? And we'll use those as a way to work backwards in probability. We use Bayes' rule to say, I have a noisy idea about where I will be if I started there. I have a noisy idea of what the sonars would have said if I started there. But I don't know where I started. Where did I start? That's the way we're going to use the probability theory. So for example, if I thought I was here, and if I thought I was going ahead two units in space per unit in time, I would think that the next time I am here. But since I'm not quite sure where I was, maybe I'll be there, and maybe I'll be there. But there's very little chance that I'll be there. That's what I mean by a transition model. It's a probabilistic way of describing the difference between where I start and where I finish in one step. Similarly, we'll think about an observation model. If I think I'm here, what do I think the sonars will have said? Well, I think I've got some distribution that it's very likely that they'll give me the right answer. But it might be a little short. It might be a little long. Maybe it'll make a bigger error. So I'll think about two things. Where do I think I will be based on how I'm going? And where do I think I'll be based on my observations? And then we'll try to formalize that into a structure that gives me a better idea of where I am. That's the point of the exercises next week when we won't have a lecture. So this week, we're going to learn how to do some very simple ideas with modeling probabilities, with thinking about these kinds of distributions, and the idea next week then is going to be incorporating it into a structure that will let us figure out where the robot is in some sort of an optimal sense. So thinking about optimal, let's come back to the original question. How much would you pay me to play the game? We had some votes. They didn't add up to one. What should I do to make them add up to one? AUDIENCE MEMBER 2, INAUDIBLE Divided by the sum. Look at all you know already. So you now know all this great probability theory. So the question is, can we use probability theory to come up with a rational way of thinking how much it's worth? Most of you thought that it's worth less than $10. So how do we think about this? How do we use the theory that we just generated to come up with a rational decision about how much that's worth? OK. Thinking about the bet quantitatively. What we're going to try to do is think about it with probability theory. There are five possibilities inside the bag. Originally, there could have been four white, or three white and one red, or two and two, or one and three, or zero and four. That was the original case. You didn't know. I didn't know. They were thrown into the bag over here. We didn't know. How much would that game, how much should you be willing to pay to play that game? OK. Someone asked, how many white ones and how many red ones did the person put in the bag? I don't have a clue. We need a model for the person. Since I don't have a clue, one very common strategy is to say, all these things I know nothing about, let's just assume they're all equally likely. OK? That's called maximum likelihood when you do that. There's other possible strategies. I'll use the maximum likelihood idea just because it's easy. So I have no idea. Let's just assume that here's all of the conditions that could have happened. The number of red that are in the bag could have been 0, 1, 2, 3, or 4. I have no idea how the person chose the number of LEGO parts. So I'll assume that each of those cases is one fifth likely, since those five cases. OK. Now I'll think about what's my expected value or the amount of money that I'll make if the random variable S, which is the number of red things that are in the bag, was little s, which is either 0, 1, 2, 3, or 4. OK, well if there's 0, how much money do you expect to make? None. If there's four reds, how much money would you expect to make? 20 bucks, right? If there are two reds, you would expect to make 10 bucks. And everybody see that? I'm trying to think through a logical sequence of steps for thinking about how much is it worth to play the game. So this is the amount of money that you would expect given that the number of red in the bag, which you don't know, were 0, 1, 2, 3, or 4. That's this row. What's the probability? What's the expected value of the amount of money you would get, and that happens? Well, I have to use Bayes' rule. The probability that the, what I need to do is I have to take this probability times that amount to get that dollar value. So over here, in the event that there are four reds in the bag, I am expecting to get $20, but that's only one fifth likely. Right? Because there don't have to be four reds in the bag. So I multiply the one fifth times the 20, and I get $4. So my expected outcome for this trial is $4. Here, I'm expecting to make $10 if I knew that there was two reds in the bag, but I don't know there's two reds in the bag. There's a one fifth probability there's two reds in the bag. So one fifth of my expected amount of money, which is $10, is $2. So then in order to figure out my expected amount of money, I just add these all up. That's like marginalizing. And I get the expectation, 4 plus 3 is 7, plus 2 is 9, plus 1 is 10. So this theory says that if I can regard the person who put the Legos in the bag as being completely random, I should expect to make $10 on the experiment. So that means you should be willing to pay $10, because on average, you'll get back $10. If you wanted to make a profit, you ought to be willing to pay $9. Because then you would pay $9 expecting to get $10. If you really would like to make a loss, then you should pay $11. Yeah? AUDIENCE MEMBER 2 What if we assume that these events are equally likely? PROFESSOR. completely arbitrary. So there's theories, more advanced theories, for how you would make that choice. So for example, if in your head you thought that the person just took a large collection of Lego parts and reached in, then you would think that the number of red and white might depend on the number that started out in the bin. But I don't think that's probably true. The person was probably looking at them saying, I'll throw in one red, I'll throw in one white. So you need a theory for doing that. And I'm saying that in the absence of any other information, let me assume that those are equally likely and see what the consequence of that would be. The consequence of assuming that is that I should expect to get $10 back. What happens if you pull out a red, as we did? How does that affect things? Well, it increases the bottom line. I started out, again, with the assumption that all five cases are equally likely. Now I have to ask the case, how likely is it that the one that we pulled out was red? Well, it's not very likely that the one that I pulled out was red if they were all white. The probability that happened is 0. What's the probability if there were two that the person pulled out a red? Well, two of them are red, two of them are white. Two out of four cases would have showed this case of pulling out a red. So this line then tells me how likely is it that the red was pulled. Then what I want to do is think about what's the probability that I pulled out a red and there was 0, 1, 2, 3, or 4. So I multiply 0 over 4 to get 0 over 20, 1 fifth times 1 fourth to get 1 over 20, 1 fifth times 2 fourth to get 2 over 20. So those are the probabilities of each individual event happening. But they don't sum to 1. So then the next step, I have to make them sum to 1. So the sum of these is 1 half. So I make them sum to 1 this way. So now what's happened is it's relatively more likely, 4 out of 10, that this case happened than that case. I know for sure, for example, that there's not 4 whites. The probability of 4 whites is 0, 0 out of 10. So what I've done is I've skewed the distribution toward more red. By learning that there's at least one, I now know that I know additional information. These were not equally likely. In fact, the ones with more red were relatively more likely. So if I compute this probability times that expected amount, I now get a much bigger answer for the high number of reds. So I still get 0, just like I did before for this case, because there's no reds in the bag. But now it's much more likely that they're all red, because I know there was at least one red. And then the answer comes out 15. So the idea is, my overall assessment, don't go to Vegas. You could have made a lot more money by offering $13, because on average, you should have expected to make 15. So what I wanted to do by this example is go through a specific example of how you can speak quantitatively about things that are uncertain. And that's the theme for the rest of the course."
    },
    {
        "Lec 9 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-FANl3evX0FQ.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So today I want to finish up thinking about circuits. And the major topic for today is just one, thinking about abstractions that we can use for thinking about circuits. The abstractions are things that capitalize on linearity. And they include things like Thevenin and Norton and superposition. So what I want to do is finish up thinking about circuits. And just to get you thinking about circuits, let's think about where we are. Last time, we saw that one of the issues in thinking about circuit design is the fact that every part in principle interacts with every other part, which makes the design process harder than the design process was for things like linear systems, where we thought about boxes having inputs and outputs. In linear systems, when we thought about boxes having inputs and outputs, the output didn't necessarily have any effect on the input unless there was an explicit path, feedback. In circuits, there's feedback always. There's no way to avoid it. And in fact, that coupling can make thinking about the circuits very difficult. And we introduced that idea by just thinking about, even if you wanted to close a switch in a circuit, that's logically the same as adding a new part. And when you do that, that's going to change the currents and voltages everywhere. So that's kind of the big thing. That's kind of the thing that's different about circuits from what we've been thinking about before. Because that complicates design, we'd like some way of dealing with it. And the way we introduced and the way that you used in the lab so far has been to use a buffer. We can use an op amp to make a buffer. And the buffer has this nice isolation property. In this particular circuit, it has the property that it copies this voltage to the output, regardless of what's at the output. That means we could put a switch here. We could change the light bulb. We could do anything we wanted to. And we would still know, because of the properties of the op amp, we would still know that this voltage is going to be 8 volts, regardless of what we put there. So that's very nice. That gives us a modularity. That gives us a way to design things. It lets us design the left, make a circuit that generates 8 volts without knowing precisely what's going to be the ultimate circuit that that drives. So that's nice. It allows us to do modularity. And in many instances, that's a very good solution. In fact, in the problems that you're working on in lab, where you're trying to drive a motor, that's an excellent solution. It's not always an excellent solution. In some sense, it's very expensive. An op amp is a complicated part. If you were to look inside an op amp, there's some two dozen transistors in most op amps. So it's not an inexpensive part, especially when you think about this kind of a circuit that only has three parts to the left. The op amp is actually a more complex device than anything else up there. So op amps are wonderful. Op amps allow us to make buffers. Buffers are wonderful. But they're not always the best solution for thinking about modularity. And in fact, there's other ways. And so that's what we want to think about today, is other ways for achieving modularity in circuit design. And the key to thinking about this is to think about, well, what's the worst thing that could happen? If I change this part arbitrarily, just how bad can things get? So I'll let you answer that. Think about that circuit. Assume that this is a 90 volt power supply, 3 ohm, 6 ohm. But this can change. R0 can change. I've tabulated some values of R0 and putative corresponding values for V0 and I0. Are my putative answers right? So take a minute, talk to your neighbor, figure out how many of these 10 blue numbers are right or wrong. OK. OK. It's very quiet. You are allowed to talk to people. OK. OK. So how many of the numbers are wrong? So raise your hand. Indicate by a number of fingers how many mistakes are in the table. More votes, more votes. Come on, come on. If you had talked more, you could blame it on your neighbor more easily. So talk quickly. OK. So does everybody agree with their neighbor? OK, I don't see a single right answer. So take 30 more seconds and think about it again. I don't see any right answers. So assume your answer is wrong. Colorblind. OK, so how many of the voltages and currents are incorrect? OK, how about a re-vote? So how many of the blue numbers, the currents and voltages, how many of V0 and I0, how many of those are incorrect? OK, not very many votes. I'd say about 80% correct now. That's definitely an improvement. What would have to be true? If I wanted to prove that some of these numbers were wrong, what could I do? Give me a condition that would have to be true if the numbers were correct. Yes? AUDIENCE MEMBER 2, INC. V0 has people I0, R0? V0 has people I0, R0. You know that this resistor better obey Ohm's law. That's what we mean by that symbol. So it better be the case that V0 is I0, R0. So you'd want this number to be the product of that and that. So 0 is the product of 0 and 30. That looks good. 30 is 2 times 15. 36 is 3 times 12. 48 is 6 times 8. 60, well, that's a little marginal. How about if I rearrange it a little bit and say, what if R is V over I? If I is 0, that would make the resistor infinity. So there's a way of thinking about that. Last line is correct. It's a little funny. Maybe the forward way of thinking about it is, well, what if I made the resistor infinite? If I made the resistor infinite, what would be I? 0. And what would be V? What would be the voltage of the resistor? What would be the voltage of the resistor? Voltage divider. So if there's no current here, then you can use the voltage relationship. The voltage divider relationship here, 6 over 3 plus 6 times 90, which is 60. So if you use straightforward reasoning, saying what if the resistor were infinitely large, what would happen? Then you would conclude that the bottom line is OK. If you show that R equals V over I, are you done? What else has to be true in order for the numbers to be true? Not very hard. So the voltage drop on the R map, the resistor has to be the same as the R map. So the voltages, that's an instance. What you just said is an instance of KVL. Basically, the voltage around all the loops better be 0. The sum of the voltages around all the loops better be 0. And the sum of the currents in all the closed surfaces better be 0. KVL better be satisfied everywhere. And KCL better be satisfied everywhere. So in particular, we could ask about this node. We could say, do the currents flowing into that node sum to 0? So if you take a line here, let's take this line. If V0 happens to be the voltage here, so if V0 were 36, then that puts some voltage on this leg. 36 would be what? 54. So then you'd have to say, you'd have to compute then the current through here and the current through there and see if those currents sum to 0. And in fact, if you do those calculations, you can convince yourself that those are all true. So the answer was 0, although those answers were true. The point of doing the exercise was to just remind you about how you solve circuits, but also to let us look at patterns. The interesting thing in this problem is the pattern that results between the V's and the I's. So if I were to make a plot, in fact, those V's and I's all fall on a straight line. Well, that's pretty interesting. So if I plot just this V versus this I, so V equals 0, 30, 36, 0, 30, 36, 48, 60, and the corresponding I's, 30, 15, 12, 6, 0, 30, 15, 12, 6, 0, those points all fall on a straight line. That suggests there's some pattern here. And if there's a pattern, then there might be a way to exploit it. So that's what I'm trying to develop, is a way to exploit the pattern that results when parts of circuits interact. The interesting thing that we, so not only is it true that there's a simple pattern, but it turns out that the pattern is completely independent of the thing that I put on the right. The pattern is a property of the circuit to the left. One way to convince yourself of that is to substitute, take that resistor out and put in a voltage source, and redo the problem. This time, instead of assuming that there's an Ohm's law type resistor here, assume there's a constant voltage, and that constant voltage is adjusted to 0, 30, 36, 48, 60. If you resolve that problem, you get exactly the same current. The answer continues to fall on exactly the same line. So that's a very interesting pattern. The idea is that when the circuit on the left interacts with the circuit on the right, regardless of what the circuit on the right is, you get a simple relationship between the voltage and current that comes out of the circuit on the left. So that motivates the idea that we can think about the left-hand circuit as some kind of a generic part. We call that generic part a one-port. Think about this circuit on the left. The thing that's in the red box, it's got at its terminals. The terminals are the things that poke through the red box. First off, it has two terminals. Two terminals is just like all of our other parts. It's like a resistor, it's like a voltage source, it's like a current source. It's a two-terminal device. And just like a voltage source or a resistor or a current source, there's some voltage across those terminals and there's some current that flows in those terminals. So there's some current that goes into plus, and that same current comes out the minus. That'll be true for this circuit, just the same as it's true for a resistor or a voltage source or anything else. The only difference is the thing that's inside the box, the thing that's inside the one-port, is more complicated than it was for a simple resistor or a voltage source or a current source. So what we can think about is this whole red box just looks like a super part. So the interesting thing that happens is this red box behaves like a one-port, like a super part, and just like a resistor has a relationship between V and I, V equals IR, or a voltage source has some kind of a constraint, V equals V0, or a current source has some kind of a constraint, I equals I0. This funny part has a relationship between V and I that's given by that curve. In some sense, it's not very different. So what we want to do now is figure out the rules that govern the currents and voltages that flow through one-ports. And in particular, how special was this straight line thing? I mean, if they were always a straight line, that would be really easy, right? So the next question I'd like to ask is just how often are we expecting to see straight lines there? Well, I already said the primitive elements that we think about, ohms, ohm resistors, voltage sources, current sources, they have straight line constraints between the voltages and currents that they can generate. So think about what I'm doing. I'm trying to think about a rule that's going to let me describe the currents and voltages into that red box from the previous slide, much like I would describe the voltages and currents in an ohm's law resistor. I can tell you the voltage-current relationship, V equals IR for an ohm's law resistor, independent of what it's connected to. That's the reasoning that I'm using here. I'm going to try to figure out, independent of what it's connected to, what will be the voltage-current relationship for the red box? So the question is, when are we expecting straight lines, and when are we not expecting straight lines? So here's a simple circuit. Here's a super part made out of one linear resistor, one ohm's law resistor, and one voltage source. What's the current-voltage relationship for that part? What if I put a red box around the whole thing and ask you to draw the IV curve, the current-voltage curve? Which would it look like? A, B, C, D, which should have been 1, 2, 3, 4, so you can raise figures, or none of the above? So take 30 seconds, figure out what would be the IV curve for this part. Now, beloved friends of straw So if you map a to d into 1 to 4, which of those plots describes the current voltage relationship for that circuit? Map a to d to 1 to 4. OK, you're quiet, and the success rate is smaller than usual. OK, about 50% correct. Take 30 seconds, reconsider your answer. Try to get this up to 85% or so. So which is the best plot? Which plot best characterizes the circuit on the top? OK, that's much better. That's by 75%, certainly not 100%, but better. OK, so how do you think about this? One way to think about it is special cases. Can you think of any special cases that are particularly easy to check? Zero. OK. What equals zero? V or i. Yes. So what if you were to make i zero? If you were to make i equal to zero, first off, if you made i equal to zero, what's special in the plots? If you made i equal to zero, then you were on the x-axis. So if you make i zero, what would v be? If you made i be zero, how big would it be? 5 volts, right? If you make i be zero, then there's no voltage across the resistor. So the total voltage here will be the same as the voltage across the voltage source, v equals 5. So the intersection on the x-axis should be at the point v equals 5. Make sense? Now, if you set v equals zero, how big would i be? If you set v equal to zero, how do you set v equal to zero? Negative 2.5. Negative 2.5. So what is negative 2.5? Negative i. So how do you set v equal to zero? What's the circuit way of saying set v equal to zero? You set v equal to zero in a circuit by grounding, by putting a wire. So you run a wire from here to here. Voltage across a wire will always be zero, right? So you set something to zero by putting a wire across it. We call that a short circuit. There's a short path for the voltage to travel. So you put a short circuit here, and then i becomes v over r. And the only trick is it's up, right? Current likes to flow down the electrochemical gradient. So it likes to go down the electrical gradient. So the current's going to go up. But the reference direction for this i is down. So i is minus 2.5. So the intersection on this axis is minus 2.5. So we know that it has to go through a negative point on the bottom, and it has to go through a positive v on the x axis, and the only curve that does that is a. And if we wanted to be a little bit more fancy, we could figure out the general rule. We could just write an expression for vr. Well, by kvl, vr is always the difference between v and 5 volts. And we could write an expression for the current through the resistor. That's just v over r. vr is v minus 5. r is 2. So I get some expression which is, lo and behold, linear in v. So just like that more complicated circuit that I looked at, I ended up with a straight line relationship in the current voltage. The relationship between the current and voltage falls on a straight line. So how special is that? Well, it actually happens pretty robustly. Think about what would happen if I had two parts that are generically, so I'm thinking about these as being generic boxes. Inside could be anything. There could be a current source, a voltage source, a linear resistor, or some other one port. So if I had a generic box here and a generic box here, and I connected them in parallel, under the condition that each of the generic boxes had a straight line current voltage relationship, it's easy to argue that the resulting relationship between current and voltage for the parallel combination is also a straight line. All you need to do is realize that if you hook two things in parallel, the parallel voltage is the same as v1 and v2. So all the voltages are the same, and the currents add. So Ip, the current end of the parallel combination, is the sum of the two i's. So if you think about the relationship between v1 and i1 and the relationship between v2 and i2, you could derive the relationship between Ip and vp by just adding. v1 equals v2 equals vp. That's what we saw here. And the current at the bottom is the sum of the other two currents. So in particular, if this was a straight line and that was a straight line, the sum of two straight lines is a straight line, right? If you add two straight lines, you get a new straight line. So what this shows is that if you started with parts that were themselves straight lines, parallel combinations would generate a new part with another straight line. Same sort of thing happens if the two parts were in series. In series, we would have I series is the same as I1 equals I2. So that's equivalent of the y-axes, where previously we had equivalence of the x-axes. And now if both of these boxes had linear IV curves, I would now add horizontally rather than adding vertically. But I'd have the same result. If the two individual one ports had linear IV curves, and if I add horizontally, I'll get a new linear IV curve. And in fact, if you put any combination of parts that have linear IV curves together to form a new one port, the IV curve for that new one port will be a linear function. And the way to see that is to think about linear equations. Remember when we solve a circuit, we have to have one law for every element. Like Ohm's law, it's V equals IR, or voltage source is V equals V0, or something like that. We need one law for every element. And then we have KVL and KCL. If we start with the assumption that each component has a straight line relationship between voltage and current, which is true for linear resistors, voltage sources, and current sources, those equations that describe those parts are what we call linear equations. Linear equation is an equation where the function of the unknowns is a quote linear function. It has the form some constant times an unknown, plus some other constant times some other unknown, plus in principle any number of those. But it has to be a linear function. So you're not allowed to have things like V squared in there. But if you think about things like voltage sources and current sources and Ohm's law, they don't have squares in there, they're linear equations. And the idea then, when we're solving for the IV relationship for this new one-port thing, all we're doing is we're solving a system of linear equations. Sort of in the abstract, the idea is that we write down all the component equations inside the box, we write down all the relevant KVL and KCL, and then we solve. So each one of those equations is going to be linear. So it's going to be something like, say, a0, x0, if x is my unknown, x could be a current or a voltage. And then I might have sum of x1, and I might have sum of x2, and I might have a whole bunch of things, and they all add together to be some constant, like that. So that might represent part one. That might represent one of the components in the box. Then I would have another component, which would be some other linear equation. Then I would have a KCL equation. Well, KCL is easy, because that only has currents in it, and the multipliers are all 1 or minus 1. So that's clearly linear. And I have KVL equations, those are linear. And the point is that when you solve linear equations, you get a new linear equation. So think about solving this by the method of substitution. I could figure out what is x0 here. If I use this equation to figure out x0, x0 would be a linear function of all the other x's. So then if I plug that linear function into here, I replace this linear equation with a new linear equation with one fewer or none. If I keep doing that, I just keep replacing linear equations with other linear equations. When I'm all done, I'm left with a new linear equation. That's why if you start with parts that have linear IV curves, you'll end up with a straight line IV curve. So that idea that the IV function is a straight line is quite robust. It will happen any time you build a circuit out of ideal parts, whereby ideal parts I mean ohms, law of resistors, voltage sources, and current sources. So that has a very interesting circuit interpretation. If I know that an arbitrary circuit can be represented by a straight line, if the IV curve can be represented by a straight line, well, that generates an equivalent set. There's obviously more than one circuit that could generate the same straight line. Here's a circuit that can generate that straight line. In fact, it will always be true that I can generate a circuit with one voltage source and one ohm's law resistor that will mimic the behavior of any arbitrary combination of resistors, voltage sources, and current sources. All I need to do is think about, you take the complicated thing, figure out its straight line plot. Now you read off some critical numbers from this plot. You say, OK, well what if the current were 0? Well, if the current were 0, I'd be on this axis. If the current were 0 in the circuit, V would be V0. So that means you look over here, you figure out the x intercept, and the x intercept is the value of the voltage source. Similarly, if you figure out the rate of growth of I, so more generally, if you solve for I, I will be the difference between V and V0 divided by R. That's just Ohm's law for the resistor. And that then lets you figure out the law for the slope. The slope over here is going to turn out to be 1 over this resistor. This should be R0. These two should match. So the slope of this line is 1 over that resistor. So in general, regardless of how complicated the box is, figure out the straight line in IV curves, read off the x axis, read off the slope, and that lets you construct a simple circuit that has the same VI curve. We call that circuit the Thevenin equivalent. What that means is you can think about a complicated circuit, regardless of how many parts, by a circuit that just has two. That's an abstraction that lets us think more simply about complicated circuits, even if there's no buffers. This is true always. Always. It's not true. It's not the case that if I change the thing that I put here, if I change the thing I put here, this relationship is still true. It's the equivalent of Ohm's law for a complicated circuit. Ohm's law is what I get if what's in the box is a single resistor. If what's in the box is complicated, more generally, this is what I get, the Thevenin equivalent. Of course, there's lots of circuits that have this IV curve. So a different one is a current source with a resistor. That's called a Norton equivalent. You do the same sort of thing. What would happen if here, if I set V to be 0? Well, how do I make V be 0? I make V be 0. Did I write that on the slide? No, I didn't write it on the slide. I make V be 0 by putting a short circuit here. I connect a wire. If I put a wire here, then how big is the current I? Well, if I put a wire here, it's easier for this current to go through the wire than it is to go through that resistor. So all of this current goes through the wire. All of this current is I0. So if I put a short circuit across V, if I make V be 0, how big is I? Minus I0. All of the current I0 goes through. It just goes through backwards. So that's how I get this point. So if I wanted to replace some complicated circuit with a Norton equivalent, I would take the complicated system, figure out the I-V curve, read off the intercept on this axis, change the sign. That's the most confusing part, by the way. This is the error that you all make. There's a minus sign in the current relationship. It just makes us feel good to have the arrow go up. And if you make the arrow go up, it's in the wrong direction to I. So when we did the Thevenin equivalent, there was no sign from it. This was V0. So the voltage on the voltage source is equal to the x-intercept. But when we do the Norton, the current in the current source is minus the y-intercept. Same idea, though. So the idea is that Thevenin and Norton equivalent circuits are equivalent in the sense that they generate the same voltage that the more complicated circuit did. So that means for thinking about the circuit, we can ignore the complicated stuff and just know two numbers, V0 and R0 or I0 and R0. So one more step. What this all means is that if you can represent the current voltage relationship for an arbitrary circuit in terms of a straight line, that means when we're trying to characterize an arbitrary circuit, it doesn't matter how complicated it is. It could have 100 parts in it. Regardless of how complicated it is, I only need to measure two things in order to fully characterize it. If a circuit is made out of linear resistors, voltage sources, and current sources, three special linear parts, if a circuit is composed entirely out of linear parts, it doesn't matter how many parts are in it. There could be 100. There could be 1,000. I only need to measure two things to get a complete description. And that's because two points determine a straight line. I know by having proved it by using linear algebra. I know by using linear algebra that the solution is a straight line. I know from geometry that two points determine a straight line, I only need to find out two points. So by convention, the easiest two points is usually, the simplest cases, set the voltage to 0 and set the current to be 0. So that motivates the idea that if I have an arbitrary circuit, if I want to figure out this reduced complexity abstraction, say I want to make a Thevenin equivalent, what I would do is first ask the question, how big is the open circuit voltage? Open circuit means there's no connection between the terminals. That means the current is 0. If the current is 0, I'm on the x-axis. So over here, I'm on the x-axis. So I'm thinking about the red point. Open circuit over here means there's no connection here, which means the current is 0. And all I need to ask is, regardless of how complicated is the circuit, how big is the voltage that I would measure here? So in this circuit, if I have a, I confused myself for a moment, if this current is 0, then this voltage drop is 0. This voltage source prescribes the voltage between these nodes to be 1 volt. So v0 is 1 volt. So I just found one point. I set i to be 0. I found the open circuit voltage. Then I need to find one other point, because I only need to find two. So I'll find the short circuit current. Imagine that I put a wire between the input and the output, and I'll compute how much current this circuit generates in that wire. Again, the only confusing part is that the reference directions are backwards to the way you might have expected them to be. If I put a short circuit here, then the voltage between these nodes is still 1 volt. That's what the voltage source always says. So the current that flows through this wire, the short circuit current, is just this v over that r, except it's in the negative direction. And the way you can think about that is that the slope of this curve has to be positive, because of the way Ohm's law works. Increasing the voltage better tend to increase the current, because that's what Ohm's law resistors do. So I need to have this slope be positive if it's going to be Ohm's law. So I characterize just those two points, and then the resistance is simply the ratio of the two. So the resistance is related to the slope. It's 1 over the slope. You don't need to worry about that. The resistance is always v over i. So you just take the open circuit voltage and divide by the short circuit current, minus sign, and you get that the resistor must be 2 Ohms. Is that all clear? The idea is that we're trying to build an abstraction that lets us simplify the way we think about circuits without introducing buffers. So that means then that these two circuits are equivalent to that circuit in the sense that they all share the same VI curve. If you substituted one circuit for the other, you couldn't tell from outside the red box, which was on the inside of the red box. So I'll do an example now. Think about what if I wanted to find the equivalent, the Thevenin equivalent for this circuit. I just do the things I just told you to do. First thing I think about is what's the open circuit voltage. So if I think about open circuit, there's no connection here. That means this current is 0. That means that the voltage that develops is the voltage divider. So I get 7 and 1 half volts, 3 over 1 plus 3 times 10. So that gives me one point. That tells me the voltage source for the Thevenin equivalent. Then for the second point, I want to think about the short circuit current. So I consider putting a wire here. And then I compute the amount of current that flows in that wire. And in this circuit, this wire shorts out that resistor. So all the current goes through this wire, and none of the current goes through that resistor. So that means that the total current that flows is 10 volts divided by 1 ohm, which is 10 amps. And then I know that the equivalent resistance is the ratio of the open circuit voltage to the short circuit current, except I have to worry about the minus sign. And so I end up with the equivalent resistance being the 7.5 volts, which is the open circuit voltage, divided by 10 amps. So I get 0.75 ohms. And so the answer then is that here's the circuit I started with, here's the Thevenin equivalent circuit. They're identical in the sense that they have the same VI curve. And you can just sort of see why that has to be true. If you think about, here's the two circuits. And if you think about the simple cases, if you set i to be 0, the voltages better be the same. Well, over here it's 7.5 by the voltage divider. Over here it's 7.5 by the fact that there's no current going through that resistor. And the short circuit current better be the same. So over here, if you short this out, you're going to get 10 amps. Over here, if you short it out, you're going to get 10 amps. So you can always go back and forth. The point is that you can substitute the simpler circuit for the more complex circuit because they have the same VI curve. OK, just to make sure that you're following me, here's a question that has to do with taking the same circuit but considering the Thevenin equivalent, Thevenin or Norton equivalent, at three different ports. Here I'm thinking about what would happen if I looked in terminal A? What if I looked in terminal B or looked in terminal C? Figure out the Thevenin and Norton parameters and see if there's an error in the table. OK. OK. OK. OK. OK. OK. OK. OK. OK. So how many errors are in the table? OK. About 50% correct again. So which entry don't you like? 2D, that's exactly right. So 2D. How do I figure out 1A? How do I figure out V0 for the A circuit? So the definition of V0 is the open circuit voltage. So I need to figure out for the A circuit how big would be the voltage across A if there was no current flowing in the leg of A. Is everybody clear on that? So the thing that I would do is I'd think about there's no connection here. What's the voltage here? So how would I calculate that? Yeah. So the current-divider relationship, figure out that the current going through that branch is going to be 4 amps. Precisely. So the first thing you do is you say, here's a current source that's going through two resistor legs. So that's perfect set up for the current divider. So the amount of current that goes in this leg compared to that leg is the ratio of this resistance to the sum resistance. And if you work that out, you're going to get 4 amps coming through here. So then after you know the current through this leg, it's an easy matter to take the current and turn it into a voltage. So the voltage at the A port is 4 amps times 5 ohms is 20 volts. Is that clear? And similarly, but with a different answer, the voltage at the B terminal is the same 4 amps, but now times 10 ohms. So that's how we got 40. And at C, it's the same 4 amps, but now it's times the sum, 4 times 15 is 60. Everybody's happy about that? So the point is that when you generate an equivalent circuit, it depends upon which set of terminals you're using. You can't just take a circuit and say, give me the Thevenin equivalent. You have to say, give me the Thevenin equivalent looking somewhere. So I could look in the A port, the B port, or the C port, and I get different V0s. How would I compute the I0? Short circuit current. So what I would do is I would short this out. When I do that, all of the current flows in this leg, and none of the current flows that way. So that means I have equivalently 10 ohms in parallel with 10 ohms. Then by the current divider, how much current goes down one leg? Half of it. I get a different answer over here, because now I short out that node, which shorts out that resistor. So now I get a different ratio of resistors. It's not the same as the first, so I know that this answer can't be 5. And in fact, if you work it out, the answer is 20 over 3. And finally, if you short here, then you know that that short circuit shorts out both this series combination and that one. So all of the 10 amps goes through that, so you get that 10 amps. Then how do you get r0, which is the ratio of v0 over i0? So if you take v0 over i0, you get 4. Here, if you do the right answer, you get 6. And here, if you do that, you get 6. The point is that the Thevenin equivalent that you get is different. The Norton equivalent that you get is different, depending on which ports you're looking at. OK. There's two reasons for thinking about this. So why am I thinking about all these equivalent circuits? So I wanted to have an abstraction that was useful for thinking about how parts interact. I wanted a way of thinking about what would happen if I changed the load on a circuit without having to recalculate all the voltages and currents throughout the circuit. And so the Thevenin and Norton idea is a way of doing that. That's important from a practical sense, because when you buy a part, when you buy an electronic part, they tell you how it works by telling you the Thevenin equivalent, or the Norton equivalent, or whatever is the easy way to think about it. So there's a practical reason. When you buy an op amp, they tell you how good the op amp is by telling you how big is the equivalent resistance at the output. So it has a practical value, because it's the way you specify an electronic part. When the manufacturer makes a part, they can't know what you're going to do with it. So they tell you how it works by telling you something about the equivalent circuit. There's also a different reason for thinking about this, and that is because it's conceptually simplifying to think about Thevenin and Norton equivalents. So here's an example that's very much like the first problem that I worked out. What would be the effect of closing this switch on the current I? We solved a problem very much like this last time. And one way you can solve it is you figure out I in two cases, when the switch is open and when the switch is closed, and you figure out whether it went up or down, and you know the answer. The point is that if you think about this in terms of equivalent circuits, it's completely trivial. If I think about what would happen, I'm interested in what happens at I. What I do is split the circuit into two pieces, the stuff to the left of I and the stuff to the right of I. I make a Thevenin equivalent for both of them, and then I jam together the two Thevenin equivalents. What I get in detail is showed here. So if I look left, I see a 20 volt source with a 4, 4, and a 2, that has a Thevenin equivalent that's showed here, which is independent of the state of the switch. So the same Thevenin occurs on the two sides, switch open or switch closed. If I make a Thevenin over here, there's no sources, so it's just going to be a resistance. So when the switch is open, I have 2 ohms. When the switch is closed, I have 1 ohm. I don't even need to figure out what are these part values to see that if I make this resistance smaller, which happens if I close the switch, the current goes up. I mean, it's true that I showed here that the Thevenin voltage is 10 and the Thevenin resistor is 4. I don't even care. Regardless of what the Thevenin voltage was, regardless of what the Thevenin current is, it's going to be the same when the switch is open and closed. And that's a powerful statement. I know it's the same. So when I close the switch, all I've really done is I've made that resistor smaller. The net series resistance is down, the current is up. So there's two reasons for thinking about Thevenin and Norton equivalents. One is practical value. The other is conceptual simplicity. It lets you simplify the way you think about a circuit, and as a way of gaining intuition without even solving the equations. Most circuit designers don't solve circuit equations. They know what it's going to do just by looking at it. And this is the kind of reasoning that they use. OK, there's one more topic. The idea of Thevenin and Norton is really derived from linear algebra. The basic parts of most interest are linear. And when you put together a system out of linear parts, you get a linear system. There's one more consequence of linearity that is terribly useful, and that's the idea of superposition. If you have a system of equations that's linear, and if you have multiple sources, things are very simple. You can see that over here. If I had multiple sources, the system of equations that I get wouldn't look quite like this. These terms, the constant terms, they're the ones that come from the drives, the voltage sources, the current sources. So if I had two drives, I would get something else here. I would get plus a, n plus 1, for example, source 1, source 2, and I would get source 1, source 2. So the idea is that if I have a system that has multiple sources, I know from the structure of the linear equations that it's just more constant terms. OK, what's that mean? That means you can just use linear algebra to see that the answer to this problem is the sum of the answers to that problem plus the answers to that problem. That's just linear algebra. What that means in terms of circuits is I can figure out the response to a circuit by turning on the sources one at a time. That's called superposition. And generally speaking, it's a lot easier than solving the circuit out the long way. So here I've got two sources. So say I wanted to compute i in response to v0 and i0. What I would do is I would turn off the i0 source and calculate the voltage that results just from the voltage source. That's the same as setting this to 0 and finding the response to this. So if I want to set i to 0, the way you set i to be 0 is open circuit. If you open circuit something, there's no current going to flow through it. So I replace the current source with an open circuit and compute the i that would result when the current source isn't there. OK, well that's easy. If the current source were not there, this would be an open circuit, the current i would just be the total voltage going through R1 and R2. So the answer i1, the first component of the current i, would be v0, the result of the voltage source, divided by the sum of R1 and R2. Now that's not the whole answer. That would be the whole answer if i0 weren't there. So now I have to worry about the other case. What if only i0 were there? Well, now I have to set v to be 0. Well, setting v to 0 is not open circuiting it. You can't just reach in, grab the voltage source, and throw it away, because that will make the current 0. If I want the voltage to be 0, I have to short circuit it. So what I do then is I leave the current source alone, and I replace the voltage source with a short circuit, guaranteeing that v is 0. Then I ask, how big is i2, the component of i that results from the current source? Well, if I've short circuited this, then I just get a current divider. This current has to do with how readily the current divides between R1 and R2. The amount that goes through the R1 side is in proportion to R2. Make R2 bigger, more of it goes through R1. Standard current divider, except for the slippery minus sign. So if I only had the current source, the current i2 would have been current divider operating on i0 with the slippery minus sign. So that means, by superposition, that the result of having both sources on is just the sum of those answers. That's a very big simplification. If you've got multiple sources in a circuit, you can think about them all at once. And if you're really good at writing linear equations and solving them with pencils, you'll get the right answer. But there's an enormous simplification if you just turn off all but one and do them one at a time. So here's a problem. Here's a very simple circuit. Compute v using superposition. v is the voltage across the resistor. How big would v be if you use the idea of superposition? So what's the answer? 50% correct, roughly speaking. OK, I want to use superposition. So how big would the voltage v be if all I had was the voltage source? One, how big would the voltage be if I only had the current source? Ah, got half of you. That explains the 50% correct. How big would the voltage be if I only had the current source? It's tempting to say one. I say tempting because that's the wrong answer. What's wrong about the answer one? What's the voltage due to the current source? Voltage due to the current source is the voltage that the current source would have generated if the voltage source weren't there. If the voltage source weren't there, then half of you got it right, so half of you can shout the answer. So if the voltage source weren't there, the voltage source would be 0. If the voltage at the voltage source were 0, then v would be 0. So the voltage due to the voltage source is 1. The voltage due to the current source is 0. The sum of the two is 1. The answer is 1. OK? Make sense? So very closely related problem. What's the current I? Solve that by superposition. So how big is the current I according to superposition? Wonderful. So how big is the current I generated by the voltage source? One. How big is the current I generated by the current source? Negative 1. This one is 0. So there's two problems. They're trivial by superposition. They're not too hard by not superposition. But the point is that they're trivial by superposition. So what we saw today, the goal for today was to generate some abstractions that let you think about the way parts interact with each other. Because that's a central issue when you're thinking about circuit design. And we built the idea of Thevenin's and Norton's and superpositions. That was the basic idea. And the sub-theme, or maybe I should say the major theme, is really that. It's the importance of linear algebra. So the take-home message is probably take 18.06. So this was all about the application of 18.06 to the solving of circuits. See you later."
    },
    {
        "Rec 7 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-lF-7mmPHhG0.mp3": " Hi. Today I'd like to talk to you about poles. Last time I ended up talking to you about LTI representations and manipulations, and in particular I want to emphasize the relationship between feedforward and feedback systems in order to segue us into poles. Using what we know about that relationship, we can find the base of the geometric sequence, and that geometric sequence actually represents the long-term response of our system to a unit sample response, or a delta. That value is also what we refer to as the poles. At this point I'll go over how to solve for them and very basic properties of them so that we can use the information that we have about the poles to try to actually predict the future or look at the long-term behavior of our system. First, a quick review. Last time we talked about feedforward systems, and in particular I want to emphasize the fact that if you have a transient input to your feedforward system, you're going to end up with a transient response. There's no method by which a feedforward system can retain information over more than the amount of time steps that you feed information into it. Feedback systems, on the other hand, represent a persistent response to a transient input. Because you're working with a feedback system, information that you put in can be reflected in more than one time step, and possibly multiple time steps depending upon how many delays you have working in your feedback system. Last time I also drew out the relationship between feedforward and feedback systems. You can actually turn a feedback system or talk about a feedback system in terms of a feedforward system that takes infinite samples of the input and feeds them through a summation that takes infinite delays through your system. You can represent that translation using a geometric sequence. The basis of that geometric sequence is the object that we're going to use in order to predict the future, and that's what we're talking about when we talk about poles. You can have multiple geometric sequences involved in actually determining the long-term behaviors of your system. If you only have one, then things are pretty simple. You find your system function. You find the value associated with p0 in this expression. It's OK if there's some sort of scalar on the outside of this expression. We're working with linear time invariant systems, so that scalar is going to affect the initial response to your system, but in terms of the long-term behavior, it doesn't matter as much. So don't worry about it right now. Relatedly, if you're solving for these expressions in second or higher order systems, you're going to end up having to solve partial fractions. You can do this, and in part, one of the reasons that you would want to do this is so that you can get out those scalars if you're going to be talking about the very short-term response to something like a transient input. We're not going to be too interested in those in this course. We're mostly going to be talking about long-term response. So we can get around the fact that we're dealing with higher order systems and not solving partial fractions by substituting in for an expression called z, which actually represents the inverse power of r, and then solving for the roots of that equation. If you substitute z in for 1 over r in this denominator and then solve for the root associated with that expression, you'll get the same result. You'll actually end up out with p0. So now we know how to find the pole or multiple poles, if we're interested in multiple poles. What do we do now? I still haven't gone over how to figure out the long-term behavior of your system. The first thing you do is look at the magnitude of all the poles that you solved for and select the poles with the largest magnitude. If there are multiple poles with the same magnitude, then you'll end up looking at all of them. If you have different properties than the ones here, you can end up with some complex behavior. I would not worry about that too much, or I would ask a professor at TA when that happens. But in the general sense, if your dominant pole has a magnitude greater than 1, then you're going to see long-term divergence in your system. This makes sense if you think about it. If at every time step, your unit sample response is multiplied by a value that is greater than 1, then it's going to increase. And in fact, the extent to which the magnitude of your dominant pole is greater than 1 is going to determine your rate of increase and also determine how fast your envelope explodes. Similarly, if the magnitude of your dominant pole is less than 1, then in response to a unit sample input or a delta, your system's going to converge. This also makes sense intuitively. If you are progressively multiplying the values in your system by a scalar that is less than 1, then eventually you're going to end up converging to 0. To cover the only category we haven't talked about, if your dominant pole is actually equal in magnitude to 1, then you're not going to see convergence or divergence. And this is one of the places in which the magnitude of the scalar that you end up multiplying your system by can become relevant. We're not going to focus on this situation too much, but it's good to know what actually happens when the magnitude of your dominant pole is equal to 1. The other feature that we're interested in when we're looking at the dominant pole of a system is if we were to represent the dominant pole in this form, what the angle associated with that pole is if you were to graph that pole on the complex plane using polar coordinates. If your pole stays on the real axis, or if your pole does not have a complex component, then you'll see one of two things. The first thing that it's possible for you to see is that you'll get absolutely non-alternating behavior. Your system response stays on one side of the x-axis and either converges, diverges, or remains constant as a consequence of input of the unit sample. And you won't see any sort of alternating or oscillating behavior. This only happens when your dominant pole is real and positive. If your dominant pole is real and negative, this also means that it's still on the real axis, but its value is negative. So if you're looking at polar coordinates, it's going to have an angle of pi associated with it. This means you get alternating behavior. And what I mean when I say alternating behavior is that your unit sample response is going to jump across the x-axis at every time step. This is also equivalent to having a period of two. The other situation you can run into is that this angle is neither 0 nor pi. And at that point, you're going to be talking about oscillatory behavior, or a sinusoidal response that retains its edges at the envelope of your function. In order to find the period, or in order to find the amount of time it takes for your unit sample response to complete one period, you're going to take the angle associated with your dominant pole and divide 2 pi by it. This is the general equation for a period. This covers the basics of what you want to do once you already have your poles. Next time, I'm actually going to solve a pole problem and show you what the long-term response looks like, and also talk about some things about poles that I've pretty much skimmed over. And at that point, you should be able to solve and look at poles for yourself."
    },
    {
        "Lec 3 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-CG4ihzTaGdM.mp3": " The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. MIT.edu. Hello and welcome. So last week we started to think about programming. Programming was the first module in this class. And it was important for two different reasons. First, we're going to use programming throughout the term in the study of all the different things that we do. So it's important that you learn to program now just so that you can use that tool. More importantly, perhaps, we didn't just learn how to program, we focused on how to program in a fashion that would let us construct complicated systems out of simpler systems. This is a way that we can manage complexity. This is the only possible way that we can make complicated systems. So that was the more important intellectual theme from the first part, where we introduced our mantra, PCAP. Primitives, means of combination, abstraction, and identifying patterns. That's the key to this modular approach to hierarchical kind of design. Today what I want to do is start the second major theme. First theme was the design of complex systems. We saw that by reference to programming. Today we're going to start thinking about modeling and controlling physical systems. The idea here is not so much how you construct systems, but we will get back to that. The idea is to characterize the systems that you've constructed and say something about their metrics as being positive or negative. So what we want to do is, in fact, focus on behavior. So to illustrate that, I'll start with an example. This is an example that you did in Design Lab last week, or for some of you yesterday. The idea was to program the robot so that it could sense the distance to a wall, represented two ways here, the view that you would get from SOAR, and a more schematic representation showing the position of a robot, the position of a wall. The idea is that you can sense the position to the wall using the sonars. You know where you would like to be, because some user told you, you'd like to be, say, half a meter away from the wall, and your job was to move the robot, write a program, that moves the robot from where it is to where you'd like it to be. So here's the kind of behavior we might have liked. So I'll do that again. So we might have liked that if you started here, you'd have a nice, smooth progression up to where you'd like to be, very graceful, ballet type, and you just sort of smoothly glide into the position that you'd like. Some of you probably achieved that behavior, and some of you probably did other things. So that might be the intended behavior. One way to achieve the intended behavior is to use what we call a proportional controller. In a proportional controller, you make the command be in some way proportionate to the intended response. So imagine this code, which might establish a class for finding the wall. So the important thing, as we saw for all state machines, the important thing is to define a start state and a getNextValues routine. What this getNextValues routine does is it establishes the desired distance to be a half a meter. It figures out the current distance to the wall by reading the sonars, and then it specifies an action. So the first question is, what would you like fval to be in order to make a proportional controller? Which of those expressions makes sense? Take 30 seconds, talk to your neighbor, figure out some answer between 1 and 5. I will ask you in 30 seconds to raise your hand with that number of fingers. So what's the right kind of expression if we wanted the controller to be proportionate? Everybody raise your hand, show me some number of fingers. The vast majority is saying 2. Everybody likes the idea of current minus desired. Why is that the right answer? That is the right answer. Why is that the right answer? How do you prove that to somebody? Yeah? AUDIENCE MEMBER 2. Well, the terms of by-side desired, once you reach your desired distance, then you're still going to have to store a velocity of 1, which really doesn't make sense. So that method we might call extreme cases. Think of the simplifying cases that give you some insight into the problem. So one simple case is, what if desired and current were the same? You'd better stop. That's a simple case. So the simple case says that you better have one of these, whatever the right answer is, it better have the property that when current equals desired, f-val is 0. Otherwise, it's not going to work. And in fact, using just that one simple case, you can eliminate all the ones except 2. There are some other simple cases. What are some other simple cases? Simple cases. Yeah? AUDIENCE MEMBER 2. So if current were bigger, that would mean that you were starting out way over here someplace. So if you were way over there someplace, you'd want the velocity to be positive. The forward velocity should be positive. That's how you would disambiguate the sign. Similarly, if the current were shorter than desired, if you were too close to the wall, you'd like the forward velocity to be negative. So that's the proportional controller that we'd like. So we might fill in our wall finder class this way. And then when we built it, if things went really well, we would get exactly the behavior we wanted. But if things went more naturally, it wouldn't quite work that way. And you might get a different kind of behavior. Let's do that again. So here's the resulting behavior for that simple controller that I just showed. So in some sense, that's not as good behavior. And the way we want to think about behaviors, the way we're going to develop today, is to think about behaviors in terms of signals, plots. So the first question is, what plot best represents the behavior that I saw here? So which of those plots best represents that behavior? Take 30 seconds, talk to your neighbor, figure out what's the right answer. OK. OK. OK. OK, so which behavior best represents the illustrated cartoon that I showed previously? Raise your hand. Show some number of fingers so that I can see if you're with it. Not all correct, but more than 90% correct. More than 90% answer is number 2. What's good about number 2 that's not good about numbers 1, 3, and 4? Yeah? The initial line is different than the final line. The initial is different, so here we're showing the initial and final being roughly the same. So what's showed here is a plot, current distance on the y-axis, step number on the x-axis. And so we can see from here, and the implication of the axes, by the way, is 0. So the implication of the vertical axis is it intersects this at 0 unless I label it otherwise. And similarly, this horizontal line intersects the vertical at 0 unless I label it otherwise. So that's the point 0, 0. So the implication here is that the initial and the final values are the same as they are here. Here they're not. What makes 2 better than 1? Yes? Yes? AUDIENCE MEMBER 2 The current distance starts out bigger than it ultimately is. So we start out bigger than the final value. The final value is presumably half a meter. We start out roughly twice that far. And then we see some approach. The approach is not monotonic. So the answer is 2. Why do you think it undershot? So on the way to going from 1 to 1 to 1, it transiently went through something smaller than 1.5. Why do you think it did that? Yeah? It's a small time where it's moving about 1 to 1. It could overshoot it before it's more data. So there's a small interval of time between when it does one thing and it does something else, when it senses and moves, for example. So there could be a time delay in the system. And in fact, that's true. There is a time delay. So it takes some amount of time for the sonars to register the distance. Then it takes some amount of time for the computer inside the robot to register that the sonar is told it's something different. Then it takes some time from the time the computer commands the wheels until the robot starts moving. All of those cumulative effects mean that you have the potential to overshoot where you're going because there's delay in the system. There's inertia in the robot. All of those reasons can lead to overshoot. And the point of today is to figure out some way of predicting and correcting for those kinds of unintended behaviors. So what we will do then is develop an approach focused on signals, not systems. So far we've been thinking about how do you build the system. Now we're going to think about behavior. We're going to think about analyzing that behavior. And the focus is going to be on what was the input, what was desired, what was the output, what was achieved. So we're going to be looking at those output signals in order to figure out how good the behavior was. That approach is called the signals and systems approach. The idea is characterize your system, whatever that system is, a physical system, a mathematical system, a computational system, whatever it is, think about it by the way it transforms an input signal into an output signal. That's kind of a bizarre way to think about systems. So let me just illustrate that by way of a system that you've all seen before. Here's a simple system. You've all seen this, right? Anything like 801 may ring any bells. So that's a simple system. You all know how to solve it. There's a gazillion ways you could solve this system. Free body diagrams, kinetic energy, potential energy. There's a gazillion ways you could do it. You all know how to do it. That's not the point. The point is that we're going to learn a different way to solve it. We're going to think about the mass and spring system not like potential energy and kinetic energy, not like free body diagrams. We're going to think about it as transforming an input signal into an output signal. So the input signal, it's kind of arbitrary what I use besides the input and the output. But clearly the thing I have control over is my hand. So it seems natural to associate a variable with the position of my hand. That would be x. Also seems natural to associate a variable with the position of the mass. That could be the output y. That's not unique. Every time we try to solve a problem, we ask ourselves, what's the meaningful input? What's the meaningful output? The meaningful output could have been the force on the spring. I'm just sort of arbitrarily saying for the purpose of my analysis, I'm going to consider the input to be the position of the hand, the output to be the position of the mass. And I'm going to think about the mass and spring system as a box that transforms x into y. So rather than thinking about it in terms of free body diagrams and kinetic energy and potential energy, I'm going to think there's some input signal x and there's some output signal y. And what I'd like to do is, given x, calculate y. OK, bizarre. Why would I do that? One of the reasons I want to do that is that it's a very general way of thinking about behaviors. It works for the mass and spring system. It works for water tanks. What happens if I have water flowing into a tank that's leaky? Well, it leaks into another tank, which is leaky, and that leaks more. Completely different physics. Probably wasn't covered in 8.01. Probably is something you could figure out. The point is that from the signals and systems point of view, I'm going to map this physics, whatever it is, into this structure. Think about the tank system as the system that transforms some input. The input is that there's water spurting for some small interval of time. The output is that there's water coming out. And the idea is that I'm going to characterize the system, whatever it is, as the rule that transforms the input signal into the output signal. Here's a third example. I could think of a cell phone system. Here again, there are very complicated ways we could think about the system. But I'm going to take the particularly simple approach called signals and systems, that I'm going to characterize the phone system by the way it transforms some input sound into some output sound. And as you can imagine, there's a way of thinking about performance in terms of that transformation. Ideally, we would like this to bear some resemblance to that. So the point then is that we're going to focus on behaviors, and to do so, we're going to think about the signals and systems approach. Represent a system, whatever it is, by the way it transforms inputs into outputs. One of the reasons we like the approach is that it's so general. So you can use it for virtually any kind of a system for which you can develop a mathematical underpinning. You can use it to analyze electrical systems, mechanical systems, optical systems, acoustic systems, biological systems, financial systems if you're on the dark side. So there's lots of different kinds of systems that are amenable to this kind of approach. Also, this approach has a nice modularity. Having represented a cell phone by a transformation from sound in to electromagnetic field out, as illustrated by this cartoon depicting sound going in and having a radio wave transmitting to a tower. Then we represent tower-to-tower communication some way, maybe via an optical fiber, maybe via a satellite. Then tower-to-cell by the same kind of reverse transformation that we used in the first one. We can piece those all together. We can treat them as modules because each box takes an input signal and makes an output signal. The method is oblivious to the underlying physics. That affords a certain amount of power. And in particular, it's very modular. You can put together modules that represent very different physical substrates. That allows us to go back to PCAP. If the underlying representations for the different physical substrates are the same, we will be able to, and we will over the next three weeks, develop a bunch of techniques for combining multiple modules into one. That provides the same kind of abstraction and modularity that we saw in programming for the last two weeks. So that's the idea. What we want to do is represent a system by the way it transforms an input signal to an output signal. There are many different kinds of inputs and outputs, but a fundamental distinction that we are going to have to make is continuous time and discrete time. This system works in what we will refer to as continuous time, because my position of my hand is a continuous function of the continuous variable time. The robots, by contrast, work with what we will call discrete time, steps. It turns out the math for those two different approaches are very different. And we will focus entirely in this class on discrete time, because our area of application is the robot. It's not that continuous is deeper or harder or anything like that, it's just different. So we'll focus on discrete time. And the point of today then is to develop some representations for signals and systems of the discrete time nature that will let us analyze and predict behaviors of systems like the robot system. The first class of methods for representing such systems is difference equations. Difference equations are a lot like differential equations, except there's no differentials. There's differences. Difference equations are the discrete time analog of differential equations for continuous time systems. Simplest possible example here. Say I have an output y that, for reasons that I don't care about, I know can be represented as the difference between two values of the input, x at time n and x at time n minus 1. That's a way to represent the behavior of a system, a discrete time system, by using a difference equation. That's, in fact, almost a complete description of the system. So let me just explain the way you would use that. It's almost because I didn't tell you anything about the input. I'll tell you something about the input for the purpose of example, where I'll use the simplest possible input that I can imagine, something that we'll call a delta function. A delta function is a signal. It's a discrete time signal. It has the value 1 if the time index is 0, and has the value 0 everywhere else. It's in some sense the simplest possible signal that we could imagine. So it's natural to start there. So what I want to do now is think about if this were a characterization of the system, and if this were the input to that system, what would the output be? That's, after all, that was the question that I posed at the beginning. We would like to build a representation for a system so that we can predict the output given the input. So how does that work? So given the difference equation, all we need to do is step through it. It's trivial. We call that analyzing by step by step. So given the difference equation, given the input signal, all we need to do is sequentially go through the different values of n and think about the implication of the system on that input. So if I were to use the value of n being minus 1, this general form of the difference equation tells me that the minus 1 value of n for the output is related by this difference with the input. So y of minus 1 is x of minus 1 minus x of minus 2. Since both of those are 0, it says that the output at time minus 1 is 0. Trivial, right? And similarly, we can just iterate through the solution to the whole signal. So y of 0 is x of 0 minus x of minus 1. x of 0 is that special 1 that is 1. So now we get 1 minus 0, which is 1. y of 1 is x of 1 minus x of 0. Now the special 1 is on the other side of the minus sign. So the answer is minus 1. y of 2 is x of 2 minus x of 1. They're both 0. And in fact, all the answers from now on are going to be 0. So what I just did is a trivial example of I used a difference equation to represent a system. And I figured out the output signal from the input signal. So that's the method that we call, that's the representation for discrete time systems that we refer to as difference equations. Difference equations are very powerful. As we will see of all the representations we look at, difference equations is the most compact representation. But there are features of other kinds of representations that are also valuable. So the next representation I want to look at is a block diagram. What I'm trying to show here is a picture, a diagram, that represents the same system that we just analyzed with difference equations. Here though, I'm thinking about it as a signal flow path. I'm thinking about what's the cascade of operations that you need to do on each sample in order to get from the input to the output. So the difference equation said every value of the output should be equal to the corresponding value of the input less the value before that. I represent that in a block diagram by saying there's a straight-through path, y of n is equal to x of n. The plus just adds the signals on these two paths. So y of n is equal to x of n. Subtract out, because I'm multiplying by minus 1. Delay, so I'm getting the one from before. So this block diagram just is a symbolic representation of that difference equation. The value of the block diagram, we'll see several of them, but one of them is focus on signal flow path. If you want to visualize the transformation from input to output, the block diagram can provide insight, visual insight, into what that transformation is like. So as before, if I gave you this representation rather than the difference equation, you could still step by step and figure out the way it worked. It's still easy. There's one new caveat here. We have to start the system in a state. The state that we will usually talk about is what we will call at rest. At rest just means all the outputs of all the delays are initially 0. So that specifies the state of the system at the time the signal is turned on. So the system is at rest, which means that all the delays start out with an output equal to 0. So at rest means this delay has an output of 0. Well, if I tell you the output is 0 at time 0 for this delay, then it's a simple matter of stepping through what is the output for each corresponding input. So the special value of the delta function is that it's 1 at the time 0. So at the time 0, there's 1 coming in. That 1 makes it through the adder, adds to 0. Well, this is 0 because it was at rest. So the 1 adds to 0 and the output becomes 1. Notice that the 1 also goes down this path and goes through the gain of minus 1 to give me minus 1. But I'm in step 0. So at step 0, the output of the delay is 0, not minus 1. So the output then for time equals 0 is y equals 1, just like we saw for the difference equation. After all, I'm hypothesizing that those two systems are the same. It better give me the same answer. So then at the next instant, as the time index goes from 0 to 1, two things happen. The input goes from 1 to 0, and the delay box gets updated. The delay is now reporting to me the value that was at its input. So the output of the delay, which had been 0 because it was at rest, becomes minus 1. So then what happens? The 1 goes to 0, the 0 goes to minus 1, the 0 adds to minus 1, and we get an answer which is minus 1. Then the input becomes 0. That 0 comes down here, the minus 1 goes to 0. We end up with 0 being added to 0, and the next answer is 0, et cetera. So the idea then is that you can step through the block diagram representation just like you would a difference equation. It's just that now we're thinking about these blocks characterizing the system rather than thinking of math as characterizing the system. Why on earth would you do that? What's good and bad? What's the relative merits of difference equations versus block diagrams? Take 30 seconds, talk to your neighbor, figure out some good feature of each. I'll take it out. Multiply. OK, we'll start with the easy one. What's a feature? What's a property of the difference equation that makes it very good? I already said it. What's good about difference equations? Yeah? They can be solved mathematically. They can be solved mathematically. The block diagram could be solved, maybe not mathematically, but kind of. Could you refine that a little more? What's special about difference equations that's different from block diagrams? You can't. You can't? Yeah, yeah. They're concise. They're little. It's a very simple expression to say that. It's, by contrast, a bit more complicated to draw this picture. It's mathematically concise. It's completely accurate, self-contained, concise, small. It's a very concise representation of a system. So why would we want to use block diagrams? Can anybody think of any good reason for block diagrams, other than Freeman's upfront saying, do block diagrams? Electrical engineering? Electrical engineering. There should be a zebra reason, I would hope. I don't disagree with that reason. Why do electrical engineers like this? Yes? It's a more physical representation of this. It's more physical. Is there anything that you can see in this that you can't see in that? Yes? In the way that you're actually going to be programming it, you're going to make a new label, and you're going to make a state machine that won't apply the vector 1, and you're going to see how to connect them. That's a really good point. It's kind of isomorphic to the implementation. Everybody get that? It's kind of a picture of the way you would build the system. Along those lines, there's some bit of information. There's actually more information in this one than there is in that one. There's exactly one bit more information in the block diagram. What's that bit? Delay? There's kind of a delay by the n minus 1. Yeah? AUDIENCE MEMBER 2. The input and output are explicit. Yes. The errors are the big difference. You can't tell from the difference equation what's the input and the output. You can tell from the block diagram what is the input and the output by the direction of the arrows. So there's more information in the block diagram. There's another way of thinking about it, and this is kind of a summary of several comments that came from the audience. The difference equation is declarative. It tells you a true statement about what the system will do. The block diagram is imperative. It tells you what to do now. Take the input, put it into an adder. Take the input, multiply by minus 1. Put it into a delay. Take the delay output. Put it into an adder. The representation with a block diagram is imperative. It tells you what to do. So there's extra information, but it comes at a cost. It's a more complicated representation. It's a whole picture instead of just an equation. What we'd like to do, and what I'm going to do now, is develop a different mathematical approach where you get a different equation that has the same properties of concision, the same conciseness, but also contains all of the information that was in the block diagram. And the way to do that is to change our focus. And this is the big abstraction of the day. Change our focus from thinking about samples to thinking about signals. Stop thinking about X of n. Start thinking about the input signal X. This is the same kind of lumping that was key to abstraction in Python. Put all the interesting data together into a list. Put all the interesting operations together into a definition. Here, put all of the interesting samples together into one signal. So what we want to do is develop a math by which we can operate on signals instead of samples. So what I'm going to do is replace the representation x of n, little x of n, with cap X. Cap X means all of the n's. It's the signal X. Cap Y means the signal Y. And I'm going to reinterpret all the boxes. This box means take this signal, the whole thing, all n values of it, multiply it sample by sample by minus 1, flip the whole signal upside down. So I'm going to reinterpret all of the operations on the block diagram in terms of signals rather than samples. To do that, I need a mathematical representation for the delay box. And I'm going to call that R, the right shift operator. If you apply the right shift operator to a signal X, it takes the whole signal X and shifts it to the right one. That's all it does. So I'm going to say Y equals R applied to X, or more abbreviated, RX, simply says let Y represent the signal that is the same as X except every sample is shifted to the right. The entire signal is shifted to the right. That's going to let me represent this block diagram this way. Y, the whole signal Y, is the sum of the whole signal X, subtract out R applied to X. Or even more concisely, calculate Y by applying to X the operator 1 minus R. So I'm thinking now of an operator. An operator is not something that works on a number. Operations work on numbers. Operators operate on signals. So I'm thinking about operator expressions. I'm going to try to formulate the transformation from the input to the output in terms of operators. The way I take X, which is the input, and turn it into Y is to operate on it with the operator 1 minus R. Just to see that you're with me, connecting signals and samples, assume that Y is RX. Which of the following is also true? Take 30 seconds, talk to your neighbor, figure out some numbers 1 through 5. Take 30 seconds, talk to your neighbor, figure out some numbers 1 through 5. Take 30 seconds, talk to your neighbor, figure out some numbers 1 through 5. Take 30 seconds, talk to your neighbor, figure out some numbers 1 through 5. OK, so which representation works best? 1, 2, 3, 4, or none of the above? Everybody raise their hands, tell me some number. OK, virtually 100% correct. The answer is 2. Why is the answer 2? Can somebody explain that concisely? No, no, no, I asked that wrong. Of course everybody can explain it concisely. Do I have a volunteer to explain it concisely? Yes? The R operator just shifts all of the values in X and M to go right, so you just add in 1 to these, basically? So if you think about it, a good way to think about this is to think about simple cases. That's the same thing I talked about earlier. What's a simple case? Well, what if X, let's imagine that X is simple. So let's say that X looks like that. So X is delta. What would happen, what is the signal Rx? It's a right shift operator. So what's the signal Rx look like? It's shifted to the right, right? That's the whole point. So the right shift operator gives you that signal. And we've said that that's y. So is it true that y of n is x of n for all n? No. Is it true that y of n plus 1 is equal to x of n for all n? Well, is it true for n equals 0? Yeah. Thank you. So if we did n equals 0, we would get y of 1, which happens to be 1, and x of 0, which also happens to be 1. And if we chose any other n, we would get two 0's. So at least for this simple case, it seems to be true. And if you think about building up upon this simple case, you can convince yourself that number 2 is always true. And in fact, the general rule is going to be that the left hand side has to have a number that's bigger than the right hand side, which is only true for number 2. So the idea then is that by changing our focus, by looking not at samples, but looking instead at signals, we can generate an algebra that looks for all the world like difference equations, except it knows the direction from the input to the output. So it's more powerful. And in fact, this new algebra is going to obey some very simple properties, which we can get a hint at here. If we were to cascade two systems, imagine this system, which looks just like the system we've been looking at, but it's cascaded with a clone. The question is, what would be the behavior of that cascade? Well, according to our operator representation, this y1 signal is just the 1 minus r operator applied to x. Analogously, the y2 signal should be a similar 1 minus r operator applied to the y1 signal, which gives us a very concise representation for the cascade. The point of the slide is that the operator representation gives us a representation that is just as compact as difference equations. It has other features that it can be manipulated just like difference equations. So if we continue with the same example and try to think of the transformation on a sample level, we could say that y2 is y1, the straight path, minus y1 delayed. But then we could substitute for y1 of n, that y1 of n is x of n, this path, subtract x of n minus 1. Similarly, collapse, and we get a simple expression. Now, if we think about that same sequence of operations in operator notation, we get a much simpler expression. Throw away the index arithmetic. It's just r. So the y2 operator is 1 minus r applied to the y1 signal. The y1 signal is 1 minus r applied to the x signal. The total is 1 minus r the operator's squared, which by polynomial math is just 1 minus 2r plus r squared, the same thing we got there. The point is, the operator notation is just as compact as the difference equation representation. And it contains all the information that's in block diagrams. And it's just as easy to manipulate as a polynomial. So it's got lots of features that make it superior to difference equations. The most important of which is, you will be able to understand all systems that we represent using the r operator, using polynomial arithmetic, something you learned in high school. There's nothing new here. That's what we like, representations that simplify the task of finding an answer. We'll be able to find the answer to these operator expressions by treating them as polynomials. So you can get a feeling for the way that works by looking here, the power of this. So the power here is that, among other things, you'll be able to use the operator representation to prove equivalences. The idea is that here's a system that we looked at before that was the cascade of two simple delay systems. Here's a somewhat more complicated, somewhat simpler representation. The point is, it's different. And if we compare the operator representations for the two, we see that they are the same. And what I'm about to prove is that when the operator representations are the same, the systems represent the same transformations, provided they all start at rest. The provided is important. Obviously, since they have different delays in them, if the delays didn't all start out at 0, the differences in the delays could propagate into the output. So all of the statements that I'm making are premised on the idea of initial rest. The other important thing to remember about operators is that it's a higher level abstraction. We can think about the operator as composing things, but the things that are composed are whole signals, not samples. And here's an illustration of how to think about that previous example. How do you think about this transformation, 1 minus 2r plus r squared? What happens when you apply that operator to an input signal x? Well, let's say that x was our unit sample signal. In order to apply this operator, 1 minus 2r plus r squared, all we need to do is think about each component. 1 times x is x. Minus 2r applied to x is minus 2rx. Plus r squared x is just plus r squared x. So we start out with x, which is a unit sample. Minus 2rx means shift it to the right and multiply by minus 2. Shift it to the right, multiply by minus 2. Plus r squared x means shift it twice to the right. So the result, this operator 1 minus 2r plus r squared applied to x is just the sum of these things. So you can think about the operator expression. It's just like algebra, except that the elements are signals, not samples. And as I alluded previously, you can make powerful statements about the way these operators work, which map isomorphically onto polynomial math. So for example, it's easy to prove that if you were to cascade the 1 minus r operator with r. So start with x, apply 1 minus r, start with x, apply 1 minus r, then apply r. That's going to result in the same signal, assuming initial rest, as if you were to flip those operators. The way I can see that is by thinking about signal flow. You remember that I said one of the features, one of the powers of the block diagram representation is that we can look at signal flow paths. We can use that as a way of proving things. This system has two signal flow paths, the one that goes straight through that way, and the one that goes down this way and up that way. And because of the addition, the adder just adds the result of those two flow paths. So the first flow path introduces one delay. The second flow path inverts and puts in two delays. So there are two ways to get from the input to the output. Similarly, here there are two ways to get from the input to the output. One of them goes through a delay and then goes through the adder. The other goes through two delays and a minus one. But that's the same two signal paths that were in the top way, so that's a way of using the block diagram to prove a relationship about the operator. What I've just showed is that the operators obey commutativity. So what I was able to show is that I can commute these two operators. Doing a general proof is slightly more complicated. I proved it in a special case. But the general case works, too. The just like polynomials, operators commute. And I indicated why you should think that's true by thinking about signal flow paths. Multiplication distributes over addition. I apologize, the diagram in your notes is wrong. This is right. I will always post the notes. I get up in the morning. I bake coffee. I read the lecture notes. And I say, oh my goodness, there's a wrong figure. In this particular case, one of the staff members wrote me an email and said, hey Freeman, your slide something or other is wrong. He was right. So this is the right diagram. So the idea is that if multiplication distributes over addition, we should expect that r applied to 1 minus r would give r minus r squared. And we can again get a feeling for why that ought to be true by thinking about the signal flow paths. The two signal flow paths that represent here, this says, take the 1 minus r operator and apply it to x. Then apply r to the 1 minus r operator result. As opposed to, this one says, apply the r operator to x. Then apply the r squared operator to x and subtract them. If you think about the signal flow paths through those two systems, they're also the same two signal flow paths. And here's a more complicated example that shows associativity. You can think through that. Same idea. The idea, the big important point is, difference equations are a good representation for discrete time systems. They're mathematically compact. Block diagrams are a good representation, but they have more information. They tell you what is the input, what is the output, and what are all the different flow paths through the system. Operators kind of combine the best features of both. It's mathematically concise. It tells you which is the input and which is the output. And you can visualize all the flow paths by thinking about all the adds in the operator expression. OK, to make sure that everybody's up with me, how many of the following systems are equivalent? You have 30 seconds. OK. OK, so how many of those are equivalent? More hands, more hands. Not necessarily more fingers, but more hands. OK, about 75% correct, roughly speaking. OK, how many distinct signal flow paths are going through the first system? How many distinct signal flow paths can you see? Well, here's one. How many more are there? Three more. So here's one, here's one, here's one, and here's one. All we need to do to think about the system is think about all of the signal flow paths through all of them, make a sum, and see how many of them have the same sum. So the path with the greatest delay through this path is 2 times 2 delay, delay, 2 delays multiplied by 4. What's the path with the biggest delay through this one? Also straight through. Also a delay of 2, also a coefficient of 4. How about this one? So that's this way. So they all have the same path with maximum delay. The delay is 2 and the coefficient is 4. This one has four possible paths. This one only has three. So there's one straight through this way. There's one that has one fewer delay. And there's one that only has, so there's a straight through one. There's a delay of 2, and there's a delay of 1. So let's do the straight through one. This one has a straight through path, no delay, coefficient is 1. This one has a straight through path, coefficient is 1. This one has a straight through path, coefficient is 1. So all three systems have the same maximum delay path. They all have the same minimum delay path. We only have one in the middle yet. This one has two ways to get a delay of 1, 2 delay, 2r, or 2r. Since they're both 2r, they sum, so that's 4r. This one only has one way that we can get one delay, and that is to come this way and then go that way. That's 4r. This one, to get one delay, I take the center path. That's 4r. Each path has the same ways to get through the system with 0 delay, 1 delay, and 2 delays. They're equivalent in the sense that if you started them with initial rest, they would all generate the same output given the same input. So the answer is 3. OK? So far, I've only worked with systems that propagate the inputs systematically through to the outputs. We call such systems feed-forward. Things are a little bit different when you have cycles. We call such systems feedback. feedback. So what I want to think about now is how do you think about a system that has a feedback loop in it? The interesting thing that happens when you have a feedback loop is that the operator expression no longer represents a simple sum of input signals. Let's look at what happens here. So y is apparently the sum of two things. It's the signal Ry, which comes around that way. Everybody see that? So if I think about labeling this input as x, labeling this output as y, then the correct label for this point is, don't everybody shout at once. If this can be labeled as the point y, what is the correct label to label this point? Ry. So the signal y must be Ry plus x. What that says is that if I apply the 1 minus r operator to y, I should get x. That's a fine operator expression, except that it's not a formulaic operator applied to the input. The operator is applied to the output. The difference here is the difference between an imperative system. Like we talked about block diagrams, when we were thinking about samples, the block diagram tells you what to do, step by step. The block diagram, regardless of whether you have feedback, the block diagram always tells you, take x of n, add it to y of n, whatever. There's an imperative rule. Do this. We took the block diagrams, and we turned them into operators, and we ended up with something that is not imperative. This is much more the kind of statement we got when we did difference equations. This is a statement of truth. It's declarative. If you tell me the signal x, it must be true that the resulting signal y, when operated upon by 1 minus r, is x. So the idea is that it's a declaration. It's not an imperative rule. Does everybody get that? So this statement up here told me a rule. Start with x, apply the 1 minus r operator, and you will get y. That's an imperative operation. Do this. Operation, do this. This is a declaration. If you tell me x, y must be the signal that, when operated on by 1 minus r, gives you x. But it doesn't tell me a way to find it. It tells me a truth, but it doesn't tell me how to find that truth. So let's go back. Let's back up. We got a representation. We like the representation. It's concise. It has many of the features of block diagrams. It doesn't seem to be imperative. That's a problem. So let's back up. Think about how the same system that ran into a problem with the operator, think about what must the answer be? Well, the answer we can figure out by doing step by step. Imagine that it starts at rest. So the output starts at 0. And now I just tick through the samples. So when the first sample comes in, x is equal to 1. I'm thinking about the unit sample response. We call the delta function the unit sample. When the unit sample at time n equals 0 comes in, it has a value of 1. The 1 adds to the initial condition, which is 0, to give me 1. Then this output is 1. So when the clock ticks, the input goes from 1 to 0. But the output of the delay goes from 0 to 1. So when the clock ticks, I get another 1. And that persists. Does everybody see what's going on? So I initially had a 0 coming out of the delay. The unit sample made the first output be 1. But then that 1 fed back in to make this be 1, which combined with the next 0 to give me the same 1. And that state persisted. What's different is that the output signal persists long after the input went away. In fact, there is a prescriptive way to figure out the relationship between the input and the output. It's just that it takes an infinite number of delays. Here's an alternative system that would generate the same response to a unit sample signal as was generated by the simple feedback system. It needs to generate the answer 1, 1, 1, 1, 1 when the input is just 1. Well, the output at 0 happens through this path. The output at 1 happens through this path. The output at 2 happens through this path. 3, et cetera. There's a separate path for every one of those separate components of the output. That's how we can think about this construction. The input had a single non-zero entry. The output has an infinite number. We can think about that as resulting from an infinite number of paths. There's something similar about the simple feedback system, which can be represented by that operator representation, and the infinite system, the infinite feed-forward system. This is a simple feedback system. This is an infinite feed-forward system. There's something the same about those two. In fact, they're equivalent in the sense that if all the delays start out with an initial conditions of 0, they would generate the same response to all possible input signals. Those two signals are equivalent, and that's proved here. All you do is you say, OK, y2 is some relationship to r1. y2 depends on x2 this way. If x2 is the same as x1, I can substitute it. But x1, according to this rule, looks like 1 minus r, y1. When you multiply out this mess, you get y1. What I just showed is that if x1 is equal to x2, then y1 is equal to y2. Those two systems are the same. Well, that's weird. So there's something the same about that operator and that operator. We write that this way. So here's the feedback system. We think about that as representing the operator y over x, 1 over 1 minus r. So in order to calculate x, cross multiply by x, y is the operator 1 minus r applied to x. So we want to say y is the operator 1 minus r applied to x. What is the operator 1 over 1 minus r? Well, if you didn't know anything but polynomial math, you might have expanded this in a series. And in fact, that gives you exactly the right answer. If you were to expand 1 over r in a series, so for example, evaluate it using synthetic division, evaluate it with a Taylor series, however you want to do it. Think about r as though it were a number, just like you would if it were a polynomial. Expand it just like you would if it were a polynomial. And what you see is that there's a representation for this operator, 1 over 1 minus r, that is equivalent. That's exactly the same as if I applied the operator 1 plus r plus r squared plus r cubed to x. Those two are equivalent in the sense that if both systems start out at rest, and if they are both applied to the same input, they both generate the same output. So that gives us then a way of thinking about operators that have numerators and denominators. So make sure you're up to speed. A system is described by the following operator expression. Determine the output of the system when the input is a unit sample. What's the first thing I should do? This is one of those systems that has the r polynomial in the bottom. So it says that x must be the same signal by cross multiplying. x must be the same signal as the 1 plus 2r operator on the y. OK, that's backwards. That's not the way I want to think about it. How do I make that into a forward statement that tells me what operator gets applied to x? The answer is that. So what do I do? Multiply by? You could cross multiply. How do I convert this into an operator that looks like just a numerator times x? Yeah? Exactly. What I want to do is convert it by synthetic division, Taylor series, whatever method I want to think about what would 1 over 1 plus 2r look like. What's the reciprocal of 1 plus 2r? That is 1 minus 2r plus 4r squared, et cetera. So now I have this, which I apply to x, which is a unit sample. So now I want to think about applying this operator to the unit sample signal. But that's easy. The first term just brings out delta. Minus 2r applied to delta shifts the delta by 1. Gives me delta of n minus 1 and multiplies by minus 2. And that whole mess then just says that my output looks like this. If the input was x, which was a unit sample signal, my output has an infinite number of terms. Each one is a delayed version of the predecessor. And the weights go 1, minus 2, plus 4, minus 8, plus 16, and diverge. So what we just did was pretty complicated. We just solved a block diagram. But we did it with polynomial math. We did it with math that you learned in high school. That's the point. In fact, the point of today is that any system that's built out of simple parts, delays, adders, scalars, that sort of thing, can be represented by a difference equation. Fine, that's good. Difference equations are wonderful. They can equivalently be represented by an operator equation. The operator equation has more information in it. It knows how to get from the input to the output. It's imperative. It's easy to manipulate. You use the same rules that you use for polynomials. So all in all, this is a more powerful kind of representation. And any system that can be represented by a difference equation can similarly be represented by an operator equation. That's why we're focusing on operators. So final question. Do everything backwards now. Here's a block diagram. Find the associated difference equation. And the idea is to take advantage of operators. In the interest of time, let me just do it. If we wanted to do it, because I'm running out of time. So I could start with a block diagram. I could stay in block diagram domain. Presumably that'll work. That's hard. I want to do the easy way. So convert it to operators. How do you convert a block diagram to operators? Replace the delays by r. Label all the signals. x becomes x, y becomes y. I don't have a name for this, so I'll call it e, error. I don't have a name for this, so I'll call it w, who knows? And then I'll express each of the relationships imposed by the plus sign, this r or this r, by a line of operator reasoning. The plus says that the e signal is x plus w. The r says that the y signal is r applied to e. This box says that the w signal is r applied to y. I get three equations in r. I just solve algebraically. None of this different stuff. None of these square brackets with n's in them. I just use algebra. So I solve it algebraically, and I get this. And that translates into a corresponding difference equation, showed here. The point. The point is three different representations. Difference equations, block diagrams, operators. Operators are easiest. Even when I was asked to solve a problem that has no operators in it, it's easier to cast it into an operator expression, solve it in the operator domain, and then turn it back into a difference equation. Starting next week, we will figure out much more powerful things that we can do with operators. This is just the beginning. So with that, let me just summarize that we looked at three representations, and the point of the labs for the week are going to be to exercise this to get some experience with representing signals in Python."
    },
    {
        "Rec 12 _ MIT 6.01SC Introduction to Electrical Engineering and Computer Science I, Spring 2011-rOA1VC5aQ7Q.mp3": " Last time, we talked about op amps and the fact that they allowed us to abstract away certain components of a circuit and sample particular voltages from a circuit and then modify those voltages as we would like in an LTI fashion. Today I'd like to talk to you about some other interesting things that fall out of the fact that we're only dealing with LTI systems, in particular, of Thevenin-Norton equivalents and superposition. Thevenin-Norton equivalents is the idea that if you have a very complex circuit but it's still a LTI circuit, you can express it using a linear curve. And superposition is the idea that because you're also dealing with LTI components, if you're trying to solve a circuit, you can take the individual contributions of independent sources to that circuit and sum them in order to find out either the total current flowing through a particular component or the total voltage drop across a particular component. At this point, I'll walk you through both. Thevenin-Norton equivalents is an important concept in that you may have a very complicated circuit and you don't really want to talk about the entire complicated circuit. You just want to sample the voltage drop or current in a very particular location. Because we're dealing with LTI systems, we can actually express that particular sample as its relationship between I and V and possibly whatever resistive component is associated with the voltage drop across that sample. We can solve for this curve by looking at the location that we're sampling and finding the open circuit voltage associated with that position. Or if we were to leave the two terminals from where we're sampling open, what is the voltage drop across that section of our circuit? That's the point right here where the current flowing through the system is 0 and there's a given voltage associated with it. Likewise, if we want to find the other intercept, we can close those two terminals by running a wire across them and then look at the current that flows across that wire. That's the closed circuit current. The slope is going to tell us about our resistance, if any. If we're only dealing with a voltage source or a current source, then we're only going to be dealing with a straight line. Once we've solved for these values, we tend to express them either as a Thevenin equivalent circuit or a Norton equivalent circuit. And you can convert between the two. Let's walk through an example. Here I've got a simple circuit, and I would like to find the Thevenin equivalent of that circuit given that I'm sampling from above this resistor and below this resistor. The first thing that I'm going to do is find the open circuit voltage. Or if I were sampling from this point to this point, what is the voltage drop across this section of the circuit? Well, I've got 36 volts. There's 12 ohms of resistance in the section that I'm sampling, and there's 6 ohms of resistance outside the section that I'm sampling. So 2 thirds of my total voltage drop across this voltage divider is going to be accounted for inside my Thevenin equivalent circuit. So my Thevenin equivalent voltage is 24 volts. Now I want to go after the short circuit current. If I were to draw a wire from this terminal to this terminal, I'm curious what this value would be. As a consequence of connecting these two terminals, these resistors are going to be completely bypassed. We don't actually have to drop any voltage or put any current through these resistors because there's actually an infinitely easier way for the current to go. In this case, we're just going to follow Ohm's law. 36 divided by 6 is 6 amperes. But because of the directionality on the short circuit current, current in this system is going to flow in this direction. So our short circuit current is actually negative 6 amperes. That just leaves our Th. Once again, we're going to rely on Ohm's law. If we divide our voltage by our current, you can't have a negative resistance. We're actually going to divide by the negative of this. We'll get out four arms. Let's do it again, this time with a slightly different sample point, and find the Norton equivalent of our circuit. So if I'm only sampling across the 9 Ohm resistor, I personally still solve for the Thevenin equivalent voltage first because I think it's easiest. I'll just make note of it over here. The voltage drop across this resistor is determined by the fact that this is a simple voltage divider. The other resistors in this circuit sum to 9 ohms. So the voltage drop across this resistor is going to represent half of the total voltage in our circuit. That's 18 volts. The short circuit current associated with connecting these two terminals is 36 volts divided by the sum of these two resistors. Once again, we're going to end up bypassing this resistor entirely because think about it this way. If there was a resistor across here with zero resistance, the current would be the same as the current that we have in the resistor. So think about it this way, if there was a resistor across here with zero resistance, all the current would sink through it, or it would represent the location for which infinite current would flow. 36 divided by 9 is 4. In the opposite direction, we're going to say this is negative 4 amperes. In this orientation, the Norton equivalent amperage is actually the negative of this value. You sometimes see the arrow pointing downwards. This preserves our property of short circuit current. If this was short circuited, Isc would still be negative 4 amperes. And our Vth. If our Vth was the voltage drop across these two points, then the voltage drop across this resistor would be negative 4 amperes. If our Vth was the voltage drop across these two points, I haven't solved for Rth yet. There are a few ways to do this, but I'm going to use Vth and In. 18 over 4, we can reduce it to 9 over 2. And if I were to solve back out for the voltage drop across this resistor, just as a consequence of Inorton, I would get 18 volts, which is also the Thevenin equivalent voltage for the circuit. That covers Thevenin and Norton equivalents. I'm going to talk about superposition really quickly. Superposition is yet another circuit solving strategy. It falls out as a consequence of only working with LTI components, and it means that in order to solve for a given component current or component voltage, you could actually find it by taking the linear combination of every contribution as a consequence of independent sources. What do I mean when I say that? Well, if you have a circuit like this, and you might recognize it from the NBCC lecture earlier, you can actually express it as a linear combination of just the voltage source and just the current source. Note when I remove the current source, I end up with no connection, because if you have zero current flowing through a system, it's the same as if the connection has been removed. Note that when I remove the voltage source, I no longer have a voltage drop across that connection, but I maintain the connection. If in this example I was just looking for I1, I1 would be the linear combination of the contribution of I1 in this particular circuit plus I1 in this particular circuit. So this is the expression that I'm looking for. In the first circuit, I'm just going to use v equals IR, and I'm going to have three amperes in this direction. In this circuit, I'm actually going to have to use a current divider, identify that I have five parts total to distribute among these two branches, identify that this side is going to get the inverse of this side's contribution to the total parts, or this side is going to get the proportion of parts equivalent to this side's contribution, and vice versa. That this side is the side that I'm interested in, and so 2 fifths of the total current flowing through this current divider is going to end up in this branch. 2 fifths of 10 is 4. It's in the opposite direction of this current flow, so I'm actually going to contribute negative 4 amperes from this sub-circuit. This should match the results from the NVCC lecture. That's my super fast coverage of superposition, and also Thevenin and Norton equivalents. Next time, we're going to start a whole new module."
    }
]