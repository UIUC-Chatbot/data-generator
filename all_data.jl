{"gpt-3": [{"text": "Title: Finite State Machine Design Examples, Part I \n Text: {Finite State Machine Design Examples, Part I}\n\nThis set of notes uses a series of examples to illustrate design principles \nfor the implementation of finite state machines (FSMs) using digital logic.\nWe begin with an overview of the design process for a digital FSM, from\nthe development of an abstract model through the implementation of\nfunctions for the next-state variables and output signals.\nOur first few examples cover only the concrete aspects:\nwe implement several counters, which illustrate the basic \nprocess of translating a concrete and complete state transition diagram\ninto an implementation based on flip-flops and logic gates.\nWe next consider a counter with a number of states that is not a power of\ntwo, with which we illustrate the need for FSM initialization.\n As part of solving the initialization problem, we also introduce \n a general form of selection logic called a multiplexer.\n\nWe then consider the design process as a whole through a more general\nexample of a counter with multiple inputs to control its behavior. \nWe work from\nan abstract model down to an implementation, illustrating how semantic\nknowledge from the abstract model can be used to simplify the \nimplementation.  Finally, we illustrate how the choice of representation\nfor the FSM's internal state affects the complexity of the implementation.\nFortunately, designs that are more intuitive and easier for humans to\nunderstand also typically make the best designs in terms of \nother metrics, such as logic complexity.\n\n\n \n Question: \nQ. How does the design process for a digital FSM work? \n Answer: \n\nA. The design process for a digital FSM typically begins with the development of an abstract model. This model is then used to generate the functions for the next-state variables and output signals. Finally, the design is implemented using flip-flops and logic gates."}, {"text": "Title: Steps in the Design Process \n Text: {Steps in the Design Process}\n\nBefore we begin exploring designs, let's talk briefly about the general\napproach that we take when designing an FSM.  We follow a six-step\nprocess:{-8pt}\n\n{{}{}\n{}{}{}\n{develop an abstract model}{step-abs}\n{specify I/O behavior}{step-io}\n{complete the specification}{step-complete}\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n}\n{-8pt}\n\nIn Step {step-abs}, we translate our description in human language\ninto a model with states and desired behavior.  At this stage, we \nsimply try to capture the intent of the description and are not\nparticularly thorough nor exact.\n\nStep {step-io} begins to formalize the model, starting with its\ninput and output behavior.  If we eventually plan to develop an\nimplementation of our FSM as a digital system (which is not the \nonly choice, of course!), all input and output\nmust consist of bits.  Often, input and/or output specifications\nmay need to match other digital systems to which we plan to connect\nour FSM.  In fact, { most problems in developing large digital systems\ntoday arise because of incompatibilities when composing two or more\nseparately designed pieces} (or { modules}) into an integrated system.\n\nOnce we know the I/O behavior for our FSM, in Step {step-complete}\nwe start to make\nany implicit assumptions clear and to make any other decisions\nnecessary to the design.  Occasionally, we may choose to leave\nsomething undecided in the hope of simplifying the design with\n``don't care'' entries in the logic formulation.\n\nIn Step {step-repn}, we select an internal representation\nfor the bits necessary to encode the state of our FSM.  In practice,\nfor small designs, this representation can be selected by a computer \nin such a way as to optimize the implementation.  However, for large\ndesigns, such as the LC-3 instruction set architecture that we\nstudy later in this class, humans do most of the work by hand.\n\nIn the later examples in this set of notes, we show how even a \nsmall design can\nleverage meaningful information from the design when selecting\nthe representation, leading to an implementation that is simpler\nand is easier to build correctly.\n\nWe also show how one can\nuse abstraction to simplify an implementation.\n\nBy Step {step-logic}, our design is a complete specification in\nterms of bits, and we need merely derive logic expressions for the\nnext-state variables and the output signals.  This process is no\ndifferent than for combinational logic, and should already be fairly \nfamiliar to you.\n\n\n\nFinally, in Step {step-gates}, we translate our logic expressions\ninto gates and use flip-flops (or registers) to hold the internal\nstate bits of the FSM.  In later notes, we use more complex\nbuilding blocks when implementing an FSM, building up abstractions\nin order to simplify the design process in much the same way that\nwe have shown for combinational logic.\n\n\n \n Question: \nQ. Why is it important to design digital systems that are compatible with other digital systems? \n Answer: \n\nA. Most problems in developing large digital systems today arise because of incompatibilities when composing two or more\nseparately designed pieces into an integrated system."}, {"text": "Title: Example: A Two-Bit Gray Code Counter \n Text: {Example: A Two-Bit Gray Code Counter}\n\nLet's begin with a two-bit Gray code counter with no inputs.\nAs we mentioned in Notes Set 2.1, a Gray code is a cycle over all\nbit patterns of a certain length in which consecutive patterns differ\nin exactly one bit.\n\nFor simplicity, our first few examples are based on counters and\nuse the internal state\nof the FSM as the output values.  You should already know\nhow to design combinational logic for the outputs if it were necessary.\n\nThe inputs to a counter, if any, are typically limited to functions\nsuch as starting and stopping the counter, controlling the counting \ndirection, and resetting the counter to a particular state.\n\nA fully-specified transition diagram for \na two-bit Gray code counter appears below.\nWith no inputs, the states simply form a loop, with\nthe counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_1S_0 \n(before the ``/'') and the output Z_1Z_0 (after the ``/''), which are \nalways equal for this counter.\n\nBased on the transition diagram, we can fill in the K-maps for the \nnext-state values S_1^+ and S_0^+ as shown to the right of the\ntransition diagram, then \nderive algebraic expressions in the usual way to obtain\nS_1^+=S_0 and S_0^+={{S_1}}.\n\nWe then use the next-state logic to develop the implementation\nshown on the far right, completing our first counter design.\n\n\n\n\n\n\n\n\n\n\n\n\n \n Question: \nQ. What is a Gray code? \n Answer: \n\nA Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit."}, {"text": "Title: Example: A Three-Bit Gray Code Counter \n Text: {Example: A Three-Bit Gray Code Counter}\n\n\nNow we'll add a third bit to our counter, but again use a Gray code\nas the basis for the state sequence.\n\nA fully-specified transition diagram for such a counter appears to \nthe right.  As before, with no inputs, the states simply form a loop, \nwith the counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_2S_1S_0 \n(before ``/'') and the output Z_2Z_1Z_0 (after ``/''). \n\n\n\n\n\n\nBased on the transition diagram, we can fill in the K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then \nderive algebraic expressions.  The results are more complex this \ntime.\n\n\n\n\n\n\n\n\n\nFor our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_0 + S_1 {{S_0}} \nS_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}} \nS_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1\n{eqnarray*}\n\n\nNotice that the equations for S_2^+ and S_1^+ share a common term,\nS_1{{S_0}}.\n\nThis design does not allow much choice in developing good equations for\nthe next-state logic, but some designs may enable you to reduce \nthe design complexity by explicitly identifying and making use of \ncommon algebraic terms and sub-expressions for different outputs.\nIn modern design processes, identifying such opportunities is generally\nperformed by a computer program, but it's important to understand\nhow they arise.  Note that the common term becomes a single AND gate\nin the implementation of our counter, as shown to the right.\n\nLooking at the counter's implementation diagram, notice that the vertical\nlines carrying the current state values and their inverses back to the\nnext state\nlogic inputs have been carefully ordered to simplify\nunderstanding the diagram.  In particular, they are ordered from\nleft to right (on the left side of the figure) as \n{{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.\nWhen designing any logic diagram, be sure to make use of a reasonable\norder so as to make it easy for someone (including yourself!) to read \nand check the correctness of the logic.\n\n\n\n\n\n\n \n Question: \nQ. How does a three-bit gray code counter work? \n Answer: \n\nA.\n\nA three-bit gray code counter works by cycling through a sequence of states, with each state corresponding to a different output code. The output code is determined by the state of the three flip-flops in the counter. The output code is a gray code, which means that only one bit changes between successive states."}, {"text": "Title: Example: A Color Sequencer \n Text: {Example: A Color Sequencer}\n\n\nEarly graphics systems used a three-bit red-green-blue (RGB) \nencoding for colors.  The color mapping for such a system is shown to\nthe right.\n\nImagine that you are charged with creating a counter to drive a light\nthrough a sequence of colors.  The light takes an RGB input as just\ndescribed, and the desired pattern is\n\n{off (black)     yellow     violet     green     blue}\n\nYou immediately recognize that you merely need a counter with five\nstates.  How many flip-flops will we need?  At least three, since\n_2 (5)=3.  Given that we need three flip-flops, \nand that the colors we need to produce as\n\n\n{c|l}\nRGB& color \n000& black\n001& blue\n010& green\n011& cyan\n100& red\n101& violet\n110& yellow\n111& white\n\n\n\noutputs are all unique\nbit patterns, we can again choose to use the counter's internal \nstate directly as our output values.\n\n\nA fully-specified transition diagram for our color sequencer\nappears to the right.  The states again form a loop,\nand are marked with the internal state value S_2S_1S_0 \nand the output RGB.\n\n\n\n\n\n\nAs before, we can use the transition diagram to fill in K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+, as shown to the right.\nFor each of the three states not included in our transition diagram,\nwe have inserted x's\n\n\n\n\n\n\n\n\n\ninto the K-maps to indicate ``don't care.'' \nAs you know, we can treat each x as either a 0 or a 1, whichever\nproduces better results (where ``better'' usually means simpler \nequations).  The terms that we have chosen for our algebraic \nequations are illustrated in the K-maps.  The x's within the ellipses\nbecome 1s in the implementation, and the x's outside of the ellipses\nbecome 0s.\n\n\nFor our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}} \nS_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}} \nS_0^+ &=& S_1\n{eqnarray*}\n\nAgain our equations for S_2^+ and S_1^+ share a common term,\nwhich becomes a single AND gate in the implementation shown to the\nright.\n\n\n\n\n\n \n Question: \nQ. Is it possible to create a counter with fewer than 3 flip-flops? \n Answer: \n\nA. No, it is not possible."}, {"text": "Title: Identifying an Initial State \n Text: {Identifying an Initial State}\n\nLet's say that you go the lab and build the implementation above, \nhook it up\nto the light, and turn it on.  Does it work?  Sometimes.\nSometimes it works perfectly, but sometimes\nthe light glows cyan or red briefly first.\nAt other times, the light is an\nunchanging white.\n\n\nWhat could be going wrong?\n\nLet's try to understand.  We begin by deriving\nK-maps for the implementation, as shown to the right.  In these\nK-maps, each of the x's in our design has been replaced by either a 0\nor a 1.  These entries are highlighted with green italics.\n\n\n{file=part3/figs/colS2-bad.eps,width=1.00in}\n\n{file=part3/figs/colS1-bad.eps,width=1.00in}\n\n{file=part3/figs/colS0-bad.eps,width=1.00in}\n\n\nNow let's imagine what might happen if somehow our FSM got into the\nS_2S_1S_0=111 state.  In such a state, the light would appear white,\nsince RGB=S_2S_1S_0=111.\n\nWhat happens in the next cycle?\n\nPlugging into the equations or looking into the K-maps gives (of\ncourse) the same answer: the next state is the\nS_2^+S_1^+S_0^+=111 state.\nIn other words, the light stays white indefinitely!\n\nAs an exercise, you should check what happens \nif the light is red or cyan.\n\nWe can extend the transition diagram that we developed for our design\nwith the extra states possible in the implementation, as shown below.\nAs with the five states in the design, the extra states are named with\nthe color of light that they produce.\n\n{{file=part3/figs/colors-full.eps,width=5.8in}}\n\nNotice that the FSM does not move out of the WHITE state (ever).  \n\nYou may at this point wonder whether more careful decisions \nin selecting our next-state expressions might address this issue.\nTo some extent, yes.  For example, if we replace the \nS_2S_1 term in the equation for S_2^+ with S_2{{S_0}}, \na decision allowed\nby the ``don't care'' boxes in the K-map for our design,\nthe resulting transition diagram does not suffer from the problem\nthat we've found.\n\nHowever, even if we do change our implementation slightly, we need\nto address another aspect of the problem:\n\nhow can the FSM ever get into the unexpected states?\n\n\nWhat is the initial state of the three flip-flops in our implementation?\n\n{ The initial state may not even be 0s and 1s unless we have an \nexplicit mechanism for initialization.} \n\nInitialization can work in two ways.  \n\nThe first approach makes use of the flip-flop design.\nAs you know, a flip-flop is built from a pair of latches, and\nwe can \nmake use of the internal reset lines on these latches\nto force each flip-flop into the 0 state (or the 1 state) using an\nadditional input. \n\nAlternatively, we can add some extra logic to our design.\n\nConsider adding a few AND gates and a  input\n(active low), as shown in the dashed box in the figure to the right.\nIn this case, when we assert  by setting it to 0,\nthe FSM moves to state 000 in the next cycle, putting it into\nthe BLACK state.  The approach taken here is for clarity; one can\noptimize the design, if desired.  For example, we could simply connect\n as an extra input into the three AND gates on the\nleft rather than adding new ones, with the same effect.\n\nWe may sometimes want a more powerful initialization mechanism---one\nthat allows us to force the FSM into any specific state in the next\ncycle.  In such a case, we can add multiplexers to each of our \nflip-flop inputs, allowing us to use the INIT input to choose between\nnormal operation (INIT=0) of the FSM and forcing the FSM into the\nnext state given by I_2I_1I_0 (when INIT=1).\n\n\n\n\n\n\n\n\n \n Question: \nQ. What is the purpose of the INIT input in the design? \n Answer: \n\nA. The INIT input allows us to force the FSM into a specific state in the next cycle."}, {"text": "Title: Developing an Abstract Model \n Text: {Developing an Abstract Model}\n\n\nWe are now ready to discuss the design process for an FSM from start\nto finish.\n\nFor this first abstract FSM example, we build upon something\nthat we have already seen: a two-bit Gray code counter.\nWe now want a counter that allows us to start and stop the\n\n\n{c|ccc}\nstate&    no input&  halt button& go button \ncounting& counting&      halted& \nhalted&   halted&              & counting\n\n\n\ncount.\n\nWhat is the mechanism for stopping and starting?  To\nbegin our design, we could sketch out an abstract next-state\ntable such as the one shown to the right above.  In this form of the table,\nthe first column lists the states, while each of the other columns lists\nstates to which the FSM transitions after a clock cycle for a particular\ninput combination. \n\nThe table contains two states, counting and halted, and specifies\nthat the design uses two distinct buttons to move between the\nstates.\nThe table further implies that if the counter is halted,\nthe ``halt'' button has no additional effect, and if the counter\nis counting, the ``go'' button has no additional effect.\n\n\nA counter with a single counting state, of course, does not provide\nmuch value.  We extend the table with four counting states and four\nhalted states, as shown to the right.  This version of the\ntable also introduces more formal state names, for which these notes \nuse all capital letters.\n\nThe upper four states represent uninterrupted counting, in which \nthe counter cycles through these states indefinitely.\n\nA user can stop the counter in any state by pressing the ``halt''\nbutton, causing the counter to retain its current value until the\nuser presses the ``go'' button.\n\nBelow the state table is an abstract transition diagram, which provides\nexactly the same information in graphical form.  Here circles represent\nstates (as labeled) and arcs represent transitions from one state\nto another based on an input combination (which is used to label the\narc).\n\nWe have already implicitly made a few choices about our counter design.\n\nFirst, the counter\n\n\n{c|ccc}\nstate&    no input&  halt button& go button \n{ COUNT A}& { COUNT B}& { HALT A}& \n{ COUNT B}& { COUNT C}& { HALT B}& \n{ COUNT C}& { COUNT D}& { HALT C}& \n{ COUNT D}& { COUNT A}& { HALT D}& \n{ HALT A}&  { HALT A}&              & { COUNT B}\n{ HALT B}&  { HALT B}&              & { COUNT C}\n{ HALT C}&  { HALT C}&              & { COUNT D}\n{ HALT D}&  { HALT D}&              & { COUNT A}\n\n\n\n\nshown retains the current state of the system when\n``halt'' is pressed.\nWe could instead reset the counter state whenever it\nis restarted, in which case we need only five states: four for\ncounting and one more for a halted counter.\n\nSecond, we've designed the counter to stop\nwhen the user presses ``halt'' and to resume counting \nwhen the user presses ``go.''  We could instead choose to delay these \neffects by a cycle.  For example, pressing ``halt'' in state { COUNT B}\ncould take the counter to state { HALT C}, and pressing ``go'' \nin state { HALT C} could take the system to state { COUNT C}.\n\nIn these notes, we implement only the diagrams shown.\n\n \n Question: \nQ. What is the mechanism for stopping and starting? \n Answer: \n\nA. The mechanism for stopping and starting is that a user can stop the counter in any state by pressing the \"halt\" button, causing the counter to retain its current value until the user presses the \"go\" button."}, {"text": "Title: Specifying I/O Behavior \n Text: {Specifying I/O Behavior}\n\n\nWe next start to formalize our design by specifying its input and \noutput behavior digitally.  Each of the two control buttons provides\na single bit of input.  The ``halt'' button we call H, and the\n``go'' button we call G.\n\nFor the output, we use a two-bit \nGray code.  With these choices, we can redraw the transition diagram \nas show to the right.\n\nIn this figure, the states are marked with output values Z_1Z_0 and\ntransition arcs are labeled in terms of our two input buttons, G and H.  \nThe uninterrupted counting cycle is labeled with \nto indicate that it continues until we press H.\n\n\n\n \n\n \n Question: \nQ. What is the purpose of the two-bit Gray code? \n Answer: \n\nA. The purpose of the two-bit Gray code is to allow for easy determination of the output value when the input value changes."}, {"text": "Title: Completing the Specification \n Text: {Completing the Specification}\n\nNow we need to think about how the system should behave if something \noutside of our initial expectations occurs.  Having drawn out a partial\ntransition diagram can help with this process, since we can use the\ndiagram to systematically consider all possible input conditions from\nall possible states.  The state table form can make the missing\nparts of the specification even more obvious.\n\n\n\n\nFor our counter, the symmetry between counting states makes the problem \nsubstantially simpler.  Let's write out part of a list of states and\npart of a state table with one \ncounting state and one halt state, as shown to the right.\nFour values of the inputs HG \nare possible (recall that N bits allow 2^N possible patterns).\nWe list the columns in Gray code order, since we may want to\ntranscribe this table into K-maps later.\n\n\n{\n\n& \nfirst counting state& { COUNT A}& counting, output Z_1Z_0=00\n  first halted state&  { HALT A}& halted, output Z_1Z_0=00\n\n\n{c|cccc}\n&{HG}\n        state&            00&            01&          11&           10 \n{ COUNT A}& { COUNT B}&   unspecified& unspecified& { HALT A}\n { HALT A}&  { HALT A}& { COUNT B}& unspecified&  unspecified\n\n\n\nLet's start with the { COUNT A} state.  \n\nWe know that if neither button is pressed (HG=00), we want \nthe counter to move to the { COUNT B} state.  And, if we press the\n``halt'' button (HG=10), we want the counter to move to the { HALT A}\nstate.  What should happen if a user presses the ``go'' button (HG=01)?\nOr if the user presses both buttons (HG=11)?\n\nAnswering these questions is part of fully specifying our design.  We\ncan choose to leave some parts unspecified, but { any implementation of\nour system will imply answers}, and thus we must be careful.\n\nWe choose to ignore the ``go'' button while counting, and to have the\n``halt'' button override the ``go'' button.  Thus, if HG=01 when the\ncounter is in state { COUNT A}, the counter moves to state { COUNT B}.\nAnd, if HG=11, the counter moves to state { HALT A}.\n\nUse of explicit bit patterns for the inputs HG may help you to check \nthat all four possible input values are covered from each state.  If \nyou choose to use a transition diagram instead of a state table,\nyou might even want to add four arcs from each state, each labeled \nwith a specific\nvalue of HG.  When two arcs connect the same two states, we can either \nuse multiple labels or can indicate bits that do not matter using a\n{ don't-care} symbol, x.  For example, the arc from state { COUNT A}\nto state { COUNT B} could be labeled HG=00,01 or HG=0x.  The\narc from state { COUNT A} to state { HALT A} could be labeled\nHG=10,11 or HG=1x.  We can also use logical expressions as labels,\nbut such notation can obscure unspecified transitions.\n\nNow consider the state { HALT A}.  The transitions specified so far\nare that when we press ``go'' (HG=01), the counter moves to \nthe { COUNT B} state, and that the counter remains halted in \nstate { HALT A} if no buttons are pressed (HG=00).\nWhat if the ``halt'' button is pressed (HG=10), or\nboth buttons are pressed (HG=11)?  For consistency, we decide that\n``halt'' overrides ``go,'' but does nothing special if it alone is pressed\nwhile the counter is halted.  Thus, input patterns HG=10 and HG=11 also \ntake state { HALT A} back to itself.\nHere the arc could be labeled HG=00,10,11 or, equivalently,\nHG=00,1x or HG=x0,11.\n\n\nTo complete our design, we apply the same decisions that we made for \nthe { COUNT A} state to all of the other counting states, and the \ndecisions that we made for the { HALT A} state to all of the other \nhalted states.  If we had chosen not to specify an answer, an implementation\ncould produce different behavior from the different counting\nand/or halted states, which might confuse a user.\n\nThe resulting design appears to the right.\n\n\n\n \n\n\n \n Question: \nQ. Why is it important to specify all possible inputs for each state? \n Answer: \n\nAns. It is important to specify all possible inputs for each state because any implementation of the system will imply answers to these inputs, and thus we must be careful."}, {"text": "Title: Choosing a State Representation \n Text: {Choosing a State Representation}\n\nNow we need to select a representation for the states.  Since our counter\nhas eight states, we need at least three (_2 (8)=3)\nstate bits S_2S_1S_0 to keep track of the current state.\n\nAs we show later, { the choice of representation for an FSM's states\ncan dramatically affect the design complexity}.  For a design as simple as \nour counter, you could just let a computer implement all possible \nrepresentations (there aren't more than 840, if we consider simple \nsymmetries) and select one according to whatever metrics are interesting.\n\nFor bigger designs, however, the number of possibilities quickly becomes\nimpossible to explore completely.\n\nFortunately, { use of abstraction in selecting a representation \nalso tends to produce better designs} for a wide variety of metrics\n(such as design complexity, area, power consumption, and performance).\n\nThe right strategy is thus often to start by selecting a representation \nthat makes sense to a human, even if it requires more bits than are\nstrictly necessary.  The\nresulting implementation will be easier to\ndesign and to debug than an implementation in which only the global \nbehavior has any meaning.\n\n\nLet's return to our specific example, the counter.  We can use one bit, \nS_2, to record whether or not our counter is counting (S_2=0) or\nhalted (S_2=1).  The other two bits can then record the counter state\nin terms of the desired output.  Choosing this representation\nimplies that only wires will be necessary to compute outputs Z_1 \nand Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting\ndesign, in which states are now labeled with both internal state and\noutputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version,\nwe have changed the arc labeling to use logical expressions, which\ncan sometimes help us to think about the implementation.\n\n\n\n\n\nThe equivalent state listing and state table appear below.  We have ordered\nthe rows of the state table in Gray code order to simplify transcription\nof K-maps.\n\n\n\n& S_2S_1S_0& \n{ COUNT A}& 000& counting, output Z_1Z_0=00\n{ COUNT B}& 001& counting, output Z_1Z_0=01\n{ COUNT C}& 011& counting, output Z_1Z_0=11\n{ COUNT D}& 010& counting, output Z_1Z_0=10\n { HALT A}& 100& halted, output Z_1Z_0=00\n { HALT B}& 101& halted, output Z_1Z_0=01\n { HALT C}& 111& halted, output Z_1Z_0=11\n { HALT D}& 110& halted, output Z_1Z_0=10\n\n\n{rc|cccc}\n&&{HG}\n&S_2S_1S_0& 00& 01& 11& 10 \n{ COUNT A}&000& 001& 001& 100& 100\n{ COUNT B}&001& 011& 011& 101& 101\n{ COUNT C}&011& 010& 010& 111& 111\n{ COUNT D}&010& 000& 000& 110& 110\n { HALT D}&110& 110& 000& 110& 110\n { HALT C}&111& 111& 010& 111& 111\n { HALT B}&101& 101& 011& 101& 101\n { HALT A}&100& 100& 001& 100& 100\n\n\n\nHaving chosen a representation, we can go ahead and implement our\ndesign in the usual way.  As shown to the right, K-maps for the \nnext-state logic are complicated, since we have five variables\nand must consider implicants that are not contiguous in the K-maps.\nThe S_2^+ logic is easy enough: we only need two terms, \nas shown.\n\nNotice that we have used color and\nline style to distinguish different\n\n\n\n\n\n\n\n\n\nimplicants in the K-maps.  Furthermore, the symmetry of the design\nproduces symmetry in the S_1^+ and S_0^+ formula, so we have\nused the same color and line style for analogous terms in these\ntwo K-maps.\n\nFor S_1^+, we need four terms.  The green \nellipses in the HG=01 column are part of the same term, as are\nthe two halves of the dashed blue circle.  In S_0^+, we still\nneed four terms, but three of them are split into two pieces \nin the K-map.  As you can see, the utility of the K-map is starting\nto break down with five variables.\n\n\n \n Question: \nQ. Why is it often a good idea to start by selecting a state representation that makes sense to a human, even if it requires more bits than are strictly necessary? \n Answer: \n\nA. The right strategy is thus often to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning."}, {"text": "Title: Abstracting Design Symmetries \n Text: {Abstracting Design Symmetries}\n\nRather than implementing the design as two-level logic, let's try to\ntake advantage of our design's symmetry to further simplify the\nlogic (we reduce gate count at the expense of longer, slower paths).\n\nLooking back to the last transition diagram, in which the arcs\nwere labeled with logical expressions, let's calculate an expression\nfor when the counter should retain its current value in the next\ncycle.  We call \nthis variable HOLD.  In the counting states, when S_2=0, \nthe counter stops (moves into a halted state without changing value) \nwhen H is true.\nIn the halted states, when S_2=1, the counter stops (stays in \na halted state) when H+ is true.  We can thus write\n\n{eqnarray*}\nHOLD &=& {S_2}  H + S_2  ( H +  )\nHOLD &=& {S_2} H + S_2 H + S_2 \nHOLD &=& H + S_2 \n{eqnarray*}\n\nIn other words, the counter should hold its current \nvalue (stop counting) if we press the ``halt'' button or if the counter\nwas already halted and we didn't press the ``go'' button.  As desired,\nthe current value of the counter (S_1S_0) has no impact on this \ndecision.  You may have noticed that the expression we derived for\nHOLD also matches S_2^+, the next-state value of S_2 in the \nK-map on the previous page.\n\nNow let's re-write our state transition table in terms of HOLD.  The\nleft version uses state names for clarity; the right uses state values\nto help us transcribe K-maps.\n\n{\n{rc|cc}\n&&{HOLD}\n&S_2S_1S_0& 0& 1 \n{ COUNT A}&000& { COUNT B}& { HALT A}\n{ COUNT B}&001& { COUNT C}& { HALT B}\n{ COUNT C}&011& { COUNT D}& { HALT C}\n{ COUNT D}&010& { COUNT A}& { HALT D}\n { HALT A}&100& { COUNT B}& { HALT A}\n { HALT B}&101& { COUNT C}& { HALT B}\n { HALT C}&111& { COUNT D}& { HALT C}\n { HALT D}&110& { COUNT A}& { HALT D}\n\n{rc|cc}\n&&{HOLD}\n&S_2S_1S_0& 0& 1 \n{ COUNT A}&000& 001& 100\n{ COUNT B}&001& 011& 101\n{ COUNT C}&011& 010& 111\n{ COUNT D}&010& 000& 110\n { HALT A}&100& 001& 100\n { HALT B}&101& 011& 101\n { HALT C}&111& 010& 111\n { HALT D}&110& 000& 110\n\n\n\nThe K-maps based on the HOLD abstraction are shown to the right.\nAs you can see, the necessary logic has been simplified substantially,\nrequiring only two terms each for both S_1^+ and S_0^+.  Writing\nthe next-state logic algebraically, we obtain\n\n{eqnarray*}\nS_2^+ &=& HOLD\nS_1^+ &=&   S_0 + HOLD  S_1\nS_0^+ &=&   {{S_1}} + HOLD  S_0\n{eqnarray*}\n\n\n\n\n\n\n\n\n\nNotice the similarity between the equations for S_1^+S_0^+ and the \nequations for a {2-to-1} mux: when HOLD=1, the counter retains \nits state, and when HOLD=0, it counts.\n\n\n\n\n\nAn implementation appears below.\n\nBy using semantic meaning in our choice of representation---in\nparticular the use of S_2 to record whether\nthe counter is currently halted (S_2=1) or counting (S_2=0)---we\nhave enabled ourselves to \nseparate out the logic for deciding whether to advance the counter\nfairly cleanly from the logic for advancing the counter itself.\nOnly the HOLD bit in the diagram is used to determine\nwhether or not the counter should advance in the current cycle.\n\nLet's check that the implementation matches our original design.\n\nStart by verifying that the HOLD variable is calculated correctly,\nHOLD=H+S_2,\nthen look back at the K-map for S_2^+ in the low-level design to\nverify that the expression we used does indeed match.\n\n\n\nNext, check the mux abstraction.\n\nWhen HOLD=1, the next-state logic for S_1^+ and S_0^+ \nreduces to S_1^+=S_1 and S_0^+=S_0;\nin other words, the counter stops counting and simply stays in its \ncurrent state.  When HOLD=0, these equations become\nS_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating\nsequence for S_1S_0 of 00, 01, 11, 10, as desired.\nYou may want to look back at our two-bit Gray code counter design\nto compare the next-state equations.\n\nWe can now verify that the implementation produces the correct transition\nbehavior.  In the counting states, S_2=0, and the HOLD value simplifies\nto HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and\nand the counter continues to count in the correct sequence.\nWhen H=1, HOLD=1, and the counter stops at its current value\n(S_2^+S_1^+S_0^+=1S_1S_0, \nwhich is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).\n\nIn any of the halted states, S_2=1, and we can reduce HOLD to\nHOLD=H+.  Here, so long as we press the ``halt'' button\nor do not press the ``go'' button, the counter stays in its current\nstate, because HOLD=1.  If we release ``halt'' and press ``go,''\nwe have HOLD=0, and the counter resumes counting\n(S_2^+S_1^+S_0^+=0S_0{{S_1}},\nwhich is shorthand for S_2^+=0, S_1^+=S_0, and \nS_0^+={{S_1}}).\n\nWe have now verified the implementation.\n\nWhat if you wanted to build a three-bit Gray code counter with the same\ncontrols for starting and stopping?  You could go back to basics and struggle \nwith six-variable {K-maps}.  Or you could simply copy the HOLD \nmechanism from the two-bit design above, insert muxes between the next \nstate logic and the flip-flops of the three-bit Gray code counter that \nwe designed earlier, and control the muxes with the HOLD bit.  \nAbstraction is a powerful tool.\n\n\n\n \n Question: \nQ. What is the value of HOLD when S_2=0 and H=1? \n Answer: \n\nA. The value of HOLD is 1."}, {"text": "Title: Impact of the State Representation \n Text: {Impact of the State Representation}\n\nWhat happens if we choose a bad representation?  For the same FSM---the\ntwo-bit Gray code counter with start and stop inputs---the \ntable below shows a poorly chosen mapping from states to internal \nstate representation.\n\nBelow the table is a diagram of an implementation using that\nrepresentation.\n\nVerifying that the implementation's behavior\nis correct is left as an exercise for the determined reader.\n\n{\n{|c|c|c|c|c|}{1-2}{4-5}\nstate& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5}\n{ COUNT A}& 000& & { HALT A}& 111 \n{ COUNT B}& 101& & { HALT B}& 110 \n{ COUNT C}& 011& & { HALT C}& 100 \n{ COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}\n\n}\n\n\n\n\n\n\n\n\n \n Question: \nQ. How does changing the state representation impact the design of the FSM? \n Answer: \n\nA. Changing the state representation can impact the design of the FSM in several ways. For example, if the states are represented by numbers instead of by letters, the transition table and state diagram will be different. In addition, the implementation of the FSM may be more complex if the states are represented by numbers instead of by letters."}, {"text": "Title: Random Access Memories \n Text: {Random Access Memories}\n\nThis set of notes describes random access memories (RAMs), providing slightly\nmore detail than is available in the textbook.  We begin with a discussion\nof the memory abstraction and the types of memory most commonly used in\ndigital systems, then examine how one can build memories (static RAMs) \nusing logic.  We next introduce tri-state buffers as a way of simplifying\nouput connections, and illustrate how memory chips can be combined to\nprovide larger and wider memories.  A more detailed description of dynamic \nRAMs finishes this set.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}\n\n \n Question: \nQ. What is the main difference between static RAMs and dynamic RAMs? \n Answer: \n\nA. Static RAMs store data in a flip-flop, while dynamic RAMs store data in a capacitor."}, {"text": "Title: Memory \n Text: \n\nA computer { memory} is a group of storage elements and the logic\nnecessary to move data in and out of the elements.  The size of the\nelements in a memory---called the { addressability} of the \nmemory---varies from a single binary digit, or { bit},\nto a { byte} (8 bits) or more.  Typically, we refer to data\nelements larger than a byte as { words}, but the size of a word\ndepends on context. \n\nEach element in a memory is assigned a unique name, called an {\naddress}, that allows an external circuit to identify the particular\nelement of interest.  These addresses are not unlike the street\naddresses that you use when you send a letter.  Unlike street\naddresses, however, memory addresses usually have little or no\nredundancy; each possible combination of bits in an address identifies\na distinct set of bits in the memory.  The figure on the right below \nillustrates the concept.  Each house represents a storage element and \nis associated with a unique address.\n\n{{file=part3/figs/lec18-1.eps,width=4in}}\n\nThe memories that we consider in this class have several properties in\ncommon.  These memories support two operations: { write} places a\nword of data into an element, and { read} retrieves a copy of a\nword of data from an element.  The memories are also { volatile},\nwhich means that the data held by a memory are erased when electrical\npower is turned off or fails.  { Non-volatile} forms of memory\ninclude magnetic and optical storage media such as DVDs, CD-ROMs, disks, \nand tapes, capacitive storage media such as Flash drives,\nand some programmable logic devices.\nFinally, the memories considered in this class are { random access\nmemories (RAMs)}, which means that the time required to access an\nelement in the memory is independent of the element being accessed.\nIn contrast, { serial memories} such as magnetic tape require much\nless time to access data near the current location in the tape than\ndata far away from the current location.\n\nThe figure on the left above shows a generic RAM structure.  The\nmemory contains 2^k elements of N bits each.  A {k-bit}\naddress input, ADDR, identifies the memory element of interest for\nany particular operation.  The write enable\ninput, WE, selects the operation to be performed: if\nWE is high, the operation is a write; if it is low, the\noperation is a read.  Data to be written into an element are provided\nthrough N inputs at the top, and data read from an element appear on\nN outputs at the bottom.  Finally, a { chip select} input, CS,\nfunctions as an enable control for the memory; when CS is low, the\nmemory neither reads nor writes any location.\n\nRandom access memory further divides into two important types: {\nstatic RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}.\nSRAM employs active logic in the form of a two-inverter loop to\nmaintain stored values.  DRAM uses a charged capacitor to store a bit;\nthe charge drains over time and must be replaced, giving rise to the\nqualifier ``dynamic.''  ``Static'' thus serves only to differentiate\nmemories with active logic elements from those with capacitive\nelements.  Both types are volatile, that is, both lose all data when the\npower supply is removed.  We consider both SRAM and DRAM \nin this course, but the details of DRAM operation are beyond our scope. \n\n\n \n Question: \nQ. What is the main difference between SRAM and DRAM? \n Answer: \n\nSRAM uses active logic elements to store data, while DRAM uses capacitive elements."}, {"text": "Title: Static Random Access Memory \n Text: {Static Random Access Memory}\n\nStatic random access memory is used for high-speed applications such\nas processor caches and some embedded designs.  As SRAM bit\n{density---the} number of bits in a given chip {area---is}\nsignificantly lower than DRAM bit density, most applications with less\ndemanding speed requirements use DRAM.  The main memory in most\ncomputers, for example, is DRAM, whereas the memory on the same chip\nas a processor is SRAM.{Chips combining both DRAM and processor\nlogic are available, and are used by some processor manufacturers (such \nas IBM).  Research is underway to couple such logic types more efficiently\nby building 3D stacks of chips.}  DRAM is also unavailable\nwhen recharging its capacitors, which can be a problem for\napplications with stringent real-time needs.\n\n\nA diagram of an SRAM { cell} (a single bit) appears to\nthe right.  A dual-inverter loop stores the bit, and is connected\nto opposing BIT lines through transistors controlled by a SELECT\nline.  \n\nThe cell works as follows.  When SELECT is high, the\ntransistors connect the inverter loop to the bit lines.  When writing\na cell, the bit lines are held at opposite logic values, forcing the\ninverters to match the values on the lines and storing the value from\nthe BIT input.  When reading a cell, the bit lines are disconnected\nfrom other logic, allowing the inverters to drive the lines with\ntheir current outputs.  \n\n\n{file=part3/figs/lec18-2.eps,width=2.20in}\n\n\nThe value stored previously is thus copied onto\nthe BIT line as an output, and the opposite value is placed on the\n line.  When SELECT is low, the transistors\ndisconnect the inverters from the bit lines, and the cell\nholds its current value until SELECT goes high again.\n\nThe actual operation of an SRAM cell is more complicated than we\nhave described.  For example, when writing a bit, the BIT lines \ncan temporarily connect high voltage to ground (a short).  The \ncircuit must be designed carefully to minimize the power consumed\nduring this process.  When reading a bit, the BIT lines\nare pre-charged halfway between high-voltage and ground, and \nanalog devices called sense amplifiers are used to detect the\nvoltage changes on the BIT lines (driven by the inverter loop)\nas quickly as possible.  These analog design issues are outside of \nthe scope of our class.\n\n\nA number of cells are combined into a { bit slice}, as shown to\nthe right.\n\nThe labels along the bottom of the figure are external inputs to the \nbit slice, and match the labels for the abstract\n\n\n{file=part3/figs/lec18-3.eps,width=5in}\n\n\nmemory discussed earlier.  The \nbit slice in the figure can be thought of as a {16-address},\n{1-bit-addressable} memory (2^4b).\n\nThe cells in a bit slice\nshare bit lines and analog read and write logic, which appears to the\nright in the figure.  Based on the ADDR input, a decoder sets one\ncell's SELECT line high to enable a read or write operation to the\ncell.  \n\nThe chip select input CS drives the enable input of\nthe decoder, so none of the memory cells is active when chip select is\nlow (CS=0), and exactly one of the memory cells is active when\nchip select is high (CS=1).\n\nActual bit slices can contain many more cells than are shown in the \nfigure---more cells means less extra logic per cell, but slower memory,\nsince longer wires have higher capacitance.\n\nA read operation is performed as follows.  We set CS=1 and WE=0,\nand place the address of the cell to be read on the ADDR input.\nThe decoder outputs a 1 on the appropriate cell's SELECT line,\nand the read logic reads the bit from the cell and delivers it\nto its Q output, which is then available on the bit \nslice's {DATA-OUT} output.\n\nFor a write operation, we set CS=1 and WE=1.  We again place the\naddress of the cell to be written on the ADDR input and set the\nvalue of the bit slice's {DATA-IN} input to the value to be written\ninto the memory cell.  When the decoder activates the cell's SELECT line,\nthe write logic writes the new value from its D input into\nthe memory cell.  Later reads from that cell then produce the new value.\n\n{{file=part3/figs/lec18-4.eps,width=6.15in}}\n\nThe outputs of the cell selection decoder can be used to control\nmultiple bit slices, as shown in the figure above of a {2^6b}\nmemory.  Selection between bit slices is\nthen based on other bits from the address (ADDR).  In the figure\nabove, a {2-to-4} decoder is used to deliver write requests to\none of four bit slices, and a {4-to-1} mux is used to choose\nthe appropriate output bit for read requests.\n\nThe {4-to-16} decoder now activates one cell in each of the four \nbit slices.  For a read operation, WE=0, and the {2-to-4} decoder \nis not enabled, so it outputs all 0s.  All four bit slices thus perform\nreads, and the desired result bit is forwarded to {DATA-OUT} by the \n{4-to-1} mux.  The tri-state buffer between the mux \nand {DATA-OUT} is explained in a later section.\n\nFor a write operation, exactly one of the bit\nslices has its WE input set to 1 by the {2-to-4} decoder.\nThat bit slice writes the bit value delivered to all bit slices\nfrom {DATA-IN}.  The other three bit slices perform reads, but their \nresults are simply discarded.\n\nThe approach shown above, in which a cell is selected\nthrough a two-dimensional indexing scheme, is known as { coincident\nselection}.  The qualifier ``coincident'' arises from the notion that\nthe desired cell coincides with the intersection of the active row and\ncolumn outputs from the decoders.\n\nThe benefit of coincident selection is easily calculated in terms of\nthe number of gates required for the decoders.  Decoder complexity is\nroughly equal to the number of outputs, as each output is a minterm\nand requires a unique gate to calculate it.  \n\n Fanout trees for input terms and inverted terms add relatively few gates.  \n\nConsider a 1M8b RAM chip.  The number of addresses is 2^,\nand the total number of memory cells is 8,388,608 (2^).\nOne option is to use eight bit slices and a {20-to-1,048,576}\ndecoder, or about 2^ gates.  Alternatively, we can use 8,192 bit\nslices of 1,024 cells.  For the second implementation, we need \ntwo {10-to-1024} decoders, or about 2^ gates.  As chip \narea is roughly proportional to the number of gates, the savings are \nsubstantial.  Other schemes are possible as well: if we want a more \nsquare chip area, we might choose to use 4,096 bit slices of 2,048 \ncells along with one {11-to-2048} decoder and\none {9-to-512} decoder.  This approach requires roughly 25 more\ndecoder gates than our previous example, but is still far superior to\nthe eight-bit-slice implementation.\n\nMemories are typically unclocked devices.  However, as you have seen,\nthe circuits are highly structured, which enables engineers to cope\nwith the complexity of sequential feedback design.  Devices used to\ncontrol memories are typically clocked, and the interaction between\nthe two can be fairly complex.  \n\n\nTiming diagrams for reads and writes\nto SRAM are shown to the right.  A write operation\nappears on the left.  In the first cycle, the controller raises the\nchip select signal and places the memory address to be written on the\naddress inputs.  Once the memory has had time to set up the \nappropriate\n\n\n{file=part3/figs/lec18-6.eps,width=4in}\n\n\nselect lines\ninternally, the WE input is raised, and data are placed\non the data inputs.  The delay, which is specified by the memory\nmanufacturer, is necessary to avoid writing data to the incorrect\nelement within the memory.  The timing shown in the\nfigure rounds this delay up to a single clock cycle, but the\nactual delay needed depends on the clock speed and the memory's \nspecification.  At some point after new data have been\ndelivered to the memory, the write operation completes within the\nmemory.  The time from the application of the address until the\n(worst-case) completion of the write operation is called the {\nwrite cycle} of the memory, and is also specified by the memory \nmanufacturer.  Once the write cycle has passed, the controlling logic \nlowers WE, waits for the change to settle within the memory,\nthen removes the address and lowers the chip select signal.  The\nreason for the delay between these signal changes is the same: to \navoid mistakenly overwriting another memory location.\n\nA read operation is quite similar.  As shown on the right, the\ncontrolling logic places the address on the input lines and raises the\nchip select signal.  No races need be considered, as read operations\non SRAM do not affect the stored data.  After a delay called the {\nread cycle}, the data can be read from the data outputs.  The address\ncan then be removed and the chip select signal lowered.\n\nFor both reads and writes, the number of cycles required for an\noperation depends on a combination of the clock cycle of the\ncontroller and the cycle time of the memory.  For example, with a\n25 nanosecond write cycle and a 10 nanosecond clock cycle, a write\nrequires three cycles.  In general, the number of cycles required is\ngiven by the formula {memory cycle time}/{clock cycle\ntime}.\n\n\n\n \n Question: \nQ. What is SRAM? \n Answer: \n\nA. SRAM is a type of static random access memory."}, {"text": "Title: Tri-State Buffers and Combining Chips \n Text: {Tri-State Buffers and Combining Chips}\n\nRecall the buffer symbol---a triangle like an inverter, but with no\ninversion bubble---between the mux and the {DATA-OUT} \nsignal of the {2^6b} memory shown earlier.  This \n{ tri-state buffer} serves to disconnect the memory logic \nfrom the output line when the memory is not performing a read. \n\n\nAn implementation diagram for a tri-state buffer appears to the right \nalong with the symbolic\nform and a truth table.  The ``Z'' in the truth table output means \nhigh impedance (and is sometimes written ``hi-Z'').  In other words,\nthere is effectively no electrical connection between the tri-state \nbuffer and the output OUT.\n\nThis logical disconnection is achieved by using the outer\n\n\n{file=part3/figs/tri-state.eps,width=3in}\n\n\n{cc|c}\nEN& IN& OUT \n0& x& Z\n1& 0& 0\n1& 1& 1\n\n\n\n(upper and lower)\npair of transistors in the logic diagram.  When EN=0, both transistors\nturn off, meaning that regardless of the value of IN, OUT is connected\nneither to high voltage nor to ground.\n\nWhen EN=1, both transistors turn on, and the tri-state buffer acts as\na pair of back-to-back inverters, copying the signal from IN to OUT,\nas shown in the truth table.\n\nWhat benefit does this logical disconnection provide?\n\nSo long as only one memory's chip select input is high at any time,\nthe same output line can be shared by more than one memory\nwithout the need for additional multiplexers.\n\nMemory chips were often combined in this way to produce larger memories.\n\n\nThe figure to the right illustrates how larger memories can be constructed\nusing multiple chips.  In the case shown, two {2^-bit} \nmemories are used to implement a {2^{k+1}-bit} memory.\nOne of the address bits---in the case shown, the most significant bit---is\nused to drive a decoder that determines which of the two chips is \nactive (CS=1).  The decoder is enabled with the chip select signal for\nthe larger memory, so neither chip is enabled when the external CS is\nlow, as desired.  The\n\n\n{file=part3/figs/mem-larger.eps,width=4.75in}\n\n\nrest of the address bits, as well as the external\ndata inputs and write enable signal, are simply delivered to both memories.\nThe external data outputs are also connected to both memories.  \nEnsuring that at most one chip select signal is high at any time\nguarantees that at most one of the two memory chips drives logic values\non the data outputs.\n\n\nMultiple chips can also be used to construct wider memories, as shown to\nthe right.  In the case shown, two {2^-bit} \nmemories are used to implement a {2^-bit} memory.\nBoth chips are either active or inactive at the same time, so the external \naddress, write enable, and chip select inputs are routed to both chips.\nIn contrast, the data inputs and outputs are separate: the left chip\nhandles the high N bits of input on writes and produces the high N\nbits of output on reads, while the right chip handles the low N bits of \ninput and produces the low N bits of output.\n\n\n{file=part3/figs/mem-wider.eps,width=4.05in}\n\n\nHistorically, tri-state buffers were also used to reduce the number of\npins needed on chips.  Pins have long been a scarce resource, and the \namount of data that can cross a chip's pins in a second (the product of the\nnumber of pins and the data rate per pin) has not grown nearly as rapidly \nas the number of transistors packed into a fixed area.\n\nBy combining inputs and outputs, chip designers were able to halve the\nnumber of pins needed.  For example, data inputs and outputs of memory\nwere often combined into a single set of data wires, with bidirectional\nsignals.  When performing a read from a memory chip, the memory chip\ndrove the data pins with the bits being read (tri-state buffers on the\nmemory chip were enabled).  When performing a write, other logic such as \na processor wrote the value to be stored onto the data pins (tri-state \nbuffers were not enabled).\n\n\n \n Question: \nQ. How does a tri-state buffer work? \n Answer: \n\nA. A tri-state buffer is a device that can be used to disconnect a circuit from an output line. The buffer has three states: high, low, and high impedance. When the buffer is in the high impedance state, the circuit is effectively disconnected from the output line."}, {"text": "Title: Dynamic Random Access Memory* \n Text: {Dynamic Random Access Memory*}\n\nDynamic random access memory, or DRAM, is used for main memory in\ncomputers and for other applications in which size is more important\nthan speed.  While slower than SRAM, DRAM is denser (has\nmore bits per chip area).  A substantial part of DRAM density is\ndue to transistor count: typical SRAM cells use six transistors\n(two for each inverter, and two more to connect the inverters to the \nbit lines), while DRAM cells use only a single transistor.\nHowever, memory designers have also made significant advances in\nfurther miniaturizing DRAM cells to improve density beyond the \nbenefit available from simple transistor count.\n\n\nA diagram of a DRAM cell appears to the right.  \nDRAM storage is capacitive: a bit is stored by charging or not charging \na capacitor.  The capacitor is attached to a BIT line \nthrough a transistor controlled by a SELECT line.  \n\nWhen SELECT is low, the capacitor is isolated and \nholds its charge.  However, the transistor's resistance is\nfinite, and some charge leaks out onto the bit line.  Charge also\nleaks into the substrate on which the transistor is constructed.  After\nsome amount of time, all of the charge dissipates, and the bit is\nlost.  To avoid such loss, the cell must be { refreshed}\nperiodically by reading the contents and writing them back with active\nlogic.\n\n\n{file=part3/figs/lec18-8.eps,width=1.1in}\n\n\nWhen the SELECT line is high during a write operation, logic driving\nthe bit line forces charge onto the capacitor or removes all charge\nfrom it.  For a read operation, the bit line is first brought to an\nintermediate voltage level (a voltage level between 0 and 1), then\nSELECT is raised, allowing the capacitor to either pull a small\namount of charge from the bit line or to push a small amount of charge\nonto the bit line.  The resulting change in voltage is then detected\nby a { sense amplifier} at the end of the bit line.  A sense amp \nis analogous to a marble on a mountaintop: a small push causes the\nmarble to roll rapidly downhill in the direction of the push.\nSimilarly, a small change in voltage causes a sense amp's output to\nmove rapidly to a logical 0 or 1, depending on the direction of the\nsmall change.  As mentioned earlier, sense amplifiers also appear in \nSRAM implementations.\nWhile not technically necessary, as they are with DRAM, the use of a\nsense amp to react to small changes in voltage makes reads faster.\n\nEach read operation on a DRAM cell brings the voltage on its capacitor\ncloser to the intermediate voltage level, in effect destroying the\ndata in the cell.  DRAM is thus said to have { destructive reads}.\nTo preserve data during a read, the bits must be written back\ninto the cells after a read.  For example, the output of the sense \namplifiers can\nbe used to drive the bit lines, rewriting the cells with the\nappropriate data.\n\nAt the chip level, typical DRAM inputs and outputs differ from those\nof SRAM.  \n\nDue to the large size and high density of DRAM,\naddresses are split into row and column components and provided\nthrough a common set of pins.  The DRAM stores the components in\nregisters to support this approach.  Additional inputs, known as the\n{ row} and { column address} {{ strobes}---RAS} and\nCAS, {respectively---are} used to indicate when address\ncomponents are available.  As\nyou might guess from the structure of coincident selection, DRAM\nrefresh occurs on a row-by-row basis (across bit slices---on columns\nrather than rows in the figures earlier in these notes, but the terminology\nof DRAM is a row).  Raising the SELECT line for a\nrow destructively reads the contents of all cells on that row, forcing\nthe cells to be rewritten and effecting a refresh.  The row is thus a\nnatural basis for the refresh cycle.  The DRAM data pins provide\nbidirectional signals for reading and writing elements of the DRAM.\nAn { output enable} input, OE, controls tri-state buffers with\nthe DRAM to determine whether or not the DRAM drives the data pins.\nThe WE input, which controls the type of operation, is\nalso present.\n\n\nTiming diagrams for writes and reads on a historical DRAM implementation\nappear to the right.  In both cases, the row component of the address is \nfirst applied to the address pins, then RAS is raised.  In the\nnext cycle of the controlling logic, the column component is applied\nto the address pins, and CAS is raised.  \n\nFor a write, as shown on the left, the WE signal and the\ndata can\n\n\n{file=part3/figs/lec18-9.eps,width=4in}\n\n\nalso be applied in the second cycle.  The DRAM has internal\ntiming and control logic that prevent races from overwriting an\nincorrect element (remember that the row and column addresses have to\nbe stored in registers).  The DRAM again specifies a write cycle,\nafter which the operation is guaranteed to be complete.  In order, the\nWE, CAS, and RAS signals are then lowered.  \n\nFor a read operation, the output enable signal, OE, is raised after\nCAS is raised.  The DATA pins, which should be floating (in other\nwords, not driven by any logic), are then driven by the DRAM.  After the \nread cycle, valid data appear on the DATA pins, and OE, CAS, and\nRAS are lowered in order after the data are read.\n\nModern DRAM chips are substantially more sophisticated than those\ndiscussed here, and many of the functions that used to be provided\nby external logic are now integrated onto the chips themselves.\n\nAs an example of modern DRAMs, one can obtain\nthe data sheet for Micron Semiconductor's 8Gb ({2^b},\nfor example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.\n\nThe ability to synchronize to an external clock has become prevalent in \nthe industry, leading to the somewhat confusing term SDRAM, which stands \nfor { synchronous DRAM}.  The memory structures themselves\nare still unclocked, but logic is provided on the chip to synchronize \naccesses to the external clock without the need for additional \nlogic.\n\nThe clock provided to the Micron chip just mentioned\ncan be as fast as 1.6 GHz, and data can be\ntransferred on both the rising and falling edges of the clock\n(hence the name DDR, or { double data rate}).  \n\nIn addition to row and\ncolumn components of the address, these chips further separate cells into\n{ banks} and groups of banks.  These allow a user to exploit parallelism\nby starting reads or writes to separate banks at the same time, thus\nimproving the speed at which data can move in and out of the memory.\n\nFor the {2^b} version of the Micron chip,\nthe cells are structured into 4 groups of 4 banks (16 banks total),\neach with 131,072 rows and 1,024 columns.  \n\nDRAM implementations provide interfaces for specifying\nrefresh operations in addition to reads and writes.\nManaging refresh timing and execution is\ngenerally left to an external DRAM controller.\n\nFor the Micron chip, refresh commands must be issued every \n7.8 microseconds at normal temperatures.  Each\ncommand refreshes about 2^ cells, so 8,192 commands refresh\nthe whole chip in less than 64 milliseconds.\n\nAlternatively, the chip can handle refresh on-chip in\norder to maintain memory contents when the rest of the system is \npowered down.\n\n\n\n \n Question: \nQ. What is an advantage of using DRAM over SRAM? \n Answer: \n\nA. DRAM is denser (has more bits per chip area) than SRAM."}, {"text": "Title: Design of the Finite State Machine for the Lab \n Text: {Design of the Finite State Machine for the Lab}\n\nThis set of notes explains the process that Prof. Doug Jones used to develop\nthe FSM for the lab.\n\nThe lab simulates a vending machine mechanism for automatically \nidentifying coins (dimes and quarters only), tracking the amount \nof money entered by the user, accepting or rejecting \ncoins, and emitting a signal when a total of 35 cents has been \naccepted.  In the lab, we will only drive a light with \nthe ``paid in full'' signal.  \n\nSorry, neither candy nor Dew will be distributed!\n\nProf. Doug Jones designed the vending machine application and the FSM,\nwhile Prof. Chris Schmitz prototyped and constructed the physical elements \nwith some help from the ECE shop.\n\nProf. Volodymyr Kindratenko together with Prof. Geoffrey Herman created \nboth the wiki documentation and the Altera Quartus portions of the lab\n(the latter were based on earlier Mentor Graphics work by Prof. Herman).\n\nProf. Kindratenko also helped to scale the design \nin a way that made it possible to deliver to the over 400 students entering\nECE every semester.  \n\nProf. Juan Jos'e Jaramillo later identified\ncommon failure modes, including variability caused by sunshine through \nthe windows in ECEB,{No wonder people say that engineers hate \nsunlight!} and made some changes to improve robustness.  He also\ncreated the PowerPoint slides that are typically used to describe the lab in\nlecture.  Casey Smith, head guru of the ECE Instructional Labs,\ndeveloped a new debounce design and made some other hardware \nimprovements to reduce the rate of student headaches.\nFinally, Prof. Kirill Levchenko together with UA Saidivya Ashok\nstruck a blow against COVID-19 by developing an inexpensive and\nportable replacement for the physical ``vending machine'' systems\nused for testing in previous semesters.\n\n \n Question: \nQ. How does the vending machine work? \n Answer: \n\nA. The vending machine uses a FSM to automatically identify coins, track the amount of money entered by the user, accept or reject coins, and emit a signal when a total of 35 cents has been accepted."}, {"text": "Title: Physical Design, Sensors, and Timing \n Text: {Physical Design, Sensors, and Timing}\n\nA user inserts a coin into a slot at one end of the device.  The coin\nthen rolls down a slope towards a gate controlled by a servo.  The gate\ncan be raised or lowered, and determines whether the coin exits from the\nother side or the bottom of the device.\n\nAs the coin rolls, it passes two optical sensors.{The full system\nactually allows four sensors to differentiate four types of coins, but\nour lab uses only two of these sensors.}  One of these sensors is \npositioned high enough above the slope that a dime passes beneath the\nsesnor, allowing the signal T produced by the sensor to tell us whether \nthe coin is a dime or a quarter.  The second sensor is positioned so\nthat all coins pass in front of it.  The sensor positions are chosen \ncarefully to ensure that, in the case of a quarter, the coin is still\nblocking the first sensor when it reaches the second sensor.  \nBlocked sensors give a signal of 1 in this design, so the rising edge \nthe signal from the second sensor can be used as a ``clock'' for our \nFSM.  When the rising edge occurs, the signal T from the first sensor \nindicates whether the coin is a quarter (T=1) or a dime (T=0).\n\n\nA sample timing diagram for the lab appears to the right.  The clock\nsignal generated by the lab is not only not a square wave---in other words,\nthe high and low portions are not equal---but is also unlikely to be periodic.\nInstead, the ``cycle'' is defined by the time between coin insertions.\nThe T signal serves as the single input to our FSM.  In the timing\n\n\n{file=part3/figs/lab-timing.eps,width=2.55in}\n\n\ndiagram, T is shown as rising and falling before the clock edge.\nWe use positive edge-triggered flip-flops to implement our FSM,\nthus the aspect of the relative timing that matters to our design\nis that, when the clock rises, the value of T is stable and indicates \nthe type of coin entered.  The signal T may fall before or after\nthe clock does---the two are equivalent for our FSM's needs.\n\nThe signal A in the timing diagram is an output from the FSM, and\nindicates whether or not the coin should be accepted.  This signal \ncontrols the servo that drives the gate, and thus determines whether\nthe coin is accepted (A=1) as payment or rejected (A=0) and returned\nto the user.  \n\nLooking at the timing diagram, you should note that our FSM makes \na decision based on its current state and the input T and enters a \nnew state at the rising clock edge.  The value of A in the next cycle\nthus determines the position of the gate when the coin eventually\nrolls to the end of the slope.\n\nAs we said earlier, our FSM is thus a Moore machine: the output A\ndoes not depend on the input T, but only on the current internal \nstate bits of the the FSM.\n\nHowever, you should also now realize that making A depend on T\nis not adequate for this lab.  If A were to rise with T and\nfall with the rising clock edge (on entry to the next state), or\neven fall with the falling edge of T, the gate would return to the\nreject position by the time the coin reached the gate, regardless\nof our FSM's decision!\n\n \n Question: \nQ. What is the purpose of the signal A in the timing diagram? \n Answer: \n\nA. The signal A in the timing diagram serves as an output from the FSM, and\nindicates whether or not the coin should be accepted."}, {"text": "Title: An Abstract Model\\vspace4pt \n Text: {An Abstract Model}\n\n\nWe start by writing down states for a user's expected behavior.\nGiven the fairly tight constraints that we have placed on our lab,\nfew combinations are pos-\n\n\n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nSTART& DIME& QUARTER& & no\nDIME& & PAID& yes& no\nQUARTER& PAID& & yes& no\nPAID& & & yes& yes\n\n\n\nsible.  For a total of 35 cents, a user should either insert a dime \nfollowed by a quarter, or a quarter followed by a dime.\n\nWe begin in a START state, which transitions to states DIME or QUARTER\nwhen the user inserts the first coin.  With no previous coin, we need not\nspecify a value for A.  No money has been deposited, so we set \noutput P=0 in the START state.\n\nWe next create DIME and QUARTER states corresponding to the user having\nentered one coin.  The first coin should be accepted, but more money is\nneeded, so both of these states output A=1 and P=0.\nWhen a coin of the opposite type is entered, each state moves to a\nstate called PAID, which we use for the case in which a total of 35 cents has\nbeen received.  For now, we ignore the possibility that the same type\nof coin is deposited more than once.  Finally, the PAID state accepts\nthe second coin (A=1) and indicates that the user has paid the full\nprice of 35 cents (P=1).\n\n\nWe next extend our design to handle user mistakes.  If a user enters\na second dime in the DIME state, our FSM should reject the coin.  We\ncreate a REJECTD state and add it as the next state from\n\n\n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nSTART& DIME& QUARTER& & no\nDIME& REJECTD& PAID& yes& no\nREJECTD& REJECTD& PAID& no& no\nQUARTER& PAID& REJECTQ& yes& no\nREJECTQ& PAID& REJECTQ& no& no\nPAID& & & yes& yes\n\n\n\nDIME when a dime is entered.\nThe REJECTD state rejects the dime (A=0) and\ncontinues to wait for a quarter (P=0).  What should we use as next \nstates from REJECTD?  If the user enters a third dime (or a fourth, \nor a fifth, and so on), we want to reject the new dime as well.  \nIf the user enters a quarter, we want to accept the coin, at which point\nwe have received 35 cents (counting the first dime).  We use\nthis reasoning to complete the description of REJECTD.  We also create\nan analogous state, REJECTQ, to handle a user who inserts more than\none quarter.\n\nWhat should happen after a user has paid 35 cents and bought \none item?  The FSM at that point is in the PAID state, which delivers\nthe item by setting P=1.\n\nGiven that we want the FSM to allow the user to purchase another item, \nhow should we choose the next states from PAID?\n\nThe behavior that we want from PAID is identical to the behavior that\nwe defined from START.  The 35 cents already \ndeposited was used to pay for the item delivered, so the machine is\nno longer holding any of the user's money.\n\nWe can thus simply set the next states from PAID to be DIME when a \ndime is inserted and QUARTER when a quarter is inserted.\n\n\n\n\nAt this point, we make a decision intended primarily to simplify the\nlogic needed to build the lab.  Without a physical item delivery \nmechanism with a specification for how its in-\n\n\n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nPAID& DIME& QUARTER& yes& yes\nDIME& REJECTD& PAID& yes& no\nREJECTD& REJECTD& PAID& no& no\nQUARTER& PAID& REJECTQ& yes& no\nREJECTQ& PAID& REJECTQ& no& no\n\n\n\nput must be driven, \nthe behavior of the output signal P can be fairly flexible.  \nFor example, we could build a delivery mechanism that used the rising\nedge of P to open a chute.  In this case, the output P=0 in the\nstart state is not relevant, and we can merge the state START with\nthe state PAID.  The way that we handle P in the lab, we might\nfind it strange to have a ``paid'' light turn on before inserting any\nmoney, but keeping the design simple enough for a first lab exercise \nis more important.  Our final abstract state table appears above.\n\n \n Question: \nQ. What is the purpose of the state table?\nQ. \nQ. The state table is used to define the behavior of a finite state machine. In this case, it is used to define the expected behavior of a user inserting coins into a vending machine. \n Answer: "}, {"text": "Title: Picking the Representation \n Text: {Picking the Representation}\n\nWe are now ready to choose the state representation for the lab FSM.\n\nWith five states, we need three bits of internal state.\n\nProf. Jones decided to leverage human meaning in assigning the\nbit patterns, as follows:\n\n{\n\nS_2& type of last coin inserted (0 for dime, 1 for quarter)\nS_1& more than one quarter inserted? (1 for yes, 0 for no)\nS_0& more than one dime inserted? (1 for yes, 0 for no)\n\n}\n\n\nThese meanings are not easy to apply to all of our states.  For example,\nin the PAID state, the last coin inserted may have been of either type,\nor of no type at all, since we decided to start our FSM in that state as \nwell.  However, for the other four states, the meanings provide a clear\nand unique set of bit pattern assignments, as shown to the right.  We\ncan choose any of the remaining four bit patterns (010, 011, 101, or 111)\nfor the PAID state.  In fact, { we can choose all of the remaining\npatterns} for the PAID state.  We can always represent any state\n\n\n\nstate& S_2S_1S_0  \nPAID& ???\nDIME& 000\nREJECTD& 001\nQUARTER& 100\nREJECTQ& 110\n\n\n\nwith more\nthan one pattern if we have spare patterns available.  Prof. Jones\nused this freedom to simplify the logic design.\n\nThis particular example is slightly tricky.  The four free patterns do\nnot share any single bit in common, so we cannot simply insert x's\ninto all {K-map} entries for which the next state is PAID.\nFor example, if we insert an x into the {K-map} for  S_2^+,\nand then choose a function for S_2^+ that produces a value of 1\nin place of the don't care, we must also produce a 1 in\nthe corresponding entry of the {K-map} for S_0^+.  Our options\nfor PAID include 101 and 111, but not 100 nor 110.  These latter\ntwo states have other meanings.\n\n\nLet's begin by writing a next-state table consisting mostly of bits,\nas shown to the right.  We use this table to write out a {K-map}\nfor S_2^+ as follows: any of the patterns that may be used for the\nPAID state obey the next-state rules for PAID.  Any next-state marked\nas PAID is marked as don't care in the {K-map},\n\n\n{cc|cc}\n&&{|c}{S_2^+S_1^+S_0^+}\nstate& S_2S_1S_0& T=0& T=1 \nPAID& PAID& 000& 100\nDIME& 000& 001& PAID\nREJECTD& 001& 001& PAID\nQUARTER& 100& PAID& 110\nREJECTQ& 110& PAID& 110\n\n\n\n\n\n\nsince we can\nchoose patterns starting with either or both values to represent our\nPAID state.  The resulting {K-map} appears to the far right.\nAs shown, we simply set S_2^+=T, which matches our\noriginal ``meaning'' for S_2.  That is, S_2 is the type of the\nlast coin inserted.\n\n\nBased on our choice for S_2^+, we can rewrite the {K-map} as\nshown to the right, with green italics and shading marking the\nvalues produced for the x's in the specification.  Each of these\nboxes corresponds to one transition into the PAID state.  By \nspecifying the S_2 value, we cut the number of possible choices\nfrom four to two in each case.  For those combinations in which the\nimplementation produces S_2^+=0, we must choose S_1^+=1, but\nare still free to leave S_0^+ marked as a don't care.  Similarly,\nfor those combinations in which the implementation produces S_2^+=1, \nwe must choose S_0^+=1, but\nare still free to leave S_1^+ marked as a don't care.\n\n\n\n\n\n\nThe {K-maps} for S_1^+ and S_0^+ are shown to the right.\nWe have not given algebraic expressions for either, but have indicated\nour choices by highlighting the resulting replacements of don't care\nentries with the values produced by our expressions.\n\nAt this point, we can review the state patterns actually produced by\neach of the four next-state transitions into the PAID state.  From\nthe DIME state, we move into the 101 state when the user inserts a\n\n\n\n\n\n\n\n\nquarter.  The result is the same from the REJECTD state.  From the\nQUARTER state, however, we move into the 010 state when the user \ninserts a dime.  The result is the same from the REJECTQ state.  We\nmust thus classify both patterns, 101 and 010, as PAID states.  The\nremaining two patterns, 011 and 111, cannot\n\n\nbe reached from any of\nthe states in our design.  We might then try to leverage the fact\nthat the next-state patterns from these two states are not relevant \n(recall that we fixed the next-state patterns for all four of the \npossible PAID states) to further simplify our logic, but doing so \ndoes not provide any advantage (you may want to check our claim).\n\nThe final state table is shown to the right.  We have included the\nextra states at the bottom of the table.  We have specified the\nnext-state logic for these\n\n\n{cc|cc|cc}\n&&{|c|}{S_2^+S_1^+S_0^+}\nstate& S_2S_1S_0& T=0& T=1& A& P \nPAID1& 010& 000& 100& 1& 1\nPAID2& 101& 000& 100& 1& 1\nDIME& 000& 001& 101& 1& 0\nREJECTD& 001& 001& 101& 0& 0\nQUARTER& 100& 010& 110& 1& 0\nREJECTQ& 110& 010& 110& 0& 0 \nEXTRA1& 011& 000& 100& x& x\nEXTRA2& 111& 000& 100& x& x\n\n\n\nstates, but left the output bits as don't\ncares.  A state transition diagram appears at the bottom of this page.\n\n \n Question: \nQ. What does the bit pattern S_2S_1S_0 represent in the state table? \n Answer: \n\nThe bit pattern S_2S_1S_0 represents the internal state of the FSM."}, {"text": "Title: Testing the Design \n Text: {Testing the Design}\n\nHaving a complete design on paper is a good step forward, but humans\nmake mistakes at all stages.  How can we know that a circuit that\nwe build in the lab correctly implements the FSM that we have outlined \nin these notes?\n\nFor the lab design, we have two problems to solve.\n\nFirst, we have not specified an initialization scheme for the FSM.\nWe may want the FSM to start in one of the PAID states, but adding\ninitialization logic to the design may mean requiring you to wire together\nsignificantly more chips.  Second, we need a sequence of inputs that\nmanages to test that all of the next-state and output logic implementations\nare correct.\n\nTesting sequential logic, including FSMs, is in general extremely difficult.\nIn fact, large sequential systems today are generally converted into \ncombinational logic by using shift registers to fill the \nflip-flops with a particular pattern, \nexecuting the logic for one clock cycle, and checking that the resulting \npattern of bits in the flip-flops is correct.  This approach is called \n{ scan-based testing}, and is discussed in ECE 543.  You \nwill make use of a similar approach\nwhen you test your combinational logic in the second week of the lab,\nbefore wiring up the flip-flops.\n\nWe have designed our FSM to be easy to test (even small FSMs\nmay be challenging) with a brute force approach.  In particular, we \nidentify two input sequences that together serve both to initialize and \nto test a correctly implemented variant of our FSM.  Our initialization\nsequence forces the FSM into a specific state regardless of its initial\nstate.  And our test sequence crosses every transition arc leaving the\nsix valid states.\n\n\n\nIn terms of T, the coin type, we initialize the FSM with the\ninput sequence 001.  Notice that such a sequence takes any initial \nstate into PAID2.\n\nFor testing, we use the input sequence 111010010001.  You should trace \nthis sequence, starting from PAID2, on the diagram below to see how the\ntest sequence covers all of the possible arcs.  As we test, we need also\nto observe the A and P outputs in each state to check the output\nlogic.\n\n{{file=part3/figs/lab-diag-notes.eps,width=4.25in}}\n\n\n\n \n Question: \nQ. What does the T input sequence 111010010001 test for?\nQ. \nQ. This test sequence covers all of the possible arcs, and also observes the A and P outputs in each state to check the output logic. \n Answer: "}, {"text": "Title: Finite State Machine Design Examples, Part II \n Text: {Finite State Machine Design Examples, Part II}\n\nThis set of notes provides several additional examples of FSM design.\nWe first design an FSM to control a vending machine, introducing\nencoders and decoders as components that help us to implement our\ndesign.  We then design a game controller for a logic puzzle\nimplemented as a children's game.  Finally, we analyze a digital FSM\ndesigned to control the stoplights at the intersection of two roads.\n\n\n \n Question: \nQ. What is an encoder? \n Answer: \n\nA. An encoder is a component that helps to implement an FSM design. It converts input signals into a code that can be read by a machine."}, {"text": "Title: Design of a Vending Machine \n Text: {Design of a Vending Machine}\n\nFor the next example, we design an FSM to control a simple vending machine.  \nThe machine accepts {U.S. coins}{Most countries have small \nbills or coins in demoninations suitable for vending machine prices, so think \nabout some other currency if you prefer.} as payment and offers a choice\nof three items for sale.\n\nWhat states does such an FSM need?  The FSM needs to keep track of how\nmuch money has been inserted in order to decide whether a user can \npurchase one of the items.  That information alone is enough for the\nsimplest machine, but let's create a machine with adjustable item\nprices.  We can use registers to hold the item prices, which \nwe denote P_1, P_2, and P_3.\n\nTechnically, the item prices are also part of the internal state of the \nFSM.  However,  we leave out discussion (and, indeed, methods) for setting\nthe item prices, so no state with a given combination of prices has any \ntransition to a state with a different set of item prices.\nIn other words, any given combination of item prices induces a subset \nof states that operate independently of the subset induced by a distinct \ncombination of item prices.  By abstracting away the prices in this way,\nwe can focus on a general design that allows the owner of the machine\nto set the prices dynamically.\n\n\nOur machine will not accept pennies, so let's have the FSM keep track of\nhow much money has been inserted as a multiple of 5 cents (one nickel).\nThe table to the right shows five types of coins, their value in \ndollars, and their value in terms of nickels.\n\nThe most expensive item in the machine might cost a dollar or two, so\nthe FSM must track at least 20 or 40 nickels of value.  Let's\n\n\n{l|c|c}\n{c|}{coin type}& value& # of nickels \nnickel&      0.05& 1\ndime&        0.10& 2\nquarter&     0.25& 5\nhalf dollar& 0.50& 10\ndollar&      1.00& 20\n\n\n\ndecide to\nuse six bits to record the number of nickels, which allows the machine\nto keep track of up to 3.15 (63 nickels).  We call the abstract\nstates { STATE00} through { STATE63}, and refer to a state with\nan inserted value of N nickels as { STATE{<}N{>}}.\n\nLet's now create a next-state table, as shown at the top of the next page.\nThe user can insert one of the five coin types, or can pick one of the \nthree items.  What should happen if the user inserts more money than the \nFSM can track?  Let's make the FSM reject such coins.  Similarly, if the \nuser tries to buy an item without inserting enough money first, the FSM \nmust reject the request.  For each of the possible input events, we add a \ncondition to separate the FSM states that allow the input event to \nbe processed as the user desires from those states that do not.  For example,\nif the user inserts a quarter, those states with N<59 transition to\nstates with value N+5 and accept the quarter.  Those states with\nN reject the coin and remain in { STATE{<}N{>}}.\n\n\n{c|l|c|l|c|c}\n&&& {|c}{final state}\n&&& {|c}{}& & release \ninitial state& {|c|}{input event}& condition& {|c}& & product \n{ STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none\n{ STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none\n{ STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none\n{ STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none\n{ STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none\n{ STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1\n{ STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2\n{ STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3\n{ STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none\n\n\n\nWe can now begin to formalize the I/O for our machine.  Inputs include \ninsertion of coins and selection of items for purchase.  Outputs include\na signal to accept or reject an inserted coin as well as signals to release\neach of the three items.\n\n\nFor input to the FSM, we assume that a coin inserted in any given cycle \nis classified and delivered to our FSM using the three-bit representation \nshown to the right.\n\nFor item selection, we assume that the user has access to three buttons,\nB_1, B_2, and B_3, that indicate a desire to purchase the \ncorresponding item.\n\nFor output, the FSM must produce a signal A indicating whether a coin\nshould be accepted.  To control the release of items that have been purchased,\nthe FSM must produce the signals R_1, R_2, and R_3, corresponding\nto the re-\n\n\n{l|c}\n{c|}{coin type}& C_2C_1C_0 \nnone&        110\nnickel&      010\ndime&        000\nquarter&     011\nhalf dollar& 001\ndollar&      111\n\n\n\nlease of each item.  Since outputs in our class depend only on\nstate, we extend the internal state of the FSM to include bits for each of\nthese output signals.  The output signals go high in the cycle after\nthe inputs that generate them.  Thus, for example, the accept signal A\ncorresponds to a coin inserted in the previous cycle, even if a second\ncoin is inserted in the current cycle.  This meaning must be made clear to\nwhomever builds the mechanical system to return coins.\n\nNow we are ready to complete the specification.  How many states does the\nFSM have?  With six bits to record money inserted and four bits to \ndrive output signals, we have a total of 1,024 (2^) states!\nSix different coin inputs are possible, and the selection buttons allow\neight possible combinations, giving 48 transitions from each state.\nFortunately, we can use the meaning of the bits to greatly simplify\nour analysis.\n\nFirst, note that the current state of the coin accept bit and item\nrelease bits---the four bits of FSM state that control the outputs---have\nno effect on the next state of the FSM.  Thus, we can consider only the\ncurrent amount of money in a given state when thinking about the \ntransitions from the state.  As you have seen, we can further abstract\nthe states using the number N, the number of nickels currently held by \nthe vending machine.\n\nWe must still consider all 48 possible transitions from { STATE{<}N{>}}.\nLooking back at our abstract next-state table, notice that we had only\neight types of input events (not counting ``no input'').  If we\nstrictly prioritize these eight possible events, we can safely ignore\ncombinations.  Recall that we adopted a similar strategy for several \nearlier designs, including the ice cream dispenser in Notes Set 2.2 and\nthe keyless entry system developed in Notes Set 3.1.3.\n\nWe choose to prioritize purchases over new coin insertions, and to \nprioritize item 3 over item 2 over item 1.  These prioritizations\nare strict in the sense that if the user presses B_3, both other\nbuttons are ignored, and any coin inserted is rejected, regardless of\nwhether or not the user can actually purchase item 3 (the machine\nmay not contain enough money to cover the item price).\n\nWith the choice of strict prioritization, all transitions from all\nstates become well-defined.  We apply the transition rules in order of\ndecreasing priority,\nwith conditions, and with {don't-cares} for lower-priority inputs. \nFor example, for any of the 16 { STATE50}'s (remember that the four\ncurrent output bits do not affect transitions), the table below lists all\npossible transitions assuming that P_3=60, P_2=10, and P_1=35.\n\n{\n{ccccc|ccccc}\n&&&&& {|c}{next state}\ninitial state& B_3& B_2& B_1& C_2C_1C_0& state& A& R_3& R_2& R_1 \n{ STATE50}& 1&x&x& xxx& { STATE50}& 0& 0&0&0\n{ STATE50}& 0&1&x& xxx& { STATE40}& 0& 0&1&0\n{ STATE50}& 0&0&1& xxx& { STATE15}& 0& 0&0&1\n{ STATE50}& 0&0&0& 010& { STATE51}& 1& 0&0&0\n{ STATE50}& 0&0&0& 000& { STATE52}& 1& 0&0&0\n{ STATE50}& 0&0&0& 011& { STATE55}& 1& 0&0&0\n{ STATE50}& 0&0&0& 001& { STATE60}& 1& 0&0&0\n{ STATE50}& 0&0&0& 111& { STATE50}& 0& 0&0&0\n{ STATE50}& 0&0&0& 110& { STATE50}& 0& 0&0&0\n\n}\nNext, we need to choose a state representation.  But this task is \nessentially done: each output bit (A, R_1, R_2, and R_3) is\nrepresented with one bit in the internal representation, and the\nremaining six bits record the number of nickels held by the vending\nmachine using an unsigned representation.\n\nThe choice of a numeric representation for the money held is important,\nas it allows us to use an adder to compute the money held in\nthe next state.\n\n \n Question: \nQ. -What is the output of the vending machine if a user inserts a quarter and selects item 2? \n Answer: \n\nA. The output of the vending machine would be \"1\" for the release of item 2, and \"0\" for everything else."}, {"text": "Title: Encoders and Decoders \n Text: {Encoders and Decoders}\n\nSince we chose to prioritize purchases, let's begin by building logic\nto perform state transitions for purchases.  Our first task is to\nimplement prioritization among the three selection buttons.  For this\npurpose, we construct a {4-input} { priority encoder}, which \ngenerates a signal P whenever any of its four input lines is active\nand encodes the index of the highest active input as a two-bit unsigned\nnumber S.   A truth table for our priority encoder appears on the \nleft below, with {K-maps} for each of the output bits on the right.\n\n\n{cccc|cc}\nB_3& B_2& B_1& B_0& P& S \n1&x&x&x& 1& 11\n0&1&x&x& 1& 10\n0&0&1&x& 1& 01\n0&0&0&1& 1& 00\n0&0&0&0& 0& xx\n\n\n\n\n\n\n\n\n\n\nFrom the {K-maps}, we extract the following equations:\n{eqnarray*}\nP &=& B_3 + B_2 + B_1 + B_0\nS_1 &=& B_3 + B_2\nS_0 &=& B_3 + {B_2}B_1\n{eqnarray*}\nwhich allow us to implement our encoder as shown to the right.\n\nIf we connect our buttons B_1, B_2, and B_3 to the priority \nencoder (and feed 0 into the fourth input), it produces a signal P \nindicating that the user is trying to make a purchase and a two-bit\nsignal S indicating which item the user wants.\n\n\n\n\n\nWe also need to build logic to control the item release outputs R_1, R_2,\nand R_3.  An item should be released only when it has been selected \n(as indicated by the priority encoder signal S) and the vending machine\nhas enough money.  For now, let's leave aside calculation of the item \nrelease signal, which we call R, and focus on how we can produce the\ncorrect values of R_1, R_2, and R_3 from S and R.\n\nThe component to the right is a { decoder} with an enable input.  A \ndecoder takes an input signal---typically one coded as a binary number---and \nproduces one output for each possible value of the signal.  You may\nnotice the similarity with the structure of a mux: when the decoder\nis enabled (EN=1), each of the AND gates produces\n\n\n\n\n\none minterm of the input signal S.  In\nthe mux, each of the inputs is then included in\none minterm's AND gate, and the outputs of all AND gates are ORd together.\nIn the decoder, the AND gate outputs are the outputs of the decoder.\nThus, when enabled, the decoder produces exactly one 1 bit on its outputs.\nWhen not enabled (EN=0), the decoder produces all 0 bits.\n\nWe use a decoder to generate the release signals for the vending machine\nby connecting the signal S produced by the \npriority encoder to the decoder's S input and connecting the item\nrelease signal R to the decoder's EN input.  The outputs D_1,\nD_2, and D_3 then correspond to the individual item release \nsignals R_1, R_2, and R_3 for our vending machine.\n\n \n Question: \nQ. What is the purpose of a priority encoder? \n Answer: \n\nA. The purpose of a priority encoder is to generate a signal P whenever any of its four input lines is active and encode the index of the highest active input as a two-bit unsigned number S."}, {"text": "Title: Vending Machine Implementation \n Text: {Vending Machine Implementation}\n\n\nWe are now ready to implement the FSM to handle purchases, as shown to the \nright.  The current number of nickels, N, is stored in a register in the\ncenter of the diagram.  Each cycle, N is fed into a {6-bit} adder,\nwhich subtracts the price of any purchase requested in that cycle. \n\nRecall that we chose to record item prices in registers.  We avoid the \nneed to negate prices before adding them by storing the negated prices in\nour registers.  Thus, the value of register PRICE1 is -P_1, the\nthe value of register PRICE2 is -P_2, and the\nthe value of register PRICE3 is -P_3.\n\nThe priority encoder's S signal is then used to select the value of \none of these three registers (using a {24-to-6} mux) as the second\ninput to the adder.\n\nWe use the adder to execute a subtraction, so the carry out C_ \nis 1 whenever the value of N is at least as great as the amount \nbeing subtracted.  In that case, the purchase is successful.  The AND\ngate on the left calculates the signal R indicating a successful purchase,\nwhich is then used to select the next value of N using the {12-to-6}\nmux below the adder.  \n\nWhen no item selection buttons are pushed, P and thus R are both 0, \nand the mux below the adder keeps N unchanged in the next cycle.  \nSimilarly, if P=1 but N is\ninsufficient, C_ and thus R are both 0, and again N does\nnot change.  Only when P=1 and C_=1 is the purchase successful,\nin which case the price is subtracted from N in the next cycle.\n\n\n\n\n\nThe signal R is also used to enable a decoder that generates the three\nindividual item release outputs.  The correct output is generated based on\nthe decoded S signal from the priority encoder, and all three output\nbits are latched into registers to release the purchased item in the next \ncycle.\n\nOne minor note on the design so far: by hardwiring C_ to 0, we created \na problem for items that cost nothing (0 nickels): in that case, C_ is\nalways 0.  We could instead store\n-P_1-1 in PRICE1 (and so forth) and feed P in to C_, but maybe\nit's better not to allow free items.\n\n\nHow can we support coin insertion?  Let's use the same adder to add\neach inserted coin's value to N.  The table at the right shows\nthe value of each coin as a {5-bit} unsigned number of nickels.\nUsing this table, we can fill in {K-maps} for each bit of V, as\nshown below.  Notice that we have marked the two undefined bit patterns\nfor the coin type C as don't cares in the {K-maps}.\n\n\n{l|c|c}\n{c|}{coin type}& C_2C_1C_0& V_4V_3V_2V_1V_0 \nnone&        110& 00000\nnickel&      010& 00001\ndime&        000& 00010\nquarter&     011& 00101\nhalf dollar& 001& 01010\ndollar&      111& 10100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the {K-maps} gives the following equations, which we\nimplement as shown to the right.\n\n{eqnarray*}\nV_4 &=& C_2C_0\nV_3 &=& {C_1}C_0\nV_2 &=& C_1C_0\nV_1 &=& {C_1}\nV_0 &=& {C_2}C_1\n{eqnarray*}\n\n\n\n\n\n\nNow we can extend the design to handle coin insertion, as shown to the right\nwith new elements highlighted in blue.  The output of the coin value \ncalculator is extended with a leading 0 and then fed into a {12-to-6} mux.\nWhen a purchase is requested, P=1 and the mux forwards the item price to \nthe adder---recall that we chose to give purchases priority over coin\ninsertion.  When no purchase is requested, the value of any coin inserted\n(or 0 when no coin is inserted) is passed to the adder.\n\nTwo new gates have been added on the lower left.  First, let's verify that\npurchases work as before.  When a purchase is requested, P=1, so the\nNOR gate outputs 0, and the OR gate simple forwards R to control the\nmux that decides whether the purchase was successful, just as in our\noriginal design.\n\nWhen no purchase is made (P=0, and R=0), the adder adds the value of \nany inserted coin to N.  If the addition overflows, C_=1, and\nthe output of the NOR gate is 0.  Note that the NOR gate output is stored\nas the output A in the next cycle, so a coin that causes overflow in the\namount of money stored is rejected.  The OR gate also outputs 0, and N\nremains unchanged.  If the addition does not overflow, the NOR gate\noutputs a 1, the coin is accepted (A=1 in the next cycle), and \nthe mux allows the sum N+V to be written back as the new value of N.\n\n\n\n\n\nThe tables at the top of the next page define all of the state variables,\ninputs, outputs, and internal signals used in the design, and list the\nnumber of bits for each variable.\n\n\n\n\n{|ccl|}\n{|r|}{{ FSM state}}\nPRICE1& 6& negated price of item 1 (-P_1)\nPRICE2& 6& negated price of item 2 (-P_2)\nPRICE3& 6& negated price of item 3 (-P_3)\nN& 6& value of money in machine (in nickels)\nA& 1& stored value of accept coin output\nR_1& 1& stored value of release item 1 output\nR_2& 1& stored value of release item 2 output\nR_3& 1& stored value of release item 3 output \n{|r|}{{ internal signals}}\nV& 5& inserted coin value in nickels\nP& 1& purchase requested (from priority encoder)\nS& 2& item # requested (from priority encoder)\nR& 1& release item (purchase approved) \n\n\n{|ccl|}\n{|r|}{{ inputs}}\nB_1& 1& item 1 selected for purchase\nB_2& 1& item 2 selected for purchase\nB_3& 1& item 3 selected for purchase\nC& 3& coin inserted (see earlier table \n& & for meaning) \n{|r|}{{ outputs}}\nA& 1& accept inserted coin (last cycle)\nR_1& 1& release item 1\nR_2& 1& release item 2\nR_3& 1& release item 3\n{|l|}{{ Note that outputs correspond one-to-one}}\n{|l|}{{ with four bits of FSM state.}} \n\n\n\n\n \n Question: \nQ. How does the vending machine determine the value of an inserted coin? \n Answer: \n\nA. The vending machine uses the coin type input C and the coin value input V to determine the value of an inserted coin."}, {"text": "Title: Design of a Game Controller \n Text: {Design of a Game Controller}\n\nFor the next example, imagine that you are part of a team building a\ngame for children to play at Engineering Open House.\n\nThe game revolves around an old logic problem in which a farmer must\ncross a river in order to reach the market.  The farmer is traveling\nto the market to sell a fox, a goose, and some corn.  The farmer has\na boat, but the boat is only large enough to carry the fox, the goose,\nor the corn along with the farmer.  The farmer knows that if he leaves\nthe fox alone with the goose, the fox will eat the goose.  Similarly,\nif the farmer leaves the goose alone with the corn, the goose will \neat the corn.\n\nHow can the farmer cross the river?\n\nYour team decides to build a board illustrating the problem with\na river from top to bottom and lights illustrating the positions of \nthe farmer (always with the boat), the fox, the goose, and the\ncorn on either the left bank or the right bank of the river.\nEverything starts on the left bank, and the children can play \nthe game until they win by getting everything to the right bank or\nuntil they make a mistake.\n\nAs the ECE major on your team, you get to design the FSM!\n\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n\nSince the four entities (farmer, fox, goose, and corn) can be only\non one bank or the other, we can use one bit to represent the location\nof each entity.  Rather than giving the states names, let's just\ncall a state FXGC.  The value of F represents the location of the farmer,\neither on the left bank (F=0) or the right bank (F=1).  Using\nthe same representation (0 for the left bank, 1 for the right bank),\nthe value of X represents the location of the fox, G represents the\nlocation of the goose, and C represents the location of the corn.\n\n\nWe can now put together an abstract next-state table, as shown to\nthe right.  Once the player wins or loses, let's have the game indicate\ntheir final status and stop accepting requests to have the farmer cross\nthe river.  We can use a reset button to force the game back into the\noriginal state for the next player.\n\nNote that we have included conditions for some of the input events, as \nwe did previously\n\n\n{c|l|c|c}\ninitial state& {|c|}{input event}& condition& final state \nFXGC& no input& always& FXGC\nFXGC& reset & always& 0000\nFXGC& cross alone& always& XGC\nFXGC& cross with fox& F=X& GC\nFXGC& cross with goose& F=G& XC\nFXGC& cross with corn& F=C& XG\n\n\n\nwith the vending machine design.\n\nThe conditions here require that the farmer be on the same bank as any\nentity that the player wants the farmer to carry across the river.\n\nNext, we specify the I/O interface. \n\nFor input, the game has five buttons.  A reset button R forces the\nFSM back into the initial state.  The other four buttons cause the\nfarmer to cross the river: B_F crosses alone, B_X with the fox,\nB_G with the goose, and B_C with the corn.\n\nFor output, we need position indicators for the four entities, but let's\nassume that we can simply output the current state FXGC and have\nappropriate images or lights appear on the correct banks of the \nriver.  We also need two more indicators: W for reaching the winning\nstate, and L for reaching a losing state.\n\nNow we are ready to complete the specification.  We could use a strict\nprioritization of input events, as we did with earlier examples.  Instead,\nin order to vary the designs a bit, we use a strict prioritization among \nallowed inputs.  The reset button R has the highest priority, followed\nby B_F, B_C, B_G, and finally B_X.  However, only those buttons \nthat result in an allowed move are considered when selecting one button \namong several pressed in a single clock cycle.\n\n\nAs an example, consider the \nstate FXGC=0101.  The farmer is not on the same bank as the fox, nor\nas the corn, so the B_X and B_C buttons are ignored, leading to the\nnext-state table to the right.  Notice that B_G is accepted even if B_C\nis pressed because the farmer is not on the same\n\n\n{cccccc|c}\nFXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+ \n0101& 1& x& x& x& x& 0000\n0101& 0& 1& x& x& x& 1101\n0101& 0& 0& x& 1& x& 1111\n0101& 0& 0& x& 0& x& 0101\n\n\n\nbank as the corn.\nAs shown later, this approach to prioritization\nof inputs is equally simple in terms of implementation.  \n\n\nRecall that we want to stop the game when the player wins or loses.\nIn these states, only the reset button is accepted.  For example, the \nstate FXGC=0110 is a losing state because\n\n\n{cccccc|c}\nFXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+ \n0110& 1& x& x& x& x& 0000\n0110& 0& x& x& x& x& 0110\n\n\n\nthe farmer\nhas left the fox with the goose on the opposite side of the river.\nIn this case, the player can reset the game, but other buttons\nare ignored.\n\nAs we have already chosen a representation, we now move on to implement\nthe FSM.  Let's begin by calculating the next state ignoring the\nreset button and the winning and losing states, as shown in the logic \ndiagram below.\n\n\n\nThe left column of XOR gates determines whether the farmer is on the same\nbank as the corn (top gate), goose (middle gate), and fox (bottom gate).\nThe output of each gate is then used to mask out the corresponding button:\nonly when the farmer is on the same bank are these buttons considered.\nThe adjusted button values are then fed into a priority encoder, which\nselects the highest priority input event according to the scheme that\nwe outlined earlier (from highest to lowest, B_F, B_C, B_G, and\nB_X, ignoring the reset button).  \n\nThe output of the priority encoder is then used to drive another column\nof XOR gates on the right in order to calculate the next state.\nIf any of the allowed buttons is pressed, the priority encoder \noutputs P=1, and the farmer's bank is changed.  If B_C is allowed\nand selected by the priority encoder (only when B_F is not pressed),\nboth the farmer and the corn's banks are flipped.  The goose and the fox\nare handled in the same way.\n\n\nNext, let's build a component to produce the win and lose signals.\nThe one winning state is FXGC=1111, so we simply need an AND gate.\nFor the lose signal L, we can fill in a {K-map} and derive an \nexpression, as shown to the right, then implement as shown in the logic\ndiagram to the far right.  For the {K-map}, remember that the player\nloses whenever the fox and the goose are on the same side of the river,\nbut opposite from the farmer, or whenever the goose and the corn are on\nthe same side of the river, but opposite from the farmer.\n\n\n\n{eqnarray*}\nL &=& F   +  X G +\n&& F   +  G C\n{eqnarray*}\n\n\n\n\n\n\nFinally, we complete our design by integrating the next-state\ncalculation and the win-lose calculation with a couple of muxes, as\nshown to the right.\n\nThe lower mux controls the final value of the next state: note that\nit selects between the constant state FXGC=0000 when the reset button R\nis pressed and the output of the upper mux when R=0.\n\nThe upper mux is controlled by W+L, and retains the current state\nwhenever either signal is 1.  In other words, once the player has won\nor lost, the upper mux prevents further state changes until the reset\nbutton is pressed.\n\nWhen R, W, and L are all 0, the next state is calculated\naccording to whatever buttons have been pressed.\n\n\n\n\n\n\n\n \n Question: \nQ. What is the output of the priority encoder when the B_C button is pressed? \n Answer: \n\nA. The output of the priority encoder is 011."}, {"text": "Title: Analysis of a Stoplight Controller \n Text: {Analysis of a Stoplight Controller}\n\nIn this example, we begin with a digital FSM design and analyze it to\nunderstand how it works and to verify that its behavior is appropriate.\n\nThe FSM that we analyze has been designed to control the stoplights\nat the intersection of two roads.  For naming purposes, we assume that one\nof the roads runs East and West (EW), and the second runs North and \nSouth (NS).  \n\nThe stoplight controller has two inputs, each of which \nsenses vehicles approaching from either direction on one of the two roads.\nThe input V^=1 when a vehicle approaches from either the East or the\nWest, and the input V^=1 when a vehicle approaches from either the\nNorth or the South.  These inputs are also active when vehicles are stopped\nwaiting at the corresponding lights.\n\nAnother three inputs, A, B, and C, control the timing behavior of the\nsystem; we do not discuss them here except as variables.\n\n\nThe outputs of the controller consist of two {2-bit} values, \nL^ and L^, that specify the light colors for the two roads.\nIn particular, L^ controls the lights facing East and West, and\nL^ controls the lights facing North and South.  The meaning of these\noutputs is given in the table to the right.\n\n\n{c|c}\nL& light color \n0x& red\n10& yellow\n11& green\n\n\n\nLet's think about the basic operation of the controller.\n\nFor safety reasons, the controller must ensure that the lights on one\nor both roads are red at all times.  \n\nSimilarly, if a road has a green light, the controller should \nshow a yellow light before showing a red light to give drivers some\nwarning and allow them to slow down.\n\nFinally, for fairness, the controller should alternate green lights\nbetween the two roads.\n\nNow take a look at the logic diagram below.\n\nThe state of the FSM has been split into two pieces: a {3-bit} \nregister S and a {6-bit} timer.  The timer is simply a binary \ncounter that counts downward and produces an output of Z=1 when it \nreaches 0.  Notice that the register S only takes a new value\nwhen the timer reaches 0, and that the Z signal from the timer\nalso forces a new value to be loaded into the timer in the next \ncycle.  We can thus think of transitions in the FSM on a cycle by \ncycle basis as consisting of two types.  The first type simply\ncounts downward for a number of cycles while holding the register S\nconstant, while the second changes the value of S and sets the\ntimer in order to maintain the new value of S \nfor some number of cycles.\n\n\n\n3.45\n\n\nLet's look at the next-state logic for S, which feeds into the IN\ninputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice \nthat none of the inputs to the FSM directly affect these values.  The\nstates of S thus act like a counter.  By examining the connections,\nwe can derive equations for the next state and draw a transition\ndiagram, as shown to the right.\n\nAs the figure shows, there are six states in the loop defined by the \nnext-state logic, with the two remaining states converging into the\nloop after a single cycle.\n\nLet's now examine the outputs for each\nstate in order to understand how the stoplight sequencing\nworks.\n\nWe derive equations for the outputs that control the lights, as shown\nto the right, then calculate values and colors for each\nstate, as shown to the far right.  For completeness, the table \nincludes the states outside of the desired loop.  The \nlights are all red in both of these states, which is necessary for safety.\n\n\n{eqnarray*}\n\nS_2^+ &=& {S_2} + S_0\nS_1^+ &=& {S_2}  S_1\nS_0^+ &=& {S_2}\n{eqnarray*}\n\n{eqnarray*}\nL_1^ &=& S_2 S_1\nL_0^ &=& S_0\nL_1^ &=& S_2 {S_1}\nL_0^ &=& S_0\n{eqnarray*}\n\n\n\n{c|cc|cc}\n&&& EW& NS\n&&& light& light\nS& L^& L^& color& color \n000& 00& 00&    red&    red\n111& 11& 01&  green&    red\n110& 10& 00& yellow&    red\n010& 00& 00&    red&    red\n101& 01& 11&    red&  green\n100& 00& 10&    red& yellow \n001& 01& 01&    red&    red\n011& 01& 01&    red&    red\n\n\n\n\nNow let's think about how the timer works.  As we already noted, the\ntimer value is set whenever S enters a new state, but it can also be\nset under other conditions---in particular, by the signal F calculated\nat the bottom of the FSM logic diagram.  \n\n\nFor now, assume that F=0.  In this case, the timer is set only when\nthe state S changes, and we can find the duration of each state by\nanalyzing the muxes.  The bottom mux selects A when S_2=0, and \nselects the output of the top mux when S_2=1.  The top mux selects B\nwhen S_0=1, and selects C when S_0=0.  Combining these results,\nwe can calculate the duration of the next states of S when F=0, \nas shown in the table to the right.  We can then combine the next\nstate duration with our previous calculation of the state sequencing \n(also the order in the table) to obtain the durations of each state, also\nshown in the rightmost column of the table.\n\n\n{c|cc|cc}\n& EW& NS& next& current\n& light& light& state& state\nS& color& color& duration& duration \n000&    red&    red& A& C\n111&  green&    red& B& A\n110& yellow&    red& C& B\n010&    red&    red& A& C\n101&    red&  green& B& A\n100&    red& yellow& C& B \n001&    red&    red& A& ---\n011&    red&    red& A& ---\n\n\n\nWhat does F do?  Analyzing the gates that produce it gives \nF=S_1S_0{V^+{S_1}S_0{V^.  If we \nignore the two states outside of the main loop for S, the first term \nis 1 only when the lights are green on the East and West roads and the \ndetector for the North and South roads indicates that no vehicles are \napproaching.  Similarly, the second term is 1 only when the lights are \ngreen on the North and South roads and the detector for the East and \nWest roads indicates that no vehicles are approaching.\n\nWhat happens when F=1?  First, the OR gate feeding into the timer's\nLD input produces a 1, meaning that the timer loads a new value\ninstead of counting down.  Second, the OR gate controlling the lower\nmux selects the A input.  In other words, the timer is reset to A\ncycles, corresponding to the initial value for the green light states.\nIn other words, the light stays green until vehicles approach on \nthe other road, plus A more cycles.\n\nUnfortunately, the signal F may also be 1 in the unused states of S,\nin which case the lights on both roads may remain red even though cars\nare waiting on one of the roads.  To avoid this behavior, we must be \nsure to initialize the state S to one of the six states in the\ndesired loop.\n\n\n\n \n Question: \nQ. How does the FSM ensure that the lights are always red on one or both roads? \n Answer: \n\nA. The FSM always has at least one red light on at all times by design. This is to ensure safety at the intersection."}, {"text": "Title: From FSM to Computer \n Text: {From FSM to Computer}\n\nThe FSM designs we have explored so far have started with a human-based\ndesign process in which someone writes down the desired behavior in\nterms of states, inputs, outputs, and transitions.  Such an approach\nmakes it easier to build a digital FSM, since the abstraction used\ncorresponds almost directly to the implementation.\n\nAs an alternative, one can start by mapping the desired task into a\nhigh-level programming language, then using components such as registers,\ncounters, and memories to implement the variables needed.  In this approach,\nthe control structure of the code maps into a high-level FSM design.\nOf course, in order to implement our FSM with digital logic, we eventually\nstill need to map down to bits and gates.\n\nIn this set of notes, we show how one can transform a piece of code\nwritten in a high-level language into an FSM.  This process is meant to\nhelp you understand how we can design an FSM that executes simple\npieces of a flow chart such as assignments, { if} statements, and \nloops.  Later, we generalize this concept and build an FSM that allows\nthe pieces to be executed to be specified after the FSM is built---in \nother words, the FSM executes a program specified by bits stored in \nmemory.  This more general model, as you might have already guessed, \nis a computer.  \n\n \n Question: \nQ. What is the difference between an FSM and a computer? \n Answer: \n\nA. The difference between an FSM and a computer is that an FSM is a specific type of finite state machine that is used to execute simple pieces of a flow chart, while a computer is a more general model that allows the pieces to be executed to be specified after the FSM is built."}, {"text": "Title: Specifying the Problem \n Text: {Specifying the Problem}\n\nLet's begin by specifying the problem that we want to solve.\nSay that we want to find the minimum value in a set of 10 integers.\nUsing the C programming language, we can write the following fragment of \ncode:\n\n\n{\n\naaaa=aaaa=\nint >values[10];    /* 10 integers--filled in by other code */\nint >idx;\nint >min\n\nmin = values[0];\nfor (idx = 1; 10 > idx; idx = idx + 1) {\n>  if (min > values[idx]) {\n>  >  min = values[idx];\n>  }\n}\n/* The minimum value from array is now in min. */\n\n}\n\n\nThe code uses array notation, which we have not used previously in our \nclass, so let's first discuss the meaning of the code.\n\nThe code uses three variables.\n\nThe variable { values} represents the 10 values in our set.\nThe suffix ``[10]'' after the variable name tells the compiler that\nwe want an array of 10 integers ({ int}) indexed from 0 to 9.\nThese integers can be treated as 10 separate variables, but can be\naccessed using the single name ``{ values}'' along with an index\n(again, from 0 to 9 in this case).\n\nThe variable { idx} holds a loop index that we use to examine each\nof the values one by one in order to find the minimum value in the set.\n\nFinally, the variable { min} holds the smallest known value as \nthe program examines each of the values in the set.\n\nThe program body consists of two statements.  \n\nWe assume that some other piece of code---one not shown here---has \ninitialized the 10 values in our set before the code above executes.\n\nThe first statement initializes the\nminimum known value ({ min}) to the value stored at index 0 in the \narray ({ values[0]}).\nThe second statement is a loop in which the variable { index} \ntakes on values from 1 to 9.  For each value, an { if} statement\ncompares the current\nknown minimum with the value stored in the array at index given by the\n{ idx} variable.  If the stored value is smaller, the current known \nvalue (again, { min}) is updated to reflect the program's\nhaving found a smaller value.  When the loop finishes all nine iterations,\nthe variable { min} holds the smallest value among the set of 10 \nintegers stored in the { values} array.\n\n\nAs a first step towards designing an FSM to implement the code, we transform\nthe code into a flow chart, as shown to the right.  The program again begins\nwith initialization, which appears in the second column of the flow chart.  \nThe loop in the program translates to the third column of the flow chart, \nand the { if} statement to the middle comparison and update \nof { min}.\n\nOur goal is now to design an FSM to implement the flow chart.  In order\nto do so, we want to leverage the same kind of abstraction that we used\nearlier, when extending our keyless entry system with a timer.  Although the\ntimer's value was technically also\n\n\n{{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}\n\n\npart of the FSM's state, we treated it\nas data and integrated it into our next-state decisions in only a couple\nof cases.\n\nFor our minimum value problem, we have two sources of data.  First, an\nexternal program supplies data in the form of a set of 10 integers.  If\nwe assume {32-bit} integers, these data technically form 320 input bits!\nSecond, as with the keyless entry system timer, we have data used internally\nby our FSM, such as the loop index and the current minimum value.  These\nare technically state bits.  For both types of data, we treat them\nabstractly as values rather than thinking of them individually as bits,\nallowing us to develop our FSM at a high-level and then to implement it \nusing the components that we have developed earlier in our course.\n\n \n Question: \nQ. How many variables does the code use? \n Answer: \n\nA. The code uses three variables."}, {"text": "Title: Choosing Components and Identifying States \n Text: {Choosing Components and Identifying States}\n\nNow we are ready to design an FSM that implements the flow chart.\nWhat components do we need, other than our state logic?\nWe use registers and counters to implement the variables { idx}\nand { min} in the program.\nFor the array { values}, we use a {1632-bit} \nmemory.{We technically only need a {1032-bit} \nmemory, but we round up the size of the address space to reflect more\nrealistic memory designs; one can always optimize later.}\nWe need a comparator to implement the test for the { if} statement.\nWe choose to use a serial comparator, which allows us to illustrate again\nhow one logical high-level state can be subdivided into many actual states.\nTo operate the serial comparator, we make use of two shift registers that \npresent the comparator with one bit per cycle on each input, and a counter\nto keep track of the comparator's progress.\n\nHow do we identify high-level states from our flow chart?  Although\nthe flow chart attempts to break down the program into `simple' steps,\none step of a flow chart may sometimes require more than one state\nin an FSM.  Similarly, one FSM state may be able to implement several\nsteps in a flow chart, if those steps can be performed simultaneously.\nOur design illustrates both possibilities.\n\nHow we map flow chart elements into FSM states also depends to some \ndegree on what components we use, which is why we began with some discussion\nof components.  In practice, one can go back and forth between the two, \nadjusting components to better match the high-level states, and adjusting \nstates to better match the desired components.\n\nFinally, note that we are only concerned with high-level states, so we do \nnot need to provide details (yet) down to the level of individual clock \ncycles, but we do want to define high-level states that can be implemented\nin a fixed number of cycles, or at least a controllable number of cycles.\nIf we cannot specify clearly when transitions occur from an FSM state, we\nmay not be able to implement the state.\n\n\n\nNow let's go through the flow chart and identify states.  Initialization of\n{ min} and { idx} need not occur serially, and the result of the\nfirst comparison between { idx} and the constant 10 is known in advance,\nso we can merge all three operations into a single state, which we \ncall { INIT}.\n\nWe can also merge the updates of { min} and { idx} into a second\nFSM state, which we call { COPY}.  However, the update to { min} \noccurs only when the comparison ({ min > value[idx]}) is true.  \nWe can use logic to predicate execution of the update.  In other words, we \ncan use the output of the comparator, which is available after the comparator \nhas finished comparing the two values (in a high-level FSM state that we \nhave yet to define), to determine whether or not the register holding \n{ min} loads a new value in the { COPY} state.\n\nOur model of use for this FSM involves external logic filling the memory\n(the array of integer values), executing the FSM ``code,'' and then\nchecking the answer.  To support this use model, we create a FSM state \ncalled { WAIT} for cycles in which the FSM has no work to do.\nLater, we also make use of an external input signal { START} \nto start the FSM\nexecution.  The { WAIT} state logically corresponds to the ``START'' \nbubble in the flow chart.\n\n\nOnly the test for the { if} statement remains.  Using a serial\ncomparator to compare two {32-bit} values requires 32 cycles.\nHowever, we need an additional cycle to move values into our shift \nregisters so that the comparator can see the first bit.  Thus our\nsingle comparison operation breaks into two high-level states.  In the\nfirst state, which we call { PREP}, we copy { min} to one\nof the shift registers, copy { values[idx]} to the other shift\nregister, and reset the counter that measures the cycles needed for\nour serial comparator.  We then move to a second high-level state,\nwhich we call { COMPARE}, in which we feed one bit per cycle\nfrom each shift register to the serial comparator.  The { COMPARE} \nstate\n\n\n{{file=part3/figs/part3-min-flow-chart-states.eps,width=3.96in}}\n\n\nexecutes for 32 cycles, after which the comparator\nproduces the one-bit answer that we need, and we can move to the\n{ COPY} state.  The association between the flow chart and the\nhigh-level FSM states is illustrated in the figure shown to the right\nabove.\n\n\nWe can now also draw an abstract state diagram for our FSM, as shown\nto the right.  The FSM begins in the { WAIT} state.  After external\nlogic fills the { values} array, it signals the FSM to begin by\nraising the { START} signal.  The FSM transitions into the \n{ INIT} state, and in the next cycle into the { PREP} state.\nFrom { PREP}, the FSM always moves to { COMPARE}, where it\nremains for 32 cycles while the serial comparator executes a comparison.\nAfter { COMPARE}, the FSM moves to the { COPY}\n\n\n{{file=part3/figs/part3-min-state-diag.eps,width=3in}}\n\n\nstate, where\nit remains for one cycle.  The transition from { COPY} depends on\nhow many loop iterations have executed.  If more loop iterations remain,\nthe FSM moves to { PREP} to execute the next iteration.  If the\nloop is done, the FSM returns to { WAIT} to allow external logic\nto read the result of the computation.\n\n\n\n \n Question: \nQ. What components do we need to implement the FSM? \n Answer: \n\nWe need registers and counters to implement the variables idx and min in the program. For the array values, we use a 1632-bit memory. We need a comparator to implement the test for the if statement."}, {"text": "Title: Laying Out Components \n Text: {Laying Out Components}\n\n\nOur high-level FSM design tells us what our components need to be able to\ndo in any given cycle.  For example, when we load new values into the shift\nregisters that provide bits to the serial comparator, we always copy \n{ min} into one shift register and { values[idx]} into the second.\nUsing this information, we can put together our components and simplify our\ndesign by fixing the way in which bits flow between them.\n\nThe figure at the right shows how we can organize our components.\nAgain, in practice, one goes back and forth thinking about states,\ncomponents, and flow from state to state.  In these notes, we \npresent only a completed design.\n\nLet's take a detailed look at each of the components.\n\nAt the upper left of the figure is a {4-bit} binary counter called\n{ IDX} to hold the { idx}\nvariable.  The counter can be reset to 0 using the { RST} input.\nOtherwise, the { CNT} input controls whether or not the counter\nincrements its value.  With this counter design, we can force { idx} \nto 0 in the { WAIT} state and then count upwards in the { INIT}\nand { COPY} states.\n\n\n{{file=part3/figs/part3-min-components.eps,width=3.84in}}\n\n\nA memory labeled { VALUES} to hold the array { values} appears \nin the upper right of\nthe figure.  The read/write control for the memory is hardwired to 1 (read)\nin the figure, and the data input lines are unattached.  To integrate \nwith other logic that can operate our FSM, we need to add more \ncontrol logic to allow writing into the memory and to attach the data\ninputs to something that provides the data bits.  The address input of\nthe memory comes always from the { IDX} counter value; in other words,\nwhenever we access this memory by making use of the data output lines,\nwe read { values[idx]}.\n\nIn the middle left of the figure is a {32-bit} register for the \n{ min} variable.  It has a control input { LD} that \ndetermines whether or not it loads a new value at the end of the clock\ncycle.  If a new value is loaded, the new value always corresponds to\nthe output of the { VALUES} memory, { values[idx]}.  Recall\nthat { min} always changes in the { INIT} state, and may change\nin the { COPY} state.  But the new value stored in { min}\nis always { values[idx]}. \nNote also that when the FSM completes its task, the result of the \ncomputation is left in the { MIN} register for external logic to\nread (connections for this purpose are not shown in the figure).\n\nContinuing downward in the figure, we see two right shift registers\nlabeled { A} and { B}.  Each has a control input { LD}\nthat enables a parallel load.  Register { A} loads from register\n{ MIN}, and register { B} loads from the memory data output\n({ values[idx]}).  These loads are needed in the { PREP}\nstate of our FSM.  When { LD} is low, the shift registers\nsimply shifts to the right.  The serial output { SO} makes the\nleast significant bit of each shift register available.  Shifting\nis necessary to feed the serial comparator in the { COMPARE} state.\n\nBelow register { A} is a {5-bit} binary counter called { CNT}.\nThe counter is used to control the serial comparator in the { COMPARE}\nstate.  A reset input { RST} allows it to be forced to 0 in the \n{ PREP} state.  When the counter value is exactly zero, the \noutput { Z} is high.\n\n\n\nThe last major component is the serial comparator, which is based on the\ndesign developed in Notes Set 3.1.  The two bits to be compared in \na cycle come from shift registers { A} and { B}.  The first\nbit indicator comes from the zero indicator of counter { CNT}.\nThe comparator actually produces two outputs ({ Z1} and { Z0}),\nbut the meaning of the\n{ Z1} output by itself is { A > B}.  In the diagram,\nthis signal has been labeled { THEN}.\n\nThere are two additional elements in the figure that we have yet to discuss.\nEach simply compares the value in a register with a fixed constant and \nproduces a {1-bit} signal.  When the FSM finishes an iteration of\nthe loop in the { COPY} state, it must check the loop condition\n({ 10 > idx}) and move either to the { PREP} state or, when\nthe loop finishes, to the { WAIT} state to let the external logic\nread the answer from the { MIN} register.  The loop is done when the\ncurrent iteration count is nine, so we compare { IDX} with nine to\nproduce the { DONE} signal.  The other constant comparison is between\nthe counter { CNT} and the value 31 to produce the { LAST} signal,\nwhich indicates that the serial comparator is on its last cycle of \ncomparison.  In the cycle after { LAST} is high, the { THEN} \noutput of the comparator indicates whether or not { A > B}.\n\n \n Question: \nQ. * What is the purpose of the DONE and LAST signals? \n Answer: \n\n\nThe DONE and LAST signals are used to compare the value in a register with a fixed constant. The DONE signal compares the value in the IDX register with the constant 9, and the LAST signal compares the value in the CNT register with the constant 31. These comparisons are used to determine when the loop is finished and when the serial comparator is on its last cycle of comparison, respectively."}, {"text": "Title: Control and Data \n Text: {Control and Data}\n\nOne can think of the components and the interconnections between them\nas enabling the movement of data between registers, while the high-level \nFSM controls which data move from register to register in each cycle.  \n\nWith this model in mind, we call the components and interconnections\nfor our design the { datapath}---a term that we will see again when\nwe examine the parts of a computer in the coming weeks.\n\nThe datapath requires several inputs to control the operation of the\ncomponents---these we can treat as outputs of the FSM.\nThese signals allow the FSM to control the motion of data in the \ndatapath, so we call them { control signals}.\n\nSimilarly, the datapath produces several outputs that we can treat\nas inputs to the FSM.\n\nThe tables below summarize the control signals (left) and outputs (right)\nfrom the datapath for our FSM.\n\n[t]\n{|c|l|}\ndatapath& \ninput& {|c|} \n{ IDX.RST}& reset { IDX} counter to 0\n{ IDX.CNT}& increment { IDX} counter\n{ MIN.LD}& load new value into { MIN} register\n{ A.LD}& load new value into shift register { A}\n{ B.LD}& load new value into shift register { B}\n{ CNT.RST}& reset { CNT} counter \n\n\n[t]\n{|c|l|c|}\ndatapath& &\noutput& {|c|}& based on \n{ DONE}& last loop iteration finished& { IDX = 9}\n{ LAST}& serial comparator executing& { CNT = 31}\n&    last cycle&\n{ THEN}& { if} statement condition true& { A > B} \n{}\n{}\n\n\n\nUsing the datapath controls signals and outputs, we can now write a more\nformal state transition table for the FSM, as shown below.\n\nThe ``actions'' column of the table\nlists the changes to register and counter values\nthat are made in each of the FSM states.  The notation used to represent\nthe actions is called { register transfer language} ({ RTL}).\nThe meaning of an individual action is similar to the meaning of the \ncorresponding statement from our C code or from the flow chart.\nFor example, in the { WAIT} state, ``{ IDX  0}''\nmeans the same thing as ``{ idx = 0;}''.  In particular, both mean\nthat the value currently stored in the { IDX} counter is overwritten \nwith the number 0 (all 0 bits).\n\n\n\nThe meaning of RTL is slightly different from the usual interpretation of\nhigh-level programming languages, however, in terms of when the actions\nhappen.  A list of C statements is generally executed one at a time.\n\nIn contrast, the entire list of RTL actions\n\n\n{|c|l|c|c|}\nstate& {|c|}{actions (simultaneous)}& condition& next state \n{ WAIT}& { IDX}  0 (to read { VALUES[0]} in { INIT})& { START}& { INIT} \n&& {{ START}}& { WAIT} \n{ INIT}& { MIN}  { VALUES[IDX]} ({ IDX} is 0 in this state)& (always)& { PREP} \n& { IDX}  { IDX} + 1& & \n{ PREP}& { A}  { MIN}& (always)& { COMPARE}\n& { B}  { VALUES[IDX]}& &\n& { CNT}  0& & \n{ COMPARE}& run serial comparator& { LAST}& { COPY}\n&& {{ LAST}}& { COMPARE} \n{ COPY}& { THEN}: { MIN}  { VALUES[IDX]}& { DONE}& { WAIT} \n& { IDX}  { IDX} + 1& {{ DONE}}& { PREP} \n\n\n\nfor an FSM state is executed\nsimultaneously, at the end of the clock cycle.  As you know, an FSM moves \nfrom its current state into a new state at the end of every clock cycle,\nso actions during different cycles usually are associated with different \nstates.\nWe can, however, change the value in more than one register at the end \nof the same clock cycle, so we can execute more than one RTL action in\nthe same state, so long as the actions do not exceed the capabilities\nof our datapath (the components must be able to support the simultaneous \nexecution of the actions).  Some care must be taken with states that \nexecute for more than one cycle to ensure that repeating the \nRTL actions is appropriate.  In our design, only the { WAIT} and\n{ COMPARE} states execute for more than one cycle.  The { WAIT}\nstate resets the { IDX} counter repeatedly, which causes no problems.\nThe { COMPARE} statement has no RTL actions---all of the shifting,\ncomparison, and counting activity needed to do its work occurs within \nthe datapath itself.\n\nOne additional piece of RTL syntax needs explanation.  In the { COPY}\nstate, the first action begins with ``{ THEN}:,'' which means that the \nprefixed RTL action occurs only when the { THEN} signal is high.\nRecall that the { THEN} signal indicates that the comparator has\nfound { A > B}, so the equivalent C code is ``{ if (A > B) \n{min = values[idx]}}''.\n\n \n Question: \nQ. How does the FSM move from its current state into a new state? \n Answer: \n\nA. The FSM moves from its current state into a new state at the end of every clock cycle."}, {"text": "Title: State Representation and Logic Expressions \n Text: {State Representation and Logic Expressions}\n\nLet's think about the representation for the FSM states.  The FSM has \nfive states, so we could use as few as three flip-flops.  Instead, we choose\nto use a { one-hot encoding}, in which any valid bit pattern has exactly\none 1 bit.  In other words, we use five flip-flops instead of three,\nand our states are represented with the bit patterns 10000, 01000, 00100,\n00010, and 00001.\n\nThe table below shows the mapping from each high-level state to \nboth the five-bit encoding for the state as well as the six control signals \nneeded for the datapath.  For each state, the values of the control signals\ncan be found by examining the actions necessary in that state.\n\n{\n{|c|c|cccccc|}\nstate& { S_4S_3S_2S_1S_0}& { IDX.RST}& { IDX.CNT}& { MIN.LD}& { A.LD}& { B.LD}& { CNT.RST} \n{ WAIT}& { 1 0 0 0 0}& 1& 0& 0& 0& 0& 0\n{ INIT}& { 0 1 0 0 0}& 0& 1& 1& 0& 0& 0\n{ PREP}& { 0 0 1 0 0}& 0& 0& 0& 1& 1& 1\n{ COMPARE}& { 0 0 0 1 0}& 0& 0& 0& 0& 0& 0\n{ COPY}& { 0 0 0 0 1}& 0& 1& { THEN}& 0& 0& 0 \n{}\n\n}\n\nThe { WAIT} state needs to set { IDX} to 0 but need not affect \nother register or counter values, so { WAIT} produces a 1 only for\n{ IDX.RST}.  The { INIT} state needs to load { values[0]} into \nthe { MIN} register while simultaneously incrementing the { IDX}\ncounter (from 0 to 1), so { INIT} produces 1s for { IDX.CNT}\nand { MIN.LD}.  The { PREP} state loads both shift registers\nand resets the counter { CNT} by producing 1s for { A.LD},\n{ B.LD}, and { CNT.RST}.  The { COMPARE} state does not\nchange any register values, so it produces all 0s.  Finally, the { COPY}\nstate increments the { IDX} counter while simultaneously loading a\nnew value into the { MIN} register.  The { COPY} state produces 1\nfor { IDX.CNT}, but must use the signal { THEN} coming from the\ndatapath to decide whether or not { MIN} is loaded.\n\n\n\nThe advantage of a one-hot encoding becomes obvious when we write\nequations for the six control signals and the next-state logic, as shown\nto the right.  \n\nImplementing the logic to complete our design now requires only a handful \nof small logic gates.\n\n\n{eqnarray*}\n{ IDX.RST} & = & { S}_4\n{ IDX.CNT} & = & { S}_3 + { S}_0\n{ MIN.LD} & = & { S}_3 + { S}_0  { THEN}\n{ A.LD} & = & { S}_2\n{ B.LD} & = & { S}_2\n{ CNT.RST} & = & { S}_2\n{eqnarray*}\n\n\n{eqnarray*}\n{ S}_4^+ & = & { S}_4  { START} + { S}_0  { DONE}\n{ S}_3^+ & = & { S}_4  { START}\n{ S}_2^+ & = & { S}_3 + { S}_0  { DONE}\n{ S}_1^+ & = & { S}_2 + { S}_1  { LAST}\n{ S}_0^+ & = & { S}_1  { LAST}\n{eqnarray*}\n\n\nNotice that the terms in each control signal can be read directly from \nthe rows of the state table and OR'd together.  The terms in each of the\nnext-state equations represent the incoming arcs for the corresponding\nstate.  For example, the { WAIT} state has one self-loop (the first\nterm) and a transition arc coming from the { COPY} state when the\nloop is done.\n\nThese expressions complete our design.\n\n\n\n\n\n \n Question: \nQ. What is the advantage of using a one-hot encoding to represent the FSM states? \n Answer: \n\nA. The advantage of using a one-hot encoding is that it is easy to write equations for the six control signals and the next-state logic."}, {"text": "Title: Extending Keyless Entry with a Timeout \n Text: {Extending Keyless Entry with a Timeout}\n\nThis set of notes builds on the keyless entry control FSM that we\ndesigned earlier.  In particular, we use a counter to make the alarm\ntime out, turning itself off after a fixed amount of time.  The goal\nof this extension is to illustrate how we can make use of components\nsuch as registers and counters as building blocks for our FSMs\nwithout fully expanding the design to explicitly illustrate all\npossible states.\n\n \n Question: \nQ. What is the goal of this extension?\nQ. \nQ. The goal of this extension is to illustrate how we can make use of components such as registers and counters as building blocks for our FSMs without fully expanding the design to explicitly illustrate all possible states. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing \n Text: {Physical Design, Sensors, and Timing}\n\n\nTo begin, let's review the FSM that we designed earlier for keyless \nentry.  The state transition diagram for our design is replicated to \nthe right.\n\nThe four states are labeled with state bits and output bits,\nS_1S_0/DRA, where D indicates that the driver's door should\nbe unlocked, R indicates that the rest of the doors should be\nunlocked, and A indicates that the alarm should be on.\n\nTransition arcs in the diagram are labeled with concise versions of \nthe inputs ULP (using don't cares), where U represents an unlock\nbutton, L represents a lock button, and P represents a panic \nbutton.\n\nIn this design, once a user presses the panic button P, the alarm\nsounds until the user presses the\n\n\n{file=part3/figs/ke-trans-diag-brief.eps,width=4.2in}\n\n\nlock button L to turn it off.\nInstead of sounding the alarm indefinitely, we might want to turn\nthe alarm off after a fixed amount of time.  In other words, after \nthe system has been in the ALARM state for, say, thirty or sixty seconds,\nwe might want to move back to the LOCKED state even if the user has\nnot pushed the lock button.  The blue annotation in the diagram indicates\nthe arc that we must adjust.  But thirty or sixty seconds is a large \nnumber of clock cycles, and our FSM must keep track of the time.\nDo we need to draw all of the states?\n\nInstead of following the design process that we outlined earlier, let's\nthink about how we can modify our existing design to incorporate the new\nfunctionality.  In order to keep track of time, we use a binary counter.\n\nLet's say that we want our timeout to be T cycles.\n\nWhen we enter the alarm state, we want to set the counter's value \nto T-1, then let the counter count down until it reaches 0, at \nwhich point a timeout occurs.\n\nTo load the initial value, our counter should have a parallel load \ncapability that sets the counter value when input LD=1.  When LD=0,\nthe counter counts down. \n\nThe counter also has an output Z that indicates that the counter's \nvalue is currently zero, which we can use to indicate a timeout on\nthe alarm.\n\nYou should be able to build such a counter based on what you have learned\nearlier in the class.  Here, we will assume that we can just make use of\nit.\n\nHow many bits do we need in our counter?  The answer depends on T.\nIf we add the counter to our design, the state of the counter is \ntechnically part of the state of our FSM, but we can treat it\nsomewhat abstractly.  For example, we only plan to make use of the \ncounter value in the ALARM state, so we ignore the counter bits in the\nthree other states.  In other words, S_1S_0=10 means that the system \nis in the LOCKED state regardless of the counter's value.\n\n\nWe expand the ALARM state into T separate states based on the value\nof the counter.  As shown to the right, we name the states ALARM(1) \nthrough ALARM(T).  All of these alarm states use S_1S_0=01, but\nthey can be differentiated using a ``timer'' (the counter value).\n\nWe need to make design decisions about how the arcs entering and\nleaving the ALARM state in our original design should be used once\nwe have incorporated the timeout.  As a first step, we decide that\nall arcs entering ALARM from other states now enter ALARM(1).  \nSimilarly, if the user presses the panic button P in any of the\nALARM(t) states, the system returns to ALARM(1).  Effectively, \npressing the panic button resets the timer.\n\nThe only arc leaving the ALARM state goes to the LOCKED state on \nULP=x10.  We replicate this arc for all ALARM(t) states: the\nuser can push the lock button at any time to silence the alarm.\n\nFinally, the self-loop back to the ALARM state on ULP=x00 becomes\nthe countdown arcs in our expanded states, taking ALARM(t) to ALARM(t+1),\nand ALARM(T) to LOCKED.\n\nNow that we have a complete specification for the extended design, we\ncan implement it.  We want to reuse our original design as much as possible,\nbut we have three new features that must be considered.  First, when\nwe enter the ALARM(1) state, we need to set the counter value to T-1.\nSecond, we need the counter value to count downward while in the ALARM\nstate.  Finally, we need to move back to the LOCKED state when a timeout\noccurs---that is, when the counter reaches zero.\n\n\n{file=part3/figs/ke-alarm-expansion.eps,width=1.75in}\n\n\nThe first problem is fairly easy.  Our counter supports parallel load,\nand the only value that we need to load is T-1, so we apply the constant\nbit pattern for T-1 to the load inputs and raise the LD input whenever\nwe enter the ALARM(1) state.  In our original design, we chose to enter\nthe ALARM state whenever the user pressed P, regardless of the other\nbuttons.  Hence we can connect P directly to our counter's LD input.\n\nThe second problem is handled by the counter's countdown functionality.\nIn the ALARM(t) states, the counter will count down each cycle, moving\nthe system from ALARM(t) to ALARM(t+1).\n\nThe last problem is slightly trickier, since we need to change S_1S_0.\nNotice that S_1S_0=01 for the ALARM state and S_1S_0=00 for the LOCKED\nstate.  Thus, we need only force S_0 to 0 when a timeout occurs.\nWe can use a single {2-to-1} multiplexer for this purpose.  The\n``0'' input of the mux comes from the original S_0^+ logic, and the\n``1'' input is a constant 0.  All other state logic remains unchanged.  \nWhen does a timeout occur? \n\nFirst, we must be in the ALARM(T) state, so S_1S_0=01 and the \ncounter's Z output is raised.  Second, the input combination must\nbe ULP=xx0---notice that both ULP=x00 and ULP=x10 return to \nLOCKED from ALARM(T).  A single, four-input AND gate thus suffices to\nobtain the timeout signal, {S_1}S_0Z, which we connect \nto the select input of the mux between the S_0^+ logic and the\nS_0 flip-flop.\n\nThe extension thus requires only a counter, a mux, and a gate, as shown below.\n\n{{file=part3/figs/ke-alarm-exp-impl.eps,width=2.65in}}\n\n\n\n\n\n \n Question:  \n Answer: \nWhat is the function of the counter in the extended design?\n\nThe counter is used to keep track of time so that the alarm can sound for a fixed amount of time."}, {"text": "Title: Serialization and Finite State Machines \n Text: {Serialization and Finite State Machines}\n\nThe third part of our class builds upon the basic combinational and\nsequential logic elements that we developed in the second part.\n\nAfter discussing a simple application of stored state\nto trade between area and performance,\n\nwe introduce a powerful abstraction for formalizing and reasoning about\ndigital systems, the Finite State Machine (FSM).\n\nGeneral FSM models are broadly applicable in a range of engineering\ncontexts, including not only hardware and software design but also\nthe design of control systems and distributed systems.  We limit our\nmodel so as to avoid circuit timing issues in your first exposure, but\nprovide some amount of discussion as to how, when, and why you should \neventually learn the more sophisticated models.\n\nThrough development a range of FSM examples, we illustrate important \ndesign issues for these systems and motivate a couple of more advanced \ncombinational logic devices that can be used as building blocks.\n\nTogether with the idea of memory, another form of stored state,\nthese elements form the basis for development of our first computer.\n\nAt this point we return to the textbook, in which Chapters 4 and 5\nprovide a solid introduction to the von Neumann model of computing systems\nand the {LC-3} (Little Computer, version 3) instruction set \narchitecture.  By the end of this part of the course, you will have\nseen an example of the boundary between hardware and software, and will\nbe ready to write some instructions yourself.\n\nIn this set of notes, we cover the first few parts of this material.\nWe begin by describing the conversion of bit-sliced designs into \nserial designs, which store a single bit slice's output in \nflip-flops and then feed the outputs back into the bit slice in the next\ncycle.  As a specific example, we use our bit-sliced comparator \nto discuss tradeoffs in area and performance.  We introduce\nFinite State Machines and some of the tools used to design them,\nthen develop a handful of simple counter designs.  Before delving\ntoo deeply into FSM design issues, we spend a little time discussing\nother strategies for counter design and placing the material covered\nin our course in the broader context of digital system design.\n\nRemember that\n{ sections marked with an asterisk are provided solely for your\ninterest,} but you may need to learn this material in later\nclasses.\n\n\n \n Question: \nQ. How does the design of a finite state machine differ from that of a counter? \n Answer: \n\nThe design of a finite state machine differs from that of a counter in a few ways. For one, a finite state machine has a finite number of states that it can be in, while a counter can have an infinite number of states. Additionally, a finite state machine's output is determined by its current state, while a counter's output is determined by its current state and the inputs it receives. Finally, a finite state machine can have multiple inputs and outputs, while a counter usually has a single input and a single output."}, {"text": "Title: Serialization: General Strategy \n Text: {Serialization: General Strategy}\n\nIn previous notes, we discussed and illustrated the development of bit-sliced\nlogic, in which one designs a logic block to handle one bit of a multi-bit\noperation, then replicates the bit slice logic to construct a design for \nthe entire operation.  We developed ripple carry adders in this way in \nNotes Set 2.3 and both unsigned and 2's complement comparators in \nNotes Set 2.4.\n\n\nAnother interesting design strategy is { serialization}: rather than \nreplicating the bit slice, we can use flip-flops to store the bits passed\nfrom one bit slice to the next, then present the stored bits { to the\nsame bit slice} in the next cycle.  Thus, in a serial design, we only\nneed one copy of the bit slice logic!  The area needed for a serial design\nis usually much less than for a bit-sliced design, but such a design is\nalso usually slower.  After illustrating the general design strategy,\nwe'll consider these tradeoffs more carefully\nin the context of a detailed example.\n\n\nRecall the general bit-sliced design approach, as illustrated to the right.\n\nSome number of copies of the logic for a single bit slice are connected\nin sequence.  Each bit slice accepts P bits of operand input and\nproduces Q bits of external output.  Adjacent bit slices receive\nan additional M bits of information from the previous bit slice\nand pass along M bits to the next bit slice, generally using some\nrepresentation chosen by the designer.\n\n\n{file=part3/figs/gen-slice-comp.eps,width=3.8in}\n\n\nThe first bit slice is initialized\nby passing in constant values, and some calculation may be performed\non the final bit slice's results to produce R bits more external output.\n\n\n\nWe can transform this bit-sliced design to a serial design with a single\ncopy of the bit slice logic, M+Q flip-flops, and M gates (and sometimes\nan inverter).  The strategy is illustrated on the right below.  A single\ncopy of the bit slice operates on one set of P external input bits \nand produces one set of Q external output bits each clock cycle.  In\nthe design shown, these output bits are available during the next \ncycle, after they have been stored in the flip-flops. \nThe M bits to be passed to the ``next'' bit slice are also \nstored in flip-flops,\nand in the next cycle are provided back to the same physical bit slice\nas inputs.  The first cycle of a multi-cycle operation\nmust be handled slightly differently, so we\nadd selection logic and an control signal, F.  For the first cycle, we\napply F=1, and the initial values are passed into the bit slice.  \nFor all other bits, we apply F=0, and the values stored in the flip-flops\nare returned to the bit slice's inputs.  After all bits have passed through\nthe bit slice---after N cycles for an {N-bit} design---the\nfinal M bits are stored in the flip-flops, and the results are calculated\nby the output logic. \n\n{\n\n{file=part3/figs/init-ser-slice.eps,width=1.4in}\n\n\n{file=part3/figs/ser-slice-comp.eps,width=3.25in}\n\n}\n\nThe selection logic merits explanation.  Given that the original design\ninitialized the bits to constant values (0s or 1s), we need only simple\nlogic for selection.  The two drawings on the left above illustrate \nhow {B_i}, the complemented flip-flop output for a bit i, can be\ncombined with the first-cycle signal F to produce an appropriate input\nfor the bit slice.  Selection thus requires one extra gate for each of \nthe M inputs, and we need an inverter for F if any of the initial \nvalues is 1.\n\n\n \n Question: \nQ. What are the tradeoffs of a serial design compared to a bit-sliced design? \n Answer: \n\nThe main tradeoff of a serial design is that it is usually slower than a bit-sliced design. However, it requires less area, which may be important in some applications."}, {"text": "Title: Serialization: Comparator Example \n Text: {Serialization: Comparator Example}\n\n\nWe now apply the general strategy to a specific example, our bit-sliced\nunsigned comparator from Notes Set 2.4.  The result is shown to the right.\nIn terms of the general model, the single comparator bit slice accepts P=2\nbits of inputs each cycle, in this case a single bit from each of the two\nnumbers being compared, presented to the bit slice in increasing order\nof significance.  The bit slice produces no external output other than\nthe final result (Q=0).  Two bits (M=2) are produced each cycle by \nthe bit slice and stored\n\n\n{file=part3/figs/ser-compare.eps,width=3.5in}\n\n\ninto flip flops B_1 and B_0.  These bits\nrepresent the relationship between the two numbers compared so far\n(including only the bit already seen by the comparator bit slice).\nOn the first cycle, when the least significant bits of A and B are\nbeing fed into the bit slice, we set F=1, which forces the C_1 \nand C_0 inputs of the bit slice to 0 independent of the values stored\nin the flip-flops.  In all other cycles, F=0, and the NOR gates\nset C_1=B_1 and C_0=B_0.  Finally, after N cycles for an {N-bit}\ncomparison, the output logic---in this case simply wires, as shown \nin the dashed box---places the result of the comparison on \nthe Z_1 and Z_0 outputs (R=2 in the general model).  \nThe result is encoded in the \nrepresentation defined for constructing the bit slice (see Notes Set 2.4,\nbut the encoding does not matter here).\n\n\nHow does the serial design compare with the bit-sliced design?\n\nAs an estimate of area, let's count gates.  Our optimized \ndesign is replicated to the right for convenience.  \n\nEach bit slice requires six {2-input} gates and two inverters.  \n\nAssume that each \nflip-flop requires eight {2-input} gates and two inverters,\nso the serial design overall requires 24 gates\n\n\n{file=part3/figs/comparator-opt-nn.eps,width=4.1in}\n\n\nand six inverters to handle any number of bits.\n\nThus, for any number of bits N, the serial design is smaller\nthan the bit-sliced design, and the benefit grows with N.\n\nWhat about performance?  In Notes Set 2.4, we counted gate delays for \nour bit-sliced design.  The path from A or B to the outputs is\nfour gate delays, but the C to Z paths are all two gate delays.\nOverall, then, the bit-sliced design requires 2N+2 gate delays \nfor N bits.  What about the serial design?\n\nThe performance of the serial design is likely to be much worse\nfor three reasons.\n\nFirst, all paths in the design matter, not just the paths from bit\nslice to bit slice.  None of the inputs can be assumed to be available\nbefore the start of the clock cycle, so we must consider all paths\nfrom input to output.  \n\nSecond, we must also count gate delays for the\nselection logic as well as the gates embedded in the flip-flops.\n\nFinally, the result of these calculations may not matter, since the\nclock speed may well be limited by other logic elsewhere in the\nsystem.  If we want a common clock for all of our logic, the clock\nmust not go faster than the slowest element in the entire system, or\nsome of our logic will not work properly.\n\nWhat is the longest path through our serial comparator?\n\nLet's assume that \nthe path through a flip-flop is eight gate delays, with four on each \nside of the clock's rising edge.  The inputs A and B are\nlikely to be driven by flip-flops elsewhere in our system, so\nwe conservatively count four gate delays to A and B and\nfive gate delays to C_1 and C_0 (the extra one comes from\nthe selection logic).  The A and B paths thus dominate inside\nthe bit slice, adding four more gate delays to the outputs Z_1\nand Z_0.  Finally, we add the last four gate delays to flip\nthe first latch in the flip-flops for a total of 12 gate delays.\n\nIf we assume that our serial comparator limits the clock frequency\n(that is, if everything else in the system can use a faster clock),\nwe take 12 gate delays per cycle, or 12N gate delays to compare\ntwo {N-bit} numbers.\n\nYou might also notice that adding support for 2's complement is no \nlonger free.  We need extra logic to swap the A and B inputs\nin the cycle corresponding to the sign bits of A and B.  In\nother cycles, they must remain in the usual order.  This extra\nlogic is not complex, but adds further delay to the paths.\n\nThe bit-sliced and serial designs represent two extreme points in \na broad space of design possibilities.  Optimization of the entire\nN-bit logic function (for any metric) represents a third extreme.\nAs an engineer, you should realize that you can design systems\nanywhere in between these points as well.  At the end of Notes Set 2.4,\nfor example, we showed a design for a logic slice that compares \ntwo bits at a time.  In general, we can optimize logic for any \nnumber of bits and then apply multiple copies of the resulting\nlogic in space (a generalization of the bit-sliced approach),\nor in time (a generalization of the serialization approach),\nor in a combination of the two.\n\nSometimes these tradeoffs may happen at a higher level.\nAs mentioned in Notes Set 2.3, computer software uses \nthe carry out of an adder to perform addition of larger \ngroups of bits (over multiple clock cycles)\nthan is supported by the processor's adder hardware.\n\nIn computer system design, engineers often design hardware elements\nthat are general enough to support this kind of extension in software.\n\nAs a concrete example of the possible tradeoffs,\nconsider a serial comparator design based on the {2-bit} slice variant.\n\nThis approach leads to a serial design with 24 gates and 10 inverters, \nwhich is not much\nlarger than our earlier serial design.  In terms of gate delays,\nhowever, the new design is identical, meaning that we finish a comparison\nin half the time.  More realistic area and timing metrics show\nslightly more difference between the two designs.  These differences can \ndominate the \nresults if we blindly scale the idea to handle more bits without \nthinking carefully about the design.  Neither many-input gates\nnor gates driving many outputs work well in practice.\n\n\n\n \n Question: \nQ. In the serial comparator design, why is it necessary to count gate delays for the selection logic as well as the gates embedded in the flip-flops? \n Answer: \n\nA. The serial comparator design uses a flip-flop to store the result of each comparison bit slice. The selection logic is used to determine when the flip-flop should be updated based on the inputs A and B. The gate delays for the selection logic must be counted in order to accurately determine the longest path through the serial comparator."}, {"text": "Title: Finite State Machines \n Text: {Finite State Machines}\n\nA { finite state machine} (or { FSM}) is a model for understanding\nthe behavior of a system by describing the system as occupying one of a\nfinite set of states, moving between these states in response to\nexternal inputs, and producing external outputs.  In any given state,\na particular input may cause the FSM to move to another state; this \ncombination is called a { transition rule}.\n\nAn FSM comprises five parts: a finite set of states, a set of \npossible inputs, a set of possible outputs, a set of transition rules,\nand methods for calculating outputs.  \n\nWhen an FSM is implemented as a digital system, all states must be\nrepresented as patterns using a fixed number of bits, all inputs must\nbe translated into bits, and all outputs must be translated into bits.\n\nFor a digital FSM, transition rules must be { complete}; in other words,\ngiven any state of the FSM, and any pattern of input bits,\na transition must be defined from that state to another state \n(transitions from a state to itself, called { self-loops}, \nare acceptable).\n\nAnd, of course, calculation of outputs for a digital FSM reduces\nto Boolean logic expressions.\n\nIn this class, we focus on clocked synchronous FSM implementations,\nin which the FSM's internal state bits are stored in flip-flops.\n\nIn this section, we introduce the tools used to describe, develop, and\nanalyze implementations of FSMs with digital logic.  In the next\nfew weeks, we will show you how an FSM can serve as the central control\nlogic in a computer.  At the same time, we will illustrate connections\nbetween FSMs and software and will make some connections with other\nareas of interest in ECE, such as the design and analysis of digital \ncontrol systems.\n\nThe table below gives a { list of abstract states} for a typical \nkeyless entry system for a car.  In this case, we have merely named the\nstates rather than specifying the bit patterns to be used for each\nstate---for this reason, we refer to them as abstract states.\nThe description of the states in the first column is an optional element\noften included in the early design stages for an FSM, when identifying\nthe states needed for the design.\nA list may also include the outputs for each state.  Again, in\nthe list below, we have specified these outputs abstractly.\n\nBy including outputs for each state,\nwe implicitly assume that outputs depend only on\nthe state of the FSM.  We discuss this assumption in more detail\nlater in these notes (see ``Machine Models''), \nbut will make the assumption throughout our class.\n\n\n\n{\n\nmeaning& state& driver's door& other doors& alarm on \nvehicle locked& LOCKED& locked& locked& no\ndriver door unlocked& DRIVER& unlocked& locked& no\nall doors unlocked& UNLOCKED& unlocked& unlocked& no\nalarm sounding& ALARM& locked& locked& yes\n\n}\n\n\n\nAnother tool used with FSMs is the { next-state table} (sometimes\ncalled a { state transition table}, or just a { state table}), \nwhich maps the current state and input combination into the next state of \nthe FSM.  The abstract variant shown below outlines desired behavior at\na high level, and is often ambiguous, incomplete, and even inconsistent.\nFor example, what happens if a user pushes two buttons?  What happens\nif they push unlock while the alarm is sounding?  These questions \nshould eventually be considered.  However, we can already start to see \nthe intended use of the design: starting from a locked car, a user\ncan push ``unlock'' once to gain entry to the driver's seat, or push\n``unlock'' twice to open the car fully for passengers.  To lock the\ncar, a user can push the ``lock'' button at any time.  And, if a user\nneeds help, pressing the ``panic'' button sets off an alarm. \n\n\n\n{\n\nstate& action/input& next state \nLOCKED& push ``unlock''& DRIVER\nDRIVER& push ``unlock''& UNLOCKED\n(any)& push ``lock''& LOCKED\n(any)& push ``panic''& ALARM\n\n}\n\n\n\n\nA { state transition diagram} (or { transition diagram}, or\n{ state diagram}),\nas shown to the right, illustrates the\ncontents of the next-state table graphically, with each state drawn\nin a circle, and arcs between states labeled with the input combinations\nthat cause these transitions from one state to another.\n\nPutting the FSM design into this graphical form does not solve the\nproblems with the abstract model.  The questions that we asked\nin regard to the next-state table remain unanswered.\n\nImplementing an FSM using digital logic requires that we translate\nthe design into bits, eliminate any ambiguity, and complete the\nspecification.  How many internal bits should we use?  What are the\npossible input values, and how are their meanings represented in bits? \nWhat are the possible output values, and how are their meanings\nrepresented in bits?  We will consider these questions for several\nexamples in the coming weeks.\n\n\n{file=part3/figs/ke-abs-trans-diag.eps,width=3.45in}\n\n\nFor now, we simply define answers for our example design, the keyless\nentry system.  Given four states, we need at \nleast _2(4)=2 bits of internal state, which we\nstore in two flip-flops and call S_1S_0.  The table below lists input\nand output signals and defines their meaning. \n\n{\n\noutputs&D& driver door; 1 means unlocked\n&R& other doors (remaining doors); 1 means unlocked\n&A& alarm; 1 means alarm is sounding\ninputs& U& unlock button; 1 means it has been pressed\n& L& lock button; 1 means it has been pressed\n& P& panic button; 1 means it has been pressed\n\n}\n\nWe can now choose a representation for our states and rewrite the list\nof states, using bits both for the states and for the outputs.  We\nalso include the meaning of each state for clarity in our example.\nNote that we can choose the internal representation in any way.  Here\nwe have matched the D and R outputs when possible to simplify\nthe output logic needed for the implementation.  The order of states\nin the list is not particularly important, but should be chosen for\nconvenience and clarity (including transcribing bits into to {K-maps}, \nfor example).\n\n{\n\n& & & driver's door& other doors& alarm on\nmeaning& state& S_1S_0& D& R& A \nvehicle locked& LOCKED& 00& 0& 0& 0\ndriver door unlocked& DRIVER& 10& 1& 0& 0\nall doors unlocked& UNLOCKED& 11& 1& 1& 0\nalarm sounding& ALARM& 01& 0& 0& 1\n\n}\n\n\n\nWe can also rewrite the next-state table in terms of bits.  We use\nGray code order on both axes, as these orders make it more \nconvenient to use {K-maps}.  The values represented in this table \nare the next FSM state given the current state S_1S_0 and the\ninputs U, L, and P.  Our symbols for the next-state bits\nare S_1^+ and S_0^+.  The ``+'' superscript\nis a common way of expressing\nthe next value in a discrete series, here induced by the use of\nclocked synchronous logic in implementing the FSM.  In other words,\nS_1^+ is the value of S_1 in the next clock cycle, and S_1^+\nin an FSM implemented as a digital system is a Boolean expression\nbased on the current state and the inputs.  For our example problem, we\nwant to be able to write down expressions for S_1^+(S_1,S_0,U,L,P) \nand S_1^+(S_1,S_0,U,L,P), as well as expressions for the \noutput logic D(S_1,S_0), R(S_1,S_0), and A(S_1,S_0). \n\n\n{\n{c|cccccccc}\ncurrent state & {|c}{ULP}\nS_1S_0& 000& 001& 011& 010& 110& 111& 101& 100 \n00& 00& 01& 01& 00& 00& 01& 01& 10\n01& 01& 01& 01& 00& 00& 01& 01& 01\n11& 11& 01& 01& 00& 00& 01& 01& 11\n10& 10& 01& 01& 00& 00& 01& 01& 11 \n\n}\n\n\nIn the process of writing out the next-state table, \nwe have made decisions for all of the questions that we asked earlier\nregarding the abstract state table.  These decisions are also reflected\nin the complete state transition diagram shown to the right.\nThe states have been extended with state bits and output bits,\nas S_1S_0/DRA.\nYou should recognize that we can\nalso leave some questions unanswered by placing x's (don't cares)\ninto our table.  However, you should also understand at this point\nthat any implementation will produce bits, not x's, so we must be\ncareful not to allow arbitrary choices unless any of the choices\nallowed is indeed acceptable for our FSM's purpose.  We will discuss\nthis process and the considerations necessary as we cover more FSM\ndesign examples.\n\n\n{file=part3/figs/ke-trans-diag.eps,width=4.2in}\n\n\nWe have deliberately omitted calculation of expressions for the\nnext-state variables S_1^+ and S_0^+,\nand for the outputs D,\nR, and A.  We expect that you are able to do so from the \ndetailed state table above, and may assign such an exercise as\npart of your homework.\n\n\n \n Question: \nQ. What is a finite state machine? \n Answer: \n\nA finite state machine is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs."}, {"text": "Title: Synchronous Counters \n Text: {Synchronous Counters}\n\nA { counter} is a clocked sequential circuit with a state diagram\nconsisting of a single logical cycle.\n\nNot all counters are synchronous.\nIn other words, not all flip-flops in a counter are required to use \nthe same clock signal.  A counter in which all flip-flops do \nutilize the same clock\nsignal is called a { synchronous counter}.  \n\nExcept for a brief introduction to other types of counters in the\nnext section, our class focuses entirely on clocked synchronous designs,\nincluding counters.\n\n {The definition ignores transition\n arcs resulting from functionalit such as a reset signal that\n forces the counter back into an initial state, \n\n\nThe design of synchronous counter circuits is a fairly straightforward\nexercise given the desired cycle of output patterns. \n\nThe task can be more complex if the internal state bits\nare allowed to differ from the output bits, so for now\nwe assume that output Z_i is equal to internal bit S_i.\nNote that distinction between internal states and outputs is necessary \nif any output pattern appears more than once in the desired cycle. \n\nThe cycle of states shown to the right corresponds to the\nstates of a {3-bit} binary counter.  The numbers in the states\nrepresent both internal state bits S_2S_1S_0 and output bits Z_2Z_1Z_0.\nWe transcribe this diagram into the next-state table shown on the left below.\nWe then write out {K-maps} for\nthe next state bits S_2^+, S_1^+, and S_0^+, as shown to the right,\nand use the {K-maps} to find expressions for these variables in\nterms of the current state.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n{\n\n{ccc|ccc}\nS_2& S_1& S_0& S_2^+& S_1^+& S_0^+ \n0& 0& 0& 0& 0& 1\n0& 0& 1& 0& 1& 0\n0& 1& 0& 0& 1& 1\n0& 1& 1& 1& 0& 0\n1& 0& 0& 1& 0& 1\n1& 0& 1& 1& 1& 0\n1& 1& 0& 1& 1& 1\n1& 1& 1& 0& 0& 0\n\n\n\n{file=part3/figs/sbin3-s2.eps,width=1in}\n{file=part3/figs/sbin3-s1.eps,width=1in}\n{file=part3/figs/sbin3-s0.eps,width=1in} {-10pt}\n{eqnarray*}\nS_2^+ =& {S_2}S_1S_0+S_2{S_1}+S_2{S_0} &= S_2(S_1S_0) \nS_1^+ =& S_1{S_0}+{S_1}S_0 &= S_1{S_0} \nS_0^+ =& {S_0} &= S_0 \n{eqnarray*}\n\n}\n\nThe first form of the expression for each next-state variable is\ntaken directly from the corresponding {K-map}.  We have rewritten\neach expression to make the emerging pattern more obvious.\n\nWe can also derive the pattern intuitively by asking the following:\n\ngiven a binary counter in state S_{N-1}S_{N-2} S_{j+1}S_jS_{j-1}\nS_1S_0, when does S_j change in the subsequent state?  The answer,\nof course, is that S_j changes when all of the\nbits below S_j are 1.  Otherwise, S_j remains the\nsame in the next state.  We thus \nwrite S_j^+=S_j (S_{j-1} S_1S_0) and\nimplement the counter as shown below for a {4-bit} design.\nNote that the \nusual order of\noutput bits along the bottom is reversed in the figure, with the most\nsignificant bit at the right rather than the left.\n\n{{file=part3/figs/ser-gating.eps,width=4.5in}}\n\nThe calculation of the left inputs to the XOR gates in the counter \nshown above is performed with a series of two-input AND gates.  Each \nof these gates AND's\nanother flip-flop value into the product.  This\napproach, called { serial gating}, implies that an {N-bit}\ncounter requires more than {N-2} gate delays\nto settle into the next state.  An alternative approach, called\n{ parallel gating}, calculates each input independently with a\nsingle logic gate, as shown below.  The blue inputs to the AND gate\nfor S_3 highlight the difference from the previous figure (note that\nthe two approaches differ only for bits S_3 and above).\nWith parallel gating, the \n{ fan-in} of the gates (the number of inputs)\nand the { fan-out} of the flip-flop outputs (number of other gates\ninto which an output feeds) grow with the size of the counter.  \nIn practice, large counters use a combination of these two approaches.\n\n{{file=part3/figs/par-gating.eps,width=4.5in}}\n\n\n \n Question: \nQ. What is the main difference between a synchronous and an asynchronous counter? \n Answer: \n\nA. The main difference between a synchronous and an asynchronous counter is that all flip-flops in a synchronous counter are required to use the same clock signal, while this is not the case for an asynchronous counter."}, {"text": "Title: Ripple Counters \n Text: {Ripple Counters}\n\nA second class of\ncounter drives some of its flip-flops with a clock signal and feeds\nflip-flop outputs into the clock inputs of its remaining flip-flops,\npossibly through additional logic.  Such a counter is called a {\nripple counter}, because the effect of a clock edge ripples through\nthe flip-flops.  The delay inherent to the ripple effect, along with\nthe complexity of ensuring that timing issues do not render the design\nunreliable, are the major drawbacks of ripple counters.  Compared with\nsynchronous counters, however, ripple counters consume less energy,\nand are sometimes used for devices with restricted energy supplies.\n\n\nGeneral ripple counters\ncan be tricky because of timing issues, but certain types are easy.\n\nConsider the design of binary ripple counter.  The state diagram for \na {3-bit} binary counter is replicated to the right.\nLooking\nat the states, notice that the least-significant bit alternates with\neach state, while higher bits flip whenever the next smaller bit (to\nthe right) transitions from one to zero.  To take advantage of these\nproperties, we use positive edge-triggered D flip-flops with\ntheir complemented () outputs wired back to their inputs.\nThe clock input is fed only into the first\nflip-flop, and the complemented output of each flip-flop is also\nconnected to the clock of the next.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n\nAn implementation of a {4-bit} binary ripple counter appears to the\nright.\nThe order of bits in the figure matches the order used for our synchronous\nbinary counters: least significant on the left, most significant on the\nright.\nAs you can see from the figure, the technique generalizes to arbitrarily \nlarge binary ripple coun-\n\n\n\n\n\nters, but the\ntime required for the outputs to settle after a clock edge scales with\nthe number of flip-flops in the counter.  On the other hand, an\naverage of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n), which reduces the power requirements.{Recall that\nflip-flops record the clock state internally.  The logical activity\nrequired to record such state consumes energy.}\n\n\n\nBeginning with the state 0000, at the rising clock edge, the left (S_0)\nflip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\nfalling clock edge and does nothing, leaving the counter in\nstate 0001.  When the next rising clock edge arrives, the left\nflip-flop toggles back to 0, which the second flip-flop sees as a\nrising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\nsees the second flip-flop's change as a falling edge and does nothing,\nand the state settles as 0010.  We leave verification of the remainder \nof the cycle as an exercise.\n\n \n Question: \nQ. What is a ripple counter? \n Answer: \n\nA. A ripple counter is a type of digital counter which uses a clock signal to trigger the flip-flops and feed the outputs of those flip-flops back into the clock inputs of other flip-flops. This type of counter is called a ripple counter because the effect of a clock edge ripples through the flip-flops."}, {"text": "Title: Timing Issues* \n Text: {Timing Issues*}\n\nRipple counters are a form of a more general strategy known as clock\ngating.{Fall 2012 students: This part may seem a little\nredundant, but we're going to remove the earlier mention of clock \ngating in future semesters.}\n{ Clock gating} uses logic to control the visibility of a\nclock signal to flip-flops (or latches).  Historically, digital system\ndesigners rarely used clock gating techniques because of the \ncomplexity introduced for the circuit designers, who must ensure \nthat clock edges are\ndelivered with little skew along a dynamically changing set of\npaths to flip-flops.  Today, however, the power benefits of hiding\nthe clock signal from flip-flops have made clock gating an attractive \nstrategy.\nNevertheless, digital logic designers and computer architects still almost\nnever use clock gating strategies directly.  In most of the industry,\nCAD tools insert logic for clock gating automatically.  \nA handful of companies (such as Intel and Apple/Samsung) design \ncustom circuits rather than relying on CAD tools to synthesize \nhardware designs from standard libraries of elements.\nIn these companies, clock gating is used widely by the circuit\ndesign teams, and some input is occasionally necessary from the \nhigher-level designers.\n\nMore aggressive gating strategies are also used in modern designs, but\nthese usually require more time to transition between the on and off \nstates and can be more\ndifficult to get right automatically (with the tools), hence\nhardware designers may need to provide high-level information about\ntheir designs.  A flip-flop that does not see any change in its clock\ninput still has connections to high voltage and ground, and thus allows\na small amount of { leakage current}.  In contrast,\nwith { power gating}, the voltage difference is removed, and the \ncircuit uses no power at all.  Power gating can be tricky---as you know,\nfor example, when you turn the power on, you need to make sure that\neach latch settles into a stable state.  Latches may need to be \ninitialized to guarantee that they settle, which requires time after\nthe power is restored.\n\nIf you want a deeper understanding of gating issues, take ECE482, \nDigital Integrated Circuit Design, or ECE527, System-on-a-Chip Design.\n\n\n \n Question: \nQ. What is the difference between clock gating and power gating? \n Answer: \n\nA. Clock gating uses logic to control the visibility of a clock signal to flip-flops (or latches). Power gating uses logic to control the voltage difference between the flip-flops (or latches)."}, {"text": "Title: Machine Models \n Text: {Machine Models}\n\nBefore we dive fully into FSM design, we must point out that we have\nplaced a somewhat artificial restriction on the types of FSMs that\nwe use in our course.\n\nHistorically, this restriction was given a name, and machines of the type\nthat we have discussed are called Moore machines.\n\nHowever, outside of introductory classes, almost no one cares about\nthis name, nor about the name for the more general model used almost\nuniversally in hardware design, Mealy machines.\n\nWhat is the difference?  In a { Moore machine}, outputs depend only on\nthe internal state bits of the FSM (the values stored in the flip-flops).\nIn a { Mealy machine}, outputs\nmay be expressed as functions both of internal state and FSM inputs.\n\nAs we illustrate shortly, the benefit of using input signals to calculate\noutputs (the Mealy machine model) is that input bits effectively serve \nas additional system state, which means that the number of internal \nstate bits can be reduced.\n\nThe disadvantage of including input signals in the expressions for \noutput signals is that timing characteristics of input signals may not\nbe known, whereas an FSM designer may want to guarantee certain\ntiming characteristics for output signals.\n\nIn practice, when such timing guarantees are needed, the designer simply\nadds state to the FSM to accommodate the need, and the problem is solved.\n\nThe coin-counting FSM that we designed for our class' lab assignments,\nfor example, \nrequired that we use a Moore machine model to avoid sending the\nservo controlling the coin's path an output pulse that was too short\nto enforce the FSM's decision about which way to send the coin.\n\nBy adding more states to the FSM, we were able to hold the servo in\nplace, as desired.\n\nWhy are we protecting you from the model used in practice?\n\nFirst, timing issues add complexity to a topic that is complex enough \nfor an introductory course.\n\nAnd, second, most software FSMs are Moore machines, so the abstraction\nis a useful one in that context, too.\n\nIn many design contexts, the timing issues implied by a Mealy model\ncan be relatively simple to manage.  When working in a single clock\ndomain, all of the input signals come from flip-flops in the same \ndomain, and are thus stable for most of the clock cycle.  Only rarely\ndoes one need to keep additional state to improve timing characteristics\nin these contexts.  In contrast, when interacting across clock domains,\nmore care is sometimes needed to ensure correct behavior.\n\nWe now illustrate the state reduction benefit of the Mealy machine\nmodel with a simple example, an FSM that recognizes the \npattern of a 0 followed by a 1 on a single input and outputs\na 1 when it observes the pattern.\n\nAs already mentioned,\nMealy machines often require fewer flip-flops.\nIntuitively, the number of combinations of states and\ninputs is greater than the number of combinations of states alone, and\nallowing a function to depend on inputs reduces the number of internal\nstates needed.  \n\nA Mealy implementation of the FSM appears on the left below, and\nan example timing diagram illustrating the FSM's behavior is shown on\nthe right.\n\nThe machine shown below occupies state A when the last bit seen was a 0, and\nstate B when the last bit seen was a 1.\n\nNotice that the transition arcs in the state diagram\nare labeled with two values instead\nof one.  Since outputs can depend on input values as well as state,\ntransitions in a Mealy machine are labeled with input/output\ncombinations, while states are labeled only with their internal bits\n(or just their names, as shown below).  Labeling states with outputs\ndoes not make sense for a Mealy machine, since outputs may vary\nwith inputs.\n\nNotice that the outputs indicated on any given transition\nhold only until that transition is taken (at the rising clock edge), as is\napparent in the timing diagram.  When inputs are asynchronous, \nthat is,\nnot driven by the same clock signal, output pulses from a Mealy\nmachine can be arbitrarily short, which can lead to problems.\n\n\n\n{{file=part3/figs/lec17-3.eps,width=5in}}\n\n\n\nFor a Moore machine, we must create a special state in which the\noutput is high.  Doing so requires that we split state B into two\nstates, a state C in which the last two bits seen were 01, and a\nstate D in which the last two bits seen were 11.  Only state C\ngenerates output 1.  State D also becomes the starting state for the\nnew state machine.  The state diagram on the left below illustrates \nthe changes, using the transition diagram style that we introduced \nearlier to represent Moore machines.\nNotice in the associated timing diagram that the output pulse lasts a\nfull clock cycle.\n\n\n\n{{file=part3/figs/lec17-4.eps,width=5in}}\n\n\n\n\n\n \n Question: \nQ. Why is the Mealy machine model used almost universally in hardware design? \n Answer: \n\nA. There are several reasons for this. The main reason is that the Mealy machine model can be implemented with fewer flip-flops than the Moore machine model, which reduces the cost of hardware implementation. Additionally, the Mealy machine model is more flexible than the Moore machine model, and can be more easily adapted to changes in the input or output requirements. Finally, the Mealy machine model is more resistant to timing issues than the Moore machine model, which makes it more reliable in practice."}, {"text": "Title: Summary of Part 3 of the Course \n Text: {Summary of Part 3 of the Course}\n\n\nIn this short summary, we \ngive you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nStudents often find this part of the course more challenging than the\nearlier parts of the course.\n\nIn addition to these notes, you should read Chapters 4 and 5 of the \nPatt and Patel textbook, which cover the von Neumann\nmodel, instruction processing, and ISAs.  \n\nStudents typically find that the homeworks in this part of the course\nrequire more time than did those in earlier parts of the course.\nProblems on the exam will be similar in nature but designed to require \nless actual time to solve (assuming that you have been doing the homeworks).  \n\nWe'll start with the easy stuff.  \n\nYou should recognize all of these terms and be able\nto explain what they mean.  For the specific circuits, you should be able \nto draw them and explain how they work.  Actually, we don't care whether \nyou can draw something from memory---a mux, for example---provided that \nyou know what a mux does and can derive a gate diagram correctly for one \nin a few minutes.  Higher-level skills are much more valuable.\n\n[t]\n{}{{}{}\n{}{}{}\n\n{digital systems terms\n{--}{{}{}\n{}{}{}\n module\n fan-in\n fan-out\n machine models: Moore and Mealy\n\n}\n\n{simple state machines\n{--}{{}{}\n{}{}{}\n synchronous counter\n ripple counter\n serialization (of bit-sliced design)\n\n}\n\n{finite state machines (FSMs)\n{--}{{}{}\n{}{}{}\n states and state representation\n transition rule\n self-loop\n next state (+) notation\n meaning of don't care in input  combination\n meaning of don't care in output\n unused states and initialization\n completeness (with regard to  FSM specification)\n list of (abstract) states\n next-state table/state transition table/state table\n state transition diagram/transition  diagram/state diagram\n\n}\n\n{memory\n{--}{{}{}\n{}{}{}\n number of addresses\n addressability\n read/write logic\n serial/random access memory (RAM)\n volatile/non-volatile (N-V)\n static/dynamic RAM (SRAM/DRAM)\n SRAM cell\n DRAM cell\n design as a collection of cells\n coincident selection\n bit lines and sense amplifiers\n\n}\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann model\n{--}{{}{}\n{}{}{}\n{processing unit\n{--}{{}{}\n{}{}{}\n register file\n arithmetic logic unit (ALU)\n word size\n\n}\n{control unit\n{--}{{}{}\n{}{}{}\n program counter (PC)\n instruction register (IR)\n implementation as FSM\n\n}\n input and output units\n{memory\n{--}{{}{}\n{}{}{}\n memory address register (MAR)\n memory data register (MDR)\n\n}\n{processor datapath}\n\n{control signal}\n\n}\n\n{tri-state buffer\n{--}{{}{}\n{}{}{}\n meaning of Z/hi-Z output\n use in distributed mux\n\n}\n\n{instruction processing}\n{-}{{}{}\n{}{}{}\n\n\n\n{register transfer language (RTL)}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (of an encoded instruction)}\n{operation code (opcode)}\n{types of instructions}\n{-}{{}{}\n{}{}{}\n\n{data movement}\n{control flow}\n\n{addressing modes}\n{-}{{}{}\n{}{}{}\n\n\n{PC-relative}\n\n{base + offset}\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Transform a bit-sliced design into a serial design, and explain the \ntradeoffs involved in terms of area and time required to compute a result.}\n{Based on a transition diagram, implement a synchronous counter from \nflip-flops and logic gates.}\n{Implement a binary ripple counter (but not necessarily a more general \ntype of ripple counter) from flip-flops and logic gates.}\n{Given an FSM implemented as digital logic, analyze the FSM to produce \na state transition diagram.}\n{Design an FSM to meet an abstract specification for a task, including \nproduction of specified output signals, and possibly including selection \nof appropriate inputs.}\n{Complete the specification of an FSM by ensuring that each state \nincludes a transition rule for every possible input combination.}\n{Compose memory chips into larger memory systems, using additional\ndecoders when necessary.}\n{Encode {LC-3} instructions into machine code.}\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n\nAt a higher level, we expect that you understand the concepts and ideas \nsufficiently well to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Abstract design symmetries from an FSM specification in order to \nsimplify the implementation.}\n{Make use of a high-level state design, possibly with many sub-states \nin each high-level state, to simplify the implementation.}\n{Use counters to insert time-based transitions between states (such \nas timeouts).}\n{Implement an FSM using logic components such as registers, \ncounters, comparators, and adders as building blocks.}\n{Explain the basic organization of a computer's microarchitecture\nas well as the role played by elements of a von Neumann design in the\nprocessing of instructions.}\n{Identify the stages of processing an instruction (such as fetch,\ndecode, getting operands, execution, and writing back results) in a \nprocessor control unit state machine diagram.}\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the difference between the Moore and Mealy machine models, \nas well as why you might find each of them useful when designing an FSM.}\n{Understand the need for initialization of an FSM, be able to analyze \nand identify potential problems arising from lack of initialization, and \nbe able to extend an implementation to include initialization to an \nappropriate state when necessary.}\n{Understand how the choice of internal state bits for an FSM can \naffect the complexity of the implementation of next-state and output \nlogic, and be able to select a reasonable state assignment.}\n{Identify and fix design flaws in simple FSMs by analyzing an existing \nimplementation, comparing it with the specification, and removing any \ndifferences by making any necessary changes to the implementation.}\n\n\n\n\n\n{   }  empty 3rd page\n\n\n\n\n\n \n Question: \nQ. What is the difference between the Moore and Mealy machine models? \n Answer: \n\nA. The Moore and Mealy machine models differ in how they generate output signals. Moore machines generate outputs only on state transitions, while Mealy machines generate outputs on every clock cycle, based on the current input and state."}, {"text": "Title: Instruction Set Architecture* \n Text: {Instruction Set Architecture*}\n\nThis set of notes discusses \ntradeoffs and design elements of instruction set architectures (ISAs).\n{ The material is beyond the scope of our class, and is provided purely\nfor your interest.}  Those who find these topics interesting\nmay also want to read the ECE391 notes, which describe similar material\nwith a focus on the x86 ISA.\n\nAs you know, the ISA\ndefines the interface between software and hardware, abstracting the\ncapabilities of a computer's datapath and standardizing the format of\ninstructions to utilize those capabilities.  Successful ISAs are\nrarely discarded, as success implies the existence of large amounts of\nsoftware built to use the ISA.  Rather, they are extended, and their\noriginal forms must be supported for decades (consider, for example,\nthe IBM 360 and the Intel x86).  Employing sound design principles\nis thus imperative in an ISA.\n\n \n Question: \nQ. What are some design principles that are important to consider when designing an ISA? \n Answer: \n\nA. Some design principles that are important to consider when designing an ISA include:\n\n-Ease of use: the ISA should be easy for programmers to use and understand.\n\n-Efficiency: the ISA should be designed to be efficient in terms of both execution time and memory usage.\n\n-Compatibility: the ISA should be compatible with existing software and hardware."}, {"text": "Title: Formats and Fields* \n Text: {Formats and Fields*}\n\nThe LC-3 ISA \nemploys fixed-length instructions and a load-store architecture, two\naspects that help to reduce the design space to a manageable set of\nchoices.  In a general ISA design, many other options exist for\ninstruction formats.\n\nRecall the idea of separating the bits of an instruction into\n(possibly non-contiguous) fields.  One of the fields must contain an\nopcode, which specifies the type of operation to be performed by the\ninstruction.  In the {LC-3} ISA, most opcodes specify both\nthe type of operation and the types of arguments to the operation.  \nMore generally, many addressing modes are possible for\neach operand, and we can think of the bits\nthat specify the addressing mode as a separate field, known as the\n{ mode} field.  \n\nAs a simple example, the {LC-3's} ADD and AND instructions \ncontain a {1-bit} mode field that specifies whether the second\noperand of the ADD/AND comes from a register or is an immediate value.\n\nSeveral questions must be answered in order to define the possible\ninstruction formats for an ISA.  First, are instructions fixed-length\nor variable-length?  Second, how many addresses are needed for each\ninstruction, and how many of the addresses can be memory addresses?\nFinally, what forms of addresses are possible for each operand?\nFor example, can one use\nfull memory addresses or only limited offsets relative to a register?\n\nThe answer to the first question depends on many factors, but several\nclear advantages exist for both answers.  { Fixed-length\ninstructions} are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions.  As an example of this\nsimplification, note that the {LC-3} ISA does not support \naddressing for individual bytes, only for {16-bit} words.\n\n{ Variable-length instructions} also have benefits, however.\nVariable-length encodings allow more efficient encodings, saving both\nmemory and disk space.  A register transfer operation, for example,\nclearly requires fewer bits than addition of values at two direct\nmemory addresses for storage at a third.  Fixed-length instructions\nmust be fixed at the length of the longest possible instruction,\nwhereas variable-length instructions can use lengths appropriate to\neach mode.  The same tradeoff has another form in the sense that\nfixed-length ISAs typically eliminate many addressing modes in order\nto limit the size of the instructions.  Variable-length instructions\nthus allow more flexibility; indeed, extensions to a variable-length\nISA can incorporate new addressing modes that require longer\ninstructions without affecting the original ISA.\n\nFor example, the maximum length of x86 instructions has grown from six \nbytes in 1978 (the 8086 ISA) to fifteen bytes in today's version of the\nISA.\n\n\n\n\nMoving to the last of the three questions posed for instruction format\ndefinition, operand address specification,\nwe explore a range of answers developed over the last few\ndecades.  Answers are usually chosen based on the number of bits\nnecessary, and we use this metric to organize the possibilities.  The\nfigure below separates approaches into two dimensions: the vertical\ndimension divides addressing into registers and memory, and the\nhorizontal dimension into varieties within each type.\n\n{{file=part4/figs/lec23-1.eps,width=4in}}\n\nAs a register file contains fewer registers than a memory does words,\nthe use of register operands rather than memory addresses reduces the\nnumber of bits required to specify an operand.  The {LC-3} ISA\nuses a restricted set of addressing modes to stay within the limit\nimposed by the use of {16-bit} instructions.  Both\nregister and memory addresses, however, admit a wide range of\nimplementations.\n\n{ Implicit operands} of either type require no additional bits for\nthe implicit address.  The {LC-3} procedure call instruction, JSR,\nfor example, stores the return address in R7.  No bits in the JSR \nencoding name the R7 register; R7 is used implicitly for every JSR\nexecuted.\n\nSimilarly, the procedure call instructions in many ISAs push the \nreturn address onto a stack using an implicit register for the top \nof stack pointer.\n\nMemory addresses can also be implicitly equated to other memory\naddresses.  An increment instruction operating on a memory address, for\nexample, implicitly writes the result back to the same address.  \n\nAt the opposite extreme, an instruction may include a full address,\neither to any register in the register file or to any address in the \nmemory.  The term { general-purpose registers} indicates that \nregisters are used in any operation.\n\n{ Special-purpose registers}, in contrast, split the register file\nand allow only certain registers to be used in each operation.  For\nexample, the Motorola 680x0 series, used in early Apple\nMacintosh computers, provides distinct sets of address and data\nregisters.  Loads and stores use the address registers; arithmetic,\nlogic, and shift operations use the data registers.  As a result, \neach instruction\nselects from a smaller set of registers and thus requires fewer bits\nin the instruction to name the register for use.\n\nAs full memory addresses require many more bits than full register\naddresses, a wider range of techniques has been employed to reduce the\nlength.  ``Zero page'' addresses, as defined in the 6510 (6502) ISA\nused by Commodore PET's,{My computer in junior high school.}\nC64's,{My computer in high school.} and VIC 20's, prefixed a\none-byte address with a zero byte, allowing shorter instructions when\nmemory addresses fell within the first 256 memory locations.  Assembly\nand machine language programmers made heavy use of these locations to\nproduce shorter programs.  \n\nRelative addressing is quite common in the {LC-3} ISA, in which many\naddresses are PC-relative.  Typical commerical ISAs also make use of\nrelative addressing.  The Alpha ISA, for example, has a PC-relative form of\nprocedure call with a {21-bit} offset (plus or minus a megabyte),\nand the x86 ISA has a ``short'' form of branch instructions that\nuses an {8-bit} offset.\n\nSegmented memory is a form of relative addressing that uses a register\n(usually implicit) to provide the high bits of an address and an\nexplicit memory address (or another register) to provide the low bits.\nIn the early x86 ISAs, for example, {20-bit} addresses are\nfound by adding a {16-bit} segment register extended with four\nzero bits to a {16-bit} offset.\n\n\n\n\n\n \n Question: \nQ. What are the benefits of using fixed-length instructions? \n Answer: \n\nA. Fixed-length instructions are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions."}, {"text": "Title: Addressing Architectures* \n Text: {Addressing Architectures*}\n\nOne question remains for the definition of instruction formats: how\nmany addresses are needed for each instruction, and how many of the\naddresses can be memory addresses?  The first part of this question\nusually ranges from zero to three, and is rarely allowed to go\nbeyond three.  The answer to the second part determines the {\naddressing architecture} implemented by an ISA.  We now illustrate the\ntradeoffs between five distinct addressing architectures through the\nuse of a running example, the assignment X=AB+C/D.\n\nA binary operator requires two source operands and one destination\noperand, for a total of three addresses.  The ADD instruction, for\nexample, has a { {3-address}} format:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B,C>; M[A]  M[B] + M[C]\n>or>ADD>R1,R2,R3>; R1  R2 + R3\n{-6pt}\n\nIf all three addresses can be memory addresses, the ISA is dubbed a\n{ memory-to-memory architecture}.  Such architectures may have\nsmall register sets or even lack a register file completely.  To\nimplement the assignment, we assume the availability of two memory\nlocations, T1 and T2, for temporary storage:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>MUL>T1,A,B>; T1  M[A] * M[B]\n>DIV>T2,C,D>; T2  M[C] / M[D]\n>ADD>X,T1,T2>; X  M[T1] + M[T2]\n{-6pt}\n\nThe assignment requires only three instructions to implement, but each\ninstruction contains three full memory addresses, and is thus quite long.\n\nAt the other extreme is the { load-store architecture} used by the\n{LC-3} ISA.  In a load-store architecture, only\nloads and stores can use memory addresses; all other operations use\nonly registers.  As most instructions use only registers, this type of\naddressing architecture is also called a { register-to-register\narchitecture}.  The example assignment translates to the code shown below,\nwhich assumes that R1, R2, and R3 are free for use (the instructions\nare { NOT} {LC-3} instructions, but rather a generic assembly\nlanguage for a load-store architecture). \n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>LD>R2,B>; R2  M[B]\n>MUL>R1,R1,R2>; R1  R1 * R2\n>LD>R2,C>; R2  M[C]\n>LD>R3,D>; R3  M[D]\n>DIV>R2,R2,R3>; R2  R2 / R3\n>ADD>R1,R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nEight instructions are necessary, but no instruction requires more\nthan one full memory address, and several use only register addresses,\nallowing the use of shorter instructions.  The need to move data in\nand out of memory explicitly, however, also requires a reasonably\nlarge register set, as is available in the ARM, Sparc, Alpha, and IA-64\nISAs.  \n\nArchitectures that use other combinations of memory and register\naddresses with {3-address} formats are not named.  Unary\noperators and transfer operators require only one source operand, thus\ncan use a {2-address} format (for example, NOT A,B).  Binary operations\ncan also use { {2-address}} format if one operand is implicit,\nas in the following instructions:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B>; M[A]  M[A] + M[B]\n>or>ADD>R1,B>; R1  R1 + M[B]\n{-6pt}\n\nThe second instruction, in which one address is a register and the\nsecond is a memory address, defines a { register-memory\narchitecture}.  As shown by the code on the next page, \nsuch architectures strike a balance\nbetween the two architectures just discussed.\n\n\n\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>MUL>R1,B>; R1  R1 * M[B]\n>LD>R2,C>; R2  M[C]\n>DIV>R2,D>; R2  R2 / M[D]\n>ADD>R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nThe assignment now requires six instructions using at most one memory\naddress each; like memory-to-memory architectures, register-memory\narchitectures use relatively few registers.  Note that two-register\noperations are also allowed.  Intel's x86 ISA is a register-memory\narchitecture.\n\nSeveral ISAs of the past{The 6510/6502 as well, if memory\nserves, as the 8080, Z80, and Z8000, which used to drive parlor video\ngames.}  used a special-purpose register called the accumulator for\nALU operations, and are called { accumulator architectures}.  The\naccumulator in such architectures is implicitly both a source and the\ndestination for any such operation, allowing a { {1-address}}\nformat for instructions, as shown below.\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>B>; ACC  ACC + M[B]\n>or>ST>E>; M[E]  ACC\n{-6pt}\n\nAccumulator architectures strike the same balance as register-memory\narchitectures, but use fewer registers.  Note that memory location X\nis used as a temporary storage location as well as the final storage\nlocation in the following code:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>A>; ACC  M[A]\n>MUL>B>; ACC  ACC * M[B]\n>ST>X>; M[X]  ACC\n>LD>C>; ACC  M[C]\n>DIV>D>; ACC  ACC / M[D]\n>ADD>X>; ACC  ACC + M[X]\n>ST>X>; M[X]  ACC\n{-6pt}\n\nThe last addressing architecture that we discuss is rarely used for\nmodern general-purpose processors, but may be familiar to\nyou because of its historical use in scientific and engineering calculators.\n\nA { stack architecture}\nmaintains a stack of values and draws all ALU operands from this\nstack, allowing these instructions to use a { {0-address}}\nformat.  A special-purpose stack pointer (SP) register points to the\ntop of the stack in memory, and operations analogous to load ({\npush}) and store ({ pop}) are provided to move values on and off\nthe stack.  To implement our example assignment, we first transform it\ninto postfix notation (also called reverse Polish notation):\n\n{-6pt}\n=\n>A  B  *  C  D  /  +\n{-6pt}\n\nThe resulting sequence of symbols transforms on a one-to-one basis\ninto instructions for a stack architecture:\n\n{-6pt}\n=WWWW=WW=; M[SP+1]  M[SP] + M[SP + 1], SP  SP + 1WW=AB+C/DW=AB+C/DW=\n>PUSH>A>; SP  SP - 1, M[SP]  M[A]>A\n>PUSH>B>; SP  SP - 1, M[SP]  M[B]>B>A\n>MUL>>; M[SP+1]  M[SP+1] * M[SP], SP  SP + 1>AB\n>PUSH>C>; SP  SP - 1, M[SP]  M[C]>C>AB\n>PUSH>D>; SP  SP - 1, M[SP]  M[D]>D>C>AB\n>DIV>>; M[SP+1]  M[SP+1] / M[SP], SP  SP + 1>C/D>AB\n>ADD>>; M[SP+1]  M[SP+1] + M[SP], SP  SP + 1>AB+C/D\n>POP>X>; M[X]  M[SP], SP  SP + 1\n{-6pt}\n\nThe values to the right are the values on the stack, starting with the\ntop value on the left and progressing downwards, { after the\ncompletion of each instruction}.\n\n\n\n\n\n \n Question: \nQ. What is the main difference between a memory-to-memory architecture and a load-store architecture? \n Answer: \n\n\nA. In a memory-to-memory architecture, all operations can use memory addresses, whereas in a load-store architecture, only loads and stores can use memory addresses."}, {"text": "Title: Common Special-Purpose Registers* \n Text: {Common Special-Purpose Registers*}\n\nThis section illustrates the uses of special-purpose registers through\na few examples. \n\nThe { stack pointer (SP)} points to the top of the stack in memory.\nMost older architectures support push and pop operations that\nimplicitly use the stack pointer.  Modern architectures assign a\ngeneral-purpose register to be the stack pointer and reference it\nexplicitly, although an assembler may support\ninstructions that appear to use implicit operands but in fact\ntranslate to machine instructions with explicit reference to the\nregister defined to be the SP.\n\nThe { program counter (PC)} points to the next instruction to be\nexecuted.  Some modern architectures expose it as a general-purpose\nregister, although its distinct role in the implementation keeps such\na model from becoming as common as the use of a general-purpose\nregister for the SP.\n\nThe { processor status register (PSR)}, also known as the {\nprocessor status word (PSW)}, contains all status bits as well as a\nmode bit indicating whether the processor is operating in user mode or\nprivileged (operating system) mode.  Having a register with this\ninformation allows more general access than is possible solely through\nthe use of control flow instructions.\n\nThe { zero register} appears in modern architectures of the RISC\nvariety (defined in the next section of these notes).  The register is\nread-only and serves both as a useful constant and as a destination\nfor operations performed only for their side-effects (for example, setting\nstatus bits).  The availability of a zero register also allows certain\nopcodes to serve double duty.  A register-to-register add instruction\nbecomes a register move instruction when one source operand is zero.\nSimilarly, an immediate add instruction becomes an immediate load\ninstruction when one source operand is zero.\n\n\n \n Question: \nQ. What is the purpose of the zero register? \n Answer: \n\nA. The zero register is used as a constant and as a destination for operations performed only for their side-effects."}, {"text": "Title: Reduced Instruction Set Computers* \n Text: {Reduced Instruction Set Computers*}\n\nBy the mid-1980's, the VAX architecture dominated the workstation and\nminicomputer markets, which included most universities.  Digital\nEquipment Corporation, the creator of the VAX, was second only to IBM\nin terms of computer sales.  VAXen, as the machines were called, used\nmicroprogrammed control units and supported numerous addressing modes\nas well as complex instructions ranging from ``square root'' to\n``find roots of polynomial equation.''\n\nThe impact of increasingly dense integrated circuit technology had\nbegun to have its effect, however, and in view of increasing processor\nclock speeds, more and more programmers were using high-level\nlanguages rather than writing assembly code.  Although assembly\nprogrammers often made use of the complex VAX instructions, compilers\nwere usually unable to recognize the corresponding high-level language\nconstructs and thus were unable to make use of the instructions.\n\nIncreasing density also led to rapid growth in memory sizes, to the\npoint that researchers began to question the need for variable-length\ninstructions.  Recall that variable-length instructions allow shorter\ncodes by providing more efficient instruction encodings.  With the\ntrend toward larger memories, code length was less important.  The\nperformance advantage of fixed-length instructions, which simplifies\nthe datapath and enables pipelining, on the other hand, was \nattractive.\n\nResearchers leveraged these ideas, which had been floating around the\nresearch community (and had appeared in some commercial architectures)\nto create { reduced instruction set computers}, or { RISC}\nmachines.  The competing VAXen were labeled { CISC} machines, which\nstands for { complex instruction set computers}.\n\nRISC machines employ fixed-length instructions and a load-store\narchitecture, allowing only a few addressing modes and small offsets.\nThis combination of design decisions enables deep pipelines and\nmultiple instruction issues in a single cycle (termed superscalar\nimplementations), and for years, RISC machines were viewed by many\nresearchers as the proper design for future ISAs.  However, companies\nsuch as Intel soon learned to pipeline microoperations after decoding\ninstructions, and CISC architectures now offer competitive if not\nsuperior performance in comparison with RISC machines.  The VAXen are\ndead, of course,{Unless you talk with customer support\nemployees, for whom no machine ever dies.}  having been replaced by\nthe Alpha, which in turn fell to x86, which is now struggling with ARM\nto enter the mobile market.  \n\n\n \n Question: \nQ. What are some of the reasons that RISC machines were viewed as the proper design for future ISAs? \n Answer: \n\nA. RISC machines were viewed as the proper design for future ISAs because they employed fixed-length instructions and a load-store architecture, which allowed only a few addressing modes and small offsets. This combination of design decisions enabled deep pipelines and multiple instruction issues in a single cycle (termed superscalar implementations), and for years, RISC machines were viewed by many researchers as the proper design for future ISAs."}, {"text": "Title: Procedure and System Calls* \n Text: {Procedure and System Calls*}\n\nA { procedure} is a sequence of instructions that executes a\nparticular task.  Procedures are used as building blocks for multiple,\nlarger tasks.  The concept of a procedure is fundamental to\nprogramming, and appears in some form in every high-level language as\nwell as in most ISAs.\n\nFor our purposes, the terms procedure, subroutine,\nfunction, and method are synonymous, although they usually have\nslightly different meanings from the linguistic point of view.\nProcedure calls are supported through { call} and { return}\ncontrol flow instructions.  The first instruction in the code below,\nfor example, transfers control to the procedure ``DoSomeWork,'' which\npresumably does some work, then returns control to the instruction\nfollowing the call.\n\n{-6pt}\n\n=DoSomeWork:WW=WWWW=DoSomeWorkWW= \n>loop:>CALL>DoSomeWork\n>>CMP>R6,#1>; compare return value in R6 to 1\n>>BEQ>loop>; keep doing work until R6 is not 1\n\n>DoSomeWork:>>> ; set R6 to 0 when all work is done, 1 otherwise \n>>RETN\n\n{-6pt}\n\nThe procedure also places a return value in R6, which the instruction\nfollowing the call compares with immediate value 1.  Until the two are\nnot equal (when all work is done), the branch returns control to the\ncall and executes the procedure again.\n\nAs you may recall, the call and return use the stack pointer to keep\ntrack of nested calls.  Sample RTL for these operations appears below.\n\n\n{eqnarray*}\n{call RTL}&&SP  SP - 1\n&& M[SP]  PC\n&& PC  {procedure start}\n{eqnarray*}\n\n\n{eqnarray*}\n{return RTL}&&PC  M[SP]\n&&SP  SP + 1\n{eqnarray*}\n\n\nWhile an ISA provides the call and return instructions necessary to\nsupport procedures, it does not specify how information is passed to\nor returned from a procedure.  A standard for such decisions is\nusually developed and included in descriptions of the architecture,\nhowever.  This { calling convention} specifies how information is\npassed between a caller and a callee.  In particular, it specifies the\nfollowing: where arguments must be placed, either in registers or in\nspecific stack memory locations; which registers can be used or\nchanged by the procedure; and where any return value must be placed.\n\nThe term ``calling convention'' is also used in the programming\nlanguage community to describe the convention for deciding what\ninformation is passed for a given call operation.  For example, are\nvariables passed by value, by pointers to values, or in some other\nway?  However, once the things to be sent are decided, the\narchitectural calling convention that we discuss here is used\nto determine where to put the data in order for the callee to be able\nto find it.\n\nCalling conventions for architectures with large register sets\ntypically pass arguments in registers, and nearly all conventions\nplace the return value in a register.  A calling convention also\ndivides the register set into { caller-saved} and \n{ callee-saved} registers.  Caller-saved registers can be modified \narbitrarily\nby the called procedure, whereas any value in a callee-saved register\nmust be preserved.  Similarly, before calling a procedure, a caller\nmust preserve the values of any caller saved registers that are needed\nafter the call.  Registers of both types usually saved on the stack by\nthe appropriate code (caller or callee).\n\n\n\n\n\nA typical stack structure appears in the figure to the right.  In\npreparation for a call, a caller first stores any caller-saved\nregisters on the stack.  Arguments to the procedure to be called are\npushed next.  The procedure is called next, implicitly pushing the\nreturn address (the address of the instruction following the call\ninstruction).  Finally, the called procedure may allocate space on the\nstack for storage of callee-saved registers as well as local\nvariables.\n\nAs an example, the following calling convention can be applied to \nan {8-register} load-store architecture similar to the {LC-3}\nISA: the first three arguments must be placed in R0 through R2 (in order), \nwith any remaining arguments on the stack; the return value must be placed \nin R6; R0 through R2 are caller-saved, as\nis R6, while R3 through R5 are callee-saved; R7 is used as the stack\npointer.  The code fragments below use this calling convention to\nimplement a procedure and a call of that procedure.\n\n\n{file=part4/figs/lec23-2.eps,width=1.25in}\n\n\n[t]\n{\n[t]\n\nint =add3 (int n1, int n2, int n3) {\n>return (n1 + n2 + n3);\n}\n\nprintf (``d'', add3 (10, 20, 30));\n\n\n\nby convention:\n= n1 is in R0\n>n2 is in R1\n>n3 is in R2\n>return value is in R6\n\n\n\n[t]\n\nadd3:  = WWWW= WWWWW= \nadd3:>ADD>R0,R0,R1\n>ADD>R6,R0,R2\n>RETN\n>\n>PUSH>R1>; save the value in R1\n>LDI>R0,#10>; marshal arguments\n>LDI>R1,#20\n>LDI>R2,#30\n>CALL>add3\n>MOV>R1,R6>; return value becomes 2nd argument\n>LDI>R0,``d''>; load a pointer to the string\n>CALL>printf\n>POP>R1>; restore R1\n\n\n\nThe add3 procedure takes three integers as arguments, adds them\ntogether, and returns the sum.  The procedure is called with the\nconstants 10, 20, and 30, and the result is printed.  By the calling\nconvention, when the call is made, R0 must contain the value 10, R1\nthe value 20, and R2 the value 30.  We assume that the caller wants to\npreserve the value of R1, but does not care about R3 or R5.  In the\nassembly language version on the right, R1 is first saved to the\nstack, then the arguments are marshaled into position, and finally the\ncall is made.  The procedure itself needs no local storage and does\nnot change any callee-saved registers, thus must simply add the\nnumbers together and place the result in R6.  After add3 returns, its\nreturn value is moved from R6 to R1 in preparation for the call to\nprintf.  After loading a pointer to the format string into R0, the\nsecond call is made, and R1 is restored, completing the translation.\n\n{ System calls} are almost identical to procedure calls.  As with\nprocedure calls, a calling convention is used: before invoking a\nsystem call, arguments are marshaled into the appropriate registers or\nlocations in the stack; after a system call returns, any result\nappears in a pre-specified register.  The calling convention used for\nsystem calls need not be the same as that used for procedure calls.\nRather than a call instruction, system calls are usually initiated\nwith a { trap} instruction, and system calls are also known as\ntraps.  With many architectures, a system call places the processor in\nprivileged or kernel mode, and the instructions that implement the\ncall are considered to be part of the operating system.  The term\nsystem call arises from this fact.\n\n\n\n\n\n \n Question: \nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t] \n Answer: \nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. ["}, {"text": "Title: Interrupts and Exceptions* \n Text: {Interrupts and Exceptions*}\n\nUnexpected processor interruptions arise both from interactions\nbetween a processor and external devices and from errors or unexpected\nbehavior in the program being executed.  The term { interrupt} is\nreserved for asynchronous interruptions generated by other devices,\nincluding disk drives, printers, network cards, video cards,\nkeyboards, mice, and any number of other possibilities.  {\nExceptions} occur when a processor encounters an unexpected opcode or\noperand.  An undefined instruction, for example, gives rise to an\nexception, as does an attempt to divide by zero.  Exceptions usually\ncause the current program to terminate, although many operating\nsystems will allow the program to catch the exception and to handle it\nmore intelligently.  The table below summarizes the characteristics of\nthe two types and compares them to system calls.\n\n{\n{|l|l|l|c|c|}\n{|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected \ninterrupt& external device& packet arrived at network card& yes& yes\nexception& invalid opcode or operand& divide by zero& no& yes\ntrap/system call& deliberate, via trap instruction& print character to console& no& no \n\n}\n\nInterrupts occur asynchronously with respect to the program.  Most\ndesigns only recognize interrupts between instructions.  In other words, \nthe presence of interrupts is checked only after completing an instruction\nrather than in every cycle.  In pipelined designs, however,\ninstructions execute simultaneously, and the decision as to which\ninstructions occur ``before'' an interrupt and which occur ``after''\nmust be made by the processor.  Exceptions are not asynchronous in the\nsense that they occur for a particular instruction, thus no decision\nneed be made as to instruction ordering.  After determining which\ninstructions were before an interrupt, a pipelined processor discards\nthe state of any partially executed instructions that occur ``after''\nthe interrupt and completes all instructions that occur ``before.''\nThe terminated instructions are simply restarted after the interrupt\ncompletes.  Handling the decision, the termination, and the\ncompletion, however, significantly increases the design complexity of\nthe system.\n\nThe code associated with an interrupt, an exception, or a system call\nis a form of procedure called a { handler}, and is found by looking\nup the interrupt number, exception number, or trap number in a table\nof functions called a { vector table}.  Vector tables\nfor each type (interrupts, exceptions, and system calls) may be separate,\nor may be combined into a single table.\nInterrupts and exceptions share a need to save all registers and\nstatus bits before execution of the corresponding handler code (and to\nrestore those values afterward).  Generally, the\n{values---including} the status word {register---are} placed\non the stack.  With system calls, saving and restoring any necessary\nstate is part of the calling convention.  A special return from\ninterrupt instruction is used to return control from the interrupt\nhandler to the interrupted code; a similar instruction forces the\nprocessor back into user mode when returning from a system call.\n\nInterrupts are also interesting in the sense that typical computers\noften have many interrupt-generating devices but only a few\ninterrupts.  Interrupts are prioritized by number, and only an\ninterrupt with higher priority can interrupt another interrupt.\nInterrupts with equal or lower priority are blocked while an interrupt\nexecutes.  Some interrupts can also be blocked in some architectures\nby setting bits in a special-purpose register called an interrupt\nmask.  While an interrupt number is masked, interrupts of that type\nare blocked, and can not occur.\n\nAs several devices may generate interrupts with the same interrupt\nnumber, interrupt handlers can be { chained} together.  Each\nhandler corresponds to a particular device.  When an interrupt occurs,\ncontrol is passed to the handler for the first device, which accesses\ndevice registers to determine whether or not that device generated an\ninterrupt.  If it did, the appropriate service is provided.  If not,\nor after the service is complete, control is passed to the next\nhandler in the chain, which handles interrupts from the second device,\nand so forth until the last handler in the chain completes.  At this\npoint, registers and processor state are restored and control is\nreturned to the point at which the interrupt occurred.\n\n\n\n\n\n \n Question: \nQ. What is an interrupt? \n Answer: \n\nAn interrupt is an asynchronous signal from an external device indicating that it requires attention."}, {"text": "Title: Control Flow Conditions* \n Text: {Control Flow Conditions*}\n\nControl flow instructions may change the PC, loading it with an\naddress specified by the instruction.  Although any addressing mode\ncan be supported, the most common specify an address directly in the\ninstruction, use a register as an address, or use an address relative\nto a register.  \n\nUnconditional control flow instructions typically provided by an ISA\ninclude procedure calls and returns, traps, and jumps.  Conditional\ncontrol flow instructions are branches, and are logically based on\nstatus bits set by two types of instructions: { comparisons} and\n{ bit tests}.  Comparisons subtract one value from another to set\nthe status bits, whereas bit tests use an AND operation to\ncheck whether certain bits are set or not in a value.\n\nMany ISAs implement\nstatus bits as special-purpose registers and implicitly set them when\nexecuting\ncertain instructions.  A branch based on R2 being less or equal to R3\ncan then be written as shown below.  The status bits are set by\nsubtracting R3 from R2 with the ALU.\n\n\n=WWWWW=WW,WW,WW=\n>CMP>R2,R3>; R2 < R3: CNZ  110, R2 = R3: CNZ  001,\n>>>;     R2 > R3: CNZ  000\n>BLE>R1>; Z  C = 1: PC  R1\n{-6pt}\n\nThe status bits are not always implemented as special-purpose\nregisters; instead, they may be kept in general-purpose registers or\nnot kept at all.  For example, the Alpha ISA stores the results of\ncomparisons in general-purpose registers, and the same branch is\ninstead implemented as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>CMPLE>R4,R2,R3>; R2  R3: R4  1, R2 > R3: R4  0\n>BNE>R4,R1>; R4  0: PC  R1\n{-6pt}\n\nFinally, status bits can be calculated, used, and discarded within\na single instruction, in which case the branch is written as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>BLE>R1,R2,R3>; R2  R3: PC  R1\n{-6pt}\n\nThe three approaches have advantages and disadvantages similar to\nthose discussed in the section on addressing architectures: the first\nhas the shortest instructions, the second is the most general and\nsimplest to implement, and the third requires the fewest instructions.\n\n\n \n Question: \nQ. What are the advantages and disadvantages of using special-purpose registers to store status bits? \n Answer: \n\nA. The advantage of using special-purpose registers to store status bits is that it is the shortest instruction. The disadvantage is that it is the most general and simplest to implement."}, {"text": "Title: Stack Operations* \n Text: {Stack Operations*}\n\nTwo types of stack operations are commonly supported.  Push and pop\nare the basic operations in many older architectures, and values can\nbe placed upon or removed from the stack using these instructions.  In\nmore modern architectures, in which the SP becomes a general-purpose\nregister, push and pop are replaced with indexed loads and stores,\nthat is, loads and stores using the stack pointer and an offset as the\naddress for the memory operation.  Stack updates are performed using\nthe ALU, subtracting and adding immediate values from the SP as\nnecessary to allocate and deallocate local storage.\n\nStack operations serve three purposes in a typical architecture.  The\nfirst is to support procedure calls, as illustrated in a previous\nsection.  The second is to provide temporary storage during\ninterrupts, which was also mentioned earlier.  \n\nThe third use of stack operations is to support { spill code}\ngenerated by compilers.  Compilers first translate high-level\nlanguages into an intermediate representation much like assembly code\nbut with an extremely large (theoretically infinite) register set.\nThe final translation step translates this intermediate representation\ninto assembly code for the target architecture, assigning\narchitectural registers as necessary.  However, as real ISAs support\nonly a finite number of registers, the compiler must occasionally\nspill values into memory.  For example, if ten values are in use at\nsome point in the code, but the architecture has only eight registers,\nspill code must be generated to store the remaining two values on the\nstack and to restore them when they are needed.\n\n\n \n Question: \nQ. How does stack spill code work? \n Answer: \n\nA. When the compiler encounters a situation where there are more values in use than there are registers available, it generates code to store the excess values on the stack and to restore them when they are needed."}, {"text": "Title: I/O* \n Text: {I/O*}\n\nAs a final topic, we now consider how a processor\nconnects to other devices to allow input and output.  We have already\ndiscussed interrupts, which are a special form of I/O in which only\nthe signal requesting attention is conveyed to the processor.\nCommunication of data occurs through instructions similar to loads and\nstores.  A processor is designed with a number of {{ I/O\nports}---usually} read-only or write-only registers to which devices\ncan be attached with opposite semantics.  That is, a port is usually\nwritten by the processor and read by a device or written by a device\nand read by the processor.\n\nThe question of exactly how I/O ports are accessed is an interesting\none.  One option is to create special instructions, such as the {\nin} and { out} instructions of the x86 architecture.  Port\naddresses can then be specified in the same way that memory addresses\nare specified, but use a distinct address space.  Just as two sets of\nspecial-purpose registers can be separated by the ISA, such an {\nindependent I/O} system separates I/O ports from memory addresses by\nusing distinct instructions for each class of operation.\n\nAlternatively, device registers can be accessed using the same load and store\ninstructions as are used to access memory.  This approach, known as\n{ memory-mapped I/O}, requires no new instructions for I/O, but\ndemands that a region of the memory address space be set aside for\nI/O.  The memory words with those addresses, if they exist, can not be\naccessed during normal processor operations.\n\n\n\n\n\n \n Question: \nQ. What is the difference between independent I/O and memory-mapped I/O? \n Answer: \n\n\n\nA. Independent I/O uses separate instructions to access I/O ports, while memory-mapped I/O uses the same load and store instructions as are used to access memory."}, {"text": "Title: Control Unit Design \n Text: {Control Unit Design}\n\nAppendix C of the Patt and Patel textbook describes a microarchitecture for\nthe LC-3 ISA, including a control unit implementation.\n\nIn this set of notes, we introduce a few concepts and strategies for\ncontrol unit design, using the textbook's {LC-3} microarchitecture \nto help illustrate them.  Several figures from the textbook are\nreproduced with permission in these notes as an aid to understanding.\n\nThe control unit of a computer based on the von Neumann model can be viewed\nas an FSM that fetches instructions from memory and executes them.  Many\npossible implementations exist both for the control unit itself and for the\nresources that it controls, the other components in the von Neumann model,\nwhich we collectively call the { datapath}.\n\nIn this set of notes, we discuss two strategies for structured control\nunit design and introduce the idea of using memories to encode logic \nfunctions.\n\nLet's begin by recalling that the control unit consists of three parts: a \nhigh-level FSM that controls instruction processing, a program counter (PC)\nregister that holds the address of the next instruction to be executed,\nand an instruction register (IR) that holds the current instruction as\nit executes.\n\nOther von Neumann components provide inputs to the control unit.  The\nmemory unit, for example, contains the instructions and data on which \nthe program executes.  \n\nThe processing unit contains a register file and condition \ncodes (N, Z, and P for the {LC-3} ISA).  \n\nThe outputs of the control unit are signals that control operation of the\ndatapath: the processing unit, the memory, and the I/O interfaces.\n\nThe basic problem that we must solve, then, for control unit design, is to \nmap instruction processing and the state of the FSM (including the PC and\nthe IR) into appropriate sequences of { control signals} for the datapath.\n\n \n Question: \nQ. What are the inputs to the control unit? \n Answer: \n\nA. The inputs to the control unit are the instructions and data from memory, and the condition codes from the processing unit."}, {"text": "Title: LC-3 Datapath Control Signals \n Text: {LC-3 Datapath Control Signals}\n\nAs we have skipped over the implementation details of interrupts and \nprivilege of the {LC-3} in our class, let's consider a \nmicroarchitecture and datapath without those capabilities.  \n\nThe figure on the next page (Patt and Patel Figure C.3) shows an {LC-3}\ndatapath and control signals without support for interrupts and privilege.\n\n\nSome of the datapath control signals mentioned\nin the textbook are no longer necessary in the simplified design.\n\nLet's discuss the signals that remain and give some examples of how they\nare used.  A list appears to the right.\n\nFirst, we have a set of seven {1-bit} control signals (starting \nwith ``LD.'') that specifies whether registers in the datapath load \nnew values.\n\nNext, there are four {1-bit} signals (starting with ``Gate'') for \ntri-state buffers that control access to the bus.  These four implement \na distributed mux for the bus.  Only one value can\n\n\n\nsignal& meaning \nLD.MAR& load new value into memory address register\nLD.MDR& load new value into memory data register\nLD.IR& load new value into instruction register\nLD.BEN& load new value into branch enable register\nLD.REG& load new value into register file\nLD.CC& load new values into condition code registers (N,Z,P)\nLD.PC& load new value into program counter \nGatePC& write program counter value onto bus\nGateMDR& write memory data register onto bus\nGateALU& write arithmetic logic unit result onto bus\nGateMARMUX& write memory address register mux output onto bus \nPCMUX& select value to write to program counter (2 bits)\nDRMUX& select value to write to destination register (2 bits)\nSR1MUX& select register to read from register file (2 bits)\nADDR1MUX& select register component of address (1 bit)\nADDR2MUX& select offset component of address (2 bits)\nMARMUX& select type of address generation (1 bit) \nALUK& select arithmetic logic unit operation (2 bits) \nMIO.EN& enable memory \nR.W& read or write from memory \n\n\n\nappear on the bus in \nany cycle, so at most one of these signals can be 1; others must all \nbe 0 to avoid creating a short.\n\n\n\n{file=part4/patt-patel-appendix-C/APPC03.eps,width=6.5in}\n\n\n\nThe third group (ending with ``MUX'') of signals\ncontrols multiplexers in the datapath.\nThe number of bits for each depends on the number of inputs to the mux;\nthe total number of signals is 10.\nThe last two groups of signals control the ALU and the memory, \nrequiring a total of 4 more signals.  The total of all groups is thus\n25 control signals for the datapath without support for privilege\nand interrupts.\n\n \n Question: \nQ. \nQ. 25 \n Answer: \nQ. 23\nQ. 26"}, {"text": "Title: Example Control Word: ADD \n Text: {Example Control Word: ADD}\n\nBefore we begin to discuss control unit design in more detail, let's\nwork through a couple of examples of implementing specific RTL with\nthe control signals available.  The figure below (Patt and Patel \nFigure C.2) shows a state machine for the {LC-3} ISA (again \nwithout detail on interrupts and privilege).\n\n{{file=part4/patt-patel-appendix-C/APPC02.eps,width=6.25in}}\n\nConsider now the state that implements the ADD instruction---state\nnumber 1 in the figure on the previous page, just below and to the \nleft of the decode state.\n\nThe RTL for the state is: DR  SR + OP2, set CC.\n\nWe can think of the 25 control signals that implement the desired RTL\nas a { control word} for the datapath. \n\nLet's begin with the register load control signals.  The RTL writes\nto two types of registers: the register file and the condition codes.\nTo accomplish these simultaneous writes, LD.REG and LD.CC must be high.\nNo other registers in the datapath should change, so the other five\nLD signals---LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC---should be low.\n\nWhat about the bus?  In order to write the result of the add operation\ninto the register file, the control signals must allow the ALU to write\nits result on to the bus.  So we need GateALU=1.  And the other Gate\nsignals---GatePC, GateMDR, and GateMARMUX---must all be 0.  The \ncondition codes are also calculated from the value on the bus, but\nthey are calculated on the same value as is written to the register\nfile (by the definition of the {LC-3} ISA).\n\nIf the RTL for an FSM state implicitly requires more than one value \nto appear on the bus in the same cycle, that state is impossible to \nimplement using the given datapath.  Either the datapath or the state \nmachine must be changed in such a case.  The textbook's design has been \nfairly thoroughly tested and debugged.\n\n\nThe earlier figure of the datapath does not \nshow all of the muxes.  The remaining muxes appear in the figure \nto the right (Patt and Patel Figure C.6).\n\nSome of the muxes in the datapath must be used to enable the\naddition needed for ADD to occur.  The\nDRMUX must select its IR[11:9] \ninput in order to write to the destination register specified by the \nADD instruction.  Similarly, the SR1MUX must select its IR[8:6] input \nin order to\n\n\n{file=part4/patt-patel-appendix-C/APPC06.eps,width=4.45in}\n\n\npass the first source register specified by the ADD to the ALU as input A\n(see the datapath figure).  SR2MUX is always controlled by the \nmode bit IR[5], so the control unit does not need to generate anything\n(note that this signal was not in the list given earlier).\n\nThe rest of the muxes in the datapath---PCMUX, ADDR1MUX, ADDR2MUX, and\nMARMUX---do not matter, and the signals controlling them are don't cares.\nFor example, since the PC does not change, the output of the PCMUX\nis simply discarded, thus which input the PCMUX forwards to its output\ncannot matter.\n\nThe ALU must perform an addition, so we must set the operation type\nALUK appropriately.  And memory should not be enabled (MIO.EN=0),\nin which case the read/write control for memory, R.W, is a don't\ncare.  These 25 signal values together (including seven don't cares)\nimplement the RTL for the single state of execution for an \nADD instruction.\n\n\n \n Question: \nQ. What is the control word for the state that implements the ADD instruction? \n Answer: \n\nA. LD.REG = 1, LD.CC = 1, GateALU = 1, ALUK = 0, and MIO.EN = 0"}, {"text": "Title: Example Control Word: LDR \n Text: {Example Control Word: LDR}\n\nAs a second example, consider the first state in the sequence that \nimplements the LDR instruction---state number 6 in the figure on the \nprevious page.\n\nThe RTL for the state is: MAR  BaseR + off6, but BaseR is\nabbreviated to ``B'' in the state diagram.\n\nWhat is the control word for this state?\n\nLet's again begin with the register load control signals.  Only the\nMAR is written by the RTL, so we need LD.MAR=1 and the other load\nsignals all equal to 0.\n\nThe address (BaseR + off6) is generated by the address adder, then \npasses through the MARMUX to the bus, from which it can be written into\nthe MAR.  To allow the MARMUX to write to the bus, we set GateMARMUX\nhigh and set the other three Gate control signals low.\n\nMore of the muxes are needed for this state's RTL than we needed for\nthe ADD execution state's RTL.\n\nThe SR1MUX must again select its IR[8:6] input, this time in order to \npass the BaseR specified by the instruction to ADDR1MUX.  ADDR1MUX\nmust then select the output of the register file in order to pass the \nBaseR to the address adder.  The other input of the address adder should\nbe off6, which corresponds to the sign-\nextended version of IR[5:0].\nADDR2MUX must select this input to pass to the address adder. \nFinally, the MARMUX must select the output of the address adder.\nThe PCMUX and DRMUX do not matter for this case, and can be left as\ndon't cares.  Neither the PC nor any register in the register file\nis written.\n\nThe output of the ALU is not used, so which operation it performs is \nirrelevant, and the ALUK controls are also don't cares.  Memory is\nalso not used, so again we set MIO.EN=0.  And, as before, the R.W\ncontrol for memory is a don't care.\n\nThese 25 signal values together (again including seven don't cares)\nimplement the RTL for the first state of LDR execution.\n\n\n \n Question: \nQ. The RTL for the state is: MAR  BaseR + off6, but BaseR is\nQ. abbreviated to ``B'' in the state diagram.\nQ. \nQ. What is the control word for this state? \n Answer: \n\nThe control word for this state is:\n\nLD.MAR=1, GateMARMUX=1, ADDR1MUX=IR[8:6], ADDR2MUX=off6, MARMUX=ADDR+, PCMUX=X, DRMUX=X, ALUK=X, MIO.EN=0, R.W=X."}, {"text": "Title: Hardwired Control \n Text: {Hardwired Control}\n\n\nNow we are ready to think about how control signals can be generated.\n\nAs illustrated to the right, instruction processing consists of two \nsteps repeated infinitely: fetch an instruction, then execute the \ninstruction.\n\nLet's say that we choose a fixed number of cycles for each of these two\nsteps.  We can then control our system with a\n\n\n{file=part4/figs/inst-loop.eps,width=2.5in}\n\n\ncounter, using the counter's value and the IR to generate the\ncontrol signals through combinational\nlogic.  The PC is used only as data and has little or no direct effect on \nhow the system fetches and processes instructions.{Some ISAs do \nsplit the address space into privileged and non-privileged regions, but \nwe ignore that possibility here.}\n\nThis approach in general is called { hardwired control}.\n\nHow many cycles do we need for instruction fetch?  How many cycles do\nwe need for instruction processing?  The answers depend on the factors:\nthe complexity of the ISA, and the complexity of the datapath.\n\nGiven a simple ISA, we can design a datapath that\nis powerful enough to process any instruction in a single cycle.  \nThe control unit for such a design is\nan example of { single-cycle, hardwired control}.  While this\napproach simplifies the control unit, the cycle time\nfor the clock is limited by the slowest instruction.\nThe clock must also be slow enough to let memory \noperations complete in single cycle, both for instruction fetch and\nfor instructions that require a memory access.\n\nSuch a low clock rate is usually not acceptable.\n\nMore generally, we can use a simpler datapath and break both instruction\nfetch and instruction processing into multiple steps.  Using the datapath\nin the figure from Patt and Patel, for example, instruction fetch requires \nthree steps, and the number of steps for instruction processing depends \non the type of instruction being processed.  The state diagram shown\nearlier illustrates the steps for each opcode.\n\nAlthough the datapath is not powerful enough to complete instructions\nin a single cycle, we can build a { multi-cycle, hardwired control} \nunit (one based on combinational logic).  \n\nIn fact, this type of control unit is not much more complex than the \nsingle-cycle version.  For the control unit's FSM, we can use a binary \ncounter to enumerate first the steps of fetch, then the steps\nof processing.  The counter value along with the IR register can \ndrive combinational logic to produce control signals.  And, to avoid\nprocessing at the speed of the slowest instruction (the opcode that\nrequires the largest number of steps; LDI and STI in the {LC-3}\nstate diagram), we can add a reset signal to the counter to force it \nback to instruction fetch.  The FSM counter reset signal is simply \nanother control signal.  Finally, we can add one more signal that \npauses the counter while waiting for a memory operation to complete.  \nThe system clock can then run at the speed of the logic rather than \nat the speed of memory.\n\nThe control unit implementation discussed in Appendix C of Patt and Patel \nis not hardwired, but it does make use of a memory ready signal to \nachieve this decoupling between the clock speed of the processor and the\naccess time of the memory.\n\n\nThe figure to the right illustrates a general \nmulti-cycle, hardwired control unit.  The three blocks on\nthe left are the control unit state.  The combinational logic in\nthe middle uses the control unit state along with some\ndatapath status bits to compute the control signals for the datapath\nand the extra controls needed for the FSM counter, IR, and PC.\nThe datapath appears to the right in the figure.\n\n\n\n\n\nHow complex is the combinational logic?  As mentioned earlier, we \nassume that the PC does not directly affect control.  But we still \nhave 16 bits of IR, the FSM counter state, and the datapath status\nsignals.  Perhaps we need 24-variable {K-maps}?  \n\nHere's where engineering and human design come to \nthe rescue: by careful design of the ISA and the encoding, the\nauthors have made many of the datapath control signals for the {LC-3}\nISA quite simple.\nFor example, the register that appears on the register file's SR2\noutput is always specified by IR[2:0].  The SR1 output \nrequires a mux, but the choices are limited to IR[11:9] and IR[8:6]\n(and R6 in the design with support for interrupts).\nSimilarly, the destination register in the register file\nis always R7 or IR[11:9] (or, again, R6 when supporting interrupts).\n\nThe control signals for an {LC-3}\ndatapath depend almost entirely on the state of the control unit \nFSM (counter bits in a hardwired design) and the opcode IR[15:12].\n\nThe control signals are thus reduced to fairly simple functions.\n\nLet's imagine building a hardwired control unit for the {LC-3}.\n\nLet's start by being more precise about the number of inputs to\nthe combinational logic.\n\nAlthough most decisions are based on the opcode, the datapath\nand state diagram shown earlier for the {LC-3} ISA do have\none instance of using another instruction bit to determine behavior.\nSpecifically, the JSR instruction has two modes, and the control\nunit uses IR[11] to choose between them.  So we need to have\nfive bits of IR instead of four as input to our logic.\n\nHow many datapath status signals are needed?\n\nWhen the control unit accesses memory, it must wait until the \nmemory finishes the access, as indicated by a memory ready signal R.\nAnd the control unit must implement the conditional part of conditional\nbranches, for which it uses the datapath's branch enable signal BEN.\n\nThese two datapath status signals suffice for our design.\n\nHow many bits do we need for the counter?\nInstruction fetch requires three cycles: one to move\nthe PC to the MAR and increment the PC, a second to read from\nmemory into MDR, and a third to move the instruction bits across\nthe bus from MDR into IR.  Instruction decoding in a hardwired design\nis implicit and \nrequires no cycles: since all of our control signals can depend on\nthe IR, we do not need a cycle to change the FSM state to reflect the opcode.\nLooking at the {LC-3} state diagram, we see\nthat processing an instruction requires at most five cycles.  In\ntotal, at\nmost eight steps are needed to fetch and process any {LC-3} \ninstruction, so we can use a {3-bit} binary counter.\n\nWe thus have a total of ten bits of input: IR[15:11], R, BEN, and\na {3-bit} counter.\n\nAdding the RESET and PAUSE controls for our FSM counter to the \n25 control signals listed earlier, we need to find 27 functions\non 10 variables.\n\nThat's still a lot of big {K-maps} to solve.  Is there an\neasier way?\n\n\n \n Question: \nQ. How many datapath status signals are needed for a hardwired control unit? \n Answer: \n\nA. Two datapath status signals are needed for a hardwired control unit."}, {"text": "Title: Using a Memory for Logic Functions \n Text: {Using a Memory for Logic Functions}\n\nConsider a case in which you need to compute many functions on a small \nnumber of bits, such as we just described for the multi-cycle, hardwired\ncontrol unit.  One strategy is to use a memory (possibly a read-only memory).\nA {2^m} memory can be viewed as computing N arbitrary\nfunctions on m variables.  The functions to be computed are specified\nby filling in the bits of the memory.  So long as the value of m is fairly\nsmall, the memory (especially SRAM) can be fast.\n\nSynthesis tools (or hard work) can, of course, produce smaller designs\nthat use fewer gates.  Actually, tools may be able to optimize a fixed\ndesign expressed as read-only memory, too.  But designing the functions\nwith a memory makes them easier to modify later.  If we make a mistake,\nfor example, in computing one of the functions, we need only change a bit\nor two in the memory instead of solving equations and reoptimizing and\nreplacing logic gates.  We can also extend our design if we have space\nremaining (that is, if the functions are undefined for some combinations \nof the m inputs).  The\nCray T3D supercomputer, for example, used a similar approach to add\nnew instructions to the Alpha processors on which it was based.\n\nThis strategy is effective in many contexts, so let's briefly discuss two \nanalogous cases.  In software, a memory becomes a lookup table.  Before\nhandheld calculators, lookup tables were used by humans to compute \ntranscendental functions such as sines, cosines, logarithms.  Computer \ngraphics hardware and software used a similar approach for transcendental\nfunctions in order to reduce cost and improve \nspeed.  Functions such as counting 1 bits in a word are useful for \nprocessor scheduling and networking, but not all ISAs provide this type\nof instruction.  In such cases, lookup tables in software are often the\nbest solution.\n\nIn programmable hardware such as Field Programmable Gate Arrays (FPGAs),\nlookup tables (called LUTs in this context) have played an important role\nin implementing arbitrary logic functions.\n\nThe FPGA is the modern form of the programmable logic array (PLA)\nmentioned in the textbook, and will be your main tool \nfor developing digital hardware in ECE385.\n\nFor many years, FPGAs served as a hardware prototyping platform, but\nmany companies today ship their first round products using \ndesigns mapped to FPGAs.  Why?  Chips are more and more expensive\nto design, and mistakes are costly to fix.  In contrast, while\ncompanies pay more to buy an FPGA than to produce a chip (after the \nfirst chip!), errors in the design can usually be fixed by sending \ncustomers a new version through the Internet.\n\nLet's return to our {LC-3} example.\n\nInstead of solving the {K-maps}, we can use a small memory:\n2^ bits (27,648 bits total).\n\nWe just need calculate the bits, put\nthem into the memory, and use the memory to produce the control signals.\n\nThe ``address'' input to the memory are the same 10 bits that we\nneeded for our combinational logic: IR[15:11], R, BEN, and the FSM counter.\nThe data outputs of the memory are the control signals and the\nRESET and PAUSE inputs to the FSM counter.  And we're done.\n\nWe can do a little better, though.  The datapath in the textbook was\ndesigned to work with the textbook's control unit.  If we add a little\nlogic, we can significantly simplify our memory-based, hardwired implementation.\nFor example, we only need to pause the FSM counter when waiting for memory.\nIf we can produce a control signal that indicates a need to wait for\nmemory, say {WAIT-MEM}, we can use a couple of gates to compute the\nFSM counter's PAUSE signal as {WAIT-MEM} AND (NOT R).  Making this change\nshrinks our memory to 2^9 bits.  The extra two control\nsignals in this case are RESET and {WAIT-MEM}.\n\nNext, look at how BEN is used in the state diagram:\nthe only use is to terminate the\nprocessing of branch instructions when no branch should occur (when \nBEN=0).  We can fold that functionality into the FSM counter's RESET\nsignal by producing a branch reset signal, {BR-RESET}, to reset\nthe counter to end a branch and a second signal, {INST-DONE},\nwhen an instruction is done.  The RESET input for the FSM counter\nis then ({BR-RESET} AND (NOT BEN)) OR {INST-DONE}.\n\nAnd our memory further shrinks to 2^8 bits, where the extra\nthree control signals are {WAIT-MEM}, {BR-RESET}, and \n{INST-DONE}.\n\nFinally, recall that the only need for IR[11] is to implement the two\nforms of JSR.  But we can add wires to connect SR1 to PCMUX's fourth \ninput, then control the PCMUX output selection using IR[11] when appropriate\n(using another control signal).  With this extension, we can implement\nboth forms with a single state, writing to both R7 and PC in the same\ncycle.\n\nOur final memory can then be 2^7 bits (3,712 bits total),\nwhich is less than one-seventh the number of bits that we needed before \nmodifying the datapath.\n\n\n\n\n \n Question: \nQ. How does using a memory to compute logic functions simplify the design process? \n Answer: \n\nA. Using a memory to compute logic functions allows for easy modification of the design if a mistake is made. Additionally, it allows for easy extension of the design if more space is needed."}, {"text": "Title: Microprogrammed Control \n Text: {Microprogrammed Control}\n\nWe are now ready to discuss the second approach to control unit design.\nTake another look at the state diagram for the the {LC-3} ISA.  Does it \nremind you of anything?  Like a flowchart, it has relatively few arcs \nleaving each state---usually only one or two.\n\nWhat if we treat the state diagram as a program?  We can use a small\nmemory to hold { microinstructions} (another name for control words) \nand use the FSM state number as the memory address.  \n\nWithout support for interrupts or privilege, and with the datapath \nextension for JSR mentioned for hardwired control, the {LC-3} state \nmachine requires fewer than 32 states.\n\nThe datapath has 25 control signals, but we need one more for the\ndatapath extension for JSR.\n\nWe thus start with {5-bit} state number (in a register)\nand a {2^5 bit} memory, which we call\nour control ROM (read-only memory) to distinguish\nit from the big, slow, von Neumann memory.\n\nEach cycle, the { microprogrammed control} unit applies the FSM state \nnumber to the control ROM (no IR bits, \njust the state number), gets back a set of control signals, and\nuses them to drive the datapath.\n\n\nTo write our microprogram, we need to calculate the control signals\nfor each microinstruction and put them in the control ROM, but we also\nneed to have a way to decide which microinstruction should execute \nnext.  We call the latter problem { sequencing} or microsequencing.\n\nNotice that most of the time there's no choice: we have only { one}\nnext microinstruction.  One simple approach is then to add the address\n(the {5-bit} state ID) of the next microinstruction to the control ROM.\nInstead of 26 bits per FSM state, we now have 31 bits per FSM state.\n\nSometimes we do need to have two possible next states.  When waiting\nfor memory (the von Neumann memory, not the control ROM) to\nfinish an access, for example, we want our FSM to stay\nin the same state, then move to the next state when the access completes.\nLet's add a second address to each microinstruction, and add\na branch control signal for the microprogram to decide whether we\nshould use the first address or the second for the next\nmicroinstruction.  This design, using a 2^5 bit memory\n(1,152 bits total), appears to the right.\n\n\n{file=part4/figs/microprogrammed-no-decode.eps,width=2in}\n\n\n\nThe microprogram branch control signal is a Boolean logic expression\nbased on the memory ready signal R and \nIR[11].  We can implement it with a state ID comparison and\na mux, as shown to the right.  For the branch instruction execution state, \nthe mux selects input 1, BEN.  For all other states, the mux selects \ninput 0, R.  When an FSM state has only a single next state, we set both \nIDs in the control ROM to the ID for that state, so the value of R \nhas no effect.\n\n\n\n\n\n We can simplify the implementation of the microprogram branch control\n by setting both next-state addresses to the same value when a state\n does not branch.  Then the branch control becomes a don't care for that\n state.\n\n\nWhat's missing?  Decode!  We do have one FSM state in which we need to be\nable to branch to one of sixteen possible next states, one for each\nopcode.  Let's just add another mux and choose the state IDs for starting\nto process each opcode in an easy way, as shown to the right with\nextensions highlighted in blue.  The textbook assigns the first state \nfor processing each opcode the IDs IR[15:12] preceeded by two 0s (the\ntextbook's design requires {6-bit} state IDs).  We adopt the same\nstrategy.  For example, the first FSM state for processing an ADD is 00001, \nand the first state for a TRAP is 01111.  In each case, the opcode \nencoding specifies the last four bits.\n\nNow we can pick the remaining state IDs arbitrarily and fill in the \ncontrol ROM with control \nsignals that implement each state's RTL and the two possible next states.\nTransitions from the decode state are handled by the extra mux.\n\nThe microprogrammed control unit implementation in Appendix C of \nPatt and Patel is similar to the one that we have developed here,\nbut is slightly more complex so that it can handle interrupts\nand privilege.   \n\n\n\n\n\n\n\n\n \n Question: \nQ. We are now ready to discuss the second approach to control unit design.\nQ. Take another look at the state diagram for the the {LC-3} ISA.  Does it \nQ. remind you of anything?  Like a flowchart, it has relatively few arcs \nQ. leaving each state---usually only one or two.\nQ. \nQ. What if we treat the state diagram as a program?  We can use a small\nQ. memory to hold { microinstructions} (another name for control words) \nQ. and use the FSM state number as the memory address.  \nQ. \nQ. Without support for interrupts or privilege, and with the datapath \nQ. extension for JSR mentioned for hardwired control, the {LC-3} state \nQ. machine requires fewer than 32 states.\nQ. \nQ. The datapath has 25 control signals, but we need one more for the\nQ. datapath extension for JSR.\nQ. \nQ. We thus start with {5-bit} state number (in a register)\nQ. and a {2^5 bit} memory, which we call\nQ. our control ROM (read-only memory) to distinguish\nQ. it from the big, slow, von Neumann memory.\nQ. \nQ. Each cycle, the { micro \n Answer: programmed control} unit applies the FSM state \nQ. number to the control ROM (no IR bits, \nQ. just the state number), gets back a set of control signals, and\nQ. uses them to drive the datapath.\nQ. \nQ. \nQ. To write our microprogram, we need to calculate the control signals\nQ. for each microinstruction and put them in the control ROM, but we also\nQ. need to have a way to decide which microinstruction should execute \nQ. next.  We call the latter problem { sequencing} or microsequencing.\nQ. \nQ. Notice that most of the time there's no choice: we have only { one}\nQ. next microinstruction.  One simple approach is then to add the address\nQ. (the {5-bit} state ID) of the next microinstruction to the control ROM.\nQ. Instead of 26 bits per FSM state, we now have 31 bits per FSM state.\nQ. \nQ. Sometimes we do need to have two possible next states.  When waiting\nQ. for memory (the von Neumann memory, not the"}, {"text": "Title: Redundancy and Coding \n Text: {Redundancy and Coding}\n\nThis set of notes introduces the idea of using sparsely populated\nrepresentations to protect against accidental changes to bits.\nToday, such representations are used in almost every type of storage\nsystem, from bits on a chip to main memory to disk to archival tapes.\n\nWe begin our discussion with examples of representations in which some \nbit patterns have no meaning, then consider what happens when a bit \nchanges accidentally.  We next outline a general scheme \nthat allows a digital system to detect a single bit error.\n\nBuilding on the mechanism underlying this scheme,\nwe describe a distance metric that enables us to think more broadly about \nboth detecting and correcting such errors, and then show a general\napproach that allows correction of a single bit error.\n\nWe leave discussion of more sophisticated schemes to classes on\ncoding and information theory.\n\n\n \n Question: \nQ. What is the purpose of using a sparsely populated representation? \n Answer: \n\nA. The purpose of using a sparsely populated representation is to\nprotect against accidental changes to bits."}, {"text": "Title: Sparse Representations \n Text: {Sparse Representations}\n\nRepresentations used by computers must avoid ambiguity: a single\nbit pattern in a representation cannot be used to represent more than\none value.  However, the converse need not be true.  A representation can\nhave several bit patterns representing the same value, and\nnot all bit patterns in a representation need be\nused to represent values.\n\nLet's consider a few example of representations with unused patterns.\nHistorically, one common class of representations of this type was \nthose used to represent individual decimal digits.  We examine three\nexamples from this class.\n\nThe first is Binary-coded Decimal (BCD), in which\ndecimal digits are encoded individually \nusing their representations in the unsigned\n(binary) representation.  Since we have 10 decimal digits, we need \n10 patterns, and four bits for each digit.  But four bits allow \n2^4=16 bit patterns.  In BCD, the patterns 1010,1011,,1111\nare unused.  It is important to note that BCD is not the same as the\nunsigned representation.  The decimal number 732, for example, requires 12 bits\nwhen encoded as BCD: 0111 0011 0010.  When written using a {12-bit}\nunsigned representation, 732 is written 001011011100.  Operations\non BCD values were implemented in early processors, including the\n8086, and are thus still available in the x86 instruction set architecture\ntoday!\n\n\nThe second example is an {Excess-3} code, in which each decimal\ndigit d is represented by the pattern corresponding to the \n{4-bit} unsigned pattern for d+3.  For example, the digit 4\nis represented as 0111, and the digit 7 is represented as 1010.\nThe {Excess-3} code has some attractive aspects when using \nsimple hardware.  For example, we can use a {4-bit} binary \nadder to add two digits c and d represented in the {Excess-3} code, \nand the carry out signal produced by the adder is the same as the carry\nout for the decimal addition, since c+d is equivalent to \n(c+3)+(d+3).\n\nThe third example of decimal digit representations is a {2-out-of-5}\ncode.  In such a code, five bits are used to encode each digit.  \n\nOnly patterns with exactly two 1s are used.  There are exactly ten \nsuch patterns, and an example representation is shown to the right (more\nthan one assignment of values to patterns has been used in real systems).\n\n\n{c|c}\n& a 2-out-of-5\ndigit& representation \n1& 00011\n2& 00101\n3& 00110\n4& 01001\n5& 01010\n6& 01100\n7& 10001\n8& 10010\n9& 10100\n0& 11000\n\n\n\n\n\n \n Question: \nQ. What is a 2-out-of-5 code? \n Answer: \n\nA. A 2-out-of-5 code is a code in which five bits are used to encode each digit, and only patterns with exactly two 1s are used."}, {"text": "Title: Error Detection \n Text: {Error Detection}\n\nErrors in digital systems\ncan occur for many reasons, ranging from cosmic ray strikes to\ndefects in chip fabrication to errors in the design of the digital system.\n\nAs a simple model, we assume that an error takes the form of changes to\nsome number of bits.  In other words, a bit that should have the value 0\ninstead has the value 1, or a bit that should have the value 1 instead\nhas the value 0.  Such an error is called a { bit error}.\n\n\n\nDigital systems can be designed with or without tolerance to errors.\nWhen an error occurs, no notification nor identification of the error\nis provided.  Rather, if error tolerance is needed,\nthe system must be designed to be able to \nrecognize and identify errors automatically.\n\nOften, we assume that each of the bits may be in error independently\nof all of the others, each with some low probability.  With such\nan assumption, multiple bit errors\nare much less likely than single bit errors, and we can focus on \ndesigns that tolerate a single bit error.\n\nWhen a bit error occurs, however, we must assume that \nit can happen to any of the bits.\n\nThe use of many patterns to represent a smaller number of values, as\nis the case in a {2-out-of-5} code, enables a system\nto perform { error detection}.\n\nLet's consider what happens when a value represented using a \n{2-out-of-5} code is subjected to a single bit error.\nImagine that we have the digit 7.  In the table on the previous page,\nnotice that the digit 7 is represented with the pattern 10001.\n\nAs we mentioned, we must assume that the bit error can occur in any \nof the five bits, thus we have five possible bit patterns after the error \noccurs.  If the error occurs in the first bit, we have the pattern 00001.\nIf the error occurs in the second bit, we have the pattern 11001.\nThe complete set of possible error patterns is 00001, 11001, 10101,\n10011, and 10000.  \n\nNotice that none of the possible error patterns has exactly two 1s, and \nthus none of them is a meaningful pattern in our {2-out-of-5} code.\nIn other words, whenever a digital system represents the digit 7 and \na single bit error occurs, the system will be able to detect that an \nerror has occurred.\n\nWhat if the system needs to represent a different digit?  \nRegardless of which digit is represented, the pattern with no errors \nhas exactly two 1s,\nby the definition of our representation.  If we then flip one of the \nfive bits by subjecting it to a bit error, the resulting error pattern\nhas either one 1 (if the bit error changes a 1 to a 0) or three 1s\n(if the bit error changes a 0 to a 1).  In other words, regardless of\nwhich digit is represented, and regardless of which bit has an error,\nthe resulting error pattern never has a meaning in \nthe {2-out-of-5} code.  So this representation enables a digital\nsystem to detect any single bit error!\n\n\n \n Question: \nQ. What if the system needs to represent a different digit? \n Answer: \n\nA. If the system needs to represent a different digit, the pattern with no errors will still have exactly two 1s. If a bit error occurs, the resulting error pattern will have either one 1 or three 1s, which will not have a meaning in the 2-out-of-5 code. So this representation will still enable the digital system to detect any single bit error."}, {"text": "Title: Parity \n Text: \n\nThe ability to detect any single bit error is certainly useful.\nHowever, so far we have only shown how to protect ourselves when we\nwant to represent decimal digits.  Do we need to develop a separate\nerror-tolerant representation for every type of information that\nwe might want to represent?  Or can we instead come up with a more\ngeneral approach?\n\nThe answer to the second question is yes: we can, in fact, systematically\ntransform any representation into a representation that allows detection of a\nsingle bit error.  The key to this transformation is the idea of\n{ parity}.\n\n\nConsider an arbitrary representation for some type of information.\nBy way of example, we use the {3-bit} unsigned representation.\nFor each pattern used in the representation, we can count the number\nof 1s.  The resulting count is either odd or even.  By adding an extra\nbit---called a { parity bit}---to the representation, and \nselecting the parity bit's value \nappropriately for each bit pattern, we can ensure that the count of 1s\nis odd (called { odd parity}) or even (called { even parity})\nfor all values represented.  The idea is\n\nillustrated in the table\nto the right for the {3-bit} unsigned representation.  The parity\nbits are shown in bold.\n\n\n{c|c|c|c|c}\nvalue      &   3-bit & number& with odd& with even \nrepresented& unsigned& of 1s & parity  & parity  \n0& 000& 0& 000{ 1}& 000{ 0}\n1& 001& 1& 001{ 0}& 001{ 1}\n2& 010& 1& 010{ 0}& 010{ 1}\n3& 011& 2& 011{ 1}& 011{ 0}\n4& 100& 1& 100{ 0}& 100{ 1}\n5& 101& 2& 101{ 1}& 101{ 0}\n6& 110& 2& 110{ 1}& 110{ 0}\n7& 111& 3& 111{ 0}& 111{ 1}\n\n\n\nEither approach to selecting the parity bits ensures that any single\nbit error can be detected.  For example, if we choose to use odd\nparity, a single\nbit error changes either a 0 into a 1 or a 1 into a 0.\nThe number of 1s in the resulting error pattern thus\ndiffers by exactly one from the original pattern, and the parity of\nthe error pattern is even.  But all valid patterns have odd parity,\nso any single bit error can be detected by simply counting the number\nof 1s.\n\n\n \n Question: \nQ. What is the purpose of a parity bit? \n Answer: \n\nA. A parity bit is used to detect single bit errors in a representation."}, {"text": "Title: Hamming Distance \n Text: {Hamming Distance}\n\nNext, let's think about how we might use representations---we might\nalso think of them as { codes}---to protect a system against \nmultiple bit errors.  As we have seen with parity, one strategy that\nwe can use to provide such error tolerance is the use of representations\nin which only some of the patterns actually represent values.\nLet's call such patterns { code words}.  In other words, the\ncode words in a representation are those patterns that correspond to \nreal values of information.  Other patterns in the representation have no \nmeaning.\n\nAs a tool to help us understand error tolerance, let's define\na measure of the distance between code words in a representation.\nGiven two code words X and Y, we can calculate the number N_ of \nbits that must change to transform X into Y.  Such a calculation\nmerely requires that we compare the patterns bit by bit and count\nthe number of places in which they differ.  Notice that this\nrelationship is symmetric: the same number of changes are required\nto transform Y into X, so N_=N_.  \n\nWe refer to this number N_ as the { Hamming distance} between\ncode word X and code word Y.  \n\nThe metric is named after Richard Hamming, a computing pioneer and an \nalumnus of the UIUC Math department.\n\nThe Hamming distance between two code words tells us how many bit errors\nare necessary in order for a digital system to mistake one code word\nfor the other.  Given a representation, we can calculate the minimum\nHamming distance between any pair of code words used by the \nrepresentation.  The result is called the { Hamming distance of the \nrepresentation}, and represents the minimum of bit errors that must\noccur before a system might fail to detect errors in a stored value.\n\nThe Hamming distance for nearly all of the representations that we \nintroduced in earlier sections is 1.  Since more than half of the\npatterns (and often all of the patterns!) correspond to meaningful\nvalues, some pairs of code words must differ in only one bit, and\nthese representations cannot tolerate any errors.  For example,\nthe decimal value 42 is stored as 101010 using a {6-bit} unsigned\nrepresentation, but any bit error in that pattern produces another \nvalid pattern corresponding to one of the following \ndecimal numbers: 10, 58, 34, 46, 40, 43.  Note that the Hamming distance\nbetween any two patterns is not necessarily 1.  Rather, the Hamming \ndistance of the unsigned representation, which corresponds to the \nminimum between any pair of valid patterns, is 1.\n\nIn contrast, the Hamming distance of the {2-out-of-5} code that\nwe discussed earlier is 2.  Similarly, the Hamming distance of any\nrepresentation extended with a parity bit is at least 2.\n\nNow let's think about the problem slightly differently.\n\nGiven a particular representation, \n\nhow many bit errors can we detect in values using that representation?\n\n{ A representation with Hamming distance d can detect up to d-1 bit errors.}\n\nTo understand this claim, start by selecting a code word from the\nrepresentation and changing up to d-1 of the bits.  No matter\nhow one chooses to change the bits, these changes cannot result in\nanother code word, since we know that any other code word has to \nrequire at least d changes from our original code word, by the\ndefinition of the representation's Hamming distance.\n\nA digital system using the representation can thus detect up to d-1\nerrors.  However, if d or more errors occur, the system might sometimes\nfail to detect any error in the stored value.\n\n\n \n Question: \nQ. How many bit errors can a representation with Hamming distance 2 detect? \n Answer: \n\nA. A representation with Hamming distance 2 can detect 1 bit error."}, {"text": "Title: Error Correction \n Text: {Error Correction}\n\nDetection of errors is important, but may sometimes not be enough.\nWhat can a digital system do when it detects an error?  In some\ncases, the system may be able to find the original value elsewhere, or\nmay be able to re-compute the value from other values.  In other \ncases, the value is simply lost, and the digital system may need\nto reboot or even shut down until a human can attend to it.\n\nMany real systems cannot afford such a luxury.  Life-critical systems\nsuch as medical equipment and airplanes should not turn themselves off\nand wait for a human's attention.  Space vehicles face a similar dilemma,\nsince no human may be able to reach them.\n\nCan we use a strategy similar to the one that we have developed for error\ndetection in order to try to perform { error correction}, recovering\nthe original value?  Yes, but the overhead---the\nextra bits that we need to provide such functionality---is higher.\n\n\n\nLet's start by thinking about a code with Hamming distance 2, such\nas {4-bit} 2's complement with odd parity.  We know that such a \ncode can detect one bit error.  Can it correct such a bit error, too?\n\nImagine that a system has stored the decimal value 6 using the \npattern 0110{ 1}, where the last bit is the odd parity bit.\nA bit error occurs, changing the stored pattern to 0111{ 1}, which is\nnot a valid pattern, since it has an even number of 1s.  But can the\nsystem know that the original value stored was 6?  No, it cannot.\nThe original value may also have been 7, in which case the original\npattern was 0111{ 0}, and the bit error occurred in the final\nbit.  The original value may also have been -1, 3, or 5.  The system\nhas no way of resolving this ambiguity.\n\nThe same problem arises if a digital system uses a code with\nHamming distance d to detect up to d-1 errors.\n\n\nError correction is possible, however, if we assume that fewer bit\nerrors occur (or if we instead use a representation with a larger Hamming\ndistance).\n\nAs a simple example, let's create a representation for the numbers 0\nthrough 3 by making three copies of the {2-bit} unsigned \nrepresentation, as shown to the right.  The Hamming distance of the\nresulting code is 3, so any two bit errors can be detected.  However,\nthis code also enables us to correct a single bit error.  Intuitively, \nthink of the three copies as voting on the right answer.\n\n\n{c|c}\nvalue      & three-copy\nrepresented& code \n0& 000000\n1& 010101\n2& 101010\n3& 111111\n\n\n\nSince a\nsingle bit error can only corrupt one copy, a majority vote always\ngives the right answer!\nTripling the number of bits needed in a representation is not a good\ngeneral strategy, however. \nNotice also that ``correcting'' a pattern with two bit errors can produce\nthe wrong result.\n\nLet's think about the problem in terms\nof Hamming distance.  Assume that we use a code with Hamming distance d\nand imagine that up to k bit errors affect a stored value.\nThe resulting pattern then falls within a neighborhood of distance k\nfrom the original code word.  This neighborhood contains all bit \npatterns within Hamming distance k of the original pattern.\nWe can define such a neighborhood around each code word.  Now, \nsince d bit errors are needed to transform a code word into\nany other code word, these neighborhoods are disjoint so long\nas 2k{d-1}.  In other words, if the inequality holds,\nany bit pattern in the representation can be in at most one code word's \nneighborhood.  The digital system can then correct the errors by \nselecting the unique value identified by the associated neighborhood.\nNote that patterns encountered as a result of up to k bit errors\nalways fall within the original code word's neighborhood; the inequality\nensures that the neighborhood identified in this way is unique.\nWe can manipulate the inequality to express the number of errors k that\ncan be corrected in terms of the Hamming distance d of the code.\n{ A code with Hamming distance d allows up to {d-1}\nerrors to be corrected}, where  represents the\ninteger floor function on x, or rounding x down to the nearest \ninteger.\n\n\n \n Question: \nQ. What is error correction? \n Answer: \n\nError correction is the process of detecting and correcting errors in digital data."}, {"text": "Title: Hamming Codes \n Text: {Hamming Codes}\n\nHamming also developed a general and efficient approach for \nextending an arbitrary representation to allow correction of\na single bit error.  The approach yields codes with Hamming distance 3.\n\nTo understand how a { Hamming code} works, think of the bits in the\nrepresentation as being numbered starting from 1.  For example, if\nwe have seven bits in the code, we might write a bit pattern X \nas x_7x_6x_5x_4x_3x_2x_1.\n\nThe bits with indices that are powers of two are parity check bits.\nThese include x_1, x_2, x_4, x_8, and so forth.  The remaining\nbits can be used to hold data.  For example, we could use a {7-bit}\nHamming code and map the bits from a {4-bit} unsigned representation \ninto bits x_7, x_6, x_5, and x_3.  Notice that Hamming codes are\nnot so useful for small numbers of bits, but require only logarithmic\noverhead for large numbers of bits.  That is, in an {N-bit}\nHamming code, only _2(N+1) bits are used for parity\nchecks.\n\nHow are the parity checks defined?  Each parity bit is used to\nprovide even parity for those bits with indices for which the index,\nwhen written in binary, includes a 1 in the single position in which\nthe parity bit's index contains a 1.  The x_1 bit, for example,\nprovides even parity on all bits with odd indices.  The x_2 bit\nprovides even parity on x_2, x_3, x_6, x_7, x_, and so\nforth.\n\nIn a {7-bit} Hamming code, for example, \nx_1 is chosen so that\nit has even parity together with x_3, x_5, and x_7.\n\nSimilarly, x_2 is chosen so that\nit has even parity together with x_3, x_6, and x_7.\n\nFinally, x_4 is chosen so that\nit has even parity together with x_5, x_6, and x_7.\n\n\nThe table to the right shows the result of embedding \na {4-bit} unsigned representation into a {7-bit}\nHamming code.\n\nA Hamming code provides a convenient way to identify\nwhich bit should be corrected when a single bit error occurs.  \nNotice that each bit is protected by a unique subset of the parity bits\ncorresponding to the binary form of the bit's index.  Bit x_6, for\nexample, is protected by bits x_4 and x_2, because the number 6\nis written 110 in binary.\nIf a bit is affected by an error, the parity bits\nthat register the error are those corresponding to 1s in the binary\nnumber of the index.  So if we calculate check bits as 1 to represent\nan error (odd parity) and 0 to represent no error (even parity),\nthen concatenate those bits into a binary number, we obtain the\nbinary value of the index of the single bit affected by an error (or \nthe number 0\nif no error has occurred).\n\n\n{c|c|c|c|c|c}\n           &  4-bit         &     &     &     &7-bit\nvalue      & unsigned       &     &     &     &Hamming\nrepresented&(x_7x_6x_5x_3)&x_4&x_2&x_1&code \n 0& 0000& 0& 0& 0& 0000000\n 1& 0001& 0& 1& 1& 0000111\n 2& 0010& 1& 0& 1& 0011001\n 3& 0011& 1& 1& 0& 0011110\n 4& 0100& 1& 1& 0& 0101010\n 5& 0101& 1& 0& 1& 0101101\n 6& 0110& 0& 1& 1& 0110011\n 7& 0111& 0& 0& 0& 0110100\n 8& 1000& 1& 1& 1& 1001011\n 9& 1001& 1& 0& 0& 1001100\n10& 1010& 0& 1& 0& 1010010\n11& 1011& 0& 0& 1& 1010101\n12& 1100& 0& 0& 1& 1100001\n13& 1101& 0& 1& 0& 1100110\n14& 1110& 1& 0& 0& 1111000\n15& 1111& 1& 1& 1& 1111111\n\n\n\nLet's do a couple of examples based on the pattern for the decimal\nnumber 9, 1001100.  First, assume that no error occurs.  We calculate\ncheck bit c_4 by checking whether x_4, x_5, x_6, and x_7 together\nhave even parity.  Since no error occurred, they do, so c_4=0.\nSimilarly, for c_2 we consider x_2, x_3, x_6, and x_7.\nThese also have even parity, so c_2=0.  Finally, for c_1, we \nconsider x_1, x_3, x_5, and x_7.  As with the others, these\ntogether have even parity, so c_1=0.  Writing c_4c_2c_1, we obtain 000,\nand conclude that no error has occurred.\n\nNext assume that bit 3 has an error, giving us the pattern 1001000.\nIn this case, we have again that c_4=0, but the bits corresponding to\nboth c_2 and c_1 have odd parity, so c2=1 and c_1=1.  Now when\nwe write the check bits c_4c_2c_1, we obtain 011,\nand we are able to recognize that bit 3 has been changed.\n\nA Hamming code can only correct one bit error, however.  If two bit\nerrors occur, correction will produce the wrong answer.  Let's imagine\nthat both bits 3 and 5 have been flipped in our example pattern for\nthe decimal number 9, producing the pattern 1011000.  Calculating the\ncheck bits as before and writing them as c_4c_2c_1, we obtain 110,\nwhich leads us to incorrectly conclude that bit 6 has been flipped.\nAs a result, we ``correct'' the pattern to 1111000, which represents\nthe decimal number 14.\n\n\n \n Question: \nQ. What is the Hamming distance of a Hamming code? \n Answer: \n\nA. The Hamming distance of a Hamming code is 3."}, {"text": "Title: SEC-DED Codes \n Text: {SEC-DED Codes}\n\nWe now consider one final extension of Hamming codes to enable a system\nto perform single error correction while also detecting any two bit errors.\nSuch codes are known as { Single Error Correction, Double Error \nDetection (SEC-DED)} codes.  Creating such a code from a Hamming code is\ntrivial: add a parity bit covering the entire Hamming code.  The extra\nparity bit increases the Hamming distance to 4.  A Hamming distance of 4\nstill allows only single bit error correction, but avoids the problem\nof Hamming distance 3 codes when two bit errors occur, since patterns\nat Hamming distance 2 from a valid code word cannot be within distance 1\nof another code word, and thus cannot be ``corrected'' to the wrong\nresult.\n\nIn fact, one can add a parity bit to any representation with an odd\nHamming distance to create a new representation with Hamming distance\none greater than the original representation.  To proof this convenient\nfact, begin with a representation with Hamming distance d, where d\nis odd.  If we choose two code words from the representation, and their \nHamming distance is already greater than d, their distance in the \nnew representation will also be greater than d.  Adding a parity\nbit cannot decrease the distance.  On the other hand, if the two code\nwords are exactly distance d apart, they must have opposite parity,\nsince they differ by an odd number of bits.  Thus the new parity bit\nwill be a 0 for one of the code words and a 1 for the other, increasing\nthe Hamming distance to d+1 in the new representation.  Since all\npairs of code words have Hamming distance of at least d+1, the\nnew representation also has Hamming distance d+1.\n\n\n\n\n\n \n Question: \nQ. What is the Hamming distance of a SEC-DED code? \n Answer: \n\nA. The Hamming distance of a SEC-DED code is 4."}, {"text": "Title: Summary of Part 4 of the Course \n Text: {Summary of Part 4 of the Course}\n\nWith the exception of control unit design strategies and redundancy \nand coding, most of the material in this part of the course is drawn from\nPatt and Patel Chapters 4 through 7.  You may also want to read Patt and \nPatel's Appendix C for details of their control unit design.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nWe'll start with the easy stuff.  You should recognize all of these terms\nand be able to explain what they mean.  \n For the specific circuits, you \n should be able to draw them and explain how they work.\n(You may skip the *'d terms in Fall 2012.)\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann elements}\n{-}{{}{}\n{}{}{}\n{program counter (PC)}\n{instruction register (IR)}\n{memory address register (MAR)}\n{memory data register (MDR)}\n{processor datapath}\n\n{control signal}\n{instruction processing}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (in an encoded instruction)}\n{operation code (opcode)}\n\n\n{assemblers and assembly code}\n{-}{{}{}\n{}{}{}\n{opcode mnemonic (such as ADD, JMP)}\n{two-pass process}\n\n{symbol table}\n{pseudo-op / directive}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{systematic decomposition}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n no documentation, and advanced topics ... no testing\n\n {logic design optimization}\n {-}{{}{}\n {}{}{}\n {bit-sliced (including multiple bits per slice)}\n \n {pipelined logic}\n {tree-based}\n \n\n{control unit design strategies}\n{-}{{}{}\n{}{}{}\n{control word / microinstruction}\n{sequencing / microsequencing}\n{hardwired control}\n{-}{{}{}\n{}{}{}\n{single-cycle}\n{multi-cycle}\n\n{microprogrammed control}\n {pipelining (of instruction processing)}\n\n\n{error detection and correction\n{--}{{}{}\n{}{}{}\n code/sparse representation\n code word\n bit error\n odd/even parity bit\n Hamming distance between code words\n Hamming distance of a code\n Hamming code\n SEC-DED\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n FIXME ... should write something about critical path, but expecting\n them to do much with it isn't reasonable\n\n {Implement arbitrary Boolean logic, and be able to reason about\n alternative designs in terms of their critical path delays and number\n of gates required to implement.}\n\n{Map RTL (register transfer language) operations into control words\nfor a given processor datapath.}\n\n{Systematically decompose a (simple enough) problem to the level \nof {LC-3} instructions.}\n\n{Encode {LC-3} instructions into machine code.}\n\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n{Test and debug a small program in {LC-3} assembly/machine code.}\n\n{Be able to calculate the Hamming distance of a code/representation.}\n\n{Know the relationships between Hamming distance and the abilities\nto detect and to correct bit errors.}\n\n\n\nWe expect that you will understand the concepts and ideas to the extent\nthat you can do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the role of different types of instructions in allowing\na programmer to express a computation.}\n\n FIXME: ISA design is beyond the scope of this course; just include\n for interest, if at all\n\n {Explain the tradeoffs in different addressing modes so as to motivate\n inclusion of multiple addressing modes in an ISA.}\n\n{Explain the importance of the three types of subdivisions in systematic\ndecomposition (sequential, conditional, and iterative).}\n\n{Explain the process of transforming assembly code into machine code\n(that is, explain how an assembler works, including describing the use of\nthe symbol table).}\n\n{Be able to use parity for error detection, and Hamming codes for\nerror correction.}\n\n\n\nAt the highest level, \nwe hope that, while you do not have direct substantial experience in \nthis regard from our class (and should not expect to be tested on these\nskills), that you will nonetheless be able to begin\nto do the following when designing combinational logic:\n\n{}{{}{}\n{}{}{}\n\n{Design and compare implementations using gates, decoders, muxes, and/or memories \nas appropriate, and including reasoning about the relevant design tradeoffs \nin terms of area and delay.}\n\n{Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based \ndesign, again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Design and compare implementations of processor control units\nusing both hardwired and microprogrammed strategies,\nand again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Understand basic tradeoffs in the sparsity of code words with error \ndetection and correction capabilities.}\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n \n Question: \nQ. What is the role of an assembler? \n Answer: \n\nAn assembler is a program that takes assembly code, which is code written in a human-readable form, and translates it into machine code, which is code that can be run by a computer."}, {"text": "Title: Example: Bit-Sliced Addition \n Text: {Example: Bit-Sliced Addition}\n\nIn this set of notes, we illustrate basic logic design using integer\naddition as an example.  By recognizing and mimicking the structured \napproach used by humans to perform addition, we introduce an important \nabstraction for logic design.  We follow this approach to design an\nadder known as a ripple-carry adder, then discuss some of the \nimplications of the approach and highlight how the same approach can \nbe used in software.  In the next set of notes, we use the same\ntechnique to design a comparator for two integers.\n\n \n Question: \nQ. What is the difference between a ripple-carry adder and a traditional adder? \n Answer: \n\nA. A ripple-carry adder is a type of adder that uses a ripple-carry technique to propagate the carry bit through each stage of the addition process. A traditional adder, on the other hand, does not use this technique and instead propagates the carry bit through each stage in a different way."}, {"text": "Title: One Bit at a Time \n Text: {One Bit at a Time}\n\nMany of the operations that we want to perform on groups of bits can \nbe broken down into repeated operations on individual bits.\n\nWhen we add two binary numbers, for example, we first add the least\nsignificant bits, then move to the second least significant, and so on.\nAs we go, we may need to carry from lower bits into higher bits.\n\nWhen we compare two (unsigned) binary numbers with the same number of bits,\nwe usually start with the most significant bits and move downward in\nsignificance until we find a difference or reach the end of the two \nnumbers.  In the latter case, the two numbers are equal.\n\nWhen we build combinational logic to implement this kind of calculation,\nour approach as humans can be leveraged as an abstraction technique.\nRather than building and optimizing a different Boolean function for \nan 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size\nthat we might want, we can instead design a circuit that adds a single\nbit and passes any necessary information into another copy of itself.\nBy using copies of this { bit-sliced} adder circuit, we can\nmimic our approach as humans and build adders of any size, just as\nwe expect that a human could add two binary numbers of any size.\n\nThe resulting designs are, of course, slightly less efficient than\ndesigns that are optimized for their specific purpose (such as adding\ntwo 17-bit numbers), but the simplicity of the approach makes the\ntradeoff an interesting one.\n\n\n \n Question: \nQ. Q. What is the advantage of using a bit-sliced adder circuit?\nQ. \nQ. A. The advantage of using a bit-sliced adder circuit is that it is much simpler to design than an adder circuit that is optimized for a specific purpose. \n Answer: "}, {"text": "Title: Abstracting the Human Process \n Text: {Abstracting the Human Process}\n\n\nThink about how we as humans add two {N-bit}\nnumbers, A and B.\n\nAn illustration appears to the right, using N=8.\n\nFor now, let's assume that our numbers are stored in an unsigned \nrepresentation.\n\nAs you know, addition for 2's complement is identical except for the\ncalculation of overflow.\n\nWe start adding from the least significant bit and move to the left.\nSince adding two 1s can overflow a single bit, we carry a 1 when\nnecessary into the next column.  Thus, in general, we are actually\nadding three input bits.  The carry from the previous column is usually\nnot written explicitly by humans, but in a digital system\nwe need to write a 0 instead of leaving the value blank.\n\nFocus now on the addition of a single column.  Except for the\nfirst and last bits, which we might choose to handle slightly \ndifferently, the addition process is identical \nfor any column.  We add a carry in bit (possibly 0) with one\nbit from each of our numbers to produce a sum bit and a carry\nout bit for the next column.  Column addition is the task\nthat our bit slice logic must perform.\n\nThe diagram to the right shows an abstract model of our \nadder bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming\nfrom the top or left \nand outputs going to the bottom or right.  Outside\nof the bit slice logic, we index the carry bits using the\n\n\n\n{{file=part2/figs/add-abs.eps,width=1.90in}}\n\n\n\n\nbit number.  The bit slice has C^M provided as an input and \nproduces C^{M+1} as an output.\n\nInternally, we use C_ to denote the carry input,\nand C_ to denote the carry output.\n\nSimilarly, the\nbits A_M and B_M from the numbers A and B are\nrepresented internally as A and B, and the bit S_M produced for\nthe sum S is represented internally as S.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.\n\nThe abstract device for adding three inputs bits and producing\ntwo output bits is called a { full adder}.  You may \nalso encounter the term { half adder}, which adds only two\ninput bits.  To form an {N-bit} adder, we integrate N\ncopies of the full adder---the bit slice that we design next---as \nshown below.  The result is called a { ripple carry adder}\nbecause the carry information moves from the low bits to the high\nbits slowly, like a ripple on the surface of a pond.\n\n{{file=part2/figs/add-integrated.eps,width=5.5in}}\n\n\n \n Question: \nQ. How does a full adder work? \n Answer: \n\nA. A full adder works by adding three input bits and producing two output bits."}, {"text": "Title: Designing the Logic \n Text: {Designing the Logic}\n\nNow we are ready to design our adder bit slice.\nLet's start by writing a truth table for C_ and S,\nas shown on the left below.\n\nTo the right of the truth tables are {K-maps} for each output,\nand equations for each output are then shown to the right of the \n{K-maps}.\n\nWe suggest that you work through identification of the prime implicants \nin the {K-maps} and check your work with the equations.\n\n{\n[t]\n{ccc|cc}\nA& B& C_& C_& S \n0& 0& 0& 0& 0\n0& 1& 0& 0& 1\n0& 0& 1& 0& 1\n0& 1& 1& 1& 0\n1& 0& 0& 0& 1\n1& 0& 1& 1& 0\n1& 1& 0& 1& 0\n1& 1& 1& 1& 1\n\n\n\n{file=part2/figs/add-cin.eps,width=1in}\n{file=part2/figs/add-s.eps,width=1in}\n\n\n{eqnarray*}\nC_&=&A B+A C_+B C_   \nS&=&A B C_+A  {C_+\n&& B {C_+  C_\n&=&A{C_\n{eqnarray*}\n\n}\n\nThe equation for C_ implements a { majority function}\non three bits.  In particular, a carry is produced whenever at least two \nout of the three input bits (a majority) are 1s.  Why do we mention\nthis name?  Although we know that we can build any logic function from\nNAND gates, common functions such as those used to add numbers may\nbenefit from optimization.  Imagine that in some technology, creating\na majority function directly may produce a better result than implementing\nsuch a function from logic gates.  In such a case, we want the \nperson designing the circuit to know that can make use of such an \nimprovement.\n\nWe rewrote the equation for S to make use of the XOR operation for \na similar reason: the implementation of XOR gates from transistors\nmay be slightly better than the implementation of XOR based on NAND gates.\nIf a circuit designer provides an optimized variant of XOR, we want\nour design to make use of the optimized version.\n\n{{file=part2/figs/adder-bit.eps,width=2.55in}\n{file=part2/figs/adder-bit-nn.eps,width=2.55in}}\n\nThe gate diagrams above implement a single bit slice for an adder.\nThe version on\nthe left uses AND and OR gates (and an XOR for the sum), while the\nversion on the right uses NAND gates, leaving the XOR as an XOR.\n\nLet's discuss the design in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\nFor each bit, we need three {2-input} NAND gates, one {3-input}\nNAND gate, and a {3-input} XOR gate (a big gate; around 30 transistors).\n\nFor speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.  \n\nWe can thus estimate our design's\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer.\nHere we have two gate delays from any of the inputs\nto the C_ output.  The XOR gate may be a little slower, but\nnone of its inputs come from other gates anyway.\n\nWhen we connect multiple copies of our bit slice logic together to \nform an adder, the A and B inputs\nto the outputs is not as important as the delay from C_\nto the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis---this propagation delay\ngives rise to the name ``ripple carry.''\nLooking again at the diagram, \nnotice that we have two gate delays from C_\nto C_.\nThe total delay for an {N-bit} comparator based on this\nimplementation is thus two gate delays per bit, \nfor a total of 2N gate delays.\n\n\n \n Question: \nQ. What are the inputs and outputs of a bit slice? \n Answer: \n\nA. The inputs to a bit slice are A and B, and the outputs are S and C_."}, {"text": "Title: Adders and Word Size\\vspace12pt \n Text: {Adders and Word Size}\n\n\nNow that we know how to build an {N-bit} adder, we can add\nsome detail to the diagram that we drew when we \nintroduced 2's complement back in Notes Set 1.2, as shown to the right.\n\nThe adder is important enough to computer systems to merit its own\nsymbol in logic diagrams, which is shown to the right with the inputs\nand outputs from our design added as labels.  The text in the middle\nmarking the symbol as an adder is only included for clarity: { any time \nyou see a symbol of the shape shown to the right, it is an adder} (or \nsometimes a device that can add and do other operations).  The width \nof the operand input and output lines then tells you the size of the \nadder.\n\n\n{file=part2/figs/adder-trad.eps,width=1.3in}\n\n\nYou may already know that most computers have a { word size}\nspecified as part of the Instruction Set Architecture.  The word\nsize specifies the number of bits in each operand when the computer\nadds two numbers, and is often used widely within the \nmicroarchitecture as well (for example, to decide the number of \nwires to use when moving bits around).  Most desktop and laptop machines\nnow have a word size of 64 bits, but many phone processors (and\ndesktops/laptops a few years ago) use a {32-bit} word size.\nEmbedded microcontrollers may use a {16-bit} or even \nan {8-bit} word size.\n\n\nHaving seen how we can build an {N-bit} adder from simple\nchunks of logic operating on each pair of bits, you should not have\nmuch difficulty in understanding the diagram to the right.\n\nIf we start with a design for an {N-bit} adder---even if that\ndesign is not built from bit slices, but is instead optimized for\nthat particular size---we can create a {2N-bit} adder by \nsimply connecting two copies of the {N-bit} adder.  We give\nthe adder for the less significant bits (the one on the right\nin the figure) an initial carry of 0,\nand pass the carry produced by the adder for the less significant\nbits into the carry input of the adder for the more significant\nbits.  We calculate overflow based on the results of the adder\nfor more significant bits (the one on the left in the figure), \nusing the method appropriate to the \ntype of operands we are adding (either unsigned or 2's complement).\n\n\n{file=part2/figs/adder-x2.eps,width=2.15in}\n\n\nYou should also realize that this connection need not be physical.\nIn other words, if a computer has an {N-bit} adder, it can\nhandle operands with 2N bits (or 3N, or 10N, or 42N) by\nusing the {N-bit} adder repeatedly, starting with the\nleast significant bits and working upward until all of the bits\nhave been added.  The computer must of course arrange to have the\noperands routed to the adder a few bits at a time, and must\nensure that the carry produced by each addition is then delivered to\nthe carry input (of the same adder!) for the next addition.\nIn the coming months, you will learn how to design hardware that\nallows you to manage bits in this way, so that by the end of our\nclass, you will be able to design a simple computer on your own.\n\n\n\n\n\n \n Question: \nQ. What is an adder? \n Answer: \n\nA.\n\nAn adder is a device that can add two numbers.  The adder is important enough to computer systems to merit its own symbol in logic diagrams, which is shown to the right with the inputs and outputs from our design added as labels.  The width of the operand input and output lines then tells you the size of the adder."}, {"text": "Title: Summary of Part 2 of the Course \n Text: {Summary of Part 2 of the Course}\n\nThese notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.\n\nThe difficulty of learning depends on the type of task involved.\nRemembering new terminology is relatively easy, while applying\nthe ideas underlying design decisions shown by example to new problems \nposed as human tasks is relatively hard.\n\nIn this short summary, we give you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nWe'll start with the skills, and leave the easy stuff for the next page.\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Design a CMOS gate for a simple Boolean function from n-type \nand p-type transistors.}\n\n{Apply DeMorgan's laws repeatedly to simplify the form of\nthe complement of a Boolean expression.}\n\n{Use a K-map to find a reasonable expression for a Boolean function (for\nexample, in POS or SOP form with the minimal number of terms).}\n\n{More generally, translate Boolean logic functions among \nconcise algebraic, truth table, K-map, and canonical (minterm/maxterm) forms.}\n\n\n\nWhen designing combinational logic, we expect you to be able to apply\nthe following design strategies:\n\n{}{{}{}\n{}{}{}\n\n{Make use of human algorithms \n(for example, multiplication from addition).}\n\n{Determine whether a bit-sliced approach is applicable, and, if so,\nmake use of one.}\n\n{Break truth tables into parts so as to solve each part of a function \nseparately.}\n\n{Make use of known abstractions (adders, comparators, muxes, or other\nabstractions available to you) to simplify the problem.}\n\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Understand and be able to reason at a high-level about circuit design\ntradeoffs between area/cost and performance (and to know that power is also \nimportant, but we haven't given you any quantification methods).}\n\n{Understand the tradeoffs typically made to develop bit-sliced \ndesigns---typically, bit-sliced designs are simpler but bigger and \nslower---and how one can develop variants between the extremes of\nthe bit-sliced approach and optimization of functions specific\nto an {N-bit} design.}\n\n{Understand the pitfalls of marking a function's value as ``don't care'' \nfor some input combinations, and recognize that implementations do not \nproduce ``don't care.''}\n\n{Understand the tradeoffs involved in selecting a representation for\ncommunicating information between elements in a design, such as the bit \nslices in a bit-sliced design.}\n\n{Explain the operation of a latch or a flip-flop, particularly in \nterms of the bistable states used to hold a bit.}\n\n{Understand and be able to articulate the value of the clocked \nsynchronous design abstraction.}\n\n\n\n\n\n\n\nYou should recognize all of these terms\nand be able to explain what they mean.  For the specific circuits, you \nshould be able to draw them and explain how they work.\n\nActually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  \n\n[t]\n{}{{}{}\n{}{}{}\n\n{Boolean functions and logic gates}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n\n\n{majority function}\n\n\n{specific logic circuits}\n{-}{{}{}\n{}{}{}\n{full adder}\n{half adder}\n{ripple carry adder}\n N-to-M multiplexer (mux)\n N-to-2N decoder\n{{- latch}}\n{{R-S latch}}\n{gated D latch}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation of a positive edge-triggered D flip-flop}\n{dual-latch implementation of a positive edge-triggered D flip-flop}\n{(bidirectional) shift register}\n{register supporting parallel load}\n\n\n{design metrics}\n{-}{{}{}\n{}{}{}\n\n\n\n\n{power, area/cost, performance}\n{computer-aided design (CAD) tools}\n{gate delay}\n\n\n{general math concepts}\n{-}{{}{}\n{}{}{}\n{canonical form}\n{domain of a function}\n{{N-dimensional} hypercube}\n\n\n{tools for solving logic problems}\n{-}{{}{}\n{}{}{}\n{truth table}\n{Karnaugh map (K-map)}\n\n{prime implicant}\n{bit-slicing}\n{timing diagram}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{device technology}\n{-}{{}{}\n{}{}{}\n{complementary metal-oxide semiconductor (CMOS)}\n{field effect transistor (FET)}\n{transistor gate, source, drain}\n\n\n{Boolean logic terms}\n{-}{{}{}\n{}{}{}\n\n{algebraic properties}\n{dual form, principle of duality}\n{sum, product}\n{minterm, maxterm}\n{sum-of-products (SOP)}\n{product-of-sums (POS)}\n{canonical sum/SOP form}\n{canonical product/POS form}\n{logical equivalence}\n\n\n{digital systems terms}\n{-}{{}{}\n{}{}{}\n{word size}\n{{N-bit} Gray code}\n{combinational/combinatorial logic}\n{-}{{}{}\n{}{}{}\n{two-level logic}\n{``don't care'' outputs (x's)}\n\n{sequential logic}\n{-}{{}{}\n{}{}{}\n\n{active low input}\n{set a bit (to 1)}\n{reset a bit (to 0)}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation}\n{dual-latch implementation}\n{positive edge-triggered}\n\n{clock signal}\n{-}{{}{}\n{}{}{}\n{square wave}\n{rising/positive clock edge}\n{falling/negative clock edge}\n{clock gating}\n\n{clocked synchronous sequential circuit}\n{parallel/serial load of register}\n FIXME?  too informal to ask them to remember it\n {glue logic}\n{logical/arithmetic/cyclic shift}\n\n\n\n\n\n\n\n\n\n{   }  blank 3rd page\n\n\n\n\n\n\n\n \n Question: \nQ. What is the difference between a latch and a flip-flop? \n Answer: \n\nA. A latch is a circuit that is used to store a bit of information, while a flip-flop is a type of latch that is used to store a bit of information in a way that allows it to be changed."}, {"text": "Title: Registers \n Text: \n\nThis set of notes introduces registers, an abstraction used for \nstorage of groups of bits in digital systems.  We introduce some\nterminology used to describe aspects of register design and\nillustrate the idea of a shift register.  The registers shown here\nare important abstractions for digital system design.\n\n { In the Fall 2012 offering of our course, we will cover this\n material on the third midterm.}\n\n\n \n Question: \nQ. What is a register?\nQ. \nQ. A register is an abstraction used for storage of groups of bits in digital systems. \n Answer: "}, {"text": "Title: Registers \n Text: \n\n\nA { register} is a storage element composed from one or more\nflip-flops operating on a common clock.\n\nIn addition to the flip-flops,\nmost registers include logic to control the bits stored by the register.\n\nFor example, D flip-flops \ncopy their inputs at the rising edge of each clock cycle,\ndiscarding whatever bits they have stored before the rising edge\n(in the previous clock cycle).\n\nTo enable a flip-flop to retain its value, we might try to hide the \nrising edge of the clock from the flip-flop, as shown to the right.\n\nThe LOAD input controls the clock signals through a method known as\n{ clock gating}.\n\n\n{{file=part2/figs/lec16-1a.eps,width=2.45in}}\n{file=part2/figs/lec16-1b.eps,width=3.4in}\n\n\nWhen LOAD is high, the circuit reduces to a\nregular D flip-flop.  When LOAD is low, the flip-flop clock \ninput, c, is held high, and the flip-flop stores its \ncurrent value.\n\nThe problems with clock gating are twofold.  First, adding logic to\nthe clock path introduces clock skew, which may cause timing problems\nlater in the development process (or, worse, in future projects that\n\nuse your circuits as components).  Second, in the design shown above,\nthe LOAD signal\ncan only be lowered while the clock is high to prevent spurious rising\nedges from causing incorrect behavior, as shown in the timing diagram.\n\n\nA better approach is to use a mux and a feedback loop from the \nflip-flop's output, as shown in the figure to the right.  \n\nWhen LOAD is low, the mux selects the feedback line, and the \nregister reloads its current value.  \n\nWhen LOAD is high, the mux selects the IN input, and the register \nloads a new value.  \n\nThe result is similar to a gated D latch with distinct write enable \nand clock lines.\n\n\n{file=part2/figs/lec16-2.eps,width=2in}\n\n\n\nWe can use this extended flip-flop as a bit slice for a multi-bit register.\n\nA four-bit register of this type\nis shown to the right.  Four data {lines---one} for each\n{bit---enter} the registers from the top of the figure.  \nWhen LOAD is low, the logic copies each flip-flop's value back to its\ninput,\n\n\n{file=part2/figs/lec16-3.eps,width=5in}\n\n\nand the IN input lines are ignored.  When LOAD is high,\nthe muxes forward each IN line to the corresponding flip-flop's D\ninput, allowing the register to load the new {4-bit} value.\nThe use of one input line per bit to load a multi-bit\nregister in a single cycle is termed a { parallel load}.\n\n \n Question: \nQ. How does the extended flip-flop work? \n Answer: \n\nA. The extended flip-flop uses a mux and a feedback loop from the flip-flop's output to reload its current value when LOAD is low. When LOAD is high, the mux selects the IN input, and the register loads a new value."}, {"text": "Title: Shift Registers \n Text: {Shift Registers}\n\n\nCertain types of registers include logic to manipulate data held\nwithin the register.  A { shift register} is an important example\nof this\n\n\n{file=part2/figs/lec16-4.eps,width=5in}\n\n\ntype.  The simplest shift register is a series of D flip-flops,\nwith the output of each attached to the input of the next, as shown to the\nright above.  In the circuit shown, a serial input SI accepts a single bit \nof data per cycle and delivers the bit four cycles later to a serial \noutput SO.  Shift registers serve many purposes in modern systems, from the\nobvious uses of providing a fixed delay and performing bit shifts for\nprocessor arithmetic to rate matching between components and reducing\nthe pin count on programmable logic devices such as field programmable\ngate arrays (FPGAs), the modern form of the programmable logic array\nmentioned in the textbook.\n\nAn example helps to illustrate the rate matching problem: \nhistorical I/O buses used fairly slow clocks, as they had to\ndrive signals and be arbitrated over relatively long distances.\nThe Peripheral Control\nInterconnect (PCI) standard, for example, provided for 33 and 66 MHz\nbus speeds.  To provide adequate data rates, such buses use many wires\nin parallel, either 32 or 64 in the case of PCI.  In contrast, a\nGigabit Ethernet (local area network) signal travelling over a fiber\nis clocked at 1.25 GHz, but sends only one bit per cycle.  Several\nlayers of shift registers sit between the fiber and the I/O bus to\nmediate between the slow, highly parallel signals that travel over the\nI/O bus and the fast, serial signals that travel over the \nfiber.  The latest variant of PCI, PCIe (e for ``express''),\nuses serial lines at much higher clock rates.\n\nReturning to the figure above, imagine that the outputs Q_i feed\ninto logic clocked at 1/4^ the rate of the shift register \n(and suitably synchronized).  Every four cycles, the flip-flops fill\nup with another four bits, at which point the outputs are read in\nparallel.  The shift register shown can thus serve to transform serial\ndata to {4-bit-parallel} data at one-quarter the clock speed.\nUnlike the registers discussed earlier, the shift register above does\nnot support parallel load, which prevents it from transforming a slow,\nparallel stream of data into a high-speed serial stream.  The use of\n{ serial load} requires N cycles for an {N-bit}\nregister, but can reduce the number of wires needed to support the\noperation of the shift register.  How would you add support for\nparallel load?  How many additional inputs would be necessary?\n\nThe shift register above also shifts continuously, and cannot store a \nvalue.  A set of muxes, analogous to those that we used to control \nregister loading, can be applied to control shifting, as shown \nbelow.\n\n{{file=part2/figs/lec16-5.eps,width=5.3in}}\n\nUsing a {4-to-1} mux, we can construct a shift\nregister with additional functionality.  The bit slice at the top\nof the next page allows us to build a { bidirectional shift register} with \nparallel load capability and the ability to retain its value indefinitely.\nThe two-bit control input C uses a representation that\nwe have chosen for the four operations supported by our shift register, \nas shown in the table below the bit slice design.\n\n\nThe bit slice allows us to build {N-bit} shift registers by\nreplicating the slice and adding a fixed amount of ``{ glue logic}.''\nFor example, the figure below represents a {4-bit} bidirectional \nshift register constructed in this way.  The mux\nused for the SO output logic is the glue logic needed in addition\nto the four bit slices.\n\nAt each rising clock edge, the action specified by C_1C_0 is taken.  \nWhen C_1C_0=00, the\nregister holds its current value, with the register\nvalue appearing on\nQ[3:0] and each flip-flop feeding its output back into its input.\nFor C_1C_0=01, the shift register shifts left: the serial input,\nSI, is fed into flip-flop 0, and Q_3 is passed to the serial\noutput, SO.  Similarly, when C_1C_0=11, the shift register shifts\nright: SI is fed into flip-flop 3, and Q_0 is passed to SO.\nFinally, the case C_1C_0=10 causes all flip-flops to accept new\nvalues from IN[3:0], effecting a parallel load.\n\n\n{file=part2/figs/lec16-6.eps,width=2.3in}\n{c|c}\nC_1C_0& meaning \n00& retain current value\n01& shift left (low to high)\n10& load new value (from IN)\n11& shift right (high to low)\n\n{-4pt}\n\n{{file=part2/figs/lec16-7.eps,width=5.2in}}\n\nSeveral specialized shift operations are used to support data\nmanipulation in modern processors (CPUs).  Essentially, these\nspecializations dictate the glue logic for a shift\nregister as well as the serial input value.  The simplest is a {\nlogical shift}, for which SI is hardwired to 0: incoming\nbits are always 0.  A { cyclic shift} takes SO and feeds it\nback into SI, forming a circle of register bits through which the\ndata bits cycle.\n\nFinally, an { arithmetic shift} treats the shift register contents\nas a number in 2's complement form.  For non-negative numbers and left\nshifts, an arithmetic shift is the same as a logical\nshift.  When a negative number is arithmetically shifted to\nthe right, however, the sign bit is retained, resulting in a function\nsimilar to division by two.  The difference lies in the rounding\ndirection.  Division by two rounds towards zero in most \nprocessors: -5/2 gives -2.\nArithmetic shift right rounds away from zero for negative numbers (and\ntowards zero for positive numbers): -5>>1 gives -3.  We transform our\nprevious shift register into one capable of arithmetic shifts by\neliminating the serial input and feeding the most significant bit,\nwhich represents the sign in 2's complement form, back into itself for\nright shifts, as shown below.  The bit shifted in for left shifts\nhas been hardwired to 0.\n\n{{file=part2/figs/lec16-8.eps,width=5.2in}}\n\n\n\n \n Question: \nQ. What is a shift register? \n Answer: \n\nA shift register is a register that includes logic to manipulate data held within the register. A shift register is an important example of this type."}, {"text": "Title: Boolean Properties and Don't Care Simplification \n Text: {Boolean Properties and Don't Care Simplification}\n\nThis set of notes begins with a brief illustration of a few properties\nof Boolean logic, which may be of use to you in manipulating algebraic\nexpressions and in identifying equivalent logic functions without resorting \nto truth tables.\n\nWe then discuss the value of underspecifying a logic function so as\nto allow for selection of the simplest possible implementation.\n\nThis technique must be used carefully to avoid incorrect behavior,\nso we illustrate the possibility of misuse with an example, then\ntalk about several ways of solving the example correctly.\n\nWe conclude by generalizing the ideas in the example to several\nimportant application areas and talking about related problems.\n\n \n Question: \nQ. What is the value of underspecifying a logic function? \n Answer: \n\nA. The value of underspecifying a logic function is that it allows for selection of the simplest possible implementation."}, {"text": "Title: Logic Properties \n Text: {Logic Properties}\n\nTable  (on the next page)\nlists a number of properties of Boolean logic.\nMost of these are easy to derive from our earlier definitions, but\na few may be surprising to you.  In particular, in the algebra of\nreal numbers, multiplication\ndistributes over addition, but addition does not distribute over\nmultiplication.  For example, 3{(4+7)}=(3)+(3),\nbut 3+(4)=(3+4){(3+7)}.  In Boolean algebra,\nboth operators distribute over one another, as indicated in\nTable .  The consensus properties may also be \nnonintuitive.  Drawing a {K-map} may help you understand the\nconsensus property on the right side of the table.  For the \nconsensus variant on the left side of the table,\nconsider that since either A or  must be 0,\neither B or C or both must be 1 for the first two factors on the left\nto be 1 when ANDed together.  But in that case, the third factor \nis also 1, and is thus redundant.\n\nAs mentioned previously, Boolean\nalgebra has an elegant symmetry known as a duality, in which any\nlogic statement (an expression or an equation) is related to a\nsecond logic statement.\n\nTo calculate the { dual form} of a Boolean expression or equation, \nreplace 0 with 1, replace 1 with 0, \nreplace AND with OR, and\nreplace OR with AND.\n\n{ Variables are not changed when finding the dual form.}\n\nThe dual form of a dual form is the original logic statement.\n\nBe careful when calculating a dual form: our convention for ordering \narithmetic operations is broken by the exchange, so you may want\nto add explicit parentheses before calculating the dual.  For\nexample, the dual of AB+C is not A+BC.\nRather, the dual of AB+C is (A+B)C.\n\n{ Add parentheses as necessary when calculating a dual form to ensure\nthat the order of operations does not change.}\n\nDuality has several useful practical applications.  First, the \n{ principle of duality} states that any theorem or identity has \nthe same truth value in dual form (we do not prove the principle\nhere).\n\nThe rows of Table  are organized according to this\nprinciple: each row contains two equations that are the duals \nof one another.  \n However, we rename variables freely to\n simplify the dual form.  For example, taking the dual of 1+A=1,\n we obtain 0=0.  But we created variable A \n solely for the purpose of expressing the property, so we might\n instead have called the complementary value A.  We rename\n (using the same name!) to obtain 0=0, as shown in the table.\n\nSecond, the dual form is useful when designing certain types of logic,\nsuch as the networks of transistors connecting the output of a CMOS\ngate to high voltage and ground.  If you look at the gate designs in\nthe textbook (and particularly those in the exercises),\nyou will notice that these networks are duals.  \n\nA function/expression is not a theorem nor an identity,\nthus the principle of duality does not apply to the dual of an\nexpression.\n\nHowever, if you treat the value 0 as ``true,'' the dual form of an\nexpression has the same truth values as the original (operating with\nvalue 1 as ``true'').\n\nFinally, you can calculate the complement of a Boolean function\n(any expression) by calculating the dual form and then complementing\neach variable.\n\n \n Question: \nQ. The principle of duality states that any theorem or identity has the same truth value in dual form. What does this mean? \n Answer: \n\nA. The principle of duality states that any theorem or identity has the same truth value in dual form. This means that any theorem or identity has the same truth value in its dual form."}, {"text": "Title: Choosing the Best Function \n Text: {Choosing the Best Function}\n\nWhen we specify how something works using a human language, we\nleave out details.  Sometimes we do so deliberately, assuming that a\nreader or listener can provide the details themselves: ``Take me to the \nairport!''\nrather than ``Please bend your right arm at the elbow and shift your\nright upper arm forward so as to place your hand near the ignition key.\nNext, ...''  \n\n\n{\n{|lll|}\n1+A=1& 0=0&\n1=A& 0+A=A&\nA+A=A& A=A&\nA=0& A+=1&\n{A+B}= &\n=+& DeMorgan's laws\n(A+B)C=AC+BC&\nA B+C=(A+C)(B+C)&distribution\n(A+B)(+C)(B+C)=(A+B)(+C)&\nA B+ C+B C=A B+ C& consensus\n\n}\n{Boolean logic properties.  The two columns are dual forms of\none another.}\n\n\n\nYou know the basic technique for implementing a Boolean function\nusing { combinational logic}: use a {K-map} to identify a\nreasonable SOP or POS form, draw the resulting design, and perhaps\nconvert to NAND/NOR gates.\n\n\n\nWhen we develop combinational logic designs, we may also choose to\nleave some aspects unspecified.  In particular, the value of a\nBoolean logic function to be implemented may not matter for some\ninput combinations.  If we express the function as a truth table,\nwe may choose to mark the function's value for some input combinations \nas ``{ don't care},'' which is written as ``x'' (no quotes).\n\nWhat is the benefit of using ``don't care'' values?  \n\nUsing ``don't care'' values allows you to choose from among several\npossible logic functions, all of which produce the desired results\n(as well as some combination of 0s and 1s in place of the ``don't\ncare'' values).  \n\nEach input combination marked as ``don't care'' doubles the number\nof functions that can be chosen to implement the design, often enabling \nthe logic needed for implementation to be simpler.\n\n\nFor example, the {K-map} to the right specifies a function F(A,B,C)\nwith two ``don't care'' entries.  \n\nIf you are asked to design combinational logic for this function,\nyou can\nchoose any values for the two ``don't care'' entries.  When identifying\nprime implicants, each ``x'' can either be a 0 or a 1.\n\n\n\n\n\n\nDepending on the choices made for the x's, we obtain one of \nthe following four functions:\n\n{eqnarray*}\nF&=& B+B C\nF&=& B+B C+A  \nF&=&B\nF&=&B+A \n{eqnarray*}\n\n\n\n\n\nGiven this set of choices, a designer typically chooses the third: F=B,\nwhich corresponds to the {K-map} shown to the right of the \nequations.  The design then \nproduces F=1 when A=1, B=1, and C=0 (ABC=110), \nand produces F=0 when A=1, B=0, and C=0 (ABC=100).\nThese differences are marked with shading and green italics in the new\n{K-map}.  No implementation ever produces an ``x.''\n\n \n Question: \nQ. Why is it beneficial to use \"don't care\" values when implementing a Boolean logic function? \n Answer: \n\nA. Using \"don't care\" values allows you to choose from among several possible logic functions, all of which produce the desired results. This can often enable the logic needed for implementation to be simpler."}, {"text": "Title: Caring about Don't Cares \n Text: {Caring about Don't Cares}\n\nWhat can go wrong?\n\nIn the context of a digital system, unspecified details\nmay or may not be important.  However, { any implementation of \na specification \nimplies decisions} about these details, so decisions should only be left \nunspecified if any of the possible answers is indeed acceptable.\n\nAs a concrete example, let's design logic to control an ice cream \ndispenser.  The dispenser has two flavors, lychee and mango, but\nalso allows us to create a blend of the two flavors.\nFor each of the two flavors, our logic must output two bits\nto control the amount of ice cream that comes out of the dispenser.\nThe two-bit C_L[1:0] output of our logic must specify the number \nof half-servings of lychee ice cream as a binary number, and\nthe two-bit C_M[1:0] output must specify the number of \nhalf-servings of mango ice cream.  Thus, for either flavor,\n00 indicates none of that flavor,\n01 indicates one-half of a serving, and \n10 indicates a full serving.\n\nInputs to our logic will consist of three buttons: an L button to\nrequest a serving of lychee ice cream, a B button to request a\nblend---half a serving of each flavor, and an M button to request\na serving of mango ice cream.  Each button produces a 1 \nwhen pressed and a 0 when not pressed.\n\n\n\nLet's start with the assumption that the user only presses one button\nat a time.  In this case, we can treat input combinations in which\nmore than one button is pressed as ``don't care'' values in the truth\ntables for the outputs.  K-maps for all four output bits appear below.\nThe x's indicate ``don't care'' values.\n\n\n\nWhen we calculate the logic function for an output, each ``don't care''\nvalue can be treated as either 0 or 1, whichever is more convenient\nin terms of creating the logic.  In the case of C_M[1], for \nexample, we can treat the three x's in the ellipse as 1s, treat the x\noutside of the ellipse as a 0, and simply\nuse M (the implicant represented by the ellipse) for C_M[1].  The\nother three output bits are left as an exercise, although the result \nappears momentarily.\n\n\nThe implementation at right takes full advantage of the ``don't care''\nparts of our specification.  In this case, we require no logic at all;\nwe need merely connect the inputs to the correct outputs.  Let's verify\nthe operation.  We have four cases to consider.  First, if none of the \nbuttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00 \nand C_L=00).  Second, if we request lychee ice cream (LBM=100), the\noutputs are C_L=10 and C_M=00, so we get a full serving of lychee\nand no mango.  Third, if we request a blend (LBM=010), the outputs\nare C_L=01 and C_M=01, giving us half a serving of each flavor.\nFinally, if we request mango ice cream (LBM=001), we get no lychee\nbut a full serving of mango.\n\n\n\n\n\nThe K-maps for this implementation appear below.  Each of\nthe ``don't care'' x's from the original design has been replaced with\neither a 0 or a 1 and highlighted with shading and green italics.\nAny implementation produces \neither 0 or 1 for every output bit for every possible input combination.\n\n{{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}\n\nAs you can see, leveraging ``don't care'' output bits can sometimes\nsignificantly simplify our logic.  In the case of this example, we were\nable to completely eliminate any need for gates!  Unfortunately, \nthe resulting implementation may sometimes produce unexpected results. \n\nBased on the implementation, what happens if a user presses more\nthan one button?  The ice cream cup overflows!\n\nLet's see why.\n\nConsider the case LBM=101, in which we've pressed both the lychee and\nmango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a \nfull serving of each flavor, or two servings total.  Pressing other \ncombinations may have other repercussions as well.\nConsider pressing lychee and blend (LBM=110).  The outputs are then\nC_L=11 and C_M=01.  Hopefully the dispenser simply gives us one\nand a half servings of lychee and a half serving of mango.  However,\nif the person who designed the dispenser assumed that no one would ever\nask for more than one serving, something worse might happen.  In other\nwords, giving an input of C_L=11 to the ice cream dispenser may lead to\nother unexpected behavior if its designer decided that that input \npattern was a ``don't care.''\n\nThe root of the problem is that { while we don't care about the value of\nany particular output marked ``x'' for any particular input combination,\nwe do actually care about the relationship between the outputs}.  \n\nWhat can we do?  When in doubt, it is safest to make \nchoices and to add the new decisions to the specification rather than \nleaving output values specified as ``don't care.''\n\nFor our ice cream dispenser logic, rather than leaving the outputs \nunspecified whenever a user presses more than one button, we could \nchoose an acceptable outcome for each input combination and \nreplace the x's with 0s and 1s.  We might, for example, decide to\nproduce lychee ice cream whenever the lychee button is pressed, regardless\nof other buttons (LBM=1xx, which means that we don't care about the\ninputs B and M, so LBM=100, LBM=101, LBM=110, or LBM=111).  \nThat decision alone covers three of the\nfour unspecified input patterns.  We might also decide that when the \nblend and mango buttons are pushed together (but without the lychee\nbutton, LBM=011), our logic produces a blend.  \n\nThe resulting K-maps are shown below, again with shading and green italics \nidentifying\nthe combinations in which our original design specified ``don't care.''\n\n{{file=part2/figs/CLhigh-priority.eps,width=1.00in}{file=part2/figs/CLlow-priority.eps,width=1.00in}{file=part2/figs/CMhigh-priority.eps,width=1.00in}{file=part2/figs/CMlow-priority.eps,width=1.00in}}\n\n\nThe logic in the dashed box to the right implements the set of choices\njust discussed, and matches the K-maps above.\n\nBased on our additional choices, this implementation enforces\na strict priority scheme on the user's button presses.\n\nIf a user requests lychee, they can also press either or\nboth of the other buttons with no effect.  The lychee button has\npriority.  Similarly, if the user does not press lychee, but press-\n\n\n\n\n\nes the blend button, pressing the mango button at the same time has no\neffect.  Choosing mango requires that no other buttons be pressed.\nWe have thus chosen a prioritization order for the buttons and imposed \nthis order on the design.\n\nWe can view this same implementation in another way.\n\nNote the one-to-one correspondence between inputs (on the left) and \noutputs (on the right) for the dashed box.\n\nThis logic takes the user's button presses and chooses at most one of\nthe buttons to pass along to our original controller implementation \n(to the right of the dashed box).\n\nIn other words, rather than thinking of the logic in the dashed\nbox as implementing a specific set of decisions, we can think of the\nlogic as cleaning up the inputs to ensure that only valid\ncombinations are passed to our original implementation.\n\nOnce the inputs are cleaned up, the original implementation is \nacceptable, because input combinations containing more than a \nsingle 1 are in fact impossible.\n\nStrict prioritization is one useful way to clean up our inputs.\n\nIn general, we can design logic to map each\nof the four undesirable input patterns into one of the permissible \ncombinations (the\nfour that we specified explicitly in our original design, with LBM \nin the set ).  Selecting a prioritization scheme\nis just one approach for making these choices in a way\nthat is easy for a user to understand and \nis fairly easy to implement.\n\n\nA second simple approach is to ignore illegal\ncombinations by mapping them into the ``no buttons pressed'' \ninput pattern.\n\nSuch an implementation appears to the right, laid out to show that\none can again view the logic in the dashed box either as cleaning up \nthe inputs (by mentally grouping the logic with the inputs) or as a specific \nset of choices for our ``don't care'' output values (by grouping the \nlogic with the outputs).\n\nIn either case, the logic shown \nenforces our as-\n\n\n\n\n\nsumptions in a fairly conservative way:\nif a user presses more than one button, the logic squashes all button\npresses.  Only a single 1 value at a time can pass through to \nthe wires on the right of the figure.\n\n\n\nFor completeness, the K-maps corresponding to this implementation are given\nhere.\n\n{{file=part2/figs/CLhigh-conserve.eps,width=1.00in}{file=part2/figs/CLlow-conserve.eps,width=1.00in}{file=part2/figs/CMhigh-conserve.eps,width=1.00in}{file=part2/figs/CMlow-conserve.eps,width=1.00in}}\n\n\n \n Question: \nQ. What is the difference between a \"don't care\" output bit and an output bit that is specified as 0 or 1? \n Answer: \n\nA. A \"don't care\" output bit can be either 0 or 1, while an output bit that is specified as 0 or 1 must be one or the other."}, {"text": "Title: Generalizations and Applications* \n Text: {Generalizations and Applications*}\n\nThe approaches that we illustrated to clean up the input signals to\nour design have application in many areas.  The ideas in this \nsection are drawn from the field and are sometimes the subjects of \nlater classes, but { are not exam material for our class.}\n\nPrioritization of distinct\ninputs is used to arbitrate between devices attached to a\nprocessor.  Processors typically execute much more quickly than do devices.\nWhen a device needs attention, the device signals the processor\nby changing the voltage on an interrupt line (the name comes from the\nidea that the device interrupts the processor's current activity, such\nas running a user program).  However, more than\none device may need the attention of the processor simultaneously, so\na priority encoder is used to impose a strict order on the devices and\nto tell the processor about their needs one at a time.\n\nIf you want to learn more about this application, take ECE391.\n\nWhen components are designed together, assuming that some input patterns\ndo not occur is common practice, since such assumptions can dramatically\nreduce the number of gates required, improve performance, reduce power\nconsumption, and so forth.  As a side effect, when we want to test a\nchip to make sure that no defects or other problems prevent the chip\nfrom operating correctly, we have to be careful so as not to ``test''\nbit patterns that should never occur in practice.  Making up random bit\npatterns is easy, but can produce bad results or even destroy the chip\nif some parts of the design have assumed that a combination produced \nrandomly can never occur.  To avoid these problems, designers add extra\nlogic that changes the disallowed patterns into allowed patterns, just\nas we did with our design.  The use of random bit patterns is common in\nBuilt-In Self Test (BIST), and so the process of inserting extra logic\nto avoid problems is called BIST hardening.  BIST hardening can add\n{10-20} additional logic to a design.\n\nOur graduate class on digital system testing, ECE543, covers this\nmaterial, but has not been offered recently.\n\n\n\n\n\n \n Question: \nQ. Who is the audience for this text? \n Answer: \n\nA. The audience for this text is primarily students in an introductory digital design class."}, {"text": "Title: Optimizing Logic Expressions \n Text: {Optimizing Logic Expressions}\n\nThe second part of the course covers digital design more\ndeeply than does the textbook.  The lecture notes will explain the\nadditional material, and we will provide further examples in lectures\nand in discussion sections.  Please let us know if you need further\nmaterial for study.\n\nIn the last notes, we introduced Boolean logic operations and showed\nthat with AND, OR, and NOT, we can express any Boolean function on any\nnumber of variables.\n\nBefore you begin these notes, please read the first two sections \nin Chapter 3 of the textbook,\nwhich discuss the operation of { complementary metal-oxide semiconductor}\n({ CMOS}) transistors, illustrate how gates implementing the\nAND, OR, and NOT operations can be built using transistors,\nand introduce DeMorgan's laws. \n\nThis set of notes exposes you to a mix of techniques,\nterminology, tools, and philosophy.  Some of the material is not\ncritical to our class (and will not be tested), but is useful for\nyour broader education, and may help you in later classes.  The value\nof this material has changed substantially in the last couple of decades,\nand particularly in the last few years, as algorithms for tools\nthat help with hardware design have undergone rapid advances.  We\ntalk about these issues as we introduce the ideas.\n\nThe notes begin with a discussion of the ``best'' way to\nexpress a Boolean function and some techniques used historically\nto evaluate such decisions.\n\nWe next introduce the terminology necessary to understand manipulation\nof expressions, and use these terms to explain the Karnaugh map, or\n{K-map}, a tool that we will use for many purposes this semester.\n\nWe illustrate the use of {K-maps} with a couple of\nexamples, then touch on a few important questions and useful\nways of thinking about Boolean logic.\n\nWe conclude with a discussion of the general problem of \nmulti-metric optimization, introducing some ideas and approaches\nof general use to engineers.\n\n\n \n Question: \nQ. What is the best way to express a Boolean function? \n Answer: \n\nA. The best way to express a Boolean function is to use a truth table."}, {"text": "Title: Defining Optimality \n Text: {Defining Optimality}\n\nIn the notes on logic operations, you learned how to express an\narbitrary function on bits as an OR of minterms (ANDs with one\ninput per variable on which the function operates). \nAlthough this approach demonstrates logical completeness, the results\noften seem inefficient, as you can see by comparing the following \nexpressions for the carry out C from the\naddition of two {2-bit} unsigned numbers, A=A_1A_0 and B=B_1B_0.\n\n\nC &=& A_1B_1 + (A_1+B_1)A_0B_0  \n&=& A_1B_1 + A_1A_0B_0 + A_0B_1B_0  \n&=& \n {A_1} A_0 B_1 B_0 +\nA_1 {A_0} B_1 {B_0} +\nA_1 {A_0} B_1 B_0 +\n&&A_1 A_0 {B_1} B_0 +\nA_1 A_0 B_1 {B_0} +\nA_1 A_0 B_1 B_0 \n\n\nThese three expressions are identical in the sense that they have\nthe same truth tables---they are the same mathematical function.\n\nEquation () is the form that we gave when we introduced\nthe idea of using logic to calculate overflow.  In this form, we were\nable to explain the terms intuitively.\n\nEquation () results from distributing the parenthesized OR\nin Equation ().\n\nEquation () is the result of our logical completeness\nconstruction.  \n\nSince the functions are identical, does the form actually matter at all?\nCertainly either of the first two forms is easier for us to write \nthan is the third.  If we think\nof the form of an expression as a mapping from the function that we\nare trying to calculate into the AND, OR, and NOT functions that we\nuse as logical building blocks, we might also say that the first two\nversions use fewer building blocks.  That observation does have some\ntruth, but let's try to be more precise by framing a question.\n\nFor any given function, there are an infinite number of ways that we can\nexpress the function (for example, given one variable A on which the\nfunction depends, you can OR together any number of copies \nof A without changing the function).\n\n{ What exactly makes one expression better than another?}\n\n\n\nIn 1952, Edward Veitch wrote an article on simplifying truth functions.\nIn the introduction, he said, ``This general problem can be very complicated\nand difficult.  Not only does the complexity increase greatly with the\nnumber of inputs and outputs, but the criteria of the best circuit will\nvary with the equipment involved.''\n\nSixty years later, the answer is largely the same: the criteria depend\nstrongly on the underlying technology (the gates and the devices used\nto construct the gates), and no single { metric}, or way of measuring,\nis sufficient to capture the important differences between expressions \nin all cases.\n\nThree high-level metrics commonly used to evaluate chip\ndesigns are cost, power, and performance.  Cost usually represents the\nmanufacturing cost, which is closely related to the physical silicon\narea required for the design: the larger the chip, the more expensive\nthe chip is to produce.  Power measures energy consumption over\ntime.  A chip that consumes more power means that a user's energy bill\nis higher and, in a portable device, either that the device is heavier or has\na shorter battery life.  Performance measures the speed at which the\ndesign operates.  A faster design can offer more functionality, such as\nsupporting the latest games, or can just finish the same work in less\ntime than a slower design.  These metrics are sometimes related: if\na chip finishes its work, the chip can turn itself off, saving energy.\n\nHow do such high-level metrics relate to the problem at hand?  Only\nindirectly in practice.  There are too many factors involved to make\ndirect calculations of cost, power, or performance at the level of\nlogic expressions.  \n\nFinding an { optimal} solution---the best formulation of a specific\nlogic function for a given metric---is often impossible using the \ncomputational resources and algorithms available to us.\n\nInstead, tools typically use heuristic approaches\nto find solutions that strike a balance between these metrics.\nA { heuristic} approach is one that is\nbelieved to yield fairly good solutions to a problem, but does\nnot necessarily find an optimal solution.\n\nA human engineer can typically impose { constraints}, such as\nlimits on the chip area or limits on the minimum performance,\nin order to guide the process.\n\nHuman engineers may also restructure the implementation of a \nlarger design, such as a design to perform floating-point arithmetic,\nso as to change the logic functions used in the design.\n\n{ Today, manipulation of logic expressions for the purposes of \noptimization is performed almost entirely by computers.}  Humans must\nsupply the logic functions of interest, and must program the acceptable \ntransformations between equivalent forms, but computers do the grunt\nwork of comparing alternative formulations and deciding which one is\nbest to use in context.\n\nAlthough we believe that hand optimization of Boolean expressions is no\nlonger an important skill for our graduates, we do think that you\nshould be exposed to the ideas and metrics historically used for\nsuch optimization.  The rationale for retaining this exposure is \nthreefold. \n\nFirst, we believe that you still need to be able to perform basic\nlogic reformulations (slowly is acceptable)\nand logical equivalence checking (answering the question, ``Do two \nexpressions represent the same function?'').\n\nSecond, the complexity of the problem is a good way to introduce you\nto real engineering.\n\nFinally, the contextual information will help you to develop a better\nunderstanding of finite state machines and higher-level abstractions\nthat form the core of digital systems and are still defined directly\nby humans today.\n\nTowards that end, we conclude this introduction by discussing two\nmetrics that engineers traditionally used to optimize \nlogic expressions.  These metrics are now embedded in { computer-aided \ndesign} ({ CAD}) tools and tuned to specific underlying technologies,\nbut the reasons for their use are still interesting.\n\nThe first metric of interest is a heuristic for the area needed for\na design.\n\nThe measurement is simple: count the number of variable occurrences in\nan expression.  Simply go through and add up how many variables you see.\nUsing our example function C, \nEquation () gives a count of 6,\nEquation () gives a count of 8, and\nEquation () gives a count of 24.\nSmaller numbers represent better expressions, so \nEquation () is the best choice by this metric.\n\nWhy is this metric interesting?\nRecall how gates are built from transistors.\nAn {N-input} gate requires roughly 2N transistors, so if you \ncount up the number of variables in the\nexpression, you get an estimate of the number of transistors needed,\nwhich is in turn an estimate for the area required for the design.\n\nA variation on variable counting is to add the number of operations,\nsince each gate also takes space for wiring (within as well as between\ngates).  Note that we ignore the number of inputs to the operations,\nso a {2-input} AND counts as 1, but a {10-input} AND also counts\nas 1.  We do not usually count complementing variables as an operation\nfor this metric because the complements of variables are sometimes \navailable at no extra cost in gates or wires.\n\nIf we add the number of operations in our example, we get\na count of 10 for Equation ()---two ANDs, two ORs,\nand 6 variables,\n\na count of 12 for Equation ()---three ANDS, one OR,\nand 8 variables,\n\nand a count of 31 for Equation ()---six ANDs, one OR,\nand 24 variables.\n\nThe relative differences between these equations \nare reduced when one counts operations.\n\nA second metric of interest is a heuristic for the performance of a design.\n\nPerformance is inversely related to the delay necessary for a design\nto produce an output once its inputs are available.  For example, if\nyou know how many seconds it takes to produce a result, you can easily\ncalculate the number of results that can be produced per second, which \nmeasures performance.\n\nThe measurement needed is the longest chain of operations\nperformed on any instance of a variable.  The complement of\na variable is included if the variable's complement is not \navailable without using an inverter.\n\nThe rationale for this metric is that gate outputs do not change \ninstantaneously when their inputs\nchange. Once an input to a gate has reached an appropriate voltage to represent \na 0 or a 1, the transistors in the gate switch (on or off) and electrons \nstart to move.\nOnly when the output of the gate reaches the appropriate new voltage \ncan the gates driven by the output start to change. \nIf we count each\nfunction/gate as one delay (we call this time a { gate delay}), we \nget an estimate of the time needed to compute\nthe function.\n\nReferring again to our example equations, we find that\nEquation () requires 3 gate delays,\nEquation () requires 2 gate delays,\nEquation () requires 2 or 3 gate delays, depending\non whether we have variable complements available.  Now \nEquation () looks more attractive: better performance than\nEquation () in return for a small extra cost in area.\n\nHeuristics for estimating energy use are too \ncomplex to introduce at this point, \nbut you should be aware that every time electrons move, they\ngenerate heat, so we might favor an expression that minimizes the number\nof bit transitions inside the computation.  Such a measurement is\nnot easy to calculate by hand,\nsince you need to know the likelihood of input combinations.\n\n\n \n Question: \nQ. What is the meaning of the term \"heuristic\" as it relates to optimization of logic expressions? \n Answer: \n\nA. A heuristic is an approach to problem-solving that is designed to find a good, but not necessarily optimal, solution. In the context of logic expression optimization, a heuristic approach is one that is believed to yield fairly good solutions, but does not necessarily find an optimal solution."}, {"text": "Title: Terminology \n Text: \n\nWe use many technical terms when we talk about simplification of\nlogic expressions, so we now introduce those terms so as to make\nthe description of the tools and processes easier to understand.\n\nLet's assume that we have a logic function F(A,B,C,D) that we want\nto express concisely.  A { literal} in an expression of F refers \nto either one of the variables or its complement.  In other words,\nfor our function F, the following is a complete set of literals:\nA, , B, ,\nC, , D, and .\n\nWhen we introduced the AND and OR functions, we also introduced \nnotation borrowed from arithmetic, using multiplication to represent AND\nand addition to represent OR.  We also borrow the related terminology, \nso a { sum} \nin Boolean algebra refers to a number of terms OR'd together \n(for example, A+B, or AB+CD), and\na { product} in Boolean algebra refers to a number of terms AND'd\ntogether (for example, A, or AB(C+D).  Note that\nthe terms in a sum or product may themselves be sums, products, or\nother types of expressions (for example, A).\n\nThe construction method that we used to demonstrate logical completeness\nmade use of minterms for each input combination for which the \nfunction F produces a 1.\n\nWe can now use the idea of a literal to give a simpler definition of minterm:\na { minterm} for a function on N variables is a product (AND \nfunction) of N literals in which each variable or its complement \nappears exactly once.  For our function F, examples of minterms\ninclude ABC,\nACD, and\nBC.\nAs you know, a minterm produces a 1 for exactly\none combination of inputs.\n\nWhen we sum minterms for each output value of 1\nin a truth table to express a function, as we did to obtain\nEquation (), we\nproduce an example of the sum-of-products form.\nIn particular, a { sum-of-products} ({ SOP}) is a sum composed of \nproducts of literals.\nTerms in a sum-of-products need not be minterms, however.\nEquation () is also in sum-of-products form.\nEquation (), however, is not, since the last\nterm in the sum is not a product of literals.\n\nAnalogously to the idea of a minterm, \nwe define a { maxterm} for a function on N variables as a sum (OR \nfunction) of N literals in which each variable or its complement \nappears exactly once.  Examples for F include (A+B++D),\n(A+++D), and\n(++C+).\nA maxterm produces a 0 for exactly\none combination of inputs.  Just as we did with minterms, we can\nmultiply a maxterm corresponding to each input combination for which\na function produces 0 (each row in a truth table that produces a 0\noutput) to create\nan expression for the function.  The resulting expression is in\na { product-of-sums} ({ POS}) form: a product of sums of literals.  \n\nThe carry out function that we used to produce \nEquation () has 10 input combinations that produce 0,\nso the expression formed in this way is unpleasantly long:\n\n{eqnarray*}\nC&=&\n({A_1}+{A_0}+B_1+B_0)\n({A_1}+A_0+B_1+{B_0})\n({A_1}+A_0+B_1+B_0)\n(A_1+{A_0}+{B_1}+B_0)\n&&(A_1+{A_0}+B_1+{B_0})\n(A_1+{A_0}+B_1+B_0)\n(A_1+A_0+{B_1}+{B_0})\n(A_1+A_0+{B_1}+B_0)\n&&(A_1+A_0+B_1+{B_0})\n(A_1+A_0+B_1+B_0)\n{eqnarray*}\n\nHowever, the approach can be helpful with functions that produce mostly 1s.\nThe literals in maxterms are complemented with respect to the\nliterals used in minterms.  For example, the maxterm\n({A_1}+{A_0}+B_1+B_0) in the equation above\nproduces a zero for input combination A_1=1, A_0=1, B_1=0, B_0=0.\n\nAn { implicant} G of a function F is defined to be a second\nfunction operating on the same variables for which\nthe implication G is true.  In terms of logic functions\nthat produce 0s and 1s, { if G is an implicant of F, \nthe input combinations for which G produces 1s are a subset of \nthe input combinations for which F produces 1s}.\n\nAny minterm for which F produces a 1, for example, is an implicant of F.\n\nIn the context of logic design, { the term implicant is used\nto refer to a single product of literals.}  In other words, if we\nhave a function F(A,B,C,D), examples of possible implicants of F\ninclude AB, B, ABC, and .\nIn contrast, although they may technically imply F, we typically\ndo not call expressions such as (A+B), C(+D), nor\nA+C implicants.\n\nLet's say that we have expressed function F in sum-of-products \nform.\nAll of the individual product terms in the expression are implicants of F.  \n\nAs a first step in simplification, we can ask: for each implicant, is it\npossible to remove any of the literals that make up the product?\n\nIf we have an implicant G for which the answer is no, we \ncall G a { prime implicant} of F.\n\nIn other words, if one removes any of the literals from a prime \nimplicant G of F,\nthe resulting product is not an implicant of F.\n\nPrime implicants are the main idea that we use to simplify logic expressions,\nboth algebraically and with graphical tools (computer tools use algebra\ninternally---by graphical here we mean drawings on paper).\n\n\n \n Question: \nQ. What is the definition of a minterm? \n Answer: \n\nA. A minterm is a product (AND function) of N literals in which each variable or its complement appears exactly once."}, {"text": "Title: Veitch Charts and Karnaugh Maps \n Text: {Veitch Charts and Karnaugh Maps}\n\nVeitch's 1952 paper was the first to introduce the idea of\nusing a graphical representation to simplify logic expressions.\nEarlier approaches were algebraic.\nA year later, Maurice Karnaugh published\na paper showing a similar idea with a twist.  The twist makes the use of \n{ Karnaugh maps} to simplify expressions\nmuch easier than the use of Veitch charts.  As a result,\nfew engineers have heard of Veitch, but everyone who has ever\ntaken a class on digital logic knows how to make use of a { K-map}. \n\n\n\nBefore we introduce the Karnaugh map, let's think about the structure\nof the domain of a logic function.  Recall that a function's { domain}\nis the space on which the function is defined, that is, for which the\nfunction produces values.  For a Boolean logic function on N variables,\nyou can think of the domain as sequences of N bits, but you can also\nvisualize the domain as an {N-dimensional} hypercube.  \nAn\n\n\n\n\n\n{ {N-dimensional} hypercube} is the generalization\nof a cube to N dimensions.  Some people only use the term hypercube when\nN, since we have other names for the smaller values: a point\nfor N=0, a line segment for N=1, a square for N=2, and a cube\nfor N=3.  The diagrams above and to the right illustrate the cases that \nare easily drawn on paper.  The black dots represent specific input\ncombinations, and the blue edges connect input combinations that differ\nin exactly one input value (one bit).\n\n\n\nBy viewing a function's domain in this way, we can make a connection\nbetween a product of literals and the structure of the domain.  Let's\nuse the {3-dimensional} version as an example.  We call the \nvariables A, B, and C, and note that the cube has 2^3=8 corners\ncorresponding to the 2^3 possible combinations\nof A, B, and C.\nThe simplest product of literals in this\ncase is 1, which is the product of 0 literals.  Obviously, the product 1 \nevaluates to 1 for any variable values.  We can thus think of it as\ncovering the entire domain of the function.  In the case of our example,\nthe product 1 covers the whole cube.  In order for the product 1 to \nbe an implicant of a function, the function itself must be the function 1.\n\nWhat about a product consisting of a single literal, such as A \nor ?  The dividing lines in the diagram illustrate the\nanswer: any such product term evaluates to 1 on a face of the cube,\nwhich includes 2^2=4 of the corners.  If a function evaluates to 1\non any of the six faces of the cube, the corresponding product term\n(consisting of a single literal) is an implicant of the function.\n\nContinuing with products of two literals, we see that any product of \ntwo literals, such as A or C, corresponds\nto an edge of our {3-dimensional} cube.  The edge includes 2^1=2\ncorners.  And, if a function evaluates to 1 on any of the 12 edges\nof the cube, the corresponding product term (consisting of two literals)\nis an implicant of the function.\n\nFinally, any product of three literals, such as B,\ncorresponds to a corner of the cube.  But for a function on three variables,\nthese are just the minterms.  As you know, if a function evaluates to 1\non any of the 8 corners of the cube, that minterm is an implicant of the\nfunction (we used this idea to construct the function to prove\nlogical completeness).\n\nHow do these connections help us to simplify functions?  If we're\ncareful, we can map cubes onto paper in such a way that product terms\n(the possible implicants of the function) usually form contiguous \ngroups of 1s, allowing us to spot them easily.  Let's work upwards\nstarting from one variable to see how this idea works.  The end result\nis called a Karnaugh map.\n\n\nThe first drawing shown to the right replicates our view of \nthe {1-dimensional} hypercube, corresponding to the domain of a\nfunction on one variable, in this case the variable A.  \nTo the right of the hypercube (line segment) are two variants\nof a Karnaugh map on one variable.  The middle variant clearly \nindicates the column corresponding to the product A (the other \ncolumn corresponds to ).  The right variant simply labels\nthe column with values for A.\n\n\n\n\n\n\nThe three drawings shown to the right illustrate the three possible product\nterms on one variable.  { The functions shown in \nthese Karnaugh maps are arbitrary, except that we have chosen\nthem such that each implicant shown is a prime implicant for the\nillustrated function.}\n\n\n\n\n\n\n\nLet's now look at two-variable functions.  We have replicated\nour drawing of the {2-dimensional} hypercube (square) to the right \nalong with two variants of Karnaugh maps on two variables.\nWith only two variables (A and B), the extension is fairly \nstraightforward, since we can use the second dimension of the \npaper (vertical) to express the second variable (B).\n\n\n\n\n\n\nThe number of possible products of literals grows rapidly with the \nnumber of variables.\nFor two variables, nine are possible, as shown to the right.\nNotice that all implicants have two properties.  First, they\noccupy contiguous regions of the grid.  And, second, their height\nand width are always powers of two.  These properties seem somewhat\ntrivial at this stage, but they are the key to the utility of\n{K-maps} on more variables.\n\n\n\n\n\n\nThree-variable functions are next.  The cube diagram is again replicated\nto the right.  But now we have a problem: how can we map four points\n(say, from the top half of the cube) into a line in such a way that \nany points connected by a blue line are adjacent in the {K-map}?\nThe answer is that we cannot, but we can preserve most of the connections\nby choosing an order such as the one illustrated by the arrow.\nThe result\n\n\n\n\n\nis called a Gray code.  Two {K-map} variants again appear\nto the right of the cube.  Look closely at the order of the two-variable\ncombinations along the top, which allows us to have as many contiguous\nproducts of literals as possible.  Any product of literals that contains\n but not A nor  wraps around the edges\nof the {K-map}, so you should think of it as rolling up into a cylinder \nrather than a grid.  Or you can think that we're unfolding the cube to\nfit the corners onto a sheet of paper, but the place that we split\nthe cube should still be considered to be adjacent when looking for \nimplicants.  The use of a Gray code is the one difference between a\n{K-map} and a Veitch chart; Veitch used the base 2 order, which\nmakes some implicants hard to spot.\n\nWith three variables, we have 27 possible products of literals.  You\nmay have noticed that the count scales as 3^N for N variables;\ncan you explain why?  We illustrate several product terms\nbelow.  Note that we sometimes\nneed to wrap around the end of the {K-map}, but that if we account\nfor wrapping, the squares covered by all product terms are contiguous.\nAlso notice that both the width and the height of all product terms \nare powers of two.  { Any square or rectangle that meets these two \nconstraints corresponds to a product term!}  And any such square or \nrectangle that is filled with 1s is an implicant of the function\nin the {K-map}.\n\n\n\nLet's keep going.  With a function on four variables---A, B, C, \nand D---we can use a Gray code order on two of the variables in \neach dimension.  Which variables go with which dimension in the grid\nreally doesn't matter, so we'll assign AB to the horizontal dimension\nand CD to the vertical dimension.  A few of the 81 possible product\nterms are illustrated at the top of the next page.  Notice that while wrapping \ncan now occur\nin both dimensions, we have exactly the same rule for finding implicants\nof the function: any square or rectangle (allowing for wrapping) \nthat is filled with 1s and has both height and width equal to (possibly\ndifferent) powers of two is an implicant of the function.  { \nFurthermore, unless such a square or rectangle is part of a larger\nsquare or rectangle that meets these criteria, the corresponding\nimplicant is a prime implicant of the function.}\n\n\n\n\n\n\n\n\nFinding a simple expression for a function using a {K-map} then\nconsists of solving the following problem: pick a minimal set of prime \nimplicants such that every 1 produced by the function is covered\nby at least one prime implicant.  The metric that you choose to\nminimize the set may vary in practice, but for simplicity, let's say \nthat we minimize the number of prime implicants chosen.\n\n\nLet's try a few!  The table on the left below reproduces (from\nNotes Set 1.4) the truth\ntable for addition of two {2-bit} unsigned numbers, A_1A_0\nand B_1B_0, to produce a sum S_1S_0 and a carry out C.\n{K-maps} for each output bit appear to the right.\nThe colors are used only to make the different prime implicants\neasier to distinguish.\nThe equations produced by summing these prime implicants appear\nbelow the {K-maps}.\n\n\n{cccc|ccc}\n{c|}& \nA_1& A_0& B_1& B_0& C& S_1& S_0 \n0&0& 0&0& 0& 0&0\n0&0& 0&1& 0& 0&1\n0&0& 1&0& 0& 1&0\n0&0& 1&1& 0& 1&1\n0&1& 0&0& 0& 0&1\n0&1& 0&1& 0& 1&0\n0&1& 1&0& 0& 1&1\n0&1& 1&1& 1& 0&0\n1&0& 0&0& 0& 1&0\n1&0& 0&1& 0& 1&1\n1&0& 1&0& 1& 0&0\n1&0& 1&1& 1& 0&1\n1&1& 0&0& 0& 1&1\n1&1& 0&1& 1& 0&0\n1&1& 1&0& 1& 0&1\n1&1& 1&1& 1& 1&0\n\n\n\n\n\n\n{eqnarray*}\nC&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0\n\nS_1&=&\nA_1 {B_1} {B_0}+\nA_1 {A_0} {B_1}+\n{A_1} {A_0} B_1+\n{A_1} B_1 {B_0}+\n&&{A_1} A_0 {B_1} B_0+\nA_1 A_0 B_1 B_0\n\nS_0&=&A_0 {B_0}+{A_0} B_0\n{eqnarray*}\n\n\nIn theory, {K-maps} extend to an arbitrary number of variables.\nCertainly Gray codes can be extended.  An { {N-bit} Gray code} \nis a sequence of {N-bit} patterns that includes all possible\npatterns such that any two adjacent patterns differ in only one bit.\nThe code is actually a cycle: the first and last patterns also differ\nin only one bit.  You can construct a Gray code recursively as follows:\nfor an {(N+1)-bit} Gray code, write the sequence for an\n{N-bit} Gray code, then add a 0 in front of all patterns.\nAfter this sequence, append a second copy of the {N-bit} Gray\ncode in reverse order, then put a 1 in front of all patterns in the\nsecond copy.  The result is an {(N+1)-bit} Gray code.\nFor example, the following are Gray codes:\n\n\n1-bit& 0, 1\n2-bit& 00, 01, 11, 10\n3-bit& 000, 001, 011, 010, 110, 111, 101, 100\n4-bit& \n0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100,\n1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000\n\n\nUnfortunately, some of the beneficial properties of {K-maps} do\nnot extend beyond two variables in a dimension.  { Once you have three\nvariables in one dimension}, as is necessary if a function operates\non five or more variables, { not all product terms are contiguous\nin the grid}.  The terms still require a total number of rows and columns\nequal to a power of two, but they don't all need to be a contiguous\ngroup.  Furthermore, { some contiguous groups of appropriate size do\nnot correspond to product terms}.  So you can still make use of {K-maps}\nif you have more variables, but their use is a little trickier.\n\n \n Question: \nQ. \nQ. What is the difference between a Veitch chart and a Karnaugh map? \n Answer: \n\nA. The difference between a Veitch chart and a Karnaugh map is that a Karnaugh map uses a Gray code order, which makes some implicants easier to spot."}, {"text": "Title: Canonical Forms \n Text: {Canonical Forms}\n\nWhat if we want to compare two expressions to determine whether they\nrepresent the same logic function?  Such a comparison is a test of\n{ logical equivalence}, and is an important part of hardware design.\nTools today provide help with this problem, but you should understand\nthe problem.\n\nYou know that any given function can be expressed in many ways, and that\ntwo expressions that look quite different may in fact represent the\nsame function (look back at Equations () to ()\nfor an example).  But what if we rewrite the function using only \nprime implicants?  Is the result unique?  Unfortunately, no.\n\n\nIn general, { a sum of products is not unique (nor is a product of sums),\neven if the sum contains only prime implicants}.\n\nFor example, consensus terms may or may not be included in our\nexpressions.  (They are necessary for reliable design of \ncertain types of systems, as you will learn in a later ECE class.)\n\nThe green ellipse in the K-map to the right represents the consensus \nterm BC.\n{eqnarray*}\nZ &=& A C +  B + B C\nZ &=& A C +  B\n{eqnarray*}\n\n\n\n\n\n\nSome functions allow several equivalent formulations as\nsums of prime implicants, even without consensus terms.  \nThe K-maps shown to the right, for example, illustrate \nhow one function might be written in either of the following ways:\n\n{eqnarray*}\nZ &=&\n  D +\n C  +\nA B C +\nB  D\nZ &=&\n  C +\nB C  +\nA B D +\n  D\n{eqnarray*}\n\n\n\n\n\n\n\n\nWhen we need to compare two things (such as functions), we need to\ntransform them into what in mathematics is known as a { canonical form},\nwhich simply means a form that is defined so as to be unique \nfor each thing of the given type.\nWhat can we use for logic functions?  You already know two answers!  \n\nThe { canonical sum} of a function (sometimes called the\n{ canonical SOP form}) is the sum of minterms.  \n\nThe { canonical product} of a function (sometimes called the\n{ canonical POS form}) is the product of maxterms.  \n\nThese forms technically\nonly meet the mathematical definition of canonical if we agree on an order\nfor the min/maxterms, but that problem is solvable.\n\nHowever, as you already know, the forms are not particularly \nconvenient to use.  \n\nIn practice, people and tools in the industry use more compact \napproaches when comparing functions, but those solutions are a subject for\na later class (such as ECE 462).\n\n\n\n\n \n Question: \nQ. What is the canonical SOP form of a logic function? \n Answer: \n\nThe canonical SOP form of a logic function is the sum of minterms."}, {"text": "Title: Two-Level Logic \n Text: {Two-Level Logic}\n\n\n{ Two-level logic} is a popular way of expressing logic functions.\nThe two levels refer simply to the number of functions through which an\ninput passes to reach an output, and both the SOP and POS forms are \nexamples of two-level logic.  In this section, we illustrate one of the \nreasons for this popularity and \nshow you how to graphically manipulate\nexpressions, which can sometimes help when trying to understand gate\ndiagrams.\n\nWe begin with one of DeMorgan's laws, which we can illustrate both \nalgebraically and graphically:\nC  =  B+A  =   \n\n\n{file=part2/figs/demorgan-nand.eps,width=0.95in}\n\n\n\n\nLet's say that we have a function expressed in SOP form, such as\nZ=ABC+DE+FGHJ.  The diagram on the left below shows the function\nconstructed from three AND gates and an OR gate.  Using DeMorgan's\nlaw, we can replace the OR gate with a NAND with inverted inputs.\nBut the bubbles that correspond to inversion do not need to sit at the\ninput to the gate.  We can invert at any point along the wire,\nso we slide each bubble down the wire to the output of the first column\nof AND gates.  { Be careful: if the wire splits, which does not\nhappen in our example, you have to replicate\nthe inverter onto the other output paths as you slide past the split\npoint!}  The end result is shown on the right: we have not changed the \nfunction, but now we use only NAND gates.  Since CMOS technology\nonly supports NAND and NOR directly, using two-level logic makes\nit simple to map our expression into CMOS gates.\n\n{file=part2/figs/SOP-equiv.eps,width=6.5in}\n\n\nYou may want to make use of DeMorgan's other law, illustrated graphically\nto the right, to perform the same transformation on a POS expression.  What\ndo you get?\n\n\n{file=part2/figs/demorgan-nor.eps,width=0.95in}\n\n\n\n \n Question: \nQ. What is the significance of two-level logic? \n Answer: \n\nA. Two-level logic is a popular way of expressing logic functions.\nThe two levels refer simply to the number of functions through which an\ninput passes to reach an output."}, {"text": "Title: Multi-Metric Optimization \n Text: {Multi-Metric Optimization}\n\nAs engineers, almost every real problem that you encounter will admit \nmultiple metrics for evaluating possible designs.  Becoming a good\nengineer thus requires not only that you be able to solve problems\ncreatively so as to improve the quality of your solutions, but also\nthat you are aware of how people might evaluate those solutions and\nare able both to identify the most important metrics and to balance \nyour design effectively according to them.  In this section, we\nintroduce some general ideas and methods that may be of use to you\nin this regard.\n\n{ We will not test you on the concepts in this section.}\n\nWhen you start thinking about a new problem, your first step\nshould be to think carefully about metrics of possible interest.\n\nSome important metrics may not be easy to quantify.  \n\nFor example, compatibility of a design with other products already \nowned by a customer has frequently defined the success or failure\nof computer hardware and software solutions.\n\nBut how can you compute the compability of your approach as\na number?\n\nHumans---including engineers---are not good at\ncomparing multiple metrics simultaneously.\n\nThus, once you have a set of metrics that you feel is complete, \nyour next step is to get rid of as many as you can.\n\nTowards this end, you may identify metrics that have no practical \nimpact in current technology, set threshold values for other metrics\nto simplify reasoning about them, eliminate redundant metrics,\ncalculate linear sums to reduce the count of metrics, and, finally,\nmake use of the notion of Pareto optimality.  All of these ideas are\ndescribed in the rest of this section.\n\nLet's start by considering metrics that we can quantify as real\nnumbers.\n\nFor a given metric, we can divide possible measurement values into\nthree ranges.\n\nIn the first range,\nall measurement values are equivalently useful.\nIn the second range, \npossible values are ordered and interesting with respect to\none another.\nValues in the third range are all impossible to use in practice.\nUsing power consumption as\nour example, the first range corresponds to systems in which\nwhen a processor's power consumption in a digital \nsystem is extremely low relative to the\npower consumption\nof the system.\nFor example, the processor in a computer might use less than 1 \nof the total used by \nthe system including the disk drive, the monitor, the power \nsupply, and so forth.  One power consumption value in this range \nis just as good as any\nanother, and no one cares about the power consumption of the processor \nin such cases.  In the second range, power consumption of the\nprocessor makes a difference.  Cell phones use most of their energy\nin radio operation, for example, but if you own a phone with a powerful\nprocessor, you may have noticed that you can turn off the phone and \ndrain the battery fairly quickly by playing a game.  Designing a\nprocessor that uses half as much power lengthens the battery life in\nsuch cases.  Finally,\nthe third region of power consumption measurements is impossible:\nif you use so much power, your chip will overheat or even burst\ninto flames.  Consumers get unhappy when such things happen.\n\nAs a first step, you can remove any metrics for which all solutions\nare effectively equivalent.\n\nUntil a little less than a decade ago, for example, the power \nconsumption of a desktop\nprocessor actually was in the first range that we discussed.  Power\nwas simply not a concern to engineers: all designs of \ninterest consumed so little power that no one cared.\n\nUnfortunately, at that point, power consumption jumped into the\nthird range rather quickly.  Processors hit a wall, and \nproducts had to be cancelled.  Given that the time spent designing\na processor has historically\nbeen about five years, a lot of engineering effort\nwas wasted because people had not thought carefully enough about\npower (since it had never mattered in the past).\n\nToday, power is an important metric that engineers must take into\naccount in their designs. \n\nHowever, in some areas, such as desktop and high-end server\nprocessors,\nother metrics (such as performance) may be so \nimportant that we always want to operate at the edge of the\ninteresting range.  In such cases, we might choose to treat \na metric such as power consumption as a { threshold}: stay\nbelow 150 Watts for a desktop processor, for example.  One still\nhas to make a coordinated effort to ensure that the system as\na whole does not exceed the threshold, but reasoning about \nthreshold values, a form of constraint, is easier than trying to\nthink about multiple metrics at once.\n\nSome metrics may only allow discrete quantification.  For example,\none could choose to define compatibility with previous processor\ngenerations as binary: either an existing piece of software\n(or operating system)\nruns out of the box on your new processor, or it does not.  If you \nwant people who own that software to make use of your new processor,\nyou must ensure that the value of this binary metric is 1, which\ncan also be viewed as a threshold.\n\nIn some cases, two metrics may be strongly { correlated}, meaning\nthat a design that is good for one of the metrics is frequently \ngood for the other metric as well.\n\nChip area and cost, for\nexample, are technically distinct ways to measure a digital design,\nbut we rarely consider them separately.\n\nA design that requires a larger chip is probably more complex,\nand thus takes more engineering time to get right (engineering time\ncosts money).  \n\nEach silicon wafer costs money to fabricate, and fewer copies of a \nlarge design fit on one wafer, so large chips mean more fabrication\ncost.\n\nPhysical defects in silicon can cause some chips not to work.  A large\nchip uses more silicon than a small one, and is thus more likely to suffer\nfrom defects (and not work).  Cost thus goes up again for large chips\nrelative to small ones.\n\nFinally, large chips usually require more careful testing to ensure\nthat they work properly (even ignoring the cost of getting the design\nright, we have to test for the presence of defects), which adds still\nmore cost for a larger chip.\n\nAll of these factors tend to correlate chip area and chip cost, to the\npoint that most engineers do not consider both metrics.\n\n\nAfter you have tried to reduce your set of metrics as much as possible,\nor simplified them by turning them into thresholds, you should consider\nturning the last few metrics into a weighted linear sum.  All remaining\nmetrics must be quantifiable in this case.\n\nFor example, if you are left with three metrics for which a given\ndesign has values A, B, and C, you might reduce these to one\nmetric by calculating D=w_AA+w_BB+w_CC.  What are the w values?\nThey are weights for the three metrics.  Their values represent the\nrelative importance of the three metrics to the overall evaluation.\nHere we've assumed that larger values of A, B, and C are either\nall good or all bad.  If you have metrics with different senses,\nuse the reciprocal values.  For example, if a large value of A is good,\na small value of 1/A is also good.  \n\nThe difficulty with linearizing metrics is that not everyone agrees\non the weights.  Is using less power more important than having a cheaper\nchip?  The answer may depend on many factors.\n\nWhen you are left with several metrics of interest, you can use the\nidea of Pareto optimality to identify interesting designs.\n\nLet's say that you have two metrics.  If a design D_1 is better than\na second design D_2 for both metrics, we say that D_1 \n{ dominates} D_2.\n\nA design D is then said to be { Pareto optimal} if no other design\ndominates D.  Consider the figure on the left below, which illustrates\nseven possible designs measured with two metrics.  The design corresponding\nto point B dominates the designs corresponding to points A and C, so \nneither of the latter designs is Pareto optimal.  No other point \nin the figure dominates B, however, so that design is Pareto optimal.\nIf we remove all points that do not represent Pareto optimal designs,\nand instead include only those designs that are Pareto optimal, we\nobtain the version shown on the right.  These are points in a two-dimensional\nspace, not a line, but we can imagine a line going through the points,\nas illustrated in the figure: the points that make up the line are\ncalled a { Pareto curve}, or, if you have more than two metrics,\na { Pareto surface}.\n\n{\n\n\n\n\nAs an example of the use of Pareto optimality, consider the figure\nto the right, which is copied with permission from Neal Crago's Ph.D. \ndissertation (UIUC ECE, 2012).  The figure compares hundreds of thousands \nof possible\ndesigns based on a handful of different core approaches for\nimplementing a processor.  The axes in the graph are two metrics \nof interest.  The horizontal axis measures the average performance of a \ndesign when\nexecuting a set of benchmark applications, normalized to\na baseline processor design.  The vertical axis measures the energy\nconsumed by a design when\n\n\n{file=part2/cited/bench_pareto.eps,width=3in}\n\n\nexecuting the same benchmarks, normalized\nagain to the energy consumed by a baseline design.  The six sets of\npoints in the graph represent alternative design techniques for the\nprocessor, most of which are in commercial use today.  The points\nshown for each set are the subset of many thousands of possible variants\nthat are Pareto optimal.  In this case, more performance and less energy\nconsumption are the good directions, so any point in a set for which\nanother point is both further to the right and further down is not\nshown in the graph.  The black line represents an absolute power\nconsumption of 150 Watts, which is a nominal threshold for a desktop\nenvironment.  Designs above and to the right of that line are not\nas interesting for desktop use.  The { design-space exploration} that\nNeal reported in this figure was of course done by many computers using\nmany hours of computation, but he had to design the process by which the\ncomputers calculated each of the points.\n\n\n\n\n\n \n Question: \nQ. What is the design-space exploration? \n Answer: \n\nA. Design-space exploration is a process in which computers calculate points in a given space in order to find possible solutions to a problem."}, {"text": "Title: Sequential Logic \n Text: {Sequential Logic}\n\nThese notes introduce logic components for storing bits, building up\nfrom the idea of a pair of cross-coupled inverters through an\nimplementation of a flip-flop, the storage abstractions used in most \nmodern logic design processes.  We then introduce simple forms of\ntiming diagrams, which we use to illustrate the behavior of\na logic circuit.\n\nAfter commenting on the benefits of using a clocked synchronous \nabstraction when designing systems that store and manipulate bits,\nwe illustrate timing issues and explain how these are abstracted\naway in clocked synchronous designs.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later\nclasses.} \n\n\n\n \n Question: \nQ. What is the main purpose of introducing logic components? \n Answer: \n\nA. The main purpose of introducing logic components is to store bits and build up from the idea of a pair of cross-coupled inverters."}, {"text": "Title: Storing One Bit \n Text: {Storing One Bit}\n\nSo far we have discussed only implementation of Boolean functions:\ngiven some bits as input, how can we design a logic circuit to calculate\nthe result of applying some function to those bits?  The answer to\nsuch questions is called { combinational logic} (sometimes \n{ combinatorial logic}), a name stemming from the fact that we\nare combining existing bits with logic, not storing new bits.\n\nYou probably already know, however, that combinational logic alone is\nnot sufficient to build a computer.  We need the ability to store bits,\nand to change those bits.  Logic designs that make use of stored \nbits---bits that can be changed, not just wires to high voltage and \nground---are called { sequential logic}.  The name comes from the idea \nthat such a system moves through a sequence of stored bit patterns (the \ncurrent stored bit pattern is called the { state} of the system).\n\n\nConsider the diagram to the right.  What is it?  A {1-input} \nNAND gate, or an inverter drawn badly?  If you think carefully about\nhow these two gates are built, you will realize that they are the\nsame thing.  Conceptually, we use two inverters to store a bit, but\nin most cases we make use of NAND gates to simplify the \nmechanism for changing the stored bit.\n\n\n{file=part2/figs/latch-step-1.eps,width=1.9in}\n\n\n\nTake a look at the design to the right.  Here we have taken two \ninverters (drawn as NAND gates) and coupled each gate's output to\nthe other's input.  What does the circuit do?  Let's make some\nguesses and see where they take us.  Imagine that the value at Q\nis 0.  In that case, the lower gate drives P to 1.  \nBut P drives the upper gate, which forces Q to 0.\nIn other words, this combination forms a\n\n\n{file=part2/figs/latch-step-2.eps,width=1.65in}\n\n\n{|cc}\nQ& P \n0& 1\n1& 0\n\n\n\nstable state of the system:\nonce the gates reach this state, they continue to hold these values.\nThe first row of the truth table to the right (outputs only) shows\nthis state.\n\nWhat if Q=1, though?  In this case, the lower gate forces P to 0,\nand the upper gate in turn forces Q to 1.  Another stable state!\nThe Q=1 state appears as the second row of the truth table.\n\nWe have identified all of the stable states.{Most logic\nfamilies also allow unstable states in which the values alternate rapidly \nbetween 0 and 1.  These metastable states are \nbeyond the scope of our class, but ensuring that they do not occur\nin practice is important for real designs.}  Notice that our cross-coupled\ninverters can store a bit.  Unfortunately, we have no way to \nspecify which value should be stored, nor to change the bit's value\nonce the gates have settled into a stable state.\nWhat can we do?  \n\n\n\n\n\n\nLet's add an input to the upper gate, as shown to the \nright.  We call the input .  The ``S'' stands for set---as\nyou will see, our new input allows us to { set} our stored bit Q to 1.\nThe use of a complemented name for the input indicates that the input\nis { active low}.  In other words, the input performs its intended \ntask (setting Q to 1) when its value is 0 (not 1).\n\n\n{file=part2/figs/latch-step-3.eps,width=2.1in}\n\n\n{c|cc}\n& Q& P \n1& 0& 1\n1& 1& 0\n0& 1& 0\n\n\n\nThink about what happens when the new input is not active, \n=1.  As you know, ANDing any value with 1 produces the \nsame value, \nso our new input has no effect when =1.  The first two\nrows of the truth table are simply a copy of our previous table: the \ncircuit can store either bit value when =1.\nWhat happens when =0?  In that case, the upper gate's\noutput is forced to 1, and thus the lower gate's is forced to 0.\nThis third possibility is reflected in the last row of the truth table.\n\n\nNow we have the ability to force bit Q to have value 1, but if we\nwant Q=0, we just have to hope that the circuit happens to settle into\nthat state when we turn on the power.  What can we do?\n\nAs you probably guessed, we add an input to the other gate, as \nshown to the right.  We call the new input : the input's\npurpose is to { reset} bit Q to 0, and the input\nis active low.  We extend the truth table to include a row \nwith =0 and =1, which forces Q=0 and P=1.\n\n\n{file=part2/figs/latch-step-4.eps,width=2.1in}\n\n\n{cc|cc}\n& & Q& P \n1& 1& 0& 1\n1& 1& 1& 0\n1& 0& 1& 0\n0& 1& 0& 1\n0& 0& 1& 1\n\n\n\n is active low.  We extend the truth table to include a row \n with =0 and =1, which forces Q=0 and P=1.\n\nThe circuit that we have drawn has a name: an \n{ {- latch}}. \nOne can also build {R-S latches} (with active high set and reset\ninputs).  The textbook also shows an\n{- latch} (labeled \nincorrectly).\nCan you figure out how to build an {R-S} latch yourself?\n\nLet's think a little more about\nthe {- latch}. \nWhat happens if we set =0 and =0 at\nthe same time?  Nothing bad happens immediately.  Looking at the design,\nboth gates produce 1, so Q=1 and P=1.  The bad part happens later:\nif we raise both  and  back to 1 at around\nthe same time, the stored bit may end up in either state.{Or,\nworse, in a metastable state, as mentioned earlier.}\n\nWe can avoid the problem by adding gates to prevent the\ntwo control inputs ( and )\nfrom ever being 1 at the same time.  A single inverter\nmight technically suffice, but let's build up the structure shown below,\nnoting that the two inverters in sequence connecting D to \nhave no practical effect at the moment.\n\nA truth table is shown to the right of the logic diagram.\n\nWhen D=0,  is forced to 0, and the bit is reset.\n\nSimilarly, when D=1,  is forced to 0, and the bit is set.\n\n{\n\n{file=part2/figs/latch-step-5.eps,width=3.25in}\n\n\n{c|cccc}\nD& & & Q& P \n0& 0& 1& 0& 1\n1& 1& 0& 1& 0\n\n\n}\n\nUnfortunately, except for some interesting timing characteristics, \nthe new design has the same functionality as a piece of wire.\nAnd, if you ask a circuit designer, thin wires also have some \ninteresting timing characteristics.  What can we do?  Rather than \nhaving Q always reflect the current value of D, let's add\nsome extra inputs to the new NAND gates that allow us to control\nwhen the value of D is copied to Q, as shown on the next page.\n\n{\n\n{file=part2/figs/latch-step-6.eps,width=3.35in}\n\n\n{cc|cccc}\nWE& D& & & Q& P \n1& 0& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0\n0& 0& 1& 1& 0& 1\n0& 1& 1& 1& 0& 1\n0& 0& 1& 1& 1& 0\n0& 1& 1& 1& 1& 0\n\n\n}\n\n\nThe WE (write enable) input controls whether or not Q mirrors\nthe value of D.  The first two rows in the truth table are replicated\nfrom our ``wire'' design: a value of WE=1 has no effect on the first\ntwo NAND gates, and Q=D.  A value of WE=0 forces the\nfirst two NAND gates to output 1, thus =1, =1,\nand the bit Q can\noccupy either of the two possible states, regardless of the value\nof D, as reflected in the lower four lines of the truth table.\n\nThe circuit just shown is called a { gated D latch}, and is an\nimportant mechanism\n\n\n{file=part2/figs/latch-step-7.eps,width=1.1in}\n\n\nfor storing state in sequential \nlogic.  (Random-access memory uses a slightly different \ntechnique to connect the cross-coupled inverters, but latches are\nused for nearly every other application of stored state.)\nThe ``D''\nstands for ``data,'' meaning that the bit stored is matches\nthe value of the input.  Other types of latches (including\n{S-R latches}) have been used historically, but\nD latches are used predominantly today, so we omit discussion of\nother types.\nThe ``gated'' qualifier refers to the presence of an enable input\n(we called it WE) to control\nwhen the latch copies its input into the stored bit.\nA symbol for a gated D latch appears to the right.  Note that we have\ndropped the name P in favor of , since P=\nin a gated D latch. \n\n \n Question: \nQ. In order to set the bit to 0, what input is used? \n Answer: \n\nA. The input is used to set the bit to 0."}, {"text": "Title: The Clock Abstraction \n Text: {The Clock Abstraction}\n\nHigh-speed logic designs often use latches directly.  Engineers\nspecify the number of latches as well as combinational logic \nfunctions needed to connect one latch to the next,\nand the CAD tools optimize the combinational logic.\nThe enable inputs of successive groups of latches are then driven\nby what we call a clock signal, a single bit line distributed across\nmost of the chip that alternates between 0 and 1 with a regular\nperiod.  While the clock is 0, one set of latches holds its bit values fixed,\nand combinational logic uses those latches as inputs to produce \nbits that are copied into a\nsecond set of latches.  When the clock switches to 1, the second\nset of latches stops storing their data inputs and\nretains their bit values in order to drive other combinational logic,\nthe results of which are copied into a third set of latches.\nOf course, some of the latches in the first and third sets may be the\nsame.\n\nThe timing of signals in such designs plays a critical role in their\ncorrect operation.  Fortunately, we have developed powerful abstractions \nthat allow engineers to ignore much of the complexity while thinking\nabout the Boolean logic needed for a given design.\n\nTowards that end, we make a simplifying assumption for the\nrest of our class, and for most of your career as an undergraduate:\nthe clock signal is a { square wave} delivered uniformly across a\nchip.  For example, if the period of a clock is 0.5 nanoseconds (2 GHz),\nthe clock signal is a 1 for 0.25 nanoseconds, then a 0 for\n0.25 nanoseconds.  We assume that the clock signal changes instantaneously\nand at the same time across the chip.  Such a signal can never exist in\nthe real world: voltages do not change instantaneously, and the \nphrase ``at the same time'' may not even make sense at these scales.\nHowever, circuit designers can usually provide a clock signal that\nis close enough, allowing us to forget for now that no physical signal can\nmeet our abstract definition.\n\n\n SSL altered terminology on 3 Dec 21\n\n\n\n\nThe device shown to the right is a { master-slave} implementation of a \n\nThe device shown to the right is a { dual-latch} implementation of a \n{ positive edge-triggered} D flip-flop.  As you can see, we have \nconstructed it from two gated D latches with opposite senses of write\nenable.  The ``D'' part of the name has the same meaning as with a\ngated D latch: the bit stored is the same as the one delivered\n\n\n{file=part2/figs/latch-step-8.eps,width=2.75in}\n\n\n{file=part2/figs/latch-step-9.eps,width=0.95in}\n\n\nto the\ninput.  Other variants of flip-flops have also been built, but this \ntype dominates designs today.  Most are actually generated automatically\nfrom hardware ``design'' languages (that is, computer programming languages\nfor hardware design).\n\nWhen the clock is low (0), the first latch copies its value from\nthe flip-flop's D input to the midpoint (marked X in our figure, but\nnot usually given a name).  When the clock is high (1), the second\nlatch copies its value from X to the flip-flop's output Q.\nSince X can not change when the clock is high,\nthe result is that the output changes each time the clock changes\nfrom 0 to 1, which is called the { rising edge} or { positive edge}\n(the derivative) of the clock signal.  Hence the qualifier \n``positive edge-triggered,'' which describes the flip-flop's behavior.\n\nThe ``master-slave'' implementation refers to the use of two latches.\n\nThe ``dual-latch'' implementation refers to the use of two \nlatches.{Historically, this implementation was called ``master-slave,''\nbut ECE Illinois has decided to eliminate use of such terminology.}\nlatches.\nIn practice, flip-flops are almost never built this way.  To see a \ncommercial design, look up 74LS74, which uses six {3-input} NAND\ngates and allows set/reset of the flip-flop (using two\nextra inputs).\n\n\n\nThe { timing diagram} to the right illustrates the operation of\nour flip-flop.  In a timing diagram, the horizontal axis represents\n(continuous) increasing time, and the individual lines represent\nvoltages for logic signals.  The relatively simple version shown here\nuses only binary values for each signal.  One can also draw \ntransitions more realistically (as taking finite time).  The dashed\nvertical lines here represent the times at which the clock rises.\nTo make the\n\n\n\n\n\nexample interesting, we have varied D over two clock\ncycles.  Notice that even though D rises and falls during the second\nclock cycle, its value is not copied to the output of our flip-flop.\nOne can build flip-flops that ``catch'' this kind of behavior\n(and change to output 1), but we leave such designs for later in your\ncareer.\n\nCircuits such as latches and flip-flops are called { sequential\nfeedback} circuits, and the process by which they are designed \nis beyond the scope of our course.  The ``feedback'' part of the\nname refers to the fact that the outputs of some gates are fed back \ninto the inputs of others.  Each cycle in a sequential feedback \ncircuit can store one bit.\n\nCircuits that merely use\nlatches and flip-flops as building blocks are called\n{ clocked synchronous sequential circuits}.  Such designs are\nstill sequential: their behavior depends on the bits currently\nstored in the latches and flip-flops.  However, their behavior\nis substantially simplified by the use of a clock signal (the ``clocked''\npart of the name) in a way that all elements change at the same\ntime (``synchronously'').\n\nThe value of using flip-flops and assuming a square-wave\nclock signal with uniform timing may not be clear to you yet,\nbut it bears emphasis.\n\nWith such assumptions, { we can treat time as having \ndiscrete values.}  In other words, time ``ticks'' along discretely,\nlike integers instead of real numbers.\n\nWe can look at the state of the system, calculate the inputs to our\nflip-flops through the combinational logic that drives their D\ninputs, and be confident that, when time moves to the next discrete\nvalue, we will know the new bit values stored in our flip-flops,\nallowing us to repeat the process for the next clock cycle without\nworrying about exactly when things change.  Values change only\non the rising edge of the clock!\n\nReal systems, of course, are not so simple, and we do not have one\nclock to drive the universe, so engineers must also design systems\nthat interact even though each has its own private clock signal\n(usually with different periods).\n\n\n\n \n Question: \nQ. What is the difference between a sequential feedback circuit and a clocked synchronous sequential circuit? \n Answer: \n\nA.\n\nA sequential feedback circuit is a circuit where the outputs of some gates are fed back into the inputs of others. A clocked synchronous sequential circuit is a circuit that uses a clock signal to control when values change."}, {"text": "Title: Static Hazards: Causes and Cures* \n Text: {Static Hazards: Causes and Cures*}\n\nBefore we forget about the fact that real designs do not provide\nperfect clocks, let's explore some of the issues that engineers\nmust sometimes face.  \n\nWe discuss these primarily to ensure that you appreciate the power\nof the abstraction that we use in the rest of our course.\nIn later classes (probably our 298, which will absorb material \nfrom 385), you may be required to master this material.\n{ For now, we provide it simply for your interest.}\n\nConsider the circuit shown below, for which the output is given by \nthe equation S=AB+. \n\n{{file=part2/figs/lec15-1.eps,width=4in}}\n\nThe timing diagram on the right shows a { glitch} in the output\nwhen the input shifts from ABC=110 to 100, that is, when B falls.\nThe problem lies in the possibility that the upper AND gate, driven \nby B, might go low before the lower AND gate, driven by , goes\nhigh.  In such a case, the OR gate output S falls until the second\nAND gate rises, and the output exhibits a glitch.\n\nA circuit that might exhibit a glitch in an output that functionally\nremains stable at 1 is said to have a { static-1 hazard}.  The\nqualifier ``static'' here refers to the fact that we expect the output\nto remain static, while the ``1'' refers to the expected value of the\noutput.\n\n\n\nThe presence of hazards in circuits can be problematic in certain\ncases.  In domino logic, for example, an output is precharged and kept\nat 1 until the output of a driving circuit pulls it to 0, at which\npoint it stays low (like a domino that has been knocked over).  If the\ndriving circuit contains {static-1} hazards, the output may fall\nin response to a glitch.\n\nSimilarly, hazards can lead to unreliable behavior in sequential\nfeedback circuits.  Consider the addition of a feedback loop to the\ncircuit just discussed, as shown in the figure below.  The output of\nthe circuit is now given by the equation S^*=AB+S,\nwhere S^* denotes the state after S feeds back through the lower\nAND gate.  In the case discussed previously, the transition from\nABC=110 to 100, the glitch in S can break the feedback, leaving\nS low or unstable.  The resulting sequential feedback circuit is\nthus unreliable.  \n\n{{file=part2/figs/lec15-2.eps,width=4in}}\n\n\nEliminating static hazards from {two-level} circuits\nis fairly straightforward.  The Karnaugh map to the right corresponds\nto our original circuit; the solid lines indicate the \nimplicants selected by the AND gates.  A {static-1} hazard is\npresent when two adjacent 1s in the {K-map} are not covered by\na common implicant.  {Static-0} hazards do not occur in\n{two-level} SOP circuits.\n\n\n{file=part2/figs/lec15-3.eps,width=1in}\n\n\nEliminating static hazards requires merely extending the circuit with\nconsensus terms in order to ensure that some AND gate remains high\nthrough every transition between input states with\noutput 1.{Hazard elimination is not in general simple; we\nhave considered only {two-level} circuits.}  In the\n{K-map} shown, the dashed line indicates the necessary\nconsensus term, A.\n\n\n\n \n Question: \nQ. Which type of hazard does not occur in two-level SOP circuits? \n Answer: \n\nA. Static-0 hazards."}, {"text": "Title: Dynamic Hazards* \n Text: {Dynamic Hazards*}\n\nConsider an input transition for which we expect to see a change in an\noutput.  Under certain timing conditions, the output may not\ntransition smoothly, but instead bounce between its original value\nand its new value before coming to rest at the new value.  A circuit\nthat might exhibit such behavior is said to contain a { dynamic hazard}.\nThe qualifier ``dynamic'' refers to the expected change in the output.\n\nDynamic hazards appear only in more complex circuits, such as the one\nshown below.  The output of this circuit is defined by the equation Q=B+++BD. \n\n{{file=part2/figs/lec15-4.eps,width=3in}}\n\n\nConsider the transition from the input state ABCD=1111 to 1011,\nin which B falls from 1 to 0.  For simplicity, assume that each\ngate has a delay of 1 time unit.  If B goes low at time T=0, the\ntable shows the progression over time of logic levels at several\nintermediate points in the circuit and at the output Q.  Each gate\nmerely produces the appropriate output based on its inputs in the\nprevious time step.  After one delay, the three gates with B as a\ndirect input change their outputs (to stable, final values).  After\nanother delay, at T=2, the other three gates\nre-{-8pt}\n\n\n{|c|c|c|c|c|c|c|}\nT&f&g&h&i&j&Q \n0& 0& 0& 0& 1& 1& 1\n1& 1& 1& 1& 1& 1& 1\n2& 1& 1& 1& 0& 0& 0\n3& 1& 1& 1& 0& 1& 1\n4& 1& 1& 1& 0& 1& 0\n\n\nspond to the initial changes and flip their outputs.  The resulting\nchanges induce another set of changes at T=3, which in turn causes\nthe output Q to change a final time at T=4.\n\nThe output column in the table illustrates the possible impact of a\ndynamic hazard: rather than a smooth transition from 1 to 0, the\noutput drops to 0, rises back to 1, and finally falls to 0\nagain.  The dynamic hazard in this case can be attributed to the\npresence of a static hazard in the logic that produces intermediate\nvalue j.\n\n\n\n \n Question:  \n Answer: \nThe output Q of the circuit will have a value of 1 at time t because:\n\nA. The output of the gate with input B is 1.\n\nB. The output of the gate with input B is 0.\n\nC. The output of the gate with input B is 1, and the output of the gate with input D is 0.\n\nD. The output of the gate with input B is 0, and the output of the gate with input D is 1."}, {"text": "Title: Essential Hazards* \n Text: {Essential Hazards*}\n\n{ Essential hazards} are inherent to the function of a circuit and\nmay appear in any implementation.  In sequential feedback circuit\ndesign, they must be addressed at a low level to ensure that\nvariations in logic path lengths ({ timing skew}) through a circuit\ndo not expose them.  With clocked synchronous circuits, essential\nhazards are abstracted into a single form: { clock skew}, or\ndisparate clock edge arrival times at a circuit's flip-flops.\n\nAn example demonstrates the possible effects: consider the\nconstruction of a clocked synchronous circuit to recognize {0-1}\nsequences on an input IN.  Output Q should be held high for one\ncycle after recognition, that is, until the next rising clock edge.  A\ndescription of states and a state diagram for such a circuit appear\nbelow.\n\n\n{\n\nS_1S_0 &state& meaning\n00& A& nothing, 1, or 11 seen last\n01& B& 0 seen last\n10& C& 01 recognized (output high)\n11& unused&\n\n}\n\n\n{{file=part2/figs/lec15-5.eps,width=2in}}\n\n\nFor three states, we need two (=_2 3) flip-flops.\nDenote the internal state S_1S_0.  The specific internal\nstate values for each logical state (A, B, and C) simplify the\nimplementation and the example.  A { state table} \nand {K-maps} for the next-state logic appear\nbelow.  The state table uses one line per state with separate \ncolumns\nfor each input combination, making the table more compact than one\nwith one line per state/input combination.  Each column contains the\nfull next-state information, including output.  Using this form of the\nstate table, the {K-maps} can be read directly from the table.\n\n\n{\n\n& {IN}\nS_1S_0& 0& 1 \n00& 01/0& 00/0\n01& 01/0& 10/0\n11& x& x\n10& 01/1& 00/1\n\n}\n\n\n{{file=part2/figs/lec15-6.eps,width=3.5in}}\n\n\nExamining the {K-maps}, we see that the excitation and output\nequations are S_1^+=IN S_0, S_0^+=, and Q=S_1.\nAn implementation of the circuit using two D flip-flops appears below.\nImagine that mistakes in routing or process variations have made the\nclock signal's path to flip-flop 1 much longer than its path into\nflip-flop 0, as illustrated.\n\n{{file=part2/figs/lec15-7.eps,width=3in}}\n\nDue to the long delays, we cannot assume that rising clock edges\narrive at the flip-flops at the same time.  The result is called clock\nskew, and can make the circuit behave improperly by exposing essential\nhazards.  In the logical B to C transition, for example, we begin in\nstate S_1S_0=01 with IN=1 and the clock edge rising.  Assume that\nthe edge reaches flip-flop 0 at time T=0.  After a flip-flop delay\n(T=1), S_0 goes low.  After another AND gate delay (T=2), input\nD_1 goes low, but the second flip-flop has yet to change state!\nFinally, at some later time, the clock edge reaches flip-flop 1.\nHowever, the output S_1 remains at 0, leaving the system in state A\nrather than state C.\n\nFortunately, in clocked synchronous sequential circuits, all essential\nhazards are related to clock skew.  This fact implies that we can\neliminate a significant amount of complexity from circuit design by\ndoing a good job of distributing the clock signal.  It also implies\nthat, as a designer, you should avoid specious addition of logic in a\nclock path, as you may regret such a decision later, as you try to\ndebug the circuit timing.\n\n\n\n \n Question: \nQ. Hazards are inherent to the function of a circuit. \n Answer: \n\nA. True"}, {"text": "Title: Proof Outline for Clocked Synchronous Design* \n Text: {Proof Outline for Clocked Synchronous Design*}\n\nThis section outlines a proof of the claim made regarding clock\nskew being the only source of essential hazards for clocked\nsynchronous sequential circuits.  A { proof outline} suggests\nthe form that a proof might take and provides some of the logical\narguments, but is not rigorous enough to be considered a proof.\nHere we use a D flip-flop to illustrate a method for identifying\nessential hazards ({ the D flip-flop has no essential hazards, however}),\nthen argue that the method can be applied generally to collections\nof flip-flops in a clocked synchronous design to show that essential\nhazards occur only in the form of clock skew. \n\n\n{\n\n&{1-2}\nlow&L&clock low, last input low\nhigh&H&clock high, last input low\npulse low&PL&clock low, last input high (output high, too)\npulse high&PH&clock high, last input high (output high, too)\n\n}\n\n\n{{file=part2/figs/lec15-8.eps,width=2in}}\n\n\nConsider the sequential feedback state table for a positive\nedge-triggered D flip-flop, shown above.  In designing and analyzing\nsuch circuits, we assume that only one input bit changes at a time.\nThe state table consists of one row for each state and one column for\neach input combination.  Within a row, input combinations that have no\neffect on the internal state of the circuit (that is, those that do \nnot cause any change in\nthe state) are said to be stable; these states are circled.  Other\nstates are unstable, and the circuit changes state in response to\nchanges in the inputs.\n\nFor example, given an initial state L with low output, low clock,\nand high input D, the solid arcs trace the reaction of the circuit to a\nrising clock edge.  From the 01 input combination, we move along the\ncolumn to the 11 column, which indicates the new state, PH.  Moving\ndown the column to that state's row, we see that the new state is\nstable for the input combination 11, and we stop.  If PH were not\nstable, we would continue to move within the column until coming to\nrest on a stable state.\n\nAn essential hazard appears in such a table as a difference between\nthe final state when flipping a bit once and the final state when\nflipping a bit thrice in succession.  The dashed arcs in the figure\nillustrate the concept: after coming to rest in the PH state, we reset\nthe input to 01 and move along the PH row to find a new state of PL.\nMoving up the column, we see that the state is stable.  We then flip\nthe clock a third time and move back along the row to 11, which\nindicates that PH is again the next state.  Moving down the column, we\ncome again to rest in PH, the same state as was reached after one\nflip.  Flipping a bit three times rather than once evaluates the\nimpact of timing skew in the circuit; if a different state is reached\nafter two more flips, timing skew could cause unreliable behavior.  As\nyou can verify from the table, a D flip-flop has no essential hazards.\n\nA group of flip-flops, as might appear in a clocked synchronous\ncircuit, can and usually does have essential hazards, but only dealing\nwith the clock.  As you know, the inputs to a clocked synchronous\nsequential circuit consist of a clock signal and other inputs (either\nexternal of fed back from the flip-flops).  Changing an input other\nthan the clock can change the internal state of a flip-flop (of the\n\n master-slave variety), but flip-flop designs do not capture the number\n\ndual-latch variety), but flip-flop designs do not capture the number\nof input changes in a clock cycle beyond one, and changing an input\nthree times is the same as changing it once.  Changing the clock, of\ncourse, results in a synchronous state machine transition.\n\nThe detection of essential hazards in a clocked synchronous design\nbased on flip-flops thus reduces to examination of the state machine.\nIf the next state of the machine has any dependence on the current\nstate, an essential hazard exists, as a second rising clock edge moves\nthe system into a second new state.  For a single D flip-flop, the\nnext state is independent of the current state, and no essential\nhazards are present.\n\n\n\n\n\n \n Question: \nQ. What is an essential hazard? \n Answer: \n\nAn essential hazard is a difference between the final state when flipping a bit once and the final state when flipping a bit thrice in succession."}, {"text": "Title: Using Abstraction to Simplify Problems \n Text: {Using Abstraction to Simplify Problems}\n\nIn this set of notes, we illustrate the use of abstraction to simplify\nproblems, then introduce a component called a multiplexer that allows\nselection among multiple inputs.\n\nWe begin by showing how two specific \nexamples---integer subtraction and identification of letters\nin ASCII---can be implemented using logic functions that we have already\ndeveloped.  We also introduce a conceptual technique for\nbreaking functions into smaller pieces, which allows us to solve\nseveral simpler problems and then to compose a full solution from \nthese partial solutions.\n\nTogether with the idea of bit-sliced designs that we introduced earlier,\nthese techniques help to simplify the process of designing logic that\noperates correctly.  The techniques can, of course, lead to \nless efficient designs,\nbut { correctness is always more important than performance}.\n\nThe potential loss of efficiency is often acceptable for three reasons.\n\nFirst, as we mentioned earlier, computer-aided design tools for \noptimizing logic functions are fairly effective, and in many cases\nproduce better results than human engineers (except in the rare cases \nin which the human effort required to beat the tools is worthwhile).\n\nSecond, as you know from the design of the {2's complement} \nrepresentation, we may be able to reuse specific pieces of hardware if\nwe think carefully about how we define our problems and representations.\n\nFinally, many tasks today are executed in software, which is designed\nto leverage the fairly general logic available via an instruction set\narchitecture.  A programmer cannot easily add new logic to a user's\nprocessor.  As a result, the hardware used to execute a\nfunction typically is not optimized for that function.\n\nThe approaches shown in this set of notes illustrate how abstraction\ncan be used to design logic.\n\n \n Question: \nQ. How does abstraction help to simplify problems? \n Answer: \n\nA. Abstraction helps to simplify problems by breaking them down into smaller pieces that can be more easily solved."}, {"text": "Title: Subtraction \n Text: \n\nOur discussion of arithmetic implementation has focused so far on \naddition.  What about other operations, such as subtraction, multiplication,\nand division?  The latter two require more work, and we will not\ndiscuss them in detail until later in our class (if at all).\n\nSubtraction, however, can be performed almost trivially using logic that\nwe have already designed.\n\nLet's say that we want to calculate the difference D between \ntwo {N-bit} numbers A and B.  In particular, we want \nto find D=A-B.  For now, think of A, B, and D \nas 2's complement values.\n\nRecall how we defined the 2's complement representation: \nthe {N-bit} pattern that we use to represent -B is the \nsame as the base 2 bit pattern for (2^N-B), so we can use an adder if we\nfirst calculate the bit pattern for -B, then add the resulting\npattern to A.\nAs you know, our {N-bit} adder always produces a result that\nis correct modulo 2^N, so the result of such an operation,\nD=2^N+A-B, is correct so long as the subtraction does not overflow.\n\n\nHow can we calculate 2^N-B?  The same way that we do by hand!\nCalculate the 1's complement, (2^N-1)-B, then add 1.\n\nThe diagram to the right shows how we can use the {N-bit} adder\nthat we designed in Notes Set 2.3 to build an {N-bit} subtracter.\n\nNew elements appear in blue in the figure---the rest of the logic\nis just an adder.  The box labeled ``1's comp.'' calculates \nthe {1's complement} of the value B, which together with the\ncarry in value of 1 correspond to calculating -B.  What's in the\n``1's comp.'' box?  One inverter per bit in B.  That's all \nwe need to calculate the 1's complement.\n\nYou might now ask: does this approach also work for unsigned numbers?\nThe answer is yes, absolutely.  However, the overflow conditions for\nboth 2's complement and unsigned subtraction are different than the\noverflow condition for either type of addition.  What does the carry\nout of our adder signify, for example?  The answer may not be \nimmediately obvious.\n\n\n\n\n\nLet's start with the overflow condition for unsigned subtraction.\n\nOverflow means that we cannot represent the result.  With an {N-bit}\nunsigned number, we have A-B[0,2^N-1].  Obviously, the difference\ncannot be larger than the upper limit, since A is representable and\nwe are subtracting a non-negative (unsigned) value.  We can thus assume\nthat overflow occurs only when A-B<0.  In other words, when A<B.\n\n\n\nTo calculate the unsigned subtraction overflow condition in terms of the\nbits, recall that our adder is calculating 2^N+A-B.  The carry out\nrepresents the 2^N term.  When A, the result of the adder\nis at least 2^N, and we see a carry out, C_=1.  However, when\nA<B, the result of the adder is less than 2^N, and we see no carry\nout, C_=0.  { Overflow for unsigned subtraction is thus inverted \nfrom overflow for unsigned addition}: a carry out of 0 indicates an \noverflow for subtraction.\n\nWhat about overflow for 2's complement subtraction?  We can use arguments\nsimilar to those that we used to reason about overflow of 2's complement\naddition to prove that subtraction of one negative number from a second\nnegative number can never overflow.  Nor can subtraction of a non-negative\nnumber from a second non-negative number overflow.\n\nIf A and B<0, the subtraction overflows iff A-B{2^{N-1}}.\nAgain using similar arguments as before, we can prove that the difference D\nappears to be negative in the case of overflow, so the product\n{A_{N-1}} B_{N-1} D_{N-1} evaluates to 1 when this type of\noverflow occurs (these variables represent the most significant bits of the \ntwo operands and the difference; in the case of 2's complement, they are\nalso the sign bits).\n\nSimilarly, if A<0 and B, we have overflow when A-B<-2^{N-1}.\nHere we can prove that D on overflow, so \nA_{N-1} {B_{N-1}} {D_{N-1}} evaluates to 1.\n\nOur overflow condition for {N-bit} 2's complement subtraction\nis thus given by the following:\n\n{eqnarray*}\n{A_{N-1}} B_{N-1} D_{N-1}+A_{N-1} {B_{N-1}} {D_{N-1}}\n{eqnarray*}\n\nIf we calculate all four overflow conditions---unsigned and 2's complement,\naddition and subtraction---and provide some way to choose whether or not to \ncomplement B and to control the C_ input, we can use the same hardware\nfor addition and subtraction of either type.\n\n \n Question: \nQ. What does the carry out of our adder signify for unsigned subtraction? \n Answer: \n\nA. A carry out of 1 for unsigned subtraction indicates an overflow."}, {"text": "Title: Checking ASCII for Upper-case Letters \n Text: {Checking ASCII for Upper-case Letters}\n\nLet's now consider how we can check whether or not an ASCII character is\nan upper-case letter.  Let's call the {7-bit}\nletter C=C_6C_5C_4C_3C_2C_1C_0 and the function that we want to\ncalculate U(C).  The function U should equal 1 whenever C represents\nan upper-case letter, and should equal 0 whenever C does not.\n\nIn ASCII, the {7-bit} patterns from 0x41 through 0x5A correspond\nto the letters A through Z in alphabetic order.  Perhaps you want to draw \na {7-input} {K-map}?  Get a few large sheets of paper!\n\nInstead, imagine that we've written the full {128-row} truth\ntable.  Let's break the truth table into pieces.  Each piece will\ncorrespond to one specific pattern of the three high bits C_6C_5C_4,\nand each piece will have 16 entries for the four low bits C_3C_2C_1C_0.\nThe truth tables for high bits 000, 001, 010, 011, 110, and 111\nare easy: the function is exactly 0.  The other two truth tables appear \non the left below.  We've called the two functions T_4 and T_5, where\nthe subscripts correspond to the binary value of the three high bits of C.\n\n{\n\n{cccc|cc}\nC_3& C_2& C_1& C_0& T_4& T_5 \n0& 0& 0& 0& 0& 1\n0& 0& 0& 1& 1& 1\n0& 0& 1& 0& 1& 1\n0& 0& 1& 1& 1& 1 \n0& 1& 0& 0& 1& 1\n0& 1& 0& 1& 1& 1\n0& 1& 1& 0& 1& 1\n0& 1& 1& 1& 1& 1 \n1& 0& 0& 0& 1& 1\n1& 0& 0& 1& 1& 1\n1& 0& 1& 0& 1& 1\n1& 0& 1& 1& 1& 0 \n1& 1& 0& 0& 1& 0\n1& 1& 0& 1& 1& 0\n1& 1& 1& 0& 1& 0\n1& 1& 1& 1& 1& 0\n\n\n\n{file=part2/figs/ascii-t4.eps,width=1.15in}\n{file=part2/figs/ascii-t5.eps,width=1.15in}\n\n\n{eqnarray*}\nT_4&=&C_3+C_2+C_1+C_0      \nT_5&=&{C_3}+{C_2} {C_1}+{C_2} {C_0}\n{eqnarray*}\n\n\nAs shown to the right of the truth tables, we can then draw simpler\n{K-maps} for T_4 and T_5, and can solve the {K-maps}\nto find equations for each, as shown to the right (check that you get\nthe same answers).\n\nHow do we merge these results to form our final expression for U?\n\nWe AND each of the term functions (T_4 and T_5) with the \nappropriate minterm for the\nhigh bits of C, then OR the results together, as shown here:\n\n{eqnarray*}\nU&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5\n\n&=&C_6 {C_5} {C_4} (C_3+C_2+C_1+C_0)+\nC_6 {C_5} C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0})\n{eqnarray*}\n\nRather than trying to optimize by hand, we can at this point let the CAD\ntools take over, confident that we have the right function to identify\nan upper-case ASCII letter.\n\nBreaking the truth table into pieces and using simple logic to reconnect\nthe pieces is one way to make use of abstraction when solving complex\nlogic problems.  \n\nIn fact, recruiters for some companies often ask\nquestions that involve using specific logic elements as building blocks\nto implement other functions.  Knowing that you can implement\na truth table one piece at a time will help you to solve this type of\nproblem.\n\nLet's think about other ways to tackle the problem of calculating U.\nIn Notes Sets 2.3 and 2.4, we developed adders and comparators.  Can\nwe make use of these components as building blocks to check whether C \nrepresents an\nupper-case letter?  Yes, of course we can: by comparing C with the\nends of the range of upper-case letters, we can check whether or not C\nfalls in that range.\n\nThe idea is illustrated on the left below using two {7-bit} comparators\nconstructed as discussed in Notes Set 2.4.\nThe comparators are the black parts of the drawing, while the blue parts\nrepresent our extensions to calculate U.  Each comparator is given\nthe value C as one input.  The second value to the comparators is \neither the letter A (0x41) or the letter Z (0x5A).  The meaning of \nthe {2-bit} input to and result of each comparator is given in the \ntable on the right below.  The inputs on the right of each comparator\nare set to 0 to ensure that equality is produced if C matches\nthe second input (B).\n\nOne output from each comparator is then routed to\na NOR gate to calculate U.  Let's consider how this combination works.\nThe left comparator compares C with the letter A (0x41).  If C 0x41,\nthe comparator produces Z_0=0.  In this case, we may have a letter.\nOn the other hand, if C< 0x41, the comparator produces Z_0=1, and\nthe NOR gate outputs U=0, since we do not have a letter in this case.\n\nThe right comparator compares C with the letter Z (0x5A).  If C 0x5A,\nthe comparator produces Z_1=0.  In this case, we may have a letter.\nOn the other hand, if C> 0x5A, the comparator produces Z_1=1, and\nthe NOR gate outputs U=0, since we do not have a letter in this case.\n\nOnly when 0x41  0x5A does U=1, as desired. \n\n\n\n{\n\n{file=part2/figs/ascii-cmp-based.eps,width=3.6in}\n\n\n{cc|l}\nZ_1& Z_0& meaning \n0& 0& A=B\n0& 1& A<B\n1& 0& A>B\n1& 1& not used\n\n\n\n\n\n\n\n\n\nWhat if we have only {8-bit} adders available for our use, such as\nthose developed in Notes Set 2.3?  Can we still calculate U?  Yes.\nThe diagram shown to the right illustrates the approach, again with black\nfor the adders and blue for our extensions.  Here we are \nactually using the adders as subtracters, but calculating the 1's complements\nof the constant values by hand.  The ``zero extend'' box simply adds\na leading 0 to our {7-bit} ASCII letter.  The left adder \nsubtracts the letter A from C: if no carry is produced, we know that\nC< 0x41 and thus C does not represent an upper-case letter, and U=0.\n\nSimilarly, the right adder subtracts 0x5B (the letter Z plus one)\nfrom C.  If a carry is produced, we know that C 0x5B, and \nthus C does not represent an upper-case letter, and U=0.\n\nWith the right combination of carries (1 from the left and 0 from the\nright), we obtain U=1.\n\n\n{file=part2/figs/ascii-add-based.eps,width=2.75in}\n\n\nLooking carefully at this solution, however, you might be struck by the\nfact that we are calculating two sums and then discarding them.  Surely\nsuch an approach is inefficient?\n\nWe offer two answers.  First, given the design shown above, a good CAD tool\nrecognizes that the sum outputs of the adders are not being used,\nand does not generate logic to calculate them.  The logic for the two \ncarry bits used to calculate U can then be optimized.  Second, the design\nshown, including the calculation of the sums, is similar in efficiency\nto what happens at the rate of about 10^ times per second, \n24 hours a day, seven days a week,\ninside processors in data centers processing HTML, XML, and other types\nof human-readable Internet traffic.  Abstraction is a powerful tool.\n\nLater in our class, you will learn how to control logical connections\nbetween hardware blocks so that you can make use of the same hardware\nfor adding, subtracting, checking for upper-case letters, and so forth.\n\n \n Question: \nQ. What does the function U represent in this context? \n Answer: \n\nA. U represents the value of an ASCII character compared to the letters A through Z. If the character is equal to or greater than A and less than or equal to Z, then U will equal 1. If the character is not in that range, then U will equal 0."}, {"text": "Title: Checking ASCII for Lower-case Letters \n Text: {Checking ASCII for Lower-case Letters}\n\nHaving developed several approaches for checking for an upper-case letter,\nthe task of checking for a lower-case letter should be straightforward.\nIn ASCII, lower-case letters are represented by the {7-bit} patterns \nfrom 0x61 through 0x7A.\n\nOne can now easily see how more abstract designs make solving similar\ntasks easier.  If we have designed our upper-case checker \nwith {7-variable} K-maps,\nwe must start again with new K-maps for the lower-case checker.  If\ninstead we have taken the approach of designing logic for the upper and \nlower bits of the ASCII character, we can reuse most of that logic,\nsince the functions T_4 and T_5 are identical when checking for\na lower-case character.  Recalling the algebraic form of U(C), we can\nthen write a function L(C) (a lower-case checker) as shown on the\nleft below.\n\n\n{eqnarray*}\nU&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5\n\nL&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5\n\n&=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+\n&&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0})\n{eqnarray*}\n\n\n{file=part2/figs/ascii-cmp-lower.eps,width=3.6in}\n\n\nFinally, if we have used a design based on comparators or adders, the\ndesign of a lower-case checker becomes trivial: simply change the numbers\nthat we input to these components, as shown in the figure on the right above\nfor the comparator-based design.  The only changes from the upper-case checker\ndesign are the inputs to the comparators and the output produced, \nhighlighted with blue text in the figure.\n\n\n\n \n Question: \nQ. What is the advantage of using a design based on comparators or adders when checking for a lower-case letter? \n Answer: \n\nA. The advantage of using a design based on comparators or adders when checking for a lower-case letter is that the design becomes trivial: simply change the numbers that we input to these components."}, {"text": "Title: The Multiplexer \n Text: {The Multiplexer}\n\nUsing the more abstract designs for checking ranges of ASCII characters,\nwe can go a step further and create a checker for both upper- and lower-case\nletters.  To do so, we add another input S that allows us to select the \nfunction that we want---either the upper-case checker U(C) or the \nlower-case checker L(C).\n\nFor this purpose, we make use of a logic block called a { multiplexer},\nor { mux}.\nMultiplexers are an important abstraction for digital logic.  In \ngeneral, a multiplexer allows us to use one digital signal to \nselect which of several others is forwarded to an output.\n\n\nThe simplest form of the multiplexer is the 2-to-1 multiplexer shown to \nthe right.   The logic diagram illustrates how the mux works.  The block \nhas two inputs from the left and one from the top.  The top input allows \nus to choose which of the left inputs is forwarded to the output.  \nWhen the input S=0, the upper AND gate outputs 0, and the lower AND gate\noutputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0.\nSimilarly, when input S=1, the upper AND gate outputs D_1, and the\nlower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1.\n\n\n\n\n\nThe symbolic form of the mux is a trapezoid with data inputs on the \nlarger side, an output on the smaller side, and a select input on the\nangled part of the trapezoid.  The labels inside the trapezoid indicate \nthe value of the select input S for which the adjacent data signal, \nD_1 or D_0, is copied to the output Q.\n\nWe can generalize multiplexers in two ways.  First, we can extend the \nsingle select input to a group of select inputs.  An {N-bit}\nselect input allows selection from amongst 2^N inputs.  A {4-to-1} \nmultiplexer is shown below, for example.  The logic diagram on the left\nshows how the {4-to-1 mux} operates.  For any combination of S_1S_0,\nthree of the AND gates produce 0, and the fourth outputs the D input\ncorresponding to the interpretation of S as an unsigned number.\nGiven three zeroes and one D input, the OR gate thus reproduces one of \nthe D's.  When S_1S_0=10, for example, the third AND gate copies D_2,\nand Q=D_2.\n\n{{file=part2/figs/mux4-to-1.eps,width=5.60in}}\n\nAs shown in the middle figure, a {4-to-1} mux can also be built from\nthree {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux \nappears on the right in the figure.\n\n\n\nThe second way in which we can generalize multiplexers is by using\nseveral multiplexers of the same type and using the same signals for \nselection.  For example, we might use a single select bit T to choose \nbetween any number of paired inputs.  Denote input pair by i D_1^i \nand D_0^i.  For each pair, we have an output Q_i.  \n\nWhen T=0, Q_i=D_0^i for each value of i.\n\nAnd, when T=1, Q_i=D_1^i for each value of i.\n\nEach value of i requires a {2-to-1} mux with its select input\ndriven by the global select signal T.\n\nReturning to the example of the upper- and lower-case checker, we\ncan make use of two groups of seven {2-to-1} muxes, all controlled by\na single bit select signal S, to choose between the inputs needed\nfor an upper-case checker and those needed for a lower-case checker.\n\nSpecific configurations of multiplexers are often referred to\nas { N-to-M multiplexers}.  Here the value N refers to the\nnumber of inputs, and M refers to the number of outputs.  The\nnumber of select bits can then be calculated as _2(N/M)---N/M \nis generally a power of two---and one way to build such a \nmultiplexer is to use M copies of\nan  (N/M)-to-1 multiplexer.\n\nLet's extend our upper- and lower-case checker to check\nfor four different ranges of ASCII characters, as shown below.\nThis design uses two {28-to-7} muxes to create a single\nchecker for the four ranges.  Each of the muxes in the figure logically \nrepresents seven {4-to-1} muxes.\n\n{{file=part2/figs/ascii-four-range.eps,width=3.75in}}\n\n\nThe table to the right describes the behavior of the checker.\n\nWhen the select input S is set to 00, the left mux selects the value 0x00,\nand the right mux selects the value 0x1F, which checks whether the ASCII\ncharacter represented by C is a control character.\n\nWhen the select input S=01, the muxes produce the values needed to check\nwhether C is an upper-case letter.\n\nSimilarly, when the select input S=10,\n\n\n{c|c|c|c}\n& left& right& \n& comparator& comparator& \nS_1S_0& input& input& R(C) produced  \n00& 0x00& 0x1F& control character?\n01& 0x41& 0x5A& upper-case letter?\n10& 0x61& 0x7A& lower-case letter?\n11& 0x30& 0x39& numeric digit? \n\n\n\nthe muxes produce the values \nneeded to check whether C is a lower-case letter.\n\nFinally, when the select input S=11, the left mux selects the value 0x30,\nand the right mux selects the value 0x39, which checks whether the ASCII\ncharacter represented by C is a digit (0 to 9).\n\n\n\n\n \n Question: \nQ. Is the multiplexer a basic logic gate? \n Answer: \n\nA. No, the multiplexer is not a basic logic gate."}, {"text": "Title: Example: Bit-Sliced Comparison \n Text: {Example: Bit-Sliced Comparison}\n\nThis set of notes develops comparators for unsigned and 2's complement \nnumbers using the bit-sliced approach that we introduced in Notes Set 2.3.  \nWe then use algebraic manipulation and variation of the internal \nrepresentation to illustrate design tradeoffs.\n\n\n \n Question: \nQ. What is the difference between unsigned and 2's complement numbers? \n Answer: \n\nA.\n\nThe difference between unsigned and 2's complement numbers is that 2's complement numbers can represent both positive and negative numbers, while unsigned numbers can only represent positive numbers."}, {"text": "Title: Comparing Two Numbers \n Text: {Comparing Two Numbers}\n\n\nLet's begin by thinking about how we as humans compare two {N-bit}\nnumbers, A and B.\n\nAn illustration appears to the right, using N=8.\n\nFor now, let's assume that our numbers are stored in an unsigned \nrepresentation, so we can just think of them as binary numbers\nwith leading 0s.\n\nWe handle 2's complement values later in these notes.\n\nAs humans, we typically start comparing at the most significant bit.\nAfter all, if we find a difference in that bit, we are done, saving\nourselves some time.  In the example to the right, we know that A<B \nas soon as we reach bit 4 and observe that A_4<B_4.\n\nIf we instead start from the least significant bit,\nwe must always look at all of the bits.\n\nWhen building hardware to compare all of the bits at once, however,\nhardware for comparing each bit must exist, and the final result \nmust be able to consider\n\n\n\n\n\nall of the bits.  Our choice of direction should thus instead depend on \nhow effectively we can build the \ncorresponding functions.  For a single bit slice, the two directions \nare almost identical.  Let's develop a bit slice for\ncomparing from least to most significant.\n\n{ NOTE TO SELF: We should either do the bit-by-bit comparator \nstate machine in the notes or as an FSM homework problem.  Probably\nthe lattter.}\n\n \n Question: \nQ. What is the advantage of starting the comparison from the most significant bit? \n Answer: \n\nA. If a difference is found in the most significant bit, the comparison can be stopped, saving time."}, {"text": "Title: An Abstract Model \n Text: {An Abstract Model}\n\nComparison of two numbers, A and B, can produce three possible\nanswers: A<B, A=B, or A>B (one can also build an equality\ncomparator that combines the A<B and A>B cases into a single \nanswer).\n\nAs we move from bit to bit in our design, how much information needs \nto pass from one bit to the next?\n\nHere you may want to think about how you perform the task yourself.\nAnd perhaps to focus on the calculation for the most significant bit.\nYou need to know the values of the two bits that you are comparing.\nIf those two are not equal, you are done.\n\nBut if the two bits are equal, what do you do?\n\nThe answer is fairly simple: pass along the result\nfrom the less significant bits.\n\nThus our bit slice logic for bit M needs to be able to accept \nthree possible answers from the bit slice logic for bit M-1 and must\nbe able to pass one of three possible answers to the logic for bit M+1.\n\nSince _2(3)=2, we need two bits of input and two bits\nof output in addition to our input bits from numbers A and B.\n\n\nThe diagram to the right shows an abstract model of our \ncomparator bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming from the top or left \nand outputs going to the bottom or right.  Outside of the bit \nslice logic, we index\nthese comparison bits using the bit number.  The bit slice\nhas C_1^{M-1} and C_0^{M-1} provided as inputs and \nproduces C_1^M and C_0^M as outputs.\n\nInternally, we use C_1 and C_0 to denote these inputs,\nand Z_1 and Z_0 to denote the outputs.\n\nSimilarly, the\n\n\n\n\n\nbits A_M and B_M from the numbers A and B are\nrepresented internally simply as A and B.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.\n\n\n\n\n \n Question: \nQ. What does the bit slice logic need to be able to do? \n Answer: \n\nA. The bit slice logic needs to be able to accept three possible answers from the bit slice logic for bit M-1 and must be able to pass one of three possible answers to the logic for bit M+1."}, {"text": "Title: A Representation and the First Bit \n Text: {A Representation and the First Bit}\n\n\nWe need to select a representation for our three possible answers before\nwe can design any logic.  The representation chosen affects the\nimplementation, as we discuss later in these notes.  For now, we simply\nchoose the representation to the right, which seems reasonable.\n\nNow we can design the logic for the first bit (bit 0).  In keeping with\nthe bit slice philosophy, in practice we simply use another\ncopy of the full bit slice design for bit 0 and attach the C_1C_0 \ninputs to ground (to denote A=B).  Here we tackle the simpler problem\nas a warm-up exercise.\n\n\n{cc|l}\nC_1& C_0& meaning \n0& 0& A=B\n0& 1& A<B\n1& 0& A>B\n1& 1& not used\n\n\n\n\nThe truth table for bit 0 appears to the right (recall that we use Z_1\nand Z_0 for the output names).  Note that the bit 0 function has\nonly two meaningful inputs---there is no bit to the right of bit 0.\n\nIf the two inputs A and B are the same, we output equality.\nOtherwise, we do a {1-bit} comparison and use our representation mapping\nto select the outputs.\n\nThese functions are fairly straightforward to derive by inspection.\nThey are:{-12pt}\n\n{eqnarray*}\nZ_1 &=& A \nZ_0 &=&  B\n{eqnarray*}\n\n\n{\n{cc|cc}\nA& B& Z_1& Z_0 \n0& 0& 0& 0\n0& 1& 0& 1\n1& 0& 1& 0\n1& 1& 0& 0\n\n}\n\n\nThese forms should also be intuitive, given the representation\nthat we chose: \nA>B if and only if A=1 and B=0;\nA<B if and only if A=0 and B=1. \n\n\nImplementation diagrams for our one-bit functions appear to the right.\nThe diagram to the immediate right shows the implementation as we might\ninitially draw it, and the diagram on the far right shows the \nimplementation\n\n\n\n\n\n\n\n\nconverted to NAND/NOR gates for a more accurate estimate of complexity\nwhen implemented in CMOS.\n\nThe exercise of designing the logic for bit 0 is also useful in the sense\nthat the logic structure illustrated forms the core of the full design\nin that it identifies the two cases that matter: A<B and A>B.\n\nNow we are ready to design the full function.  Let's start by writing\na full truth table, as shown on the left below.\n\n[t]\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0\n0& 0& 1& 1& x& x \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1\n0& 1& 1& 1& x& x \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0\n1& 0& 1& 1& x& x \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0\n1& 1& 1& 1& x& x\n\n\n\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0 \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1 \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0 \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0 \nx& x& 1& 1& x& x\n  \n\n\n\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0 \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1 \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0 \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0 \n{c|}& x& x\n  \n\n\n\nIn the truth table, we \nmarked the outputs as ``don't care'' (x's) whenever C_1C_0=11.\nYou might recall that we ran into problems with our ice cream dispenser\ncontrol in Notes Set 2.2.  However, in that case we could not safely\nassume that a user did not push multiple buttons.  Here, our bit\nslice logic only accepts inputs from other copies of itself (or a fixed\nvalue for bit 0), and---assuming that we design the logic\ncorrectly---our bit slice never generates the 11 combination.\nIn other words, that input combination is impossible (rather than\nundesirable or unlikely), so the result produced on the outputs is \nirrelevant.\n\nIt is tempting to shorten the full truth table by replacing groups of rows.\nFor example, if AB=01, we know that A<B, so the less significant\nbits (for which the result is represented by the C_1C_0 inputs)\ndon't matter.  We could write one row with input pattern ABC_1C_0=01xx and\noutput pattern Z_1Z_0=01.  We might also collapse our ``don't care'' output\npatterns: whenever the input matches ABC_1C_0=xx11, we don't care\nabout the output, so Z_1Z_0=xx.  But these two rows overlap in the\ninput space!  In other words, some input patterns, such as ABC_1C_0=0111,\nmatch both of our suggested new rows.  Which output should take precedence?\nThe answer is that a reader should not have to guess.  { Do not use \noverlapping rows to shorten a truth table.}  In fact, the first of the \nsuggested new rows is not valid: we don't need to produce output 01 \nif we see C_1C_0=11.  Two valid short forms of this truth table appear to \nthe right of the full table.  If you have an ``other'' entry, as\nshown in the rightmost table, this entry should always appear as the \nlast row.  Normal rows, including rows representing multiple input\npatterns, are not required\nto be in any particular order.  Use whatever order makes the table\neasiest to read for its purpose (usually by treating the input pattern\nas a binary number and ordering rows in increasing numeric order).\n\n\nIn order to translate our design into algebra, we transcribe the truth\ntable into a {K-map} for each output variable, as shown to the right.\nYou may want to perform this exercise yourself and check that you \nobtain the same solution.  Implicants for each output are marked\nin the {K-maps}, giving the following equations:\n\n{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\nZ_0 &=&  B +  C_0 + B C_0 \n{eqnarray*}\n\n\n\n\n\n\n\n\n\nAn implementation based on our equations appears to the right.  \nThe figure makes it easy to see the symmetry between the inputs, which\narises from the representation that we've chosen.  Since the design\nonly uses two-level logic (not counting the inverters on the A\nand B inputs, since inverters can be viewed as {1-input} \nNAND or NOR gates), converting to NAND/NOR simply requires replacing\nall of the AND and OR gates with NAND gates.\n\nLet's discuss the design's efficiency roughly in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\n\nOur initial design uses two inverters, six {2-input} gates, and \ntwo {3-input} gates.\n\nFor speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.  \n\nWe can thus estimate our design's\n\n\n{file=part2/figs/comparator-try-one.eps,width=2.8in}\n\n\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer,\nbut, as we have discussed, the diagram above is equivalent on a \ngate-for-gate basis.  Here we have three gate delays from the A \nand B inputs to the outputs (through the inverters).\n\nBut when we connect multiple copies of our bit slice logic together to \nform a comparator, as shown on the next page, the delay from \nthe A and B inputs\nto the outputs is not as important as the delay from the C_1 and C_0 \ninputs to the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis.  Looking again at the diagram, \nnotice that we have only two gate delays from the C_1 and C_0 inputs \nto the\noutputs.  The total delay for an {N-bit} comparator based on this\nimplementation is thus three gate delays for bit 0 and two more gate\ndelays per additional bit, for a total of 2N+1 gate delays.\n\n\n\n\n \n Question: \nQ. [\nQ. \nQ. Q: What does it mean when a K-map is marked with an ``x'' in a row?\nQ. \nQ. A: It means that the output is ``don't care'' for that input pattern.\nQ. \nQ. ]\nQ. \nQ. \nQ. \nQ. [\nQ. \nQ. Q: How can you convert a truth table to a K-map?\nQ. \nQ. A: Here is a method that is easy to use, but not necessarily the best.\nQ. \nQ. {\nQ. \\begin{enumerate}\nQ. \\item  If a row has a ``0'' in the output column, mark a ``0'' in the\nQ.   corresponding cell (column and row) of the K-map.\nQ. \\item  If a row has a ``1'' in the output column, mark a ``1'' in the\nQ.   corresponding cell (column and row) of the K-map.\nQ. \\item  If a row has an ``x'' in the output column, mark an ``x'' in the\nQ.   corresponding cell (column and row) of the K-map.\nQ. \\item  Mark all other cells of the K-map with ``x.''\nQ. \\item  Repeat for each output of the truth table.\nQ. \\end{enumerate} \n Answer: \nQ. }\nQ. \nQ. ]\n\n\n\n[{The One-Bit Comparator in More Detail}\n\n\nBefore moving to the full design, let's briefly discuss the design\nof the bit slice logic more carefully, starting with the one-bit\ncomparator.\n\nOur truth table for bit 0 included four empty rows (indicated by ``x'').\nThese rows are not needed since we know the values of the C_1C_0 inputs\nfor those rows.  Thus, we can remove those rows from the truth table,\ngiving the truth table shown to the right.\n\n{file=part2/figs/ice-cream-try-one.eps,width=3.3in}\n\nThe truth table and the {K-maps} (shown to the right below)\nindicate that our design should be fairly simple.  In fact, we can\nderive the following {implicants} for each output variable:\n\n{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\nZ_0 &=&  B +  C_0 + B C_0 "}, {"text": "Title: Optimizing Our Design \n Text: {Optimizing Our Design}\n\nWe have a fairly good design at this point---good enough for a homework\nor exam problem in this class, certainly---but let's consider how we\nmight further optimize it.  Today, optimization of logic at this level is \ndone mostly by computer-aided design (CAD) tools, but we want you to \nbe aware of the sources of optimization potential and the tradeoffs \ninvolved.  And, if the topic interests you, someone has to continue to \nimprove CAD software!\n\nThe first step is to manipulate our algebra to expose common terms that\noccur due to the design's symmetry.  Starting with our original equation\nfor Z_1, we have\n\n{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\n    &=& A  + ( A +  )  C_1\n    &=& A  +  B} C_1\n     Z_0 &=&  B + {A  C_0\n{eqnarray*}{-12pt}\n\nNotice that the second term in each equation now includes the complement of \nfirst term from the other equation.  For example, the Z_1 equation includes\nthe complement of the B product that we need to compute Z_0.  \nWe may be able to improve our design by combining these computations.\n\n\nAn implementation based on our new algebraic formulation appears to the \nright.  In this form, we seem to have kept the same number of gates,\nalthough we have replaced the {3-input} gates with inverters.\nHowever, the middle inverters disappear when we convert to NAND/NOR form,\nas shown below to the right.  Our new design requires only two inverters\nand six {2-input} gates, a substantial reduction relative to the\noriginal implementation.\n\nIs there a disadvantage?  Yes, but only a slight one.  Notice that the\npath from the A and B inputs to the outputs is now four gates (maximum)\ninstead of three.  Yet the path from C_1 and C_0 to the outputs is\nstill only two gates.  Thus, overall, we have merely increased our\n{N-bit} comparator's delay from 2N+1 gate delays to\n2N+2 gate delays.\n\n\n{file=part2/figs/comparator-opt.eps,width=4.1in}\n\n{file=part2/figs/comparator-opt-nn.eps,width=4.1in}\n\n\n\n\n \n Question: \nQ. Why is it advantageous to have a shorter path from the A and B inputs to the outputs? \n Answer: \n\nA. A shorter path from the A and B inputs to the outputs results in a faster overall comparator."}, {"text": "Title: Extending to 2's Complement \n Text: {Extending to 2's Complement}\n\nWhat about comparing 2's complement numbers?  Can we make use of the\nunsigned comparator that we just designed?\n\nLet's start by thinking about the sign of the numbers A and B.\nRecall that 2's complement records a number's sign in the most \nsignificant bit.  For example, in the {8-bit} numbers shown in the \nfirst diagram in this set of notes, the sign \nbits are A_7 and B_7.\n\nLet's denote these sign bits in the general case by A_s and B_s.\n\nNegative numbers have a sign bit equal to 1, and non-negative numbers\nhave a sign bit equal to 0.\n\nThe table below outlines an initial evaluation of the four possible\ncombinations of sign bits.\n\n{cc|c|l}\nA_s& B_s& interpretation& solution \n0& 0& A AND B& use unsigned comparator on remaining bits\n0& 1& A AND B<0& A>B\n1& 0& A<0 AND B& A<B\n1& 1& A<0 AND B<0& unknown\n\n\n\nWhat should we do when both numbers are negative?  Need we design \na completely separate logic circuit?  Can we somehow convert a \nnegative value to a positive one?\n\nThe answer is in fact much simpler.  Recall that {2's complement}\nis defined based on modular arithmetic.  Given an {N-bit} negative\nnumber A, the representation for the bits A[N-2:0] is the same\nas the binary (unsigned) representation of A+2^{N-1}.  An example\nappears to the right.\n\n\n{file=part2/figs/comparing-2s.eps,width=2.55in}\n\n\nLet's define A_r=A+2^{N-1} as the value of the \nremaining bits for A and B_r similarly for B.\n\nWhat happens if we just go ahead and compare A_r and B_r using \nan {(N-1)-bit} unsigned comparator?\n\nIf we find that A_r<B_r we know that A_r-2^{N-1}<B_r-2^{N-1} as well, \nbut that means A<B!  We can do the same with either of the other\npossible results.  In other words, simply comparing A_r with B_r\ngives the correct answer for two negative numbers as well.\n\n\nAll we need to design is a logic block for the sign bits.  At this point,\nwe might write out a {K-map}, but instead let's rewrite our\nhigh-level table with the new information, as shown to the right.\n\nLooking at the table, notice the similarity \nto the high-level design for a single bit of an unsigned value.  The\n\n\n{cc|l}\nA_s& B_s& solution \n0& 0& pass result from less significant bits\n0& 1& A>B\n1& 0& A<B\n1& 1& pass result from less significant bits\n\n\n\nonly difference is that the two A=B cases are reversed.  If we\nswap A_s and B_s, the function is identical.  We can simply use\nanother bit slice but swap these two inputs.\n\nImplementation of an {N-bit} 2's complement comparator based\non our bit slice comparator is shown below.  The blue circle highlights\nthe only change from the {N-bit} unsigned comparator, which is\nto swap the two inputs on the sign bit.\n\n{{file=part2/figs/integrated-2s.eps,width=5.5in}}\n\n\n\n \n Question: \nQ. What is the difference between the two's complement comparator and the unsigned comparator? \n Answer: \n\nA. The only difference is that the two A=B cases are reversed."}, {"text": "Title: Further Optimization \n Text: {Further Optimization}\n\n\nLet's return to the topic of optimization.  To what extent \ndid the representation of the three outcomes affect our ability\nto develop a good bit slice design?  Although selecting a good \nrepresentation can be quite important, for this particular problem\nmost representations lead to similar implementations.\n\nSome representations, however, have interesting properties.  Consider\n\n\n{cc|l|l}\nC_1& C_0& original& alternate \n0& 0& A=B& A=B\n0& 1& A<B& A>B\n1& 0& A>B& not used\n1& 1& not used& A<B\n\n\n\nthe alternate representation on the right, for example (a copy of the \noriginal representation is included for comparison).  Notice that \nin the alternate representation, C_0=1 whenever A=B.  \n\nOnce we have found the numbers to be different in some bit, the end\nresult can never be equality, so perhaps with the right \nrepresentation---the new one, for example---we might be able to\ncut delay in half?\n\n\nAn implementation based on the alternate representation appears in the\ndiagram to the right.  As you can see, in terms of gate count,\nthis design replaces one {2-input} gate with an inverter and\na second {2-input} gate with a {3-input} gate.  The path\nlengths are the same, requiring 2N+2 gate delays for \nan {N-bit} comparator.\nOverall, it is about the same as our original design.\n\n\n{file=part2/figs/comparator-opt-alt.eps,width=4.1in}\n\n\nWhy didn't it work?  Should we consider still other representations?\nIn fact, none of the possible representations that we might choose\nfor a bit slice can cut the delay down to one gate delay per bit.  \nThe problem is fundamental, and is related to the nature of CMOS.\nFor a single bit slice, we define the incoming and outgoing \nrepresentations to be the same.  We also need to have at least\none gate in the path to combine the C_1 and C_0 inputs with\ninformation from the bit slice's A and B inputs.  But all CMOS\ngates invert the sense of their inputs.  Our choices are limited\nto NAND and NOR.  Thus we need at least two gates in the path to\nmaintain the same representation.\n\nOne simple answer is to use different representations for odd and\neven bits.  Instead, we optimize a logic circuit\nfor comparing two bits.  We base our design on the alternate \nrepresentation.  The implementation is shown below.  The left\nshows an implementation based on the algebra, and the right shows\na NAND/NOR implementation.  Estimating by gate count and number of\ninputs, the {two-bit} design doesn't save much over two\nsingle bit slices in terms of area.  In terms of delay, however,\nwe have only two gate delays from C_1 and C_0 to either output.\nThe longest path from the A and B inputs to the outputs is\nfive gate delays.  Thus, for an {N-bit} comparator built\nwith this design, the total delay is only N+3 gate delays.\nBut N has to be even.\n\n{file=part2/figs/comparator-two.eps,width=3.125in}\n{file=part2/figs/comparator-two-nn.eps,width=3.125in}\n\nAs you can imagine, continuing to scale up the size of our logic\nblock gives us better performance at the expense of a more complex\ndesign.  Using the alternate representation may help you to see\nhow one can generalize the approach to larger groups of bits---for\nexample, you may have noticed the two bitwise \ncomparator blocks on the left of the implementations above.\n\n\n\n\n\n \n Question: \nQ. What is the difference between the original representation and the alternate representation? \n Answer: \n\nA. The difference between the original representation and the alternate representation is that in the alternate representation, C_0=1 whenever A=B."}, {"text": "Title: Logic Operations \n Text: {Logic Operations}\n\n \n Boolean logic functions, truth tables, etc.\n \n overflow expression using logic functions (and difficulty with\n    binary overflow!)\n \n completeness\n \n implications of completeness: enable abstraction, and evaluate\n     future possible implementations of devices quickly: logically complete\n     or not?\n \n examples\n \n generalizing to operations on sets of bits\n \n\nThis set of notes briefly describes a generalization to truth tables,\nthen introduces Boolean logic operations as well as \nnotational conventions and tools that we use to express general\nfunctions on bits.  We illustrate how logic operations enable \nus to express functions such as overflow conditions concisely,\nthen show by construction that a small number of logic operations\nsuffices to describe any operation on any number of bits.  \nWe close by discussing a few implications and examples.\n\n\n\n \n Question: \nQ. What is the main idea of this text?\nQ. \nQ. The main idea of this text is to introduce Boolean logic operations, truth tables, and their implications. \n Answer: "}, {"text": "Title: Truth Tables \n Text: {Truth Tables}\n\nYou have seen the basic form of truth tables in the textbook and in class.\nOver the semester, we will introduce several\nextensions to the basic concept, mostly with the\ngoal of reducing the amount of writing necessary when using truth\ntables.  For example, the truth table to the right uses two generalizations\nto show the carry out C (also the unsigned overflow indicator) and the \nsum S produced by\nadding two {2-bit} unsigned numbers.\nFirst, rather than writing each input bit separately, we have grouped\npairs of input bits into the numbers A and B.  \nSecond, we have defined multiple \noutput columns so as to include both bits of S as well as C in \nthe same table.\nFinally, we have \ngrouped the two bits of S into one column.\n\nKeep in mind as you write truth tables that only rarely does an operation\ncorrespond to a simple and familiar process such as addition of base 2\nnumbers.  We had to choose the unsigned and 2's complement representations\ncarefully to allow ourselves to take advantage of a familiar process.\nIn general, for each line of a truth table for an operation, you may\nneed to make use of the input representation to identify the input values,\ncalculate the operation's result as a value, and then translate the value\nback into the correct bit pattern using the output representation.\nSigned magnitude addition, for example, does not always correspond to\nbase 2 addition: when the\n\n\n{cc|cc}\n{c|}& \nA& B& C& S \n00& 00& 0& 00\n00& 01& 0& 01\n00& 10& 0& 10\n00& 11& 0& 11\n01& 00& 0& 01\n01& 01& 0& 10\n01& 10& 0& 11\n01& 11& 1& 00\n10& 00& 0& 10\n10& 01& 0& 11\n10& 10& 1& 00\n10& 11& 1& 01\n11& 00& 0& 11\n11& 01& 1& 00\n11& 10& 1& 01\n11& 11& 1& 10\n\n\n\nsigns of the two input operands differ, \none should instead use base 2 subtraction.  For other operations or\nrepresentations, base 2 arithmetic may have no relevance at all.\n\n\n \n Question: \nQ. What is the purpose of a truth table? \n Answer: \n\nA. The purpose of a truth table is to show the carry out C and the sum S produced by adding two unsigned 2-bit numbers."}, {"text": "Title: Boolean Logic Operations \n Text: {Boolean Logic Operations}\n\nIn the middle of the 19^ century, George Boole introduced a \nset of logic operations that are today known as { Boolean logic}\n(also as { Boolean algebra}).  These operations today form one of\nthe lowest abstraction levels in digital systems, and an understanding\nof their meaning and use is critical to the effective development of \nboth hardware and software.\n\nYou have probably seen these functions many times already in your \neducation---perhaps first in set-theoretic form as Venn diagrams.  \nHowever, given the use of common\nEnglish words { with different meanings} to name some of the functions,\nand the sometimes confusing associations made even by engineering\neducators, we want to provide you with a concise set of\ndefinitions that generalizes correctly to more than two operands.\nYou may have learned these functions based on truth values \n(true and false), but we define them based on bits, \nwith 1 representing true and 0 representing false.\n\nTable  on the next page lists logic operations.\n\nThe first column in the table lists the name of each function.  The second\nprovides a fairly complete set of the notations that you are likely\nto encounter for each function, including both the forms used in\nengineering and those used in mathematics.  The third column defines \nthe function's\nvalue for two or more input operands (except for NOT, which operates on\na single value).  The last column shows the form generally used in\nlogic schematics/diagrams and mentions the important features used\nin distinguishing each function (in pictorial form usually called\na { gate}, in reference to common physical implementations) from the \nothers.\n\n\n{\n{|c|c|c|c|}\n{ Function}& { Notation}& { Explanation}& { Schematic} \nAND& {A AND B}{AB}{A}{A}{A}& {the ``all'' function: result is 1 iff}{{ all} input operands are equal to 1}& {flat input, round output}  \nOR& {A OR B}{A+B}{A}& {the ``any'' function: result is 1 iff}{{ any} input operand is equal to 1}& {{-6pt}}{round input, pointed output} \nNOT& {NOT A}{A'}{}{}& {logical complement/negation:}{NOT 0 is 1, and NOT 1 is 0}& {triangle and circle} \n{exclusive OR}& {A XOR B}{A}& {the ``odd'' function: result is 1 iff an { odd}}{number of input operands are equal to 1}& {{-6pt}}{OR with two lines}{on input side} \n{``or''}& A, B, or C& {the ``one of'' function: result is 1 iff exactly}{{ one of} the input operands is equal to 1}& (not used) \n\n}\n{Boolean logic operations, notation, definitions, and symbols.}{-12pt}\n\n\n\n\n\nThe first function of importance is { AND}.  Think of { AND} as the\n``all'' function: given a set of input values as operands, AND evaluates \nto 1 if and only if { all} of the input values are 1.  The first\nnotation line simply uses the name of the function.  In Boolean algebra,\nAND is typically represented as multiplication, and the middle three\nforms reflect various ways in which we write multiplication.  The last\nnotational variant is from mathematics, where the AND function is formally\ncalled { conjunction}.\n\nThe next function of importance is { OR}.  Think of { OR} as the\n``any'' function: given a set of input values as operands, OR evaluates\nto 1 if and only if { any} of the input values is 1.  The actual\nnumber of input values equal to 1 only matters in the sense of whether\nit is at least one.  The notation for OR is organized in the same way\nas for AND, with the function name at the top, the algebraic variant that\nwe will use in class---in this case addition---in the middle, and\nthe mathematics variant, in this case called { disjunction}, at the\nbottom.\n\n{ The definition of Boolean OR is not the same as our use of \nthe word ``or'' \nin English.}  For example, if you are fortunate enough to enjoy\na meal on a plane, you might be offered several choices: ``Would you like\nthe chicken, the beef, or the vegetarian lasagna today?''  Unacceptable\nanswers to this English question include: ``Yes,'' ``Chicken and lasagna,''\nand any other combination that involves more than a single choice!\n\nYou may have noticed that we might have instead mentioned that\nAND evaluates to 0 if any input value is 0, and that OR evaluates to 0\nif all input values are 0.  These relationships reflect a mathematical\nduality underlying Boolean logic that has important practical\nvalue in terms of making it easier for humans to digest complex logic\nexpressions.\nWe will talk more about duality later in the course, but you\nshould learn some of the practical\nvalue now: if you are trying to evaluate an AND function, look for an input\nwith value 0; if you are trying to evaluate an OR function, look for an \ninput with value 1.  If you find such an input, you know the function's\nvalue without calculating any other input values.\n\nWe next consider the { logical complement} function, { NOT}.  The NOT\nfunction is also called { negation}.  Unlike our\nfirst two functions, NOT accepts only a single operand, and reverses\nits value, turning 0 into 1 and 1 into 0.  The notation follows the\nsame pattern: a version using the function name at the top, followed\nby two variants used in Boolean algebra, and finally the version\nfrequently used in mathematics.  For the NOT gate, or { inverter},\nthe circle is actually the important part: the triangle by itself\nmerely copies the input.  You will see the small circle added to other\ngates on both inputs and outputs; in both cases the circle implies a NOT\nfunction.\n\n\n\nLast among the Boolean logic functions, we have the\n{ XOR}, or { exclusive OR} function.\nThink of XOR as the ``odd'' function: given a set of input values as\noperands, XOR evaluates to 1 if and only if { an odd number} of\nthe input values are 1.  Only two variants of XOR notation are given:\nthe first using the function name, and the second used with Boolean\nalgebra.  Mathematics rarely uses this function.\n\nFinally, we have included the meaning of the word ``or'' in English as\na separate function entry to enable you to compare that meaning\nwith the Boolean logic functions easily.  Note that many people refer\nto English'\n\n\nuse of the word ``or'' as ``exclusive'' because one\ntrue value excludes all others from being true.  Do not let this \nhuman language ambiguity confuse you about XOR!  For all logic design\npurposes, { XOR is the odd function}.\n\nThe truth table to the right provides values illustrating these functions operating\non three inputs.  The AND, OR, and XOR functions are all \nassociative---(A  B)  C = A  (B  C)---and\ncommutative---A  B = B  A,\nas you may have already realized from their definitions.\n\n\n{ccc|cccc}\n{c|}& \nA& B& C& ABC& A+B+C& & A \n0& 0& 0& 0& 0& 1& 0\n0& 0& 1& 0& 1& 1& 1\n0& 1& 0& 0& 1& 1& 1\n0& 1& 1& 0& 1& 1& 0\n1& 0& 0& 0& 1& 0& 1\n1& 0& 1& 0& 1& 0& 0\n1& 1& 0& 0& 1& 0& 0\n1& 1& 1& 1& 1& 0& 1\n\n\n\n\n\n \n Question: \nQ. What is the definition of the Boolean logic function OR? \n Answer: \n\nThe OR function evaluates to 1 if any of the input values is 1."}, {"text": "Title: Overflow as Logic Expressions \n Text: {Overflow as Logic Expressions}\n\nIn the last set of notes, we discussed overflow conditions for unsigned\nand 2's complement representations.  Let's use Boolean logic to express\nthese conditions.  \n\nWe begin with addition of two {1-bit} unsigned numbers.  \nCall the two input bits A_0 and B_0.  If you write a truth table for\nthis operation, you'll notice that overflow occurs only when all (two)\nbits are 1.  If either bit is 0, the sum can't exceed 1, so overflow\ncannot occur.  In other words, overflow in this case can be written using\nan AND operation: \n\n{eqnarray*}\nA_0B_0\n{eqnarray*}\n\nThe truth table for adding two {2-bit} unsigned numbers is four\ntimes as large, and seeing the structure may be difficult.  One way of\nwriting the expression for overflow of {2-bit} unsigned addition is\nas follows:\n\n{eqnarray*}\nA_1B_1 + (A_1+B_1)A_0B_0\n{eqnarray*}\n\nThis expression is slightly trickier to understand.  Think about the\nplace value of the bits.  If both of the most significant bits---those\nwith place value 2---are 1,\nwe have an overflow, just as in the case of {1-bit} addition.\nThe A_1B_1 term represents this case.  We also have an overflow if\none or both (the OR) of the most significant bits are 1 and the\nsum of the two next significant bits---in this case those with place \nvalue 1---generates a carry.\n\nThe truth table for adding two {3-bit} unsigned numbers is \nprobably not something that you want to write out.  Fortunately,\na pattern should start to become clear with the following expression:\n\n{eqnarray*}\nA_2B_2 + (A_2+B_2)A_1B_1 + (A_2+B_2)(A_1+B_1)A_0B_0\n{eqnarray*}\n\nIn the {2-bit} case, we mentioned the ``most significant bit''\nand the ``next most significant bit'' to help you see the pattern.\nThe same reasoning describes the first two product terms in our\noverflow expression for {3-bit} unsigned addition (but the place\nvalues are 4 for the most significant bit and 2 for the next most\nsignificant bit).  The last term represents the overflow case in which\nthe two least significant bits generate a carry which then propagates\nup through all of the other bits because at least one of the two bits\nin every position is a 1.\n\n\n\n\nThe overflow condition for addition of two {N-bit} 2's complement\nnumbers can be written fairly concisely in terms of the first bits\nof the two numbers and the first bit of the sum.  Recall that overflow\nin this case depends only on whether the three numbers are negative\nor non-negative, which is given by\n\n\n{ 0pt\n 0pt\n\n&A_{N-1}&A_{N-2}&&A_2&A_1&A_0\n+   &B_{N-1}&B_{N-2}&&B_2&B_1&B_0 \n&S_{N-1}&S_{N-2}&&S_2&S_1&S_0\n\n\n}\n\n\nthe most significant bit.  Given\nthe bit names as shown to the right, we can write the overflow condition\nas follows:\n\n{eqnarray*}\nA_{N-1} B_{N-1} {S_{N-1}}+\n{A_{N-1}} {B_{N-1}} S_{N-1}\n{eqnarray*}\n\nThe overflow condition does of course depend on all of the bits in\nthe two numbers being added.  In the expression above, we have simplified\nthe form by using S_{N-1}.  But S_{N-1} depends on the bits A_{N-1}\nand B_{N-1} as well as the carry out of bit (N-2).  \n\nLater in this set of notes, we present a technique with which you can derive\nan expression for an arbitrary Boolean logic function.  As an exercise,\nafter you have finished reading these notes, try using that technique to\nderive an overflow expression for addition of \ntwo {N-bit} 2's complement numbers based on \nA_{N-1}, B_{N-1}, and the carry out of bit (N-2) (and into \nbit (N-1)), which we might\ncall C_{N-1}.  You might then calculate C_{N-1} in terms of the rest\nof the bits of A and B\nusing the expressions for\nunsigned overflow just discussed.\n\nIn the next month or so, you will learn how to derive\nmore compact expressions yourself from truth tables or other representations\nof Boolean logic functions.\n\n\n \n Question: \nQ. Boolean logic\nQ. Arithmetic\nQ. Data representation\nQ. \nQ. What is the overflow condition for {3-bit} unsigned addition? \n Answer: \n\nA. A_2B_2 + (A_2+B_2)A_1B_1 + (A_2+B_2)(A_1+B_1)A_0B_0"}, {"text": "Title: Logical Completeness \n Text: {Logical Completeness}\n\nWhy do we feel that such a short list of functions is enough?  If you \nthink about the\nnumber of possible functions on N bits, you might think that we need\nmany more functions to be able to manipulate bits.  With 10 bits, for\nexample, there are 2^ such functions.  Obviously, some of them\nhave never been used in any computer system, but maybe we should define\nat least a few more logic operations?\nIn fact, we do not\neven need XOR.  The functions AND, OR, and NOT are sufficient, even if\nwe only allow two input operands for AND and OR!\n\nThe theorem below captures this idea, called { logical completeness}.\nIn this case, we claim that the set of functions {AND, OR, NOT} is\nsufficient to express any operation on any finite number of variables, \nwhere each variable is a bit.\n\n{ Theorem:} \n\nGiven enough {2-input} AND, {2-input} OR, and {1-input}\nNOT functions, one can express any Boolean logic function on any finite \nnumber of variables.\n\n { Proof:} \n\nThe proof of our theorem is { by construction}.  In other words, \nwe show a systematic\napproach for transforming an arbitrary Boolean logic function on an\narbitrary number of variables into a form that uses only AND, OR, and\nNOT functions on one or two operands.\n\nAs a first step, we remove the restriction on the number of inputs for\nthe AND and OR functions.  For this purpose, we state and prove two\n{ lemmas}, which are simpler theorems used to support the proof of\na main theorem.\n\n{ Lemma 1:}\n\nGiven enough {2-input} AND functions, one can express an AND function\non any finite number of variables.\n\n{ Proof:}\n\nWe prove the Lemma { by induction}.{We assume that you have\nseen proof by induction previously.}  Denote the number of inputs to a\nparticular AND function by N.\n\n\nThe base case is N=2.  Such an AND function is given.\n\nTo complete the proof, we need only show that, given \nany number of AND functions with up to N inputs, we\ncan express an AND function with N+1 inputs.  To do so, we need\nmerely use one {2-input} AND function to join together\nthe result of an {N-input} AND function with an additional\ninput, as illustrated to the right.\n\n\n\n\n\n{ Lemma 2:}\n\nGiven enough {2-input} OR functions, one can express an OR function\non any finite number of variables.\n\n{ Proof:}\n\nThe proof of Lemma 2 is identical in structure to that of Lemma 1, but\nuses OR functions instead of AND functions.\n\n\nLet's now consider a small subset of functions on N variables.  \nFor any such function, you can write out the truth table for the\nfunction.  The output of a logic function is just a bit, \neither a 0 or a 1.  Let's consider the set of functions on N\nvariables that produce a 1 for exactly one combination of the N\nvariables.  In other words, if you were to write out the truth\ntable for such a function, exactly one row in the truth table would\nhave output value 1, while all other rows had output value 0.\n\n{ Lemma 3:}\n\nGiven enough AND functions and {1-input}\nNOT functions, one can express any Boolean logic function \nthat produces a 1 for exactly one combination of\nany finite number of variables.\n\n{ Proof:}\n\nThe proof of Lemma 3 is by construction.\nLet N be the number of variables on which the function operates.\nWe construct a { minterm} on these N variables,\nwhich is an AND operation on each variable or its complement.\nThe minterm is specified by looking at the unique combination of \nvariable values that produces a 1 result for the function.\nEach variable that must be a 1 is included as itself, while\neach variable that must be a 0 is included as the variable's complement\n(using a NOT function).  The resulting minterm produces the\ndesired function exactly.  When the variables all match\nthe values for which the function should produce 1, the\ninputs to the AND function are all 1, and the function produces 1.\nWhen any variable does not match the value for which the function\nshould produce 1, that variable (or its complement) acts as a 0\ninput to the AND function, and the function produces a 0, as desired.\n\nThe table below shows all eight minterms for three variables.\n\n{\n{ccc|cccccccc}\n{c|}& \nA& B& C& \n  &\n  C&\n B &\n B C&\nA  &\nA  C&\nA B &\nA B C\n \n0& 0& 0& 1& 0& 0& 0& 0& 0& 0& 0\n0& 0& 1& 0& 1& 0& 0& 0& 0& 0& 0\n0& 1& 0& 0& 0& 1& 0& 0& 0& 0& 0\n0& 1& 1& 0& 0& 0& 1& 0& 0& 0& 0\n1& 0& 0& 0& 0& 0& 0& 1& 0& 0& 0\n1& 0& 1& 0& 0& 0& 0& 0& 1& 0& 0\n1& 1& 0& 0& 0& 0& 0& 0& 0& 1& 0\n1& 1& 1& 0& 0& 0& 0& 0& 0& 0& 1\n\n}\n\nWe are now ready to prove our theorem.\n\n{ Proof (of Theorem):}\n\nAny given function on N variables\nproduces the value 1 for some set of combinations\nof inputs.  Let's say that M such combinations produce 1.  \nNote that M{2^N}.\nFor each\ncombination that produces 1, we can use\nLemma 1 to construct an {N-input} AND function.\nThen, using Lemma 3, we can use as many as M NOT functions\nand the {N-input} AND function to construct a minterm\nfor that input combination.\nFinally, using Lemma 2, we can construct an {M-input}\nOR function and OR together all of the minterms.\nThe result of the OR is the desired function. If the function\nshould produce a 1 for some combination of inputs, that combination's \nminterm provides a 1 input to the OR, which in turn produces a 1.\nIf a combination should produce a 0, its minterm does not appear in\nthe OR; all other minterms produce 0 for that combination, and thus\nall inputs to the OR are 0 in such cases, and the OR produces 0,\nas desired.\n\nThe construction that we used to prove logical completeness does\nnot necessarily help with efficient design of logic functions.  Think\nabout some of the expressions that we discussed earlier in these\nnotes for overflow conditions.  How many minterms do you need \nfor {N-bit} unsigned overflow?  A single Boolean logic function\ncan be expressed in many different ways, and learning how to develop\nan efficient implementation of a function as well as how to determine\nwhether two logic expressions are identical without actually writing out\ntruth tables are important engineering skills that you will start to \nlearn in the coming months.\n\n\n\n \n Question: \nQ. What is logical completeness? \n Answer: \n\nLogical completeness is the ability to express any Boolean logic function on any finite number of variables using only AND, OR, and NOT functions on one or two operands."}, {"text": "Title: Implications of Logical Completeness \n Text: {Implications of Logical Completeness}\n\nIf logical completeness doesn't really help us to engineer logic functions,\nwhy is the idea important?  Think back to the layers of abstraction\nand the implementation of bits from the first couple of lectures.  \nVoltages are real numbers, not bits.  { The device layer implementations\nof Boolean logic functions must abstract away the analog properties\nof the physical system.}  Without such abstraction, we must think\ncarefully about analog issues such as noise every time we make use\nof a bit!\n\nLogical completeness assures us that no matter what we want to do\nwith bits, implementating a handful of operations correctly is\nenough to guarantee that we never have to worry.\n\nA second important value of logical completeness is as a tool in\nscreening potential new technologies for computers.  If a new technology\ndoes not allow implementation of a logically complete set of functions,\nthe new technology is extremely unlikely\nto be successful in replacing the current one.\n\nThat said, {AND, OR, and NOT} is not the only logically complete set of\nfunctions.  In fact, our current complementary metal-oxide semiconductor\n(CMOS) technology, on which most of the computer industry is now built,\ndoes not directly implement these functions, as you will see later in\nour class.\n\n\n\nThe functions that are implemented directly in CMOS are NAND and NOR, which\nare abbreviations for AND followed by NOT and OR followed by NOT, \nrespectively.  Truth tables for the two are shown to the right.\n\nEither of these functions by itself forms a logically complete set.\nThat is, both the set  and the set  are logically\ncomplete.  For now, we leave the proof of this claim to you.  Re-\n\n\n{cc|cc}\n{c|}& \n& & & {A+B}\nA& B& A NAND B& A NOR B \n0& 0& 1& 1\n0& 1& 1& 0\n1& 0& 1& 0\n1& 1& 0& 0\n\n\n\nmember that all\nyou need to show is that you can implement any set known to be\nlogically complete, so in order to prove that  is \nlogically complete (for example),\nyou need only show that you can implement AND, OR, and NOT using only\nNAND.\n\n \n Question: \nQ. Why is the idea of logical completeness important? \n Answer: \n\nA. The idea of logical completeness is important because it assures us that no matter what we want to do with bits, implementing a handful of operations correctly is enough to guarantee that we never have to worry."}, {"text": "Title: Examples and a Generalization \n Text: {Examples and a Generalization}\n\nLet's use our construction to solve a few examples.  We begin with\nthe functions that we illustrated with the first truth table from this\nset of notes,\nthe carry out C and sum S of two {2-bit}\nunsigned numbers.  Since each output bit requires a separate expression,\nwe now write S_1S_0 for the two bits of the sum.  We also need to\nbe able to make use of the individual bits of the input values, so we \nwrite these as A_1A_0 and B_1B_0, as shown on the left below.\nUsing our \nconstruction from the logical completeness theorem, we obtain the\nequations on the right.\nYou should verify these expressions yourself.\n\n{\n\n{cccc|ccc}\n{c|}& \nA_1& A_0& B_1& B_0& C& S_1& S_0 \n0&0& 0&0& 0& 0&0\n0&0& 0&1& 0& 0&1\n0&0& 1&0& 0& 1&0\n0&0& 1&1& 0& 1&1\n0&1& 0&0& 0& 0&1\n0&1& 0&1& 0& 1&0\n0&1& 1&0& 0& 1&1\n0&1& 1&1& 1& 0&0\n1&0& 0&0& 0& 1&0\n1&0& 0&1& 0& 1&1\n1&0& 1&0& 1& 0&0\n1&0& 1&1& 1& 0&1\n1&1& 0&0& 0& 1&1\n1&1& 0&1& 1& 0&0\n1&1& 1&0& 1& 0&1\n1&1& 1&1& 1& 1&0\n\n\n\n{eqnarray*}\n\nC &=& \n{A_1} A_0 B_1 B_0 +\nA_1 {A_0} B_1 {B_0} +\nA_1 {A_0} B_1 B_0 +\n&& A_1 A_0 {B_1} B_0 +\nA_1 A_0 B_1 {B_0} +\nA_1 A_0 B_1 B_0\n\n\nS_1 &=& \n{A_1} {A_0} B_1 {B_0} +\n{A_1} {A_0} B_1 B_0 +\n{A_1} A_0 {B_1} B_0 +\n&& {A_1} A_0 B_1 {B_0} +\nA_1 {A_0} {B_1} {B_0} +\nA_1 {A_0} {B_1} B_0 +\n&& A_1 A_0 {B_1} {B_0} +\nA_1 A_0 B_1 B_0\n\n\nS_0 &=& \n{A_1} {A_0} {B_1} B_0 +\n{A_1} {A_0} B_1 B_0 +\n{A_1} A_0 {B_1} {B_0} +\n&& {A_1} A_0 B_1 {B_0} +\nA_1 {A_0} {B_1} B_0 +\nA_1 {A_0} B_1 B_0 +\n&& A_1 A_0 {B_1} {B_0} +\nA_1 A_0 B_1 {B_0}\n{eqnarray*}\n\n}\n\n\nNow let's consider a new function.  Given\nan {8-bit} 2's complement number, A=A_7A_6A_5A_4A_3A_2A_1A_0,\nwe want to compare it with the value -1.  We know that\nwe can construct this function using AND, OR, and NOT, but how?\nWe start by writing the representation for -1, which is 11111111.\nIf the number A matches that representation, we want to produce a 1.\nIf the number A differs in any bit, we want to produce a 0.\nThe desired function has exactly one combination of inputs that\nproduces a 1, so in fact we need only one minterm!  In this case, we\ncan compare with -1 by calculating the expression:\n\n{eqnarray*}\nA_7  A_6  A_5  A_4  A_3  A_2  A_1  A_0\n{eqnarray*}\n\nHere we have explicitly included multiplication symbols to avoid\nconfusion with our notation for groups of bits, as we used when \nnaming the individual bits of A.\n\n\nIn closing, we briefly introduce a generalization of logic\noperations to groups of bits.  Our representations for integers,\nreal numbers, and characters from human languages all use more than\none bit to represent a given value.  When we use computers, we often\nmake use of multiple bits in groups in this way.  A { byte}, for example,\ntoday means an ordered group of eight bits.\n\nWe can extend our logic functions to operate on such groups by pairing\nbits from each of two groups and performing the logic operation on each\npair.  For example, given\nA=A_7A_6A_5A_4A_3A_2A_1A_0=01010101\nand\nB=B_7B_6B_5B_4B_3B_2B_1B_0=11110000, we calculate\nA AND B by computing the AND of each pair of bits, \nA_7 AND B_7,\nA_6 AND B_6,\nand so forth, to\nproduce the result 01010000, as shown to the right.\nIn the same way, we can extend other logic\noperations, such as OR, NOT, and XOR, to operate on bits of groups.\n\n\n{ 0pt\n 0pt\n\n&A  & 0&1& 0&1& 0&1& 0&1\n  AND   &B& 1&1&1&1&0&0&0&0 \n&& 0& 1&0&1&0&0&0&0\n\n}\n\n\n\n\n\n\n \n Question: \nQ. What is an example of a function that can be constructed using AND, OR, and NOT? \n Answer: \n\nA. The function for adding two 2-bit unsigned numbers."}, {"text": "Title: The 2's Complement Representation \n Text: {The 2's Complement Representation}\n\nThis set of notes explains the rationale for using the 2's complement\nrepresentation for signed integers and derives the representation \nbased on equivalence of the addition function to that of addition\nusing the unsigned representation with the same number of bits.\n\n \n Question: \nQ. What is the rationale for using the 2's complement representation for signed integers? \n Answer: \n\nA. The rationale for using the 2's complement representation is that it allows for the addition function to be equivalent to addition using the unsigned representation."}, {"text": "Title: Review of Bits and the Unsigned Representation \n Text: {Review of Bits and the Unsigned Representation}\n\nIn modern digital systems, we represent all types of information\nusing binary digits, or { bits}.  Logically, a bit is either 0 or 1.\nPhysically, a bit may be a voltage, a magnetic field, or even the\nelectrical resistance of a tiny sliver of glass.\n\nAny type of information can be represented with an ordered set of\nbits, provided that \n{ any given pattern of bits corresponds to only\none value} and that { we agree in advance on which pattern of bits\nrepresents which value}.\n\nFor unsigned integers---that is, whole numbers greater or equal to\nzero---we chose to use the base 2 representation already familiar to\nus from mathematics.  We call this representation the { unsigned\nrepresentation}.  For example, in a {4-bit} unsigned representation,\nwe write the number 0 as 0000, the number 5 as 0101, and the number 12\nas 1100.  Note that we always write the same number of bits for any\npattern in the representation: { in a digital system, there is no \n``blank'' bit value}.\n\n \n Question: \nQ. Why is it important that any given pattern of bits corresponds to only one value? \n Answer: \n\nA. It is important that any given pattern of bits corresponds to only one value because it allows us to unambiguously represent any value using a finite number of bits."}, {"text": "Title: Picking a Good Representation \n Text: {Picking a Good Representation}\n\nIn class, we discussed the question of what makes one representation\nbetter than another.  The value of the unsigned representation, for\nexample, is in part our existing familiarity with the base 2 analogues\nof arithmetic.  For base 2 arithmetic, we can use nearly identical\ntechniques to those that we learned in elementary school for adding, \nsubtracting, multiplying, and dividing base 10 numbers.\n\nReasoning about the relative merits of representations from a\npractical engineering perspective is (probably) currently beyond \nyour ability.\n\nSaving energy, making the implementation simple, and allowing the\nimplementation to execute quickly probably all sound attractive, but \na quantitative comparison between two representations on any of these\nbases requires knowledge that you will acquire in the\nnext few years.\n\nWe can sidestep such questions, however, by realizing that if a\ndigital system has hardware to perform operations such as addition\non unsigned values, using the same piece of hardware to operate\non other representations incurs little or no additional cost.\nIn this set of notes, we discuss the 2's complement representation,\nwhich allows reuse of the unsigned add unit (as well as a basis for\nperforming subtraction of either representation using an add unit!).\nIn discussion section and in your homework, you will \nuse the same idea to perform operations on other representations, such as\nchanging an upper case letter in ASCII to a lower case one, or converting\nfrom an ASCII digit to an unsigned representation of the same number.  \n\n \n Question: \nQ. Why is it important to be able to reasoning about the relative merits of representations from a practical engineering perspective? \n Answer: \n\nA. Reasoning about the relative merits of representations from a practical engineering perspective is important because it allows engineers to optimize digital systems for energy efficiency, simplicity, and speed."}, {"text": "Title: The Unsigned Add Unit \n Text: {The Unsigned Add Unit}\n\nIn order to define a representation for signed integers that allows\nus to reuse a piece of hardware designed for unsigned integers, we\nmust first understand what such a piece of hardware actually does (we\ndo not need to know how it works yet---we'll explore that question \nlater in our class).\n\nThe unsigned representation using {N} bits is not closed\nunder addition.  In other words, for any value of N, we can easily\nfind two {N-bit} unsigned numbers that, when added together,\ncannot be represented as an {N-bit} unsigned number.  With N=4, \nfor example, we can add 12 (1100) and 6 (0110) to obtain 18.\nSince 18 is outside of the range [0,2^4-1] representable using\nthe {4-bit} unsigned representation, our representation breaks\nif we try to represent the sum using this representation.  We call\nthis failure an { overflow} condition: the representation cannot\nrepresent the result of the operation, in this case addition.\n\n\nUsing more bits to represent the answer is not an attractive solution, \nsince we might then want to use more bits for the inputs, which in turn\nrequires more bits for the outputs, and so on.  We cannot build \nsomething supporting an infinite number of bits.  Instead, we \nchoose a value for N and build an add unit that adds two {N-bit}\nnumbers and produces an {N-bit} sum (and some overflow \nindicators, which we discuss in the next set of notes).  The diagram\nto the right shows how we might draw such a device, with two {N-bit}\nnumbers entering at from the top, and the {N-bit} sum coming out\nfrom the bottom.\n\n\n\n\n\n\nThe function used for {N-bit} unsigned addition is addition \nmodulo 2^N.  In a practical sense, you can think of this function\nas simply keeping the last N bits of the answer; other bits \nare simply discarded.  In the example to the right,\nwe add 12 and 6 to obtain 18, but then discard the extra bit on the\nleft, so the add unit produces 2 (an overflow).\n\n\n\n\n\n\n{ Modular arithmetic} defines a way of performing arithmetic for\na finite number of possible values, usually integers.  \nAs a concrete example, let's use modulo 16, which corresponds to\nthe addition unit for our {4-bit} examples.\n\nStarting with the full range of integers, we break the number\nline into contiguous groups of 16 integers, as shown to the right.\n\n\n\n\n\nThe numbers 0 to 15 form one group.  The numbers -16 to -1 form a\nsecond group, and the numbers from 16 to 31 form a third group. \nAn infinite number of groups are defined in this manner.\n\nWe then define 16 { equivalence classes} consisting of the first numbers\nfrom all groups, the second numbers from all groups, and so forth.\nFor example, the numbers , -32, -16, 0, 16, 32,  form\none such equivalence class.\n\nMathematically, we say that two numbers A and B are equivalent modulo 16,\nwhich we write as\n\n{eqnarray*}\n(A &=& B)  16, {or sometimes as}\nA && B {(mod 16)}\n{eqnarray*}\n\nif and only if A=B+16k for some integer k.\n\nEquivalence as defined by a particular modulus\ndistributes over addition and multiplication.  If, for example,\nwe want to find the equivalence class for (A + B)  16,\nwe can find the equivalence classes for A (call it C) and B \n(call it D) and then calculate the equivalence class \nof (C + D)  16.\nAs a concrete example of distribution over multiplication, \ngiven (A = 1,083,102,112  7,323,127)  10,\nfind A.\n\nFor this problem, we note that the first number is equivalent \nto 2  10, while the second number is equivalent \nto 7  10.  We then write (A = 2  7)  10,\nand, since 2  7 = 14, we have (A = 4)  10.\n\n\n \n Question: \nQ. What is the value of A in the equation A = 2 mod 7?\nQ. \nQ. 4 \n Answer: "}, {"text": "Title: Deriving 2's Complement \n Text: {Deriving 2's Complement}\n\n\nGiven these equivalence classes, we might instead choose to draw a circle\nto identify the equivalence classes and to associate each class with one\nof the sixteen possible {4-bit} patterns, as shown to the right.\nUsing this circle representation, we can add by counting clockwise around\nthe circle, and we can subtract by counting in a counterclockwise direction\naround the circle.  With an unsigned representation, we choose to use the\ngroup from [0,15] (the middle group in the diagram markings to the right)\nas the number represented by each of the patterns.  Overflow occurs\nwith unsigned addition (or subtraction) because we can only choose one\nvalue for each binary pattern.\n\n\n\n\n\nIn fact, we can choose any single value for each pattern to create a \nrepresentation, and our add unit will always produce results that\nare correct modulo 16.  Look back at our overflow example, where\nwe added 12 and 6 to obtain 2, and notice that (2=18)  16.\nNormally, only a contiguous sequence of integers makes a useful\nrepresentation, but we do not have to restrict ourselves to \nnon-negative numbers.\n\nThe 2's complement representation can then be defined by choosing a \nset of integers balanced around zero from the groups.  In the circle \ndiagram, for example, we might choose to represent numbers\nin the range [-7,7] when using 4 bits.  What about the last pattern, 1000?\nWe could choose to represent either -8 or 8.  The number of arithmetic\noperations that overflow is the same with both choices (the choices\nare symmetric around 0, as are the combinations of input operands that \noverflow), so we gain nothing in that sense from either choice.\nIf we choose to represent -8, however, notice that all patterns starting\nwith a 1 bit then represent negative numbers.  No such simple check\narises with the opposite choice, and thus an {N-bit} 2's complement \nrepresentation is defined to represent the range [-2^{N-1},2^{N-1}-1],\nwith patterns chosen as shown in the circle.\n\n \n Question: \nQ. What is the effect of choosing to represent -8 instead of 8 in a 4-bit 2's complement system? \n Answer: \n\nA. The effect of choosing to represent -8 instead of 8 in a 4-bit 2's complement system is that all patterns starting with a 1 bit then represent negative numbers."}, {"text": "Title: An Algebraic Approach \n Text: {An Algebraic Approach}\n\nSome people prefer an algebraic approach to understanding the\ndefinition of 2's complement, so we present such an approach next.\nLet's start by writing f(A,B) for the result of our add unit:\n\n{eqnarray*}\nf(A,B) = (A + B)  2^N\n{eqnarray*}\n\nWe assume that we want to represent a set of integers balanced around 0\nusing our signed representation, and that we will use the same binary\npatterns as we do with an unsigned representation to represent\nnon-negative numbers.  Thus, with an {N-bit} representation,\nthe patterns in the range [0,2^{N-1}-1] are the same as those\nused with an unsigned representation.  In this case, we are left with\nall patterns beginning with a 1 bit.\n\nThe question then is this: given an integer k, 2^{N-1}>k>0, for which we \nwant to find a pattern to represent -k, and any integer m\nthat we might want to add to -k, \ncan we find another integer p>0\nsuch that \n\n\n(-k + m = p + m)  2^N   ?\n\n\nIf we can, we can use p's representation to represent -k and our\nunsigned addition unit f(A,B) will work correctly.\n\nTo find the value p, start by subtracting m from both sides of\nEquation () to obtain:\n\n\n(-k = p)  2^N\n\n\nNote that (2^N=0)  2^N, and add this equation to \nEquation () to obtain\n\n{eqnarray*}\n(2^N-k = p)  2^N\n{eqnarray*}\n\nLet p=2^N-k.  \n\nFor example, if N=4, k=3 gives p=16-3=13, which is the pattern 1101.\nWith N=4 and k=5, we obtain p=16-5=11, which is the pattern 1011.\nIn general, since\n2^{N-1}>k>0, \nwe have 2^{N-1}<p<2^N.  But these patterns are all unused---they all\nstart with a 1 bit!---so the patterns that we have defined for negative\nnumbers are disjoint from those that we used for positive numbers, and\nthe meaning of each pattern is unambiguous.\n\nThe algebraic definition of bit patterns for negative numbers\nalso matches our circle diagram from the last\nsection exactly, of course.\n\n\n\n \n Question: \nQ. Assuming that the bit patterns for negative numbers are disjoint from those that we used for positive numbers, what is the meaning of each pattern? \n Answer: \n\nA. The meaning of each pattern is unambiguous."}, {"text": "Title: Negating 2's Complement Numbers \n Text: {Negating 2's Complement Numbers}\n\nThe algebraic approach makes understanding negation of an integer\nrepresented using 2's complement fairly straightforward, and gives \nus an easy procedure for doing so.\nRecall that given an integer k in an {N-bit} 2's complement\nrepresentation, the {N-bit} pattern for -k is given by 2^N-k \n(also true for k=0 if we keep only the low N bits of the result).  \nBut 2^N=(2^N-1)+1.  Note that 2^N-1 is the pattern of\nall 1 bits.  Subtracting any value k from this value is equivalent\nto simply flipping the bits, changing 0s to 1s and 1s to 0s.\n(This operation is called a { 1's complement}, by the way.)\nWe then add 1 to the result to find the pattern for -k.\n\nNegation can overflow, of course.  Try finding the negative pattern for -8 \nin {4-bit} 2's complement.\n\nFinally, be aware that people often overload the term 2's complement\nand use it to refer to the operation of negation in a 2's complement\nrepresentation.  In our class, we try avoid this confusion: 2's complement\nis a representation for signed integers, and negation is an operation\nthat one can apply to a signed integer (whether the representation used\nfor the integer is 2's complement or some other representation for signed\nintegers).\n\n\n\n\n\n \n Question: \nQ. Why is negation of integers in a 2's complement representation straightforward with the algebraic approach? \n Answer: \n\nA. The algebraic approach makes understanding negation of an integer in a 2's complement representation straightforward because it is simply a matter of subtracting the value from 2^N-1 and then adding 1."}, {"text": "Title: The Halting Problem \n Text: {The Halting Problem}\n\nFor some of the topics in this course, we plan to cover the material\nmore deeply than does the textbook.  We will provide notes in this\nformat to supplement the textbook for this purpose.\n\nIn order to make these notes more useful as a reference, definitions are\nhighlighted with boldface, and italicization emphasizes pitfalls or other\nimportant points.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}\n\nThese notes are broken up into four parts, corresponding to the three\nmidterm exams and the final exam.  Each part is covered by\none examination in our class.  { The last section of each of the four\nparts gives you\na summary of material that you are expected to know for the corresponding\nexam.}  Feel\nfree to read it in advance.\n\nAs discussed in the textbook and in class, a { universal\ncomputational device} (or { computing machine}) is a device \nthat is capable of computing the\nsolution to any problem that can be computed, provided that the device\nis given enough storage and time for the computation to finish.  \n\nOne might ask whether we can describe problems that we cannot answer (other \nthan philosophical ones, such as the meaning of life).\n\nThe answer is yes: there are problems that are provably { undecidable},\nfor which no amount of computation can solve the problem in general.\nThis set of notes describes the first problem known to be\nundecidable, the { halting problem}.  For our class, you need only recognize\nthe name and realize that one can, in fact, give examples of problems\nthat cannot be solved by computation.  In the future, you should be able\nto recognize this type of problem so as to avoid spending your time\ntrying to solve it.\n\n \n Question: \nQ. What is the first problem known to be undecidable? \n Answer: \n\nA. The first problem known to be undecidable is the halting problem."}, {"text": "Title: Universal Computing Machines* \n Text: {Universal Computing Machines*}\n\nThe things that we call computers today, whether we are talking about\na programmable microcontroller in a microwave oven or the Blue Waters\nsupercomputer sitting on the south end of our campus (the United\nStates' main resource to support computational science research), are\nall equivalent in the sense of what problems they can solve.  \n\nThese machines do, of course, have access to different amounts of memory, \nand compute at different speeds.  \n\nThe idea that a single model of computation could be described and proven\nto be equivalent to all other models came out of a 1936 paper by Alan Turing, \nand today we generally refer to these devices as { Turing machines}.\n\nAll computers mentioned earlier, as well as all computers with which you \nare familiar in your daily life, are provably equivalent to Turing machines.\n\nTuring also conjectured that his definition of computable was\nidentical to the ``natural'' definition (today, this claim is known\nas the { Church-Turing conjecture}).  In other words, a problem that\ncannot be solved by a Turing machine cannot be solved in any\nsystematic manner, with any machine, or by any person.  This conjecture\nremains unproven!  However, neither has anyone been able to disprove\nthe conjecture, and it is widely believed to be true.  Disproving the\nconjecture requires that one demonstrate a systematic technique (or a\nmachine) capable of solving a problem that cannot be solved by a\nTuring machine.  No one has been able to do so to date.\n\n\n \n Question: \nQ. What is the significance of the Church-Turing conjecture? \n Answer: \n\nA. The significance of the Church-Turing conjecture is that it is a statement that has yet to be proven or disproven. If it were to be proven, it would mean that there is a systematic technique or machine that can solve a problem that cannot be solved by a Turing machine. As of now, no one has been able to do so."}, {"text": "Title: The Halting Problem* \n Text: {The Halting Problem*}\n\nYou might reasonably ask whether any problems can be shown to\nbe incomputable.  More common terms for such problems---those known\nto be insolvable by any computer---are { intractable} or \nundecidable.\n\nIn the same 1936 paper in which he introduced the universal computing\nmachine, Alan Turing also provided an answer to this question\nby introducing (and proving) that there are in fact problems that cannot be\ncomputed by a universal computing machine.\nThe problem that\nhe proved undecidable, using proof techniques almost identical to those\ndeveloped for similar problems in the 1880s, is now known as { the\nhalting problem}.\n\n\n\nThe halting problem is easy to state and easy to prove undecidable.\nThe problem is this: given a Turing machine and an input to the Turing\nmachine, does the Turing machine finish computing in a finite number\nof steps (a finite amount of time)?  In order to solve the problem, an\nanswer, either yes or no, must be given in a finite amount of time\nregardless of the machine or input in question.  Clearly some machines\nnever finish.  For example, we can write a Turing machine that counts\nupwards starting from one.\n\nYou may find the proof structure for undecidability of the halting problem\neasier to understand if\nyou first think about a related problem with which you may\nalready be familiar, the Liar's paradox\n(which is at least 2,300 years old).  In its stengthened form, it is\nthe following sentence: ``This sentence is not true.''\n\n\nTo see that no Turing machine can solve the halting problem, we begin\nby assuming that such a machine exists, and then show that its\nexistence is self-contradictory.  We call the machine the ``Halting\nMachine,'' or HM for short.  HM is a machine that operates on \nanother\n\n\n\n\n\nTuring machine and its inputs to produce a yes or no answer in finite time:\neither the machine in question finishes in finite time (HM returns\n``yes''), or it does not (HM returns ``no'').  The figure illustrates\nHM's operation.\n\n\nFrom HM, we construct a second machine that we call the HM Inverter,\nor HMI.  This machine inverts the sense of the answer given by HM.  In\nparticular, the inputs are fed directly into a copy of HM, and if HM\nanswers ``yes,'' HMI enters an infinite loop.  If HM answers ``no,'' HMI\nhalts.  A diagram appears to the right.\n\nThe inconsistency can now be seen by asking HM whether HMI halts when\ngiven itself as an input (repeatedly), as\n\n\n\n\n\nshown below.  Two\ncopies of HM are thus\nbeing asked the same question.  One copy is the rightmost in the figure below\nand the second is embedded in the HMI machine that we are using as the\ninput to the rightmost HM.  As the two copies of HM operate on the same input\n(HMI operating on HMI), they should return the same answer: a Turing\nmachine either halts on an input, or it does not; they are\ndeterministic.\n\n\n\nLet's assume that the rightmost HM tells us that HMI operating on itself halts.\nThen the copy of HM in HMI (when HMI executes on itself, with itself\nas an input) must also say ``yes.''  But this answer implies that HMI\ndoesn't halt (see the figure above), so the answer should have been\nno!\n\nAlternatively, we can assume that the rightmost HM says that HMI operating on itself\ndoes not halt.  Again, the copy of HM in HMI must give the same\nanswer.  But in this case HMI halts, again contradicting our\nassumption.\n\nSince neither answer is consistent, no consistent answer can be given,\nand the original assumption that HM exists is incorrect.  Thus, no\nTuring machine can solve the halting problem.\n\n\n\n\n\n \n Question: \nQ. What does the halting problem prove? \n Answer: \n\nA. That some problems are incomputable, or insolvable by any computer."}, {"text": "Title: Programming Concepts and the C Language \n Text: {Programming Concepts and the C Language}\n\nThis set of notes introduces the C programming language and explains\nsome basic concepts in computer programming.  Our purpose in showing\nyou a high-level language at this early stage of the course is to give\nyou time to become familiar with the syntax and meaning of the language,\nnot to teach you how to program.  Throughout this semester, we will\nuse software written in C to demonstrate and validate the digital system design\nmaterial in our course.  Towards the end of the semester, you will\nlearn to program computers using instructions and assembly language.\nIn ECE 220, you will make use of the C language to write\nprograms, at which point already being familiar with the language will\nmake the material easier to master.  \n\nThese notes are meant to complement the\nintroduction provided by Patt and Patel.\n\nAfter a brief introduction to the history of C and the structure of\na program written in C, we connect the idea of representations developed \nin class to the data types used in high-level languages.\n\nWe next discuss the use of variables in C, then describe some of the \noperators available to the programmer, including arithmetic and logic\noperators.  The notes next introduce C functions that support the ability to\nread user input from the keyboard and to print results to the monitor.\n\nA description of the structure of statements in C follows, explaining\nhow programs are executed and how a programmer can create statements\nfor conditional execution as well as loops to perform repetitive tasks.\n\nThe main portion of the notes concludes with \nan example program, which is used to illustrate both the execution of \nC statements as well as the difference between variables in programs\nand variables in algebra.\n\nThe remainder of the notes covers more advanced topics.  First, we \ndescribe how the compilation process works,\nillustrating how a program written in a high-level language is\ntransformed into instructions.  You will learn this process in much\nmore detail in ECE 220.  \n\nSecond, we briefly introduce the C preprocessor.\n\nFinally, we discuss implicit and explicit data type conversion in C.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}\n\n\n\n \n Question: \nQ. What is the purpose of introducing the C programming language in this course? \n Answer: \n\nA. The purpose of introducing the C programming language in this course is to give students time to become familiar with the syntax and meaning of the language, not to teach them how to program."}, {"text": "Title: The C Programming Language \n Text: {The C Programming Language}\n\nProgramming languages attempt to bridge the semantic gap between human\ndescriptions of problems and the relatively simple instructions that\ncan be provided by an instruction set architecture (ISA).  \nSince 1954, when the Fortran language\nfirst enabled scientists to enter FORmulae symbolically and to have\nthem TRANslated automatically into instructions, people have invented\nthousands of computer languages.  \n\n\nThe C programming language was developed by Dennis Ritchie at Bell Labs\nin order to simplify the task of writing the Unix operating system.\nThe C language provides a fairly transparent mapping to typical ISAs,\nwhich makes it a good choice both for system software such as operating\nsystems and for our class.\n\nThe { syntax} used in C---that is, the rules that one must follow\nto write valid C programs---has also heavily influenced many other\nmore recent languages, such as C++, Java, and Perl.  \n\nFor our purposes, a C program consists of a set of { variable \ndeclarations} and a sequence of { statements}.  \n\n\n{\n\naaaa=/* =\nint\nmain ()\n{\n>  int answer = 42;        /* the Answer! */\n\n>  printf (\"The answer is d.n\", answer);\n\n>  /*> Our work here is done.\n>    > Let's get out of here! */\n>  return 0;\n}\n\n}\n\n\nBoth of these parts are written into a single C function called { main},\nwhich executes when the program starts.  \n\nA simple example appears to the right.  The program uses one variable\ncalled { answer}, which it initializes to the value 42.\nThe program prints a line of output to the monitor for the user,\nthen terminates using the { return} statement.  { Comments} for human\nreaders begin with the characters { /*} (a slash followed by an \nasterisk) and end with the characters { */} (an asterisk followed \nby a slash).\n\nThe C language ignores white space in programs, so we encourage\nyou to use blank lines and extra spacing to make your programs\neasier to read.\n\nThe variables defined in the { main} function allow a programmer\nto associate arbitrary { symbolic names} (sequences of English characters, \nsuch as ``sum'' or ``product'' or ``highScore'') with specific\ntypes of data, such as a {16-bit} unsigned integer or a\ndouble-precision floating-point number. \n\nIn the example program above, the variable { answer} is declared\nto be a {32-bit} {2's} complement number.\n\nThose with no programming experience may at first find the difference\nbetween variables in algebra and variables in programs slightly \nconfusing.  { As a program executes, the values of variables can \nchange from step to step of execution.}\n\nThe statements in the { main} function are executed one by one\nuntil the program terminates.  \n\nPrograms are not limited to simple sequences of statements, however.\nSome types of statements allow a programmer\nto specify conditional behavior.  For example, a program might only\nprint out secret information if the user's name is ``lUmeTTa.''\nOther types of statements allow a programmer to repeat the execution\nof a group of statements until a condition is met.  For example, a program\nmight print the numbers from 1 to 10, or ask for input until the user\ntypes a number between 1 and 10.\n\nThe order of statement execution is well-defined in C, but the\nstatements in { main} do not necessarily make up an algorithm:\n{ we can easily write a C program that never terminates}.\n\nIf a program terminates, the { main} function\nreturns an integer to the operating system, usually by executing\na { return} statement, as in the example program.\n\nBy convention, returning the value 0 indicates successful completion\nof the program, while any non-zero value indicates a program-specific\nerror.\n\nHowever, { main} is not necessarily a function in the mathematical \nsense because { the value returned from { main} is not \nnecessarily unique for a given set of input values to the program}.  \n\nFor example, we can write a program that selects a number from 1 to 10 \nat random and returns the number to the operating system.\n\n\n\n\n \n Question: \nQ. What is the difference between variables in algebra and variables in programs? \n Answer: \n\nA. In algebra, variables are unchanging values that are used to represent other values in equations. In programs, variables can change from step to step of execution, and are used to store data that can be used by the program."}, {"text": "Title: Data Types \n Text: {Data Types}\n\nAs you know, modern digital computers represent all information with\nbinary digits (0s and 1s), or { bits}.  Whether you are representing \nsomething as simple as an integer or as complex as an undergraduate \nthesis, the data are simply a bunch of 0s and 1s inside a computer.  \n\nFor any given type of information, a human selects a data type for the\ninformation.  A { data type} (often called just a { type})\nconsists of both a size in bits and a representation, such as the\n2's complement representation for signed integers, or the ASCII\nrepresentation for English text.  A { representation} is a way of\nencoding the things being represented as a set of bits, with each bit\npattern corresponding to a unique object or thing.\n\nA typical ISA supports a handful of\ndata types in hardware in the sense that it provides hardware \nsupport for operations on those data types.\n\nThe arithmetic logic units (ALUs) in most modern processors,\nfor example, support addition\nand subtraction of both unsigned and 2's complement representations, with\nthe specific data type (such as 16- or 64-bit 2's complement)\ndepending on the ISA.\n\nData types and operations not supported by the ISA must be handled in\nsoftware using a small set of primitive operations, which form the\n{ instructions} available in the ISA.  Instructions usually\ninclude data movement instructions such as loads and stores\nand control instructions such as branches and subroutine calls in\naddition to arithmetic and logic operations.  \n\nThe last quarter of our class covers these concepts in more detail\nand explores their meaning using an example ISA from the textbook.\n\nIn class, we emphasized the idea that digital systems such as computers\ndo not interpret the meaning of bits.  Rather, they do exactly what\nthey have been designed to do, even if that design is meaningless.\n\nIf, for example, you store\na sequence of ASCII characters \nin a computer's memory as \nand\nthen write computer instructions to add consecutive groups of four characters\nas 2's complement integers and to print the result to the screen, the\ncomputer will not complain about the fact that your code produces\nmeaningless garbage.  \n\nIn contrast, high-level languages typically require that a programmer\nassociate a data type with each datum in order to reduce the chance \nthat the bits \nmaking up an individual datum are misused or misinterpreted accidentally.  \n\nAttempts to interpret a set of bits differently usually generate at least\na warning message, since\n\n\n\nsuch re-interpretations of the\nbits are rarely intentional and thus rarely correct.  A compiler---a\nprogram that transforms code written in a high-level language into\ninstructions---can also generate the proper type conversion instructions \nautomatically when the \ntransformations are intentional, as is often the case with arithmetic.\n\nSome high-level languages, such as Java, \nprevent programmers from changing the type of a given datum.\nIf you define a type that represents one of your\nfavorite twenty colors, for example, you are not allowed to turn a\ncolor into an integer, despite the fact that the color is represented\nas a handful of bits.  Such languages are said to be { strongly\ntyped}.  \n\nThe C language is not strongly typed, and programmers are free to\ninterpret any bits in any manner they see fit.  Taking advantage of\nthis ability in any but a few exceptional cases, however, \nresults in arcane and non-portable code, and is thus considered to be\nbad programming practice.  We discuss conversion between types in more\ndetail later in these notes.\n\nEach high-level language defines a number of { primitive data\ntypes}, which are always available.  Most languages, including C,\nalso provide ways of defining new types in terms of primitive types,\nbut we leave that part of C for ECE 220.\n\nThe primitive data types in C include signed and unsigned integers of various\nsizes as well as single- and double-precision IEEE floating-point numbers.\n\n\nThe primitive integer types in C include both unsigned and 2's\ncomplement representations.  These types were originally defined so as\nto give reasonable performance when code was ported.  In particular,\nthe { int} type is intended to be the native integer type for the\ntarget ISA.  Using data types supported directly in hardware is faster \nthan using larger or smaller integer types.  When C was standardized in 1989,\nthese types were defined so as to include a range of existing\nC compilers rather than requiring all compilers to produce uniform\nresults.  At the\n\n\n{-8pt}{|r|l|l|}\n& {|c|}{2's complement}& {|c|}\n8 bits& { char}& { unsigned char} \n16 bits& { short}& { unsigned short}\n& { short int}& { unsigned short int} \n32 bits& { int}& { unsigned}\n&& { unsigned int} \n32 or & { long}& { unsigned long}\n64 bits& { long int}& { unsigned long int} \n64 bits& { long long}& { unsigned long long}\n& { long long int}& { unsigned long long int}\n\n{-14pt}\n\ntime, most workstations and mainframes were 32-bit machines, while\nmost personal computers were 16-bit machines, thus flexibility was somewhat\ndesirable.  For the GCC compiler on Linux, the C integer data \ntypes are defined\nin the table above.  Although the { int} and { long}\ntypes are usually the same, there is a semantic difference in common\nusage.  In particular, on most architectures and most compilers, a\n{ long} has enough bits to identify a location in the computer's\nmemory, while an { int} may not.\n\nWhen in doubt, the { size in bytes} of any type or variable can be\nfound using the built-in C function { sizeof}.\n\n\nOver time, the flexibility of size in C types has become less\nimportant (except for the embedded markets, where one often wants even\nmore accurate bit-width control), and the fact that the size of an\n{ int} can vary from machine to machine and compiler to compiler\nhas become more a source of headaches than a helpful feature.  In the\nlate 1990s, a new set of fixed-size types were recommended for\ninclusion\n\n\n{-8pt}{|r|l|l|}\n& {|c|}{2's complement}& {|c|}\n 8 bits& {  int8_t}& {  uint8_t}\n16 bits& { int16_t}& { uint16_t}\n32 bits& { int32_t}& { uint32_t}\n64 bits& { int64_t}& { uint64_t}\n\n{-14pt}\n\nin the C library, reflecting the fact that many companies\nhad already developed and were using such definitions to make their\nprograms platform-independent.\n\nWe encourage you to make use of these types, which are shown in \nthe table above.  In Linux, they can be made available by including \nthe { stdint.h} header file.\n\nFloating-point types in C include { float} and { double},\nwhich correspond respectively to single- and double-precision IEEE\nfloating-point values.  Although the {32-bit} { float} type\ncan save memory compared with use of {64-bit} { double}\nvalues, C's math library works with double-precision values, and\nsingle-precision data are uncommon in scientific and engineering\ncodes.  In contrast, single-precision floating-point operations\ndominated the\ngraphics industry until recently, and are still well-supported even\non today's graphics processing units.\n\n\n \n Question: \nQ. What is the standard data type for an integer in C? \n Answer: \n\nA. The standard data type for an integer in C is the int data type. This data type is a fixed-size type that is typically 32 bits in size."}, {"text": "Title: Variable Declarations \n Text: {Variable Declarations}\n\nThe function { main} executed by a program begins with a list\nof { variable declarations}.  Each declaration consists of two parts:\na data type specification and a comma-separated list of variable names.\nEach variable declared can also \nbe { initialized} by assigning an initial value.  A few examples \nappear below.  Notice that one can initialize a variable to have the same\nvalue as a second variable.\n\n{\n\naaaaaaa=aa=aaaaaaaaaaaaaaaaaaaaa=/=* a third 2's complement variable with unknown initial value =\nint > x > = 42; >/>* a 2's complement variable, initially equal to 42 > */\nint > y > = x;  >/>* a second 2's complement variable, initially equal to x > */\nint > z;>       >/>* a third 2's complement variable with unknown initial value > */\ndouble> a, b, c, pi = 3.1416; > >/>*\n>>>>* four double-precision IEEE floating-point variables\n>>>>* a, b, and c are initially of unknown value, while pi is\n>>>>* initially 3.1416\n>>>>*/\n\n}\n\nWhat happens if a programmer declares a variable but does not \ninitialize it?\n\nRemember that bits can only be 0 or 1.\n\nAn uninitialized variable does have a value, but its value is unpredictable.\n\nThe compiler tries to detect uses of uninitialized variables, but sometimes\nit fails to do so, so { until you are more familiar with programming,\nyou should always initialize every variable}.\n\nVariable names, also called { identifiers}, can include both letters\nand digits in C.\n\nGood programming style requires that programmers select variable names\nthat are meaningful and are easy to distinguish from one another.\n\nSingle letters are acceptable in some situations, but longer names with\nmeaning are likely to help people (including you!) understand your \nprogram.\n\nVariable names are also case-sensitive in C, which allows programmers\nto use capitalization to differentiate behavior and meaning, if desired.\nSome programs, for example, use identifiers with all capital letters\nto indicate variables with values that remain constant for the program's\nentire execution.\n\nHowever, the fact that identifiers are case-sensitive also means \nthat a programmer can declare distinct variables \nnamed { variable}, { Variable}, { vaRIable}, { vaRIabLe}, \nand { VARIABLE}.  We strongly discourage you from doing so.\n\n\n\n\n \n Question: \nQ. What is the meaning of the term \"uninitialized variable?\" \n Answer: \n\nA. A variable that has not been assigned a value."}, {"text": "Title: Expressions and Operators \n Text: {Expressions and Operators}\n\n\nThe { main} function also contains a sequence of statements.\n\nA statement is a complete specification of a single step\nin the program's execution.\n\nWe explain the structure of\nstatements in the next section.  \n\nMany statements in C include one or more { expressions},\nwhich represent calculations such as arithmetic, comparisons,\nand logic operations.\n\nEach expression is in turn composed of { operators} and { operands}.\n\nHere we give only a brief introduction to some of the operators available\nin the C language.  We deliberately omit operators with more\ncomplicated meanings, as well as operators for which the original\npurpose was to make writing common operations a little shorter.\n\nFor the interested reader, both the textbook and ECE 220 give more \ndetailed introductions.\n\nThe table to the right gives examples for the operators described \nhere.  \n\n\n\n{\n\n{int i = 42, j = 1000;}\n{/* i = 0x0000002A, j = 0x000003E8 */}\n\ni + j &  1042\ni - 4 * j &  -3958\n-j &  -1000\nj / i &  23\nj  i &   42\ni & j &   40& /* 0x00000028 */\ni | j &   1002& /* 0x000003EA */\ni  j &   962& /* 0x000003C2 */\ni &   -43& /* 0xFFFFFFD5 */\n(i) >> 2 &   -11& /* 0xFFFFFFF5 */\n((i) >> 4) &   2& /* 0x00000002 */\nj >> 4 &  62& /* 0x0000003E */ \nj << 3 &  8000& /* 0x00001F40 */ \ni > j &   0\ni <= j &   1\ni == j &   0\nj = i &   42& /* ...and j is changed! */\n\n}\n\n\n\n\n{ Arithmetic operators} in C include addition ({ +}), \nsubtraction ({ -}), negation (a minus sign not \npreceded by another expression), multiplication ({ *}), \ndivision ({ /}), and modulus ({ }).  No exponentiation\noperator exists; instead, library routines are defined for this purpose\nas well as for a range of more complex mathematical functions.\n\nC also supports { bitwise operations} on integer types, including \nAND ({ &}), OR ({ |}), XOR ({ ^{ }}), NOT ({ }), \nand left ({ <<}) and right ({ >>}) bit shifts.\nRight shifting a signed integer results in an { arithmetic right shift}\n(the sign bit is copied), while right shifting an unsigned integer\nresults in a { logical right shift} (0 bits are inserted).\n\nA range of { relational} or { comparison operators} are \navailable, including equality ({ ==}),\ninequality ({ !=}), and relative order ({ <}, { <=},\n{ >=}, and { >}).\n\nAll such operations evaluate to 1 to indicate a true relation\nand 0 to indicate a false relation.  Any non-zero value is considered\nto be true for the purposes of tests (for example, in an { if} statement\nor a { while} loop) in C---these statements are explained later in \nthese notes.\n\n{ Assignment} of a new value to a variable \nuses a single equal sign ({ =}) in C.  \n\nFor example, the expression { A = B} copies\nthe value of variable { B} into variable { A}, overwriting the\nbits representing the previous value of { A}.\n\n{ The use of two equal signs for an equality check and a single\nequal sign for assignment is a common source of errors,} although\nmodern compilers generally detect and warn about this type of mistake.\n\nAssignment in C does not solve equations, even simple equations.  \nWriting ``{ A-4=B}'', for example, generates a compiler error.\nYou must solve such equations yourself to calculate the desired\nnew value of a single variable, such as ``{  A=B+4}.''\nFor the purposes of our class, you must always write a single variable \non the left side of an assignment, and can write an arbitrary expression \non the right side.\n\nMany operators can be combined into a single expression.  When an\nexpression has more than one operator, which operator is executed first?\nThe answer depends on the operators' { precedence}, a well-defined order on\noperators that specifies how to resolve the ambiguity.  In the case\nof arithmetic, the C language's precedence specification matches the\none that you learned in elementary school.  For example, { 1+2*3}\nevaluates to 7, not to 9, because multiplication has precedence over\naddition.  For non-arithmetic operators, or for any case in which\nyou do not know the precedence specification for a language, {\ndo not look it up---other programmers will not remember the\nprecedence ordering, either!}  Instead, add parentheses to make your \nexpressions clear and easy to understand.\n\n\n \n Question: \nQ. What is the purpose of an expression? \n Answer: \n\nA. The purpose of an expression is to represent a calculation, such as an arithmetic operation, a comparison, or a logic operation."}, {"text": "Title: Basic I/O \n Text: {Basic I/O}\n\nThe { main} function returns an integer to the operating system.\nAlthough we do not discuss how additional functions can be written\nin our class, we may sometimes make use of functions that have been\nwritten in advance by making { calls} to those functions.\nA { function call} is type of expression in C, but we leave \nfurther description for ECE 220.  In our class, we make use of only\ntwo additional functions to enable our programs to receive input\nfrom a user via the keyboard and to write output to the monitor for \na user to read.\n\nLet's start with output.  The { printf} function allows a program\nto print output to the monitor using a programmer-specific format.\nThe ``f'' in { printf} stands for ``formatted.''{The \noriginal, unformatted variant of printing was never available\nin the C language.  Go learn Fortran.}\n\nWhen we want to use { printf}, we write a expression with\nthe word { printf} followed by a parenthesized, comma-separated\nlist of expressions.  The expressions in this list are called\nthe { arguments} to the { printf} function.\n\nThe first argument to the { printf} function is a format string---a \nsequence of ASCII characters between quotation marks---which tells \nthe function what kind of information we want printed to\nthe monitor as well as how to format that information.\n\nThe remaining arguments are C expressions that give { printf}\na copy of any values that we want printed.\n\nHow does the format string specify the format?\n\nMost of the characters in the format string are simply printed to \nthe monitor.  \n\nIn the first example shown to on the next page, we use { printf}\nto print a hello message followed by an ASCII newline character\nto move to the next line on the monitor.\n\n\nThe percent sign---``''---is used \nas an { escape character} in the\n{ printf} function.  When ``'' appears in the format\nstring, the function examines the next character in the format string\nto determine which format to use, then takes\nthe next expression from the sequence\nof arguments and prints the value of that expression to the \nmonitor.  Evaluating an expression generates a bunch of bits, so it is up to\nthe programmer to ensure that those bits are not misinterpreted.\nIn other words, the programmer must make sure that the number and\ntypes of formatted values match the number and types of arguments passed\nto { printf} (not counting the format string itself).\n\nThe { printf} function returns the number of characters printed\nto the monitor.\n\n\n\noutput: =\n> { printf (\"Hello, world!n\");}\noutput: > { Hello, world!} [and a newline]\n\n> { printf (\"To x or not to d...n\", 190, 380 / 2);}\noutput: > { To be or not to 190...} [and a newline]\n\n> { printf (\"My favorite number is cc.n\", 0x34, '0'+2);}\noutput: > { My favorite number is 42.} [and a newline]\n\n> { printf (\"What is pi?  f or e?n\", 3.1416, 3.1416);}\noutput: > { What is pi?  3.141600 or 3.141600e+00?} [and a newline]\n\n\n{|c|l|}\nescape  &                         \nsequence& { printf} function's interpretation of expression bits \n{ c}& 2's complement integer printed as an ASCII character\n{ d}& 2's complement integer printed as decimal\n{ e}& double printed in decimal scientific notation\n{ f}& double printed in decimal\n{ u}& unsigned integer printed as decimal\n{ x}& integer printed as hexadecimal (lower case)\n{ X}& integer printed as hexadecimal (upper case)\n{ }& a single percent sign \n\n\n\n\nA program can read input from the user with the { scanf} function.\nThe user enters characters in ASCII using the keyboard, and the\n{ scanf} function converts the user's input into C primitive types,\nstoring the results into variables.  As with { printf}, the\n{ scanf} function takes a format string followed by a comma-separated\nlist of arguments.  Each argument after the format string provides\n{ scanf} with the memory address of a variable into which the\nfunction can store a result.\n\nHow does { scanf} use the format string?\n\nFor { scanf}, the format string is usually just a sequence\nof conversions, one for each variable to be typed in by the user.\nAs with { printf}, the conversions start with ``'' and\nare followed by characters specifying the type of conversion\nto be performed.  The first example shown to the right reads\ntwo integers.\n\nThe conversions in the format string can be separated by spaces \nfor readability, as shown in the exam-\n\n\n\neffect: =unsigned =\n> { int     } > { a, b;  /* example variables */}\n> { char    } > { c;}\n> { unsigned} > { u;}\n> { double  } > { d;}\n> { float   } > { f;}\n\n> { scanf (\"dd\", &a, &b);   /* These have the */}\n> { scanf (\"d d\", &a, &b);  /* same effect.   */}\neffect: > try to convert two integers typed in decimal to\n> 2's complement and store the results in { a} and { b}\n\n> { scanf (\"cx lf\", &c, &u, &d);}\neffect: > try to read an ASCII character into { c}, a value\n> typed in hexadecimal into { u}, and a double-\n> precision > floating-point number into { d}\n\n> { scanf (\"lf f\", &d, &f);}\neffect: > try to read two real numbers typed as decimal,\n> convert the first to double-precision and store it \n> in { d}, and convert the second to single-precision \n> and store it in { f}\n\n\n\n\nple.  The spaces are ignored\nby { scanf}.  However, { any non-space characters in the\nformat string must be typed exactly by the user!}\n\nThe remaining arguments to { scanf} specify memory addresses\nwhere the function can store the converted values.  \n\nThe ampersand (``&'') in front of each variable name in the examples is an\noperator that returns the address of a variable in memory.\n\nFor each con-\n\n\n{|c|l|}\nescape  &                         \nsequence& { scanf} function's conversion to bits \n{ c}& store one ASCII character (as { char})\n{ d}& convert decimal integer to 2's complement\n{ f}& convert decimal real number to float\n{ lf}& convert decimal real number to double\n{ u}& convert decimal integer to unsigned int\n{ x}& convert hexadecimal integer to unsigned int\n{ X}& (as above) \n\n\n\nversion\nin the format string, the { scanf} function tries to convert\ninput from the user into the appropriate result, then stores the\nresult in memory at the address given by the next argument.\n\nThe programmer is responsible for ensuring that the number of \nconversions in the format string\nmatches the number of arguments provided (not counting\nthe format string itself).  The programmer must also ensure that\nthe type of information produced by each conversion can be\nstored at the address passed for that conversion---in other words,\nthe address of a\nvariable with the correct type must be\nprovided.  Modern compilers often detect missing { &} operators\nand incorrect variable types, but many only give warnings to the\nprogrammer.  The { scanf} function itself cannot tell whether\nthe arguments given to it are valid or not.\n\nIf a conversion fails---for example, if a user types ``hello'' when\n{ scanf} expects an integer---{ scanf} does not overwrite the\ncorresponding variable and immediately stops trying to convert input.\n\nThe { scanf} function returns the number of successful \nconversions, allowing a programmer to check for bad input from\nthe user.\n\n \n Question: \nQ. What is the purpose of the & operator in the scanf function? \n Answer: \n\nThe & operator is used to get the address of a variable. This is necessary because scanf needs to know where to store the result of the conversion."}, {"text": "Title: Types of Statements in C \n Text: {Types of Statements in C}\n\nEach statement in a C program specifies a complete operation.\n\nThere are three types of statements, but two of these types can\nbe constructed from additional statements, which can in turn be\nconstructed from additional statements.  The C language specifies\nno bound on this type of recursive construction, but code \nreadability does impose a practical limit.\n\n\nThe three types are shown to the right.\nThey are the { null statement}, \n{ simple statements}, \nand { compound statements}.\n\nA null statement is just a semicolon, and a compound statement \nis just a sequence of statements surrounded by braces.\n\nSimple statements can take several forms.  All of the examples\nshown to the right, including the call to { printf}, are\nsimple state-\n\n\n{\n\naaaa=aaaaaaaaaaaaaa=/* =aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa=\n;   > > /* >a null statement (does nothing) >*/\n\nA = B; > > /*  >examples of simple statements >*/\nprintf (\"Hello, world!n\");\n\n{    > > /* > a compound statement >*/ \n>  C = D; > /* > (a sequence of statements >*/\n>  N = 4; > /* > between braces) >*/ \n>  L = D - N;\n}\n\n}\n{-2pt}\n\n\nments consisting of a C expression followed by a \nsemicolon.\n\nSimple statements can also consist of conditionals or iterations, which\nwe introduce next.\n\nRemember that after variable declarations, the { main} function\ncontains a sequence of statements.  These statements are executed one\nat a time in the order given in the program, as shown to the right\nfor two statements.  We say that the statements are executed in\nsequential order.\n\nA program must also be able to execute statements only when \nsome condition holds.  In the C language, such a condition can be\nan arbitrary expression.  The expression is first evaluated.\nIf the result is 0, the condition\nis considered to be false.  Any result other than 0 is considered\nto be true.  The C statement\nfor conditional execution is called an { if}\n\n\n{file=part1/figs/part1-sd-sequential.eps,width=0.8in}\n\n\n\nstatement.  Syntactically, we put the expression for the condition\nin parentheses after the keyword { if} and follow the parenthesized\nexpression with a compound statement containing the statements\nthat should be executed when the condition is true.  Optionally,\nwe can append the keyword { else} and a second compound\nstatement containing statements to be executed when the condition\nevaluates to false.  \nThe corresponding flow chart is shown to the right.\n\n{\n\naaaa=aaaaaaaaaaaa=\n/* Set the variable y to the absolute value of variable x. */\nif (0 <= x) {> >/* Is x greater or equal to 0? */\n> y = x;     >/* Then block: assign x to y. */\n} else {\n> y = -x;    >/* Else block: assign negative x to y. */\n}\n\n}\n\n\n{file=part1/figs/part1-sd-conditional.eps,width=2in}\n\n\nIf instead we chose to assign the absolute value of variable { x}\nto itself, we can do so without an { else} block:\n\n{\n\naaaa=aaaaaaaaaaaa=\n/* Set the variable x to its absolute value. */\nif (0 > x) {> >/* Is x less than 0? */\n> x = -x;    >/* Then block: assign negative x to x. */\n}            >> /* No else block is given--no work is needed. */\n\n}\n\n\nFinally, we sometimes need to repeatedly execute a set of statements,\neither a fixed number of times or so long as some condition holds.\nWe refer to such repetition as an { iteration} or a { loop}.\nIn our class, we make use of C's { for} loop when we need to\nperform such a task.  A { for} loop is structured as follows:\n\n{ \n\nfor ([initialization] ; [condition] ; [update]) {\n    [subtask to be repeated]\n}\n\n}\n\nA flow chart corresponding to execution of a { for} loop appears\nto the right.  First, any initialization is performed.  Then the\ncondition---again an arbitrary C expression---is checked.  If the\ncondition evaluates to false (exactly 0), the loop is done.  Otherwise,\nif the condition evaluates to true (any non-zero value),\nthe statements in the compound statement, the subtask or { loop body},\nare executed.  The loop body can contain anything: a sequence of simple \nstatements, a conditional, another loop, or even just an empty list.\nOnce the loop body has finished executing, the { for} loop\nupdate rule is executed.  Execution then checks the condition again,\nand this process repeats until the condition evaluates to 0.\nThe { for} loop below, for example, prints the numbers \nfrom 1 to 42.\n\n{ \n\n/* Print the numbers from 1 to 42. */\nfor (i = 1; 42 >= i; i = i + 1) {\n    printf (\"dn\", i);\n}\n\n}\n\n\n{file=part1/figs/part1-sd-iterative.eps,width=1.35in}\n\n\n\n \n Question:  \n Answer: \nThe type of statement that consists of a C expression followed by a semicolon is a simple statement."}, {"text": "Title: Program Execution \n Text: {Program Execution}\n\n\nWe are now ready to consider the execution of a simple program,\nillustrating how variables change value from step to step and\ndetermine program behavior.\n\nLet's say that two numbers are ``friends'' if they have at least one\n1 bit in common when written in base 2.  So, for example, 100_2 and \n111_2 are friends because both numbers have a 1 in the bit with \nplace value 2^2=4.  Similarly, 101_2 and 010_2 are not friends,\nsince no bit position is 1 in both numbers.\n\nThe program to the right prints all friendships between numbers\nin the interval [0,7].\n\n\n{\n\naaaa=aaaa=aaaaaaaaaa=/* a second number to consider as check's friend =\nint\nmain ()\n{\n>  int > check;  > /* number to check for friends > */\n>  int > friend; > /* a second number to consider as check's friend > */\naaaa=aaaa=aaaa=aaaa=\n>  \n>  /* Consider values of check from 0 to 7. */\n>  for (check = 0; 8 > check; check = check + 1) {\n\n>  >  /* Consider values of friend from 0 to 7. */\n>  >  for (friend = 0; 8 > friend; friend = friend + 1) {\n\n>  >  >  /* Use bitwise AND to see if the two share a 1 bit. */\n>  >  >  if (0 != (check & friend)) {\n\n>  >  >  >  /* We have friendship! */\n>  >  >  >  printf (\"d and d are friends.n\", check, friend);\n>  >  >  }\n>  >  }\n>  }\n}\n\n}\n\n\nThe program uses two\ninteger variables, one for each of the numbers that we consider.\nWe use a { for} loop to iterate over all values of our first\nnumber, which we call { check}.  The loop initializes { check}\nto 0, continues until check reaches 8, and adds 1 to check after\neach loop iteration.  We use a similar { for} loop to iterate\nover all possible values of our second number, which we call { friend}.\nFor each pair of numbers, we determine whether they are friends\nusing a bitwise AND operation.  If the result is non-zero, they\nare friends, and we print a message.  If the two numbers are not\nfriends, we do nothing, and the program moves on to consider the\nnext pair of numbers.\n\n\nNow let's think about what happens when this program executes.\n\nWhen the program starts, both variables are filled with random bits,\nso their values are unpredictable.  \n\nThe first step is the initialization of the first { for} loop, which\nsets { check} to 0.\n\nThe condition for that loop is { 8 > check}, which is true, so\nexecution enters the loop body and starts to execute the first\nstatement, which is our second { for} loop.\n\nThe next step is then the initialization code for the second { for}\nloop, which sets { friend} to 0.\n\nThe condition for the second loop is { 8 > friend}, which is true, so\nexecution enters the loop body and starts to execute the first\nstatement, which\n\n\n\nafter executing...& { check} is...& and { friend} is... \n(variable declarations)& unpredictable bits& unpredictable bits\n{ check = 0}& 0& unpredictable bits\n{ 8 > check}& 0& unpredictable bits\n{ friend = 0}& 0& 0\n{ 8 > friend}& 0& 0\n{ if (0 != (check & friend))}& 0& 0\n{ friend = friend + 1}& 0& 1\n{ 8 > friend}& 0& 1\n{ if (0 != (check & friend))}& 0& 1\n{ friend = friend + 1}& 0& 2\n{(repeat last three lines six more times; number 0 has no friends!)}\n{ 8 > friend}& 0& 8\n{ check = check + 1}& 1& 8\n{ 8 > check}& 1& 8\n{ friend = 0}& 1& 0\n{ 8 > friend}& 1& 0\n{ if (0 != (check & friend))}& 1& 0\n{ friend = friend + 1}& 1& 1\n{ 8 > friend}& 1& 1\n{ if (0 != (check & friend))}& 1& 1\n{ printf ...}& 1& 1\n{(our first friend!?)}\n\n\n\nis the { if} statement.\n\nSince both variables are 0, the { if} condition is false, and\nnothing is printed.\n\nHaving finished the loop body for the inner loop (on { friend}),\nexecution continues with the update rule for that loop---{ friend = \nfriend + 1}---then returns to check the loop's condition again.\n\nThis process repeats, always finding that the number 0 (in { check})\nis not\nfriends (0 has no friends!) until { friend} reaches 8, at which\npoint the inner loop condition becomes false.\n\nExecution then moves to the update rule for the first { for} loop,\nwhich increments { check}.  Check is then compared with 8 to\nsee if the loop is done.  Since it is not, we once again enter the\nloop body and start the second { for} loop over.  The initialization\ncode again sets { friend} to 0, and we move forward as before.\nAs you see above, the first time that we find our { if} condition\nto be true is when both { check} and { friend} are equal to 1.\n\nIs that result what you expected?  To learn that the number 1 is\nfriends with itself?  If so, the program works.  If you assumed that\nnumbers could not be friends with themselves, perhaps we should fix the \nbug?  We could, for example, add another { if} statement to \navoid printing anything when { check == friend}.\n\nOur program, you might also realize, prints each pair of friends twice.\nThe numbers 1 and 3, for example, are printed in both possible orders.  To\neliminate this redundancy, we can change the initialization in the \nsecond { for} loop, either to { friend = check} or to\n{ friend = check + 1}, depending on how we want to define friendship\n(the same question as before: can a number be friends with itself?).\n\n\n\n \n Question:  \n Answer: \nCan a number be friends with itself?\n\nYes, a number can be friends with itself."}, {"text": "Title: Compilation and Interpretation* \n Text: {Compilation and Interpretation*}\n\n\nMany programming languages, including C, can be \n{ compiled}, which means that the program is converted into \ninstructions for a particular ISA before the program is run\non a processor that supports that ISA.\nThe figure to the right illustrates the compilation process for \nthe C language.  \n\nIn this type of figure, files and other data are represented as cylinders,\nwhile rectangles represent processes, which are usually implemented in \nsoftware.\n\nIn the figure to the right, the outer dotted box represents the full \ncompilation\nprocess that typically occurs when one compiles a C program.\nThe inner dotted box represents the work performed by the { compiler}\nsoftware itself.\n\nThe cylinders for data passed between the processes that compose\nthe full compilation process\nhave been left out of the figure; instead, we have written the type\nof data being passed next to the arrows that indicate the flow of information\nfrom one process to the next.\n\nThe C preprocessor (described later in these notes) forms the\nfirst step in the compilation process.  The preprocessor\noperates on the program's { source code} along\nwith { header files} that describe data types and\noperations.  The preprocessor merges these together\ninto a single file of preprocessed source code.  The preprocessed\nsource code is then analyzed by the front end of the compiler based on the\nspecific programming language being used (in our case, the C language),\nthen converted by the back end of the compiler\ninto instructions for the desired ISA.  The output of a compiler\nis not binary instructions, however, but is instead\na human-readable form of instructions called { assembly code},\nwhich we cover in the last quarter of our class.  A tool called\nan assembler then converts these human-readable instructions into\nbits that a processor can understand.  If a program consists of\nmulti-\n\n\n{file=part1/figs/part1-compiler.eps,width=3in}\n\n\nple source files, or needs to make use of additional \npre-programmed operations (such as math functions, graphics, or sound),\na tool called a linker merges the object code of the program with\nthose additional elements to form the final { executable image}\nfor the program.  The executable image is typically then stored on\na disk, from which it can later be read into memory in order to\nallow a processor to execute the program.\n\nSome languages are difficult or even impossible to compile.  Typically, the\nbehavior of these languages depends on input data that are only available \nwhen the program runs.  Such languages can be { interpreted}: each step \nof the algorithm described by a program is executed by a software interpreter\nfor the language.  Languages such as Java, Perl, and Python are usually\ninterpreted.  Similarly, when we use software to simulate one ISA using\nanother ISA, as we do at the end of our class with the {LC-3}\nISA described by the textbook, the simulator is a form of interpreter.\nIn the lab, you will use a simulator compiled into and executing as x86 \ninstructions in order to interpret {LC-3} instructions.  \n\nWhile a program is executing in an interpreter, enough information\nis sometimes available to compile part or all of the program to\nthe processor's ISA as the program runs, \na technique known as { ``just in time'' ( JIT) compilation}.\n\n\n\n\n \n Question: \nQ. How does the compiler work? \n Answer: \n\nA. The compiler converts the human-readable instructions in the source code into machine code, which is a set of instructions that can be executed by a processor. The machine code is then stored in an executable image, which can be run on a computer."}, {"text": "Title: The C Preprocessor* \n Text: {The C Preprocessor*}\n\nThe C language uses a preprocessor to support inclusion of common\ninformation (stored in header files) into multiple source files.\n\nThe most frequent use of the preprocessor is to enable the unique\ndefinition of new data types and operations within\nheader files that can then be included by reference within source\nfiles that make use of them.  This capability is based on the \n{ include directive}, { #include}, as shown here:  \n\n{\n\n\n#include \"my_header.h\"    = /* search in current followed by standard directories =\n#include <stdio.h>      > /* search in standard directories > */\n#include \"my_header.h\" > /* search in current followed by standard directories > */\n\n\n}\n\nThe preprocessor also supports integration of compile-time constants\ninto source files before compilation.  For example, many\nsoftware systems allow the definition of a symbol such as { NDEBUG}\n(no debug) to compile without additional debugging code included in\nthe sources.  \n\nTwo directives are necessary for this purpose: the { define directive},\n{ #define}, which\nprovides a text-replacement facility, and { conditional inclusion} (or\nexclusion) of parts of a file within { #if}/{ #else}/{\n#endif} directives.\n\nThese directives are also useful in allowing\n\n\na single header file to\nbe included multiple times without causing problems, as C does not\nallow redefinition of types, variables, and so forth, even if the\nredundant \ndefinitions are identical.  Most header files are thus wrapped as shown\nto the right.\n\n\n{\n\n#if !defined(MY_HEADER_H)\n#define MY_HEADER_H\n/* actual header file material goes here */\n#endif /* MY_HEADER_H */\n\n}\n\n\nThe preprocessor performs a simple linear pass on the source and does\nnot parse or interpret any C syntax.\n\nDefinitions for text replacement are valid as soon as they are defined\nand are performed until they are undefined or until the end of the\noriginal source file.\n\nThe preprocessor does recognize spacing and will not replace part of a\nword, thus ``{ #define i 5}'' will not wreak havoc on your {\nif} statements, but will cause problems if you name any variable { i}.\n\nUsing the text replacement capabilities of the preprocessor does have\ndrawbacks, most importantly in that almost none of the information is\npassed on for debugging purposes.  \n\n \n Question: \nQ.  What is the name of the directive that allows for the inclusion of common information into multiple source files? \n Answer: \n\nA. The name of the directive that allows for the inclusion of common information into multiple source files is the include directive."}, {"text": "Title: Changing Types in C* \n Text: {Changing Types in C*}\n\nChanging the type of a datum is necessary from time to time, but\nsometimes a compiler can do the work for you.\n\nThe most common form of { implicit type conversion} occurs with binary\narithmetic operations.  Integer arithmetic in C always uses types of\nat least the size of { int}, and all floating-point arithmetic uses\n{ double}.\n\nIf either or both operands have smaller integer types, or differ from\none another, the compiler implicitly converts them before performing\nthe operation, and the type of the result may be different from those of\nboth operands.\n\nIn general, the compiler selects the final type according to some\npreferred ordering in which floating-point is preferred over integers,\nunsigned values are preferred over signed values, and more bits are\npreferred over fewer bits.\n\nThe type of the result must be at least as large as either argument,\nbut is also at least as large as an { int} for integer operations\nand a { double} for floating-point operations.\n\nModern C compilers always extend an integer type's bit width before\nconverting from signed to unsigned.  The original C specification\ninterleaved bit width extensions to { int} with sign changes, thus\n{ older compilers may not be consistent, and implicitly require\nboth types of conversion in a single operation may lead to portability\nbugs.}\n\nThe implicit extension to { int} can also be confusing in the sense\nthat arithmetic that seems to work on smaller integers fails with\nlarger ones.  For example, multiplying two 16-bit integers set to 1000\nand printing the result works with most compilers because the 32-bit \n{ int} result is wide enough to hold the right answer.  In contrast,\nmultiplying two 32-bit integers set to 100,000 produces the wrong\nresult because the high bits of the result are discarded before it can\nbe converted to a larger type.  For this operation to produce the\ncorrect result, one of the integers must be converted explicitly (as\ndiscussed later) before the multiplication.\n\n\n\nImplicit type conversions also occur due to assignments.  Unlike\narithmetic conversions, the final type must match the left-hand side\nof the assignment (for example, a variable to which a result is assigned), and\nthe compiler simply performs any necessary conversion.\n\n{ Since the desired type may be smaller than the type of the value\nassigned, information can be lost.}  Floating-point values are\ntruncated when assigned to integers, and high bits of wider integer\ntypes are discarded when assigned to narrower integer types.  { Note\nthat a positive number may become a negative number when bits are\ndiscarded in this manner.}\n\nPassing arguments to functions can be viewed as a special case of\nassignment.  Given a function prototype, the compiler knows the type\nof each argument and can perform conversions as part of the code\ngenerated to pass the arguments to the function.  Without such a\nprototype, or for functions with variable numbers of arguments, the\ncompiler lacks type information and thus cannot perform necessary\nconversions, leading to unpredictable behavior.  By default, however,\nthe compiler extends any integer smaller than an { int}\nto the width of an { int} and converts { float} to\n{ double}.\n\n\nOccasionally it is convenient to use an { explicit type cast} to force\nconversion from one type to another.  { Such casts must be used\nwith caution, as they silence many of the warnings that a compiler\nmight otherwise generate when it detects potential problems.}  One\ncommon use is to promote integers to floating-point before an\narithmetic operation, as shown to the right.\n\n\n{\n\naaaa=\nint\nmain ()\n{\n>  int numerator = 10;\n>  int denominator = 20;\n>\n>  printf (\"fn\", numerator / (double)denominator);\n>  return 0;\n}\n\n}\n{-14pt}\n\nThe type to which a value is to be converted\nis placed in parentheses in front of the value.  In most cases,\nadditional parentheses should be used to avoid confusion about the\nprecedence of type conversion over other operations.\n\n\n\n\n\n \n Question: \nQ. What is the most common form of implicit type conversion? \n Answer: \n\nA. The most common form of implicit type conversion is with binary arithmetic operations."}, {"text": "Title: Summary of Part 1 of the Course \n Text: {Summary of Part 1 of the Course}\n\nThis short summary provides a list of both terms that we expect you to\nknow and and skills that we expect you to have after our first few weeks\ntogether.  The first part of the course is shorter than the other three\nparts, so the amount of material is necessarily less.\n\nThese notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.\n\nAccording to educational theory, the difficulty of learning depends on \nthe type of task involved.  Remembering new terminology is relatively \neasy, while applying the ideas underlying design decisions shown by \nexample to new problems posed as human tasks is relatively hard.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nThis time, we'll list the skills first and leave the easy stuff for the \nnext page.\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Represent decimal numbers with unsigned, 2's complement, and IEEE\nfloating-point representations, and be able to calculate the decimal value\nrepresented by a bit pattern in any of these representations.}\n\n{Be able to negate a number represented in the 2's complement\nrepresentation.}\n\n{Perform simple arithmetic by hand on unsigned and 2's complement\nrepresentations, and identify when overflow occurs.}\n\n{Be able to write a truth table for a Boolean expression.}\n\n{Be able to write a Boolean expression as a sum of minterms.}\n\n MOVED TO PART 4\n\n {Be able to calculate the Hamming distance of a code/representation.}\n \n {Know the relationships between Hamming distance and the abilities\n to detect and to correct bit errors.}\n\n{Know how to declare and initialize C variables with one of the \nprimitive data types.}\n\n\n\nAt a more abstract level, we expect you to be able to:\n\n{}{{}{}\n{}{}{}\n\n{Understand the value of using a common mathematical basis, such\nas modular arithmetic, in defining multiple representations (such as\nunsigned and 2's complement).}\n\n{Write Boolean expressions for the overflow conditions\non both unsigned and 2's complement addition.}\n\n MOVED TO PART 4\n\n {Be able to use parity for error detection, and Hamming codes for\n error correction.}\n\n{Be able to write single { if} statements and { for} loops\nin C in order to perform computation.}\n\n{Be able to use { scanf} and { printf} for basic input and \noutput in C.}\n\n\n\nAnd, at the highest level, we expect that you will be able to reason about\nand analyze problems in the following ways:\n\n{}{{}{}\n{}{}{}\n\n{Understand the tradeoffs between integer\n  FIXME?     not covered by book nor notes currently \n, fixed-point,    \nand floating-point representations for numbers.}\n\n{Understand logical completeness and be able to prove or disprove\nlogical completeness for sets of logic functions.}\n\n PARTIALLY MOVED TO PART 4\n\n {Understand the properties necessary in a representation, and understand\n basic tradeoffs in the sparsity of code words with error detection and\n correction capabilities.}\n{Understand the properties necessary in a representation: no ambiguity\nin meaning for any bit pattern, and agreement in advance on the meanings of \nall bit patterns.}\n\n{Analyze a simple, single-function C program and be able to explain its purpose.}\n\n\n\n\n\n\n\nYou should recognize all of these terms\nand be able to explain what they mean.\n(the parentheses give page numbers,\nor ``P&P'' for Patt & Patel).\n\nActually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  (You may skip the *'d terms in Fall 2012.)\n\nNote that we are not saying that you should, for example, be able to \nwrite down the ASCII representation from memory.  In that example, \nknowing that it is a {7-bit} representation used for English\ntext is sufficient.  You can always look up the detailed definition \nin practice.\n\n[t]\n{}{{}{}\n{}{}{}\n\n{universal computational devices /  computing machines ()\n{--}{{}{}\n{}{}{}\n undecidable ()\n the halting problem ()\n\n}\n\n pre-Fall 2015 version\n\n {Turing machines\n {--}{{}{}\n {}{}{}\n  universal computational device/ computing machine\n  intractable/undecidable\n  the halting problem\n \n }\n\n{information storage in computers\n{--}{{}{}\n{}{}{}\n bits ()\n representation (P&P)\n data type ()\n unsigned representation ()\n 2's complement representation\n\n FIXME?  not covered by book nor notes currently\n  fixed-point representation\n\n IEEE floating-point representation\n ASCII representation\n equivalence classes\n\n}\n\n{operations on bits\n{--}{{}{}\n{}{}{}\n 1's complement operation\n carry (from addition)\n overflow (on any operation) ()\n Boolean logic and algebra\n logic functions/gates\n truth table\n AND/conjunction\n OR/disjunction\n NOT/logical complement/ (logical) negation/inverter\n XOR\n logical completeness\n minterm\n\n}\n\n{mathematical terms\n{--}{{}{}\n{}{}{}\n modular arithmetic\n implication\n contrapositive\n proof approaches: by construction, by contradiction, by induction\n without loss of generality (w.l.o.g.)\n\n}\n\n MOVED TO PART 4\n\n {error detection and correction\n {--}{{}{}\n {}{}{}\n  code/sparse representation\n  code word\n  bit error\n  odd/even parity bit\n  Hamming distance between code words\n  Hamming distance of a code\n  Hamming code\n  SEC-DED\n \n }\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{high-level language concepts\n{--}{{}{}\n{}{}{}\n syntax\n{variables\n{--}{{}{}\n{}{}{}\n declaration\n primitive data types\n symbolic name/identifier\n initialization\n\n}\n FIXME?  really not necessary for them\n strongly typed languages\n expression\n statement\n\n}\n\n{C operators\n{--}{{}{}\n{}{}{}\n operands\n arithmetic\n bitwise\n comparison/relational\n assignment\n address\n arithmetic shift\n logical shift\n precedence\n\n}\n\n{functions in C\n{--}{{}{}\n{}{}{}\n { main}\n function call\n arguments\n {{ printf} and { scanf}\n{--}{{}{}\n{}{}{}\n format string\n escape character\n\n}\n { sizeof} (built-in)\n\n}\n\n{transforming tasks into programs\n{--}{{}{}\n{}{}{}\n flow chart\n sequential construct\n conditional construct\n iterative construct/iteration/loop\n loop body\n\n}\n\n{C statements\n{--}{{}{}\n{}{}{}\n statement: null, simple, compound\n { if} statement\n { for} loop\n { return} statement\n\n}\n\n THESE ARE NOT REQUIRED TOPICS\n\n {execution of C programs\n {--}{{}{}\n {}{}{}\n  compiler/interpreter\n  source code\n  header files\n  assembly code\n  instructions\n  executable image\n \n }\n \n {the C preprocessor\n {--}{{}{}\n {}{}{}\n  #include directive\n  #define directive\n \n }\n\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n\n \n Question: \nQ. What is the meaning of the term \"universal computational device\"? \n Answer: \n\nA. A universal computational device is a device that can perform any computable task."}, {"text": "Title: Overflow Conditions \n Text: {Overflow Conditions}\n\nThis set of notes discusses the overflow conditions for unsigned and\n2's complement addition.  For both types, we formally prove that\nthe conditions that we state are correct.  Many of our faculty want our\nstudents to learn to construct formal proofs, so we plan to begin\nexposing you to this process in our classes.\n\nProf. Lumetta is a fan of Prof. George Polya's educational theories\nwith regard to proof techniques, and in particular the idea that one\nbuilds up a repertoire of approaches by seeing the approaches used \nin practice.\n\n , but teaching proof\n techniques effectively is challenging, particularly when exercises\n are of the form, ``Prove that <insert true theorem> is true'' rather\n than the more open-ended form that we typically encounter in research,\n ``Prove that <insert some conjecture> is true, or find a \n counterexample.''\n\n \n Question: \nQ. What is the overflow condition for unsigned addition? \n Answer: \n\nA. An unsigned overflow occurs when the result of an unsigned addition operation is too large to be represented within the allotted space."}, {"text": "Title: Implication and Mathematical Notation \n Text: {Implication and Mathematical Notation}\n\nSome of you may not have been exposed to basics of mathematical logic, so\nlet's start with a brief introduction to implication.  We'll use \nvariables p and q to represent statements that can be either true\nor false.  For example, p might represent the statement, ``Jan is\nan ECE student,'' while q might represent the statement, ``Jan\nworks hard.''  The { logical complement} or { negation} of \na statement p,\nwritten for example as ``not p,'' has the opposite truth value:\nif p is true, not p is false, and if p is false, not p is\ntrue.\n\nAn { implication} is a logical relationship between two statements.\nThe implication itself is also a logical statement, and may be true or\nfalse.  In English, for example, we might say, ``If p, q.''\nIn mathematics, the same implication is usually written as either \n``q if p'' or ``p,'' and the latter is read \nas, ``p implies q.''  \n\nUsing our example values for p and q, we can see that\np is true: ``Jan is an ECE student'' does\nin fact imply that ``Jan works hard!''\n\nThe implication p \nis only considered false if p is true and q is false.\nIn all other cases, the implication is true.\nThis definition can be a little confusing at first, so let's use\nanother example to see why.\n\nLet  p represent the statement\n``Entity X is a flying pig,'' and let q represent \nthe statement, ``Entity X obeys air traffic control regulations.''\n\nHere the implication p is again true: \nflying pigs do not exist, so p is false, and thus \n``p'' is true---for any value of statement q!\n\nGiven an implication ``p,'' we say that the {\nconverse} of the implication is the statement \n``q,'' which is also an implication.\nIn mathematics, the converse of \np\nis sometimes written\nas ``q only if p.''  The converse of an implication may or may not have\nthe same truth value as the implication itself.  Finally,\nwe frequently use the shorthand notation, ``p if and only if q,''\n(or, even shorter, ``p iff q'') to mean \n``p { and}\nq.'' This last statement is true only when both\nimplications are true.\n\n \n Question: \nQ. Why is the converse of an implication not always true? \n Answer: \n\nThe reason that the converse of an implication is not always true is that the implication may be false."}, {"text": "Title: Overflow for Unsigned Addition \n Text: {Overflow for Unsigned Addition}\n\nLet's say that we add two {N-bit} unsigned numbers, A\nand B.  The {N-bit} unsigned representation \ncan represent integers in the range [0,2^N-1].\n\nRecall that we say that the addition operation has \noverflowed if the number represented by the {N-bit} pattern\nproduced for the sum does not actually represent the number A+B.\n\nFor clarity, let's name the bits of A by writing the number\nas a_{N-1}a_{N-2}...a_1a_0.  Similarly, let's write B as\nb_{N-1}b_{N-2}...b_1b_0.  Name the sum C=A+B.  The sum that\ncomes out of the add unit has only N bits, but recall that\nwe claimed in class that the overflow condition for unsigned \naddition is given by the { carry} out of the most significant\nbit.  So let's write the sum as \nc_c_{N-1}c_{N-2}...c_1c_0, realizing that c_N is the\ncarry out and not actually part of the sum produced by the \nadd unit.\n\n{ Theorem:}\n\nAddition of two {N-bit} unsigned numbers\nA=a_{N-1}a_{N-2}...a_1a_0\nand\nB=b_{N-1}b_{N-2}...b_1b_0\nto produce sum\nC=A+B=c_c_{N-1}c_{N-2}...c_1c_0,\noverflows if and only if\nthe carry out c_N of the addition is a 1 bit.\n\n\n\n{ Proof:}\n\nLet's start with the ``if'' direction.  In other words, c_N=1 implies\noverflow.  Recall that unsigned addition is the same as base 2 addition,\nexcept that we discard bits beyond c_{N-1} from the sum C.\nThe bit c_N has place value 2^N, so, when c_N=1 we can write that \nthe correct sum C{2^N}.  But no value that large can be represented\nusing the {N-bit} unsigned representation, so we have an overflow.\n\nThe other direction (``only if'') is slightly more complex: we need to\nshow that overflow implies that c_N=1.  We use a range-based argument\nfor this purpose.  Overflow means that the sum C is outside the\nrepresentable range [0,2^N-1].  Adding two non-negative numbers cannot\nproduce a negative number, so the sum can't be smaller than 0.  Overflow \nthus implies that C{2^N}.\n\nDoes that argument complete the proof?  No, because some numbers, such \nas 2^{N+1}, are larger than 2^N, but do not have a 1 bit in the Nth\nposition when written in binary.  We need to make use of the constraints\non A and B implied by the possible range of the representation.\n\nIn particular, given that A and B are represented as {N-bit}\nunsigned values, we can write\n\n{eqnarray*}\n0  & A &  2^N - 1\n0  & B &  2^N - 1\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n0  & C &  2^{N + 1} - 2\n{eqnarray*}\n\nCombining the new inequality with the one implied by the overflow \ncondition, we obtain\n\n{eqnarray*}\n2^N  & C &  2^{N + 1} - 2\n{eqnarray*}\n\nAll of the numbers in the range allowed by this inequality have c_N=1,\ncompleting our proof.\n\n \n Question: \nQ. -Why is it important to know when an addition operation has overflowed? \n Answer: \n\nA. Knowing when an addition operation has overflowed is important in order to avoid errors in computation. If an addition operation overflows, the result will not be the correct sum of the two numbers being added."}, {"text": "Title: Overflow for 2's Complement Addition \n Text: {Overflow for 2's Complement Addition}\n\nUnderstanding overflow for 2's complement addition is somewhat trickier,\nwhich is why the problem is a good one for you to think about on your\nown first.\n\nOur operands, A and B, are now two {N-bit} 2's complement numbers.\nThe {N-bit} 2's complement representation \ncan represent integers in the range [-2^{N-1},2^{N-1}-1].\n\nLet's start by ruling out a case that we can show never leads to overflow.\n\n{ Lemma:} \n\nAddition of two {N-bit} 2's complement numbers A and B\ndoes not overflow if one of the numbers is negative and the other is\nnot.\n\n{ Proof:}\n\nWe again make use of the constraints implied by the fact that A and B\nare represented as {N-bit} 2's complement values.  We can assume\n{ without loss of generality}{This common mathematical phrasing\nmeans that we are using a problem symmetry to cut down the length of the\nproof discussion.  In this case, the names A and B aren't particularly\nimportant, since addition is commutative (A+B=B+A).  Thus the proof\nfor the case in which A is negative (and B is not) is identical to the\ncase in which B is negative (and A is not), except that all of the \nnames are swapped.  The term ``without loss of generality'' means that\nwe consider the proof complete even with additional assumptions, in\nour case that A<0 and B.}, or { w.l.o.g.}, \nthat A<0 and B.\n\nCombining these constraints with the range representable \nby {N-bit} 2's complement, we obtain\n\n{eqnarray*}\n-2^{N-1}  & A & < 0\n0  & B & < 2^{N-1}\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n-2^{N-1}  & C & < 2^{N-1}\n{eqnarray*}\n\nBut anything in the range specified by this inequality can be represented\nwith {N-bit} 2's complement, and thus the addition does not overflow.\n\n\nWe are now ready to state our main theorem.  For convenience, \nlet's use different names for the actual sum C=A+B and the sum S\nreturned from the add unit.  We define S as the number represented by\nthe bit pattern produced by the add unit.  When overflow \noccurs, S=C, but we always have (S=C)  2^N.\n\n{ Theorem:} \n\nAddition of two {N-bit} 2's complement numbers A and B\noverflows if and only if one of the following conditions holds:\n\n{A<0 and B<0 and S}\n{A and B and S<0}\n\n\n{ Proof:}\n\nWe once again start with the ``if'' direction.  That is, if condition 1 \nor condition 2 holds, we have an overflow.  The proofs are straightforward.\nGiven condition 1, we can add the two inequalities A<0 and B<0 to \nobtain C=A+B<0.  But S, so clearly S=C, thus overflow \nhas occurred.\n\nSimilarly, if condition 2 holds, we can add the inequalities A\nand B to obtain C=A+B.  Here we have S<0, so again\nS=C, and we have an overflow.\n\nWe must now prove the ``only if'' direction, showing that any overflow\nimplies either condition 1 or condition 2.  By the \n{ contrapositive}{If we have a statement of the form\n(p implies q), its contrapositive is the \nstatement (not q implies not p).\nBoth statements have the same truth value.  In this case, we can turn\nour Lemma around as stated.} of our\nLemma, we know that if an overflow occurs, either both operands are \nnegative, or they are both positive.\n\n\n\n\n\nLet's start with the case in which both operands are negative, so A<0\nand B<0, and thus the real sum C<0 as well.  Given that A and B\nare represented as {N-bit} 2's complement, they must fall in\nthe representable range, so we can write\n\n{eqnarray*}\n-2^{N-1}  & A & < 0\n-2^{N-1}  & B & < 0\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n-2^N  & C & < 0\n{eqnarray*}\n\nGiven that an overflow has occurred, C must fall outside of the \nrepresentable range.  Given that C<0, it cannot be larger than the\nlargest possible number representable using {N-bit} 2's\ncomplement, so we can write\n\n{eqnarray*}\n-2^N  & C & < -2^{N-1}\n{eqnarray*}\n\nWe now add 2^N to each part to obtain\n\n{eqnarray*}\n0  & C + 2^N & < 2^{N-1}\n{eqnarray*}\n\nThis range of integers falls within the representable range \nfor {N-bit} 2's complement, so we can replace the middle\nexpression with S (equal to C modulo 2^N) to find that\n\n{eqnarray*}\n0  & S & < 2^{N-1}\n{eqnarray*}\n\nThus, if we have an overflow and both A<0 and B<0, the resulting\nsum S, and condition 1 holds.\n\nThe proof for the case in which we observe an overflow when \nboth operands are non-negative (A and B)\nis similar, and leads to condition 2.  We again begin with\ninequalities for A and B:\n\n{eqnarray*}\n0  & A & < 2^{N-1}\n0  & B & < 2^{N-1}\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n0  & C < & 2^N\n{eqnarray*}\n\nGiven that an overflow has occurred, C must fall outside of the \nrepresentable range.  Given that C{}0, it cannot be smaller than the\nsmallest possible number representable using {N-bit} 2's\ncomplement, so we can write\n\n{eqnarray*}\n2^{N-1}  & C & < 2^N\n{eqnarray*}\n\nWe now subtract 2^N to each part to obtain\n\n{eqnarray*}\n-2^{N-1}  & C - 2^N & < 0\n{eqnarray*}\n\nThis range of integers falls within the representable range \nfor {N-bit} 2's complement, so we can replace the middle\nexpression with S (equal to C modulo 2^N) to find that\n\n{eqnarray*}\n-2^{N-1}  & S & < 0\n{eqnarray*}\n\nThus, if we have an overflow and both A and B, the resulting\nsum S<0, and condition 2 holds.\n\nThus overflow implies either condition 1 or condition 2, completing our\nproof.\n\n\n\n\n\n \n Question: \nQ. If A and B are both positive and S is negative, does this imply an overflow has occurred? \n Answer: \n\nA. Yes, it does imply an overflow has occurred."}], "gpt-3_paragraphs": [{"text": "Title: The Halting Problem P0 \n Text: For some of the topics in this course, we plan to cover the material\nmore deeply than does the textbook.  We will provide notes in this\nformat to supplement the textbook for this purpose. \n Question: \nQ. How does the author feel about the material in this course?\nQ. \nQ. The author seems to be very excited about the material in this course and feels that it is very important. \n Answer: "}, {"text": "Title: The Halting Problem P1 \n Text: In order to make these notes more useful as a reference, definitions are\nhighlighted with boldface, and italicization emphasizes pitfalls or other\nimportant points. \n Question: \nQ. What is the difference between a pitfall and an important point? \n Answer: \n\nA. A pitfall is a potential problem or danger that could occur, while an important point is something that is noteworthy or deserves attention."}, {"text": "Title: The Halting Problem P2 \n Text: { Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.} \n Question: \nQ. \nQ. The author seems to be fairly neutral on the subject. \n Answer: \n\nA. False. The author is biased against the subject."}, {"text": "Title: The Halting Problem P3 \n Text: These notes are broken up into four parts, corresponding to the three\nmidterm exams and the final exam.  Each part is covered by\none examination in our class.  { The last section of each of the four\nparts gives you\na summary of material that you are expected to know for the corresponding\nexam.}  Feel\nfree to read it in advance. \n Question: \nQ. What is the average GPA of the students in the class?\nQ. \nQ. What is the class size? \n Answer: \nQ. \nQ. How many students are in the class?\nQ. \n\nThe average GPA of the students in the class is 3.5. The class size is 30. There are 25 students in the class."}, {"text": "Title: The Halting Problem P4 \n Text: As discussed in the textbook and in class, a { universal\ncomputational device} (or { computing machine}) is a device \nthat is capable of computing the\nsolution to any problem that can be computed, provided that the device\nis given enough storage and time for the computation to finish.   \n Question: \nQ. What is a universal computational device? \n Answer: \n\nA. A universal computational device is a device that is capable of computing the solution to any problem that can be computed, provided that the device is given enough storage and time for the computation to finish."}, {"text": "Title: The Halting Problem P5 \n Text: One might ask whether we can describe problems that we cannot answer (other \nthan philosophical ones, such as the meaning of life). \n Question: \nQ. What is the meaning of life? \n Answer: \n\nA. We cannot answer this question other than through philosophical means."}, {"text": "Title: The Halting Problem P6 \n Text: The answer is yes: there are problems that are provably { undecidable},\nfor which no amount of computation can solve the problem in general.\nThis set of notes describes the first problem known to be\nundecidable, the { halting problem}.  For our class, you need only recognize\nthe name and realize that one can, in fact, give examples of problems\nthat cannot be solved by computation.  In the future, you should be able\nto recognize this type of problem so as to avoid spending your time\ntrying to solve it. \n Question: \nQ. What is an example of a problem that is undecidable? \n Answer: \n\nA. One example of an undecidable problem is known as the halting problem. This is a problem for which no amount of computation can solve the problem in general."}, {"text": "Title: Universal Computing Machines* P0 \n Text: The things that we call computers today, whether we are talking about\na programmable microcontroller in a microwave oven or the Blue Waters\nsupercomputer sitting on the south end of our campus (the United\nStates' main resource to support computational science research), are\nall equivalent in the sense of what problems they can solve.   \n Question: \nQ. How does the author feel about the material in this course?\nQ. \nQ. The author seems to be very excited about the material in this course and feels that it is very important. \n Answer: "}, {"text": "Title: Universal Computing Machines* P1 \n Text: These machines do, of course, have access to different amounts of memory, \nand compute at different speeds.   \n Question: \nQ. What is the difference between a pitfall and an important point? \n Answer: \n\nA. A pitfall is a potential problem or danger that could occur, while an important point is something that is noteworthy or deserves attention."}, {"text": "Title: Universal Computing Machines* P2 \n Text: The idea that a single model of computation could be described and proven\nto be equivalent to all other models came out of a 1936 paper by Alan Turing, \nand today we generally refer to these devices as { Turing machines}. \n Question: \nQ. \nQ. The author seems to be fairly neutral on the subject. \n Answer: \n\nA. False. The author is biased against the subject."}, {"text": "Title: Universal Computing Machines* P3 \n Text: All computers mentioned earlier, as well as all computers with which you \nare familiar in your daily life, are provably equivalent to Turing machines. \n Question: \nQ. What is the average GPA of the students in the class?\nQ. \nQ. What is the class size? \n Answer: \nQ. \nQ. How many students are in the class?\nQ. \n\nThe average GPA of the students in the class is 3.5. The class size is 30. There are 25 students in the class."}, {"text": "Title: Universal Computing Machines* P4 \n Text: Turing also conjectured that his definition of computable was\nidentical to the ``natural'' definition (today, this claim is known\nas the { Church-Turing conjecture}).  In other words, a problem that\ncannot be solved by a Turing machine cannot be solved in any\nsystematic manner, with any machine, or by any person.  This conjecture\nremains unproven!  However, neither has anyone been able to disprove\nthe conjecture, and it is widely believed to be true.  Disproving the\nconjecture requires that one demonstrate a systematic technique (or a\nmachine) capable of solving a problem that cannot be solved by a\nTuring machine.  No one has been able to do so to date. \n Question: \nQ. What is a universal computational device? \n Answer: \n\nA. A universal computational device is a device that is capable of computing the solution to any problem that can be computed, provided that the device is given enough storage and time for the computation to finish."}, {"text": "Title: The Halting Problem* P0 \n Text: You might reasonably ask whether any problems can be shown to\nbe incomputable.  More common terms for such problems---those known\nto be insolvable by any computer---are { intractable} or \nundecidable. \n Question: \nQ. How does the author feel about the material in this course?\nQ. \nQ. The author seems to be very excited about the material in this course and feels that it is very important. \n Answer: "}, {"text": "Title: The Halting Problem* P1 \n Text: In the same 1936 paper in which he introduced the universal computing\nmachine, Alan Turing also provided an answer to this question\nby introducing (and proving) that there are in fact problems that cannot be\ncomputed by a universal computing machine.\nThe problem that\nhe proved undecidable, using proof techniques almost identical to those\ndeveloped for similar problems in the 1880s, is now known as { the\nhalting problem}. \n Question: \nQ. What is the difference between a pitfall and an important point? \n Answer: \n\nA. A pitfall is a potential problem or danger that could occur, while an important point is something that is noteworthy or deserves attention."}, {"text": "Title: The Halting Problem* P2 \n Text: The halting problem is easy to state and easy to prove undecidable.\nThe problem is this: given a Turing machine and an input to the Turing\nmachine, does the Turing machine finish computing in a finite number\nof steps (a finite amount of time)?  In order to solve the problem, an\nanswer, either yes or no, must be given in a finite amount of time\nregardless of the machine or input in question.  Clearly some machines\nnever finish.  For example, we can write a Turing machine that counts\nupwards starting from one. \n Question: \nQ. \nQ. The author seems to be fairly neutral on the subject. \n Answer: \n\nA. False. The author is biased against the subject."}, {"text": "Title: The Halting Problem* P3 \n Text: You may find the proof structure for undecidability of the halting problem\neasier to understand if\nyou first think about a related problem with which you may\nalready be familiar, the Liar's paradox\n(which is at least 2,300 years old).  In its stengthened form, it is\nthe following sentence: ``This sentence is not true.'' \n Question: \nQ. What is the average GPA of the students in the class?\nQ. \nQ. What is the class size? \n Answer: \nQ. \nQ. How many students are in the class?\nQ. \n\nThe average GPA of the students in the class is 3.5. The class size is 30. There are 25 students in the class."}, {"text": "Title: The Halting Problem* P4 \n Text: \nTo see that no Turing machine can solve the halting problem, we begin\nby assuming that such a machine exists, and then show that its\nexistence is self-contradictory.  We call the machine the ``Halting\nMachine,'' or HM for short.  HM is a machine that operates on \nanother \n Question: \nQ. What is a universal computational device? \n Answer: \n\nA. A universal computational device is a device that is capable of computing the solution to any problem that can be computed, provided that the device is given enough storage and time for the computation to finish."}, {"text": "Title: The Halting Problem* P5 \n Text: Turing machine and its inputs to produce a yes or no answer in finite time:\neither the machine in question finishes in finite time (HM returns\n``yes''), or it does not (HM returns ``no'').  The figure illustrates\nHM's operation. \n Question: \nQ. What is the meaning of life? \n Answer: \n\nA. We cannot answer this question other than through philosophical means."}, {"text": "Title: The Halting Problem* P6 \n Text: \nFrom HM, we construct a second machine that we call the HM Inverter,\nor HMI.  This machine inverts the sense of the answer given by HM.  In\nparticular, the inputs are fed directly into a copy of HM, and if HM\nanswers ``yes,'' HMI enters an infinite loop.  If HM answers ``no,'' HMI\nhalts.  A diagram appears to the right. \n Question: \nQ. What is an example of a problem that is undecidable? \n Answer: \n\nA. One example of an undecidable problem is known as the halting problem. This is a problem for which no amount of computation can solve the problem in general."}, {"text": "Title: The Halting Problem* P7 \n Text: The inconsistency can now be seen by asking HM whether HMI halts when\ngiven itself as an input (repeatedly), as \n Question: \nQ. How are computers able to solve problems? \n Answer: \n\nA. Computers are able to solve problems by using algorithms."}, {"text": "Title: The Halting Problem* P8 \n Text: shown below.  Two\ncopies of HM are thus\nbeing asked the same question.  One copy is the rightmost in the figure below\nand the second is embedded in the HMI machine that we are using as the\ninput to the rightmost HM.  As the two copies of HM operate on the same input\n(HMI operating on HMI), they should return the same answer: a Turing\nmachine either halts on an input, or it does not; they are\ndeterministic. \n Question: \nQ. How much memory do these machines have? \n Answer: \n\nA. These machines have different amounts of memory."}, {"text": "Title: The Halting Problem* P9 \n Text: Let's assume that the rightmost HM tells us that HMI operating on itself halts.\nThen the copy of HM in HMI (when HMI executes on itself, with itself\nas an input) must also say ``yes.''  But this answer implies that HMI\ndoesn't halt (see the figure above), so the answer should have been\nno! \n Question: \nQ. How does the idea that a single model of computation could be described and proven to be equivalent to all other models relate to Alan Turing's 1936 paper? \n Answer: \n\nA. In Alan Turing's 1936 paper, he argues that a single model of computation can be described and proven to be equivalent to all other models."}, {"text": "Title: The Halting Problem* P10 \n Text: Alternatively, we can assume that the rightmost HM says that HMI operating on itself\ndoes not halt.  Again, the copy of HM in HMI must give the same\nanswer.  But in this case HMI halts, again contradicting our\nassumption. \n Question: \nQ. How are Turing machines different from other computers? \n Answer: \n\nA. Turing machines are different from other computers in that they are provably equivalent to Turing machines."}, {"text": "Title: The Halting Problem* P11 \n Text: Since neither answer is consistent, no consistent answer can be given,\nand the original assumption that HM exists is incorrect.  Thus, no\nTuring machine can solve the halting problem. \n Question: \nQ. How does the Church-Turing conjecture relate to the definition of a computable problem? \n Answer: \n\nA. The Church-Turing conjecture states that a problem that cannot be solved by a Turing machine cannot be solved in any systematic manner, with any machine, or by any person."}, {"text": "Title: The 2's Complement Representation P0 \n Text: This set of notes explains the rationale for using the 2's complement\nrepresentation for signed integers and derives the representation \nbased on equivalence of the addition function to that of addition\nusing the unsigned representation with the same number of bits. \n Question: \nQ. What is the advantage of using the 2's complement representation for signed integers? \n Answer: \n\nA. The 2's complement representation for signed integers is advantageous because it allows the addition function to be represented in the same way as addition using the unsigned representation with the same number of bits."}, {"text": "Title: Review of Bits and the Unsigned Representation P0 \n Text: In modern digital systems, we represent all types of information\nusing binary digits, or { bits}.  Logically, a bit is either 0 or 1.\nPhysically, a bit may be a voltage, a magnetic field, or even the\nelectrical resistance of a tiny sliver of glass. \n Question: \nQ. What is the advantage of using the 2's complement representation for signed integers? \n Answer: \n\nA. The 2's complement representation for signed integers is advantageous because it allows the addition function to be represented in the same way as addition using the unsigned representation with the same number of bits."}, {"text": "Title: Review of Bits and the Unsigned Representation P1 \n Text: Any type of information can be represented with an ordered set of\nbits, provided that \n{ any given pattern of bits corresponds to only\none value} and that { we agree in advance on which pattern of bits\nrepresents which value}. \n Question: \nQ. What is the difference between a bit and a byte?\nQ. \nQ. A bit is a single digit (either a 0 or a 1), while a byte is a group of eight bits. \n Answer: "}, {"text": "Title: Review of Bits and the Unsigned Representation P2 \n Text: For unsigned integers---that is, whole numbers greater or equal to\nzero---we chose to use the base 2 representation already familiar to\nus from mathematics.  We call this representation the { unsigned\nrepresentation}.  For example, in a {4-bit} unsigned representation,\nwe write the number 0 as 0000, the number 5 as 0101, and the number 12\nas 1100.  Note that we always write the same number of bits for any\npattern in the representation: { in a digital system, there is no \n``blank'' bit value}. \n Question: \nQ. How is information represented with an ordered set of bits? \n Answer: \n\nA. Information is represented with an ordered set of bits by assigning a specific pattern of bits to represent a specific value."}, {"text": "Title: Picking a Good Representation P0 \n Text: In class, we discussed the question of what makes one representation\nbetter than another.  The value of the unsigned representation, for\nexample, is in part our existing familiarity with the base 2 analogues\nof arithmetic.  For base 2 arithmetic, we can use nearly identical\ntechniques to those that we learned in elementary school for adding, \nsubtracting, multiplying, and dividing base 10 numbers. \n Question: \nQ. What is the advantage of using the 2's complement representation for signed integers? \n Answer: \n\nA. The 2's complement representation for signed integers is advantageous because it allows the addition function to be represented in the same way as addition using the unsigned representation with the same number of bits."}, {"text": "Title: Picking a Good Representation P1 \n Text: Reasoning about the relative merits of representations from a\npractical engineering perspective is (probably) currently beyond \nyour ability. \n Question: \nQ. What is the difference between a bit and a byte?\nQ. \nQ. A bit is a single digit (either a 0 or a 1), while a byte is a group of eight bits. \n Answer: "}, {"text": "Title: Picking a Good Representation P2 \n Text: Saving energy, making the implementation simple, and allowing the\nimplementation to execute quickly probably all sound attractive, but \na quantitative comparison between two representations on any of these\nbases requires knowledge that you will acquire in the\nnext few years. \n Question: \nQ. How is information represented with an ordered set of bits? \n Answer: \n\nA. Information is represented with an ordered set of bits by assigning a specific pattern of bits to represent a specific value."}, {"text": "Title: Picking a Good Representation P3 \n Text: We can sidestep such questions, however, by realizing that if a\ndigital system has hardware to perform operations such as addition\non unsigned values, using the same piece of hardware to operate\non other representations incurs little or no additional cost.\nIn this set of notes, we discuss the 2's complement representation,\nwhich allows reuse of the unsigned add unit (as well as a basis for\nperforming subtraction of either representation using an add unit!).\nIn discussion section and in your homework, you will \nuse the same idea to perform operations on other representations, such as\nchanging an upper case letter in ASCII to a lower case one, or converting\nfrom an ASCII digit to an unsigned representation of the same number.   \n Question: \nQ. What is the range of numbers that can be represented in a 4-bit unsigned representation? \n Answer: \n\nA. The range of numbers that can be represented in a 4-bit unsigned representation is 0-15."}, {"text": "Title: The Unsigned Add Unit P0 \n Text: In order to define a representation for signed integers that allows\nus to reuse a piece of hardware designed for unsigned integers, we\nmust first understand what such a piece of hardware actually does (we\ndo not need to know how it works yet---we'll explore that question \nlater in our class). \n Question: \nQ. What is the advantage of using the 2's complement representation for signed integers? \n Answer: \n\nA. The 2's complement representation for signed integers is advantageous because it allows the addition function to be represented in the same way as addition using the unsigned representation with the same number of bits."}, {"text": "Title: The Unsigned Add Unit P1 \n Text: The unsigned representation using {N} bits is not closed\nunder addition.  In other words, for any value of N, we can easily\nfind two {N-bit} unsigned numbers that, when added together,\ncannot be represented as an {N-bit} unsigned number.  With N=4, \nfor example, we can add 12 (1100) and 6 (0110) to obtain 18.\nSince 18 is outside of the range [0,2^4-1] representable using\nthe {4-bit} unsigned representation, our representation breaks\nif we try to represent the sum using this representation.  We call\nthis failure an { overflow} condition: the representation cannot\nrepresent the result of the operation, in this case addition. \n Question: \nQ. What is the difference between a bit and a byte?\nQ. \nQ. A bit is a single digit (either a 0 or a 1), while a byte is a group of eight bits. \n Answer: "}, {"text": "Title: The Unsigned Add Unit P2 \n Text: \nUsing more bits to represent the answer is not an attractive solution, \nsince we might then want to use more bits for the inputs, which in turn\nrequires more bits for the outputs, and so on.  We cannot build \nsomething supporting an infinite number of bits.  Instead, we \nchoose a value for N and build an add unit that adds two {N-bit}\nnumbers and produces an {N-bit} sum (and some overflow \nindicators, which we discuss in the next set of notes).  The diagram\nto the right shows how we might draw such a device, with two {N-bit}\nnumbers entering at from the top, and the {N-bit} sum coming out\nfrom the bottom. \n Question: \nQ. How is information represented with an ordered set of bits? \n Answer: \n\nA. Information is represented with an ordered set of bits by assigning a specific pattern of bits to represent a specific value."}, {"text": "Title: The Unsigned Add Unit P3 \n Text: \nThe function used for {N-bit} unsigned addition is addition \nmodulo 2^N.  In a practical sense, you can think of this function\nas simply keeping the last N bits of the answer; other bits \nare simply discarded.  In the example to the right,\nwe add 12 and 6 to obtain 18, but then discard the extra bit on the\nleft, so the add unit produces 2 (an overflow). \n Question: \nQ. What is the range of numbers that can be represented in a 4-bit unsigned representation? \n Answer: \n\nA. The range of numbers that can be represented in a 4-bit unsigned representation is 0-15."}, {"text": "Title: The Unsigned Add Unit P4 \n Text: \n{ Modular arithmetic} defines a way of performing arithmetic for\na finite number of possible values, usually integers.  \nAs a concrete example, let's use modulo 16, which corresponds to\nthe addition unit for our {4-bit} examples. \n Question: \nQ. What are the benefits of using the unsigned representation? \n Answer: \n\nA. The benefits of using the unsigned representation are that it is familiar to us from base 2 arithmetic, and that it is easy to add, subtract, multiply, and divide unsigned numbers."}, {"text": "Title: The Unsigned Add Unit P5 \n Text: Starting with the full range of integers, we break the number\nline into contiguous groups of 16 integers, as shown to the right. \n Question: \nQ. What is the practical engineering perspective? \n Answer: \n\nA. The practical engineering perspective is that representations should be chosen based on their ability to solve engineering problems."}, {"text": "Title: The Unsigned Add Unit P6 \n Text: The numbers 0 to 15 form one group.  The numbers -16 to -1 form a\nsecond group, and the numbers from 16 to 31 form a third group. \nAn infinite number of groups are defined in this manner. \n Question: \nQ. What are some ways to save energy? \n Answer: \n\nA.\n\nThere are several ways to save energy, including:\n\n-Improving energy efficiency\n-Using renewable energy sources\n-Reducing energy demand\n-Improving energy storage"}, {"text": "Title: The Unsigned Add Unit P7 \n Text: We then define 16 { equivalence classes} consisting of the first numbers\nfrom all groups, the second numbers from all groups, and so forth.\nFor example, the numbers , -32, -16, 0, 16, 32,  form\none such equivalence class. \n Question: \nQ. Why is the 2's complement representation useful for digital systems? \n Answer: \n\nA. 2's complement representation is useful for digital systems because it allows reuse of the unsigned add unit (as well as a basis for performing subtraction of either representation using an add unit!)."}, {"text": "Title: The Unsigned Add Unit P8 \n Text: Mathematically, we say that two numbers A and B are equivalent modulo 16,\nwhich we write as \n Question: \nQ. What is the significance of understanding what a piece of hardware designed for unsigned integers actually does? \n Answer: \n\nA. Understanding what a piece of hardware designed for unsigned integers actually does is necessary in order to define a representation for signed integers that allows us to reuse the hardware."}, {"text": "Title: The Unsigned Add Unit P9 \n Text: {eqnarray*}\n(A &=& B)  16, {or sometimes as}\nA && B {(mod 16)}\n{eqnarray*} \n Question: \nQ. What is the range of values that can be represented using an N-bit unsigned number? \n Answer: \n\nA. [0, 2^N - 1]"}, {"text": "Title: The Unsigned Add Unit P10 \n Text: if and only if A=B+16k for some integer k. \n Question: \nQ. What is the advantage of using more bits to represent the answer? \n Answer: \n\nA. There is no advantage to using more bits to represent the answer. In fact, it can be disadvantageous because it requires more bits for the inputs, which in turn requires more bits for the outputs."}, {"text": "Title: The Unsigned Add Unit P11 \n Text: Equivalence as defined by a particular modulus\ndistributes over addition and multiplication.  If, for example,\nwe want to find the equivalence class for (A + B)  16,\nwe can find the equivalence classes for A (call it C) and B \n(call it D) and then calculate the equivalence class \nof (C + D)  16.\nAs a concrete example of distribution over multiplication, \ngiven (A = 1,083,102,112  7,323,127)  10,\nfind A. \n Question: \nQ. What is the function used for N-bit unsigned addition?\nQ. \nQ. Addition modulo 2^N \n Answer: ."}, {"text": "Title: The Unsigned Add Unit P12 \n Text: For this problem, we note that the first number is equivalent \nto 2  10, while the second number is equivalent \nto 7  10.  We then write (A = 2  7)  10,\nand, since 2  7 = 14, we have (A = 4)  10. \n Question: \nQ. Why is modular arithmetic useful? \n Answer: \n\nA. Modular arithmetic is useful because it is a way of performing arithmetic for a finite number of possible values, usually integers."}, {"text": "Title: Deriving 2's Complement P0 \n Text: \nGiven these equivalence classes, we might instead choose to draw a circle\nto identify the equivalence classes and to associate each class with one\nof the sixteen possible {4-bit} patterns, as shown to the right.\nUsing this circle representation, we can add by counting clockwise around\nthe circle, and we can subtract by counting in a counterclockwise direction\naround the circle.  With an unsigned representation, we choose to use the\ngroup from [0,15] (the middle group in the diagram markings to the right)\nas the number represented by each of the patterns.  Overflow occurs\nwith unsigned addition (or subtraction) because we can only choose one\nvalue for each binary pattern. \n Question: \nQ. What is the advantage of using the 2's complement representation for signed integers? \n Answer: \n\nA. The 2's complement representation for signed integers is advantageous because it allows the addition function to be represented in the same way as addition using the unsigned representation with the same number of bits."}, {"text": "Title: Deriving 2's Complement P1 \n Text: In fact, we can choose any single value for each pattern to create a \nrepresentation, and our add unit will always produce results that\nare correct modulo 16.  Look back at our overflow example, where\nwe added 12 and 6 to obtain 2, and notice that (2=18)  16.\nNormally, only a contiguous sequence of integers makes a useful\nrepresentation, but we do not have to restrict ourselves to \nnon-negative numbers. \n Question: \nQ. What is the difference between a bit and a byte?\nQ. \nQ. A bit is a single digit (either a 0 or a 1), while a byte is a group of eight bits. \n Answer: "}, {"text": "Title: Deriving 2's Complement P2 \n Text: The 2's complement representation can then be defined by choosing a \nset of integers balanced around zero from the groups.  In the circle \ndiagram, for example, we might choose to represent numbers\nin the range [-7,7] when using 4 bits.  What about the last pattern, 1000?\nWe could choose to represent either -8 or 8.  The number of arithmetic\noperations that overflow is the same with both choices (the choices\nare symmetric around 0, as are the combinations of input operands that \noverflow), so we gain nothing in that sense from either choice.\nIf we choose to represent -8, however, notice that all patterns starting\nwith a 1 bit then represent negative numbers.  No such simple check\narises with the opposite choice, and thus an {N-bit} 2's complement \nrepresentation is defined to represent the range [-2^{N-1},2^{N-1}-1],\nwith patterns chosen as shown in the circle. \n Question: \nQ. How is information represented with an ordered set of bits? \n Answer: \n\nA. Information is represented with an ordered set of bits by assigning a specific pattern of bits to represent a specific value."}, {"text": "Title: An Algebraic Approach P0 \n Text: Some people prefer an algebraic approach to understanding the\ndefinition of 2's complement, so we present such an approach next.\nLet's start by writing f(A,B) for the result of our add unit: \n Question: \nQ. What is the advantage of using the 2's complement representation for signed integers? \n Answer: \n\nA. The 2's complement representation for signed integers is advantageous because it allows the addition function to be represented in the same way as addition using the unsigned representation with the same number of bits."}, {"text": "Title: An Algebraic Approach P1 \n Text: {eqnarray*}\nf(A,B) = (A + B)  2^N\n{eqnarray*} \n Question: \nQ. What is the difference between a bit and a byte?\nQ. \nQ. A bit is a single digit (either a 0 or a 1), while a byte is a group of eight bits. \n Answer: "}, {"text": "Title: An Algebraic Approach P2 \n Text: We assume that we want to represent a set of integers balanced around 0\nusing our signed representation, and that we will use the same binary\npatterns as we do with an unsigned representation to represent\nnon-negative numbers.  Thus, with an {N-bit} representation,\nthe patterns in the range [0,2^{N-1}-1] are the same as those\nused with an unsigned representation.  In this case, we are left with\nall patterns beginning with a 1 bit. \n Question: \nQ. How is information represented with an ordered set of bits? \n Answer: \n\nA. Information is represented with an ordered set of bits by assigning a specific pattern of bits to represent a specific value."}, {"text": "Title: An Algebraic Approach P3 \n Text: The question then is this: given an integer k, 2^{N-1}>k>0, for which we \nwant to find a pattern to represent -k, and any integer m\nthat we might want to add to -k, \ncan we find another integer p>0\nsuch that  \n Question: \nQ. What is the range of numbers that can be represented in a 4-bit unsigned representation? \n Answer: \n\nA. The range of numbers that can be represented in a 4-bit unsigned representation is 0-15."}, {"text": "Title: An Algebraic Approach P4 \n Text: \n(-k + m = p + m)  2^N   ? \n Question: \nQ. What are the benefits of using the unsigned representation? \n Answer: \n\nA. The benefits of using the unsigned representation are that it is familiar to us from base 2 arithmetic, and that it is easy to add, subtract, multiply, and divide unsigned numbers."}, {"text": "Title: An Algebraic Approach P5 \n Text: \nIf we can, we can use p's representation to represent -k and our\nunsigned addition unit f(A,B) will work correctly. \n Question: \nQ. What is the practical engineering perspective? \n Answer: \n\nA. The practical engineering perspective is that representations should be chosen based on their ability to solve engineering problems."}, {"text": "Title: An Algebraic Approach P6 \n Text: To find the value p, start by subtracting m from both sides of\nEquation () to obtain: \n Question: \nQ. What are some ways to save energy? \n Answer: \n\nA.\n\nThere are several ways to save energy, including:\n\n-Improving energy efficiency\n-Using renewable energy sources\n-Reducing energy demand\n-Improving energy storage"}, {"text": "Title: An Algebraic Approach P7 \n Text: \nNote that (2^N=0)  2^N, and add this equation to \nEquation () to obtain \n Question: \nQ. Why is the 2's complement representation useful for digital systems? \n Answer: \n\nA. 2's complement representation is useful for digital systems because it allows reuse of the unsigned add unit (as well as a basis for performing subtraction of either representation using an add unit!)."}, {"text": "Title: An Algebraic Approach P8 \n Text: {eqnarray*}\n(2^N-k = p)  2^N\n{eqnarray*} \n Question: \nQ. What is the significance of understanding what a piece of hardware designed for unsigned integers actually does? \n Answer: \n\nA. Understanding what a piece of hardware designed for unsigned integers actually does is necessary in order to define a representation for signed integers that allows us to reuse the hardware."}, {"text": "Title: An Algebraic Approach P9 \n Text: For example, if N=4, k=3 gives p=16-3=13, which is the pattern 1101.\nWith N=4 and k=5, we obtain p=16-5=11, which is the pattern 1011.\nIn general, since\n2^{N-1}>k>0, \nwe have 2^{N-1}<p<2^N.  But these patterns are all unused---they all\nstart with a 1 bit!---so the patterns that we have defined for negative\nnumbers are disjoint from those that we used for positive numbers, and\nthe meaning of each pattern is unambiguous. \n Question: \nQ. What is the range of values that can be represented using an N-bit unsigned number? \n Answer: \n\nA. [0, 2^N - 1]"}, {"text": "Title: An Algebraic Approach P10 \n Text: The algebraic definition of bit patterns for negative numbers\nalso matches our circle diagram from the last\nsection exactly, of course. \n Question: \nQ. What is the advantage of using more bits to represent the answer? \n Answer: \n\nA. There is no advantage to using more bits to represent the answer. In fact, it can be disadvantageous because it requires more bits for the inputs, which in turn requires more bits for the outputs."}, {"text": "Title: Negating 2's Complement Numbers P0 \n Text: The algebraic approach makes understanding negation of an integer\nrepresented using 2's complement fairly straightforward, and gives \nus an easy procedure for doing so.\nRecall that given an integer k in an {N-bit} 2's complement\nrepresentation, the {N-bit} pattern for -k is given by 2^N-k \n(also true for k=0 if we keep only the low N bits of the result).  \nBut 2^N=(2^N-1)+1.  Note that 2^N-1 is the pattern of\nall 1 bits.  Subtracting any value k from this value is equivalent\nto simply flipping the bits, changing 0s to 1s and 1s to 0s.\n(This operation is called a { 1's complement}, by the way.)\nWe then add 1 to the result to find the pattern for -k. \n Question: \nQ. What is the advantage of using the 2's complement representation for signed integers? \n Answer: \n\nA. The 2's complement representation for signed integers is advantageous because it allows the addition function to be represented in the same way as addition using the unsigned representation with the same number of bits."}, {"text": "Title: Negating 2's Complement Numbers P1 \n Text: Negation can overflow, of course.  Try finding the negative pattern for -8 \nin {4-bit} 2's complement. \n Question: \nQ. What is the difference between a bit and a byte?\nQ. \nQ. A bit is a single digit (either a 0 or a 1), while a byte is a group of eight bits. \n Answer: "}, {"text": "Title: Negating 2's Complement Numbers P2 \n Text: Finally, be aware that people often overload the term 2's complement\nand use it to refer to the operation of negation in a 2's complement\nrepresentation.  In our class, we try avoid this confusion: 2's complement\nis a representation for signed integers, and negation is an operation\nthat one can apply to a signed integer (whether the representation used\nfor the integer is 2's complement or some other representation for signed\nintegers). \n Question: \nQ. How is information represented with an ordered set of bits? \n Answer: \n\nA. Information is represented with an ordered set of bits by assigning a specific pattern of bits to represent a specific value."}, {"text": "Title: Overflow Conditions P0 \n Text: This set of notes discusses the overflow conditions for unsigned and\n2's complement addition.  For both types, we formally prove that\nthe conditions that we state are correct.  Many of our faculty want our\nstudents to learn to construct formal proofs, so we plan to begin\nexposing you to this process in our classes. \n Question: \nQ. Why is it important to learn to construct formal proofs? \n Answer: \n\nFormal proofs are important in order to be able to show that a certain condition is correct. In many cases, such as with addition of unsigned and 2's complement numbers, the conditions that we state are correct but can be difficult to show without a formal proof. By learning to construct formal proofs, we can more easily show that our conditions are indeed correct."}, {"text": "Title: Overflow Conditions P1 \n Text: Prof. Lumetta is a fan of Prof. George Polya's educational theories\nwith regard to proof techniques, and in particular the idea that one\nbuilds up a repertoire of approaches by seeing the approaches used \nin practice. \n Question: \nQ. How does Prof. Lumetta feel about Prof. George Polya's educational theories? \n Answer: \n\nA. Prof. Lumetta is a fan of Prof. George Polya's educational theories with regard to proof techniques, and in particular the idea that one builds up a repertoire of approaches by seeing the approaches used in practice."}, {"text": "Title: Overflow Conditions P2 \n Text:  , but teaching proof\n techniques effectively is challenging, particularly when exercises\n are of the form, ``Prove that <insert true theorem> is true'' rather\n than the more open-ended form that we typically encounter in research,\n ``Prove that <insert some conjecture> is true, or find a \n counterexample.'' \n Question: \nQ. What makes teaching proof techniques effectively challenging? \n Answer: \n\nA. Teaching proof techniques effectively is challenging because it is difficult to provide exercises that are both open-ended and focused on a particular proof technique. It is also difficult to find exercises that are of the form \"Prove that <insert conjecture> is true, or find a counterexample.\""}, {"text": "Title: Implication and Mathematical Notation P0 \n Text: Some of you may not have been exposed to basics of mathematical logic, so\nlet's start with a brief introduction to implication.  We'll use \nvariables p and q to represent statements that can be either true\nor false.  For example, p might represent the statement, ``Jan is\nan ECE student,'' while q might represent the statement, ``Jan\nworks hard.''  The { logical complement} or { negation} of \na statement p,\nwritten for example as ``not p,'' has the opposite truth value:\nif p is true, not p is false, and if p is false, not p is\ntrue. \n Question: \nQ. Why is it important to learn to construct formal proofs? \n Answer: \n\nFormal proofs are important in order to be able to show that a certain condition is correct. In many cases, such as with addition of unsigned and 2's complement numbers, the conditions that we state are correct but can be difficult to show without a formal proof. By learning to construct formal proofs, we can more easily show that our conditions are indeed correct."}, {"text": "Title: Implication and Mathematical Notation P1 \n Text: An { implication} is a logical relationship between two statements.\nThe implication itself is also a logical statement, and may be true or\nfalse.  In English, for example, we might say, ``If p, q.''\nIn mathematics, the same implication is usually written as either \n``q if p'' or ``p,'' and the latter is read \nas, ``p implies q.''   \n Question: \nQ. How does Prof. Lumetta feel about Prof. George Polya's educational theories? \n Answer: \n\nA. Prof. Lumetta is a fan of Prof. George Polya's educational theories with regard to proof techniques, and in particular the idea that one builds up a repertoire of approaches by seeing the approaches used in practice."}, {"text": "Title: Implication and Mathematical Notation P2 \n Text: Using our example values for p and q, we can see that\np is true: ``Jan is an ECE student'' does\nin fact imply that ``Jan works hard!'' \n Question: \nQ. What makes teaching proof techniques effectively challenging? \n Answer: \n\nA. Teaching proof techniques effectively is challenging because it is difficult to provide exercises that are both open-ended and focused on a particular proof technique. It is also difficult to find exercises that are of the form \"Prove that <insert conjecture> is true, or find a counterexample.\""}, {"text": "Title: Implication and Mathematical Notation P3 \n Text: The implication p \nis only considered false if p is true and q is false.\nIn all other cases, the implication is true.\nThis definition can be a little confusing at first, so let's use\nanother example to see why. \n Question: \nQ. What is the logical complement of a statement? \n Answer: \n\nA. The logical complement of a statement is a statement with the opposite truth value."}, {"text": "Title: Implication and Mathematical Notation P4 \n Text: Let  p represent the statement\n``Entity X is a flying pig,'' and let q represent \nthe statement, ``Entity X obeys air traffic control regulations.'' \n Question: \nQ. What is the difference between an implication and a logical statement? \n Answer: \n\nA. The implication is a logical statement that can be true or false, while a logical statement is a statement that is either true or false."}, {"text": "Title: Implication and Mathematical Notation P5 \n Text: Here the implication p is again true: \nflying pigs do not exist, so p is false, and thus \n``p'' is true---for any value of statement q! \n Question: \nQ. What does the text say about Jan?\nQ. \nQ. The text says that Jan is an ECE student and works hard. \n Answer: "}, {"text": "Title: Implication and Mathematical Notation P6 \n Text: Given an implication ``p,'' we say that the {\nconverse} of the implication is the statement \n``q,'' which is also an implication.\nIn mathematics, the converse of \np\nis sometimes written\nas ``q only if p.''  The converse of an implication may or may not have\nthe same truth value as the implication itself.  Finally,\nwe frequently use the shorthand notation, ``p if and only if q,''\n(or, even shorter, ``p iff q'') to mean \n``p { and}\nq.'' This last statement is true only when both\nimplications are true. \n Question: \nQ. What does the definition of implication mean?\nQ. \nQ. The definition of implication means that if p is true, then q is also true. \n Answer: "}, {"text": "Title: Overflow for Unsigned Addition P0 \n Text: Let's say that we add two {N-bit} unsigned numbers, A\nand B.  The {N-bit} unsigned representation \ncan represent integers in the range [0,2^N-1]. \n Question: \nQ. Why is it important to learn to construct formal proofs? \n Answer: \n\nFormal proofs are important in order to be able to show that a certain condition is correct. In many cases, such as with addition of unsigned and 2's complement numbers, the conditions that we state are correct but can be difficult to show without a formal proof. By learning to construct formal proofs, we can more easily show that our conditions are indeed correct."}, {"text": "Title: Overflow for Unsigned Addition P1 \n Text: Recall that we say that the addition operation has \noverflowed if the number represented by the {N-bit} pattern\nproduced for the sum does not actually represent the number A+B. \n Question: \nQ. How does Prof. Lumetta feel about Prof. George Polya's educational theories? \n Answer: \n\nA. Prof. Lumetta is a fan of Prof. George Polya's educational theories with regard to proof techniques, and in particular the idea that one builds up a repertoire of approaches by seeing the approaches used in practice."}, {"text": "Title: Overflow for Unsigned Addition P2 \n Text: For clarity, let's name the bits of A by writing the number\nas a_{N-1}a_{N-2}...a_1a_0.  Similarly, let's write B as\nb_{N-1}b_{N-2}...b_1b_0.  Name the sum C=A+B.  The sum that\ncomes out of the add unit has only N bits, but recall that\nwe claimed in class that the overflow condition for unsigned \naddition is given by the { carry} out of the most significant\nbit.  So let's write the sum as \nc_c_{N-1}c_{N-2}...c_1c_0, realizing that c_N is the\ncarry out and not actually part of the sum produced by the \nadd unit. \n Question: \nQ. What makes teaching proof techniques effectively challenging? \n Answer: \n\nA. Teaching proof techniques effectively is challenging because it is difficult to provide exercises that are both open-ended and focused on a particular proof technique. It is also difficult to find exercises that are of the form \"Prove that <insert conjecture> is true, or find a counterexample.\""}, {"text": "Title: Overflow for Unsigned Addition P3 \n Text: Addition of two {N-bit} unsigned numbers\nA=a_{N-1}a_{N-2}...a_1a_0\nand\nB=b_{N-1}b_{N-2}...b_1b_0\nto produce sum\nC=A+B=c_c_{N-1}c_{N-2}...c_1c_0,\noverflows if and only if\nthe carry out c_N of the addition is a 1 bit. \n Question: \nQ. What is the logical complement of a statement? \n Answer: \n\nA. The logical complement of a statement is a statement with the opposite truth value."}, {"text": "Title: Overflow for Unsigned Addition P4 \n Text: Let's start with the ``if'' direction.  In other words, c_N=1 implies\noverflow.  Recall that unsigned addition is the same as base 2 addition,\nexcept that we discard bits beyond c_{N-1} from the sum C.\nThe bit c_N has place value 2^N, so, when c_N=1 we can write that \nthe correct sum C{2^N}.  But no value that large can be represented\nusing the {N-bit} unsigned representation, so we have an overflow. \n Question: \nQ. What is the difference between an implication and a logical statement? \n Answer: \n\nA. The implication is a logical statement that can be true or false, while a logical statement is a statement that is either true or false."}, {"text": "Title: Overflow for Unsigned Addition P5 \n Text: The other direction (``only if'') is slightly more complex: we need to\nshow that overflow implies that c_N=1.  We use a range-based argument\nfor this purpose.  Overflow means that the sum C is outside the\nrepresentable range [0,2^N-1].  Adding two non-negative numbers cannot\nproduce a negative number, so the sum can't be smaller than 0.  Overflow \nthus implies that C{2^N}. \n Question: \nQ. What does the text say about Jan?\nQ. \nQ. The text says that Jan is an ECE student and works hard. \n Answer: "}, {"text": "Title: Overflow for Unsigned Addition P6 \n Text: Does that argument complete the proof?  No, because some numbers, such \nas 2^{N+1}, are larger than 2^N, but do not have a 1 bit in the Nth\nposition when written in binary.  We need to make use of the constraints\non A and B implied by the possible range of the representation. \n Question: \nQ. What does the definition of implication mean?\nQ. \nQ. The definition of implication means that if p is true, then q is also true. \n Answer: "}, {"text": "Title: Overflow for Unsigned Addition P7 \n Text: In particular, given that A and B are represented as {N-bit}\nunsigned values, we can write \n Question: \nQ. If entity X is a flying pig, does entity X obey air traffic control regulations? \n Answer: \n\nA. No"}, {"text": "Title: Overflow for Unsigned Addition P8 \n Text: {eqnarray*}\n0  & A &  2^N - 1\n0  & B &  2^N - 1\n{eqnarray*} \n Question: \nQ. Why does the author say that the implication p is again true? \n Answer: \n\nA. The author says that the implication p is again true because flying pigs do not exist."}, {"text": "Title: Overflow for Unsigned Addition P9 \n Text: We add these two inequalities and replace A+B with C to obtain \n Question: \nQ. What is the converse of an implication? \n Answer: \n\nA. The converse of an implication is the statement q, which is also an implication."}, {"text": "Title: Overflow for Unsigned Addition P10 \n Text: {eqnarray*}\n0  & C &  2^{N + 1} - 2\n{eqnarray*} \n Question: \nQ. What is the range of the sum of two N-bit unsigned numbers?\nQ. \nQ. The range of the sum of two N-bit unsigned numbers is [0,2^N-1]. \n Answer: "}, {"text": "Title: Overflow for Unsigned Addition P11 \n Text: Combining the new inequality with the one implied by the overflow \ncondition, we obtain \n Question: \nQ. What is the difference between a number represented by a {N-bit} pattern and the number A+B? \n Answer: \n\nA. The difference is that the number represented by the {N-bit} pattern is not actually the number A+B."}, {"text": "Title: Overflow for Unsigned Addition P12 \n Text: {eqnarray*}\n2^N  & C &  2^{N + 1} - 2\n{eqnarray*} \n Question: \nQ. What is the meaning of the term \"carry out\"? \n Answer: \n\nA. The carry out of an adder is a single bit that represents the overflow condition for an unsigned addition."}, {"text": "Title: Overflow for Unsigned Addition P13 \n Text: All of the numbers in the range allowed by this inequality have c_N=1,\ncompleting our proof. \n Question: \nQ. What is the maximum value that A and B can have without causing an overflow? \n Answer: \n\nA. A and B can have a maximum value of 2^{N-1}-1 without causing an overflow."}, {"text": "Title: Overflow for 2's Complement Addition P0 \n Text: Understanding overflow for 2's complement addition is somewhat trickier,\nwhich is why the problem is a good one for you to think about on your\nown first. \n Question: \nQ. Why is it important to learn to construct formal proofs? \n Answer: \n\nFormal proofs are important in order to be able to show that a certain condition is correct. In many cases, such as with addition of unsigned and 2's complement numbers, the conditions that we state are correct but can be difficult to show without a formal proof. By learning to construct formal proofs, we can more easily show that our conditions are indeed correct."}, {"text": "Title: Overflow for 2's Complement Addition P1 \n Text: Our operands, A and B, are now two {N-bit} 2's complement numbers.\nThe {N-bit} 2's complement representation \ncan represent integers in the range [-2^{N-1},2^{N-1}-1]. \n Question: \nQ. How does Prof. Lumetta feel about Prof. George Polya's educational theories? \n Answer: \n\nA. Prof. Lumetta is a fan of Prof. George Polya's educational theories with regard to proof techniques, and in particular the idea that one builds up a repertoire of approaches by seeing the approaches used in practice."}, {"text": "Title: Overflow for 2's Complement Addition P2 \n Text: Let's start by ruling out a case that we can show never leads to overflow. \n Question: \nQ. What makes teaching proof techniques effectively challenging? \n Answer: \n\nA. Teaching proof techniques effectively is challenging because it is difficult to provide exercises that are both open-ended and focused on a particular proof technique. It is also difficult to find exercises that are of the form \"Prove that <insert conjecture> is true, or find a counterexample.\""}, {"text": "Title: Overflow for 2's Complement Addition P3 \n Text: Addition of two {N-bit} 2's complement numbers A and B\ndoes not overflow if one of the numbers is negative and the other is\nnot. \n Question: \nQ. What is the logical complement of a statement? \n Answer: \n\nA. The logical complement of a statement is a statement with the opposite truth value."}, {"text": "Title: Overflow for 2's Complement Addition P4 \n Text: We again make use of the constraints implied by the fact that A and B\nare represented as {N-bit} 2's complement values.  We can assume\n{ without loss of generality}{This common mathematical phrasing\nmeans that we are using a problem symmetry to cut down the length of the\nproof discussion.  In this case, the names A and B aren't particularly\nimportant, since addition is commutative (A+B=B+A).  Thus the proof\nfor the case in which A is negative (and B is not) is identical to the\ncase in which B is negative (and A is not), except that all of the \nnames are swapped.  The term ``without loss of generality'' means that\nwe consider the proof complete even with additional assumptions, in\nour case that A<0 and B.}, or { w.l.o.g.}, \nthat A<0 and B. \n Question: \nQ. What is the difference between an implication and a logical statement? \n Answer: \n\nA. The implication is a logical statement that can be true or false, while a logical statement is a statement that is either true or false."}, {"text": "Title: Overflow for 2's Complement Addition P5 \n Text: Combining these constraints with the range representable \nby {N-bit} 2's complement, we obtain \n Question: \nQ. What does the text say about Jan?\nQ. \nQ. The text says that Jan is an ECE student and works hard. \n Answer: "}, {"text": "Title: Overflow for 2's Complement Addition P6 \n Text: {eqnarray*}\n-2^{N-1}  & A & < 0\n0  & B & < 2^{N-1}\n{eqnarray*} \n Question: \nQ. What does the definition of implication mean?\nQ. \nQ. The definition of implication means that if p is true, then q is also true. \n Answer: "}, {"text": "Title: Overflow for 2's Complement Addition P7 \n Text: We add these two inequalities and replace A+B with C to obtain \n Question: \nQ. If entity X is a flying pig, does entity X obey air traffic control regulations? \n Answer: \n\nA. No"}, {"text": "Title: Overflow for 2's Complement Addition P8 \n Text: {eqnarray*}\n-2^{N-1}  & C & < 2^{N-1}\n{eqnarray*} \n Question: \nQ. Why does the author say that the implication p is again true? \n Answer: \n\nA. The author says that the implication p is again true because flying pigs do not exist."}, {"text": "Title: Overflow for 2's Complement Addition P9 \n Text: But anything in the range specified by this inequality can be represented\nwith {N-bit} 2's complement, and thus the addition does not overflow. \n Question: \nQ. What is the converse of an implication? \n Answer: \n\nA. The converse of an implication is the statement q, which is also an implication."}, {"text": "Title: Overflow for 2's Complement Addition P10 \n Text: \nWe are now ready to state our main theorem.  For convenience, \nlet's use different names for the actual sum C=A+B and the sum S\nreturned from the add unit.  We define S as the number represented by\nthe bit pattern produced by the add unit.  When overflow \noccurs, S=C, but we always have (S=C)  2^N. \n Question: \nQ. What is the range of the sum of two N-bit unsigned numbers?\nQ. \nQ. The range of the sum of two N-bit unsigned numbers is [0,2^N-1]. \n Answer: "}, {"text": "Title: Overflow for 2's Complement Addition P11 \n Text: Addition of two {N-bit} 2's complement numbers A and B\noverflows if and only if one of the following conditions holds: \n Question: \nQ. What is the difference between a number represented by a {N-bit} pattern and the number A+B? \n Answer: \n\nA. The difference is that the number represented by the {N-bit} pattern is not actually the number A+B."}, {"text": "Title: Overflow for 2's Complement Addition P12 \n Text: {A<0 and B<0 and S}\n{A and B and S<0} \n Question: \nQ. What is the meaning of the term \"carry out\"? \n Answer: \n\nA. The carry out of an adder is a single bit that represents the overflow condition for an unsigned addition."}, {"text": "Title: Overflow for 2's Complement Addition P13 \n Text: We once again start with the ``if'' direction.  That is, if condition 1 \nor condition 2 holds, we have an overflow.  The proofs are straightforward.\nGiven condition 1, we can add the two inequalities A<0 and B<0 to \nobtain C=A+B<0.  But S, so clearly S=C, thus overflow \nhas occurred. \n Question: \nQ. What is the maximum value that A and B can have without causing an overflow? \n Answer: \n\nA. A and B can have a maximum value of 2^{N-1}-1 without causing an overflow."}, {"text": "Title: Overflow for 2's Complement Addition P14 \n Text: Similarly, if condition 2 holds, we can add the inequalities A\nand B to obtain C=A+B.  Here we have S<0, so again\nS=C, and we have an overflow. \n Question: \nQ. Why does unsigned addition result in the same value as base 2 addition? \n Answer: \n\nA. Because in both cases, the bit c_N has place value 2^N."}, {"text": "Title: Overflow for 2's Complement Addition P15 \n Text: We must now prove the ``only if'' direction, showing that any overflow\nimplies either condition 1 or condition 2.  By the \n{ contrapositive}{If we have a statement of the form\n(p implies q), its contrapositive is the \nstatement (not q implies not p).\nBoth statements have the same truth value.  In this case, we can turn\nour Lemma around as stated.} of our\nLemma, we know that if an overflow occurs, either both operands are \nnegative, or they are both positive. \n Question: \nQ. What does overflow mean in this context? \n Answer: \n\nA. Overflow means that the sum C is outside the representable range [0,2^N-1]."}, {"text": "Title: Overflow for 2's Complement Addition P16 \n Text: Let's start with the case in which both operands are negative, so A<0\nand B<0, and thus the real sum C<0 as well.  Given that A and B\nare represented as {N-bit} 2's complement, they must fall in\nthe representable range, so we can write \n Question: \nQ. Why does the author mention the number 2^{N+1}? \n Answer: \n\nA. The author is mentioning the number 2^{N+1} to illustrate that some numbers, such as 2^{N+1}, are larger than 2^N, but do not have a 1 bit in the Nth position when written in binary."}, {"text": "Title: Overflow for 2's Complement Addition P17 \n Text: {eqnarray*}\n-2^{N-1}  & A & < 0\n-2^{N-1}  & B & < 0\n{eqnarray*} \n Question: \nQ. What do the curly braces indicate in the text?\nQ. \nQ. The curly braces indicate that A and B are both represented as N-bit unsigned values. \n Answer: "}, {"text": "Title: Overflow for 2's Complement Addition P18 \n Text: We add these two inequalities and replace A+B with C to obtain \n Question: \nQ. How does C relate to A and B?\nQ. \nQ. C is the sum of A and B. \n Answer: "}, {"text": "Title: Overflow for 2's Complement Addition P19 \n Text: {eqnarray*}\n-2^N  & C & < 0\n{eqnarray*} \n Question: \nQ. What does the new inequality combined with the overflow condition imply? \n Answer: \n\nA.\n\nThe new inequality combined with the overflow condition implies that the sum of the two numbers must be less than the larger of the two numbers."}, {"text": "Title: Overflow for 2's Complement Addition P20 \n Text: Given that an overflow has occurred, C must fall outside of the \nrepresentable range.  Given that C<0, it cannot be larger than the\nlargest possible number representable using {N-bit} 2's\ncomplement, so we can write \n Question: \nQ. What does the author mean when they say \"All of the numbers in the range allowed by this inequality have c_N=1\"? \n Answer: \n\nA. All of the numbers in the range allowed by this inequality have a value of 1 for c_N."}, {"text": "Title: Overflow for 2's Complement Addition P21 \n Text: {eqnarray*}\n-2^N  & C & < -2^{N-1}\n{eqnarray*} \n Question: \nQ. What is the significance of the text's mention of \"2's complement addition?\" \n Answer: \n\nA. The significance of the text's mention of \"2's complement addition\" is that it is a way of representing numbers in binary code that is used in computer programming."}, {"text": "Title: Overflow for 2's Complement Addition P22 \n Text: We now add 2^N to each part to obtain \n Question: \nQ. What is the range of numbers that can be represented by a {N-bit} 2's complement number? \n Answer: \n\nA. [-2^{N-1},2^{N-1}-1]"}, {"text": "Title: Overflow for 2's Complement Addition P23 \n Text: {eqnarray*}\n0  & C + 2^N & < 2^{N-1}\n{eqnarray*} \n Question: \nQ. What is the condition that can never lead to overflow?\nQ. \nQ. The condition that can never lead to overflow is when the operands are of the same sign. \n Answer: "}, {"text": "Title: Overflow for 2's Complement Addition P24 \n Text: This range of integers falls within the representable range \nfor {N-bit} 2's complement, so we can replace the middle\nexpression with S (equal to C modulo 2^N) to find that \n Question: \nQ. What does it mean if one of the numbers is negative and the other is not? \n Answer: \n\nA.\n\nThis means that one of the numbers is a negative number in 2's complement form, while the other is a positive number. If these two numbers are added together, the result will not overflow."}, {"text": "Title: Overflow for 2's Complement Addition P25 \n Text: {eqnarray*}\n0  & S & < 2^{N-1}\n{eqnarray*} \n Question: \nQ. What does the phrase \"without loss of generality\" mean in this context? \n Answer: \n\nA. That we can assume A<0 and B without loss of generality."}, {"text": "Title: Overflow for 2's Complement Addition P26 \n Text: Thus, if we have an overflow and both A<0 and B<0, the resulting\nsum S, and condition 1 holds. \n Question: \nQ. What does the author mean by \"the range representable by {N-bit} 2's complement\"? \n Answer: \n\nA. The range of values that can be represented by N-bit 2's complement notation."}, {"text": "Title: Overflow for 2's Complement Addition P27 \n Text: The proof for the case in which we observe an overflow when \nboth operands are non-negative (A and B)\nis similar, and leads to condition 2.  We again begin with\ninequalities for A and B: \n Question: \nQ. What is the difference between A and B? \n Answer: \n\nA. A is less than B."}, {"text": "Title: Overflow for 2's Complement Addition P28 \n Text: {eqnarray*}\n0  & A & < 2^{N-1}\n0  & B & < 2^{N-1}\n{eqnarray*} \n Question: \nQ. But anything in the range specified by this inequality can be represented\nQ. with {N-bit} 2's complement, and thus the addition does not overflow.\nQ. \nQ. What is the range specified by this inequality? \n Answer: \n\nThe range specified by this inequality is the range of values that can be represented with N-bit 2's complement."}, {"text": "Title: Overflow for 2's Complement Addition P29 \n Text: We add these two inequalities and replace A+B with C to obtain \n Question: \nQ. What is the definition of S? \n Answer: \n\nThe definition of S is the number represented by the bit pattern produced by the add unit."}, {"text": "Title: Overflow for 2's Complement Addition P30 \n Text: {eqnarray*}\n0  & C < & 2^N\n{eqnarray*} \n Question: \nQ. What is the minimum number of bits required for the 2's complement numbers A and B such that an overflow can occur?\nQ. \nQ. N-bit \n Answer: , where N is the number of bits required for A and B."}, {"text": "Title: Overflow for 2's Complement Addition P31 \n Text: Given that an overflow has occurred, C must fall outside of the \nrepresentable range.  Given that C{}0, it cannot be smaller than the\nsmallest possible number representable using {N-bit} 2's\ncomplement, so we can write \n Question: \nQ. What is the difference between the two conditions? \n Answer: \n\nA. The first condition requires that A and B are both less than zero, and that S is also less than zero. The second condition requires that A and B are both less than zero, but that S is greater than zero."}, {"text": "Title: Overflow for 2's Complement Addition P32 \n Text: {eqnarray*}\n2^{N-1}  & C & < 2^N\n{eqnarray*} \n Question: \nQ. What is the ``if'' direction? \n Answer: \n\nA. The ``if'' direction is if condition 1 or condition 2 holds, we have an overflow."}, {"text": "Title: Overflow for 2's Complement Addition P33 \n Text: We now subtract 2^N to each part to obtain \n Question: \nQ. What is meant by \"if condition 1 holds, we can add the inequalities A and B to obtain C=A+B\"? \n Answer: \n\n\n\nA. If condition 1 holds, then we can add the inequalities A and B to obtain C=A+B."}, {"text": "Title: Overflow for 2's Complement Addition P34 \n Text: {eqnarray*}\n-2^{N-1}  & C - 2^N & < 0\n{eqnarray*} \n Question: \nQ. What is the contrapositive of the statement \"If an overflow occurs, either both operands are negative, or they are both positive.\"? \n Answer: \n\nA. If an overflow does not occur, either both operands are not negative, or they are not both positive."}, {"text": "Title: Overflow for 2's Complement Addition P35 \n Text: This range of integers falls within the representable range \nfor {N-bit} 2's complement, so we can replace the middle\nexpression with S (equal to C modulo 2^N) to find that \n Question: \nQ. What does 2's complement mean? \n Answer: \n\n2's complement is a way to represent negative numbers in binary."}, {"text": "Title: Overflow for 2's Complement Addition P36 \n Text: {eqnarray*}\n-2^{N-1}  & S & < 0\n{eqnarray*} \n Question: \nQ. What does the author mean by \"A+B\"? \n Answer: \n\nA. The author is referring to the sum of two numbers, A and B."}, {"text": "Title: Overflow for 2's Complement Addition P37 \n Text: Thus, if we have an overflow and both A and B, the resulting\nsum S<0, and condition 2 holds. \n Question: \nQ. What is the largest possible number representable using {N-bit} 2's complement? \n Answer: \n\nA. The largest possible number representable using {N-bit} 2's complement is 2^(N-1)-1."}, {"text": "Title: Overflow for 2's Complement Addition P38 \n Text: Thus overflow implies either condition 1 or condition 2, completing our\nproof. \n Question: \nQ. What is the significance of adding 2^N to each part?\nQ. \nQ. Adding 2^N to each part creates a unique code for each section of text. This code can be used to track changes to the text over time, or to prevent unauthorized changes. \n Answer: "}, {"text": "Title: Logic Operations P0 \n Text:  \n Boolean logic functions, truth tables, etc.\n \n overflow expression using logic functions (and difficulty with\n    binary overflow!)\n \n completeness\n \n implications of completeness: enable abstraction, and evaluate\n     future possible implementations of devices quickly: logically complete\n     or not?\n \n examples\n \n generalizing to operations on sets of bits\n  \n Question: \nQ. What is the implication of completeness? \n Answer: \n\nA. The implication of completeness is that it enables abstraction, and allows for the evaluation of future possible implementations of devices quickly and easily."}, {"text": "Title: Logic Operations P1 \n Text: This set of notes briefly describes a generalization to truth tables,\nthen introduces Boolean logic operations as well as \nnotational conventions and tools that we use to express general\nfunctions on bits.  We illustrate how logic operations enable \nus to express functions such as overflow conditions concisely,\nthen show by construction that a small number of logic operations\nsuffices to describe any operation on any number of bits.  \nWe close by discussing a few implications and examples. \n Question: \nQ. What is the difference between a generalization to truth tables and Boolean logic operations? \n Answer: \n\nA. A generalization to truth tables is a way of representing a function on bits using a table. Boolean logic operations are a way of representing a function on bits using logic operations."}, {"text": "Title: Truth Tables P0 \n Text: You have seen the basic form of truth tables in the textbook and in class.\nOver the semester, we will introduce several\nextensions to the basic concept, mostly with the\ngoal of reducing the amount of writing necessary when using truth\ntables.  For example, the truth table to the right uses two generalizations\nto show the carry out C (also the unsigned overflow indicator) and the \nsum S produced by\nadding two {2-bit} unsigned numbers.\nFirst, rather than writing each input bit separately, we have grouped\npairs of input bits into the numbers A and B.  \nSecond, we have defined multiple \noutput columns so as to include both bits of S as well as C in \nthe same table.\nFinally, we have \ngrouped the two bits of S into one column. \n Question: \nQ. What is the implication of completeness? \n Answer: \n\nA. The implication of completeness is that it enables abstraction, and allows for the evaluation of future possible implementations of devices quickly and easily."}, {"text": "Title: Truth Tables P1 \n Text: Keep in mind as you write truth tables that only rarely does an operation\ncorrespond to a simple and familiar process such as addition of base 2\nnumbers.  We had to choose the unsigned and 2's complement representations\ncarefully to allow ourselves to take advantage of a familiar process.\nIn general, for each line of a truth table for an operation, you may\nneed to make use of the input representation to identify the input values,\ncalculate the operation's result as a value, and then translate the value\nback into the correct bit pattern using the output representation.\nSigned magnitude addition, for example, does not always correspond to\nbase 2 addition: when the \n Question: \nQ. What is the difference between a generalization to truth tables and Boolean logic operations? \n Answer: \n\nA. A generalization to truth tables is a way of representing a function on bits using a table. Boolean logic operations are a way of representing a function on bits using logic operations."}, {"text": "Title: Truth Tables P2 \n Text: \n{cc|cc}\n{c|}& \nA& B& C& S \n00& 00& 0& 00\n00& 01& 0& 01\n00& 10& 0& 10\n00& 11& 0& 11\n01& 00& 0& 01\n01& 01& 0& 10\n01& 10& 0& 11\n01& 11& 1& 00\n10& 00& 0& 10\n10& 01& 0& 11\n10& 10& 1& 00\n10& 11& 1& 01\n11& 00& 0& 11\n11& 01& 1& 00\n11& 10& 1& 01\n11& 11& 1& 10 \n Question: \nQ. Why is it important to reduce the amount of writing necessary when using truth tables? \n Answer: \n\nA. It is important to reduce the amount of writing necessary when using truth tables in order to save time and effort. By grouping input bits into numbers and by defining multiple output columns, the truth table can be written more concisely. This allows for a quicker and easier understanding of the logical relationships between the inputs and outputs."}, {"text": "Title: Truth Tables P3 \n Text: signs of the two input operands differ, \none should instead use base 2 subtraction.  For other operations or\nrepresentations, base 2 arithmetic may have no relevance at all. \n Question: \nQ. Q: What does the author mean when they say \"In general, for each line of a truth table for an operation, you may\nQ. need to make use of the input representation to identify the input values,\nQ. calculate the operation's result as a value, and then translate the value\nQ. back into the correct bit pattern using the output representation.\"? \n Answer: \n\nA. The author is saying that, in general, when you are writing a truth table for an operation, you will need to use the input representation to figure out what the input values are, calculate the result of the operation as a value, and then translate that value back into the correct bit pattern using the output representation."}, {"text": "Title: Boolean Logic Operations P0 \n Text: In the middle of the 19^ century, George Boole introduced a \nset of logic operations that are today known as { Boolean logic}\n(also as { Boolean algebra}).  These operations today form one of\nthe lowest abstraction levels in digital systems, and an understanding\nof their meaning and use is critical to the effective development of \nboth hardware and software. \n Question: \nQ. What is the implication of completeness? \n Answer: \n\nA. The implication of completeness is that it enables abstraction, and allows for the evaluation of future possible implementations of devices quickly and easily."}, {"text": "Title: Boolean Logic Operations P1 \n Text: You have probably seen these functions many times already in your \neducation---perhaps first in set-theoretic form as Venn diagrams.  \nHowever, given the use of common\nEnglish words { with different meanings} to name some of the functions,\nand the sometimes confusing associations made even by engineering\neducators, we want to provide you with a concise set of\ndefinitions that generalizes correctly to more than two operands.\nYou may have learned these functions based on truth values \n(true and false), but we define them based on bits, \nwith 1 representing true and 0 representing false. \n Question: \nQ. What is the difference between a generalization to truth tables and Boolean logic operations? \n Answer: \n\nA. A generalization to truth tables is a way of representing a function on bits using a table. Boolean logic operations are a way of representing a function on bits using logic operations."}, {"text": "Title: Boolean Logic Operations P2 \n Text: Table  on the next page lists logic operations. \n Question: \nQ. Why is it important to reduce the amount of writing necessary when using truth tables? \n Answer: \n\nA. It is important to reduce the amount of writing necessary when using truth tables in order to save time and effort. By grouping input bits into numbers and by defining multiple output columns, the truth table can be written more concisely. This allows for a quicker and easier understanding of the logical relationships between the inputs and outputs."}, {"text": "Title: Boolean Logic Operations P3 \n Text: The first column in the table lists the name of each function.  The second\nprovides a fairly complete set of the notations that you are likely\nto encounter for each function, including both the forms used in\nengineering and those used in mathematics.  The third column defines \nthe function's\nvalue for two or more input operands (except for NOT, which operates on\na single value).  The last column shows the form generally used in\nlogic schematics/diagrams and mentions the important features used\nin distinguishing each function (in pictorial form usually called\na { gate}, in reference to common physical implementations) from the \nothers. \n Question: \nQ. Q: What does the author mean when they say \"In general, for each line of a truth table for an operation, you may\nQ. need to make use of the input representation to identify the input values,\nQ. calculate the operation's result as a value, and then translate the value\nQ. back into the correct bit pattern using the output representation.\"? \n Answer: \n\nA. The author is saying that, in general, when you are writing a truth table for an operation, you will need to use the input representation to figure out what the input values are, calculate the result of the operation as a value, and then translate that value back into the correct bit pattern using the output representation."}, {"text": "Title: Boolean Logic Operations P4 \n Text: \n{\n{|c|c|c|c|}\n{ Function}& { Notation}& { Explanation}& { Schematic} \nAND& {A AND B}{AB}{A}{A}{A}& {the ``all'' function: result is 1 iff}{{ all} input operands are equal to 1}& {flat input, round output}  \nOR& {A OR B}{A+B}{A}& {the ``any'' function: result is 1 iff}{{ any} input operand is equal to 1}& {{-6pt}}{round input, pointed output} \nNOT& {NOT A}{A'}{}{}& {logical complement/negation:}{NOT 0 is 1, and NOT 1 is 0}& {triangle and circle} \n{exclusive OR}& {A XOR B}{A}& {the ``odd'' function: result is 1 iff an { odd}}{number of input operands are equal to 1}& {{-6pt}}{OR with two lines}{on input side} \n{``or''}& A, B, or C& {the ``one of'' function: result is 1 iff exactly}{{ one of} the input operands is equal to 1}& (not used)  \n Question: \nQ. What does the 'S' column represent? \n Answer: \n\nThe 'S' column represents the sum of the values in columns 'A' and 'B'."}, {"text": "Title: Boolean Logic Operations P5 \n Text: }\n{Boolean logic operations, notation, definitions, and symbols.}{-12pt} \n Question: \nQ. What does it mean when the signs of two input operands differ? \n Answer: \n\nA. If the signs of two input operands differ, one should use base 2 subtraction."}, {"text": "Title: Boolean Logic Operations P6 \n Text: The first function of importance is { AND}.  Think of { AND} as the\n``all'' function: given a set of input values as operands, AND evaluates \nto 1 if and only if { all} of the input values are 1.  The first\nnotation line simply uses the name of the function.  In Boolean algebra,\nAND is typically represented as multiplication, and the middle three\nforms reflect various ways in which we write multiplication.  The last\nnotational variant is from mathematics, where the AND function is formally\ncalled { conjunction}. \n Question: \nQ. What are the Boolean logic operations? \n Answer: \n\nA. The Boolean logic operations are the operations of logic that are based on the Boolean algebra. These operations are used to develop digital systems."}, {"text": "Title: Boolean Logic Operations P7 \n Text: The next function of importance is { OR}.  Think of { OR} as the\n``any'' function: given a set of input values as operands, OR evaluates\nto 1 if and only if { any} of the input values is 1.  The actual\nnumber of input values equal to 1 only matters in the sense of whether\nit is at least one.  The notation for OR is organized in the same way\nas for AND, with the function name at the top, the algebraic variant that\nwe will use in class---in this case addition---in the middle, and\nthe mathematics variant, in this case called { disjunction}, at the\nbottom. \n Question: \nQ. What does the author mean by \"common English words with different meanings\"? \n Answer: \n\nA. The author is referring to the fact that some common English words have different meanings when used in the context of Boolean logic. For example, the word \"and\" has a different meaning in Boolean logic than it does in everyday speech."}, {"text": "Title: Boolean Logic Operations P8 \n Text: { The definition of Boolean OR is not the same as our use of \nthe word ``or'' \nin English.}  For example, if you are fortunate enough to enjoy\na meal on a plane, you might be offered several choices: ``Would you like\nthe chicken, the beef, or the vegetarian lasagna today?''  Unacceptable\nanswers to this English question include: ``Yes,'' ``Chicken and lasagna,''\nand any other combination that involves more than a single choice! \n Question: \nQ. What is a logic operation?\nQ. \nQ. A logic operation is a function that takes one or more Boolean values as input and returns a Boolean value as output. The most common logic operations are AND, OR, and NOT. \n Answer: "}, {"text": "Title: Boolean Logic Operations P9 \n Text: You may have noticed that we might have instead mentioned that\nAND evaluates to 0 if any input value is 0, and that OR evaluates to 0\nif all input values are 0.  These relationships reflect a mathematical\nduality underlying Boolean logic that has important practical\nvalue in terms of making it easier for humans to digest complex logic\nexpressions.\nWe will talk more about duality later in the course, but you\nshould learn some of the practical\nvalue now: if you are trying to evaluate an AND function, look for an input\nwith value 0; if you are trying to evaluate an OR function, look for an \ninput with value 1.  If you find such an input, you know the function's\nvalue without calculating any other input values. \n Question: \nQ. What does the NOT function do?\nQ. \nQ. The NOT function is a logical operator that negates a single value. \n Answer: "}, {"text": "Title: Boolean Logic Operations P10 \n Text: We next consider the { logical complement} function, { NOT}.  The NOT\nfunction is also called { negation}.  Unlike our\nfirst two functions, NOT accepts only a single operand, and reverses\nits value, turning 0 into 1 and 1 into 0.  The notation follows the\nsame pattern: a version using the function name at the top, followed\nby two variants used in Boolean algebra, and finally the version\nfrequently used in mathematics.  For the NOT gate, or { inverter},\nthe circle is actually the important part: the triangle by itself\nmerely copies the input.  You will see the small circle added to other\ngates on both inputs and outputs; in both cases the circle implies a NOT\nfunction. \n Question: \nQ. What does the exclusive OR function do? \n Answer: \n\nA. The exclusive OR function returns 1 if an odd number of input operands are equal to 1."}, {"text": "Title: Boolean Logic Operations P11 \n Text: Last among the Boolean logic functions, we have the\n{ XOR}, or { exclusive OR} function.\nThink of XOR as the ``odd'' function: given a set of input values as\noperands, XOR evaluates to 1 if and only if { an odd number} of\nthe input values are 1.  Only two variants of XOR notation are given:\nthe first using the function name, and the second used with Boolean\nalgebra.  Mathematics rarely uses this function. \n Question: \nQ. What is the difference between a conjunction and a disjunction? \n Answer: \n\nA. A conjunction is a boolean operation that returns true if and only if both operands are true. A disjunction is a boolean operation that returns true if either operand is true."}, {"text": "Title: Boolean Logic Operations P12 \n Text: Finally, we have included the meaning of the word ``or'' in English as\na separate function entry to enable you to compare that meaning\nwith the Boolean logic functions easily.  Note that many people refer\nto English' \n Question: \nQ. What is the \"all\" function? \n Answer: \n\nA. The all function is a Boolean function that returns 1 if all of the input values are 1."}, {"text": "Title: Boolean Logic Operations P13 \n Text: \nuse of the word ``or'' as ``exclusive'' because one\ntrue value excludes all others from being true.  Do not let this \nhuman language ambiguity confuse you about XOR!  For all logic design\npurposes, { XOR is the odd function}. \n Question: \nQ. What does it mean when the text says \"the actual number of input values equal to 1 only matters in the sense of whether it is at least one?\" \n Answer: \n\nA. The actual number of input values equal to 1 only matters in the sense of whether it is at least one means that, in order for the OR function to evaluate to 1, at least one of the input values must be 1."}, {"text": "Title: Boolean Logic Operations P14 \n Text: The truth table to the right provides values illustrating these functions operating\non three inputs.  The AND, OR, and XOR functions are all \nassociative---(A  B)  C = A  (B  C)---and\ncommutative---A  B = B  A,\nas you may have already realized from their definitions. \n Question: \nQ. What does the author say about the definition of Boolean OR?\nQ. \nQ. The author does not say anything about the definition of Boolean OR. \n Answer: "}, {"text": "Title: Boolean Logic Operations P15 \n Text: \n{ccc|cccc}\n{c|}& \nA& B& C& ABC& A+B+C& & A \n0& 0& 0& 0& 0& 1& 0\n0& 0& 1& 0& 1& 1& 1\n0& 1& 0& 0& 1& 1& 1\n0& 1& 1& 0& 1& 1& 0\n1& 0& 0& 0& 1& 0& 1\n1& 0& 1& 0& 1& 0& 0\n1& 1& 0& 0& 1& 0& 0\n1& 1& 1& 1& 1& 0& 1 \n Question: \nQ. Why is it easier for humans to digest complex logic expressions when they understand the mathematical duality underlying Boolean logic?\nQ. \nQ. One reason is that this understanding can help people to more easily identify which inputs will determine the output of a particular function. For example, if someone is trying to evaluate an AND function, they can look for an input with a value of 0. If they find such an input, they know that the function will output a 0 without having to calculate any other input values. Similarly, if someone is trying to evaluate an OR function, they can look for an input with a value of 1. If they find such an input, they know that the function will output a 1 without having to calculate any other input values. This understanding can therefore save time and effort in evaluating complex logic expressions. \n Answer: "}, {"text": "Title: Overflow as Logic Expressions P0 \n Text: In the last set of notes, we discussed overflow conditions for unsigned\nand 2's complement representations.  Let's use Boolean logic to express\nthese conditions.   \n Question: \nQ. What is the implication of completeness? \n Answer: \n\nA. The implication of completeness is that it enables abstraction, and allows for the evaluation of future possible implementations of devices quickly and easily."}, {"text": "Title: Overflow as Logic Expressions P1 \n Text: We begin with addition of two {1-bit} unsigned numbers.  \nCall the two input bits A_0 and B_0.  If you write a truth table for\nthis operation, you'll notice that overflow occurs only when all (two)\nbits are 1.  If either bit is 0, the sum can't exceed 1, so overflow\ncannot occur.  In other words, overflow in this case can be written using\nan AND operation:  \n Question: \nQ. What is the difference between a generalization to truth tables and Boolean logic operations? \n Answer: \n\nA. A generalization to truth tables is a way of representing a function on bits using a table. Boolean logic operations are a way of representing a function on bits using logic operations."}, {"text": "Title: Overflow as Logic Expressions P2 \n Text: The truth table for adding two {2-bit} unsigned numbers is four\ntimes as large, and seeing the structure may be difficult.  One way of\nwriting the expression for overflow of {2-bit} unsigned addition is\nas follows: \n Question: \nQ. Why is it important to reduce the amount of writing necessary when using truth tables? \n Answer: \n\nA. It is important to reduce the amount of writing necessary when using truth tables in order to save time and effort. By grouping input bits into numbers and by defining multiple output columns, the truth table can be written more concisely. This allows for a quicker and easier understanding of the logical relationships between the inputs and outputs."}, {"text": "Title: Overflow as Logic Expressions P3 \n Text: {eqnarray*}\nA_1B_1 + (A_1+B_1)A_0B_0\n{eqnarray*} \n Question: \nQ. Q: What does the author mean when they say \"In general, for each line of a truth table for an operation, you may\nQ. need to make use of the input representation to identify the input values,\nQ. calculate the operation's result as a value, and then translate the value\nQ. back into the correct bit pattern using the output representation.\"? \n Answer: \n\nA. The author is saying that, in general, when you are writing a truth table for an operation, you will need to use the input representation to figure out what the input values are, calculate the result of the operation as a value, and then translate that value back into the correct bit pattern using the output representation."}, {"text": "Title: Overflow as Logic Expressions P4 \n Text: This expression is slightly trickier to understand.  Think about the\nplace value of the bits.  If both of the most significant bits---those\nwith place value 2---are 1,\nwe have an overflow, just as in the case of {1-bit} addition.\nThe A_1B_1 term represents this case.  We also have an overflow if\none or both (the OR) of the most significant bits are 1 and the\nsum of the two next significant bits---in this case those with place \nvalue 1---generates a carry. \n Question: \nQ. What does the 'S' column represent? \n Answer: \n\nThe 'S' column represents the sum of the values in columns 'A' and 'B'."}, {"text": "Title: Overflow as Logic Expressions P5 \n Text: The truth table for adding two {3-bit} unsigned numbers is \nprobably not something that you want to write out.  Fortunately,\na pattern should start to become clear with the following expression: \n Question: \nQ. What does it mean when the signs of two input operands differ? \n Answer: \n\nA. If the signs of two input operands differ, one should use base 2 subtraction."}, {"text": "Title: Overflow as Logic Expressions P6 \n Text: {eqnarray*}\nA_2B_2 + (A_2+B_2)A_1B_1 + (A_2+B_2)(A_1+B_1)A_0B_0\n{eqnarray*} \n Question: \nQ. What are the Boolean logic operations? \n Answer: \n\nA. The Boolean logic operations are the operations of logic that are based on the Boolean algebra. These operations are used to develop digital systems."}, {"text": "Title: Overflow as Logic Expressions P7 \n Text: In the {2-bit} case, we mentioned the ``most significant bit''\nand the ``next most significant bit'' to help you see the pattern.\nThe same reasoning describes the first two product terms in our\noverflow expression for {3-bit} unsigned addition (but the place\nvalues are 4 for the most significant bit and 2 for the next most\nsignificant bit).  The last term represents the overflow case in which\nthe two least significant bits generate a carry which then propagates\nup through all of the other bits because at least one of the two bits\nin every position is a 1. \n Question: \nQ. What does the author mean by \"common English words with different meanings\"? \n Answer: \n\nA. The author is referring to the fact that some common English words have different meanings when used in the context of Boolean logic. For example, the word \"and\" has a different meaning in Boolean logic than it does in everyday speech."}, {"text": "Title: Overflow as Logic Expressions P8 \n Text: \nThe overflow condition for addition of two {N-bit} 2's complement\nnumbers can be written fairly concisely in terms of the first bits\nof the two numbers and the first bit of the sum.  Recall that overflow\nin this case depends only on whether the three numbers are negative\nor non-negative, which is given by \n Question: \nQ. What is a logic operation?\nQ. \nQ. A logic operation is a function that takes one or more Boolean values as input and returns a Boolean value as output. The most common logic operations are AND, OR, and NOT. \n Answer: "}, {"text": "Title: Overflow as Logic Expressions P9 \n Text: \nthe most significant bit.  Given\nthe bit names as shown to the right, we can write the overflow condition\nas follows: \n Question: \nQ. What does the NOT function do?\nQ. \nQ. The NOT function is a logical operator that negates a single value. \n Answer: "}, {"text": "Title: Overflow as Logic Expressions P10 \n Text: {eqnarray*}\nA_{N-1} B_{N-1} {S_{N-1}}+\n{A_{N-1}} {B_{N-1}} S_{N-1}\n{eqnarray*} \n Question: \nQ. What does the exclusive OR function do? \n Answer: \n\nA. The exclusive OR function returns 1 if an odd number of input operands are equal to 1."}, {"text": "Title: Overflow as Logic Expressions P11 \n Text: The overflow condition does of course depend on all of the bits in\nthe two numbers being added.  In the expression above, we have simplified\nthe form by using S_{N-1}.  But S_{N-1} depends on the bits A_{N-1}\nand B_{N-1} as well as the carry out of bit (N-2).   \n Question: \nQ. What is the difference between a conjunction and a disjunction? \n Answer: \n\nA. A conjunction is a boolean operation that returns true if and only if both operands are true. A disjunction is a boolean operation that returns true if either operand is true."}, {"text": "Title: Overflow as Logic Expressions P12 \n Text: Later in this set of notes, we present a technique with which you can derive\nan expression for an arbitrary Boolean logic function.  As an exercise,\nafter you have finished reading these notes, try using that technique to\nderive an overflow expression for addition of \ntwo {N-bit} 2's complement numbers based on \nA_{N-1}, B_{N-1}, and the carry out of bit (N-2) (and into \nbit (N-1)), which we might\ncall C_{N-1}.  You might then calculate C_{N-1} in terms of the rest\nof the bits of A and B\nusing the expressions for\nunsigned overflow just discussed. \n Question: \nQ. What is the \"all\" function? \n Answer: \n\nA. The all function is a Boolean function that returns 1 if all of the input values are 1."}, {"text": "Title: Overflow as Logic Expressions P13 \n Text: In the next month or so, you will learn how to derive\nmore compact expressions yourself from truth tables or other representations\nof Boolean logic functions. \n Question: \nQ. What does it mean when the text says \"the actual number of input values equal to 1 only matters in the sense of whether it is at least one?\" \n Answer: \n\nA. The actual number of input values equal to 1 only matters in the sense of whether it is at least one means that, in order for the OR function to evaluate to 1, at least one of the input values must be 1."}, {"text": "Title: Logical Completeness P0 \n Text: Why do we feel that such a short list of functions is enough?  If you \nthink about the\nnumber of possible functions on N bits, you might think that we need\nmany more functions to be able to manipulate bits.  With 10 bits, for\nexample, there are 2^ such functions.  Obviously, some of them\nhave never been used in any computer system, but maybe we should define\nat least a few more logic operations?\nIn fact, we do not\neven need XOR.  The functions AND, OR, and NOT are sufficient, even if\nwe only allow two input operands for AND and OR! \n Question: \nQ. What is the implication of completeness? \n Answer: \n\nA. The implication of completeness is that it enables abstraction, and allows for the evaluation of future possible implementations of devices quickly and easily."}, {"text": "Title: Logical Completeness P1 \n Text: The theorem below captures this idea, called { logical completeness}.\nIn this case, we claim that the set of functions {AND, OR, NOT} is\nsufficient to express any operation on any finite number of variables, \nwhere each variable is a bit. \n Question: \nQ. What is the difference between a generalization to truth tables and Boolean logic operations? \n Answer: \n\nA. A generalization to truth tables is a way of representing a function on bits using a table. Boolean logic operations are a way of representing a function on bits using logic operations."}, {"text": "Title: Logical Completeness P2 \n Text: Given enough {2-input} AND, {2-input} OR, and {1-input}\nNOT functions, one can express any Boolean logic function on any finite \nnumber of variables. \n Question: \nQ. Why is it important to reduce the amount of writing necessary when using truth tables? \n Answer: \n\nA. It is important to reduce the amount of writing necessary when using truth tables in order to save time and effort. By grouping input bits into numbers and by defining multiple output columns, the truth table can be written more concisely. This allows for a quicker and easier understanding of the logical relationships between the inputs and outputs."}, {"text": "Title: Logical Completeness P3 \n Text: The proof of our theorem is { by construction}.  In other words, \nwe show a systematic\napproach for transforming an arbitrary Boolean logic function on an\narbitrary number of variables into a form that uses only AND, OR, and\nNOT functions on one or two operands. \n Question: \nQ. Q: What does the author mean when they say \"In general, for each line of a truth table for an operation, you may\nQ. need to make use of the input representation to identify the input values,\nQ. calculate the operation's result as a value, and then translate the value\nQ. back into the correct bit pattern using the output representation.\"? \n Answer: \n\nA. The author is saying that, in general, when you are writing a truth table for an operation, you will need to use the input representation to figure out what the input values are, calculate the result of the operation as a value, and then translate that value back into the correct bit pattern using the output representation."}, {"text": "Title: Logical Completeness P4 \n Text: As a first step, we remove the restriction on the number of inputs for\nthe AND and OR functions.  For this purpose, we state and prove two\n{ lemmas}, which are simpler theorems used to support the proof of\na main theorem. \n Question: \nQ. What does the 'S' column represent? \n Answer: \n\nThe 'S' column represents the sum of the values in columns 'A' and 'B'."}, {"text": "Title: Logical Completeness P5 \n Text: Given enough {2-input} AND functions, one can express an AND function\non any finite number of variables. \n Question: \nQ. What does it mean when the signs of two input operands differ? \n Answer: \n\nA. If the signs of two input operands differ, one should use base 2 subtraction."}, {"text": "Title: Logical Completeness P6 \n Text: We prove the Lemma { by induction}.{We assume that you have\nseen proof by induction previously.}  Denote the number of inputs to a\nparticular AND function by N. \n Question: \nQ. What are the Boolean logic operations? \n Answer: \n\nA. The Boolean logic operations are the operations of logic that are based on the Boolean algebra. These operations are used to develop digital systems."}, {"text": "Title: Logical Completeness P7 \n Text: \nThe base case is N=2.  Such an AND function is given. \n Question: \nQ. What does the author mean by \"common English words with different meanings\"? \n Answer: \n\nA. The author is referring to the fact that some common English words have different meanings when used in the context of Boolean logic. For example, the word \"and\" has a different meaning in Boolean logic than it does in everyday speech."}, {"text": "Title: Logical Completeness P8 \n Text: To complete the proof, we need only show that, given \nany number of AND functions with up to N inputs, we\ncan express an AND function with N+1 inputs.  To do so, we need\nmerely use one {2-input} AND function to join together\nthe result of an {N-input} AND function with an additional\ninput, as illustrated to the right. \n Question: \nQ. What is a logic operation?\nQ. \nQ. A logic operation is a function that takes one or more Boolean values as input and returns a Boolean value as output. The most common logic operations are AND, OR, and NOT. \n Answer: "}, {"text": "Title: Logical Completeness P9 \n Text: Given enough {2-input} OR functions, one can express an OR function\non any finite number of variables. \n Question: \nQ. What does the NOT function do?\nQ. \nQ. The NOT function is a logical operator that negates a single value. \n Answer: "}, {"text": "Title: Logical Completeness P10 \n Text: The proof of Lemma 2 is identical in structure to that of Lemma 1, but\nuses OR functions instead of AND functions. \n Question: \nQ. What does the exclusive OR function do? \n Answer: \n\nA. The exclusive OR function returns 1 if an odd number of input operands are equal to 1."}, {"text": "Title: Logical Completeness P11 \n Text: \nLet's now consider a small subset of functions on N variables.  \nFor any such function, you can write out the truth table for the\nfunction.  The output of a logic function is just a bit, \neither a 0 or a 1.  Let's consider the set of functions on N\nvariables that produce a 1 for exactly one combination of the N\nvariables.  In other words, if you were to write out the truth\ntable for such a function, exactly one row in the truth table would\nhave output value 1, while all other rows had output value 0. \n Question: \nQ. What is the difference between a conjunction and a disjunction? \n Answer: \n\nA. A conjunction is a boolean operation that returns true if and only if both operands are true. A disjunction is a boolean operation that returns true if either operand is true."}, {"text": "Title: Logical Completeness P12 \n Text: Given enough AND functions and {1-input}\nNOT functions, one can express any Boolean logic function \nthat produces a 1 for exactly one combination of\nany finite number of variables. \n Question: \nQ. What is the \"all\" function? \n Answer: \n\nA. The all function is a Boolean function that returns 1 if all of the input values are 1."}, {"text": "Title: Logical Completeness P13 \n Text: The proof of Lemma 3 is by construction.\nLet N be the number of variables on which the function operates.\nWe construct a { minterm} on these N variables,\nwhich is an AND operation on each variable or its complement.\nThe minterm is specified by looking at the unique combination of \nvariable values that produces a 1 result for the function.\nEach variable that must be a 1 is included as itself, while\neach variable that must be a 0 is included as the variable's complement\n(using a NOT function).  The resulting minterm produces the\ndesired function exactly.  When the variables all match\nthe values for which the function should produce 1, the\ninputs to the AND function are all 1, and the function produces 1.\nWhen any variable does not match the value for which the function\nshould produce 1, that variable (or its complement) acts as a 0\ninput to the AND function, and the function produces a 0, as desired. \n Question: \nQ. What does it mean when the text says \"the actual number of input values equal to 1 only matters in the sense of whether it is at least one?\" \n Answer: \n\nA. The actual number of input values equal to 1 only matters in the sense of whether it is at least one means that, in order for the OR function to evaluate to 1, at least one of the input values must be 1."}, {"text": "Title: Logical Completeness P14 \n Text: The table below shows all eight minterms for three variables. \n Question: \nQ. What does the author say about the definition of Boolean OR?\nQ. \nQ. The author does not say anything about the definition of Boolean OR. \n Answer: "}, {"text": "Title: Logical Completeness P15 \n Text: {\n{ccc|cccccccc}\n{c|}& \nA& B& C& \n  &\n  C&\n B &\n B C&\nA  &\nA  C&\nA B &\nA B C\n \n0& 0& 0& 1& 0& 0& 0& 0& 0& 0& 0\n0& 0& 1& 0& 1& 0& 0& 0& 0& 0& 0\n0& 1& 0& 0& 0& 1& 0& 0& 0& 0& 0\n0& 1& 1& 0& 0& 0& 1& 0& 0& 0& 0\n1& 0& 0& 0& 0& 0& 0& 1& 0& 0& 0\n1& 0& 1& 0& 0& 0& 0& 0& 1& 0& 0\n1& 1& 0& 0& 0& 0& 0& 0& 0& 1& 0\n1& 1& 1& 0& 0& 0& 0& 0& 0& 0& 1 \n Question: \nQ. Why is it easier for humans to digest complex logic expressions when they understand the mathematical duality underlying Boolean logic?\nQ. \nQ. One reason is that this understanding can help people to more easily identify which inputs will determine the output of a particular function. For example, if someone is trying to evaluate an AND function, they can look for an input with a value of 0. If they find such an input, they know that the function will output a 0 without having to calculate any other input values. Similarly, if someone is trying to evaluate an OR function, they can look for an input with a value of 1. If they find such an input, they know that the function will output a 1 without having to calculate any other input values. This understanding can therefore save time and effort in evaluating complex logic expressions. \n Answer: "}, {"text": "Title: Logical Completeness P16 \n Text: We are now ready to prove our theorem. \n Question: \nQ. What does the NOT function do?\nQ. \nQ. The NOT function reverses the value of its operand, turning 0 into 1 and 1 into 0. \n Answer: "}, {"text": "Title: Logical Completeness P17 \n Text: Any given function on N variables\nproduces the value 1 for some set of combinations\nof inputs.  Let's say that M such combinations produce 1.  \nNote that M{2^N}.\nFor each\ncombination that produces 1, we can use\nLemma 1 to construct an {N-input} AND function.\nThen, using Lemma 3, we can use as many as M NOT functions\nand the {N-input} AND function to construct a minterm\nfor that input combination.\nFinally, using Lemma 2, we can construct an {M-input}\nOR function and OR together all of the minterms.\nThe result of the OR is the desired function. If the function\nshould produce a 1 for some combination of inputs, that combination's \nminterm provides a 1 input to the OR, which in turn produces a 1.\nIf a combination should produce a 0, its minterm does not appear in\nthe OR; all other minterms produce 0 for that combination, and thus\nall inputs to the OR are 0 in such cases, and the OR produces 0,\nas desired. \n Question: \nQ. How does the XOR function work? \n Answer: \n\nA. The XOR function evaluates to 1 if and only if an odd number of the input values are 1."}, {"text": "Title: Logical Completeness P18 \n Text: The construction that we used to prove logical completeness does\nnot necessarily help with efficient design of logic functions.  Think\nabout some of the expressions that we discussed earlier in these\nnotes for overflow conditions.  How many minterms do you need \nfor {N-bit} unsigned overflow?  A single Boolean logic function\ncan be expressed in many different ways, and learning how to develop\nan efficient implementation of a function as well as how to determine\nwhether two logic expressions are identical without actually writing out\ntruth tables are important engineering skills that you will start to \nlearn in the coming months. \n Question: \nQ. What does the word \"or\" mean in English? \n Answer: \n\nA. The word \"or\" in English can mean either \"alternative\" or \"inclusive\"."}, {"text": "Title: Implications of Logical Completeness P0 \n Text: If logical completeness doesn't really help us to engineer logic functions,\nwhy is the idea important?  Think back to the layers of abstraction\nand the implementation of bits from the first couple of lectures.  \nVoltages are real numbers, not bits.  { The device layer implementations\nof Boolean logic functions must abstract away the analog properties\nof the physical system.}  Without such abstraction, we must think\ncarefully about analog issues such as noise every time we make use\nof a bit! \n Question: \nQ. What is the implication of completeness? \n Answer: \n\nA. The implication of completeness is that it enables abstraction, and allows for the evaluation of future possible implementations of devices quickly and easily."}, {"text": "Title: Implications of Logical Completeness P1 \n Text: Logical completeness assures us that no matter what we want to do\nwith bits, implementating a handful of operations correctly is\nenough to guarantee that we never have to worry. \n Question: \nQ. What is the difference between a generalization to truth tables and Boolean logic operations? \n Answer: \n\nA. A generalization to truth tables is a way of representing a function on bits using a table. Boolean logic operations are a way of representing a function on bits using logic operations."}, {"text": "Title: Implications of Logical Completeness P2 \n Text: A second important value of logical completeness is as a tool in\nscreening potential new technologies for computers.  If a new technology\ndoes not allow implementation of a logically complete set of functions,\nthe new technology is extremely unlikely\nto be successful in replacing the current one. \n Question: \nQ. Why is it important to reduce the amount of writing necessary when using truth tables? \n Answer: \n\nA. It is important to reduce the amount of writing necessary when using truth tables in order to save time and effort. By grouping input bits into numbers and by defining multiple output columns, the truth table can be written more concisely. This allows for a quicker and easier understanding of the logical relationships between the inputs and outputs."}, {"text": "Title: Implications of Logical Completeness P3 \n Text: That said, {AND, OR, and NOT} is not the only logically complete set of\nfunctions.  In fact, our current complementary metal-oxide semiconductor\n(CMOS) technology, on which most of the computer industry is now built,\ndoes not directly implement these functions, as you will see later in\nour class. \n Question: \nQ. Q: What does the author mean when they say \"In general, for each line of a truth table for an operation, you may\nQ. need to make use of the input representation to identify the input values,\nQ. calculate the operation's result as a value, and then translate the value\nQ. back into the correct bit pattern using the output representation.\"? \n Answer: \n\nA. The author is saying that, in general, when you are writing a truth table for an operation, you will need to use the input representation to figure out what the input values are, calculate the result of the operation as a value, and then translate that value back into the correct bit pattern using the output representation."}, {"text": "Title: Implications of Logical Completeness P4 \n Text: The functions that are implemented directly in CMOS are NAND and NOR, which\nare abbreviations for AND followed by NOT and OR followed by NOT, \nrespectively.  Truth tables for the two are shown to the right. \n Question: \nQ. What does the 'S' column represent? \n Answer: \n\nThe 'S' column represents the sum of the values in columns 'A' and 'B'."}, {"text": "Title: Implications of Logical Completeness P5 \n Text: Either of these functions by itself forms a logically complete set.\nThat is, both the set  and the set  are logically\ncomplete.  For now, we leave the proof of this claim to you.  Re- \n Question: \nQ. What does it mean when the signs of two input operands differ? \n Answer: \n\nA. If the signs of two input operands differ, one should use base 2 subtraction."}, {"text": "Title: Implications of Logical Completeness P6 \n Text: \n{cc|cc}\n{c|}& \n& & & {A+B}\nA& B& A NAND B& A NOR B \n0& 0& 1& 1\n0& 1& 1& 0\n1& 0& 1& 0\n1& 1& 0& 0 \n Question: \nQ. What are the Boolean logic operations? \n Answer: \n\nA. The Boolean logic operations are the operations of logic that are based on the Boolean algebra. These operations are used to develop digital systems."}, {"text": "Title: Implications of Logical Completeness P7 \n Text: member that all\nyou need to show is that you can implement any set known to be\nlogically complete, so in order to prove that  is \nlogically complete (for example),\nyou need only show that you can implement AND, OR, and NOT using only\nNAND. \n Question: \nQ. What does the author mean by \"common English words with different meanings\"? \n Answer: \n\nA. The author is referring to the fact that some common English words have different meanings when used in the context of Boolean logic. For example, the word \"and\" has a different meaning in Boolean logic than it does in everyday speech."}, {"text": "Title: Examples and a Generalization P0 \n Text: Let's use our construction to solve a few examples.  We begin with\nthe functions that we illustrated with the first truth table from this\nset of notes,\nthe carry out C and sum S of two {2-bit}\nunsigned numbers.  Since each output bit requires a separate expression,\nwe now write S_1S_0 for the two bits of the sum.  We also need to\nbe able to make use of the individual bits of the input values, so we \nwrite these as A_1A_0 and B_1B_0, as shown on the left below.\nUsing our \nconstruction from the logical completeness theorem, we obtain the\nequations on the right.\nYou should verify these expressions yourself. \n Question: \nQ. What is the implication of completeness? \n Answer: \n\nA. The implication of completeness is that it enables abstraction, and allows for the evaluation of future possible implementations of devices quickly and easily."}, {"text": "Title: Examples and a Generalization P1 \n Text: {cccc|ccc}\n{c|}& \nA_1& A_0& B_1& B_0& C& S_1& S_0 \n0&0& 0&0& 0& 0&0\n0&0& 0&1& 0& 0&1\n0&0& 1&0& 0& 1&0\n0&0& 1&1& 0& 1&1\n0&1& 0&0& 0& 0&1\n0&1& 0&1& 0& 1&0\n0&1& 1&0& 0& 1&1\n0&1& 1&1& 1& 0&0\n1&0& 0&0& 0& 1&0\n1&0& 0&1& 0& 1&1\n1&0& 1&0& 1& 0&0\n1&0& 1&1& 1& 0&1\n1&1& 0&0& 0& 1&1\n1&1& 0&1& 1& 0&0\n1&1& 1&0& 1& 0&1\n1&1& 1&1& 1& 1&0 \n Question: \nQ. What is the difference between a generalization to truth tables and Boolean logic operations? \n Answer: \n\nA. A generalization to truth tables is a way of representing a function on bits using a table. Boolean logic operations are a way of representing a function on bits using logic operations."}, {"text": "Title: Examples and a Generalization P2 \n Text: C &=& \n{A_1} A_0 B_1 B_0 +\nA_1 {A_0} B_1 {B_0} +\nA_1 {A_0} B_1 B_0 +\n&& A_1 A_0 {B_1} B_0 +\nA_1 A_0 B_1 {B_0} +\nA_1 A_0 B_1 B_0 \n Question: \nQ. Why is it important to reduce the amount of writing necessary when using truth tables? \n Answer: \n\nA. It is important to reduce the amount of writing necessary when using truth tables in order to save time and effort. By grouping input bits into numbers and by defining multiple output columns, the truth table can be written more concisely. This allows for a quicker and easier understanding of the logical relationships between the inputs and outputs."}, {"text": "Title: Examples and a Generalization P3 \n Text: \nS_1 &=& \n{A_1} {A_0} B_1 {B_0} +\n{A_1} {A_0} B_1 B_0 +\n{A_1} A_0 {B_1} B_0 +\n&& {A_1} A_0 B_1 {B_0} +\nA_1 {A_0} {B_1} {B_0} +\nA_1 {A_0} {B_1} B_0 +\n&& A_1 A_0 {B_1} {B_0} +\nA_1 A_0 B_1 B_0 \n Question: \nQ. Q: What does the author mean when they say \"In general, for each line of a truth table for an operation, you may\nQ. need to make use of the input representation to identify the input values,\nQ. calculate the operation's result as a value, and then translate the value\nQ. back into the correct bit pattern using the output representation.\"? \n Answer: \n\nA. The author is saying that, in general, when you are writing a truth table for an operation, you will need to use the input representation to figure out what the input values are, calculate the result of the operation as a value, and then translate that value back into the correct bit pattern using the output representation."}, {"text": "Title: Examples and a Generalization P4 \n Text: \nS_0 &=& \n{A_1} {A_0} {B_1} B_0 +\n{A_1} {A_0} B_1 B_0 +\n{A_1} A_0 {B_1} {B_0} +\n&& {A_1} A_0 B_1 {B_0} +\nA_1 {A_0} {B_1} B_0 +\nA_1 {A_0} B_1 B_0 +\n&& A_1 A_0 {B_1} {B_0} +\nA_1 A_0 B_1 {B_0}\n{eqnarray*} \n Question: \nQ. What does the 'S' column represent? \n Answer: \n\nThe 'S' column represents the sum of the values in columns 'A' and 'B'."}, {"text": "Title: Examples and a Generalization P5 \n Text: \nNow let's consider a new function.  Given\nan {8-bit} 2's complement number, A=A_7A_6A_5A_4A_3A_2A_1A_0,\nwe want to compare it with the value -1.  We know that\nwe can construct this function using AND, OR, and NOT, but how?\nWe start by writing the representation for -1, which is 11111111.\nIf the number A matches that representation, we want to produce a 1.\nIf the number A differs in any bit, we want to produce a 0.\nThe desired function has exactly one combination of inputs that\nproduces a 1, so in fact we need only one minterm!  In this case, we\ncan compare with -1 by calculating the expression: \n Question: \nQ. What does it mean when the signs of two input operands differ? \n Answer: \n\nA. If the signs of two input operands differ, one should use base 2 subtraction."}, {"text": "Title: Examples and a Generalization P6 \n Text: {eqnarray*}\nA_7  A_6  A_5  A_4  A_3  A_2  A_1  A_0\n{eqnarray*} \n Question: \nQ. What are the Boolean logic operations? \n Answer: \n\nA. The Boolean logic operations are the operations of logic that are based on the Boolean algebra. These operations are used to develop digital systems."}, {"text": "Title: Examples and a Generalization P7 \n Text: Here we have explicitly included multiplication symbols to avoid\nconfusion with our notation for groups of bits, as we used when \nnaming the individual bits of A. \n Question: \nQ. What does the author mean by \"common English words with different meanings\"? \n Answer: \n\nA. The author is referring to the fact that some common English words have different meanings when used in the context of Boolean logic. For example, the word \"and\" has a different meaning in Boolean logic than it does in everyday speech."}, {"text": "Title: Examples and a Generalization P8 \n Text: \nIn closing, we briefly introduce a generalization of logic\noperations to groups of bits.  Our representations for integers,\nreal numbers, and characters from human languages all use more than\none bit to represent a given value.  When we use computers, we often\nmake use of multiple bits in groups in this way.  A { byte}, for example,\ntoday means an ordered group of eight bits. \n Question: \nQ. What is a logic operation?\nQ. \nQ. A logic operation is a function that takes one or more Boolean values as input and returns a Boolean value as output. The most common logic operations are AND, OR, and NOT. \n Answer: "}, {"text": "Title: Examples and a Generalization P9 \n Text: We can extend our logic functions to operate on such groups by pairing\nbits from each of two groups and performing the logic operation on each\npair.  For example, given\nA=A_7A_6A_5A_4A_3A_2A_1A_0=01010101\nand\nB=B_7B_6B_5B_4B_3B_2B_1B_0=11110000, we calculate\nA AND B by computing the AND of each pair of bits, \nA_7 AND B_7,\nA_6 AND B_6,\nand so forth, to\nproduce the result 01010000, as shown to the right.\nIn the same way, we can extend other logic\noperations, such as OR, NOT, and XOR, to operate on bits of groups. \n Question: \nQ. What does the NOT function do?\nQ. \nQ. The NOT function is a logical operator that negates a single value. \n Answer: "}, {"text": "Title: Examples and a Generalization P10 \n Text: &A  & 0&1& 0&1& 0&1& 0&1\n  AND   &B& 1&1&1&1&0&0&0&0 \n&& 0& 1&0&1&0&0&0&0 \n Question: \nQ. What does the exclusive OR function do? \n Answer: \n\nA. The exclusive OR function returns 1 if an odd number of input operands are equal to 1."}, {"text": "Title: Programming Concepts and the C Language P0 \n Text: This set of notes introduces the C programming language and explains\nsome basic concepts in computer programming.  Our purpose in showing\nyou a high-level language at this early stage of the course is to give\nyou time to become familiar with the syntax and meaning of the language,\nnot to teach you how to program.  Throughout this semester, we will\nuse software written in C to demonstrate and validate the digital system design\nmaterial in our course.  Towards the end of the semester, you will\nlearn to program computers using instructions and assembly language.\nIn ECE 220, you will make use of the C language to write\nprograms, at which point already being familiar with the language will\nmake the material easier to master.   \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: Programming Concepts and the C Language P1 \n Text: These notes are meant to complement the\nintroduction provided by Patt and Patel. \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: Programming Concepts and the C Language P2 \n Text: After a brief introduction to the history of C and the structure of\na program written in C, we connect the idea of representations developed \nin class to the data types used in high-level languages. \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: Programming Concepts and the C Language P3 \n Text: We next discuss the use of variables in C, then describe some of the \noperators available to the programmer, including arithmetic and logic\noperators.  The notes next introduce C functions that support the ability to\nread user input from the keyboard and to print results to the monitor. \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: Programming Concepts and the C Language P4 \n Text: A description of the structure of statements in C follows, explaining\nhow programs are executed and how a programmer can create statements\nfor conditional execution as well as loops to perform repetitive tasks. \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: Programming Concepts and the C Language P5 \n Text: The main portion of the notes concludes with \nan example program, which is used to illustrate both the execution of \nC statements as well as the difference between variables in programs\nand variables in algebra. \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: Programming Concepts and the C Language P6 \n Text: The remainder of the notes covers more advanced topics.  First, we \ndescribe how the compilation process works,\nillustrating how a program written in a high-level language is\ntransformed into instructions.  You will learn this process in much\nmore detail in ECE 220.   \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: Programming Concepts and the C Language P7 \n Text: Second, we briefly introduce the C preprocessor. \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: Programming Concepts and the C Language P8 \n Text: Finally, we discuss implicit and explicit data type conversion in C. \n Question: \nQ. What is the difference between implicit and explicit data type conversion in C? \n Answer: \n\nA.\n\nImplicit type conversion in C is when the compiler automatically converts one data type to another, without the programmer specifying any conversion code. This can happen when an operator is applied to operands of different types, or when a function is called with arguments of different types. Explicit type conversion, on the other hand, is when the programmer explicitly specifies code to convert a data type to another."}, {"text": "Title: Programming Concepts and the C Language P9 \n Text: { Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.} \n Question: \nQ. What did the text say about the sections marked with an asterisk?\nQ. \nQ. The text said that the sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. \n Answer: "}, {"text": "Title: The C Programming Language P0 \n Text: Programming languages attempt to bridge the semantic gap between human\ndescriptions of problems and the relatively simple instructions that\ncan be provided by an instruction set architecture (ISA).  \nSince 1954, when the Fortran language\nfirst enabled scientists to enter FORmulae symbolically and to have\nthem TRANslated automatically into instructions, people have invented\nthousands of computer languages.   \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: The C Programming Language P1 \n Text: \nThe C programming language was developed by Dennis Ritchie at Bell Labs\nin order to simplify the task of writing the Unix operating system.\nThe C language provides a fairly transparent mapping to typical ISAs,\nwhich makes it a good choice both for system software such as operating\nsystems and for our class. \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: The C Programming Language P2 \n Text: The { syntax} used in C---that is, the rules that one must follow\nto write valid C programs---has also heavily influenced many other\nmore recent languages, such as C++, Java, and Perl.   \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: The C Programming Language P3 \n Text: For our purposes, a C program consists of a set of { variable \ndeclarations} and a sequence of { statements}.   \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: The C Programming Language P4 \n Text: aaaa=/* =\nint\nmain ()\n{\n>  int answer = 42;        /* the Answer! */ \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: The C Programming Language P5 \n Text: >  printf (\"The answer is d.n\", answer); \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: The C Programming Language P6 \n Text: >  /*> Our work here is done.\n>    > Let's get out of here! */\n>  return 0;\n} \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: The C Programming Language P7 \n Text: \nBoth of these parts are written into a single C function called { main},\nwhich executes when the program starts.   \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: The C Programming Language P8 \n Text: A simple example appears to the right.  The program uses one variable\ncalled { answer}, which it initializes to the value 42.\nThe program prints a line of output to the monitor for the user,\nthen terminates using the { return} statement.  { Comments} for human\nreaders begin with the characters { /*} (a slash followed by an \nasterisk) and end with the characters { */} (an asterisk followed \nby a slash). \n Question: \nQ. What is the difference between implicit and explicit data type conversion in C? \n Answer: \n\nA.\n\nImplicit type conversion in C is when the compiler automatically converts one data type to another, without the programmer specifying any conversion code. This can happen when an operator is applied to operands of different types, or when a function is called with arguments of different types. Explicit type conversion, on the other hand, is when the programmer explicitly specifies code to convert a data type to another."}, {"text": "Title: The C Programming Language P9 \n Text: The C language ignores white space in programs, so we encourage\nyou to use blank lines and extra spacing to make your programs\neasier to read. \n Question: \nQ. What did the text say about the sections marked with an asterisk?\nQ. \nQ. The text said that the sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. \n Answer: "}, {"text": "Title: The C Programming Language P10 \n Text: The variables defined in the { main} function allow a programmer\nto associate arbitrary { symbolic names} (sequences of English characters, \nsuch as ``sum'' or ``product'' or ``highScore'') with specific\ntypes of data, such as a {16-bit} unsigned integer or a\ndouble-precision floating-point number.  \n Question: \nQ. What is the semantic gap? \n Answer: \n\nA. The semantic gap is the difference between human descriptions of problems and the relatively simple instructions that can be provided by an instruction set architecture (ISA)."}, {"text": "Title: The C Programming Language P11 \n Text: In the example program above, the variable { answer} is declared\nto be a {32-bit} {2's} complement number. \n Question: \nQ. What is the C programming language? \n Answer: \n\nA. The C programming language is a language that was developed by Dennis Ritchie in order to simplify the task of writing the Unix operating system. The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class."}, {"text": "Title: The C Programming Language P12 \n Text: Those with no programming experience may at first find the difference\nbetween variables in algebra and variables in programs slightly \nconfusing.  { As a program executes, the values of variables can \nchange from step to step of execution.} \n Question: \nQ. What is the name of the language that C++, Java, and Perl are based on? \n Answer: \n\nA. C"}, {"text": "Title: The C Programming Language P13 \n Text: The statements in the { main} function are executed one by one\nuntil the program terminates.   \n Question: \nQ. What is the purpose of a variable declaration in a C program?\nQ. \nQ. A variable declaration in a C program tells the compiler what type of value a variable will hold, and what its name will be. This allows the compiler to generate code that accesses the variable correctly. \n Answer: "}, {"text": "Title: The C Programming Language P14 \n Text: Programs are not limited to simple sequences of statements, however.\nSome types of statements allow a programmer\nto specify conditional behavior.  For example, a program might only\nprint out secret information if the user's name is ``lUmeTTa.''\nOther types of statements allow a programmer to repeat the execution\nof a group of statements until a condition is met.  For example, a program\nmight print the numbers from 1 to 10, or ask for input until the user\ntypes a number between 1 and 10. \n Question: \nQ. What is the value of the answer variable?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: The C Programming Language P15 \n Text: The order of statement execution is well-defined in C, but the\nstatements in { main} do not necessarily make up an algorithm:\n{ we can easily write a C program that never terminates}. \n Question: \nQ. What is the answer to the question?\nQ. \nQ. The answer to the question is d. \n Answer: n"}, {"text": "Title: The C Programming Language P16 \n Text: If a program terminates, the { main} function\nreturns an integer to the operating system, usually by executing\na { return} statement, as in the example program. \n Question: \nQ. What is the value of the variable \"x\" at the end of the program?\nQ. \nQ. The value of the variable \"x\" at the end of the program is 0. \n Answer: "}, {"text": "Title: The C Programming Language P17 \n Text: By convention, returning the value 0 indicates successful completion\nof the program, while any non-zero value indicates a program-specific\nerror. \n Question: \nQ. What is the main function of a C program?\nQ. \nQ. The main function of a C program is to execute when the program starts. \n Answer: "}, {"text": "Title: The C Programming Language P18 \n Text: However, { main} is not necessarily a function in the mathematical \nsense because { the value returned from { main} is not \nnecessarily unique for a given set of input values to the program}.   \n Question: \nQ. What is the value of the answer variable when the program terminates?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: The C Programming Language P19 \n Text: For example, we can write a program that selects a number from 1 to 10 \nat random and returns the number to the operating system. \n Question: \nQ. What does the C language ignore?\nQ. \nQ. The C language ignores white space in programs. \n Answer: "}, {"text": "Title: Data Types P0 \n Text: As you know, modern digital computers represent all information with\nbinary digits (0s and 1s), or { bits}.  Whether you are representing \nsomething as simple as an integer or as complex as an undergraduate \nthesis, the data are simply a bunch of 0s and 1s inside a computer.   \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: Data Types P1 \n Text: For any given type of information, a human selects a data type for the\ninformation.  A { data type} (often called just a { type})\nconsists of both a size in bits and a representation, such as the\n2's complement representation for signed integers, or the ASCII\nrepresentation for English text.  A { representation} is a way of\nencoding the things being represented as a set of bits, with each bit\npattern corresponding to a unique object or thing. \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: Data Types P2 \n Text: A typical ISA supports a handful of\ndata types in hardware in the sense that it provides hardware \nsupport for operations on those data types. \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: Data Types P3 \n Text: The arithmetic logic units (ALUs) in most modern processors,\nfor example, support addition\nand subtraction of both unsigned and 2's complement representations, with\nthe specific data type (such as 16- or 64-bit 2's complement)\ndepending on the ISA. \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: Data Types P4 \n Text: Data types and operations not supported by the ISA must be handled in\nsoftware using a small set of primitive operations, which form the\n{ instructions} available in the ISA.  Instructions usually\ninclude data movement instructions such as loads and stores\nand control instructions such as branches and subroutine calls in\naddition to arithmetic and logic operations.   \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: Data Types P5 \n Text: The last quarter of our class covers these concepts in more detail\nand explores their meaning using an example ISA from the textbook. \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: Data Types P6 \n Text: In class, we emphasized the idea that digital systems such as computers\ndo not interpret the meaning of bits.  Rather, they do exactly what\nthey have been designed to do, even if that design is meaningless. \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: Data Types P7 \n Text: If, for example, you store\na sequence of ASCII characters \nin a computer's memory as \nand\nthen write computer instructions to add consecutive groups of four characters\nas 2's complement integers and to print the result to the screen, the\ncomputer will not complain about the fact that your code produces\nmeaningless garbage.   \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: Data Types P8 \n Text: In contrast, high-level languages typically require that a programmer\nassociate a data type with each datum in order to reduce the chance \nthat the bits \nmaking up an individual datum are misused or misinterpreted accidentally.   \n Question: \nQ. What is the difference between implicit and explicit data type conversion in C? \n Answer: \n\nA.\n\nImplicit type conversion in C is when the compiler automatically converts one data type to another, without the programmer specifying any conversion code. This can happen when an operator is applied to operands of different types, or when a function is called with arguments of different types. Explicit type conversion, on the other hand, is when the programmer explicitly specifies code to convert a data type to another."}, {"text": "Title: Data Types P9 \n Text: Attempts to interpret a set of bits differently usually generate at least\na warning message, since \n Question: \nQ. What did the text say about the sections marked with an asterisk?\nQ. \nQ. The text said that the sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. \n Answer: "}, {"text": "Title: Data Types P10 \n Text: such re-interpretations of the\nbits are rarely intentional and thus rarely correct.  A compiler---a\nprogram that transforms code written in a high-level language into\ninstructions---can also generate the proper type conversion instructions \nautomatically when the \ntransformations are intentional, as is often the case with arithmetic. \n Question: \nQ. What is the semantic gap? \n Answer: \n\nA. The semantic gap is the difference between human descriptions of problems and the relatively simple instructions that can be provided by an instruction set architecture (ISA)."}, {"text": "Title: Data Types P11 \n Text: Some high-level languages, such as Java, \nprevent programmers from changing the type of a given datum.\nIf you define a type that represents one of your\nfavorite twenty colors, for example, you are not allowed to turn a\ncolor into an integer, despite the fact that the color is represented\nas a handful of bits.  Such languages are said to be { strongly\ntyped}.   \n Question: \nQ. What is the C programming language? \n Answer: \n\nA. The C programming language is a language that was developed by Dennis Ritchie in order to simplify the task of writing the Unix operating system. The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class."}, {"text": "Title: Data Types P12 \n Text: The C language is not strongly typed, and programmers are free to\ninterpret any bits in any manner they see fit.  Taking advantage of\nthis ability in any but a few exceptional cases, however, \nresults in arcane and non-portable code, and is thus considered to be\nbad programming practice.  We discuss conversion between types in more\ndetail later in these notes. \n Question: \nQ. What is the name of the language that C++, Java, and Perl are based on? \n Answer: \n\nA. C"}, {"text": "Title: Data Types P13 \n Text: Each high-level language defines a number of { primitive data\ntypes}, which are always available.  Most languages, including C,\nalso provide ways of defining new types in terms of primitive types,\nbut we leave that part of C for ECE 220. \n Question: \nQ. What is the purpose of a variable declaration in a C program?\nQ. \nQ. A variable declaration in a C program tells the compiler what type of value a variable will hold, and what its name will be. This allows the compiler to generate code that accesses the variable correctly. \n Answer: "}, {"text": "Title: Data Types P14 \n Text: The primitive data types in C include signed and unsigned integers of various\nsizes as well as single- and double-precision IEEE floating-point numbers. \n Question: \nQ. What is the value of the answer variable?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: Data Types P15 \n Text: \nThe primitive integer types in C include both unsigned and 2's\ncomplement representations.  These types were originally defined so as\nto give reasonable performance when code was ported.  In particular,\nthe { int} type is intended to be the native integer type for the\ntarget ISA.  Using data types supported directly in hardware is faster \nthan using larger or smaller integer types.  When C was standardized in 1989,\nthese types were defined so as to include a range of existing\nC compilers rather than requiring all compilers to produce uniform\nresults.  At the \n Question: \nQ. What is the answer to the question?\nQ. \nQ. The answer to the question is d. \n Answer: n"}, {"text": "Title: Data Types P16 \n Text: \n{-8pt}{|r|l|l|}\n& {|c|}{2's complement}& {|c|}\n8 bits& { char}& { unsigned char} \n16 bits& { short}& { unsigned short}\n& { short int}& { unsigned short int} \n32 bits& { int}& { unsigned}\n&& { unsigned int} \n32 or & { long}& { unsigned long}\n64 bits& { long int}& { unsigned long int} \n64 bits& { long long}& { unsigned long long}\n& { long long int}& { unsigned long long int} \n Question: \nQ. What is the value of the variable \"x\" at the end of the program?\nQ. \nQ. The value of the variable \"x\" at the end of the program is 0. \n Answer: "}, {"text": "Title: Data Types P17 \n Text: time, most workstations and mainframes were 32-bit machines, while\nmost personal computers were 16-bit machines, thus flexibility was somewhat\ndesirable.  For the GCC compiler on Linux, the C integer data \ntypes are defined\nin the table above.  Although the { int} and { long}\ntypes are usually the same, there is a semantic difference in common\nusage.  In particular, on most architectures and most compilers, a\n{ long} has enough bits to identify a location in the computer's\nmemory, while an { int} may not. \n Question: \nQ. What is the main function of a C program?\nQ. \nQ. The main function of a C program is to execute when the program starts. \n Answer: "}, {"text": "Title: Data Types P18 \n Text: When in doubt, the { size in bytes} of any type or variable can be\nfound using the built-in C function { sizeof}. \n Question: \nQ. What is the value of the answer variable when the program terminates?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: Data Types P19 \n Text: \nOver time, the flexibility of size in C types has become less\nimportant (except for the embedded markets, where one often wants even\nmore accurate bit-width control), and the fact that the size of an\n{ int} can vary from machine to machine and compiler to compiler\nhas become more a source of headaches than a helpful feature.  In the\nlate 1990s, a new set of fixed-size types were recommended for\ninclusion \n Question: \nQ. What does the C language ignore?\nQ. \nQ. The C language ignores white space in programs. \n Answer: "}, {"text": "Title: Data Types P20 \n Text: \n{-8pt}{|r|l|l|}\n& {|c|}{2's complement}& {|c|}\n 8 bits& {  int8_t}& {  uint8_t}\n16 bits& { int16_t}& { uint16_t}\n32 bits& { int32_t}& { uint32_t}\n64 bits& { int64_t}& { uint64_t} \n Question: \nQ. What is the main purpose of the variables defined in the main function?\nQ. \nQ. The main purpose of the variables defined in the main function is to associate symbolic names with specific types of data. \n Answer: "}, {"text": "Title: Data Types P21 \n Text: in the C library, reflecting the fact that many companies\nhad already developed and were using such definitions to make their\nprograms platform-independent. \n Question: \nQ. What is the range of values that can be stored in a 32-bit 2's complement number?\nQ. \nQ. The range of values that can be stored in a 32-bit 2's complement number is -2147483648 to 2147483647. \n Answer: "}, {"text": "Title: Data Types P22 \n Text: We encourage you to make use of these types, which are shown in \nthe table above.  In Linux, they can be made available by including \nthe { stdint.h} header file. \n Question: \nQ. What is the difference between variables in algebra and variables in programs? \n Answer: \n\nA. In algebra, variables are used to represent unknown values. In programs, variables are used to store values that can change during execution."}, {"text": "Title: Data Types P23 \n Text: Floating-point types in C include { float} and { double},\nwhich correspond respectively to single- and double-precision IEEE\nfloating-point values.  Although the {32-bit} { float} type\ncan save memory compared with use of {64-bit} { double}\nvalues, C's math library works with double-precision values, and\nsingle-precision data are uncommon in scientific and engineering\ncodes.  In contrast, single-precision floating-point operations\ndominated the\ngraphics industry until recently, and are still well-supported even\non today's graphics processing units. \n Question: \nQ. What statement is executed last in the main function? \n Answer: \n\nThe last statement executed in the main function is the return statement."}, {"text": "Title: Variable Declarations P0 \n Text: The function { main} executed by a program begins with a list\nof { variable declarations}.  Each declaration consists of two parts:\na data type specification and a comma-separated list of variable names.\nEach variable declared can also \nbe { initialized} by assigning an initial value.  A few examples \nappear below.  Notice that one can initialize a variable to have the same\nvalue as a second variable. \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: Variable Declarations P1 \n Text: aaaaaaa=aa=aaaaaaaaaaaaaaaaaaaaa=/=* a third 2's complement variable with unknown initial value =\nint > x > = 42; >/>* a 2's complement variable, initially equal to 42 > */\nint > y > = x;  >/>* a second 2's complement variable, initially equal to x > */\nint > z;>       >/>* a third 2's complement variable with unknown initial value > */\ndouble> a, b, c, pi = 3.1416; > >/>*\n>>>>* four double-precision IEEE floating-point variables\n>>>>* a, b, and c are initially of unknown value, while pi is\n>>>>* initially 3.1416\n>>>>*/ \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: Variable Declarations P2 \n Text: What happens if a programmer declares a variable but does not \ninitialize it? \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: Variable Declarations P3 \n Text: Remember that bits can only be 0 or 1. \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: Variable Declarations P4 \n Text: An uninitialized variable does have a value, but its value is unpredictable. \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: Variable Declarations P5 \n Text: The compiler tries to detect uses of uninitialized variables, but sometimes\nit fails to do so, so { until you are more familiar with programming,\nyou should always initialize every variable}. \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: Variable Declarations P6 \n Text: Variable names, also called { identifiers}, can include both letters\nand digits in C. \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: Variable Declarations P7 \n Text: Good programming style requires that programmers select variable names\nthat are meaningful and are easy to distinguish from one another. \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: Variable Declarations P8 \n Text: Single letters are acceptable in some situations, but longer names with\nmeaning are likely to help people (including you!) understand your \nprogram. \n Question: \nQ. What is the difference between implicit and explicit data type conversion in C? \n Answer: \n\nA.\n\nImplicit type conversion in C is when the compiler automatically converts one data type to another, without the programmer specifying any conversion code. This can happen when an operator is applied to operands of different types, or when a function is called with arguments of different types. Explicit type conversion, on the other hand, is when the programmer explicitly specifies code to convert a data type to another."}, {"text": "Title: Variable Declarations P9 \n Text: Variable names are also case-sensitive in C, which allows programmers\nto use capitalization to differentiate behavior and meaning, if desired.\nSome programs, for example, use identifiers with all capital letters\nto indicate variables with values that remain constant for the program's\nentire execution. \n Question: \nQ. What did the text say about the sections marked with an asterisk?\nQ. \nQ. The text said that the sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. \n Answer: "}, {"text": "Title: Variable Declarations P10 \n Text: However, the fact that identifiers are case-sensitive also means \nthat a programmer can declare distinct variables \nnamed { variable}, { Variable}, { vaRIable}, { vaRIabLe}, \nand { VARIABLE}.  We strongly discourage you from doing so. \n Question: \nQ. What is the semantic gap? \n Answer: \n\nA. The semantic gap is the difference between human descriptions of problems and the relatively simple instructions that can be provided by an instruction set architecture (ISA)."}, {"text": "Title: Expressions and Operators P0 \n Text: \nThe { main} function also contains a sequence of statements. \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: Expressions and Operators P1 \n Text: A statement is a complete specification of a single step\nin the program's execution. \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: Expressions and Operators P2 \n Text: We explain the structure of\nstatements in the next section.   \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: Expressions and Operators P3 \n Text: Many statements in C include one or more { expressions},\nwhich represent calculations such as arithmetic, comparisons,\nand logic operations. \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: Expressions and Operators P4 \n Text: Each expression is in turn composed of { operators} and { operands}. \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: Expressions and Operators P5 \n Text: Here we give only a brief introduction to some of the operators available\nin the C language.  We deliberately omit operators with more\ncomplicated meanings, as well as operators for which the original\npurpose was to make writing common operations a little shorter. \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: Expressions and Operators P6 \n Text: For the interested reader, both the textbook and ECE 220 give more \ndetailed introductions. \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: Expressions and Operators P7 \n Text: The table to the right gives examples for the operators described \nhere.   \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: Expressions and Operators P8 \n Text: {int i = 42, j = 1000;}\n{/* i = 0x0000002A, j = 0x000003E8 */} \n Question: \nQ. What is the difference between implicit and explicit data type conversion in C? \n Answer: \n\nA.\n\nImplicit type conversion in C is when the compiler automatically converts one data type to another, without the programmer specifying any conversion code. This can happen when an operator is applied to operands of different types, or when a function is called with arguments of different types. Explicit type conversion, on the other hand, is when the programmer explicitly specifies code to convert a data type to another."}, {"text": "Title: Expressions and Operators P9 \n Text: i + j &  1042\ni - 4 * j &  -3958\n-j &  -1000\nj / i &  23\nj  i &   42\ni & j &   40& /* 0x00000028 */\ni | j &   1002& /* 0x000003EA */\ni  j &   962& /* 0x000003C2 */\ni &   -43& /* 0xFFFFFFD5 */\n(i) >> 2 &   -11& /* 0xFFFFFFF5 */\n((i) >> 4) &   2& /* 0x00000002 */\nj >> 4 &  62& /* 0x0000003E */ \nj << 3 &  8000& /* 0x00001F40 */ \ni > j &   0\ni <= j &   1\ni == j &   0\nj = i &   42& /* ...and j is changed! */ \n Question: \nQ. What did the text say about the sections marked with an asterisk?\nQ. \nQ. The text said that the sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. \n Answer: "}, {"text": "Title: Expressions and Operators P10 \n Text: \n{ Arithmetic operators} in C include addition ({ +}), \nsubtraction ({ -}), negation (a minus sign not \npreceded by another expression), multiplication ({ *}), \ndivision ({ /}), and modulus ({ }).  No exponentiation\noperator exists; instead, library routines are defined for this purpose\nas well as for a range of more complex mathematical functions. \n Question: \nQ. What is the semantic gap? \n Answer: \n\nA. The semantic gap is the difference between human descriptions of problems and the relatively simple instructions that can be provided by an instruction set architecture (ISA)."}, {"text": "Title: Expressions and Operators P11 \n Text: C also supports { bitwise operations} on integer types, including \nAND ({ &}), OR ({ |}), XOR ({ ^{ }}), NOT ({ }), \nand left ({ <<}) and right ({ >>}) bit shifts.\nRight shifting a signed integer results in an { arithmetic right shift}\n(the sign bit is copied), while right shifting an unsigned integer\nresults in a { logical right shift} (0 bits are inserted). \n Question: \nQ. What is the C programming language? \n Answer: \n\nA. The C programming language is a language that was developed by Dennis Ritchie in order to simplify the task of writing the Unix operating system. The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class."}, {"text": "Title: Expressions and Operators P12 \n Text: A range of { relational} or { comparison operators} are \navailable, including equality ({ ==}),\ninequality ({ !=}), and relative order ({ <}, { <=},\n{ >=}, and { >}). \n Question: \nQ. What is the name of the language that C++, Java, and Perl are based on? \n Answer: \n\nA. C"}, {"text": "Title: Expressions and Operators P13 \n Text: All such operations evaluate to 1 to indicate a true relation\nand 0 to indicate a false relation.  Any non-zero value is considered\nto be true for the purposes of tests (for example, in an { if} statement\nor a { while} loop) in C---these statements are explained later in \nthese notes. \n Question: \nQ. What is the purpose of a variable declaration in a C program?\nQ. \nQ. A variable declaration in a C program tells the compiler what type of value a variable will hold, and what its name will be. This allows the compiler to generate code that accesses the variable correctly. \n Answer: "}, {"text": "Title: Expressions and Operators P14 \n Text: { Assignment} of a new value to a variable \nuses a single equal sign ({ =}) in C.   \n Question: \nQ. What is the value of the answer variable?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: Expressions and Operators P15 \n Text: For example, the expression { A = B} copies\nthe value of variable { B} into variable { A}, overwriting the\nbits representing the previous value of { A}. \n Question: \nQ. What is the answer to the question?\nQ. \nQ. The answer to the question is d. \n Answer: n"}, {"text": "Title: Expressions and Operators P16 \n Text: { The use of two equal signs for an equality check and a single\nequal sign for assignment is a common source of errors,} although\nmodern compilers generally detect and warn about this type of mistake. \n Question: \nQ. What is the value of the variable \"x\" at the end of the program?\nQ. \nQ. The value of the variable \"x\" at the end of the program is 0. \n Answer: "}, {"text": "Title: Expressions and Operators P17 \n Text: Assignment in C does not solve equations, even simple equations.  \nWriting ``{ A-4=B}'', for example, generates a compiler error.\nYou must solve such equations yourself to calculate the desired\nnew value of a single variable, such as ``{  A=B+4}.''\nFor the purposes of our class, you must always write a single variable \non the left side of an assignment, and can write an arbitrary expression \non the right side. \n Question: \nQ. What is the main function of a C program?\nQ. \nQ. The main function of a C program is to execute when the program starts. \n Answer: "}, {"text": "Title: Expressions and Operators P18 \n Text: Many operators can be combined into a single expression.  When an\nexpression has more than one operator, which operator is executed first?\nThe answer depends on the operators' { precedence}, a well-defined order on\noperators that specifies how to resolve the ambiguity.  In the case\nof arithmetic, the C language's precedence specification matches the\none that you learned in elementary school.  For example, { 1+2*3}\nevaluates to 7, not to 9, because multiplication has precedence over\naddition.  For non-arithmetic operators, or for any case in which\nyou do not know the precedence specification for a language, {\ndo not look it up---other programmers will not remember the\nprecedence ordering, either!}  Instead, add parentheses to make your \nexpressions clear and easy to understand. \n Question: \nQ. What is the value of the answer variable when the program terminates?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: Basic I/O P0 \n Text: The { main} function returns an integer to the operating system.\nAlthough we do not discuss how additional functions can be written\nin our class, we may sometimes make use of functions that have been\nwritten in advance by making { calls} to those functions.\nA { function call} is type of expression in C, but we leave \nfurther description for ECE 220.  In our class, we make use of only\ntwo additional functions to enable our programs to receive input\nfrom a user via the keyboard and to write output to the monitor for \na user to read. \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: Basic I/O P1 \n Text: Let's start with output.  The { printf} function allows a program\nto print output to the monitor using a programmer-specific format.\nThe ``f'' in { printf} stands for ``formatted.''{The \noriginal, unformatted variant of printing was never available\nin the C language.  Go learn Fortran.} \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: Basic I/O P2 \n Text: When we want to use { printf}, we write a expression with\nthe word { printf} followed by a parenthesized, comma-separated\nlist of expressions.  The expressions in this list are called\nthe { arguments} to the { printf} function. \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: Basic I/O P3 \n Text: The first argument to the { printf} function is a format string---a \nsequence of ASCII characters between quotation marks---which tells \nthe function what kind of information we want printed to\nthe monitor as well as how to format that information. \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: Basic I/O P4 \n Text: The remaining arguments are C expressions that give { printf}\na copy of any values that we want printed. \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: Basic I/O P5 \n Text: How does the format string specify the format? \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: Basic I/O P6 \n Text: Most of the characters in the format string are simply printed to \nthe monitor.   \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: Basic I/O P7 \n Text: In the first example shown to on the next page, we use { printf}\nto print a hello message followed by an ASCII newline character\nto move to the next line on the monitor. \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: Basic I/O P8 \n Text: \nThe percent sign---``''---is used \nas an { escape character} in the\n{ printf} function.  When ``'' appears in the format\nstring, the function examines the next character in the format string\nto determine which format to use, then takes\nthe next expression from the sequence\nof arguments and prints the value of that expression to the \nmonitor.  Evaluating an expression generates a bunch of bits, so it is up to\nthe programmer to ensure that those bits are not misinterpreted.\nIn other words, the programmer must make sure that the number and\ntypes of formatted values match the number and types of arguments passed\nto { printf} (not counting the format string itself). \n Question: \nQ. What is the difference between implicit and explicit data type conversion in C? \n Answer: \n\nA.\n\nImplicit type conversion in C is when the compiler automatically converts one data type to another, without the programmer specifying any conversion code. This can happen when an operator is applied to operands of different types, or when a function is called with arguments of different types. Explicit type conversion, on the other hand, is when the programmer explicitly specifies code to convert a data type to another."}, {"text": "Title: Basic I/O P9 \n Text: The { printf} function returns the number of characters printed\nto the monitor. \n Question: \nQ. What did the text say about the sections marked with an asterisk?\nQ. \nQ. The text said that the sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. \n Answer: "}, {"text": "Title: Basic I/O P10 \n Text: output: =\n> { printf (\"Hello, world!n\");}\noutput: > { Hello, world!} [and a newline] \n Question: \nQ. What is the semantic gap? \n Answer: \n\nA. The semantic gap is the difference between human descriptions of problems and the relatively simple instructions that can be provided by an instruction set architecture (ISA)."}, {"text": "Title: Basic I/O P11 \n Text: > { printf (\"To x or not to d...n\", 190, 380 / 2);}\noutput: > { To be or not to 190...} [and a newline] \n Question: \nQ. What is the C programming language? \n Answer: \n\nA. The C programming language is a language that was developed by Dennis Ritchie in order to simplify the task of writing the Unix operating system. The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class."}, {"text": "Title: Basic I/O P12 \n Text: > { printf (\"My favorite number is cc.n\", 0x34, '0'+2);}\noutput: > { My favorite number is 42.} [and a newline] \n Question: \nQ. What is the name of the language that C++, Java, and Perl are based on? \n Answer: \n\nA. C"}, {"text": "Title: Basic I/O P13 \n Text: > { printf (\"What is pi?  f or e?n\", 3.1416, 3.1416);}\noutput: > { What is pi?  3.141600 or 3.141600e+00?} [and a newline] \n Question: \nQ. What is the purpose of a variable declaration in a C program?\nQ. \nQ. A variable declaration in a C program tells the compiler what type of value a variable will hold, and what its name will be. This allows the compiler to generate code that accesses the variable correctly. \n Answer: "}, {"text": "Title: Basic I/O P14 \n Text: \n{|c|l|}\nescape  &                         \nsequence& { printf} function's interpretation of expression bits \n{ c}& 2's complement integer printed as an ASCII character\n{ d}& 2's complement integer printed as decimal\n{ e}& double printed in decimal scientific notation\n{ f}& double printed in decimal\n{ u}& unsigned integer printed as decimal\n{ x}& integer printed as hexadecimal (lower case)\n{ X}& integer printed as hexadecimal (upper case)\n{ }& a single percent sign  \n Question: \nQ. What is the value of the answer variable?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: Basic I/O P15 \n Text: \nA program can read input from the user with the { scanf} function.\nThe user enters characters in ASCII using the keyboard, and the\n{ scanf} function converts the user's input into C primitive types,\nstoring the results into variables.  As with { printf}, the\n{ scanf} function takes a format string followed by a comma-separated\nlist of arguments.  Each argument after the format string provides\n{ scanf} with the memory address of a variable into which the\nfunction can store a result. \n Question: \nQ. What is the answer to the question?\nQ. \nQ. The answer to the question is d. \n Answer: n"}, {"text": "Title: Basic I/O P16 \n Text: How does { scanf} use the format string? \n Question: \nQ. What is the value of the variable \"x\" at the end of the program?\nQ. \nQ. The value of the variable \"x\" at the end of the program is 0. \n Answer: "}, {"text": "Title: Basic I/O P17 \n Text: For { scanf}, the format string is usually just a sequence\nof conversions, one for each variable to be typed in by the user.\nAs with { printf}, the conversions start with ``'' and\nare followed by characters specifying the type of conversion\nto be performed.  The first example shown to the right reads\ntwo integers. \n Question: \nQ. What is the main function of a C program?\nQ. \nQ. The main function of a C program is to execute when the program starts. \n Answer: "}, {"text": "Title: Basic I/O P18 \n Text: The conversions in the format string can be separated by spaces \nfor readability, as shown in the exam- \n Question: \nQ. What is the value of the answer variable when the program terminates?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: Basic I/O P19 \n Text: effect: =unsigned =\n> { int     } > { a, b;  /* example variables */}\n> { char    } > { c;}\n> { unsigned} > { u;}\n> { double  } > { d;}\n> { float   } > { f;} \n Question: \nQ. What does the C language ignore?\nQ. \nQ. The C language ignores white space in programs. \n Answer: "}, {"text": "Title: Basic I/O P20 \n Text: > { scanf (\"dd\", &a, &b);   /* These have the */}\n> { scanf (\"d d\", &a, &b);  /* same effect.   */}\neffect: > try to convert two integers typed in decimal to\n> 2's complement and store the results in { a} and { b} \n Question: \nQ. What is the main purpose of the variables defined in the main function?\nQ. \nQ. The main purpose of the variables defined in the main function is to associate symbolic names with specific types of data. \n Answer: "}, {"text": "Title: Basic I/O P21 \n Text: > { scanf (\"cx lf\", &c, &u, &d);}\neffect: > try to read an ASCII character into { c}, a value\n> typed in hexadecimal into { u}, and a double-\n> precision > floating-point number into { d} \n Question: \nQ. What is the range of values that can be stored in a 32-bit 2's complement number?\nQ. \nQ. The range of values that can be stored in a 32-bit 2's complement number is -2147483648 to 2147483647. \n Answer: "}, {"text": "Title: Basic I/O P22 \n Text: > { scanf (\"lf f\", &d, &f);}\neffect: > try to read two real numbers typed as decimal,\n> convert the first to double-precision and store it \n> in { d}, and convert the second to single-precision \n> and store it in { f} \n Question: \nQ. What is the difference between variables in algebra and variables in programs? \n Answer: \n\nA. In algebra, variables are used to represent unknown values. In programs, variables are used to store values that can change during execution."}, {"text": "Title: Basic I/O P23 \n Text: \nple.  The spaces are ignored\nby { scanf}.  However, { any non-space characters in the\nformat string must be typed exactly by the user!} \n Question: \nQ. What statement is executed last in the main function? \n Answer: \n\nThe last statement executed in the main function is the return statement."}, {"text": "Title: Basic I/O P24 \n Text: The remaining arguments to { scanf} specify memory addresses\nwhere the function can store the converted values.   \n Question: \nQ. What is the purpose of conditional statements in a program?\nQ. \nQ. Conditional statements allow a programmer to specify different behavior for a program based on certain conditions. \n Answer:  For example, a program might only print out secret information if the user's name is \"lUmeTTa.\""}, {"text": "Title: Basic I/O P25 \n Text: The ampersand (``&'') in front of each variable name in the examples is an\noperator that returns the address of a variable in memory. \n Question: \nQ. What is the order of statement execution in C? \n Answer: \n\nA. The order of statement execution in C is well-defined."}, {"text": "Title: Basic I/O P26 \n Text: \n{|c|l|}\nescape  &                         \nsequence& { scanf} function's conversion to bits \n{ c}& store one ASCII character (as { char})\n{ d}& convert decimal integer to 2's complement\n{ f}& convert decimal real number to float\n{ lf}& convert decimal real number to double\n{ u}& convert decimal integer to unsigned int\n{ x}& convert hexadecimal integer to unsigned int\n{ X}& (as above)  \n Question: \nQ. What value is returned to the operating system when the main function terminates? \n Answer: \n\nA. The value returned to the operating system when the main function terminates is an integer."}, {"text": "Title: Basic I/O P27 \n Text: version\nin the format string, the { scanf} function tries to convert\ninput from the user into the appropriate result, then stores the\nresult in memory at the address given by the next argument. \n Question: \nQ. Why is returning the value 0 considered successful completion of the program? \n Answer: \n\nA. Returning the value 0 is considered successful completion of the program because it indicates that the program has run without any errors."}, {"text": "Title: Basic I/O P28 \n Text: The programmer is responsible for ensuring that the number of \nconversions in the format string\nmatches the number of arguments provided (not counting\nthe format string itself).  The programmer must also ensure that\nthe type of information produced by each conversion can be\nstored at the address passed for that conversion---in other words,\nthe address of a\nvariable with the correct type must be\nprovided.  Modern compilers often detect missing { &} operators\nand incorrect variable types, but many only give warnings to the\nprogrammer.  The { scanf} function itself cannot tell whether\nthe arguments given to it are valid or not. \n Question: \nQ. What does the author mean when they say that \"main is not necessarily a function in the mathematical sense\"? \n Answer: \n\nA. The author means that the value returned from main is not necessarily unique for a given set of input values to the program."}, {"text": "Title: Basic I/O P29 \n Text: If a conversion fails---for example, if a user types ``hello'' when\n{ scanf} expects an integer---{ scanf} does not overwrite the\ncorresponding variable and immediately stops trying to convert input. \n Question: \nQ. What does the program do?\nQ. \nQ. The program selects a number from 1 to 10 at random and returns the number to the operating system. \n Answer: "}, {"text": "Title: Basic I/O P30 \n Text: The { scanf} function returns the number of successful \nconversions, allowing a programmer to check for bad input from\nthe user. \n Question: \nQ. How many binary digits are needed to represent an integer? \n Answer: \n\nA. The number of binary digits needed to represent an integer is dependent on the number itself. For example, the integer 12 can be represented with 4 binary digits (1100), but the integer 20 can be represented with 5 binary digits (10100)."}, {"text": "Title: Types of Statements in C P0 \n Text: Each statement in a C program specifies a complete operation. \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: Types of Statements in C P1 \n Text: There are three types of statements, but two of these types can\nbe constructed from additional statements, which can in turn be\nconstructed from additional statements.  The C language specifies\nno bound on this type of recursive construction, but code \nreadability does impose a practical limit. \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: Types of Statements in C P2 \n Text: \nThe three types are shown to the right.\nThey are the { null statement}, \n{ simple statements}, \nand { compound statements}. \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: Types of Statements in C P3 \n Text: A null statement is just a semicolon, and a compound statement \nis just a sequence of statements surrounded by braces. \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: Types of Statements in C P4 \n Text: Simple statements can take several forms.  All of the examples\nshown to the right, including the call to { printf}, are\nsimple state- \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: Types of Statements in C P5 \n Text: aaaa=aaaaaaaaaaaaaa=/* =aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa=\n;   > > /* >a null statement (does nothing) >*/ \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: Types of Statements in C P6 \n Text: A = B; > > /*  >examples of simple statements >*/\nprintf (\"Hello, world!n\"); \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: Types of Statements in C P7 \n Text: {    > > /* > a compound statement >*/ \n>  C = D; > /* > (a sequence of statements >*/\n>  N = 4; > /* > between braces) >*/ \n>  L = D - N;\n} \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: Types of Statements in C P8 \n Text: \nments consisting of a C expression followed by a \nsemicolon. \n Question: \nQ. What is the difference between implicit and explicit data type conversion in C? \n Answer: \n\nA.\n\nImplicit type conversion in C is when the compiler automatically converts one data type to another, without the programmer specifying any conversion code. This can happen when an operator is applied to operands of different types, or when a function is called with arguments of different types. Explicit type conversion, on the other hand, is when the programmer explicitly specifies code to convert a data type to another."}, {"text": "Title: Types of Statements in C P9 \n Text: Simple statements can also consist of conditionals or iterations, which\nwe introduce next. \n Question: \nQ. What did the text say about the sections marked with an asterisk?\nQ. \nQ. The text said that the sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. \n Answer: "}, {"text": "Title: Types of Statements in C P10 \n Text: Remember that after variable declarations, the { main} function\ncontains a sequence of statements.  These statements are executed one\nat a time in the order given in the program, as shown to the right\nfor two statements.  We say that the statements are executed in\nsequential order. \n Question: \nQ. What is the semantic gap? \n Answer: \n\nA. The semantic gap is the difference between human descriptions of problems and the relatively simple instructions that can be provided by an instruction set architecture (ISA)."}, {"text": "Title: Types of Statements in C P11 \n Text: A program must also be able to execute statements only when \nsome condition holds.  In the C language, such a condition can be\nan arbitrary expression.  The expression is first evaluated.\nIf the result is 0, the condition\nis considered to be false.  Any result other than 0 is considered\nto be true.  The C statement\nfor conditional execution is called an { if} \n Question: \nQ. What is the C programming language? \n Answer: \n\nA. The C programming language is a language that was developed by Dennis Ritchie in order to simplify the task of writing the Unix operating system. The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class."}, {"text": "Title: Types of Statements in C P12 \n Text: \nstatement.  Syntactically, we put the expression for the condition\nin parentheses after the keyword { if} and follow the parenthesized\nexpression with a compound statement containing the statements\nthat should be executed when the condition is true.  Optionally,\nwe can append the keyword { else} and a second compound\nstatement containing statements to be executed when the condition\nevaluates to false.  \nThe corresponding flow chart is shown to the right. \n Question: \nQ. What is the name of the language that C++, Java, and Perl are based on? \n Answer: \n\nA. C"}, {"text": "Title: Types of Statements in C P13 \n Text: aaaa=aaaaaaaaaaaa=\n/* Set the variable y to the absolute value of variable x. */\nif (0 <= x) {> >/* Is x greater or equal to 0? */\n> y = x;     >/* Then block: assign x to y. */\n} else {\n> y = -x;    >/* Else block: assign negative x to y. */\n} \n Question: \nQ. What is the purpose of a variable declaration in a C program?\nQ. \nQ. A variable declaration in a C program tells the compiler what type of value a variable will hold, and what its name will be. This allows the compiler to generate code that accesses the variable correctly. \n Answer: "}, {"text": "Title: Types of Statements in C P14 \n Text: If instead we chose to assign the absolute value of variable { x}\nto itself, we can do so without an { else} block: \n Question: \nQ. What is the value of the answer variable?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: Types of Statements in C P15 \n Text: aaaa=aaaaaaaaaaaa=\n/* Set the variable x to its absolute value. */\nif (0 > x) {> >/* Is x less than 0? */\n> x = -x;    >/* Then block: assign negative x to x. */\n}            >> /* No else block is given--no work is needed. */ \n Question: \nQ. What is the answer to the question?\nQ. \nQ. The answer to the question is d. \n Answer: n"}, {"text": "Title: Types of Statements in C P16 \n Text: \nFinally, we sometimes need to repeatedly execute a set of statements,\neither a fixed number of times or so long as some condition holds.\nWe refer to such repetition as an { iteration} or a { loop}.\nIn our class, we make use of C's { for} loop when we need to\nperform such a task.  A { for} loop is structured as follows: \n Question: \nQ. What is the value of the variable \"x\" at the end of the program?\nQ. \nQ. The value of the variable \"x\" at the end of the program is 0. \n Answer: "}, {"text": "Title: Types of Statements in C P17 \n Text: for ([initialization] ; [condition] ; [update]) {\n    [subtask to be repeated]\n} \n Question: \nQ. What is the main function of a C program?\nQ. \nQ. The main function of a C program is to execute when the program starts. \n Answer: "}, {"text": "Title: Types of Statements in C P18 \n Text: A flow chart corresponding to execution of a { for} loop appears\nto the right.  First, any initialization is performed.  Then the\ncondition---again an arbitrary C expression---is checked.  If the\ncondition evaluates to false (exactly 0), the loop is done.  Otherwise,\nif the condition evaluates to true (any non-zero value),\nthe statements in the compound statement, the subtask or { loop body},\nare executed.  The loop body can contain anything: a sequence of simple \nstatements, a conditional, another loop, or even just an empty list.\nOnce the loop body has finished executing, the { for} loop\nupdate rule is executed.  Execution then checks the condition again,\nand this process repeats until the condition evaluates to 0.\nThe { for} loop below, for example, prints the numbers \nfrom 1 to 42. \n Question: \nQ. What is the value of the answer variable when the program terminates?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: Types of Statements in C P19 \n Text: /* Print the numbers from 1 to 42. */\nfor (i = 1; 42 >= i; i = i + 1) {\n    printf (\"dn\", i);\n} \n Question: \nQ. What does the C language ignore?\nQ. \nQ. The C language ignores white space in programs. \n Answer: "}, {"text": "Title: Program Execution P0 \n Text: \nWe are now ready to consider the execution of a simple program,\nillustrating how variables change value from step to step and\ndetermine program behavior. \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: Program Execution P1 \n Text: Let's say that two numbers are ``friends'' if they have at least one\n1 bit in common when written in base 2.  So, for example, 100_2 and \n111_2 are friends because both numbers have a 1 in the bit with \nplace value 2^2=4.  Similarly, 101_2 and 010_2 are not friends,\nsince no bit position is 1 in both numbers. \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: Program Execution P2 \n Text: The program to the right prints all friendships between numbers\nin the interval [0,7]. \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: Program Execution P3 \n Text: aaaa=aaaa=aaaaaaaaaa=/* a second number to consider as check's friend =\nint\nmain ()\n{\n>  int > check;  > /* number to check for friends > */\n>  int > friend; > /* a second number to consider as check's friend > */\naaaa=aaaa=aaaa=aaaa=\n>  \n>  /* Consider values of check from 0 to 7. */\n>  for (check = 0; 8 > check; check = check + 1) { \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: Program Execution P4 \n Text: >  >  /* Consider values of friend from 0 to 7. */\n>  >  for (friend = 0; 8 > friend; friend = friend + 1) { \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: Program Execution P5 \n Text: >  >  >  /* Use bitwise AND to see if the two share a 1 bit. */\n>  >  >  if (0 != (check & friend)) { \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: Program Execution P6 \n Text: >  >  >  >  /* We have friendship! */\n>  >  >  >  printf (\"d and d are friends.n\", check, friend);\n>  >  >  }\n>  >  }\n>  }\n} \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: Program Execution P7 \n Text: \nThe program uses two\ninteger variables, one for each of the numbers that we consider.\nWe use a { for} loop to iterate over all values of our first\nnumber, which we call { check}.  The loop initializes { check}\nto 0, continues until check reaches 8, and adds 1 to check after\neach loop iteration.  We use a similar { for} loop to iterate\nover all possible values of our second number, which we call { friend}.\nFor each pair of numbers, we determine whether they are friends\nusing a bitwise AND operation.  If the result is non-zero, they\nare friends, and we print a message.  If the two numbers are not\nfriends, we do nothing, and the program moves on to consider the\nnext pair of numbers. \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: Program Execution P8 \n Text: \nNow let's think about what happens when this program executes. \n Question: \nQ. What is the difference between implicit and explicit data type conversion in C? \n Answer: \n\nA.\n\nImplicit type conversion in C is when the compiler automatically converts one data type to another, without the programmer specifying any conversion code. This can happen when an operator is applied to operands of different types, or when a function is called with arguments of different types. Explicit type conversion, on the other hand, is when the programmer explicitly specifies code to convert a data type to another."}, {"text": "Title: Program Execution P9 \n Text: When the program starts, both variables are filled with random bits,\nso their values are unpredictable.   \n Question: \nQ. What did the text say about the sections marked with an asterisk?\nQ. \nQ. The text said that the sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. \n Answer: "}, {"text": "Title: Program Execution P10 \n Text: The first step is the initialization of the first { for} loop, which\nsets { check} to 0. \n Question: \nQ. What is the semantic gap? \n Answer: \n\nA. The semantic gap is the difference between human descriptions of problems and the relatively simple instructions that can be provided by an instruction set architecture (ISA)."}, {"text": "Title: Program Execution P11 \n Text: The condition for that loop is { 8 > check}, which is true, so\nexecution enters the loop body and starts to execute the first\nstatement, which is our second { for} loop. \n Question: \nQ. What is the C programming language? \n Answer: \n\nA. The C programming language is a language that was developed by Dennis Ritchie in order to simplify the task of writing the Unix operating system. The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class."}, {"text": "Title: Program Execution P12 \n Text: The next step is then the initialization code for the second { for}\nloop, which sets { friend} to 0. \n Question: \nQ. What is the name of the language that C++, Java, and Perl are based on? \n Answer: \n\nA. C"}, {"text": "Title: Program Execution P13 \n Text: The condition for the second loop is { 8 > friend}, which is true, so\nexecution enters the loop body and starts to execute the first\nstatement, which \n Question: \nQ. What is the purpose of a variable declaration in a C program?\nQ. \nQ. A variable declaration in a C program tells the compiler what type of value a variable will hold, and what its name will be. This allows the compiler to generate code that accesses the variable correctly. \n Answer: "}, {"text": "Title: Program Execution P14 \n Text: after executing...& { check} is...& and { friend} is... \n(variable declarations)& unpredictable bits& unpredictable bits\n{ check = 0}& 0& unpredictable bits\n{ 8 > check}& 0& unpredictable bits\n{ friend = 0}& 0& 0\n{ 8 > friend}& 0& 0\n{ if (0 != (check & friend))}& 0& 0\n{ friend = friend + 1}& 0& 1\n{ 8 > friend}& 0& 1\n{ if (0 != (check & friend))}& 0& 1\n{ friend = friend + 1}& 0& 2\n{(repeat last three lines six more times; number 0 has no friends!)}\n{ 8 > friend}& 0& 8\n{ check = check + 1}& 1& 8\n{ 8 > check}& 1& 8\n{ friend = 0}& 1& 0\n{ 8 > friend}& 1& 0\n{ if (0 != (check & friend))}& 1& 0\n{ friend = friend + 1}& 1& 1\n{ 8 > friend}& 1& 1\n{ if (0 != (check & friend))}& 1& 1\n{ printf ...}& 1& 1\n{(our first friend!?)} \n Question: \nQ. What is the value of the answer variable?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: Program Execution P15 \n Text: is the { if} statement. \n Question: \nQ. What is the answer to the question?\nQ. \nQ. The answer to the question is d. \n Answer: n"}, {"text": "Title: Program Execution P16 \n Text: Since both variables are 0, the { if} condition is false, and\nnothing is printed. \n Question: \nQ. What is the value of the variable \"x\" at the end of the program?\nQ. \nQ. The value of the variable \"x\" at the end of the program is 0. \n Answer: "}, {"text": "Title: Program Execution P17 \n Text: Having finished the loop body for the inner loop (on { friend}),\nexecution continues with the update rule for that loop---{ friend = \nfriend + 1}---then returns to check the loop's condition again. \n Question: \nQ. What is the main function of a C program?\nQ. \nQ. The main function of a C program is to execute when the program starts. \n Answer: "}, {"text": "Title: Program Execution P18 \n Text: This process repeats, always finding that the number 0 (in { check})\nis not\nfriends (0 has no friends!) until { friend} reaches 8, at which\npoint the inner loop condition becomes false. \n Question: \nQ. What is the value of the answer variable when the program terminates?\nQ. \nQ. 42 \n Answer: "}, {"text": "Title: Program Execution P19 \n Text: Execution then moves to the update rule for the first { for} loop,\nwhich increments { check}.  Check is then compared with 8 to\nsee if the loop is done.  Since it is not, we once again enter the\nloop body and start the second { for} loop over.  The initialization\ncode again sets { friend} to 0, and we move forward as before.\nAs you see above, the first time that we find our { if} condition\nto be true is when both { check} and { friend} are equal to 1. \n Question: \nQ. What does the C language ignore?\nQ. \nQ. The C language ignores white space in programs. \n Answer: "}, {"text": "Title: Program Execution P20 \n Text: Is that result what you expected?  To learn that the number 1 is\nfriends with itself?  If so, the program works.  If you assumed that\nnumbers could not be friends with themselves, perhaps we should fix the \nbug?  We could, for example, add another { if} statement to \navoid printing anything when { check == friend}. \n Question: \nQ. What is the main purpose of the variables defined in the main function?\nQ. \nQ. The main purpose of the variables defined in the main function is to associate symbolic names with specific types of data. \n Answer: "}, {"text": "Title: Program Execution P21 \n Text: Our program, you might also realize, prints each pair of friends twice.\nThe numbers 1 and 3, for example, are printed in both possible orders.  To\neliminate this redundancy, we can change the initialization in the \nsecond { for} loop, either to { friend = check} or to\n{ friend = check + 1}, depending on how we want to define friendship\n(the same question as before: can a number be friends with itself?). \n Question: \nQ. What is the range of values that can be stored in a 32-bit 2's complement number?\nQ. \nQ. The range of values that can be stored in a 32-bit 2's complement number is -2147483648 to 2147483647. \n Answer: "}, {"text": "Title: Compilation and Interpretation* P0 \n Text: \nMany programming languages, including C, can be \n{ compiled}, which means that the program is converted into \ninstructions for a particular ISA before the program is run\non a processor that supports that ISA.\nThe figure to the right illustrates the compilation process for \nthe C language.   \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: Compilation and Interpretation* P1 \n Text: In this type of figure, files and other data are represented as cylinders,\nwhile rectangles represent processes, which are usually implemented in \nsoftware. \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: Compilation and Interpretation* P2 \n Text: In the figure to the right, the outer dotted box represents the full \ncompilation\nprocess that typically occurs when one compiles a C program.\nThe inner dotted box represents the work performed by the { compiler}\nsoftware itself. \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: Compilation and Interpretation* P3 \n Text: The cylinders for data passed between the processes that compose\nthe full compilation process\nhave been left out of the figure; instead, we have written the type\nof data being passed next to the arrows that indicate the flow of information\nfrom one process to the next. \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: Compilation and Interpretation* P4 \n Text: The C preprocessor (described later in these notes) forms the\nfirst step in the compilation process.  The preprocessor\noperates on the program's { source code} along\nwith { header files} that describe data types and\noperations.  The preprocessor merges these together\ninto a single file of preprocessed source code.  The preprocessed\nsource code is then analyzed by the front end of the compiler based on the\nspecific programming language being used (in our case, the C language),\nthen converted by the back end of the compiler\ninto instructions for the desired ISA.  The output of a compiler\nis not binary instructions, however, but is instead\na human-readable form of instructions called { assembly code},\nwhich we cover in the last quarter of our class.  A tool called\nan assembler then converts these human-readable instructions into\nbits that a processor can understand.  If a program consists of\nmulti- \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: Compilation and Interpretation* P5 \n Text: ple source files, or needs to make use of additional \npre-programmed operations (such as math functions, graphics, or sound),\na tool called a linker merges the object code of the program with\nthose additional elements to form the final { executable image}\nfor the program.  The executable image is typically then stored on\na disk, from which it can later be read into memory in order to\nallow a processor to execute the program. \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: Compilation and Interpretation* P6 \n Text: Some languages are difficult or even impossible to compile.  Typically, the\nbehavior of these languages depends on input data that are only available \nwhen the program runs.  Such languages can be { interpreted}: each step \nof the algorithm described by a program is executed by a software interpreter\nfor the language.  Languages such as Java, Perl, and Python are usually\ninterpreted.  Similarly, when we use software to simulate one ISA using\nanother ISA, as we do at the end of our class with the {LC-3}\nISA described by the textbook, the simulator is a form of interpreter.\nIn the lab, you will use a simulator compiled into and executing as x86 \ninstructions in order to interpret {LC-3} instructions.   \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: Compilation and Interpretation* P7 \n Text: While a program is executing in an interpreter, enough information\nis sometimes available to compile part or all of the program to\nthe processor's ISA as the program runs, \na technique known as { ``just in time'' ( JIT) compilation}. \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: The C Preprocessor* P0 \n Text: The C language uses a preprocessor to support inclusion of common\ninformation (stored in header files) into multiple source files. \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: The C Preprocessor* P1 \n Text: The most frequent use of the preprocessor is to enable the unique\ndefinition of new data types and operations within\nheader files that can then be included by reference within source\nfiles that make use of them.  This capability is based on the \n{ include directive}, { #include}, as shown here:   \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: The C Preprocessor* P2 \n Text: \n#include \"my_header.h\"    = /* search in current followed by standard directories =\n#include <stdio.h>      > /* search in standard directories > */\n#include \"my_header.h\" > /* search in current followed by standard directories > */ \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: The C Preprocessor* P3 \n Text: The preprocessor also supports integration of compile-time constants\ninto source files before compilation.  For example, many\nsoftware systems allow the definition of a symbol such as { NDEBUG}\n(no debug) to compile without additional debugging code included in\nthe sources.   \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: The C Preprocessor* P4 \n Text: Two directives are necessary for this purpose: the { define directive},\n{ #define}, which\nprovides a text-replacement facility, and { conditional inclusion} (or\nexclusion) of parts of a file within { #if}/{ #else}/{\n#endif} directives. \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: The C Preprocessor* P5 \n Text: These directives are also useful in allowing \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: The C Preprocessor* P6 \n Text: \na single header file to\nbe included multiple times without causing problems, as C does not\nallow redefinition of types, variables, and so forth, even if the\nredundant \ndefinitions are identical.  Most header files are thus wrapped as shown\nto the right. \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: The C Preprocessor* P7 \n Text: #if !defined(MY_HEADER_H)\n#define MY_HEADER_H\n/* actual header file material goes here */\n#endif /* MY_HEADER_H */ \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: The C Preprocessor* P8 \n Text: \nThe preprocessor performs a simple linear pass on the source and does\nnot parse or interpret any C syntax. \n Question: \nQ. What is the difference between implicit and explicit data type conversion in C? \n Answer: \n\nA.\n\nImplicit type conversion in C is when the compiler automatically converts one data type to another, without the programmer specifying any conversion code. This can happen when an operator is applied to operands of different types, or when a function is called with arguments of different types. Explicit type conversion, on the other hand, is when the programmer explicitly specifies code to convert a data type to another."}, {"text": "Title: The C Preprocessor* P9 \n Text: Definitions for text replacement are valid as soon as they are defined\nand are performed until they are undefined or until the end of the\noriginal source file. \n Question: \nQ. What did the text say about the sections marked with an asterisk?\nQ. \nQ. The text said that the sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. \n Answer: "}, {"text": "Title: The C Preprocessor* P10 \n Text: The preprocessor does recognize spacing and will not replace part of a\nword, thus ``{ #define i 5}'' will not wreak havoc on your {\nif} statements, but will cause problems if you name any variable { i}. \n Question: \nQ. What is the semantic gap? \n Answer: \n\nA. The semantic gap is the difference between human descriptions of problems and the relatively simple instructions that can be provided by an instruction set architecture (ISA)."}, {"text": "Title: The C Preprocessor* P11 \n Text: Using the text replacement capabilities of the preprocessor does have\ndrawbacks, most importantly in that almost none of the information is\npassed on for debugging purposes.   \n Question: \nQ. What is the C programming language? \n Answer: \n\nA. The C programming language is a language that was developed by Dennis Ritchie in order to simplify the task of writing the Unix operating system. The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class."}, {"text": "Title: Changing Types in C* P0 \n Text: Changing the type of a datum is necessary from time to time, but\nsometimes a compiler can do the work for you. \n Question: \nQ. What are the benefits of learning the C programming language? \n Answer: \n\nSome benefits of learning the C programming language include gaining an understanding of syntax and meaning in a high-level language, as well as becoming familiar with basic concepts in computer programming. C can also be used to write programs and validate digital system design material throughout the semester, which can make the material easier to master."}, {"text": "Title: Changing Types in C* P1 \n Text: The most common form of { implicit type conversion} occurs with binary\narithmetic operations.  Integer arithmetic in C always uses types of\nat least the size of { int}, and all floating-point arithmetic uses\n{ double}. \n Question: \nQ. Who were the two main characters in the story? \n Answer: \n\nA. The two main characters in the story were Rakesh and his father."}, {"text": "Title: Changing Types in C* P2 \n Text: If either or both operands have smaller integer types, or differ from\none another, the compiler implicitly converts them before performing\nthe operation, and the type of the result may be different from those of\nboth operands. \n Question: \nQ. How are data types used in high-level languages related to representations? \n Answer: \n\nA. Data types in high-level languages are related to representations in that they define the structure of data that is stored in memory. This structure is then used by the compiler to generate the machine code that will be executed by the processor."}, {"text": "Title: Changing Types in C* P3 \n Text: In general, the compiler selects the final type according to some\npreferred ordering in which floating-point is preferred over integers,\nunsigned values are preferred over signed values, and more bits are\npreferred over fewer bits. \n Question: \nQ. What are some of the operators available to the programmer in C?\nQ. \nQ. Some of the operators available to the programmer in C include arithmetic and logic operators. \n Answer: "}, {"text": "Title: Changing Types in C* P4 \n Text: The type of the result must be at least as large as either argument,\nbut is also at least as large as an { int} for integer operations\nand a { double} for floating-point operations. \n Question: \nQ. How does a programmer create a statement for conditional execution in C?\nQ. \nQ. A programmer can create a statement for conditional execution in C by using the if statement. \n Answer:  The if statement allows a programmer to specify a condition that must be met in order for a certain block of code to be executed."}, {"text": "Title: Changing Types in C* P5 \n Text: Modern C compilers always extend an integer type's bit width before\nconverting from signed to unsigned.  The original C specification\ninterleaved bit width extensions to { int} with sign changes, thus\n{ older compilers may not be consistent, and implicitly require\nboth types of conversion in a single operation may lead to portability\nbugs.} \n Question: \nQ. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program. \n Answer: \n\nA. This is correct. Algebra and programming are two very different things. In algebra, variables are used to represent numbers or values that can change. In programming, variables are used to store data that can be used by the program."}, {"text": "Title: Changing Types in C* P6 \n Text: The implicit extension to { int} can also be confusing in the sense\nthat arithmetic that seems to work on smaller integers fails with\nlarger ones.  For example, multiplying two 16-bit integers set to 1000\nand printing the result works with most compilers because the 32-bit \n{ int} result is wide enough to hold the right answer.  In contrast,\nmultiplying two 32-bit integers set to 100,000 produces the wrong\nresult because the high bits of the result are discarded before it can\nbe converted to a larger type.  For this operation to produce the\ncorrect result, one of the integers must be converted explicitly (as\ndiscussed later) before the multiplication. \n Question: \nQ. What is the compilation process? \n Answer: \n\nThe compilation process is a process that transforms a program written in a high-level language into instructions."}, {"text": "Title: Changing Types in C* P7 \n Text: Implicit type conversions also occur due to assignments.  Unlike\narithmetic conversions, the final type must match the left-hand side\nof the assignment (for example, a variable to which a result is assigned), and\nthe compiler simply performs any necessary conversion. \n Question: \nQ. What does the C preprocessor do?\nQ. \nQ. The C preprocessor is a program that processes C source code before it is compiled. It handles the inclusion of header files, macro expansions, and conditional compilation. \n Answer: "}, {"text": "Title: Changing Types in C* P8 \n Text: { Since the desired type may be smaller than the type of the value\nassigned, information can be lost.}  Floating-point values are\ntruncated when assigned to integers, and high bits of wider integer\ntypes are discarded when assigned to narrower integer types.  { Note\nthat a positive number may become a negative number when bits are\ndiscarded in this manner.} \n Question: \nQ. What is the difference between implicit and explicit data type conversion in C? \n Answer: \n\nA.\n\nImplicit type conversion in C is when the compiler automatically converts one data type to another, without the programmer specifying any conversion code. This can happen when an operator is applied to operands of different types, or when a function is called with arguments of different types. Explicit type conversion, on the other hand, is when the programmer explicitly specifies code to convert a data type to another."}, {"text": "Title: Changing Types in C* P9 \n Text: Passing arguments to functions can be viewed as a special case of\nassignment.  Given a function prototype, the compiler knows the type\nof each argument and can perform conversions as part of the code\ngenerated to pass the arguments to the function.  Without such a\nprototype, or for functions with variable numbers of arguments, the\ncompiler lacks type information and thus cannot perform necessary\nconversions, leading to unpredictable behavior.  By default, however,\nthe compiler extends any integer smaller than an { int}\nto the width of an { int} and converts { float} to\n{ double}. \n Question: \nQ. What did the text say about the sections marked with an asterisk?\nQ. \nQ. The text said that the sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. \n Answer: "}, {"text": "Title: Changing Types in C* P10 \n Text: \nOccasionally it is convenient to use an { explicit type cast} to force\nconversion from one type to another.  { Such casts must be used\nwith caution, as they silence many of the warnings that a compiler\nmight otherwise generate when it detects potential problems.}  One\ncommon use is to promote integers to floating-point before an\narithmetic operation, as shown to the right. \n Question: \nQ. What is the semantic gap? \n Answer: \n\nA. The semantic gap is the difference between human descriptions of problems and the relatively simple instructions that can be provided by an instruction set architecture (ISA)."}, {"text": "Title: Changing Types in C* P11 \n Text: aaaa=\nint\nmain ()\n{\n>  int numerator = 10;\n>  int denominator = 20;\n>\n>  printf (\"fn\", numerator / (double)denominator);\n>  return 0;\n} \n Question: \nQ. What is the C programming language? \n Answer: \n\nA. The C programming language is a language that was developed by Dennis Ritchie in order to simplify the task of writing the Unix operating system. The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class."}, {"text": "Title: Changing Types in C* P12 \n Text: The type to which a value is to be converted\nis placed in parentheses in front of the value.  In most cases,\nadditional parentheses should be used to avoid confusion about the\nprecedence of type conversion over other operations. \n Question: \nQ. What is the name of the language that C++, Java, and Perl are based on? \n Answer: \n\nA. C"}, {"text": "Title: Summary of Part 1 of the Course P0 \n Text: This short summary provides a list of both terms that we expect you to\nknow and and skills that we expect you to have after our first few weeks\ntogether.  The first part of the course is shorter than the other three\nparts, so the amount of material is necessarily less. \n Question: \nQ. What is the difference between a term and a skill? \n Answer: \n\nA. A term is a word or phrase that has a specific meaning within a particular context, while a skill is a learned ability to perform a task."}, {"text": "Title: Summary of Part 1 of the Course P1 \n Text: These notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely. \n Question: \nQ. Q: Can you provide an example of a time series that would be stationary? \nQ. \nQ. A: A stationary time series is one whose statistical properties do not change over time. An example of a stationary time series is a sine wave. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P2 \n Text: According to educational theory, the difficulty of learning depends on \nthe type of task involved.  Remembering new terminology is relatively \neasy, while applying the ideas underlying design decisions shown by \nexample to new problems posed as human tasks is relatively hard. \n Question: \nQ. What is the most difficult type of task to learn, according to educational theory?\nQ. \nQ. Applying the ideas underlying design decisions shown by example to new problems posed as human tasks is the most difficult type of task to learn, according to educational theory. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P3 \n Text: In this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth). \n Question: \nQ. How does the author feel about the city?\nQ. \nQ. The author seems to enjoy living in the city and finds it to be a pleasant place to live. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P4 \n Text: This time, we'll list the skills first and leave the easy stuff for the \nnext page. \n Question: \nQ. What does the author think is the \"easy stuff\"?\nQ. \nQ. What does the author think is the \"easy stuff\"? \n Answer: \n\nThe author thinks that the \"easy stuff\" is the skills that are needed to be a successful software engineer."}, {"text": "Title: Summary of Part 1 of the Course P5 \n Text: We expect you to be able to exercise the following skills: \n Question: \nQ. What is the main point of the text?\nQ. \nQ. What are the consequences of not following the advice in the text? \n Answer: \n\nIf you do not follow the advice in the text, you may end up getting sick or injured."}, {"text": "Title: Summary of Part 1 of the Course P6 \n Text: {Represent decimal numbers with unsigned, 2's complement, and IEEE\nfloating-point representations, and be able to calculate the decimal value\nrepresented by a bit pattern in any of these representations.} \n Question: \nQ. How would you represent the decimal number 12 in IEEE floating-point format? \n Answer: \n\nA. The IEEE floating-point format would represent the decimal number 12 as 1.200000*10^1"}, {"text": "Title: Summary of Part 1 of the Course P7 \n Text: {Be able to negate a number represented in the 2's complement\nrepresentation.} \n Question: \nQ. What is the 2's complement representation? \n Answer: \n\nA.\n\nThe 2's complement representation is a way of representing negative numbers in binary. To get the 2's complement of a number, you first take the number's complement (i.e. flip all the bits) and then add 1."}, {"text": "Title: Summary of Part 1 of the Course P8 \n Text: {Perform simple arithmetic by hand on unsigned and 2's complement\nrepresentations, and identify when overflow occurs.} \n Question: \nQ. How would you perform simple arithmetic on an unsigned number? \n Answer: \n\nA. You would simply add, subtract, multiply, or divide the numbers as usual. Overflow occurs when the result is too large to fit in the allotted number of bits."}, {"text": "Title: Summary of Part 1 of the Course P9 \n Text: {Be able to write a truth table for a Boolean expression.} \n Question: \nQ. What is a truth table? \n Answer: \n\nA truth table is a table that shows the truth value of a Boolean expression for every possible combination of values of the variables in the expression."}, {"text": "Title: Summary of Part 1 of the Course P10 \n Text: {Be able to write a Boolean expression as a sum of minterms.} \n Question: \nQ. What is a Boolean expression? \n Answer: \n\nA Boolean expression is an expression that evaluates to a Boolean value, that is, either true or false. A common way to represent a Boolean expression is as a sum of minterms."}, {"text": "Title: Summary of Part 1 of the Course P11 \n Text:  {Be able to calculate the Hamming distance of a code/representation.}\n \n {Know the relationships between Hamming distance and the abilities\n to detect and to correct bit errors.} \n Question: \nQ. What is the Hamming distance of a code/representation? \n Answer: \n\nA. The Hamming distance of a code/representation is the number of bit errors that can be detected and corrected by that code/representation."}, {"text": "Title: Summary of Part 1 of the Course P12 \n Text: {Know how to declare and initialize C variables with one of the \nprimitive data types.} \n Question: \nQ. What is the difference between declaring and initializing a C variable?\nQ. \nQ. When you declare a C variable, you are simply telling the compiler that a variable exists and is of a certain data type. When you initialize a C variable, you are giving it an initial value. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P13 \n Text: At a more abstract level, we expect you to be able to: \n Question: \nQ. How does the author feel about the subject?\nQ. \nQ. The author seems to be very passionate about the subject. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P14 \n Text: {Understand the value of using a common mathematical basis, such\nas modular arithmetic, in defining multiple representations (such as\nunsigned and 2's complement).} \n Question: \nQ. What is the value of using a common mathematical basis, such as modular arithmetic, in defining multiple representations (such as unsigned and 2's complement)? \n Answer: \n\nA. The value of using a common mathematical basis, such as modular arithmetic, in defining multiple representations (such as unsigned and 2's complement) is that it allows for a more consistent and efficient representation of data. It also makes it easier to reason about and manipulate the data."}, {"text": "Title: Summary of Part 1 of the Course P15 \n Text: {Write Boolean expressions for the overflow conditions\non both unsigned and 2's complement addition.} \n Question: \nQ. What are the overflow conditions for unsigned and 2's complement addition? \n Answer: \n\nA.\n\nThe overflow conditions for unsigned addition are when the sum of two unsigned numbers exceeds the maximum value that can be represented by the data type. For example, if two unsigned numbers are added together and the result is greater than 255 (the maximum value that can be represented by an 8-bit unsigned integer), then an overflow has occurred.\n\nThe overflow conditions for 2's complement addition are when the sum of two 2's complement numbers exceeds the maximum value that can be represented by the data type, or when the sum of two 2's complement numbers is less than the minimum value that can be represented by the data type. For example, if two 8-bit 2's complement numbers are added together and the result is greater than 127 (the maximum value that can be represented by an 8-bit 2's complement integer) or less than -128 (the minimum value that can be represented by an 8-bit 2's complement integer), then an overflow has occurred."}, {"text": "Title: Summary of Part 1 of the Course P16 \n Text:  {Be able to use parity for error detection, and Hamming codes for\n error correction.} \n Question: \nQ. What is parity? \n Answer: \n\nA. Parity is a method of error detection that involves adding an extra bit to a block of data. This bit is set to 1 if the number of 1 bits in the block is odd, and 0 if the number of 1 bits is even. If the parity bit is incorrect, it indicates that an error has occurred."}, {"text": "Title: Summary of Part 1 of the Course P17 \n Text: {Be able to write single { if} statements and { for} loops\nin C in order to perform computation.} \n Question: \nQ. What is the purpose of writing single if statements and for loops in C? \n Answer: \n\nA. The purpose of writing single if statements and for loops in C is to perform computation."}, {"text": "Title: Summary of Part 1 of the Course P18 \n Text: {Be able to use { scanf} and { printf} for basic input and \noutput in C.} \n Question: \nQ. What is the difference between scanf and printf? \n Answer: \n\nA. Scanf is used for input, while printf is used for output."}, {"text": "Title: Summary of Part 1 of the Course P19 \n Text: And, at the highest level, we expect that you will be able to reason about\nand analyze problems in the following ways: \n Question: \nQ. How does the author feel about the subject of the text? \n Answer: \n\nA. The author feels positively about the subject of the text."}, {"text": "Title: Summary of Part 1 of the Course P20 \n Text: {Understand the tradeoffs between integer\n  FIXME?     not covered by book nor notes currently \n, fixed-point,    \nand floating-point representations for numbers.} \n Question: \nQ. What are the tradeoffs between integer, fixed-point, and floating-point representations for numbers? \n Answer: \n\nA.\n\nInteger representations have the advantage of being very simple and straightforward to understand and use. However, they can be limited in terms of the range of values that they can represent, and may not be well suited for certain types of calculations.\n\nFixed-point representations offer more flexibility than integer representations, but can be more difficult to work with. They can also be less accurate than floating-point representations, especially when representing very small or large numbers.\n\nFloating-point representations provide the most flexibility and accuracy of the three types, but can be more complex to work with. They are also more likely to suffer from rounding errors, which can impact the accuracy of calculations."}, {"text": "Title: Summary of Part 1 of the Course P21 \n Text: {Understand logical completeness and be able to prove or disprove\nlogical completeness for sets of logic functions.} \n Question: \nQ. How would you prove or disprove logical completeness for a set of logic functions? \n Answer: \n\nA. To prove or disprove logical completeness for a set of logic functions, one would need to show that for any given formula of the functions in question, there is a way to determine whether that formula is true or false."}, {"text": "Title: Summary of Part 1 of the Course P22 \n Text:  PARTIALLY MOVED TO PART 4 \n Question: \nQ. Though many animals use camouflage to remain hidden from predators or prey, the peacock mantis shrimp is one of the few that uses the technique to attack.\nQ. \nQ. Why does the peacock mantis shrimp use camouflage to attack? \n Answer: \n\nThe peacock mantis shrimp uses camouflage to attack because it allows the shrimp to blend in with its surroundings and approach its prey undetected."}, {"text": "Title: Summary of Part 1 of the Course P23 \n Text:  {Understand the properties necessary in a representation, and understand\n basic tradeoffs in the sparsity of code words with error detection and\n correction capabilities.}\n{Understand the properties necessary in a representation: no ambiguity\nin meaning for any bit pattern, and agreement in advance on the meanings of \nall bit patterns.} \n Question: \nQ. How does one achieve agreement on the meanings of all bit patterns in advance? \n Answer: \n\nA. One way to achieve agreement on the meanings of all bit patterns is to use a standard, such as a coding standard."}, {"text": "Title: Summary of Part 1 of the Course P24 \n Text: {Analyze a simple, single-function C program and be able to explain its purpose.} \n Question: \nQ. What is the purpose of the C program?\nQ. \nQ. The purpose of the C program is to generate a question about a given text. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P25 \n Text: You should recognize all of these terms\nand be able to explain what they mean.\n(the parentheses give page numbers,\nor ``P&P'' for Patt & Patel). \n Question: \nQ. \nQ. What is the difference between a local and global variable in Java? \n Answer: \n\nA. A local variable is only accessible within the method in which it is declared. A global variable is accessible throughout the program."}, {"text": "Title: Summary of Part 1 of the Course P26 \n Text: Actually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  (You may skip the *'d terms in Fall 2012.) \n Question: \nQ. What was the main idea of the text?\nQ. \nQ. The main idea of the text is that it is more important to know the higher-level skills than it is to be able to draw something from memory. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P27 \n Text: Note that we are not saying that you should, for example, be able to \nwrite down the ASCII representation from memory.  In that example, \nknowing that it is a {7-bit} representation used for English\ntext is sufficient.  You can always look up the detailed definition \nin practice. \n Question: \nQ. What is the ASCII representation? \n Answer: \n\nThe ASCII representation is a character encoding standard for electronic communication."}, {"text": "Title: Summary of Part 1 of the Course P28 \n Text: {universal computational devices /  computing machines ()\n{--}{{}{}\n{}{}{}\n undecidable ()\n the halting problem () \n Question: \nQ. What is the halting problem? \n Answer: \n\nThe halting problem is the problem of determining whether a given program will finish running or continue running forever."}, {"text": "Title: Summary of Part 1 of the Course P29 \n Text:  {Turing machines\n {--}{{}{}\n {}{}{}\n  universal computational device/ computing machine\n  intractable/undecidable\n  the halting problem\n \n } \n Question: \nQ. What does it mean if a problem is undecidable? \n Answer: \n\nA. A problem is undecidable if it cannot be solved by a Turing machine."}, {"text": "Title: Summary of Part 1 of the Course P30 \n Text: {information storage in computers\n{--}{{}{}\n{}{}{}\n bits ()\n representation (P&P)\n data type ()\n unsigned representation ()\n 2's complement representation \n Question: \nQ. What is the difference between data type and representation? \n Answer: \n\nA. Data type refers to the type of data that is being stored, while representation refers to the way in which that data is stored."}, {"text": "Title: Summary of Part 1 of the Course P31 \n Text:  FIXME?  not covered by book nor notes currently\n  fixed-point representation \n Question: \nQ. What are the benefits of using a fixed-point representation?\nQ. \nQ. There are several benefits of using a fixed-point representation, including: \nQ. \nQ. 1) Increased accuracy over using a floating-point representation, \nQ. 2) Reduced storage requirements, \nQ. 3) Reduced complexity (fewer operations required), and \nQ. 4) Increased speed of execution. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P32 \n Text:  IEEE floating-point representation\n ASCII representation\n equivalence classes \n Question: \nQ. What is the IEEE floating-point representation? \n Answer: \n\nA. The IEEE floating-point representation is a standard for representing real numbers in computer systems. It is used in many hardware and software implementations."}, {"text": "Title: Summary of Part 1 of the Course P33 \n Text: {operations on bits\n{--}{{}{}\n{}{}{}\n 1's complement operation\n carry (from addition)\n overflow (on any operation) ()\n Boolean logic and algebra\n logic functions/gates\n truth table\n AND/conjunction\n OR/disjunction\n NOT/logical complement/ (logical) negation/inverter\n XOR\n logical completeness\n minterm \n Question: \nQ. What does the term \"minterm\" refer to in Boolean logic?\nQ. \nQ. A minterm is a function of n variables that can be expressed as the product of n literals. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P34 \n Text: {mathematical terms\n{--}{{}{}\n{}{}{}\n modular arithmetic\n implication\n contrapositive\n proof approaches: by construction, by contradiction, by induction\n without loss of generality (w.l.o.g.) \n Question: \nQ. What is the difference between modular arithmetic and regular arithmetic? \n Answer: \n\nA. Modular arithmetic is a system of arithmetic in which numbers \"wrap around\" when they reach a certain value, called the modulus. In contrast, in regular arithmetic, numbers do not wrap around and continue to increase indefinitely."}, {"text": "Title: Summary of Part 1 of the Course P35 \n Text:  {error detection and correction\n {--}{{}{}\n {}{}{}\n  code/sparse representation\n  code word\n  bit error\n  odd/even parity bit\n  Hamming distance between code words\n  Hamming distance of a code\n  Hamming code\n  SEC-DED\n \n } \n Question: \nQ. What is the Hamming distance between two code words? \n Answer: \n\nA.\n\nThe Hamming distance between two code words is the number of bits that differ between them."}, {"text": "Title: Summary of Part 1 of the Course P36 \n Text: {high-level language concepts\n{--}{{}{}\n{}{}{}\n syntax\n{variables\n{--}{{}{}\n{}{}{}\n declaration\n primitive data types\n symbolic name/identifier\n initialization \n Question: \nQ. What is a high-level language concept?\nQ. \nQ. A high-level language concept is a concept that is used in a high-level programming language. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P37 \n Text: }\n FIXME?  really not necessary for them\n strongly typed languages\n expression\n statement \n Question: \nQ. What does the text say about strongly typed languages?\nQ. \nQ. The text says that strongly typed languages are not necessary for them. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P38 \n Text: {C operators\n{--}{{}{}\n{}{}{}\n operands\n arithmetic\n bitwise\n comparison/relational\n assignment\n address\n arithmetic shift\n logical shift\n precedence \n Question: \nQ. What is the difference between an arithmetic and a logical shift? \n Answer: \n\nA. An arithmetic shift is an operation that shifts all the bits in a number to the left or right, and the most significant bit is shifted in on the other end. A logical shift is an operation that shifts all the bits in a number to the left or right, but the most significant bit is not shifted in on the other end."}, {"text": "Title: Summary of Part 1 of the Course P39 \n Text: {functions in C\n{--}{{}{}\n{}{}{}\n { main}\n function call\n arguments\n {{ printf} and { scanf}\n{--}{{}{}\n{}{}{}\n format string\n escape character \n Question: \nQ. What is the main function in C?\nQ. \nQ. The main function is the function that is called when the program starts running. It is the entry point for the program. \n Answer: "}, {"text": "Title: Summary of Part 1 of the Course P40 \n Text: {transforming tasks into programs\n{--}{{}{}\n{}{}{}\n flow chart\n sequential construct\n conditional construct\n iterative construct/iteration/loop\n loop body \n Question: \nQ. What is the difference between a flow chart and a sequential construct? \n Answer: \n\nA. A flow chart is a graphical representation of a process, while a sequential construct is a set of instructions that are executed in a specific order."}, {"text": "Title: Summary of Part 1 of the Course P41 \n Text: {C statements\n{--}{{}{}\n{}{}{}\n statement: null, simple, compound\n { if} statement\n { for} loop\n { return} statement \n Question: \nQ. What is the difference between a simple statement and a compound statement in this context? \n Answer: \n\nA. A simple statement is a statement that consists of a single instruction, while a compound statement is a statement that consists of multiple instructions."}, {"text": "Title: Summary of Part 1 of the Course P42 \n Text:  THESE ARE NOT REQUIRED TOPICS \n Question: \nQ. The text describes a character's physical appearance.\nQ. \nQ. What does the character look like? \n Answer: \n\nThe character is described as being \"slight\" with \"pale skin.\" They have \"dark hair\" and \"hazel eyes.\" They are also said to have a \"narrow face.\""}, {"text": "Title: Summary of Part 1 of the Course P43 \n Text:  {execution of C programs\n {--}{{}{}\n {}{}{}\n  compiler/interpreter\n  source code\n  header files\n  assembly code\n  instructions\n  executable image\n \n }\n \n {the C preprocessor\n {--}{{}{}\n {}{}{}\n  #include directive\n  #define directive\n \n } \n Question: \nQ. What is the difference between a compiler and an interpreter? \n Answer: \n\nA. A compiler translates source code into assembly code, which is then translated into machine code by an assembler. An interpreter translates source code into machine code and executes it directly."}, {"text": "Title: Summary of Part 1 of the Course P44 \n Text: \n{   }   blank 3rd page \n Question: \nQ. What is the author's opinion on the issue?\nQ. \nQ. The author's opinion on the issue is not clear. \n Answer: "}, {"text": "Title: Optimizing Logic Expressions P0 \n Text: The second part of the course covers digital design more\ndeeply than does the textbook.  The lecture notes will explain the\nadditional material, and we will provide further examples in lectures\nand in discussion sections.  Please let us know if you need further\nmaterial for study. \n Question: \nQ. What does the author mean when they say \"the second part of the course covers digital design more deeply than does the textbook\"? \n Answer: \n\nA. The second part of the course covers digital design in more depth than the textbook."}, {"text": "Title: Optimizing Logic Expressions P1 \n Text: In the last notes, we introduced Boolean logic operations and showed\nthat with AND, OR, and NOT, we can express any Boolean function on any\nnumber of variables. \n Question: \nQ. What is the significance of Boolean logic operations?\nQ. \nQ. Boolean logic operations are the foundation of digital logic and computer science. They are used to create Boolean functions, which are mathematical functions that return a value of either true or false. \n Answer: "}, {"text": "Title: Optimizing Logic Expressions P2 \n Text: Before you begin these notes, please read the first two sections \nin Chapter 3 of the textbook,\nwhich discuss the operation of { complementary metal-oxide semiconductor}\n({ CMOS}) transistors, illustrate how gates implementing the\nAND, OR, and NOT operations can be built using transistors,\nand introduce DeMorgan's laws.  \n Question: \nQ. What is the main difference between CMOS and other types of transistors? \n Answer: \n\nA. The main difference between CMOS and other types of transistors is that CMOS transistors use both n-type and p-type semiconductor materials, while other types of transistors only use one type of semiconductor material."}, {"text": "Title: Optimizing Logic Expressions P3 \n Text: This set of notes exposes you to a mix of techniques,\nterminology, tools, and philosophy.  Some of the material is not\ncritical to our class (and will not be tested), but is useful for\nyour broader education, and may help you in later classes.  The value\nof this material has changed substantially in the last couple of decades,\nand particularly in the last few years, as algorithms for tools\nthat help with hardware design have undergone rapid advances.  We\ntalk about these issues as we introduce the ideas. \n Question: \nQ. How have algorithms for tools that help with hardware design changed in the last few years? \n Answer: \n\nA. The algorithms for tools that help with hardware design have changed substantially in the last few years. In particular, the last few years have seen a rapid advance in the development of algorithms for tools that help with hardware design."}, {"text": "Title: Optimizing Logic Expressions P4 \n Text: The notes begin with a discussion of the ``best'' way to\nexpress a Boolean function and some techniques used historically\nto evaluate such decisions. \n Question: \nQ. What is the best way to express a Boolean function? \n Answer: \n\nThere is no one definitive answer to this question. Different methods may be better suited for different purposes. Some common techniques for expressing Boolean functions include using truth tables, Boolean algebra, and Karnaugh maps."}, {"text": "Title: Optimizing Logic Expressions P5 \n Text: We next introduce the terminology necessary to understand manipulation\nof expressions, and use these terms to explain the Karnaugh map, or\n{K-map}, a tool that we will use for many purposes this semester. \n Question: \nQ. What is the Karnaugh map? \n Answer: \n\nThe Karnaugh map is a tool that is used to manipulate expressions."}, {"text": "Title: Optimizing Logic Expressions P6 \n Text: We illustrate the use of {K-maps} with a couple of\nexamples, then touch on a few important questions and useful\nways of thinking about Boolean logic. \n Question: \nQ. What is the use of a K-map?\nQ. \nQ. A K-map is a graphical tool used to simplify and visualize Boolean logic expressions. \n Answer: "}, {"text": "Title: Optimizing Logic Expressions P7 \n Text: We conclude with a discussion of the general problem of \nmulti-metric optimization, introducing some ideas and approaches\nof general use to engineers. \n Question: \nQ. What is the general problem of multi-metric optimization? \n Answer: \n\nThe general problem of multi-metric optimization is the problem of how to optimize a system with multiple conflicting objectives. This is a difficult problem because there is often no single \"best\" solution that satisfies all of the objectives; instead, there is a trade-off between the different objectives that must be found. There are a variety of approaches that can be used to solve this problem, including multi-objective optimization, goal programming, and robust optimization."}, {"text": "Title: Defining Optimality P0 \n Text: In the notes on logic operations, you learned how to express an\narbitrary function on bits as an OR of minterms (ANDs with one\ninput per variable on which the function operates). \nAlthough this approach demonstrates logical completeness, the results\noften seem inefficient, as you can see by comparing the following \nexpressions for the carry out C from the\naddition of two {2-bit} unsigned numbers, A=A_1A_0 and B=B_1B_0. \n Question: \nQ. What does the author mean when they say \"the second part of the course covers digital design more deeply than does the textbook\"? \n Answer: \n\nA. The second part of the course covers digital design in more depth than the textbook."}, {"text": "Title: Defining Optimality P1 \n Text: \nC &=& A_1B_1 + (A_1+B_1)A_0B_0  \n&=& A_1B_1 + A_1A_0B_0 + A_0B_1B_0  \n&=& \n {A_1} A_0 B_1 B_0 +\nA_1 {A_0} B_1 {B_0} +\nA_1 {A_0} B_1 B_0 +\n&&A_1 A_0 {B_1} B_0 +\nA_1 A_0 B_1 {B_0} +\nA_1 A_0 B_1 B_0  \n Question: \nQ. What is the significance of Boolean logic operations?\nQ. \nQ. Boolean logic operations are the foundation of digital logic and computer science. They are used to create Boolean functions, which are mathematical functions that return a value of either true or false. \n Answer: "}, {"text": "Title: Defining Optimality P2 \n Text: \nThese three expressions are identical in the sense that they have\nthe same truth tables---they are the same mathematical function. \n Question: \nQ. What is the main difference between CMOS and other types of transistors? \n Answer: \n\nA. The main difference between CMOS and other types of transistors is that CMOS transistors use both n-type and p-type semiconductor materials, while other types of transistors only use one type of semiconductor material."}, {"text": "Title: Defining Optimality P3 \n Text: Equation () is the form that we gave when we introduced\nthe idea of using logic to calculate overflow.  In this form, we were\nable to explain the terms intuitively. \n Question: \nQ. How have algorithms for tools that help with hardware design changed in the last few years? \n Answer: \n\nA. The algorithms for tools that help with hardware design have changed substantially in the last few years. In particular, the last few years have seen a rapid advance in the development of algorithms for tools that help with hardware design."}, {"text": "Title: Defining Optimality P4 \n Text: Equation () results from distributing the parenthesized OR\nin Equation (). \n Question: \nQ. What is the best way to express a Boolean function? \n Answer: \n\nThere is no one definitive answer to this question. Different methods may be better suited for different purposes. Some common techniques for expressing Boolean functions include using truth tables, Boolean algebra, and Karnaugh maps."}, {"text": "Title: Defining Optimality P5 \n Text: Equation () is the result of our logical completeness\nconstruction.   \n Question: \nQ. What is the Karnaugh map? \n Answer: \n\nThe Karnaugh map is a tool that is used to manipulate expressions."}, {"text": "Title: Defining Optimality P6 \n Text: Since the functions are identical, does the form actually matter at all?\nCertainly either of the first two forms is easier for us to write \nthan is the third.  If we think\nof the form of an expression as a mapping from the function that we\nare trying to calculate into the AND, OR, and NOT functions that we\nuse as logical building blocks, we might also say that the first two\nversions use fewer building blocks.  That observation does have some\ntruth, but let's try to be more precise by framing a question. \n Question: \nQ. What is the use of a K-map?\nQ. \nQ. A K-map is a graphical tool used to simplify and visualize Boolean logic expressions. \n Answer: "}, {"text": "Title: Defining Optimality P7 \n Text: For any given function, there are an infinite number of ways that we can\nexpress the function (for example, given one variable A on which the\nfunction depends, you can OR together any number of copies \nof A without changing the function). \n Question: \nQ. What is the general problem of multi-metric optimization? \n Answer: \n\nThe general problem of multi-metric optimization is the problem of how to optimize a system with multiple conflicting objectives. This is a difficult problem because there is often no single \"best\" solution that satisfies all of the objectives; instead, there is a trade-off between the different objectives that must be found. There are a variety of approaches that can be used to solve this problem, including multi-objective optimization, goal programming, and robust optimization."}, {"text": "Title: Defining Optimality P8 \n Text: { What exactly makes one expression better than another?} \n Question: \nQ. What is a minterm? \n Answer: \n\nA minterm is an AND of literals, where each literal corresponds to a variable in the minterm."}, {"text": "Title: Defining Optimality P9 \n Text: In 1952, Edward Veitch wrote an article on simplifying truth functions.\nIn the introduction, he said, ``This general problem can be very complicated\nand difficult.  Not only does the complexity increase greatly with the\nnumber of inputs and outputs, but the criteria of the best circuit will\nvary with the equipment involved.'' \n Question: \nQ. What does the 'C' in the text represent? \n Answer: \n\nA. C represents the product of two matrices."}, {"text": "Title: Defining Optimality P10 \n Text: Sixty years later, the answer is largely the same: the criteria depend\nstrongly on the underlying technology (the gates and the devices used\nto construct the gates), and no single { metric}, or way of measuring,\nis sufficient to capture the important differences between expressions \nin all cases. \n Question: \nQ. What is a truth table? \n Answer: \n\nA. A truth table is a mathematical function that assigns a truth value to a proposition."}, {"text": "Title: Defining Optimality P11 \n Text: Three high-level metrics commonly used to evaluate chip\ndesigns are cost, power, and performance.  Cost usually represents the\nmanufacturing cost, which is closely related to the physical silicon\narea required for the design: the larger the chip, the more expensive\nthe chip is to produce.  Power measures energy consumption over\ntime.  A chip that consumes more power means that a user's energy bill\nis higher and, in a portable device, either that the device is heavier or has\na shorter battery life.  Performance measures the speed at which the\ndesign operates.  A faster design can offer more functionality, such as\nsupporting the latest games, or can just finish the same work in less\ntime than a slower design.  These metrics are sometimes related: if\na chip finishes its work, the chip can turn itself off, saving energy. \n Question: \nQ. What is the form of the equation that is used to calculate overflow? \n Answer: \n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is"}, {"text": "Title: Defining Optimality P12 \n Text: How do such high-level metrics relate to the problem at hand?  Only\nindirectly in practice.  There are too many factors involved to make\ndirect calculations of cost, power, or performance at the level of\nlogic expressions.   \n Question: \nQ. What is the purpose of parentheses?\nQ. \nQ. The purpose of parentheses is to provide extra information that is not essential to the main message of the text. \n Answer:  In the context of the text, parentheses are used to list the possible values of a variable."}, {"text": "Title: Defining Optimality P13 \n Text: Finding an { optimal} solution---the best formulation of a specific\nlogic function for a given metric---is often impossible using the \ncomputational resources and algorithms available to us. \n Question: \nQ. What is the result of the logical completeness construction? \n Answer: \n\nA. Equation () is the result of the logical completeness construction."}, {"text": "Title: Defining Optimality P14 \n Text: Instead, tools typically use heuristic approaches\nto find solutions that strike a balance between these metrics.\nA { heuristic} approach is one that is\nbelieved to yield fairly good solutions to a problem, but does\nnot necessarily find an optimal solution. \n Question: \nQ. Given that the functions are identical, does the form actually matter at all?\nQ. \nQ. What is the difference between the first two forms and the third form? \n Answer: \n\nA. The first two forms require fewer logical building blocks than the third form."}, {"text": "Title: Defining Optimality P15 \n Text: A human engineer can typically impose { constraints}, such as\nlimits on the chip area or limits on the minimum performance,\nin order to guide the process. \n Question: \nQ. How many copies of A can you OR together without changing the function? \n Answer: \n\nA. You can OR together any number of copies of A without changing the function."}, {"text": "Title: Defining Optimality P16 \n Text: Human engineers may also restructure the implementation of a \nlarger design, such as a design to perform floating-point arithmetic,\nso as to change the logic functions used in the design. \n Question: \nQ. What exactly makes one expression better than another? \n Answer: \n\nA. In the context of art, expressions are often judged based on their aesthetics, originality, and emotional power. Some expressions are better than others because they are more successful in conveying the artist's intended message or evoking a desired response from the viewer."}, {"text": "Title: Defining Optimality P17 \n Text: { Today, manipulation of logic expressions for the purposes of \noptimization is performed almost entirely by computers.}  Humans must\nsupply the logic functions of interest, and must program the acceptable \ntransformations between equivalent forms, but computers do the grunt\nwork of comparing alternative formulations and deciding which one is\nbest to use in context. \n Question: \nQ. Why does the complexity of designing truth functions increase with the number of inputs and outputs? \n Answer: \n\nA. The complexity of designing truth functions increases with the number of inputs and outputs because the number of possible combinations of input and output values increases exponentially with the number of inputs and outputs."}, {"text": "Title: Defining Optimality P18 \n Text: Although we believe that hand optimization of Boolean expressions is no\nlonger an important skill for our graduates, we do think that you\nshould be exposed to the ideas and metrics historically used for\nsuch optimization.  The rationale for retaining this exposure is \nthreefold.  \n Question: \nQ. What are the different ways of measuring the performance of digital logic gates? \n Answer: \n\nA. There is no single metric or way of measuring the performance of digital logic gates. The criteria used to evaluate the performance of digital logic gates depend on the underlying technology, and no single metric is sufficient to capture the important differences between expressions in all cases."}, {"text": "Title: Defining Optimality P19 \n Text: First, we believe that you still need to be able to perform basic\nlogic reformulations (slowly is acceptable)\nand logical equivalence checking (answering the question, ``Do two \nexpressions represent the same function?''). \n Question: \nQ. If cost, power, and performance are all important factors in chip design, why is cost the only one that is represented by a physical characteristic? \n Answer: \n\nA. Cost is the only factor that is represented by a physical characteristic because it is the only metric that is directly related to the physical silicon area required for the design. The larger the chip, the more expensive the chip is to produce."}, {"text": "Title: Defining Optimality P20 \n Text: Second, the complexity of the problem is a good way to introduce you\nto real engineering. \n Question: \nQ. How are high-level metrics related to the problem at hand? \n Answer: \n\nA. High-level metrics are related to the problem at hand in that they can help provide insight into the overall cost, power, or performance of a system. However, they cannot provide direct information about specific logic expressions."}, {"text": "Title: Defining Optimality P21 \n Text: Finally, the contextual information will help you to develop a better\nunderstanding of finite state machines and higher-level abstractions\nthat form the core of digital systems and are still defined directly\nby humans today. \n Question: \nQ. What defines an \"optimal solution?\" \n Answer: \n\nThere is no definitive answer to this question since it depends on the specific context in which the term is used. In general, an optimal solution is one that is the best possible given the constraints and objectives that have been defined."}, {"text": "Title: Defining Optimality P22 \n Text: Towards that end, we conclude this introduction by discussing two\nmetrics that engineers traditionally used to optimize \nlogic expressions.  These metrics are now embedded in { computer-aided \ndesign} ({ CAD}) tools and tuned to specific underlying technologies,\nbut the reasons for their use are still interesting. \n Question: \nQ. What is a balancing heuristic? \n Answer: \n\nA balancing heuristic is an approach that tries to find a balance between different metrics, in order to find a solution that is reasonably good."}, {"text": "Title: Defining Optimality P23 \n Text: The first metric of interest is a heuristic for the area needed for\na design. \n Question: \nQ. What are some of the constraints that a human engineer might impose on a chip design? \n Answer: \n\nA. Some of the constraints that a human engineer might impose on a chip design are limits on the chip area or limits on the minimum performance."}, {"text": "Title: Defining Optimality P24 \n Text: The measurement is simple: count the number of variable occurrences in\nan expression.  Simply go through and add up how many variables you see.\nUsing our example function C, \nEquation () gives a count of 6,\nEquation () gives a count of 8, and\nEquation () gives a count of 24.\nSmaller numbers represent better expressions, so \nEquation () is the best choice by this metric. \n Question: \nQ. What does it mean when it says that human engineers may \"restructure the implementation of a larger design?\" \n Answer: \n\nA. It means that human engineers may change the logic functions used in a design in order to change the way the design works."}, {"text": "Title: Defining Optimality P25 \n Text: Why is this metric interesting?\nRecall how gates are built from transistors.\nAn {N-input} gate requires roughly 2N transistors, so if you \ncount up the number of variables in the\nexpression, you get an estimate of the number of transistors needed,\nwhich is in turn an estimate for the area required for the design. \n Question: \nQ. How does the author feel about the role of computers in optimization?\nQ. \nQ. The author seems to feel that computers play a valuable role in optimization, doing the \"grunt work\" of comparing different formulations and deciding which one is best to use. \n Answer: "}, {"text": "Title: Defining Optimality P26 \n Text: A variation on variable counting is to add the number of operations,\nsince each gate also takes space for wiring (within as well as between\ngates).  Note that we ignore the number of inputs to the operations,\nso a {2-input} AND counts as 1, but a {10-input} AND also counts\nas 1.  We do not usually count complementing variables as an operation\nfor this metric because the complements of variables are sometimes \navailable at no extra cost in gates or wires. \n Question: \nQ. What are the three reasons for retaining exposure to hand optimization of Boolean expressions? \n Answer: \n\nA. The three reasons for retaining exposure to hand optimization of Boolean expressions are (1) to maintain familiarity with the ideas and metrics historically used for such optimization, (2) to provide a better understanding of how Boolean expressions are optimized by modern computer hardware, and (3) to allow for more efficient implementation of Boolean functions in hardware."}, {"text": "Title: Defining Optimality P27 \n Text: If we add the number of operations in our example, we get\na count of 10 for Equation ()---two ANDs, two ORs,\nand 6 variables, \n Question: \nQ. What is an example of a logical equivalence checking? \n Answer: \n\nA. An example of a logical equivalence checking would be to determine whether two boolean expressions are equivalent."}, {"text": "Title: Defining Optimality P28 \n Text: a count of 12 for Equation ()---three ANDS, one OR,\nand 8 variables, \n Question: \nQ. What are some of the challenges of engineering? \n Answer: \n\nA. There are many challenges of engineering, but some of the most common ones include finding ways to reduce costs, increase efficiency, and improve safety."}, {"text": "Title: Defining Optimality P29 \n Text: and a count of 31 for Equation ()---six ANDs, one OR,\nand 24 variables. \n Question: \nQ. What are some of the higher-level abstractions that form the core of digital systems? \n Answer: \n\nA. Some of the higher-level abstractions that form the core of digital systems include finite state machines, Boolean logic, and sequential circuits."}, {"text": "Title: Defining Optimality P30 \n Text: The relative differences between these equations \nare reduced when one counts operations. \n Question: \nQ. What are the two metrics that engineers traditionally used to optimize logic expressions? \n Answer: \n\nThe two metrics are circuit complexity and transistor count."}, {"text": "Title: Defining Optimality P31 \n Text: A second metric of interest is a heuristic for the performance of a design. \n Question: \nQ. \nQ. The heuristic for the area needed for a design is that the design should be able to fit within a certain area. \n Answer: "}, {"text": "Title: Defining Optimality P32 \n Text: Performance is inversely related to the delay necessary for a design\nto produce an output once its inputs are available.  For example, if\nyou know how many seconds it takes to produce a result, you can easily\ncalculate the number of results that can be produced per second, which \nmeasures performance. \n Question: \nQ. What is the best choice by this metric? \n Answer: \n\nA. Equation (1) is the best choice by this metric."}, {"text": "Title: Defining Optimality P33 \n Text: The measurement needed is the longest chain of operations\nperformed on any instance of a variable.  The complement of\na variable is included if the variable's complement is not \navailable without using an inverter. \n Question: \nQ. Why is it interesting to count the number of variables in an expression?\nQ. \nQ. One reason is that it can give you an estimate of the number of transistors needed for a design, which in turn is an estimate for the area required. \n Answer: "}, {"text": "Title: Defining Optimality P34 \n Text: The rationale for this metric is that gate outputs do not change \ninstantaneously when their inputs\nchange. Once an input to a gate has reached an appropriate voltage to represent \na 0 or a 1, the transistors in the gate switch (on or off) and electrons \nstart to move.\nOnly when the output of the gate reaches the appropriate new voltage \ncan the gates driven by the output start to change. \nIf we count each\nfunction/gate as one delay (we call this time a { gate delay}), we \nget an estimate of the time needed to compute\nthe function. \n Question: \nQ. How many operations are in a 2-input AND gate?\nQ. \nQ. 1 \n Answer: "}, {"text": "Title: Defining Optimality P35 \n Text: Referring again to our example equations, we find that\nEquation () requires 3 gate delays,\nEquation () requires 2 gate delays,\nEquation () requires 2 or 3 gate delays, depending\non whether we have variable complements available.  Now \nEquation () looks more attractive: better performance than\nEquation () in return for a small extra cost in area. \n Question: \nQ. How many operations are in Equation ()? \n Answer: \n\n10"}, {"text": "Title: Defining Optimality P36 \n Text: Heuristics for estimating energy use are too \ncomplex to introduce at this point, \nbut you should be aware that every time electrons move, they\ngenerate heat, so we might favor an expression that minimizes the number\nof bit transitions inside the computation.  Such a measurement is\nnot easy to calculate by hand,\nsince you need to know the likelihood of input combinations. \n Question: \nQ. What is the purpose of the text?\nQ. \nQ. The text describes a method for counting the number of logical operators in a Boolean expression. \n Answer: "}, {"text": "Title: Terminology P0 \n Text: We use many technical terms when we talk about simplification of\nlogic expressions, so we now introduce those terms so as to make\nthe description of the tools and processes easier to understand. \n Question: \nQ. What does the author mean when they say \"the second part of the course covers digital design more deeply than does the textbook\"? \n Answer: \n\nA. The second part of the course covers digital design in more depth than the textbook."}, {"text": "Title: Terminology P1 \n Text: Let's assume that we have a logic function F(A,B,C,D) that we want\nto express concisely.  A { literal} in an expression of F refers \nto either one of the variables or its complement.  In other words,\nfor our function F, the following is a complete set of literals:\nA, , B, ,\nC, , D, and . \n Question: \nQ. What is the significance of Boolean logic operations?\nQ. \nQ. Boolean logic operations are the foundation of digital logic and computer science. They are used to create Boolean functions, which are mathematical functions that return a value of either true or false. \n Answer: "}, {"text": "Title: Terminology P2 \n Text: When we introduced the AND and OR functions, we also introduced \nnotation borrowed from arithmetic, using multiplication to represent AND\nand addition to represent OR.  We also borrow the related terminology, \nso a { sum} \nin Boolean algebra refers to a number of terms OR'd together \n(for example, A+B, or AB+CD), and\na { product} in Boolean algebra refers to a number of terms AND'd\ntogether (for example, A, or AB(C+D).  Note that\nthe terms in a sum or product may themselves be sums, products, or\nother types of expressions (for example, A). \n Question: \nQ. What is the main difference between CMOS and other types of transistors? \n Answer: \n\nA. The main difference between CMOS and other types of transistors is that CMOS transistors use both n-type and p-type semiconductor materials, while other types of transistors only use one type of semiconductor material."}, {"text": "Title: Terminology P3 \n Text: The construction method that we used to demonstrate logical completeness\nmade use of minterms for each input combination for which the \nfunction F produces a 1. \n Question: \nQ. How have algorithms for tools that help with hardware design changed in the last few years? \n Answer: \n\nA. The algorithms for tools that help with hardware design have changed substantially in the last few years. In particular, the last few years have seen a rapid advance in the development of algorithms for tools that help with hardware design."}, {"text": "Title: Terminology P4 \n Text: We can now use the idea of a literal to give a simpler definition of minterm:\na { minterm} for a function on N variables is a product (AND \nfunction) of N literals in which each variable or its complement \nappears exactly once.  For our function F, examples of minterms\ninclude ABC,\nACD, and\nBC.\nAs you know, a minterm produces a 1 for exactly\none combination of inputs. \n Question: \nQ. What is the best way to express a Boolean function? \n Answer: \n\nThere is no one definitive answer to this question. Different methods may be better suited for different purposes. Some common techniques for expressing Boolean functions include using truth tables, Boolean algebra, and Karnaugh maps."}, {"text": "Title: Terminology P5 \n Text: When we sum minterms for each output value of 1\nin a truth table to express a function, as we did to obtain\nEquation (), we\nproduce an example of the sum-of-products form.\nIn particular, a { sum-of-products} ({ SOP}) is a sum composed of \nproducts of literals.\nTerms in a sum-of-products need not be minterms, however.\nEquation () is also in sum-of-products form.\nEquation (), however, is not, since the last\nterm in the sum is not a product of literals. \n Question: \nQ. What is the Karnaugh map? \n Answer: \n\nThe Karnaugh map is a tool that is used to manipulate expressions."}, {"text": "Title: Terminology P6 \n Text: Analogously to the idea of a minterm, \nwe define a { maxterm} for a function on N variables as a sum (OR \nfunction) of N literals in which each variable or its complement \nappears exactly once.  Examples for F include (A+B++D),\n(A+++D), and\n(++C+).\nA maxterm produces a 0 for exactly\none combination of inputs.  Just as we did with minterms, we can\nmultiply a maxterm corresponding to each input combination for which\na function produces 0 (each row in a truth table that produces a 0\noutput) to create\nan expression for the function.  The resulting expression is in\na { product-of-sums} ({ POS}) form: a product of sums of literals.   \n Question: \nQ. What is the use of a K-map?\nQ. \nQ. A K-map is a graphical tool used to simplify and visualize Boolean logic expressions. \n Answer: "}, {"text": "Title: Terminology P7 \n Text: The carry out function that we used to produce \nEquation () has 10 input combinations that produce 0,\nso the expression formed in this way is unpleasantly long: \n Question: \nQ. What is the general problem of multi-metric optimization? \n Answer: \n\nThe general problem of multi-metric optimization is the problem of how to optimize a system with multiple conflicting objectives. This is a difficult problem because there is often no single \"best\" solution that satisfies all of the objectives; instead, there is a trade-off between the different objectives that must be found. There are a variety of approaches that can be used to solve this problem, including multi-objective optimization, goal programming, and robust optimization."}, {"text": "Title: Terminology P8 \n Text: {eqnarray*}\nC&=&\n({A_1}+{A_0}+B_1+B_0)\n({A_1}+A_0+B_1+{B_0})\n({A_1}+A_0+B_1+B_0)\n(A_1+{A_0}+{B_1}+B_0)\n&&(A_1+{A_0}+B_1+{B_0})\n(A_1+{A_0}+B_1+B_0)\n(A_1+A_0+{B_1}+{B_0})\n(A_1+A_0+{B_1}+B_0)\n&&(A_1+A_0+B_1+{B_0})\n(A_1+A_0+B_1+B_0)\n{eqnarray*} \n Question: \nQ. What is a minterm? \n Answer: \n\nA minterm is an AND of literals, where each literal corresponds to a variable in the minterm."}, {"text": "Title: Terminology P9 \n Text: However, the approach can be helpful with functions that produce mostly 1s.\nThe literals in maxterms are complemented with respect to the\nliterals used in minterms.  For example, the maxterm\n({A_1}+{A_0}+B_1+B_0) in the equation above\nproduces a zero for input combination A_1=1, A_0=1, B_1=0, B_0=0. \n Question: \nQ. What does the 'C' in the text represent? \n Answer: \n\nA. C represents the product of two matrices."}, {"text": "Title: Terminology P10 \n Text: An { implicant} G of a function F is defined to be a second\nfunction operating on the same variables for which\nthe implication G is true.  In terms of logic functions\nthat produce 0s and 1s, { if G is an implicant of F, \nthe input combinations for which G produces 1s are a subset of \nthe input combinations for which F produces 1s}. \n Question: \nQ. What is a truth table? \n Answer: \n\nA. A truth table is a mathematical function that assigns a truth value to a proposition."}, {"text": "Title: Terminology P11 \n Text: Any minterm for which F produces a 1, for example, is an implicant of F. \n Question: \nQ. What is the form of the equation that is used to calculate overflow? \n Answer: \n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is"}, {"text": "Title: Terminology P12 \n Text: In the context of logic design, { the term implicant is used\nto refer to a single product of literals.}  In other words, if we\nhave a function F(A,B,C,D), examples of possible implicants of F\ninclude AB, B, ABC, and .\nIn contrast, although they may technically imply F, we typically\ndo not call expressions such as (A+B), C(+D), nor\nA+C implicants. \n Question: \nQ. What is the purpose of parentheses?\nQ. \nQ. The purpose of parentheses is to provide extra information that is not essential to the main message of the text. \n Answer:  In the context of the text, parentheses are used to list the possible values of a variable."}, {"text": "Title: Terminology P13 \n Text: Let's say that we have expressed function F in sum-of-products \nform.\nAll of the individual product terms in the expression are implicants of F.   \n Question: \nQ. What is the result of the logical completeness construction? \n Answer: \n\nA. Equation () is the result of the logical completeness construction."}, {"text": "Title: Terminology P14 \n Text: As a first step in simplification, we can ask: for each implicant, is it\npossible to remove any of the literals that make up the product? \n Question: \nQ. Given that the functions are identical, does the form actually matter at all?\nQ. \nQ. What is the difference between the first two forms and the third form? \n Answer: \n\nA. The first two forms require fewer logical building blocks than the third form."}, {"text": "Title: Terminology P15 \n Text: If we have an implicant G for which the answer is no, we \ncall G a { prime implicant} of F. \n Question: \nQ. How many copies of A can you OR together without changing the function? \n Answer: \n\nA. You can OR together any number of copies of A without changing the function."}, {"text": "Title: Terminology P16 \n Text: In other words, if one removes any of the literals from a prime \nimplicant G of F,\nthe resulting product is not an implicant of F. \n Question: \nQ. What exactly makes one expression better than another? \n Answer: \n\nA. In the context of art, expressions are often judged based on their aesthetics, originality, and emotional power. Some expressions are better than others because they are more successful in conveying the artist's intended message or evoking a desired response from the viewer."}, {"text": "Title: Terminology P17 \n Text: Prime implicants are the main idea that we use to simplify logic expressions,\nboth algebraically and with graphical tools (computer tools use algebra\ninternally---by graphical here we mean drawings on paper). \n Question: \nQ. Why does the complexity of designing truth functions increase with the number of inputs and outputs? \n Answer: \n\nA. The complexity of designing truth functions increases with the number of inputs and outputs because the number of possible combinations of input and output values increases exponentially with the number of inputs and outputs."}, {"text": "Title: Veitch Charts and Karnaugh Maps P0 \n Text: Veitch's 1952 paper was the first to introduce the idea of\nusing a graphical representation to simplify logic expressions.\nEarlier approaches were algebraic.\nA year later, Maurice Karnaugh published\na paper showing a similar idea with a twist.  The twist makes the use of \n{ Karnaugh maps} to simplify expressions\nmuch easier than the use of Veitch charts.  As a result,\nfew engineers have heard of Veitch, but everyone who has ever\ntaken a class on digital logic knows how to make use of a { K-map}.  \n Question: \nQ. What does the author mean when they say \"the second part of the course covers digital design more deeply than does the textbook\"? \n Answer: \n\nA. The second part of the course covers digital design in more depth than the textbook."}, {"text": "Title: Veitch Charts and Karnaugh Maps P1 \n Text: Before we introduce the Karnaugh map, let's think about the structure\nof the domain of a logic function.  Recall that a function's { domain}\nis the space on which the function is defined, that is, for which the\nfunction produces values.  For a Boolean logic function on N variables,\nyou can think of the domain as sequences of N bits, but you can also\nvisualize the domain as an {N-dimensional} hypercube.  \nAn \n Question: \nQ. What is the significance of Boolean logic operations?\nQ. \nQ. Boolean logic operations are the foundation of digital logic and computer science. They are used to create Boolean functions, which are mathematical functions that return a value of either true or false. \n Answer: "}, {"text": "Title: Veitch Charts and Karnaugh Maps P2 \n Text: { {N-dimensional} hypercube} is the generalization\nof a cube to N dimensions.  Some people only use the term hypercube when\nN, since we have other names for the smaller values: a point\nfor N=0, a line segment for N=1, a square for N=2, and a cube\nfor N=3.  The diagrams above and to the right illustrate the cases that \nare easily drawn on paper.  The black dots represent specific input\ncombinations, and the blue edges connect input combinations that differ\nin exactly one input value (one bit). \n Question: \nQ. What is the main difference between CMOS and other types of transistors? \n Answer: \n\nA. The main difference between CMOS and other types of transistors is that CMOS transistors use both n-type and p-type semiconductor materials, while other types of transistors only use one type of semiconductor material."}, {"text": "Title: Veitch Charts and Karnaugh Maps P3 \n Text: By viewing a function's domain in this way, we can make a connection\nbetween a product of literals and the structure of the domain.  Let's\nuse the {3-dimensional} version as an example.  We call the \nvariables A, B, and C, and note that the cube has 2^3=8 corners\ncorresponding to the 2^3 possible combinations\nof A, B, and C.\nThe simplest product of literals in this\ncase is 1, which is the product of 0 literals.  Obviously, the product 1 \nevaluates to 1 for any variable values.  We can thus think of it as\ncovering the entire domain of the function.  In the case of our example,\nthe product 1 covers the whole cube.  In order for the product 1 to \nbe an implicant of a function, the function itself must be the function 1. \n Question: \nQ. How have algorithms for tools that help with hardware design changed in the last few years? \n Answer: \n\nA. The algorithms for tools that help with hardware design have changed substantially in the last few years. In particular, the last few years have seen a rapid advance in the development of algorithms for tools that help with hardware design."}, {"text": "Title: Veitch Charts and Karnaugh Maps P4 \n Text: What about a product consisting of a single literal, such as A \nor ?  The dividing lines in the diagram illustrate the\nanswer: any such product term evaluates to 1 on a face of the cube,\nwhich includes 2^2=4 of the corners.  If a function evaluates to 1\non any of the six faces of the cube, the corresponding product term\n(consisting of a single literal) is an implicant of the function. \n Question: \nQ. What is the best way to express a Boolean function? \n Answer: \n\nThere is no one definitive answer to this question. Different methods may be better suited for different purposes. Some common techniques for expressing Boolean functions include using truth tables, Boolean algebra, and Karnaugh maps."}, {"text": "Title: Veitch Charts and Karnaugh Maps P5 \n Text: Continuing with products of two literals, we see that any product of \ntwo literals, such as A or C, corresponds\nto an edge of our {3-dimensional} cube.  The edge includes 2^1=2\ncorners.  And, if a function evaluates to 1 on any of the 12 edges\nof the cube, the corresponding product term (consisting of two literals)\nis an implicant of the function. \n Question: \nQ. What is the Karnaugh map? \n Answer: \n\nThe Karnaugh map is a tool that is used to manipulate expressions."}, {"text": "Title: Veitch Charts and Karnaugh Maps P6 \n Text: Finally, any product of three literals, such as B,\ncorresponds to a corner of the cube.  But for a function on three variables,\nthese are just the minterms.  As you know, if a function evaluates to 1\non any of the 8 corners of the cube, that minterm is an implicant of the\nfunction (we used this idea to construct the function to prove\nlogical completeness). \n Question: \nQ. What is the use of a K-map?\nQ. \nQ. A K-map is a graphical tool used to simplify and visualize Boolean logic expressions. \n Answer: "}, {"text": "Title: Veitch Charts and Karnaugh Maps P7 \n Text: How do these connections help us to simplify functions?  If we're\ncareful, we can map cubes onto paper in such a way that product terms\n(the possible implicants of the function) usually form contiguous \ngroups of 1s, allowing us to spot them easily.  Let's work upwards\nstarting from one variable to see how this idea works.  The end result\nis called a Karnaugh map. \n Question: \nQ. What is the general problem of multi-metric optimization? \n Answer: \n\nThe general problem of multi-metric optimization is the problem of how to optimize a system with multiple conflicting objectives. This is a difficult problem because there is often no single \"best\" solution that satisfies all of the objectives; instead, there is a trade-off between the different objectives that must be found. There are a variety of approaches that can be used to solve this problem, including multi-objective optimization, goal programming, and robust optimization."}, {"text": "Title: Veitch Charts and Karnaugh Maps P8 \n Text: \nThe first drawing shown to the right replicates our view of \nthe {1-dimensional} hypercube, corresponding to the domain of a\nfunction on one variable, in this case the variable A.  \nTo the right of the hypercube (line segment) are two variants\nof a Karnaugh map on one variable.  The middle variant clearly \nindicates the column corresponding to the product A (the other \ncolumn corresponds to ).  The right variant simply labels\nthe column with values for A. \n Question: \nQ. What is a minterm? \n Answer: \n\nA minterm is an AND of literals, where each literal corresponds to a variable in the minterm."}, {"text": "Title: Veitch Charts and Karnaugh Maps P9 \n Text: \nThe three drawings shown to the right illustrate the three possible product\nterms on one variable.  { The functions shown in \nthese Karnaugh maps are arbitrary, except that we have chosen\nthem such that each implicant shown is a prime implicant for the\nillustrated function.} \n Question: \nQ. What does the 'C' in the text represent? \n Answer: \n\nA. C represents the product of two matrices."}, {"text": "Title: Veitch Charts and Karnaugh Maps P10 \n Text: Let's now look at two-variable functions.  We have replicated\nour drawing of the {2-dimensional} hypercube (square) to the right \nalong with two variants of Karnaugh maps on two variables.\nWith only two variables (A and B), the extension is fairly \nstraightforward, since we can use the second dimension of the \npaper (vertical) to express the second variable (B). \n Question: \nQ. What is a truth table? \n Answer: \n\nA. A truth table is a mathematical function that assigns a truth value to a proposition."}, {"text": "Title: Veitch Charts and Karnaugh Maps P11 \n Text: \nThe number of possible products of literals grows rapidly with the \nnumber of variables.\nFor two variables, nine are possible, as shown to the right.\nNotice that all implicants have two properties.  First, they\noccupy contiguous regions of the grid.  And, second, their height\nand width are always powers of two.  These properties seem somewhat\ntrivial at this stage, but they are the key to the utility of\n{K-maps} on more variables. \n Question: \nQ. What is the form of the equation that is used to calculate overflow? \n Answer: \n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is"}, {"text": "Title: Veitch Charts and Karnaugh Maps P12 \n Text: \nThree-variable functions are next.  The cube diagram is again replicated\nto the right.  But now we have a problem: how can we map four points\n(say, from the top half of the cube) into a line in such a way that \nany points connected by a blue line are adjacent in the {K-map}?\nThe answer is that we cannot, but we can preserve most of the connections\nby choosing an order such as the one illustrated by the arrow.\nThe result \n Question: \nQ. What is the purpose of parentheses?\nQ. \nQ. The purpose of parentheses is to provide extra information that is not essential to the main message of the text. \n Answer:  In the context of the text, parentheses are used to list the possible values of a variable."}, {"text": "Title: Veitch Charts and Karnaugh Maps P13 \n Text: is called a Gray code.  Two {K-map} variants again appear\nto the right of the cube.  Look closely at the order of the two-variable\ncombinations along the top, which allows us to have as many contiguous\nproducts of literals as possible.  Any product of literals that contains\n but not A nor  wraps around the edges\nof the {K-map}, so you should think of it as rolling up into a cylinder \nrather than a grid.  Or you can think that we're unfolding the cube to\nfit the corners onto a sheet of paper, but the place that we split\nthe cube should still be considered to be adjacent when looking for \nimplicants.  The use of a Gray code is the one difference between a\n{K-map} and a Veitch chart; Veitch used the base 2 order, which\nmakes some implicants hard to spot. \n Question: \nQ. What is the result of the logical completeness construction? \n Answer: \n\nA. Equation () is the result of the logical completeness construction."}, {"text": "Title: Veitch Charts and Karnaugh Maps P14 \n Text: With three variables, we have 27 possible products of literals.  You\nmay have noticed that the count scales as 3^N for N variables;\ncan you explain why?  We illustrate several product terms\nbelow.  Note that we sometimes\nneed to wrap around the end of the {K-map}, but that if we account\nfor wrapping, the squares covered by all product terms are contiguous.\nAlso notice that both the width and the height of all product terms \nare powers of two.  { Any square or rectangle that meets these two \nconstraints corresponds to a product term!}  And any such square or \nrectangle that is filled with 1s is an implicant of the function\nin the {K-map}. \n Question: \nQ. Given that the functions are identical, does the form actually matter at all?\nQ. \nQ. What is the difference between the first two forms and the third form? \n Answer: \n\nA. The first two forms require fewer logical building blocks than the third form."}, {"text": "Title: Veitch Charts and Karnaugh Maps P15 \n Text: Let's keep going.  With a function on four variables---A, B, C, \nand D---we can use a Gray code order on two of the variables in \neach dimension.  Which variables go with which dimension in the grid\nreally doesn't matter, so we'll assign AB to the horizontal dimension\nand CD to the vertical dimension.  A few of the 81 possible product\nterms are illustrated at the top of the next page.  Notice that while wrapping \ncan now occur\nin both dimensions, we have exactly the same rule for finding implicants\nof the function: any square or rectangle (allowing for wrapping) \nthat is filled with 1s and has both height and width equal to (possibly\ndifferent) powers of two is an implicant of the function.  { \nFurthermore, unless such a square or rectangle is part of a larger\nsquare or rectangle that meets these criteria, the corresponding\nimplicant is a prime implicant of the function.} \n Question: \nQ. How many copies of A can you OR together without changing the function? \n Answer: \n\nA. You can OR together any number of copies of A without changing the function."}, {"text": "Title: Veitch Charts and Karnaugh Maps P16 \n Text: \nFinding a simple expression for a function using a {K-map} then\nconsists of solving the following problem: pick a minimal set of prime \nimplicants such that every 1 produced by the function is covered\nby at least one prime implicant.  The metric that you choose to\nminimize the set may vary in practice, but for simplicity, let's say \nthat we minimize the number of prime implicants chosen. \n Question: \nQ. What exactly makes one expression better than another? \n Answer: \n\nA. In the context of art, expressions are often judged based on their aesthetics, originality, and emotional power. Some expressions are better than others because they are more successful in conveying the artist's intended message or evoking a desired response from the viewer."}, {"text": "Title: Veitch Charts and Karnaugh Maps P17 \n Text: \nLet's try a few!  The table on the left below reproduces (from\nNotes Set 1.4) the truth\ntable for addition of two {2-bit} unsigned numbers, A_1A_0\nand B_1B_0, to produce a sum S_1S_0 and a carry out C.\n{K-maps} for each output bit appear to the right.\nThe colors are used only to make the different prime implicants\neasier to distinguish.\nThe equations produced by summing these prime implicants appear\nbelow the {K-maps}. \n Question: \nQ. Why does the complexity of designing truth functions increase with the number of inputs and outputs? \n Answer: \n\nA. The complexity of designing truth functions increases with the number of inputs and outputs because the number of possible combinations of input and output values increases exponentially with the number of inputs and outputs."}, {"text": "Title: Veitch Charts and Karnaugh Maps P18 \n Text: \n{cccc|ccc}\n{c|}& \nA_1& A_0& B_1& B_0& C& S_1& S_0 \n0&0& 0&0& 0& 0&0\n0&0& 0&1& 0& 0&1\n0&0& 1&0& 0& 1&0\n0&0& 1&1& 0& 1&1\n0&1& 0&0& 0& 0&1\n0&1& 0&1& 0& 1&0\n0&1& 1&0& 0& 1&1\n0&1& 1&1& 1& 0&0\n1&0& 0&0& 0& 1&0\n1&0& 0&1& 0& 1&1\n1&0& 1&0& 1& 0&0\n1&0& 1&1& 1& 0&1\n1&1& 0&0& 0& 1&1\n1&1& 0&1& 1& 0&0\n1&1& 1&0& 1& 0&1\n1&1& 1&1& 1& 1&0 \n Question: \nQ. What are the different ways of measuring the performance of digital logic gates? \n Answer: \n\nA. There is no single metric or way of measuring the performance of digital logic gates. The criteria used to evaluate the performance of digital logic gates depend on the underlying technology, and no single metric is sufficient to capture the important differences between expressions in all cases."}, {"text": "Title: Veitch Charts and Karnaugh Maps P19 \n Text: \n{eqnarray*}\nC&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0 \n Question: \nQ. If cost, power, and performance are all important factors in chip design, why is cost the only one that is represented by a physical characteristic? \n Answer: \n\nA. Cost is the only factor that is represented by a physical characteristic because it is the only metric that is directly related to the physical silicon area required for the design. The larger the chip, the more expensive the chip is to produce."}, {"text": "Title: Veitch Charts and Karnaugh Maps P20 \n Text: S_1&=&\nA_1 {B_1} {B_0}+\nA_1 {A_0} {B_1}+\n{A_1} {A_0} B_1+\n{A_1} B_1 {B_0}+\n&&{A_1} A_0 {B_1} B_0+\nA_1 A_0 B_1 B_0 \n Question: \nQ. How are high-level metrics related to the problem at hand? \n Answer: \n\nA. High-level metrics are related to the problem at hand in that they can help provide insight into the overall cost, power, or performance of a system. However, they cannot provide direct information about specific logic expressions."}, {"text": "Title: Veitch Charts and Karnaugh Maps P21 \n Text: \nIn theory, {K-maps} extend to an arbitrary number of variables.\nCertainly Gray codes can be extended.  An { {N-bit} Gray code} \nis a sequence of {N-bit} patterns that includes all possible\npatterns such that any two adjacent patterns differ in only one bit.\nThe code is actually a cycle: the first and last patterns also differ\nin only one bit.  You can construct a Gray code recursively as follows:\nfor an {(N+1)-bit} Gray code, write the sequence for an\n{N-bit} Gray code, then add a 0 in front of all patterns.\nAfter this sequence, append a second copy of the {N-bit} Gray\ncode in reverse order, then put a 1 in front of all patterns in the\nsecond copy.  The result is an {(N+1)-bit} Gray code.\nFor example, the following are Gray codes: \n Question: \nQ. What defines an \"optimal solution?\" \n Answer: \n\nThere is no definitive answer to this question since it depends on the specific context in which the term is used. In general, an optimal solution is one that is the best possible given the constraints and objectives that have been defined."}, {"text": "Title: Veitch Charts and Karnaugh Maps P22 \n Text: \n1-bit& 0, 1\n2-bit& 00, 01, 11, 10\n3-bit& 000, 001, 011, 010, 110, 111, 101, 100\n4-bit& \n0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100,\n1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000 \n Question: \nQ. What is a balancing heuristic? \n Answer: \n\nA balancing heuristic is an approach that tries to find a balance between different metrics, in order to find a solution that is reasonably good."}, {"text": "Title: Veitch Charts and Karnaugh Maps P23 \n Text: \nUnfortunately, some of the beneficial properties of {K-maps} do\nnot extend beyond two variables in a dimension.  { Once you have three\nvariables in one dimension}, as is necessary if a function operates\non five or more variables, { not all product terms are contiguous\nin the grid}.  The terms still require a total number of rows and columns\nequal to a power of two, but they don't all need to be a contiguous\ngroup.  Furthermore, { some contiguous groups of appropriate size do\nnot correspond to product terms}.  So you can still make use of {K-maps}\nif you have more variables, but their use is a little trickier. \n Question: \nQ. What are some of the constraints that a human engineer might impose on a chip design? \n Answer: \n\nA. Some of the constraints that a human engineer might impose on a chip design are limits on the chip area or limits on the minimum performance."}, {"text": "Title: Canonical Forms P0 \n Text: What if we want to compare two expressions to determine whether they\nrepresent the same logic function?  Such a comparison is a test of\n{ logical equivalence}, and is an important part of hardware design.\nTools today provide help with this problem, but you should understand\nthe problem. \n Question: \nQ. What does the author mean when they say \"the second part of the course covers digital design more deeply than does the textbook\"? \n Answer: \n\nA. The second part of the course covers digital design in more depth than the textbook."}, {"text": "Title: Canonical Forms P1 \n Text: You know that any given function can be expressed in many ways, and that\ntwo expressions that look quite different may in fact represent the\nsame function (look back at Equations () to ()\nfor an example).  But what if we rewrite the function using only \nprime implicants?  Is the result unique?  Unfortunately, no. \n Question: \nQ. What is the significance of Boolean logic operations?\nQ. \nQ. Boolean logic operations are the foundation of digital logic and computer science. They are used to create Boolean functions, which are mathematical functions that return a value of either true or false. \n Answer: "}, {"text": "Title: Canonical Forms P2 \n Text: \nIn general, { a sum of products is not unique (nor is a product of sums),\neven if the sum contains only prime implicants}. \n Question: \nQ. What is the main difference between CMOS and other types of transistors? \n Answer: \n\nA. The main difference between CMOS and other types of transistors is that CMOS transistors use both n-type and p-type semiconductor materials, while other types of transistors only use one type of semiconductor material."}, {"text": "Title: Canonical Forms P3 \n Text: For example, consensus terms may or may not be included in our\nexpressions.  (They are necessary for reliable design of \ncertain types of systems, as you will learn in a later ECE class.) \n Question: \nQ. How have algorithms for tools that help with hardware design changed in the last few years? \n Answer: \n\nA. The algorithms for tools that help with hardware design have changed substantially in the last few years. In particular, the last few years have seen a rapid advance in the development of algorithms for tools that help with hardware design."}, {"text": "Title: Canonical Forms P4 \n Text: The green ellipse in the K-map to the right represents the consensus \nterm BC.\n{eqnarray*}\nZ &=& A C +  B + B C\nZ &=& A C +  B\n{eqnarray*} \n Question: \nQ. What is the best way to express a Boolean function? \n Answer: \n\nThere is no one definitive answer to this question. Different methods may be better suited for different purposes. Some common techniques for expressing Boolean functions include using truth tables, Boolean algebra, and Karnaugh maps."}, {"text": "Title: Canonical Forms P5 \n Text: \nSome functions allow several equivalent formulations as\nsums of prime implicants, even without consensus terms.  \nThe K-maps shown to the right, for example, illustrate \nhow one function might be written in either of the following ways: \n Question: \nQ. What is the Karnaugh map? \n Answer: \n\nThe Karnaugh map is a tool that is used to manipulate expressions."}, {"text": "Title: Canonical Forms P6 \n Text: {eqnarray*}\nZ &=&\n  D +\n C  +\nA B C +\nB  D\nZ &=&\n  C +\nB C  +\nA B D +\n  D\n{eqnarray*} \n Question: \nQ. What is the use of a K-map?\nQ. \nQ. A K-map is a graphical tool used to simplify and visualize Boolean logic expressions. \n Answer: "}, {"text": "Title: Canonical Forms P7 \n Text: \nWhen we need to compare two things (such as functions), we need to\ntransform them into what in mathematics is known as a { canonical form},\nwhich simply means a form that is defined so as to be unique \nfor each thing of the given type.\nWhat can we use for logic functions?  You already know two answers!   \n Question: \nQ. What is the general problem of multi-metric optimization? \n Answer: \n\nThe general problem of multi-metric optimization is the problem of how to optimize a system with multiple conflicting objectives. This is a difficult problem because there is often no single \"best\" solution that satisfies all of the objectives; instead, there is a trade-off between the different objectives that must be found. There are a variety of approaches that can be used to solve this problem, including multi-objective optimization, goal programming, and robust optimization."}, {"text": "Title: Canonical Forms P8 \n Text: The { canonical sum} of a function (sometimes called the\n{ canonical SOP form}) is the sum of minterms.   \n Question: \nQ. What is a minterm? \n Answer: \n\nA minterm is an AND of literals, where each literal corresponds to a variable in the minterm."}, {"text": "Title: Canonical Forms P9 \n Text: The { canonical product} of a function (sometimes called the\n{ canonical POS form}) is the product of maxterms.   \n Question: \nQ. What does the 'C' in the text represent? \n Answer: \n\nA. C represents the product of two matrices."}, {"text": "Title: Canonical Forms P10 \n Text: These forms technically\nonly meet the mathematical definition of canonical if we agree on an order\nfor the min/maxterms, but that problem is solvable. \n Question: \nQ. What is a truth table? \n Answer: \n\nA. A truth table is a mathematical function that assigns a truth value to a proposition."}, {"text": "Title: Canonical Forms P11 \n Text: However, as you already know, the forms are not particularly \nconvenient to use.   \n Question: \nQ. What is the form of the equation that is used to calculate overflow? \n Answer: \n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is"}, {"text": "Title: Canonical Forms P12 \n Text: In practice, people and tools in the industry use more compact \napproaches when comparing functions, but those solutions are a subject for\na later class (such as ECE 462). \n Question: \nQ. What is the purpose of parentheses?\nQ. \nQ. The purpose of parentheses is to provide extra information that is not essential to the main message of the text. \n Answer:  In the context of the text, parentheses are used to list the possible values of a variable."}, {"text": "Title: Two-Level Logic P0 \n Text: \n{ Two-level logic} is a popular way of expressing logic functions.\nThe two levels refer simply to the number of functions through which an\ninput passes to reach an output, and both the SOP and POS forms are \nexamples of two-level logic.  In this section, we illustrate one of the \nreasons for this popularity and \nshow you how to graphically manipulate\nexpressions, which can sometimes help when trying to understand gate\ndiagrams. \n Question: \nQ. What does the author mean when they say \"the second part of the course covers digital design more deeply than does the textbook\"? \n Answer: \n\nA. The second part of the course covers digital design in more depth than the textbook."}, {"text": "Title: Two-Level Logic P1 \n Text: We begin with one of DeMorgan's laws, which we can illustrate both \nalgebraically and graphically:\nC  =  B+A  =    \n Question: \nQ. What is the significance of Boolean logic operations?\nQ. \nQ. Boolean logic operations are the foundation of digital logic and computer science. They are used to create Boolean functions, which are mathematical functions that return a value of either true or false. \n Answer: "}, {"text": "Title: Two-Level Logic P2 \n Text: Let's say that we have a function expressed in SOP form, such as\nZ=ABC+DE+FGHJ.  The diagram on the left below shows the function\nconstructed from three AND gates and an OR gate.  Using DeMorgan's\nlaw, we can replace the OR gate with a NAND with inverted inputs.\nBut the bubbles that correspond to inversion do not need to sit at the\ninput to the gate.  We can invert at any point along the wire,\nso we slide each bubble down the wire to the output of the first column\nof AND gates.  { Be careful: if the wire splits, which does not\nhappen in our example, you have to replicate\nthe inverter onto the other output paths as you slide past the split\npoint!}  The end result is shown on the right: we have not changed the \nfunction, but now we use only NAND gates.  Since CMOS technology\nonly supports NAND and NOR directly, using two-level logic makes\nit simple to map our expression into CMOS gates. \n Question: \nQ. What is the main difference between CMOS and other types of transistors? \n Answer: \n\nA. The main difference between CMOS and other types of transistors is that CMOS transistors use both n-type and p-type semiconductor materials, while other types of transistors only use one type of semiconductor material."}, {"text": "Title: Two-Level Logic P3 \n Text: \nYou may want to make use of DeMorgan's other law, illustrated graphically\nto the right, to perform the same transformation on a POS expression.  What\ndo you get? \n Question: \nQ. How have algorithms for tools that help with hardware design changed in the last few years? \n Answer: \n\nA. The algorithms for tools that help with hardware design have changed substantially in the last few years. In particular, the last few years have seen a rapid advance in the development of algorithms for tools that help with hardware design."}, {"text": "Title: Multi-Metric Optimization P0 \n Text: As engineers, almost every real problem that you encounter will admit \nmultiple metrics for evaluating possible designs.  Becoming a good\nengineer thus requires not only that you be able to solve problems\ncreatively so as to improve the quality of your solutions, but also\nthat you are aware of how people might evaluate those solutions and\nare able both to identify the most important metrics and to balance \nyour design effectively according to them.  In this section, we\nintroduce some general ideas and methods that may be of use to you\nin this regard. \n Question: \nQ. What does the author mean when they say \"the second part of the course covers digital design more deeply than does the textbook\"? \n Answer: \n\nA. The second part of the course covers digital design in more depth than the textbook."}, {"text": "Title: Multi-Metric Optimization P1 \n Text: { We will not test you on the concepts in this section.} \n Question: \nQ. What is the significance of Boolean logic operations?\nQ. \nQ. Boolean logic operations are the foundation of digital logic and computer science. They are used to create Boolean functions, which are mathematical functions that return a value of either true or false. \n Answer: "}, {"text": "Title: Multi-Metric Optimization P2 \n Text: When you start thinking about a new problem, your first step\nshould be to think carefully about metrics of possible interest. \n Question: \nQ. What is the main difference between CMOS and other types of transistors? \n Answer: \n\nA. The main difference between CMOS and other types of transistors is that CMOS transistors use both n-type and p-type semiconductor materials, while other types of transistors only use one type of semiconductor material."}, {"text": "Title: Multi-Metric Optimization P3 \n Text: Some important metrics may not be easy to quantify.   \n Question: \nQ. How have algorithms for tools that help with hardware design changed in the last few years? \n Answer: \n\nA. The algorithms for tools that help with hardware design have changed substantially in the last few years. In particular, the last few years have seen a rapid advance in the development of algorithms for tools that help with hardware design."}, {"text": "Title: Multi-Metric Optimization P4 \n Text: For example, compatibility of a design with other products already \nowned by a customer has frequently defined the success or failure\nof computer hardware and software solutions. \n Question: \nQ. What is the best way to express a Boolean function? \n Answer: \n\nThere is no one definitive answer to this question. Different methods may be better suited for different purposes. Some common techniques for expressing Boolean functions include using truth tables, Boolean algebra, and Karnaugh maps."}, {"text": "Title: Multi-Metric Optimization P5 \n Text: But how can you compute the compability of your approach as\na number? \n Question: \nQ. What is the Karnaugh map? \n Answer: \n\nThe Karnaugh map is a tool that is used to manipulate expressions."}, {"text": "Title: Multi-Metric Optimization P6 \n Text: Humans---including engineers---are not good at\ncomparing multiple metrics simultaneously. \n Question: \nQ. What is the use of a K-map?\nQ. \nQ. A K-map is a graphical tool used to simplify and visualize Boolean logic expressions. \n Answer: "}, {"text": "Title: Multi-Metric Optimization P7 \n Text: Thus, once you have a set of metrics that you feel is complete, \nyour next step is to get rid of as many as you can. \n Question: \nQ. What is the general problem of multi-metric optimization? \n Answer: \n\nThe general problem of multi-metric optimization is the problem of how to optimize a system with multiple conflicting objectives. This is a difficult problem because there is often no single \"best\" solution that satisfies all of the objectives; instead, there is a trade-off between the different objectives that must be found. There are a variety of approaches that can be used to solve this problem, including multi-objective optimization, goal programming, and robust optimization."}, {"text": "Title: Multi-Metric Optimization P8 \n Text: Towards this end, you may identify metrics that have no practical \nimpact in current technology, set threshold values for other metrics\nto simplify reasoning about them, eliminate redundant metrics,\ncalculate linear sums to reduce the count of metrics, and, finally,\nmake use of the notion of Pareto optimality.  All of these ideas are\ndescribed in the rest of this section. \n Question: \nQ. What is a minterm? \n Answer: \n\nA minterm is an AND of literals, where each literal corresponds to a variable in the minterm."}, {"text": "Title: Multi-Metric Optimization P9 \n Text: Let's start by considering metrics that we can quantify as real\nnumbers. \n Question: \nQ. What does the 'C' in the text represent? \n Answer: \n\nA. C represents the product of two matrices."}, {"text": "Title: Multi-Metric Optimization P10 \n Text: For a given metric, we can divide possible measurement values into\nthree ranges. \n Question: \nQ. What is a truth table? \n Answer: \n\nA. A truth table is a mathematical function that assigns a truth value to a proposition."}, {"text": "Title: Multi-Metric Optimization P11 \n Text: In the first range,\nall measurement values are equivalently useful.\nIn the second range, \npossible values are ordered and interesting with respect to\none another.\nValues in the third range are all impossible to use in practice.\nUsing power consumption as\nour example, the first range corresponds to systems in which\nwhen a processor's power consumption in a digital \nsystem is extremely low relative to the\npower consumption\nof the system.\nFor example, the processor in a computer might use less than 1 \nof the total used by \nthe system including the disk drive, the monitor, the power \nsupply, and so forth.  One power consumption value in this range \nis just as good as any\nanother, and no one cares about the power consumption of the processor \nin such cases.  In the second range, power consumption of the\nprocessor makes a difference.  Cell phones use most of their energy\nin radio operation, for example, but if you own a phone with a powerful\nprocessor, you may have noticed that you can turn off the phone and \ndrain the battery fairly quickly by playing a game.  Designing a\nprocessor that uses half as much power lengthens the battery life in\nsuch cases.  Finally,\nthe third region of power consumption measurements is impossible:\nif you use so much power, your chip will overheat or even burst\ninto flames.  Consumers get unhappy when such things happen. \n Question: \nQ. What is the form of the equation that is used to calculate overflow? \n Answer: \n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is used to calculate overflow is \n\nQ. What is the form of the equation that is used to calculate overflow?\n\nA. The form of the equation that is"}, {"text": "Title: Multi-Metric Optimization P12 \n Text: As a first step, you can remove any metrics for which all solutions\nare effectively equivalent. \n Question: \nQ. What is the purpose of parentheses?\nQ. \nQ. The purpose of parentheses is to provide extra information that is not essential to the main message of the text. \n Answer:  In the context of the text, parentheses are used to list the possible values of a variable."}, {"text": "Title: Multi-Metric Optimization P13 \n Text: Until a little less than a decade ago, for example, the power \nconsumption of a desktop\nprocessor actually was in the first range that we discussed.  Power\nwas simply not a concern to engineers: all designs of \ninterest consumed so little power that no one cared. \n Question: \nQ. What is the result of the logical completeness construction? \n Answer: \n\nA. Equation () is the result of the logical completeness construction."}, {"text": "Title: Multi-Metric Optimization P14 \n Text: Unfortunately, at that point, power consumption jumped into the\nthird range rather quickly.  Processors hit a wall, and \nproducts had to be cancelled.  Given that the time spent designing\na processor has historically\nbeen about five years, a lot of engineering effort\nwas wasted because people had not thought carefully enough about\npower (since it had never mattered in the past). \n Question: \nQ. Given that the functions are identical, does the form actually matter at all?\nQ. \nQ. What is the difference between the first two forms and the third form? \n Answer: \n\nA. The first two forms require fewer logical building blocks than the third form."}, {"text": "Title: Multi-Metric Optimization P15 \n Text: Today, power is an important metric that engineers must take into\naccount in their designs.  \n Question: \nQ. How many copies of A can you OR together without changing the function? \n Answer: \n\nA. You can OR together any number of copies of A without changing the function."}, {"text": "Title: Multi-Metric Optimization P16 \n Text: However, in some areas, such as desktop and high-end server\nprocessors,\nother metrics (such as performance) may be so \nimportant that we always want to operate at the edge of the\ninteresting range.  In such cases, we might choose to treat \na metric such as power consumption as a { threshold}: stay\nbelow 150 Watts for a desktop processor, for example.  One still\nhas to make a coordinated effort to ensure that the system as\na whole does not exceed the threshold, but reasoning about \nthreshold values, a form of constraint, is easier than trying to\nthink about multiple metrics at once. \n Question: \nQ. What exactly makes one expression better than another? \n Answer: \n\nA. In the context of art, expressions are often judged based on their aesthetics, originality, and emotional power. Some expressions are better than others because they are more successful in conveying the artist's intended message or evoking a desired response from the viewer."}, {"text": "Title: Multi-Metric Optimization P17 \n Text: Some metrics may only allow discrete quantification.  For example,\none could choose to define compatibility with previous processor\ngenerations as binary: either an existing piece of software\n(or operating system)\nruns out of the box on your new processor, or it does not.  If you \nwant people who own that software to make use of your new processor,\nyou must ensure that the value of this binary metric is 1, which\ncan also be viewed as a threshold. \n Question: \nQ. Why does the complexity of designing truth functions increase with the number of inputs and outputs? \n Answer: \n\nA. The complexity of designing truth functions increases with the number of inputs and outputs because the number of possible combinations of input and output values increases exponentially with the number of inputs and outputs."}, {"text": "Title: Multi-Metric Optimization P18 \n Text: In some cases, two metrics may be strongly { correlated}, meaning\nthat a design that is good for one of the metrics is frequently \ngood for the other metric as well. \n Question: \nQ. What are the different ways of measuring the performance of digital logic gates? \n Answer: \n\nA. There is no single metric or way of measuring the performance of digital logic gates. The criteria used to evaluate the performance of digital logic gates depend on the underlying technology, and no single metric is sufficient to capture the important differences between expressions in all cases."}, {"text": "Title: Multi-Metric Optimization P19 \n Text: Chip area and cost, for\nexample, are technically distinct ways to measure a digital design,\nbut we rarely consider them separately. \n Question: \nQ. If cost, power, and performance are all important factors in chip design, why is cost the only one that is represented by a physical characteristic? \n Answer: \n\nA. Cost is the only factor that is represented by a physical characteristic because it is the only metric that is directly related to the physical silicon area required for the design. The larger the chip, the more expensive the chip is to produce."}, {"text": "Title: Multi-Metric Optimization P20 \n Text: A design that requires a larger chip is probably more complex,\nand thus takes more engineering time to get right (engineering time\ncosts money).   \n Question: \nQ. How are high-level metrics related to the problem at hand? \n Answer: \n\nA. High-level metrics are related to the problem at hand in that they can help provide insight into the overall cost, power, or performance of a system. However, they cannot provide direct information about specific logic expressions."}, {"text": "Title: Multi-Metric Optimization P21 \n Text: Each silicon wafer costs money to fabricate, and fewer copies of a \nlarge design fit on one wafer, so large chips mean more fabrication\ncost. \n Question: \nQ. What defines an \"optimal solution?\" \n Answer: \n\nThere is no definitive answer to this question since it depends on the specific context in which the term is used. In general, an optimal solution is one that is the best possible given the constraints and objectives that have been defined."}, {"text": "Title: Multi-Metric Optimization P22 \n Text: Physical defects in silicon can cause some chips not to work.  A large\nchip uses more silicon than a small one, and is thus more likely to suffer\nfrom defects (and not work).  Cost thus goes up again for large chips\nrelative to small ones. \n Question: \nQ. What is a balancing heuristic? \n Answer: \n\nA balancing heuristic is an approach that tries to find a balance between different metrics, in order to find a solution that is reasonably good."}, {"text": "Title: Multi-Metric Optimization P23 \n Text: Finally, large chips usually require more careful testing to ensure\nthat they work properly (even ignoring the cost of getting the design\nright, we have to test for the presence of defects), which adds still\nmore cost for a larger chip. \n Question: \nQ. What are some of the constraints that a human engineer might impose on a chip design? \n Answer: \n\nA. Some of the constraints that a human engineer might impose on a chip design are limits on the chip area or limits on the minimum performance."}, {"text": "Title: Multi-Metric Optimization P24 \n Text: All of these factors tend to correlate chip area and chip cost, to the\npoint that most engineers do not consider both metrics. \n Question: \nQ. What does it mean when it says that human engineers may \"restructure the implementation of a larger design?\" \n Answer: \n\nA. It means that human engineers may change the logic functions used in a design in order to change the way the design works."}, {"text": "Title: Multi-Metric Optimization P25 \n Text: \nAfter you have tried to reduce your set of metrics as much as possible,\nor simplified them by turning them into thresholds, you should consider\nturning the last few metrics into a weighted linear sum.  All remaining\nmetrics must be quantifiable in this case. \n Question: \nQ. How does the author feel about the role of computers in optimization?\nQ. \nQ. The author seems to feel that computers play a valuable role in optimization, doing the \"grunt work\" of comparing different formulations and deciding which one is best to use. \n Answer: "}, {"text": "Title: Multi-Metric Optimization P26 \n Text: For example, if you are left with three metrics for which a given\ndesign has values A, B, and C, you might reduce these to one\nmetric by calculating D=w_AA+w_BB+w_CC.  What are the w values?\nThey are weights for the three metrics.  Their values represent the\nrelative importance of the three metrics to the overall evaluation.\nHere we've assumed that larger values of A, B, and C are either\nall good or all bad.  If you have metrics with different senses,\nuse the reciprocal values.  For example, if a large value of A is good,\na small value of 1/A is also good.   \n Question: \nQ. What are the three reasons for retaining exposure to hand optimization of Boolean expressions? \n Answer: \n\nA. The three reasons for retaining exposure to hand optimization of Boolean expressions are (1) to maintain familiarity with the ideas and metrics historically used for such optimization, (2) to provide a better understanding of how Boolean expressions are optimized by modern computer hardware, and (3) to allow for more efficient implementation of Boolean functions in hardware."}, {"text": "Title: Multi-Metric Optimization P27 \n Text: The difficulty with linearizing metrics is that not everyone agrees\non the weights.  Is using less power more important than having a cheaper\nchip?  The answer may depend on many factors. \n Question: \nQ. What is an example of a logical equivalence checking? \n Answer: \n\nA. An example of a logical equivalence checking would be to determine whether two boolean expressions are equivalent."}, {"text": "Title: Multi-Metric Optimization P28 \n Text: When you are left with several metrics of interest, you can use the\nidea of Pareto optimality to identify interesting designs. \n Question: \nQ. What are some of the challenges of engineering? \n Answer: \n\nA. There are many challenges of engineering, but some of the most common ones include finding ways to reduce costs, increase efficiency, and improve safety."}, {"text": "Title: Multi-Metric Optimization P29 \n Text: Let's say that you have two metrics.  If a design D_1 is better than\na second design D_2 for both metrics, we say that D_1 \n{ dominates} D_2. \n Question: \nQ. What are some of the higher-level abstractions that form the core of digital systems? \n Answer: \n\nA. Some of the higher-level abstractions that form the core of digital systems include finite state machines, Boolean logic, and sequential circuits."}, {"text": "Title: Multi-Metric Optimization P30 \n Text: A design D is then said to be { Pareto optimal} if no other design\ndominates D.  Consider the figure on the left below, which illustrates\nseven possible designs measured with two metrics.  The design corresponding\nto point B dominates the designs corresponding to points A and C, so \nneither of the latter designs is Pareto optimal.  No other point \nin the figure dominates B, however, so that design is Pareto optimal.\nIf we remove all points that do not represent Pareto optimal designs,\nand instead include only those designs that are Pareto optimal, we\nobtain the version shown on the right.  These are points in a two-dimensional\nspace, not a line, but we can imagine a line going through the points,\nas illustrated in the figure: the points that make up the line are\ncalled a { Pareto curve}, or, if you have more than two metrics,\na { Pareto surface}. \n Question: \nQ. What are the two metrics that engineers traditionally used to optimize logic expressions? \n Answer: \n\nThe two metrics are circuit complexity and transistor count."}, {"text": "Title: Multi-Metric Optimization P31 \n Text: \nAs an example of the use of Pareto optimality, consider the figure\nto the right, which is copied with permission from Neal Crago's Ph.D. \ndissertation (UIUC ECE, 2012).  The figure compares hundreds of thousands \nof possible\ndesigns based on a handful of different core approaches for\nimplementing a processor.  The axes in the graph are two metrics \nof interest.  The horizontal axis measures the average performance of a \ndesign when\nexecuting a set of benchmark applications, normalized to\na baseline processor design.  The vertical axis measures the energy\nconsumed by a design when \n Question: \nQ. \nQ. The heuristic for the area needed for a design is that the design should be able to fit within a certain area. \n Answer: "}, {"text": "Title: Multi-Metric Optimization P32 \n Text: executing the same benchmarks, normalized\nagain to the energy consumed by a baseline design.  The six sets of\npoints in the graph represent alternative design techniques for the\nprocessor, most of which are in commercial use today.  The points\nshown for each set are the subset of many thousands of possible variants\nthat are Pareto optimal.  In this case, more performance and less energy\nconsumption are the good directions, so any point in a set for which\nanother point is both further to the right and further down is not\nshown in the graph.  The black line represents an absolute power\nconsumption of 150 Watts, which is a nominal threshold for a desktop\nenvironment.  Designs above and to the right of that line are not\nas interesting for desktop use.  The { design-space exploration} that\nNeal reported in this figure was of course done by many computers using\nmany hours of computation, but he had to design the process by which the\ncomputers calculated each of the points. \n Question: \nQ. What is the best choice by this metric? \n Answer: \n\nA. Equation (1) is the best choice by this metric."}, {"text": "Title: Boolean Properties and Don't Care Simplification P0 \n Text: This set of notes begins with a brief illustration of a few properties\nof Boolean logic, which may be of use to you in manipulating algebraic\nexpressions and in identifying equivalent logic functions without resorting \nto truth tables. \n Question: \nQ. What is the use of Boolean logic? \n Answer: \n\nA. Boolean logic is a system of logic that is used to manipulate algebraic expressions and to identify equivalent logic functions."}, {"text": "Title: Boolean Properties and Don't Care Simplification P1 \n Text: We then discuss the value of underspecifying a logic function so as\nto allow for selection of the simplest possible implementation. \n Question: \nQ. What is the value of underspecifying a logic function? \n Answer: \n\nA. The value of underspecifying a logic function is that it allows for selection of the simplest possible implementation."}, {"text": "Title: Boolean Properties and Don't Care Simplification P2 \n Text: This technique must be used carefully to avoid incorrect behavior,\nso we illustrate the possibility of misuse with an example, then\ntalk about several ways of solving the example correctly. \n Question: \nQ. The cat slept on the mat.\nQ. \nQ. What did the cat sleep on? \n Answer: \n\nA. The cat slept on the mat."}, {"text": "Title: Boolean Properties and Don't Care Simplification P3 \n Text: We conclude by generalizing the ideas in the example to several\nimportant application areas and talking about related problems. \n Question: \nQ. What are some examples of important application areas that are mentioned? \n Answer: \n\nSome examples of important application areas that are mentioned include\nimage processing, computer vision, and machine learning."}, {"text": "Title: Logic Properties P0 \n Text: Table  (on the next page)\nlists a number of properties of Boolean logic.\nMost of these are easy to derive from our earlier definitions, but\na few may be surprising to you.  In particular, in the algebra of\nreal numbers, multiplication\ndistributes over addition, but addition does not distribute over\nmultiplication.  For example, 3{(4+7)}=(3)+(3),\nbut 3+(4)=(3+4){(3+7)}.  In Boolean algebra,\nboth operators distribute over one another, as indicated in\nTable .  The consensus properties may also be \nnonintuitive.  Drawing a {K-map} may help you understand the\nconsensus property on the right side of the table.  For the \nconsensus variant on the left side of the table,\nconsider that since either A or  must be 0,\neither B or C or both must be 1 for the first two factors on the left\nto be 1 when ANDed together.  But in that case, the third factor \nis also 1, and is thus redundant. \n Question: \nQ. What is the use of Boolean logic? \n Answer: \n\nA. Boolean logic is a system of logic that is used to manipulate algebraic expressions and to identify equivalent logic functions."}, {"text": "Title: Logic Properties P1 \n Text: As mentioned previously, Boolean\nalgebra has an elegant symmetry known as a duality, in which any\nlogic statement (an expression or an equation) is related to a\nsecond logic statement. \n Question: \nQ. What is the value of underspecifying a logic function? \n Answer: \n\nA. The value of underspecifying a logic function is that it allows for selection of the simplest possible implementation."}, {"text": "Title: Logic Properties P2 \n Text: To calculate the { dual form} of a Boolean expression or equation, \nreplace 0 with 1, replace 1 with 0, \nreplace AND with OR, and\nreplace OR with AND. \n Question: \nQ. The cat slept on the mat.\nQ. \nQ. What did the cat sleep on? \n Answer: \n\nA. The cat slept on the mat."}, {"text": "Title: Logic Properties P3 \n Text: { Variables are not changed when finding the dual form.} \n Question: \nQ. What are some examples of important application areas that are mentioned? \n Answer: \n\nSome examples of important application areas that are mentioned include\nimage processing, computer vision, and machine learning."}, {"text": "Title: Logic Properties P4 \n Text: The dual form of a dual form is the original logic statement. \n Question: \nQ. Why does multiplication distribute over addition in the algebra of real numbers, but addition does not distribute over multiplication? \n Answer: \n\nA. In the algebra of real numbers, multiplication distributes over addition because the product of two real numbers is always a real number. However, addition does not necessarily distribute over multiplication because the sum of two real numbers may not be a real number. For example, the sum of 3 and 4 is 7, but the sum of 3 and 4 is not necessarily 11."}, {"text": "Title: Logic Properties P5 \n Text: Be careful when calculating a dual form: our convention for ordering \narithmetic operations is broken by the exchange, so you may want\nto add explicit parentheses before calculating the dual.  For\nexample, the dual of AB+C is not A+BC.\nRather, the dual of AB+C is (A+B)C. \n Question: \nQ. What is the Boolean algebra duality? \n Answer: \n\nA. The Boolean algebra duality is a symmetry in which any logic statement (an expression or an equation) is related to a second logic statement."}, {"text": "Title: Logic Properties P6 \n Text: { Add parentheses as necessary when calculating a dual form to ensure\nthat the order of operations does not change.} \n Question: \nQ. What does it mean to \"dual form\" a Boolean expression? \n Answer: \n\nA.\n\nTo \"dual form\" a Boolean expression means to replace 0 with 1, replace 1 with 0, replace AND with OR, and replace OR with AND."}, {"text": "Title: Logic Properties P7 \n Text: Duality has several useful practical applications.  First, the \n{ principle of duality} states that any theorem or identity has \nthe same truth value in dual form (we do not prove the principle\nhere). \n Question: \nQ. What does the author mean when they say \"variables are not changed when finding the dual form\"?\nQ. \nQ. The author is referring to the fact that, when solving a problem using the dual form, the original variables are not changed. Rather, the dual form simply provides an alternative way of solving the problem. \n Answer: "}, {"text": "Title: Logic Properties P8 \n Text: The rows of Table  are organized according to this\nprinciple: each row contains two equations that are the duals \nof one another.  \n However, we rename variables freely to\n simplify the dual form.  For example, taking the dual of 1+A=1,\n we obtain 0=0.  But we created variable A \n solely for the purpose of expressing the property, so we might\n instead have called the complementary value A.  We rename\n (using the same name!) to obtain 0=0, as shown in the table. \n Question: \nQ. What is the dual form of a dual form? \n Answer: \n\nA. The dual form of a dual form is the original logic statement."}, {"text": "Title: Logic Properties P9 \n Text: Second, the dual form is useful when designing certain types of logic,\nsuch as the networks of transistors connecting the output of a CMOS\ngate to high voltage and ground.  If you look at the gate designs in\nthe textbook (and particularly those in the exercises),\nyou will notice that these networks are duals.   \n Question: \nQ. Suppose A, B, and C are linear operators on a vector space V.  What is the dual of AB+C? \n Answer: \n\nA. (A+B)C"}, {"text": "Title: Logic Properties P10 \n Text: A function/expression is not a theorem nor an identity,\nthus the principle of duality does not apply to the dual of an\nexpression. \n Question: \nQ. What does the author mean when they say \"Add parentheses as necessary when calculating a dual form to ensure that the order of operations does not change.\"?\nQ. \nQ. The author is suggesting that when calculating a dual form, parentheses should be added as necessary to ensure that the order of operations does not change. \n Answer: "}, {"text": "Title: Logic Properties P11 \n Text: However, if you treat the value 0 as ``true,'' the dual form of an\nexpression has the same truth values as the original (operating with\nvalue 1 as ``true''). \n Question: \nQ. What is the principle of duality? \n Answer: \n\nA. The principle of duality states that any theorem or identity has the same truth value in dual form."}, {"text": "Title: Logic Properties P12 \n Text: Finally, you can calculate the complement of a Boolean function\n(any expression) by calculating the dual form and then complementing\neach variable. \n Question: \nQ. What is the purpose of the table? \n Answer: \n\nA. The purpose of the table is to express relationships between variables and their complementary values."}, {"text": "Title: Choosing the Best Function P0 \n Text: When we specify how something works using a human language, we\nleave out details.  Sometimes we do so deliberately, assuming that a\nreader or listener can provide the details themselves: ``Take me to the \nairport!''\nrather than ``Please bend your right arm at the elbow and shift your\nright upper arm forward so as to place your hand near the ignition key.\nNext, ...''   \n Question: \nQ. What is the use of Boolean logic? \n Answer: \n\nA. Boolean logic is a system of logic that is used to manipulate algebraic expressions and to identify equivalent logic functions."}, {"text": "Title: Choosing the Best Function P1 \n Text: \n{\n{|lll|}\n1+A=1& 0=0&\n1=A& 0+A=A&\nA+A=A& A=A&\nA=0& A+=1&\n{A+B}= &\n=+& DeMorgan's laws\n(A+B)C=AC+BC&\nA B+C=(A+C)(B+C)&distribution\n(A+B)(+C)(B+C)=(A+B)(+C)&\nA B+ C+B C=A B+ C& consensus \n Question: \nQ. What is the value of underspecifying a logic function? \n Answer: \n\nA. The value of underspecifying a logic function is that it allows for selection of the simplest possible implementation."}, {"text": "Title: Choosing the Best Function P2 \n Text: }\n{Boolean logic properties.  The two columns are dual forms of\none another.} \n Question: \nQ. The cat slept on the mat.\nQ. \nQ. What did the cat sleep on? \n Answer: \n\nA. The cat slept on the mat."}, {"text": "Title: Choosing the Best Function P3 \n Text: You know the basic technique for implementing a Boolean function\nusing { combinational logic}: use a {K-map} to identify a\nreasonable SOP or POS form, draw the resulting design, and perhaps\nconvert to NAND/NOR gates. \n Question: \nQ. What are some examples of important application areas that are mentioned? \n Answer: \n\nSome examples of important application areas that are mentioned include\nimage processing, computer vision, and machine learning."}, {"text": "Title: Choosing the Best Function P4 \n Text: When we develop combinational logic designs, we may also choose to\nleave some aspects unspecified.  In particular, the value of a\nBoolean logic function to be implemented may not matter for some\ninput combinations.  If we express the function as a truth table,\nwe may choose to mark the function's value for some input combinations \nas ``{ don't care},'' which is written as ``x'' (no quotes). \n Question: \nQ. Why does multiplication distribute over addition in the algebra of real numbers, but addition does not distribute over multiplication? \n Answer: \n\nA. In the algebra of real numbers, multiplication distributes over addition because the product of two real numbers is always a real number. However, addition does not necessarily distribute over multiplication because the sum of two real numbers may not be a real number. For example, the sum of 3 and 4 is 7, but the sum of 3 and 4 is not necessarily 11."}, {"text": "Title: Choosing the Best Function P5 \n Text: What is the benefit of using ``don't care'' values?   \n Question: \nQ. What is the Boolean algebra duality? \n Answer: \n\nA. The Boolean algebra duality is a symmetry in which any logic statement (an expression or an equation) is related to a second logic statement."}, {"text": "Title: Choosing the Best Function P6 \n Text: Using ``don't care'' values allows you to choose from among several\npossible logic functions, all of which produce the desired results\n(as well as some combination of 0s and 1s in place of the ``don't\ncare'' values).   \n Question: \nQ. What does it mean to \"dual form\" a Boolean expression? \n Answer: \n\nA.\n\nTo \"dual form\" a Boolean expression means to replace 0 with 1, replace 1 with 0, replace AND with OR, and replace OR with AND."}, {"text": "Title: Choosing the Best Function P7 \n Text: Each input combination marked as ``don't care'' doubles the number\nof functions that can be chosen to implement the design, often enabling \nthe logic needed for implementation to be simpler. \n Question: \nQ. What does the author mean when they say \"variables are not changed when finding the dual form\"?\nQ. \nQ. The author is referring to the fact that, when solving a problem using the dual form, the original variables are not changed. Rather, the dual form simply provides an alternative way of solving the problem. \n Answer: "}, {"text": "Title: Choosing the Best Function P8 \n Text: \nFor example, the {K-map} to the right specifies a function F(A,B,C)\nwith two ``don't care'' entries.   \n Question: \nQ. What is the dual form of a dual form? \n Answer: \n\nA. The dual form of a dual form is the original logic statement."}, {"text": "Title: Choosing the Best Function P9 \n Text: If you are asked to design combinational logic for this function,\nyou can\nchoose any values for the two ``don't care'' entries.  When identifying\nprime implicants, each ``x'' can either be a 0 or a 1. \n Question: \nQ. Suppose A, B, and C are linear operators on a vector space V.  What is the dual of AB+C? \n Answer: \n\nA. (A+B)C"}, {"text": "Title: Choosing the Best Function P10 \n Text: \nDepending on the choices made for the x's, we obtain one of \nthe following four functions: \n Question: \nQ. What does the author mean when they say \"Add parentheses as necessary when calculating a dual form to ensure that the order of operations does not change.\"?\nQ. \nQ. The author is suggesting that when calculating a dual form, parentheses should be added as necessary to ensure that the order of operations does not change. \n Answer: "}, {"text": "Title: Choosing the Best Function P11 \n Text: {eqnarray*}\nF&=& B+B C\nF&=& B+B C+A  \nF&=&B\nF&=&B+A \n{eqnarray*} \n Question: \nQ. What is the principle of duality? \n Answer: \n\nA. The principle of duality states that any theorem or identity has the same truth value in dual form."}, {"text": "Title: Choosing the Best Function P12 \n Text: Given this set of choices, a designer typically chooses the third: F=B,\nwhich corresponds to the {K-map} shown to the right of the \nequations.  The design then \nproduces F=1 when A=1, B=1, and C=0 (ABC=110), \nand produces F=0 when A=1, B=0, and C=0 (ABC=100).\nThese differences are marked with shading and green italics in the new\n{K-map}.  No implementation ever produces an ``x.'' \n Question: \nQ. What is the purpose of the table? \n Answer: \n\nA. The purpose of the table is to express relationships between variables and their complementary values."}, {"text": "Title: Caring about Don't Cares P0 \n Text: In the context of a digital system, unspecified details\nmay or may not be important.  However, { any implementation of \na specification \nimplies decisions} about these details, so decisions should only be left \nunspecified if any of the possible answers is indeed acceptable. \n Question: \nQ. What is the use of Boolean logic? \n Answer: \n\nA. Boolean logic is a system of logic that is used to manipulate algebraic expressions and to identify equivalent logic functions."}, {"text": "Title: Caring about Don't Cares P1 \n Text: As a concrete example, let's design logic to control an ice cream \ndispenser.  The dispenser has two flavors, lychee and mango, but\nalso allows us to create a blend of the two flavors.\nFor each of the two flavors, our logic must output two bits\nto control the amount of ice cream that comes out of the dispenser.\nThe two-bit C_L[1:0] output of our logic must specify the number \nof half-servings of lychee ice cream as a binary number, and\nthe two-bit C_M[1:0] output must specify the number of \nhalf-servings of mango ice cream.  Thus, for either flavor,\n00 indicates none of that flavor,\n01 indicates one-half of a serving, and \n10 indicates a full serving. \n Question: \nQ. What is the value of underspecifying a logic function? \n Answer: \n\nA. The value of underspecifying a logic function is that it allows for selection of the simplest possible implementation."}, {"text": "Title: Caring about Don't Cares P2 \n Text: Inputs to our logic will consist of three buttons: an L button to\nrequest a serving of lychee ice cream, a B button to request a\nblend---half a serving of each flavor, and an M button to request\na serving of mango ice cream.  Each button produces a 1 \nwhen pressed and a 0 when not pressed. \n Question: \nQ. The cat slept on the mat.\nQ. \nQ. What did the cat sleep on? \n Answer: \n\nA. The cat slept on the mat."}, {"text": "Title: Caring about Don't Cares P3 \n Text: Let's start with the assumption that the user only presses one button\nat a time.  In this case, we can treat input combinations in which\nmore than one button is pressed as ``don't care'' values in the truth\ntables for the outputs.  K-maps for all four output bits appear below.\nThe x's indicate ``don't care'' values. \n Question: \nQ. What are some examples of important application areas that are mentioned? \n Answer: \n\nSome examples of important application areas that are mentioned include\nimage processing, computer vision, and machine learning."}, {"text": "Title: Caring about Don't Cares P4 \n Text: When we calculate the logic function for an output, each ``don't care''\nvalue can be treated as either 0 or 1, whichever is more convenient\nin terms of creating the logic.  In the case of C_M[1], for \nexample, we can treat the three x's in the ellipse as 1s, treat the x\noutside of the ellipse as a 0, and simply\nuse M (the implicant represented by the ellipse) for C_M[1].  The\nother three output bits are left as an exercise, although the result \nappears momentarily. \n Question: \nQ. Why does multiplication distribute over addition in the algebra of real numbers, but addition does not distribute over multiplication? \n Answer: \n\nA. In the algebra of real numbers, multiplication distributes over addition because the product of two real numbers is always a real number. However, addition does not necessarily distribute over multiplication because the sum of two real numbers may not be a real number. For example, the sum of 3 and 4 is 7, but the sum of 3 and 4 is not necessarily 11."}, {"text": "Title: Caring about Don't Cares P5 \n Text: \nThe implementation at right takes full advantage of the ``don't care''\nparts of our specification.  In this case, we require no logic at all;\nwe need merely connect the inputs to the correct outputs.  Let's verify\nthe operation.  We have four cases to consider.  First, if none of the \nbuttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00 \nand C_L=00).  Second, if we request lychee ice cream (LBM=100), the\noutputs are C_L=10 and C_M=00, so we get a full serving of lychee\nand no mango.  Third, if we request a blend (LBM=010), the outputs\nare C_L=01 and C_M=01, giving us half a serving of each flavor.\nFinally, if we request mango ice cream (LBM=001), we get no lychee\nbut a full serving of mango. \n Question: \nQ. What is the Boolean algebra duality? \n Answer: \n\nA. The Boolean algebra duality is a symmetry in which any logic statement (an expression or an equation) is related to a second logic statement."}, {"text": "Title: Caring about Don't Cares P6 \n Text: The K-maps for this implementation appear below.  Each of\nthe ``don't care'' x's from the original design has been replaced with\neither a 0 or a 1 and highlighted with shading and green italics.\nAny implementation produces \neither 0 or 1 for every output bit for every possible input combination. \n Question: \nQ. What does it mean to \"dual form\" a Boolean expression? \n Answer: \n\nA.\n\nTo \"dual form\" a Boolean expression means to replace 0 with 1, replace 1 with 0, replace AND with OR, and replace OR with AND."}, {"text": "Title: Caring about Don't Cares P7 \n Text: As you can see, leveraging ``don't care'' output bits can sometimes\nsignificantly simplify our logic.  In the case of this example, we were\nable to completely eliminate any need for gates!  Unfortunately, \nthe resulting implementation may sometimes produce unexpected results.  \n Question: \nQ. What does the author mean when they say \"variables are not changed when finding the dual form\"?\nQ. \nQ. The author is referring to the fact that, when solving a problem using the dual form, the original variables are not changed. Rather, the dual form simply provides an alternative way of solving the problem. \n Answer: "}, {"text": "Title: Caring about Don't Cares P8 \n Text: Based on the implementation, what happens if a user presses more\nthan one button?  The ice cream cup overflows! \n Question: \nQ. What is the dual form of a dual form? \n Answer: \n\nA. The dual form of a dual form is the original logic statement."}, {"text": "Title: Caring about Don't Cares P9 \n Text: Consider the case LBM=101, in which we've pressed both the lychee and\nmango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a \nfull serving of each flavor, or two servings total.  Pressing other \ncombinations may have other repercussions as well.\nConsider pressing lychee and blend (LBM=110).  The outputs are then\nC_L=11 and C_M=01.  Hopefully the dispenser simply gives us one\nand a half servings of lychee and a half serving of mango.  However,\nif the person who designed the dispenser assumed that no one would ever\nask for more than one serving, something worse might happen.  In other\nwords, giving an input of C_L=11 to the ice cream dispenser may lead to\nother unexpected behavior if its designer decided that that input \npattern was a ``don't care.'' \n Question: \nQ. Suppose A, B, and C are linear operators on a vector space V.  What is the dual of AB+C? \n Answer: \n\nA. (A+B)C"}, {"text": "Title: Caring about Don't Cares P10 \n Text: The root of the problem is that { while we don't care about the value of\nany particular output marked ``x'' for any particular input combination,\nwe do actually care about the relationship between the outputs}.   \n Question: \nQ. What does the author mean when they say \"Add parentheses as necessary when calculating a dual form to ensure that the order of operations does not change.\"?\nQ. \nQ. The author is suggesting that when calculating a dual form, parentheses should be added as necessary to ensure that the order of operations does not change. \n Answer: "}, {"text": "Title: Caring about Don't Cares P11 \n Text: What can we do?  When in doubt, it is safest to make \nchoices and to add the new decisions to the specification rather than \nleaving output values specified as ``don't care.'' \n Question: \nQ. What is the principle of duality? \n Answer: \n\nA. The principle of duality states that any theorem or identity has the same truth value in dual form."}, {"text": "Title: Caring about Don't Cares P12 \n Text: For our ice cream dispenser logic, rather than leaving the outputs \nunspecified whenever a user presses more than one button, we could \nchoose an acceptable outcome for each input combination and \nreplace the x's with 0s and 1s.  We might, for example, decide to\nproduce lychee ice cream whenever the lychee button is pressed, regardless\nof other buttons (LBM=1xx, which means that we don't care about the\ninputs B and M, so LBM=100, LBM=101, LBM=110, or LBM=111).  \nThat decision alone covers three of the\nfour unspecified input patterns.  We might also decide that when the \nblend and mango buttons are pushed together (but without the lychee\nbutton, LBM=011), our logic produces a blend.   \n Question: \nQ. What is the purpose of the table? \n Answer: \n\nA. The purpose of the table is to express relationships between variables and their complementary values."}, {"text": "Title: Caring about Don't Cares P13 \n Text: The resulting K-maps are shown below, again with shading and green italics \nidentifying\nthe combinations in which our original design specified ``don't care.'' \n Question: \nQ. What is the purpose of the dual form? \n Answer: \n\nA.\n\nThe dual form is useful for designing certain types of logic, such as the networks of transistors connecting the output of a CMOS gate to high voltage and ground."}, {"text": "Title: Caring about Don't Cares P14 \n Text: \nThe logic in the dashed box to the right implements the set of choices\njust discussed, and matches the K-maps above. \n Question: \nQ. Why does the principle of duality not apply to the dual of an expression? \n Answer: \n\nA. The principle of duality only applies to theorems and identities, not to expressions."}, {"text": "Title: Caring about Don't Cares P15 \n Text: Based on our additional choices, this implementation enforces\na strict priority scheme on the user's button presses. \n Question: \nQ. How is the value 0 interpreted in the dual form of an expression? \n Answer: \n\nA. The value 0 is interpreted as ``true''."}, {"text": "Title: Caring about Don't Cares P16 \n Text: If a user requests lychee, they can also press either or\nboth of the other buttons with no effect.  The lychee button has\npriority.  Similarly, if the user does not press lychee, but press- \n Question: \nQ. What does it mean to take the dual form of a Boolean function? \n Answer: \n\nA. It means to take the complement of each variable in the function."}, {"text": "Title: Caring about Don't Cares P17 \n Text: es the blend button, pressing the mango button at the same time has no\neffect.  Choosing mango requires that no other buttons be pressed.\nWe have thus chosen a prioritization order for the buttons and imposed \nthis order on the design. \n Question: \nQ. What is an example of something that a reader or listener would have to provide the details for? \n Answer: \n\nA. In the context above, an example of something that a reader or listener would have to provide the details for is how to take the speaker to the airport."}, {"text": "Title: Caring about Don't Cares P18 \n Text: We can view this same implementation in another way. \n Question: \nQ. What is the significance of the last line of the text? \n Answer: \n\nA. The last line of the text states that A B+ C+B C=A B+ C. This is known as the consensus property, which is a key principle in Boolean algebra."}, {"text": "Title: Caring about Don't Cares P19 \n Text: Note the one-to-one correspondence between inputs (on the left) and \noutputs (on the right) for the dashed box. \n Question: \nQ. What does the author mean by \"dual forms\"? \n Answer: \n\nA. The two columns are equivalent forms of each other."}, {"text": "Title: Caring about Don't Cares P20 \n Text: This logic takes the user's button presses and chooses at most one of\nthe buttons to pass along to our original controller implementation \n(to the right of the dashed box). \n Question: \nQ. What is the basic technique for implementing a Boolean function using combinational logic? \n Answer: \n\nA. The basic technique for implementing a Boolean function using combinational logic is to use a K-map to identify a reasonable SOP or POS form, draw the resulting design, and perhaps convert to NAND/NOR gates."}, {"text": "Title: Caring about Don't Cares P21 \n Text: In other words, rather than thinking of the logic in the dashed\nbox as implementing a specific set of decisions, we can think of the\nlogic as cleaning up the inputs to ensure that only valid\ncombinations are passed to our original implementation. \n Question: \nQ. What does it mean when a Boolean logic function's value is marked as \"don't care?\" \n Answer: \n\nA. This means that the function's value for this input combination is not specified and does not matter."}, {"text": "Title: Caring about Don't Cares P22 \n Text: Once the inputs are cleaned up, the original implementation is \nacceptable, because input combinations containing more than a \nsingle 1 are in fact impossible. \n Question: \nQ. \nQ. What is the benefit of using \"don't care\" values? \n Answer: \n\nA. The main benefit of using \"don't care\" values is that they can reduce the number of required input combinations (and thus the number of required test vectors) for a given circuit. This can be particularly helpful when testing complex digital circuits. In some cases, it may also be possible to use \"don't care\" values to simplify the circuit itself."}, {"text": "Title: Caring about Don't Cares P23 \n Text: Strict prioritization is one useful way to clean up our inputs. \n Question: \nQ. What are ``don't care'' values? \n Answer: \n\nA. \"Don't care\" values are values that can be either 0 or 1 without affecting the output of the logic function."}, {"text": "Title: Caring about Don't Cares P24 \n Text: In general, we can design logic to map each\nof the four undesirable input patterns into one of the permissible \ncombinations (the\nfour that we specified explicitly in our original design, with LBM \nin the set ).  Selecting a prioritization scheme\nis just one approach for making these choices in a way\nthat is easy for a user to understand and \nis fairly easy to implement. \n Question: \nQ. What is the significance of \"don't care\" input combinations? \n Answer: \n\nA. \"Don't care\" input combinations double the number of functions that can be chosen to implement the design. They often enable the logic needed for implementation to be simpler."}, {"text": "Title: Caring about Don't Cares P25 \n Text: \nA second simple approach is to ignore illegal\ncombinations by mapping them into the ``no buttons pressed'' \ninput pattern. \n Question: \nQ. What is the \"K-map\"? \n Answer: \n\nA. The K-map is a map of the Karnaugh space, which is a space used to visualize Boolean functions."}, {"text": "Title: Caring about Don't Cares P26 \n Text: Such an implementation appears to the right, laid out to show that\none can again view the logic in the dashed box either as cleaning up \nthe inputs (by mentally grouping the logic with the inputs) or as a specific \nset of choices for our ``don't care'' output values (by grouping the \nlogic with the outputs). \n Question: \nQ. What are the possible values for the two \"don't care\" entries? \n Answer: \n\nA. Since the ``x'' entries can be either 0 or 1, the two \"don't care\" entries can be either 0 or 1."}, {"text": "Title: Caring about Don't Cares P27 \n Text: In either case, the logic shown \nenforces our as- \n Question: \nQ. What are the four functions? \n Answer: \n\nA. The four functions are: y = x, y = -x, y = x^2, and y = -x^2."}, {"text": "Title: Caring about Don't Cares P28 \n Text: sumptions in a fairly conservative way:\nif a user presses more than one button, the logic squashes all button\npresses.  Only a single 1 value at a time can pass through to \nthe wires on the right of the figure. \n Question: \nQ. Why does the designer typically choose the third option? \n Answer: \n\nA. The designer typically chooses the third option because it produces the desired output for the given inputs."}, {"text": "Title: Caring about Don't Cares P29 \n Text: For completeness, the K-maps corresponding to this implementation are given\nhere. \n Question: \nQ. What does it mean when it says \"any implementation of a specification implies decisions about these details\"? \n Answer: \n\nA. \"Any implementation of a specification implies decisions about these details\" means that when you create an implementation of a specification, you have to make decisions about certain details in order to make the implementation work."}, {"text": "Title: Generalizations and Applications* P0 \n Text: The approaches that we illustrated to clean up the input signals to\nour design have application in many areas.  The ideas in this \nsection are drawn from the field and are sometimes the subjects of \nlater classes, but { are not exam material for our class.} \n Question: \nQ. What is the use of Boolean logic? \n Answer: \n\nA. Boolean logic is a system of logic that is used to manipulate algebraic expressions and to identify equivalent logic functions."}, {"text": "Title: Generalizations and Applications* P1 \n Text: Prioritization of distinct\ninputs is used to arbitrate between devices attached to a\nprocessor.  Processors typically execute much more quickly than do devices.\nWhen a device needs attention, the device signals the processor\nby changing the voltage on an interrupt line (the name comes from the\nidea that the device interrupts the processor's current activity, such\nas running a user program).  However, more than\none device may need the attention of the processor simultaneously, so\na priority encoder is used to impose a strict order on the devices and\nto tell the processor about their needs one at a time. \n Question: \nQ. What is the value of underspecifying a logic function? \n Answer: \n\nA. The value of underspecifying a logic function is that it allows for selection of the simplest possible implementation."}, {"text": "Title: Generalizations and Applications* P2 \n Text: If you want to learn more about this application, take ECE391. \n Question: \nQ. The cat slept on the mat.\nQ. \nQ. What did the cat sleep on? \n Answer: \n\nA. The cat slept on the mat."}, {"text": "Title: Generalizations and Applications* P3 \n Text: When components are designed together, assuming that some input patterns\ndo not occur is common practice, since such assumptions can dramatically\nreduce the number of gates required, improve performance, reduce power\nconsumption, and so forth.  As a side effect, when we want to test a\nchip to make sure that no defects or other problems prevent the chip\nfrom operating correctly, we have to be careful so as not to ``test''\nbit patterns that should never occur in practice.  Making up random bit\npatterns is easy, but can produce bad results or even destroy the chip\nif some parts of the design have assumed that a combination produced \nrandomly can never occur.  To avoid these problems, designers add extra\nlogic that changes the disallowed patterns into allowed patterns, just\nas we did with our design.  The use of random bit patterns is common in\nBuilt-In Self Test (BIST), and so the process of inserting extra logic\nto avoid problems is called BIST hardening.  BIST hardening can add\n{10-20} additional logic to a design. \n Question: \nQ. What are some examples of important application areas that are mentioned? \n Answer: \n\nSome examples of important application areas that are mentioned include\nimage processing, computer vision, and machine learning."}, {"text": "Title: Generalizations and Applications* P4 \n Text: Our graduate class on digital system testing, ECE543, covers this\nmaterial, but has not been offered recently. \n Question: \nQ. Why does multiplication distribute over addition in the algebra of real numbers, but addition does not distribute over multiplication? \n Answer: \n\nA. In the algebra of real numbers, multiplication distributes over addition because the product of two real numbers is always a real number. However, addition does not necessarily distribute over multiplication because the sum of two real numbers may not be a real number. For example, the sum of 3 and 4 is 7, but the sum of 3 and 4 is not necessarily 11."}, {"text": "Title: Example: Bit-Sliced Addition P0 \n Text: In this set of notes, we illustrate basic logic design using integer\naddition as an example.  By recognizing and mimicking the structured \napproach used by humans to perform addition, we introduce an important \nabstraction for logic design.  We follow this approach to design an\nadder known as a ripple-carry adder, then discuss some of the \nimplications of the approach and highlight how the same approach can \nbe used in software.  In the next set of notes, we use the same\ntechnique to design a comparator for two integers. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how to design an adder using a structured approach. \n Answer: "}, {"text": "Title: One Bit at a Time P0 \n Text: Many of the operations that we want to perform on groups of bits can \nbe broken down into repeated operations on individual bits. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how to design an adder using a structured approach. \n Answer: "}, {"text": "Title: One Bit at a Time P1 \n Text: When we add two binary numbers, for example, we first add the least\nsignificant bits, then move to the second least significant, and so on.\nAs we go, we may need to carry from lower bits into higher bits. \n Question: \nQ. What are some of the operations that we might want to perform on groups of bits? \n Answer: \n\nSome of the operations that we might want to perform on groups of bits include:\n-ANDing\n-ORing \n-XORing \n-NOTing \n-left/right shifting \n-bitwise rotation"}, {"text": "Title: One Bit at a Time P2 \n Text: When we compare two (unsigned) binary numbers with the same number of bits,\nwe usually start with the most significant bits and move downward in\nsignificance until we find a difference or reach the end of the two \nnumbers.  In the latter case, the two numbers are equal. \n Question: \nQ. What is the binary number for the decimal number 12?\nQ. \nQ. The binary number for the decimal number 12 is 1100. \n Answer: "}, {"text": "Title: One Bit at a Time P3 \n Text: When we build combinational logic to implement this kind of calculation,\nour approach as humans can be leveraged as an abstraction technique.\nRather than building and optimizing a different Boolean function for \nan 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size\nthat we might want, we can instead design a circuit that adds a single\nbit and passes any necessary information into another copy of itself.\nBy using copies of this { bit-sliced} adder circuit, we can\nmimic our approach as humans and build adders of any size, just as\nwe expect that a human could add two binary numbers of any size. \n Question: \nQ. What is the significance of the most significant bit? \n Answer: \n\nA. The most significant bit is the bit that has the most influence on the value of a binary number."}, {"text": "Title: One Bit at a Time P4 \n Text: The resulting designs are, of course, slightly less efficient than\ndesigns that are optimized for their specific purpose (such as adding\ntwo 17-bit numbers), but the simplicity of the approach makes the\ntradeoff an interesting one. \n Question: \nQ. How does the bit-sliced adder circuit work? \n Answer: \n\nA. The bit-sliced adder circuit is a combinational logic circuit that implements a bit-wise addition of two binary numbers. The adder is composed of a series of identical sub-circuits, each of which adds a single bit of the two binary numbers. The sub-circuits are interconnected so that the carry bit from each addition is passed to the next sub-circuit in the series. This carry propagation is what allows the bit-sliced adder to add two binary numbers of any size."}, {"text": "Title: Abstracting the Human Process P0 \n Text: \nThink about how we as humans add two {N-bit}\nnumbers, A and B. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how to design an adder using a structured approach. \n Answer: "}, {"text": "Title: Abstracting the Human Process P1 \n Text: An illustration appears to the right, using N=8. \n Question: \nQ. What are some of the operations that we might want to perform on groups of bits? \n Answer: \n\nSome of the operations that we might want to perform on groups of bits include:\n-ANDing\n-ORing \n-XORing \n-NOTing \n-left/right shifting \n-bitwise rotation"}, {"text": "Title: Abstracting the Human Process P2 \n Text: For now, let's assume that our numbers are stored in an unsigned \nrepresentation. \n Question: \nQ. What is the binary number for the decimal number 12?\nQ. \nQ. The binary number for the decimal number 12 is 1100. \n Answer: "}, {"text": "Title: Abstracting the Human Process P3 \n Text: As you know, addition for 2's complement is identical except for the\ncalculation of overflow. \n Question: \nQ. What is the significance of the most significant bit? \n Answer: \n\nA. The most significant bit is the bit that has the most influence on the value of a binary number."}, {"text": "Title: Abstracting the Human Process P4 \n Text: We start adding from the least significant bit and move to the left.\nSince adding two 1s can overflow a single bit, we carry a 1 when\nnecessary into the next column.  Thus, in general, we are actually\nadding three input bits.  The carry from the previous column is usually\nnot written explicitly by humans, but in a digital system\nwe need to write a 0 instead of leaving the value blank. \n Question: \nQ. How does the bit-sliced adder circuit work? \n Answer: \n\nA. The bit-sliced adder circuit is a combinational logic circuit that implements a bit-wise addition of two binary numbers. The adder is composed of a series of identical sub-circuits, each of which adds a single bit of the two binary numbers. The sub-circuits are interconnected so that the carry bit from each addition is passed to the next sub-circuit in the series. This carry propagation is what allows the bit-sliced adder to add two binary numbers of any size."}, {"text": "Title: Abstracting the Human Process P5 \n Text: Focus now on the addition of a single column.  Except for the\nfirst and last bits, which we might choose to handle slightly \ndifferently, the addition process is identical \nfor any column.  We add a carry in bit (possibly 0) with one\nbit from each of our numbers to produce a sum bit and a carry\nout bit for the next column.  Column addition is the task\nthat our bit slice logic must perform. \n Question: \nQ. How does the design simplicity compare to the efficiency of more specific designs? \n Answer: \n\nA. The simplicity of the design makes it less efficient than more specific designs."}, {"text": "Title: Abstracting the Human Process P6 \n Text: The diagram to the right shows an abstract model of our \nadder bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming\nfrom the top or left \nand outputs going to the bottom or right.  Outside\nof the bit slice logic, we index the carry bits using the \n Question: \nQ. What is the N-bit number? \n Answer: \n\nN-bit numbers are numbers that can be represented by N bits."}, {"text": "Title: Abstracting the Human Process P7 \n Text: \nbit number.  The bit slice has C^M provided as an input and \nproduces C^{M+1} as an output. \n Question: \nQ. What is the value of N in the illustration?\nQ. \nQ. N is equal to 8. \n Answer: "}, {"text": "Title: Abstracting the Human Process P8 \n Text: Internally, we use C_ to denote the carry input,\nand C_ to denote the carry output. \n Question: \nQ. What is the meaning of \"unsigned representation?\"\nQ. \nQ. In computer science, an unsigned representation is a way of storing numbers that can only represent positive values. \n Answer:  Unsigned numbers are typically stored using a bitwise representation, where each bit corresponds to a power of two."}, {"text": "Title: Abstracting the Human Process P9 \n Text: Similarly, the\nbits A_M and B_M from the numbers A and B are\nrepresented internally as A and B, and the bit S_M produced for\nthe sum S is represented internally as S.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear. \n Question: \nQ. What is the difference between addition for 2's complement and regular addition?\nQ. \nQ. In 2's complement, overflow is calculated differently. \n Answer: "}, {"text": "Title: Abstracting the Human Process P10 \n Text: The abstract device for adding three inputs bits and producing\ntwo output bits is called a { full adder}.  You may \nalso encounter the term { half adder}, which adds only two\ninput bits.  To form an {N-bit} adder, we integrate N\ncopies of the full adder---the bit slice that we design next---as \nshown below.  The result is called a { ripple carry adder}\nbecause the carry information moves from the low bits to the high\nbits slowly, like a ripple on the surface of a pond. \n Question: \nQ. What is the least significant bit? \n Answer: \n\nA. The least significant bit is the bit that is farthest to the right in a binary number. In the example above, the least significant bit is the 1 in the column labeled 2^0."}, {"text": "Title: Designing the Logic P0 \n Text: Now we are ready to design our adder bit slice.\nLet's start by writing a truth table for C_ and S,\nas shown on the left below. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how to design an adder using a structured approach. \n Answer: "}, {"text": "Title: Designing the Logic P1 \n Text: To the right of the truth tables are {K-maps} for each output,\nand equations for each output are then shown to the right of the \n{K-maps}. \n Question: \nQ. What are some of the operations that we might want to perform on groups of bits? \n Answer: \n\nSome of the operations that we might want to perform on groups of bits include:\n-ANDing\n-ORing \n-XORing \n-NOTing \n-left/right shifting \n-bitwise rotation"}, {"text": "Title: Designing the Logic P2 \n Text: We suggest that you work through identification of the prime implicants \nin the {K-maps} and check your work with the equations. \n Question: \nQ. What is the binary number for the decimal number 12?\nQ. \nQ. The binary number for the decimal number 12 is 1100. \n Answer: "}, {"text": "Title: Designing the Logic P3 \n Text: {\n[t]\n{ccc|cc}\nA& B& C_& C_& S \n0& 0& 0& 0& 0\n0& 1& 0& 0& 1\n0& 0& 1& 0& 1\n0& 1& 1& 1& 0\n1& 0& 0& 0& 1\n1& 0& 1& 1& 0\n1& 1& 0& 1& 0\n1& 1& 1& 1& 1 \n Question: \nQ. What is the significance of the most significant bit? \n Answer: \n\nA. The most significant bit is the bit that has the most influence on the value of a binary number."}, {"text": "Title: Designing the Logic P4 \n Text: {eqnarray*}\nC_&=&A B+A C_+B C_   \nS&=&A B C_+A  {C_+\n&& B {C_+  C_\n&=&A{C_\n{eqnarray*} \n Question: \nQ. How does the bit-sliced adder circuit work? \n Answer: \n\nA. The bit-sliced adder circuit is a combinational logic circuit that implements a bit-wise addition of two binary numbers. The adder is composed of a series of identical sub-circuits, each of which adds a single bit of the two binary numbers. The sub-circuits are interconnected so that the carry bit from each addition is passed to the next sub-circuit in the series. This carry propagation is what allows the bit-sliced adder to add two binary numbers of any size."}, {"text": "Title: Designing the Logic P5 \n Text: The equation for C_ implements a { majority function}\non three bits.  In particular, a carry is produced whenever at least two \nout of the three input bits (a majority) are 1s.  Why do we mention\nthis name?  Although we know that we can build any logic function from\nNAND gates, common functions such as those used to add numbers may\nbenefit from optimization.  Imagine that in some technology, creating\na majority function directly may produce a better result than implementing\nsuch a function from logic gates.  In such a case, we want the \nperson designing the circuit to know that can make use of such an \nimprovement. \n Question: \nQ. How does the design simplicity compare to the efficiency of more specific designs? \n Answer: \n\nA. The simplicity of the design makes it less efficient than more specific designs."}, {"text": "Title: Designing the Logic P6 \n Text: We rewrote the equation for S to make use of the XOR operation for \na similar reason: the implementation of XOR gates from transistors\nmay be slightly better than the implementation of XOR based on NAND gates.\nIf a circuit designer provides an optimized variant of XOR, we want\nour design to make use of the optimized version. \n Question: \nQ. What is the N-bit number? \n Answer: \n\nN-bit numbers are numbers that can be represented by N bits."}, {"text": "Title: Designing the Logic P7 \n Text: \nThe gate diagrams above implement a single bit slice for an adder.\nThe version on\nthe left uses AND and OR gates (and an XOR for the sum), while the\nversion on the right uses NAND gates, leaving the XOR as an XOR. \n Question: \nQ. What is the value of N in the illustration?\nQ. \nQ. N is equal to 8. \n Answer: "}, {"text": "Title: Designing the Logic P8 \n Text: Let's discuss the design in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\nFor each bit, we need three {2-input} NAND gates, one {3-input}\nNAND gate, and a {3-input} XOR gate (a big gate; around 30 transistors). \n Question: \nQ. What is the meaning of \"unsigned representation?\"\nQ. \nQ. In computer science, an unsigned representation is a way of storing numbers that can only represent positive values. \n Answer:  Unsigned numbers are typically stored using a bitwise representation, where each bit corresponds to a power of two."}, {"text": "Title: Designing the Logic P9 \n Text: For speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.   \n Question: \nQ. What is the difference between addition for 2's complement and regular addition?\nQ. \nQ. In 2's complement, overflow is calculated differently. \n Answer: "}, {"text": "Title: Designing the Logic P10 \n Text: We can thus estimate our design's\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer.\nHere we have two gate delays from any of the inputs\nto the C_ output.  The XOR gate may be a little slower, but\nnone of its inputs come from other gates anyway. \n Question: \nQ. What is the least significant bit? \n Answer: \n\nA. The least significant bit is the bit that is farthest to the right in a binary number. In the example above, the least significant bit is the 1 in the column labeled 2^0."}, {"text": "Title: Designing the Logic P11 \n Text: When we connect multiple copies of our bit slice logic together to \nform an adder, the A and B inputs\nto the outputs is not as important as the delay from C_\nto the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis---this propagation delay\ngives rise to the name ``ripple carry.''\nLooking again at the diagram, \nnotice that we have two gate delays from C_\nto C_.\nThe total delay for an {N-bit} comparator based on this\nimplementation is thus two gate delays per bit, \nfor a total of 2N gate delays. \n Question: \nQ. What does the author mean by \"the addition process is identical for any column\"? \n Answer: \n\nA. The author means that the process of adding a column of bits is the same regardless of the values of the bits being added."}, {"text": "Title: Adders and Word Size\\vspace12pt P0 \n Text: \nNow that we know how to build an {N-bit} adder, we can add\nsome detail to the diagram that we drew when we \nintroduced 2's complement back in Notes Set 1.2, as shown to the right. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how to design an adder using a structured approach. \n Answer: "}, {"text": "Title: Adders and Word Size\\vspace12pt P1 \n Text: The adder is important enough to computer systems to merit its own\nsymbol in logic diagrams, which is shown to the right with the inputs\nand outputs from our design added as labels.  The text in the middle\nmarking the symbol as an adder is only included for clarity: { any time \nyou see a symbol of the shape shown to the right, it is an adder} (or \nsometimes a device that can add and do other operations).  The width \nof the operand input and output lines then tells you the size of the \nadder. \n Question: \nQ. What are some of the operations that we might want to perform on groups of bits? \n Answer: \n\nSome of the operations that we might want to perform on groups of bits include:\n-ANDing\n-ORing \n-XORing \n-NOTing \n-left/right shifting \n-bitwise rotation"}, {"text": "Title: Adders and Word Size\\vspace12pt P2 \n Text: You may already know that most computers have a { word size}\nspecified as part of the Instruction Set Architecture.  The word\nsize specifies the number of bits in each operand when the computer\nadds two numbers, and is often used widely within the \nmicroarchitecture as well (for example, to decide the number of \nwires to use when moving bits around).  Most desktop and laptop machines\nnow have a word size of 64 bits, but many phone processors (and\ndesktops/laptops a few years ago) use a {32-bit} word size.\nEmbedded microcontrollers may use a {16-bit} or even \nan {8-bit} word size. \n Question: \nQ. What is the binary number for the decimal number 12?\nQ. \nQ. The binary number for the decimal number 12 is 1100. \n Answer: "}, {"text": "Title: Adders and Word Size\\vspace12pt P3 \n Text: \nHaving seen how we can build an {N-bit} adder from simple\nchunks of logic operating on each pair of bits, you should not have\nmuch difficulty in understanding the diagram to the right. \n Question: \nQ. What is the significance of the most significant bit? \n Answer: \n\nA. The most significant bit is the bit that has the most influence on the value of a binary number."}, {"text": "Title: Adders and Word Size\\vspace12pt P4 \n Text: If we start with a design for an {N-bit} adder---even if that\ndesign is not built from bit slices, but is instead optimized for\nthat particular size---we can create a {2N-bit} adder by \nsimply connecting two copies of the {N-bit} adder.  We give\nthe adder for the less significant bits (the one on the right\nin the figure) an initial carry of 0,\nand pass the carry produced by the adder for the less significant\nbits into the carry input of the adder for the more significant\nbits.  We calculate overflow based on the results of the adder\nfor more significant bits (the one on the left in the figure), \nusing the method appropriate to the \ntype of operands we are adding (either unsigned or 2's complement). \n Question: \nQ. How does the bit-sliced adder circuit work? \n Answer: \n\nA. The bit-sliced adder circuit is a combinational logic circuit that implements a bit-wise addition of two binary numbers. The adder is composed of a series of identical sub-circuits, each of which adds a single bit of the two binary numbers. The sub-circuits are interconnected so that the carry bit from each addition is passed to the next sub-circuit in the series. This carry propagation is what allows the bit-sliced adder to add two binary numbers of any size."}, {"text": "Title: Adders and Word Size\\vspace12pt P5 \n Text: You should also realize that this connection need not be physical.\nIn other words, if a computer has an {N-bit} adder, it can\nhandle operands with 2N bits (or 3N, or 10N, or 42N) by\nusing the {N-bit} adder repeatedly, starting with the\nleast significant bits and working upward until all of the bits\nhave been added.  The computer must of course arrange to have the\noperands routed to the adder a few bits at a time, and must\nensure that the carry produced by each addition is then delivered to\nthe carry input (of the same adder!) for the next addition.\nIn the coming months, you will learn how to design hardware that\nallows you to manage bits in this way, so that by the end of our\nclass, you will be able to design a simple computer on your own. \n Question: \nQ. How does the design simplicity compare to the efficiency of more specific designs? \n Answer: \n\nA. The simplicity of the design makes it less efficient than more specific designs."}, {"text": "Title: Example: Bit-Sliced Comparison P0 \n Text: This set of notes develops comparators for unsigned and 2's complement \nnumbers using the bit-sliced approach that we introduced in Notes Set 2.3.  \nWe then use algebraic manipulation and variation of the internal \nrepresentation to illustrate design tradeoffs. \n Question: \nQ. What are some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers? \n Answer: \n\nA. Some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers include:\n\n- The number of bits used to represent the numbers\n- The order in which the bits are compared\n- The number of bits used for the output"}, {"text": "Title: Comparing Two Numbers P0 \n Text: \nLet's begin by thinking about how we as humans compare two {N-bit}\nnumbers, A and B. \n Question: \nQ. What are some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers? \n Answer: \n\nA. Some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers include:\n\n- The number of bits used to represent the numbers\n- The order in which the bits are compared\n- The number of bits used for the output"}, {"text": "Title: Comparing Two Numbers P1 \n Text: An illustration appears to the right, using N=8. \n Question: \nQ. How do we compare two N-bit numbers, A and B? \n Answer: \n\nA. We compare two N-bit numbers, A and B, by looking at the most significant bit (MSB) first. If the MSBs are equal, we compare the next most significant bits, and so on until we find a difference or we reach the end of the number."}, {"text": "Title: Comparing Two Numbers P2 \n Text: For now, let's assume that our numbers are stored in an unsigned \nrepresentation, so we can just think of them as binary numbers\nwith leading 0s. \n Question: \nQ. What value of N will produce the longest path? \n Answer: \n\nA. The longest path will be produced when N is equal to 8."}, {"text": "Title: Comparing Two Numbers P3 \n Text: We handle 2's complement values later in these notes. \n Question: \nQ. How would you convert the number 1011 to decimal?\nQ. \nQ. The number 1011 in decimal would be 11. \n Answer: "}, {"text": "Title: Comparing Two Numbers P4 \n Text: As humans, we typically start comparing at the most significant bit.\nAfter all, if we find a difference in that bit, we are done, saving\nourselves some time.  In the example to the right, we know that A<B \nas soon as we reach bit 4 and observe that A_4<B_4. \n Question: \nQ. What is the value of a byte in 2's complement form?\nQ. \nQ. A byte in 2's complement form has a value of -128 to 127. \n Answer: "}, {"text": "Title: Comparing Two Numbers P5 \n Text: If we instead start from the least significant bit,\nwe must always look at all of the bits. \n Question: \nQ. Why do humans typically start comparing at the most significant bit? \n Answer: \n\nA. So that they can save time."}, {"text": "Title: Comparing Two Numbers P6 \n Text: When building hardware to compare all of the bits at once, however,\nhardware for comparing each bit must exist, and the final result \nmust be able to consider \n Question: \nQ. Why does the author say that if we start from the least significant bit, we must always look at all of the bits? \n Answer: \n\nA. The author is saying that if we start from the least significant bit, we will not be able to skip any bits because we will always need to look at them in order to determine the value of the number."}, {"text": "Title: Comparing Two Numbers P7 \n Text: all of the bits.  Our choice of direction should thus instead depend on \nhow effectively we can build the \ncorresponding functions.  For a single bit slice, the two directions \nare almost identical.  Let's develop a bit slice for\ncomparing from least to most significant. \n Question: \nQ. What is the final result of the hardware comparison? \n Answer: \n\nA. The final result of the hardware comparison is the determination of which bit is different."}, {"text": "Title: Comparing Two Numbers P8 \n Text: { NOTE TO SELF: We should either do the bit-by-bit comparator \nstate machine in the notes or as an FSM homework problem.  Probably\nthe lattter.} \n Question: \nQ. How does the direction we choose to build our functions affect the comparison of bits? \n Answer: \n\nA. The direction we choose to build our functions affects the comparison of bits because it determines the order in which the bits are compared."}, {"text": "Title: An Abstract Model P0 \n Text: Comparison of two numbers, A and B, can produce three possible\nanswers: A<B, A=B, or A>B (one can also build an equality\ncomparator that combines the A<B and A>B cases into a single \nanswer). \n Question: \nQ. What are some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers? \n Answer: \n\nA. Some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers include:\n\n- The number of bits used to represent the numbers\n- The order in which the bits are compared\n- The number of bits used for the output"}, {"text": "Title: An Abstract Model P1 \n Text: As we move from bit to bit in our design, how much information needs \nto pass from one bit to the next? \n Question: \nQ. How do we compare two N-bit numbers, A and B? \n Answer: \n\nA. We compare two N-bit numbers, A and B, by looking at the most significant bit (MSB) first. If the MSBs are equal, we compare the next most significant bits, and so on until we find a difference or we reach the end of the number."}, {"text": "Title: An Abstract Model P2 \n Text: Here you may want to think about how you perform the task yourself.\nAnd perhaps to focus on the calculation for the most significant bit.\nYou need to know the values of the two bits that you are comparing.\nIf those two are not equal, you are done. \n Question: \nQ. What value of N will produce the longest path? \n Answer: \n\nA. The longest path will be produced when N is equal to 8."}, {"text": "Title: An Abstract Model P3 \n Text: But if the two bits are equal, what do you do? \n Question: \nQ. How would you convert the number 1011 to decimal?\nQ. \nQ. The number 1011 in decimal would be 11. \n Answer: "}, {"text": "Title: An Abstract Model P4 \n Text: The answer is fairly simple: pass along the result\nfrom the less significant bits. \n Question: \nQ. What is the value of a byte in 2's complement form?\nQ. \nQ. A byte in 2's complement form has a value of -128 to 127. \n Answer: "}, {"text": "Title: An Abstract Model P5 \n Text: Thus our bit slice logic for bit M needs to be able to accept \nthree possible answers from the bit slice logic for bit M-1 and must\nbe able to pass one of three possible answers to the logic for bit M+1. \n Question: \nQ. Why do humans typically start comparing at the most significant bit? \n Answer: \n\nA. So that they can save time."}, {"text": "Title: An Abstract Model P6 \n Text: Since _2(3)=2, we need two bits of input and two bits\nof output in addition to our input bits from numbers A and B. \n Question: \nQ. Why does the author say that if we start from the least significant bit, we must always look at all of the bits? \n Answer: \n\nA. The author is saying that if we start from the least significant bit, we will not be able to skip any bits because we will always need to look at them in order to determine the value of the number."}, {"text": "Title: An Abstract Model P7 \n Text: \nThe diagram to the right shows an abstract model of our \ncomparator bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming from the top or left \nand outputs going to the bottom or right.  Outside of the bit \nslice logic, we index\nthese comparison bits using the bit number.  The bit slice\nhas C_1^{M-1} and C_0^{M-1} provided as inputs and \nproduces C_1^M and C_0^M as outputs. \n Question: \nQ. What is the final result of the hardware comparison? \n Answer: \n\nA. The final result of the hardware comparison is the determination of which bit is different."}, {"text": "Title: An Abstract Model P8 \n Text: Internally, we use C_1 and C_0 to denote these inputs,\nand Z_1 and Z_0 to denote the outputs. \n Question: \nQ. How does the direction we choose to build our functions affect the comparison of bits? \n Answer: \n\nA. The direction we choose to build our functions affects the comparison of bits because it determines the order in which the bits are compared."}, {"text": "Title: An Abstract Model P9 \n Text: bits A_M and B_M from the numbers A and B are\nrepresented internally simply as A and B.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear. \n Question: \nQ. How does the bit-by-bit comparator work? \n Answer: \n\nA. The bit-by-bit comparator compares two values bit-by-bit. It starts with the most significant bit (MSB) and compares each bit until it reaches the end of the value. If all the bits match, then the two values are equal."}, {"text": "Title: A Representation and the First Bit P0 \n Text: \nWe need to select a representation for our three possible answers before\nwe can design any logic.  The representation chosen affects the\nimplementation, as we discuss later in these notes.  For now, we simply\nchoose the representation to the right, which seems reasonable. \n Question: \nQ. What are some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers? \n Answer: \n\nA. Some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers include:\n\n- The number of bits used to represent the numbers\n- The order in which the bits are compared\n- The number of bits used for the output"}, {"text": "Title: A Representation and the First Bit P1 \n Text: Now we can design the logic for the first bit (bit 0).  In keeping with\nthe bit slice philosophy, in practice we simply use another\ncopy of the full bit slice design for bit 0 and attach the C_1C_0 \ninputs to ground (to denote A=B).  Here we tackle the simpler problem\nas a warm-up exercise. \n Question: \nQ. How do we compare two N-bit numbers, A and B? \n Answer: \n\nA. We compare two N-bit numbers, A and B, by looking at the most significant bit (MSB) first. If the MSBs are equal, we compare the next most significant bits, and so on until we find a difference or we reach the end of the number."}, {"text": "Title: A Representation and the First Bit P2 \n Text: \n{cc|l}\nC_1& C_0& meaning \n0& 0& A=B\n0& 1& A<B\n1& 0& A>B\n1& 1& not used \n Question: \nQ. What value of N will produce the longest path? \n Answer: \n\nA. The longest path will be produced when N is equal to 8."}, {"text": "Title: A Representation and the First Bit P3 \n Text: \nThe truth table for bit 0 appears to the right (recall that we use Z_1\nand Z_0 for the output names).  Note that the bit 0 function has\nonly two meaningful inputs---there is no bit to the right of bit 0. \n Question: \nQ. How would you convert the number 1011 to decimal?\nQ. \nQ. The number 1011 in decimal would be 11. \n Answer: "}, {"text": "Title: A Representation and the First Bit P4 \n Text: If the two inputs A and B are the same, we output equality.\nOtherwise, we do a {1-bit} comparison and use our representation mapping\nto select the outputs. \n Question: \nQ. What is the value of a byte in 2's complement form?\nQ. \nQ. A byte in 2's complement form has a value of -128 to 127. \n Answer: "}, {"text": "Title: A Representation and the First Bit P5 \n Text: These functions are fairly straightforward to derive by inspection.\nThey are:{-12pt} \n Question: \nQ. Why do humans typically start comparing at the most significant bit? \n Answer: \n\nA. So that they can save time."}, {"text": "Title: A Representation and the First Bit P6 \n Text: {eqnarray*}\nZ_1 &=& A \nZ_0 &=&  B\n{eqnarray*} \n Question: \nQ. Why does the author say that if we start from the least significant bit, we must always look at all of the bits? \n Answer: \n\nA. The author is saying that if we start from the least significant bit, we will not be able to skip any bits because we will always need to look at them in order to determine the value of the number."}, {"text": "Title: A Representation and the First Bit P7 \n Text: \n{\n{cc|cc}\nA& B& Z_1& Z_0 \n0& 0& 0& 0\n0& 1& 0& 1\n1& 0& 1& 0\n1& 1& 0& 0 \n Question: \nQ. What is the final result of the hardware comparison? \n Answer: \n\nA. The final result of the hardware comparison is the determination of which bit is different."}, {"text": "Title: A Representation and the First Bit P8 \n Text: \nThese forms should also be intuitive, given the representation\nthat we chose: \nA>B if and only if A=1 and B=0;\nA<B if and only if A=0 and B=1.  \n Question: \nQ. How does the direction we choose to build our functions affect the comparison of bits? \n Answer: \n\nA. The direction we choose to build our functions affects the comparison of bits because it determines the order in which the bits are compared."}, {"text": "Title: A Representation and the First Bit P9 \n Text: \nImplementation diagrams for our one-bit functions appear to the right.\nThe diagram to the immediate right shows the implementation as we might\ninitially draw it, and the diagram on the far right shows the \nimplementation \n Question: \nQ. How does the bit-by-bit comparator work? \n Answer: \n\nA. The bit-by-bit comparator compares two values bit-by-bit. It starts with the most significant bit (MSB) and compares each bit until it reaches the end of the value. If all the bits match, then the two values are equal."}, {"text": "Title: A Representation and the First Bit P10 \n Text: \nconverted to NAND/NOR gates for a more accurate estimate of complexity\nwhen implemented in CMOS. \n Question: \nQ. What does the comparison of two numbers produce? \n Answer: \n\nA. The comparison of two numbers produces three possible answers: A<B, A=B, or A>B."}, {"text": "Title: A Representation and the First Bit P11 \n Text: The exercise of designing the logic for bit 0 is also useful in the sense\nthat the logic structure illustrated forms the core of the full design\nin that it identifies the two cases that matter: A<B and A>B. \n Question: \nQ. As we move from bit to bit in our design, how much information needs to pass from one bit to the next?\nQ. \nQ. How is information passed from one bit to the next in the design? \n Answer: "}, {"text": "Title: A Representation and the First Bit P12 \n Text: Now we are ready to design the full function.  Let's start by writing\na full truth table, as shown on the left below. \n Question: \nQ. What does the text say about how to perform the task?\nQ. \nQ. The text does not say how to perform the task. \n Answer: "}, {"text": "Title: A Representation and the First Bit P13 \n Text: [t]\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0\n0& 0& 1& 1& x& x \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1\n0& 1& 1& 1& x& x \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0\n1& 0& 1& 1& x& x \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0\n1& 1& 1& 1& x& x \n Question: \nQ. What does the text say about what to do if the two bits are equal?\nQ. \nQ. If the two bits are equal, the text says to generate a question. \n Answer: "}, {"text": "Title: A Representation and the First Bit P14 \n Text: {cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0 \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1 \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0 \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0 \nx& x& 1& 1& x& x\n   \n Question: \nQ. What does it mean when it says \"pass along the result from the less significant bits\"? \n Answer: \n\nA. This means that the result of the operation on the less significant bits is propagated to the more significant bits."}, {"text": "Title: A Representation and the First Bit P15 \n Text: {cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0 \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1 \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0 \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0 \n{c|}& x& x\n   \n Question: \nQ. What are the three possible answers from bit slice logic for bit M-1?\nQ. \nQ. The three possible answers from bit slice logic for bit M-1 are 0, 1, and X. \n Answer: "}, {"text": "Title: A Representation and the First Bit P16 \n Text: In the truth table, we \nmarked the outputs as ``don't care'' (x's) whenever C_1C_0=11.\nYou might recall that we ran into problems with our ice cream dispenser\ncontrol in Notes Set 2.2.  However, in that case we could not safely\nassume that a user did not push multiple buttons.  Here, our bit\nslice logic only accepts inputs from other copies of itself (or a fixed\nvalue for bit 0), and---assuming that we design the logic\ncorrectly---our bit slice never generates the 11 combination.\nIn other words, that input combination is impossible (rather than\nundesirable or unlikely), so the result produced on the outputs is \nirrelevant. \n Question: \nQ. What does the text say about the number of bits needed for input and output? \n Answer: \n\nThe text says that, in addition to the input bits from numbers A and B, two bits of input and two bits of output are needed."}, {"text": "Title: A Representation and the First Bit P17 \n Text: It is tempting to shorten the full truth table by replacing groups of rows.\nFor example, if AB=01, we know that A<B, so the less significant\nbits (for which the result is represented by the C_1C_0 inputs)\ndon't matter.  We could write one row with input pattern ABC_1C_0=01xx and\noutput pattern Z_1Z_0=01.  We might also collapse our ``don't care'' output\npatterns: whenever the input matches ABC_1C_0=xx11, we don't care\nabout the output, so Z_1Z_0=xx.  But these two rows overlap in the\ninput space!  In other words, some input patterns, such as ABC_1C_0=0111,\nmatch both of our suggested new rows.  Which output should take precedence?\nThe answer is that a reader should not have to guess.  { Do not use \noverlapping rows to shorten a truth table.}  In fact, the first of the \nsuggested new rows is not valid: we don't need to produce output 01 \nif we see C_1C_0=11.  Two valid short forms of this truth table appear to \nthe right of the full table.  If you have an ``other'' entry, as\nshown in the rightmost table, this entry should always appear as the \nlast row.  Normal rows, including rows representing multiple input\npatterns, are not required\nto be in any particular order.  Use whatever order makes the table\neasiest to read for its purpose (usually by treating the input pattern\nas a binary number and ordering rows in increasing numeric order). \n Question: \nQ. Who/what provides the C_1^{M-1} and C_0^{M-1} inputs? \n Answer: \n\nA. The next least significant bit provides the C_1^{M-1} and C_0^{M-1} inputs."}, {"text": "Title: A Representation and the First Bit P18 \n Text: \nIn order to translate our design into algebra, we transcribe the truth\ntable into a {K-map} for each output variable, as shown to the right.\nYou may want to perform this exercise yourself and check that you \nobtain the same solution.  Implicants for each output are marked\nin the {K-maps}, giving the following equations: \n Question: \nQ. What does the author mean by \"internally\"? \n Answer: \n\nA. The author means that the inputs are denoted by C_1 and C_0, and the outputs are denoted by Z_1 and Z_0."}, {"text": "Title: A Representation and the First Bit P19 \n Text: {eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\nZ_0 &=&  B +  C_0 + B C_0 \n{eqnarray*} \n Question:  \n Answer: \nThe output of an XOR gate is A XOR B.  If we flip the inputs to the XOR\ngate (i.e. if A becomes ~A and B becomes ~B), what does the output of\nthe XOR gate become?\n\nThe output of an XOR gate is A XOR B.  If we flip the inputs to the XOR\ngate (i.e. if A becomes ~A and B becomes ~B), the output of the XOR gate becomes ~(A XOR B)."}, {"text": "Title: A Representation and the First Bit P20 \n Text: An implementation based on our equations appears to the right.  \nThe figure makes it easy to see the symmetry between the inputs, which\narises from the representation that we've chosen.  Since the design\nonly uses two-level logic (not counting the inverters on the A\nand B inputs, since inverters can be viewed as {1-input} \nNAND or NOR gates), converting to NAND/NOR simply requires replacing\nall of the AND and OR gates with NAND gates. \n Question: \nQ. What is the representation of the three possible answers?\nQ. \nQ. The representation of the three possible answers is a list. \n Answer: "}, {"text": "Title: A Representation and the First Bit P21 \n Text: Let's discuss the design's efficiency roughly in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate. \n Question: \nQ. What is the bit slice philosophy? \n Answer: \n\nThe bit slice philosophy is to design each bit of a digital circuit separately. This approach is often used in circuit design because it is simpler and easier to understand each individual bit."}, {"text": "Title: A Representation and the First Bit P22 \n Text: Our initial design uses two inverters, six {2-input} gates, and \ntwo {3-input} gates. \n Question: \nQ. What is the meaning of a C_1 and C_0 value of 1 and 1?\nQ. \nQ. This is not a valid combination of values. \n Answer: "}, {"text": "Title: A Representation and the First Bit P23 \n Text: For speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.   \n Question: \nQ. What does the bit 0 function have to do with the other bits? \n Answer: \n\nA. The bit 0 function is the exclusive OR of bits 1 and 2."}, {"text": "Title: A Representation and the First Bit P24 \n Text: We can thus estimate our design's \n Question: \nQ. What is the meaning of the phrase \"1-bit comparison\"? \n Answer: \n\nA. A 1-bit comparison is a comparison in which only one bit is considered."}, {"text": "Title: A Representation and the First Bit P25 \n Text: speed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer,\nbut, as we have discussed, the diagram above is equivalent on a \ngate-for-gate basis.  Here we have three gate delays from the A \nand B inputs to the outputs (through the inverters). \n Question: \nQ. \nQ. The purpose of the text is to provide information about the functions of the hypothalamus. \n Answer: "}, {"text": "Title: A Representation and the First Bit P26 \n Text: But when we connect multiple copies of our bit slice logic together to \nform a comparator, as shown on the next page, the delay from \nthe A and B inputs\nto the outputs is not as important as the delay from the C_1 and C_0 \ninputs to the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis.  Looking again at the diagram, \nnotice that we have only two gate delays from the C_1 and C_0 inputs \nto the\noutputs.  The total delay for an {N-bit} comparator based on this\nimplementation is thus three gate delays for bit 0 and two more gate\ndelays per additional bit, for a total of 2N+1 gate delays. \n Question: \nQ. This text describes a truth table for a two-input, two-output logic gate. What is the name of this type of logic gate? \n Answer: \n\nA. This is a truth table for a two-input, two-output logic gate called an AND gate."}, {"text": "Title: Optimizing Our Design P0 \n Text: We have a fairly good design at this point---good enough for a homework\nor exam problem in this class, certainly---but let's consider how we\nmight further optimize it.  Today, optimization of logic at this level is \ndone mostly by computer-aided design (CAD) tools, but we want you to \nbe aware of the sources of optimization potential and the tradeoffs \ninvolved.  And, if the topic interests you, someone has to continue to \nimprove CAD software! \n Question: \nQ. What are some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers? \n Answer: \n\nA. Some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers include:\n\n- The number of bits used to represent the numbers\n- The order in which the bits are compared\n- The number of bits used for the output"}, {"text": "Title: Optimizing Our Design P1 \n Text: The first step is to manipulate our algebra to expose common terms that\noccur due to the design's symmetry.  Starting with our original equation\nfor Z_1, we have \n Question: \nQ. How do we compare two N-bit numbers, A and B? \n Answer: \n\nA. We compare two N-bit numbers, A and B, by looking at the most significant bit (MSB) first. If the MSBs are equal, we compare the next most significant bits, and so on until we find a difference or we reach the end of the number."}, {"text": "Title: Optimizing Our Design P2 \n Text: {eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\n    &=& A  + ( A +  )  C_1\n    &=& A  +  B} C_1\n     Z_0 &=&  B + {A  C_0\n{eqnarray*}{-12pt} \n Question: \nQ. What value of N will produce the longest path? \n Answer: \n\nA. The longest path will be produced when N is equal to 8."}, {"text": "Title: Optimizing Our Design P3 \n Text: Notice that the second term in each equation now includes the complement of \nfirst term from the other equation.  For example, the Z_1 equation includes\nthe complement of the B product that we need to compute Z_0.  \nWe may be able to improve our design by combining these computations. \n Question: \nQ. How would you convert the number 1011 to decimal?\nQ. \nQ. The number 1011 in decimal would be 11. \n Answer: "}, {"text": "Title: Optimizing Our Design P4 \n Text: \nAn implementation based on our new algebraic formulation appears to the \nright.  In this form, we seem to have kept the same number of gates,\nalthough we have replaced the {3-input} gates with inverters.\nHowever, the middle inverters disappear when we convert to NAND/NOR form,\nas shown below to the right.  Our new design requires only two inverters\nand six {2-input} gates, a substantial reduction relative to the\noriginal implementation. \n Question: \nQ. What is the value of a byte in 2's complement form?\nQ. \nQ. A byte in 2's complement form has a value of -128 to 127. \n Answer: "}, {"text": "Title: Optimizing Our Design P5 \n Text: Is there a disadvantage?  Yes, but only a slight one.  Notice that the\npath from the A and B inputs to the outputs is now four gates (maximum)\ninstead of three.  Yet the path from C_1 and C_0 to the outputs is\nstill only two gates.  Thus, overall, we have merely increased our\n{N-bit} comparator's delay from 2N+1 gate delays to\n2N+2 gate delays. \n Question: \nQ. Why do humans typically start comparing at the most significant bit? \n Answer: \n\nA. So that they can save time."}, {"text": "Title: Extending to 2's Complement P0 \n Text: What about comparing 2's complement numbers?  Can we make use of the\nunsigned comparator that we just designed? \n Question: \nQ. What are some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers? \n Answer: \n\nA. Some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers include:\n\n- The number of bits used to represent the numbers\n- The order in which the bits are compared\n- The number of bits used for the output"}, {"text": "Title: Extending to 2's Complement P1 \n Text: Let's start by thinking about the sign of the numbers A and B.\nRecall that 2's complement records a number's sign in the most \nsignificant bit.  For example, in the {8-bit} numbers shown in the \nfirst diagram in this set of notes, the sign \nbits are A_7 and B_7. \n Question: \nQ. How do we compare two N-bit numbers, A and B? \n Answer: \n\nA. We compare two N-bit numbers, A and B, by looking at the most significant bit (MSB) first. If the MSBs are equal, we compare the next most significant bits, and so on until we find a difference or we reach the end of the number."}, {"text": "Title: Extending to 2's Complement P2 \n Text: Let's denote these sign bits in the general case by A_s and B_s. \n Question: \nQ. What value of N will produce the longest path? \n Answer: \n\nA. The longest path will be produced when N is equal to 8."}, {"text": "Title: Extending to 2's Complement P3 \n Text: Negative numbers have a sign bit equal to 1, and non-negative numbers\nhave a sign bit equal to 0. \n Question: \nQ. How would you convert the number 1011 to decimal?\nQ. \nQ. The number 1011 in decimal would be 11. \n Answer: "}, {"text": "Title: Extending to 2's Complement P4 \n Text: The table below outlines an initial evaluation of the four possible\ncombinations of sign bits. \n Question: \nQ. What is the value of a byte in 2's complement form?\nQ. \nQ. A byte in 2's complement form has a value of -128 to 127. \n Answer: "}, {"text": "Title: Extending to 2's Complement P5 \n Text: {cc|c|l}\nA_s& B_s& interpretation& solution \n0& 0& A AND B& use unsigned comparator on remaining bits\n0& 1& A AND B<0& A>B\n1& 0& A<0 AND B& A<B\n1& 1& A<0 AND B<0& unknown \n Question: \nQ. Why do humans typically start comparing at the most significant bit? \n Answer: \n\nA. So that they can save time."}, {"text": "Title: Extending to 2's Complement P6 \n Text: What should we do when both numbers are negative?  Need we design \na completely separate logic circuit?  Can we somehow convert a \nnegative value to a positive one? \n Question: \nQ. Why does the author say that if we start from the least significant bit, we must always look at all of the bits? \n Answer: \n\nA. The author is saying that if we start from the least significant bit, we will not be able to skip any bits because we will always need to look at them in order to determine the value of the number."}, {"text": "Title: Extending to 2's Complement P7 \n Text: The answer is in fact much simpler.  Recall that {2's complement}\nis defined based on modular arithmetic.  Given an {N-bit} negative\nnumber A, the representation for the bits A[N-2:0] is the same\nas the binary (unsigned) representation of A+2^{N-1}.  An example\nappears to the right. \n Question: \nQ. What is the final result of the hardware comparison? \n Answer: \n\nA. The final result of the hardware comparison is the determination of which bit is different."}, {"text": "Title: Extending to 2's Complement P8 \n Text: Let's define A_r=A+2^{N-1} as the value of the \nremaining bits for A and B_r similarly for B. \n Question: \nQ. How does the direction we choose to build our functions affect the comparison of bits? \n Answer: \n\nA. The direction we choose to build our functions affects the comparison of bits because it determines the order in which the bits are compared."}, {"text": "Title: Extending to 2's Complement P9 \n Text: What happens if we just go ahead and compare A_r and B_r using \nan {(N-1)-bit} unsigned comparator? \n Question: \nQ. How does the bit-by-bit comparator work? \n Answer: \n\nA. The bit-by-bit comparator compares two values bit-by-bit. It starts with the most significant bit (MSB) and compares each bit until it reaches the end of the value. If all the bits match, then the two values are equal."}, {"text": "Title: Extending to 2's Complement P10 \n Text: If we find that A_r<B_r we know that A_r-2^{N-1}<B_r-2^{N-1} as well, \nbut that means A<B!  We can do the same with either of the other\npossible results.  In other words, simply comparing A_r with B_r\ngives the correct answer for two negative numbers as well. \n Question: \nQ. What does the comparison of two numbers produce? \n Answer: \n\nA. The comparison of two numbers produces three possible answers: A<B, A=B, or A>B."}, {"text": "Title: Extending to 2's Complement P11 \n Text: \nAll we need to design is a logic block for the sign bits.  At this point,\nwe might write out a {K-map}, but instead let's rewrite our\nhigh-level table with the new information, as shown to the right. \n Question: \nQ. As we move from bit to bit in our design, how much information needs to pass from one bit to the next?\nQ. \nQ. How is information passed from one bit to the next in the design? \n Answer: "}, {"text": "Title: Extending to 2's Complement P12 \n Text: Looking at the table, notice the similarity \nto the high-level design for a single bit of an unsigned value.  The \n Question: \nQ. What does the text say about how to perform the task?\nQ. \nQ. The text does not say how to perform the task. \n Answer: "}, {"text": "Title: Extending to 2's Complement P13 \n Text: \n{cc|l}\nA_s& B_s& solution \n0& 0& pass result from less significant bits\n0& 1& A>B\n1& 0& A<B\n1& 1& pass result from less significant bits \n Question: \nQ. What does the text say about what to do if the two bits are equal?\nQ. \nQ. If the two bits are equal, the text says to generate a question. \n Answer: "}, {"text": "Title: Extending to 2's Complement P14 \n Text: only difference is that the two A=B cases are reversed.  If we\nswap A_s and B_s, the function is identical.  We can simply use\nanother bit slice but swap these two inputs. \n Question: \nQ. What does it mean when it says \"pass along the result from the less significant bits\"? \n Answer: \n\nA. This means that the result of the operation on the less significant bits is propagated to the more significant bits."}, {"text": "Title: Extending to 2's Complement P15 \n Text: Implementation of an {N-bit} 2's complement comparator based\non our bit slice comparator is shown below.  The blue circle highlights\nthe only change from the {N-bit} unsigned comparator, which is\nto swap the two inputs on the sign bit. \n Question: \nQ. What are the three possible answers from bit slice logic for bit M-1?\nQ. \nQ. The three possible answers from bit slice logic for bit M-1 are 0, 1, and X. \n Answer: "}, {"text": "Title: Further Optimization P0 \n Text: \nLet's return to the topic of optimization.  To what extent \ndid the representation of the three outcomes affect our ability\nto develop a good bit slice design?  Although selecting a good \nrepresentation can be quite important, for this particular problem\nmost representations lead to similar implementations. \n Question: \nQ. What are some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers? \n Answer: \n\nA. Some design tradeoffs that can be made when developing comparators for unsigned and 2's complement numbers include:\n\n- The number of bits used to represent the numbers\n- The order in which the bits are compared\n- The number of bits used for the output"}, {"text": "Title: Further Optimization P1 \n Text: Some representations, however, have interesting properties.  Consider \n Question: \nQ. How do we compare two N-bit numbers, A and B? \n Answer: \n\nA. We compare two N-bit numbers, A and B, by looking at the most significant bit (MSB) first. If the MSBs are equal, we compare the next most significant bits, and so on until we find a difference or we reach the end of the number."}, {"text": "Title: Further Optimization P2 \n Text: \n{cc|l|l}\nC_1& C_0& original& alternate \n0& 0& A=B& A=B\n0& 1& A<B& A>B\n1& 0& A>B& not used\n1& 1& not used& A<B \n Question: \nQ. What value of N will produce the longest path? \n Answer: \n\nA. The longest path will be produced when N is equal to 8."}, {"text": "Title: Further Optimization P3 \n Text: the alternate representation on the right, for example (a copy of the \noriginal representation is included for comparison).  Notice that \nin the alternate representation, C_0=1 whenever A=B.   \n Question: \nQ. How would you convert the number 1011 to decimal?\nQ. \nQ. The number 1011 in decimal would be 11. \n Answer: "}, {"text": "Title: Further Optimization P4 \n Text: Once we have found the numbers to be different in some bit, the end\nresult can never be equality, so perhaps with the right \nrepresentation---the new one, for example---we might be able to\ncut delay in half? \n Question: \nQ. What is the value of a byte in 2's complement form?\nQ. \nQ. A byte in 2's complement form has a value of -128 to 127. \n Answer: "}, {"text": "Title: Further Optimization P5 \n Text: \nAn implementation based on the alternate representation appears in the\ndiagram to the right.  As you can see, in terms of gate count,\nthis design replaces one {2-input} gate with an inverter and\na second {2-input} gate with a {3-input} gate.  The path\nlengths are the same, requiring 2N+2 gate delays for \nan {N-bit} comparator.\nOverall, it is about the same as our original design. \n Question: \nQ. Why do humans typically start comparing at the most significant bit? \n Answer: \n\nA. So that they can save time."}, {"text": "Title: Further Optimization P6 \n Text: Why didn't it work?  Should we consider still other representations?\nIn fact, none of the possible representations that we might choose\nfor a bit slice can cut the delay down to one gate delay per bit.  \nThe problem is fundamental, and is related to the nature of CMOS.\nFor a single bit slice, we define the incoming and outgoing \nrepresentations to be the same.  We also need to have at least\none gate in the path to combine the C_1 and C_0 inputs with\ninformation from the bit slice's A and B inputs.  But all CMOS\ngates invert the sense of their inputs.  Our choices are limited\nto NAND and NOR.  Thus we need at least two gates in the path to\nmaintain the same representation. \n Question: \nQ. Why does the author say that if we start from the least significant bit, we must always look at all of the bits? \n Answer: \n\nA. The author is saying that if we start from the least significant bit, we will not be able to skip any bits because we will always need to look at them in order to determine the value of the number."}, {"text": "Title: Further Optimization P7 \n Text: One simple answer is to use different representations for odd and\neven bits.  Instead, we optimize a logic circuit\nfor comparing two bits.  We base our design on the alternate \nrepresentation.  The implementation is shown below.  The left\nshows an implementation based on the algebra, and the right shows\na NAND/NOR implementation.  Estimating by gate count and number of\ninputs, the {two-bit} design doesn't save much over two\nsingle bit slices in terms of area.  In terms of delay, however,\nwe have only two gate delays from C_1 and C_0 to either output.\nThe longest path from the A and B inputs to the outputs is\nfive gate delays.  Thus, for an {N-bit} comparator built\nwith this design, the total delay is only N+3 gate delays.\nBut N has to be even. \n Question: \nQ. What is the final result of the hardware comparison? \n Answer: \n\nA. The final result of the hardware comparison is the determination of which bit is different."}, {"text": "Title: Further Optimization P8 \n Text: \nAs you can imagine, continuing to scale up the size of our logic\nblock gives us better performance at the expense of a more complex\ndesign.  Using the alternate representation may help you to see\nhow one can generalize the approach to larger groups of bits---for\nexample, you may have noticed the two bitwise \ncomparator blocks on the left of the implementations above. \n Question: \nQ. How does the direction we choose to build our functions affect the comparison of bits? \n Answer: \n\nA. The direction we choose to build our functions affects the comparison of bits because it determines the order in which the bits are compared."}, {"text": "Title: Using Abstraction to Simplify Problems P0 \n Text: In this set of notes, we illustrate the use of abstraction to simplify\nproblems, then introduce a component called a multiplexer that allows\nselection among multiple inputs. \n Question: \nQ. How does abstraction simplify problems? \n Answer: \n\nA. Abstractions simplify problems by breaking them down into smaller, more manageable pieces. By focusing on one piece at a time, we can more easily see how the pieces fit together and how they can be combined to solve the overall problem."}, {"text": "Title: Using Abstraction to Simplify Problems P1 \n Text: We begin by showing how two specific \nexamples---integer subtraction and identification of letters\nin ASCII---can be implemented using logic functions that we have already\ndeveloped.  We also introduce a conceptual technique for\nbreaking functions into smaller pieces, which allows us to solve\nseveral simpler problems and then to compose a full solution from \nthese partial solutions. \n Question: \nQ. What is a specific example of how integer subtraction can be implemented using logic functions? \n Answer: \n\nA.\n\nInteger subtraction can be implemented using a variety of logic functions, such as the adder, subtractor, and carry functions."}, {"text": "Title: Using Abstraction to Simplify Problems P2 \n Text: Together with the idea of bit-sliced designs that we introduced earlier,\nthese techniques help to simplify the process of designing logic that\noperates correctly.  The techniques can, of course, lead to \nless efficient designs,\nbut { correctness is always more important than performance}. \n Question: \nQ. What is more important, correctness or performance? \n Answer: \n\nA. Correctness is always more important than performance."}, {"text": "Title: Using Abstraction to Simplify Problems P3 \n Text: The potential loss of efficiency is often acceptable for three reasons. \n Question: \nQ. What are three reasons the potential loss of efficiency is often acceptable? \n Answer: \n\nA. The potential loss of efficiency is often acceptable for three reasons:\n\n1. The benefits of using a database may outweigh the potential loss of efficiency.\n2. The loss of efficiency may be negligible compared to the overall efficiency of the system.\n3. The database may be able to be tuned to reduce the potential loss of efficiency."}, {"text": "Title: Using Abstraction to Simplify Problems P4 \n Text: First, as we mentioned earlier, computer-aided design tools for \noptimizing logic functions are fairly effective, and in many cases\nproduce better results than human engineers (except in the rare cases \nin which the human effort required to beat the tools is worthwhile). \n Question: \nQ. What is the rare case in which the human effort required to beat the tools is worthwhile? \n Answer: \n\nA. The rare case is when the computer-aided design tools are not effective and the human engineer is required to produce better results."}, {"text": "Title: Using Abstraction to Simplify Problems P5 \n Text: Second, as you know from the design of the {2's complement} \nrepresentation, we may be able to reuse specific pieces of hardware if\nwe think carefully about how we define our problems and representations. \n Question: \nQ. What is the difference between 1's complement and 2's complement? \n Answer: \n\n1's complement is a system where every number is represented by the opposite number of the same magnitude. So, for example, the number 12 would be represented as -12, and the number -5 would be represented as 5. \n2's complement is a system where every number is represented by the opposite number of the same magnitude, plus one. So, for example, the number 12 would be represented as -13, and the number -5 would be represented as 4."}, {"text": "Title: Using Abstraction to Simplify Problems P6 \n Text: Finally, many tasks today are executed in software, which is designed\nto leverage the fairly general logic available via an instruction set\narchitecture.  A programmer cannot easily add new logic to a user's\nprocessor.  As a result, the hardware used to execute a\nfunction typically is not optimized for that function. \n Question: \nQ. Why can't a programmer easily add new logic to a user's processor? \n Answer: \n\nA. The hardware used to execute a function typically is not optimized for that function."}, {"text": "Title: Using Abstraction to Simplify Problems P7 \n Text: The approaches shown in this set of notes illustrate how abstraction\ncan be used to design logic. \n Question: \nQ. What is abstraction? \n Answer: \n\nA.\n\nAbstraction is a technique for designing complex systems by breaking them down into simpler, more manageable parts. By abstracting away the details of a system, we can focus on the big picture and design our system more effectively."}, {"text": "Title: Subtraction P0 \n Text: Our discussion of arithmetic implementation has focused so far on \naddition.  What about other operations, such as subtraction, multiplication,\nand division?  The latter two require more work, and we will not\ndiscuss them in detail until later in our class (if at all). \n Question: \nQ. How does abstraction simplify problems? \n Answer: \n\nA. Abstractions simplify problems by breaking them down into smaller, more manageable pieces. By focusing on one piece at a time, we can more easily see how the pieces fit together and how they can be combined to solve the overall problem."}, {"text": "Title: Subtraction P1 \n Text: Subtraction, however, can be performed almost trivially using logic that\nwe have already designed. \n Question: \nQ. What is a specific example of how integer subtraction can be implemented using logic functions? \n Answer: \n\nA.\n\nInteger subtraction can be implemented using a variety of logic functions, such as the adder, subtractor, and carry functions."}, {"text": "Title: Subtraction P2 \n Text: Let's say that we want to calculate the difference D between \ntwo {N-bit} numbers A and B.  In particular, we want \nto find D=A-B.  For now, think of A, B, and D \nas 2's complement values. \n Question: \nQ. What is more important, correctness or performance? \n Answer: \n\nA. Correctness is always more important than performance."}, {"text": "Title: Subtraction P3 \n Text: Recall how we defined the 2's complement representation: \nthe {N-bit} pattern that we use to represent -B is the \nsame as the base 2 bit pattern for (2^N-B), so we can use an adder if we\nfirst calculate the bit pattern for -B, then add the resulting\npattern to A.\nAs you know, our {N-bit} adder always produces a result that\nis correct modulo 2^N, so the result of such an operation,\nD=2^N+A-B, is correct so long as the subtraction does not overflow. \n Question: \nQ. What are three reasons the potential loss of efficiency is often acceptable? \n Answer: \n\nA. The potential loss of efficiency is often acceptable for three reasons:\n\n1. The benefits of using a database may outweigh the potential loss of efficiency.\n2. The loss of efficiency may be negligible compared to the overall efficiency of the system.\n3. The database may be able to be tuned to reduce the potential loss of efficiency."}, {"text": "Title: Subtraction P4 \n Text: \nHow can we calculate 2^N-B?  The same way that we do by hand!\nCalculate the 1's complement, (2^N-1)-B, then add 1. \n Question: \nQ. What is the rare case in which the human effort required to beat the tools is worthwhile? \n Answer: \n\nA. The rare case is when the computer-aided design tools are not effective and the human engineer is required to produce better results."}, {"text": "Title: Subtraction P5 \n Text: The diagram to the right shows how we can use the {N-bit} adder\nthat we designed in Notes Set 2.3 to build an {N-bit} subtracter. \n Question: \nQ. What is the difference between 1's complement and 2's complement? \n Answer: \n\n1's complement is a system where every number is represented by the opposite number of the same magnitude. So, for example, the number 12 would be represented as -12, and the number -5 would be represented as 5. \n2's complement is a system where every number is represented by the opposite number of the same magnitude, plus one. So, for example, the number 12 would be represented as -13, and the number -5 would be represented as 4."}, {"text": "Title: Subtraction P6 \n Text: New elements appear in blue in the figure---the rest of the logic\nis just an adder.  The box labeled ``1's comp.'' calculates \nthe {1's complement} of the value B, which together with the\ncarry in value of 1 correspond to calculating -B.  What's in the\n``1's comp.'' box?  One inverter per bit in B.  That's all \nwe need to calculate the 1's complement. \n Question: \nQ. Why can't a programmer easily add new logic to a user's processor? \n Answer: \n\nA. The hardware used to execute a function typically is not optimized for that function."}, {"text": "Title: Subtraction P7 \n Text: You might now ask: does this approach also work for unsigned numbers?\nThe answer is yes, absolutely.  However, the overflow conditions for\nboth 2's complement and unsigned subtraction are different than the\noverflow condition for either type of addition.  What does the carry\nout of our adder signify, for example?  The answer may not be \nimmediately obvious. \n Question: \nQ. What is abstraction? \n Answer: \n\nA.\n\nAbstraction is a technique for designing complex systems by breaking them down into simpler, more manageable parts. By abstracting away the details of a system, we can focus on the big picture and design our system more effectively."}, {"text": "Title: Subtraction P8 \n Text: Let's start with the overflow condition for unsigned subtraction. \n Question: \nQ. What about other operations, such as subtraction, multiplication, and division? \n Answer: \n\nA. The latter two require more work, and we will not discuss them in detail until later in our class (if at all)."}, {"text": "Title: Subtraction P9 \n Text: Overflow means that we cannot represent the result.  With an {N-bit}\nunsigned number, we have A-B[0,2^N-1].  Obviously, the difference\ncannot be larger than the upper limit, since A is representable and\nwe are subtracting a non-negative (unsigned) value.  We can thus assume\nthat overflow occurs only when A-B<0.  In other words, when A<B. \n Question: \nQ. How can subtraction be performed almost trivially using logic that we have already designed? \n Answer: \n\nA. By using the logic of addition, subtraction can be performed by simply reversing the sign of one of the numbers."}, {"text": "Title: Subtraction P10 \n Text: To calculate the unsigned subtraction overflow condition in terms of the\nbits, recall that our adder is calculating 2^N+A-B.  The carry out\nrepresents the 2^N term.  When A, the result of the adder\nis at least 2^N, and we see a carry out, C_=1.  However, when\nA<B, the result of the adder is less than 2^N, and we see no carry\nout, C_=0.  { Overflow for unsigned subtraction is thus inverted \nfrom overflow for unsigned addition}: a carry out of 0 indicates an \noverflow for subtraction. \n Question: \nQ. What is 2's complement? \n Answer: \n\nA. Two's complement is a system for representing negative numbers in binary code. In two's complement form, a negative number is represented by the two's complement of the absolute value of the number."}, {"text": "Title: Subtraction P11 \n Text: What about overflow for 2's complement subtraction?  We can use arguments\nsimilar to those that we used to reason about overflow of 2's complement\naddition to prove that subtraction of one negative number from a second\nnegative number can never overflow.  Nor can subtraction of a non-negative\nnumber from a second non-negative number overflow. \n Question: \nQ. How do you calculate the bit pattern for -B? \n Answer: \n\nA. The bit pattern for -B can be calculated by subtracting B from 2^N."}, {"text": "Title: Subtraction P12 \n Text: If A and B<0, the subtraction overflows iff A-B{2^{N-1}}.\nAgain using similar arguments as before, we can prove that the difference D\nappears to be negative in the case of overflow, so the product\n{A_{N-1}} B_{N-1} D_{N-1} evaluates to 1 when this type of\noverflow occurs (these variables represent the most significant bits of the \ntwo operands and the difference; in the case of 2's complement, they are\nalso the sign bits). \n Question: \nQ. How can we calculate 2^N-B?\nQ. \nQ. The same way that we do by hand! Calculate the 1's complement, (2^N-1)-B, then add 1. \n Answer: "}, {"text": "Title: Subtraction P13 \n Text: Similarly, if A<0 and B, we have overflow when A-B<-2^{N-1}.\nHere we can prove that D on overflow, so \nA_{N-1} {B_{N-1}} {D_{N-1}} evaluates to 1. \n Question: \nQ. What is the purpose of the N-bit adder?\nQ. \nQ. The purpose of the N-bit adder is to add two binary numbers together. \n Answer: "}, {"text": "Title: Subtraction P14 \n Text: Our overflow condition for {N-bit} 2's complement subtraction\nis thus given by the following: \n Question: \nQ. What does the \"1's comp.\" box do? \n Answer: \n\nThe \"1's comp.\" box calculates the 1's complement of the value B."}, {"text": "Title: Subtraction P15 \n Text: {eqnarray*}\n{A_{N-1}} B_{N-1} D_{N-1}+A_{N-1} {B_{N-1}} {D_{N-1}}\n{eqnarray*} \n Question: \nQ. What is the overflow condition for unsigned subtraction? \n Answer: \n\nA. The overflow condition for unsigned subtraction is when the carry out of the adder is not equal to the carry in."}, {"text": "Title: Subtraction P16 \n Text: If we calculate all four overflow conditions---unsigned and 2's complement,\naddition and subtraction---and provide some way to choose whether or not to \ncomplement B and to control the C_ input, we can use the same hardware\nfor addition and subtraction of either type. \n Question: \nQ. What is the overflow condition for unsigned subtraction?\nQ. \nQ. The overflow condition for unsigned subtraction is when the result of the subtraction is greater than the max value that can be represented by the data type. \n Answer: "}, {"text": "Title: Checking ASCII for Upper-case Letters P0 \n Text: Let's now consider how we can check whether or not an ASCII character is\nan upper-case letter.  Let's call the {7-bit}\nletter C=C_6C_5C_4C_3C_2C_1C_0 and the function that we want to\ncalculate U(C).  The function U should equal 1 whenever C represents\nan upper-case letter, and should equal 0 whenever C does not. \n Question: \nQ. How does abstraction simplify problems? \n Answer: \n\nA. Abstractions simplify problems by breaking them down into smaller, more manageable pieces. By focusing on one piece at a time, we can more easily see how the pieces fit together and how they can be combined to solve the overall problem."}, {"text": "Title: Checking ASCII for Upper-case Letters P1 \n Text: In ASCII, the {7-bit} patterns from 0x41 through 0x5A correspond\nto the letters A through Z in alphabetic order.  Perhaps you want to draw \na {7-input} {K-map}?  Get a few large sheets of paper! \n Question: \nQ. What is a specific example of how integer subtraction can be implemented using logic functions? \n Answer: \n\nA.\n\nInteger subtraction can be implemented using a variety of logic functions, such as the adder, subtractor, and carry functions."}, {"text": "Title: Checking ASCII for Upper-case Letters P2 \n Text: Instead, imagine that we've written the full {128-row} truth\ntable.  Let's break the truth table into pieces.  Each piece will\ncorrespond to one specific pattern of the three high bits C_6C_5C_4,\nand each piece will have 16 entries for the four low bits C_3C_2C_1C_0.\nThe truth tables for high bits 000, 001, 010, 011, 110, and 111\nare easy: the function is exactly 0.  The other two truth tables appear \non the left below.  We've called the two functions T_4 and T_5, where\nthe subscripts correspond to the binary value of the three high bits of C. \n Question: \nQ. What is more important, correctness or performance? \n Answer: \n\nA. Correctness is always more important than performance."}, {"text": "Title: Checking ASCII for Upper-case Letters P3 \n Text: {cccc|cc}\nC_3& C_2& C_1& C_0& T_4& T_5 \n0& 0& 0& 0& 0& 1\n0& 0& 0& 1& 1& 1\n0& 0& 1& 0& 1& 1\n0& 0& 1& 1& 1& 1 \n0& 1& 0& 0& 1& 1\n0& 1& 0& 1& 1& 1\n0& 1& 1& 0& 1& 1\n0& 1& 1& 1& 1& 1 \n1& 0& 0& 0& 1& 1\n1& 0& 0& 1& 1& 1\n1& 0& 1& 0& 1& 1\n1& 0& 1& 1& 1& 0 \n1& 1& 0& 0& 1& 0\n1& 1& 0& 1& 1& 0\n1& 1& 1& 0& 1& 0\n1& 1& 1& 1& 1& 0 \n Question: \nQ. What are three reasons the potential loss of efficiency is often acceptable? \n Answer: \n\nA. The potential loss of efficiency is often acceptable for three reasons:\n\n1. The benefits of using a database may outweigh the potential loss of efficiency.\n2. The loss of efficiency may be negligible compared to the overall efficiency of the system.\n3. The database may be able to be tuned to reduce the potential loss of efficiency."}, {"text": "Title: Checking ASCII for Upper-case Letters P4 \n Text: {eqnarray*}\nT_4&=&C_3+C_2+C_1+C_0      \nT_5&=&{C_3}+{C_2} {C_1}+{C_2} {C_0}\n{eqnarray*} \n Question: \nQ. What is the rare case in which the human effort required to beat the tools is worthwhile? \n Answer: \n\nA. The rare case is when the computer-aided design tools are not effective and the human engineer is required to produce better results."}, {"text": "Title: Checking ASCII for Upper-case Letters P5 \n Text: \nAs shown to the right of the truth tables, we can then draw simpler\n{K-maps} for T_4 and T_5, and can solve the {K-maps}\nto find equations for each, as shown to the right (check that you get\nthe same answers). \n Question: \nQ. What is the difference between 1's complement and 2's complement? \n Answer: \n\n1's complement is a system where every number is represented by the opposite number of the same magnitude. So, for example, the number 12 would be represented as -12, and the number -5 would be represented as 5. \n2's complement is a system where every number is represented by the opposite number of the same magnitude, plus one. So, for example, the number 12 would be represented as -13, and the number -5 would be represented as 4."}, {"text": "Title: Checking ASCII for Upper-case Letters P6 \n Text: How do we merge these results to form our final expression for U? \n Question: \nQ. Why can't a programmer easily add new logic to a user's processor? \n Answer: \n\nA. The hardware used to execute a function typically is not optimized for that function."}, {"text": "Title: Checking ASCII for Upper-case Letters P7 \n Text: We AND each of the term functions (T_4 and T_5) with the \nappropriate minterm for the\nhigh bits of C, then OR the results together, as shown here: \n Question: \nQ. What is abstraction? \n Answer: \n\nA.\n\nAbstraction is a technique for designing complex systems by breaking them down into simpler, more manageable parts. By abstracting away the details of a system, we can focus on the big picture and design our system more effectively."}, {"text": "Title: Checking ASCII for Upper-case Letters P8 \n Text: {eqnarray*}\nU&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5 \n Question: \nQ. What about other operations, such as subtraction, multiplication, and division? \n Answer: \n\nA. The latter two require more work, and we will not discuss them in detail until later in our class (if at all)."}, {"text": "Title: Checking ASCII for Upper-case Letters P9 \n Text: &=&C_6 {C_5} {C_4} (C_3+C_2+C_1+C_0)+\nC_6 {C_5} C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0})\n{eqnarray*} \n Question: \nQ. How can subtraction be performed almost trivially using logic that we have already designed? \n Answer: \n\nA. By using the logic of addition, subtraction can be performed by simply reversing the sign of one of the numbers."}, {"text": "Title: Checking ASCII for Upper-case Letters P10 \n Text: Rather than trying to optimize by hand, we can at this point let the CAD\ntools take over, confident that we have the right function to identify\nan upper-case ASCII letter. \n Question: \nQ. What is 2's complement? \n Answer: \n\nA. Two's complement is a system for representing negative numbers in binary code. In two's complement form, a negative number is represented by the two's complement of the absolute value of the number."}, {"text": "Title: Checking ASCII for Upper-case Letters P11 \n Text: Breaking the truth table into pieces and using simple logic to reconnect\nthe pieces is one way to make use of abstraction when solving complex\nlogic problems.   \n Question: \nQ. How do you calculate the bit pattern for -B? \n Answer: \n\nA. The bit pattern for -B can be calculated by subtracting B from 2^N."}, {"text": "Title: Checking ASCII for Upper-case Letters P12 \n Text: In fact, recruiters for some companies often ask\nquestions that involve using specific logic elements as building blocks\nto implement other functions.  Knowing that you can implement\na truth table one piece at a time will help you to solve this type of\nproblem. \n Question: \nQ. How can we calculate 2^N-B?\nQ. \nQ. The same way that we do by hand! Calculate the 1's complement, (2^N-1)-B, then add 1. \n Answer: "}, {"text": "Title: Checking ASCII for Upper-case Letters P13 \n Text: Let's think about other ways to tackle the problem of calculating U.\nIn Notes Sets 2.3 and 2.4, we developed adders and comparators.  Can\nwe make use of these components as building blocks to check whether C \nrepresents an\nupper-case letter?  Yes, of course we can: by comparing C with the\nends of the range of upper-case letters, we can check whether or not C\nfalls in that range. \n Question: \nQ. What is the purpose of the N-bit adder?\nQ. \nQ. The purpose of the N-bit adder is to add two binary numbers together. \n Answer: "}, {"text": "Title: Checking ASCII for Upper-case Letters P14 \n Text: The idea is illustrated on the left below using two {7-bit} comparators\nconstructed as discussed in Notes Set 2.4.\nThe comparators are the black parts of the drawing, while the blue parts\nrepresent our extensions to calculate U.  Each comparator is given\nthe value C as one input.  The second value to the comparators is \neither the letter A (0x41) or the letter Z (0x5A).  The meaning of \nthe {2-bit} input to and result of each comparator is given in the \ntable on the right below.  The inputs on the right of each comparator\nare set to 0 to ensure that equality is produced if C matches\nthe second input (B). \n Question: \nQ. What does the \"1's comp.\" box do? \n Answer: \n\nThe \"1's comp.\" box calculates the 1's complement of the value B."}, {"text": "Title: Checking ASCII for Upper-case Letters P15 \n Text: One output from each comparator is then routed to\na NOR gate to calculate U.  Let's consider how this combination works.\nThe left comparator compares C with the letter A (0x41).  If C 0x41,\nthe comparator produces Z_0=0.  In this case, we may have a letter.\nOn the other hand, if C< 0x41, the comparator produces Z_0=1, and\nthe NOR gate outputs U=0, since we do not have a letter in this case. \n Question: \nQ. What is the overflow condition for unsigned subtraction? \n Answer: \n\nA. The overflow condition for unsigned subtraction is when the carry out of the adder is not equal to the carry in."}, {"text": "Title: Checking ASCII for Upper-case Letters P16 \n Text: The right comparator compares C with the letter Z (0x5A).  If C 0x5A,\nthe comparator produces Z_1=0.  In this case, we may have a letter.\nOn the other hand, if C> 0x5A, the comparator produces Z_1=1, and\nthe NOR gate outputs U=0, since we do not have a letter in this case. \n Question: \nQ. What is the overflow condition for unsigned subtraction?\nQ. \nQ. The overflow condition for unsigned subtraction is when the result of the subtraction is greater than the max value that can be represented by the data type. \n Answer: "}, {"text": "Title: Checking ASCII for Upper-case Letters P17 \n Text: Only when 0x41  0x5A does U=1, as desired.  \n Question: \nQ. When might overflow occur when subtracting two unsigned numbers?\nQ. \nQ. Overflow might occur when subtracting two unsigned numbers if the result is less than 0. \n Answer: "}, {"text": "Title: Checking ASCII for Upper-case Letters P18 \n Text: \n{cc|l}\nZ_1& Z_0& meaning \n0& 0& A=B\n0& 1& A<B\n1& 0& A>B\n1& 1& not used \n Question: \nQ. What is the difference between unsigned subtraction and unsigned addition with respect to overflow? \n Answer: \n\nWith respect to overflow, unsigned subtraction is the inverse of unsigned addition. A carry out of 0 indicates an overflow for subtraction, while a carry out of 1 indicates an overflow for addition."}, {"text": "Title: Checking ASCII for Upper-case Letters P19 \n Text: What if we have only {8-bit} adders available for our use, such as\nthose developed in Notes Set 2.3?  Can we still calculate U?  Yes.\nThe diagram shown to the right illustrates the approach, again with black\nfor the adders and blue for our extensions.  Here we are \nactually using the adders as subtracters, but calculating the 1's complements\nof the constant values by hand.  The ``zero extend'' box simply adds\na leading 0 to our {7-bit} ASCII letter.  The left adder \nsubtracts the letter A from C: if no carry is produced, we know that\nC< 0x41 and thus C does not represent an upper-case letter, and U=0. \n Question: \nQ. What is the reason that subtraction of one negative number from a second negative number can never overflow? \n Answer: \n\nA. The reason is that, when two negative numbers are subtracted, the resulting number will always be negative. This is because the most significant bit (MSB) of a negative number is always 1, and when two negative numbers are subtracted, the MSB of the result will always be 1. Therefore, subtraction of one negative number from a second negative number can never overflow."}, {"text": "Title: Checking ASCII for Upper-case Letters P20 \n Text: Similarly, the right adder subtracts 0x5B (the letter Z plus one)\nfrom C.  If a carry is produced, we know that C 0x5B, and \nthus C does not represent an upper-case letter, and U=0. \n Question: \nQ. Why does the subtraction overflow if A and B are both negative? \n Answer: \n\nA. If A and B are both negative, the subtraction overflows if A-B{2^{N-1}}. This is because the difference D appears to be negative in the case of overflow, and the product {A_{N-1}} B_{N-1} D_{N-1} evaluates to 1 when this type of overflow occurs."}, {"text": "Title: Checking ASCII for Upper-case Letters P21 \n Text: With the right combination of carries (1 from the left and 0 from the\nright), we obtain U=1. \n Question: \nQ. Why is it important to check for overflow when performing arithmetic operations? \n Answer: \n\nOverflow can lead to unexpected results and can cause a program to malfunction."}, {"text": "Title: Checking ASCII for Upper-case Letters P22 \n Text: Looking carefully at this solution, however, you might be struck by the\nfact that we are calculating two sums and then discarding them.  Surely\nsuch an approach is inefficient? \n Question: \nQ. What is the overflow condition for 2's complement subtraction? \n Answer: \n\nA. The overflow condition for 2's complement subtraction is that the result of the subtraction is not a valid 2's complement number."}, {"text": "Title: Checking ASCII for Upper-case Letters P23 \n Text: We offer two answers.  First, given the design shown above, a good CAD tool\nrecognizes that the sum outputs of the adders are not being used,\nand does not generate logic to calculate them.  The logic for the two \ncarry bits used to calculate U can then be optimized.  Second, the design\nshown, including the calculation of the sums, is similar in efficiency\nto what happens at the rate of about 10^ times per second, \n24 hours a day, seven days a week,\ninside processors in data centers processing HTML, XML, and other types\nof human-readable Internet traffic.  Abstraction is a powerful tool. \n Question: \nQ. What is the difference between unsigned and 2's complement? \n Answer: \n\nA. The difference between unsigned and 2's complement is that 2's complement includes a bit for the sign of the number, while unsigned numbers do not."}, {"text": "Title: Checking ASCII for Upper-case Letters P24 \n Text: Later in our class, you will learn how to control logical connections\nbetween hardware blocks so that you can make use of the same hardware\nfor adding, subtracting, checking for upper-case letters, and so forth. \n Question: \nQ. What is the value of U(C) when C is an upper-case letter?\nQ. \nQ. U(C) is equal to 1 when C is an upper-case letter. \n Answer: "}, {"text": "Title: Checking ASCII for Lower-case Letters P0 \n Text: Having developed several approaches for checking for an upper-case letter,\nthe task of checking for a lower-case letter should be straightforward.\nIn ASCII, lower-case letters are represented by the {7-bit} patterns \nfrom 0x61 through 0x7A. \n Question: \nQ. How does abstraction simplify problems? \n Answer: \n\nA. Abstractions simplify problems by breaking them down into smaller, more manageable pieces. By focusing on one piece at a time, we can more easily see how the pieces fit together and how they can be combined to solve the overall problem."}, {"text": "Title: Checking ASCII for Lower-case Letters P1 \n Text: One can now easily see how more abstract designs make solving similar\ntasks easier.  If we have designed our upper-case checker \nwith {7-variable} K-maps,\nwe must start again with new K-maps for the lower-case checker.  If\ninstead we have taken the approach of designing logic for the upper and \nlower bits of the ASCII character, we can reuse most of that logic,\nsince the functions T_4 and T_5 are identical when checking for\na lower-case character.  Recalling the algebraic form of U(C), we can\nthen write a function L(C) (a lower-case checker) as shown on the\nleft below. \n Question: \nQ. What is a specific example of how integer subtraction can be implemented using logic functions? \n Answer: \n\nA.\n\nInteger subtraction can be implemented using a variety of logic functions, such as the adder, subtractor, and carry functions."}, {"text": "Title: Checking ASCII for Lower-case Letters P2 \n Text: \n{eqnarray*}\nU&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5 \n Question: \nQ. What is more important, correctness or performance? \n Answer: \n\nA. Correctness is always more important than performance."}, {"text": "Title: Checking ASCII for Lower-case Letters P3 \n Text: L&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5 \n Question: \nQ. What are three reasons the potential loss of efficiency is often acceptable? \n Answer: \n\nA. The potential loss of efficiency is often acceptable for three reasons:\n\n1. The benefits of using a database may outweigh the potential loss of efficiency.\n2. The loss of efficiency may be negligible compared to the overall efficiency of the system.\n3. The database may be able to be tuned to reduce the potential loss of efficiency."}, {"text": "Title: Checking ASCII for Lower-case Letters P4 \n Text: &=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+\n&&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0})\n{eqnarray*} \n Question: \nQ. What is the rare case in which the human effort required to beat the tools is worthwhile? \n Answer: \n\nA. The rare case is when the computer-aided design tools are not effective and the human engineer is required to produce better results."}, {"text": "Title: Checking ASCII for Lower-case Letters P5 \n Text: Finally, if we have used a design based on comparators or adders, the\ndesign of a lower-case checker becomes trivial: simply change the numbers\nthat we input to these components, as shown in the figure on the right above\nfor the comparator-based design.  The only changes from the upper-case checker\ndesign are the inputs to the comparators and the output produced, \nhighlighted with blue text in the figure. \n Question: \nQ. What is the difference between 1's complement and 2's complement? \n Answer: \n\n1's complement is a system where every number is represented by the opposite number of the same magnitude. So, for example, the number 12 would be represented as -12, and the number -5 would be represented as 5. \n2's complement is a system where every number is represented by the opposite number of the same magnitude, plus one. So, for example, the number 12 would be represented as -13, and the number -5 would be represented as 4."}, {"text": "Title: The Multiplexer P0 \n Text: Using the more abstract designs for checking ranges of ASCII characters,\nwe can go a step further and create a checker for both upper- and lower-case\nletters.  To do so, we add another input S that allows us to select the \nfunction that we want---either the upper-case checker U(C) or the \nlower-case checker L(C). \n Question: \nQ. How does abstraction simplify problems? \n Answer: \n\nA. Abstractions simplify problems by breaking them down into smaller, more manageable pieces. By focusing on one piece at a time, we can more easily see how the pieces fit together and how they can be combined to solve the overall problem."}, {"text": "Title: The Multiplexer P1 \n Text: For this purpose, we make use of a logic block called a { multiplexer},\nor { mux}.\nMultiplexers are an important abstraction for digital logic.  In \ngeneral, a multiplexer allows us to use one digital signal to \nselect which of several others is forwarded to an output. \n Question: \nQ. What is a specific example of how integer subtraction can be implemented using logic functions? \n Answer: \n\nA.\n\nInteger subtraction can be implemented using a variety of logic functions, such as the adder, subtractor, and carry functions."}, {"text": "Title: The Multiplexer P2 \n Text: \nThe simplest form of the multiplexer is the 2-to-1 multiplexer shown to \nthe right.   The logic diagram illustrates how the mux works.  The block \nhas two inputs from the left and one from the top.  The top input allows \nus to choose which of the left inputs is forwarded to the output.  \nWhen the input S=0, the upper AND gate outputs 0, and the lower AND gate\noutputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0.\nSimilarly, when input S=1, the upper AND gate outputs D_1, and the\nlower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1. \n Question: \nQ. What is more important, correctness or performance? \n Answer: \n\nA. Correctness is always more important than performance."}, {"text": "Title: The Multiplexer P3 \n Text: The symbolic form of the mux is a trapezoid with data inputs on the \nlarger side, an output on the smaller side, and a select input on the\nangled part of the trapezoid.  The labels inside the trapezoid indicate \nthe value of the select input S for which the adjacent data signal, \nD_1 or D_0, is copied to the output Q. \n Question: \nQ. What are three reasons the potential loss of efficiency is often acceptable? \n Answer: \n\nA. The potential loss of efficiency is often acceptable for three reasons:\n\n1. The benefits of using a database may outweigh the potential loss of efficiency.\n2. The loss of efficiency may be negligible compared to the overall efficiency of the system.\n3. The database may be able to be tuned to reduce the potential loss of efficiency."}, {"text": "Title: The Multiplexer P4 \n Text: We can generalize multiplexers in two ways.  First, we can extend the \nsingle select input to a group of select inputs.  An {N-bit}\nselect input allows selection from amongst 2^N inputs.  A {4-to-1} \nmultiplexer is shown below, for example.  The logic diagram on the left\nshows how the {4-to-1 mux} operates.  For any combination of S_1S_0,\nthree of the AND gates produce 0, and the fourth outputs the D input\ncorresponding to the interpretation of S as an unsigned number.\nGiven three zeroes and one D input, the OR gate thus reproduces one of \nthe D's.  When S_1S_0=10, for example, the third AND gate copies D_2,\nand Q=D_2. \n Question: \nQ. What is the rare case in which the human effort required to beat the tools is worthwhile? \n Answer: \n\nA. The rare case is when the computer-aided design tools are not effective and the human engineer is required to produce better results."}, {"text": "Title: The Multiplexer P5 \n Text: As shown in the middle figure, a {4-to-1} mux can also be built from\nthree {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux \nappears on the right in the figure. \n Question: \nQ. What is the difference between 1's complement and 2's complement? \n Answer: \n\n1's complement is a system where every number is represented by the opposite number of the same magnitude. So, for example, the number 12 would be represented as -12, and the number -5 would be represented as 5. \n2's complement is a system where every number is represented by the opposite number of the same magnitude, plus one. So, for example, the number 12 would be represented as -13, and the number -5 would be represented as 4."}, {"text": "Title: The Multiplexer P6 \n Text: The second way in which we can generalize multiplexers is by using\nseveral multiplexers of the same type and using the same signals for \nselection.  For example, we might use a single select bit T to choose \nbetween any number of paired inputs.  Denote input pair by i D_1^i \nand D_0^i.  For each pair, we have an output Q_i.   \n Question: \nQ. Why can't a programmer easily add new logic to a user's processor? \n Answer: \n\nA. The hardware used to execute a function typically is not optimized for that function."}, {"text": "Title: The Multiplexer P7 \n Text: When T=0, Q_i=D_0^i for each value of i. \n Question: \nQ. What is abstraction? \n Answer: \n\nA.\n\nAbstraction is a technique for designing complex systems by breaking them down into simpler, more manageable parts. By abstracting away the details of a system, we can focus on the big picture and design our system more effectively."}, {"text": "Title: The Multiplexer P8 \n Text: And, when T=1, Q_i=D_1^i for each value of i. \n Question: \nQ. What about other operations, such as subtraction, multiplication, and division? \n Answer: \n\nA. The latter two require more work, and we will not discuss them in detail until later in our class (if at all)."}, {"text": "Title: The Multiplexer P9 \n Text: Each value of i requires a {2-to-1} mux with its select input\ndriven by the global select signal T. \n Question: \nQ. How can subtraction be performed almost trivially using logic that we have already designed? \n Answer: \n\nA. By using the logic of addition, subtraction can be performed by simply reversing the sign of one of the numbers."}, {"text": "Title: The Multiplexer P10 \n Text: Returning to the example of the upper- and lower-case checker, we\ncan make use of two groups of seven {2-to-1} muxes, all controlled by\na single bit select signal S, to choose between the inputs needed\nfor an upper-case checker and those needed for a lower-case checker. \n Question: \nQ. What is 2's complement? \n Answer: \n\nA. Two's complement is a system for representing negative numbers in binary code. In two's complement form, a negative number is represented by the two's complement of the absolute value of the number."}, {"text": "Title: The Multiplexer P11 \n Text: Specific configurations of multiplexers are often referred to\nas { N-to-M multiplexers}.  Here the value N refers to the\nnumber of inputs, and M refers to the number of outputs.  The\nnumber of select bits can then be calculated as _2(N/M)---N/M \nis generally a power of two---and one way to build such a \nmultiplexer is to use M copies of\nan  (N/M)-to-1 multiplexer. \n Question: \nQ. How do you calculate the bit pattern for -B? \n Answer: \n\nA. The bit pattern for -B can be calculated by subtracting B from 2^N."}, {"text": "Title: The Multiplexer P12 \n Text: Let's extend our upper- and lower-case checker to check\nfor four different ranges of ASCII characters, as shown below.\nThis design uses two {28-to-7} muxes to create a single\nchecker for the four ranges.  Each of the muxes in the figure logically \nrepresents seven {4-to-1} muxes. \n Question: \nQ. How can we calculate 2^N-B?\nQ. \nQ. The same way that we do by hand! Calculate the 1's complement, (2^N-1)-B, then add 1. \n Answer: "}, {"text": "Title: The Multiplexer P13 \n Text: \nThe table to the right describes the behavior of the checker. \n Question: \nQ. What is the purpose of the N-bit adder?\nQ. \nQ. The purpose of the N-bit adder is to add two binary numbers together. \n Answer: "}, {"text": "Title: The Multiplexer P14 \n Text: When the select input S is set to 00, the left mux selects the value 0x00,\nand the right mux selects the value 0x1F, which checks whether the ASCII\ncharacter represented by C is a control character. \n Question: \nQ. What does the \"1's comp.\" box do? \n Answer: \n\nThe \"1's comp.\" box calculates the 1's complement of the value B."}, {"text": "Title: The Multiplexer P15 \n Text: When the select input S=01, the muxes produce the values needed to check\nwhether C is an upper-case letter. \n Question: \nQ. What is the overflow condition for unsigned subtraction? \n Answer: \n\nA. The overflow condition for unsigned subtraction is when the carry out of the adder is not equal to the carry in."}, {"text": "Title: The Multiplexer P16 \n Text: Similarly, when the select input S=10, \n Question: \nQ. What is the overflow condition for unsigned subtraction?\nQ. \nQ. The overflow condition for unsigned subtraction is when the result of the subtraction is greater than the max value that can be represented by the data type. \n Answer: "}, {"text": "Title: The Multiplexer P17 \n Text: \n{c|c|c|c}\n& left& right& \n& comparator& comparator& \nS_1S_0& input& input& R(C) produced  \n00& 0x00& 0x1F& control character?\n01& 0x41& 0x5A& upper-case letter?\n10& 0x61& 0x7A& lower-case letter?\n11& 0x30& 0x39& numeric digit?  \n Question: \nQ. When might overflow occur when subtracting two unsigned numbers?\nQ. \nQ. Overflow might occur when subtracting two unsigned numbers if the result is less than 0. \n Answer: "}, {"text": "Title: The Multiplexer P18 \n Text: the muxes produce the values \nneeded to check whether C is a lower-case letter. \n Question: \nQ. What is the difference between unsigned subtraction and unsigned addition with respect to overflow? \n Answer: \n\nWith respect to overflow, unsigned subtraction is the inverse of unsigned addition. A carry out of 0 indicates an overflow for subtraction, while a carry out of 1 indicates an overflow for addition."}, {"text": "Title: The Multiplexer P19 \n Text: Finally, when the select input S=11, the left mux selects the value 0x30,\nand the right mux selects the value 0x39, which checks whether the ASCII\ncharacter represented by C is a digit (0 to 9). \n Question: \nQ. What is the reason that subtraction of one negative number from a second negative number can never overflow? \n Answer: \n\nA. The reason is that, when two negative numbers are subtracted, the resulting number will always be negative. This is because the most significant bit (MSB) of a negative number is always 1, and when two negative numbers are subtracted, the MSB of the result will always be 1. Therefore, subtraction of one negative number from a second negative number can never overflow."}, {"text": "Title: Sequential Logic P0 \n Text: These notes introduce logic components for storing bits, building up\nfrom the idea of a pair of cross-coupled inverters through an\nimplementation of a flip-flop, the storage abstractions used in most \nmodern logic design processes.  We then introduce simple forms of\ntiming diagrams, which we use to illustrate the behavior of\na logic circuit. \n Question: \nQ. What are the basic components of a logic circuit? \n Answer: \n\nA. The basic components of a logic circuit are cross-coupled inverters, which can be used to store bits. Flip-flops are also used to store bits, and are the standard storage abstraction in most modern logic design processes. Timing diagrams can be used to illustrate the behavior of a logic circuit."}, {"text": "Title: Sequential Logic P1 \n Text: After commenting on the benefits of using a clocked synchronous \nabstraction when designing systems that store and manipulate bits,\nwe illustrate timing issues and explain how these are abstracted\naway in clocked synchronous designs. \n Question: \nQ. How does a clocked synchronous abstraction help when designing systems that store and manipulate bits? \n Answer: \n\nA clocked synchronous abstraction helps when designing systems that store and manipulate bits by abstracting away timing issues. This means that the designer does not have to worry about the timing of signal changes in the system, which makes the design process simpler and more efficient."}, {"text": "Title: Sequential Logic P2 \n Text: { Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later\nclasses.}  \n Question: \nQ. What is the author's purpose in writing this text? \n Answer: \n\nA. The author's purpose in writing this text is to provide a brief overview of the history of the English language."}, {"text": "Title: Storing One Bit P0 \n Text: So far we have discussed only implementation of Boolean functions:\ngiven some bits as input, how can we design a logic circuit to calculate\nthe result of applying some function to those bits?  The answer to\nsuch questions is called { combinational logic} (sometimes \n{ combinatorial logic}), a name stemming from the fact that we\nare combining existing bits with logic, not storing new bits. \n Question: \nQ. What are the basic components of a logic circuit? \n Answer: \n\nA. The basic components of a logic circuit are cross-coupled inverters, which can be used to store bits. Flip-flops are also used to store bits, and are the standard storage abstraction in most modern logic design processes. Timing diagrams can be used to illustrate the behavior of a logic circuit."}, {"text": "Title: Storing One Bit P1 \n Text: You probably already know, however, that combinational logic alone is\nnot sufficient to build a computer.  We need the ability to store bits,\nand to change those bits.  Logic designs that make use of stored \nbits---bits that can be changed, not just wires to high voltage and \nground---are called { sequential logic}.  The name comes from the idea \nthat such a system moves through a sequence of stored bit patterns (the \ncurrent stored bit pattern is called the { state} of the system). \n Question: \nQ. How does a clocked synchronous abstraction help when designing systems that store and manipulate bits? \n Answer: \n\nA clocked synchronous abstraction helps when designing systems that store and manipulate bits by abstracting away timing issues. This means that the designer does not have to worry about the timing of signal changes in the system, which makes the design process simpler and more efficient."}, {"text": "Title: Storing One Bit P2 \n Text: \nConsider the diagram to the right.  What is it?  A {1-input} \nNAND gate, or an inverter drawn badly?  If you think carefully about\nhow these two gates are built, you will realize that they are the\nsame thing.  Conceptually, we use two inverters to store a bit, but\nin most cases we make use of NAND gates to simplify the \nmechanism for changing the stored bit. \n Question: \nQ. What is the author's purpose in writing this text? \n Answer: \n\nA. The author's purpose in writing this text is to provide a brief overview of the history of the English language."}, {"text": "Title: Storing One Bit P3 \n Text: \nTake a look at the design to the right.  Here we have taken two \ninverters (drawn as NAND gates) and coupled each gate's output to\nthe other's input.  What does the circuit do?  Let's make some\nguesses and see where they take us.  Imagine that the value at Q\nis 0.  In that case, the lower gate drives P to 1.  \nBut P drives the upper gate, which forces Q to 0.\nIn other words, this combination forms a \n Question: \nQ. What is the difference between combinational and combinatorial logic? \n Answer: \n\nThere is no difference between the two terms."}, {"text": "Title: Storing One Bit P4 \n Text: {|cc}\nQ& P \n0& 1\n1& 0 \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Combinational logic relies only on the current inputs to determine the output, while sequential logic also takes into account the stored state of the system."}, {"text": "Title: Storing One Bit P5 \n Text: stable state of the system:\nonce the gates reach this state, they continue to hold these values.\nThe first row of the truth table to the right (outputs only) shows\nthis state. \n Question: \nQ. What is the difference between a 1-input NAND gate and an inverter? \n Answer: \n\nA. The difference between a 1-input NAND gate and an inverter is that an inverter is drawn badly."}, {"text": "Title: Storing One Bit P6 \n Text: What if Q=1, though?  In this case, the lower gate forces P to 0,\nand the upper gate in turn forces Q to 1.  Another stable state!\nThe Q=1 state appears as the second row of the truth table. \n Question: \nQ. What is the purpose of the inverters in this circuit? \n Answer: \n\nA. The inverters in this circuit form a positive feedback loop, which causes the circuit to oscillate between the two values 0 and 1."}, {"text": "Title: Storing One Bit P7 \n Text: We have identified all of the stable states.{Most logic\nfamilies also allow unstable states in which the values alternate rapidly \nbetween 0 and 1.  These metastable states are \nbeyond the scope of our class, but ensuring that they do not occur\nin practice is important for real designs.}  Notice that our cross-coupled\ninverters can store a bit.  Unfortunately, we have no way to \nspecify which value should be stored, nor to change the bit's value\nonce the gates have settled into a stable state.\nWhat can we do?   \n Question: \nQ. What does the author mean by \"P\"? \n Answer: \n\nP refers to the permutation matrix."}, {"text": "Title: Storing One Bit P8 \n Text: \nLet's add an input to the upper gate, as shown to the \nright.  We call the input .  The ``S'' stands for set---as\nyou will see, our new input allows us to { set} our stored bit Q to 1.\nThe use of a complemented name for the input indicates that the input\nis { active low}.  In other words, the input performs its intended \ntask (setting Q to 1) when its value is 0 (not 1). \n Question: \nQ. What does the author mean by \"stable state of the system\"? \n Answer: \n\nA. The stable state of the system is the state in which the system remains after the gates have reached their final values."}, {"text": "Title: Storing One Bit P9 \n Text: {c|cc}\n& Q& P \n1& 0& 1\n1& 1& 0\n0& 1& 0 \n Question: \nQ. What does it mean when the lower gate forces P to 0? \n Answer: \n\nA. That the output of the lower gate is always 0, regardless of the input."}, {"text": "Title: Storing One Bit P10 \n Text: Think about what happens when the new input is not active, \n=1.  As you know, ANDing any value with 1 produces the \nsame value, \nso our new input has no effect when =1.  The first two\nrows of the truth table are simply a copy of our previous table: the \ncircuit can store either bit value when =1.\nWhat happens when =0?  In that case, the upper gate's\noutput is forced to 1, and thus the lower gate's is forced to 0.\nThis third possibility is reflected in the last row of the truth table. \n Question: \nQ. How can we specify which value should be stored in the cross-coupled inverters? \n Answer: \n\nA. We can specify the value to be stored by setting the input of one of the inverters to 1 and the input of the other inverter to 0."}, {"text": "Title: Storing One Bit P11 \n Text: \nNow we have the ability to force bit Q to have value 1, but if we\nwant Q=0, we just have to hope that the circuit happens to settle into\nthat state when we turn on the power.  What can we do? \n Question: \nQ. Why does the input use a complemented name? \n Answer: \n\nA. The input is active low, which means that it performs its intended task when its value is 0 (not 1)."}, {"text": "Title: Storing One Bit P12 \n Text: As you probably guessed, we add an input to the other gate, as \nshown to the right.  We call the new input : the input's\npurpose is to { reset} bit Q to 0, and the input\nis active low.  We extend the truth table to include a row \nwith =0 and =1, which forces Q=0 and P=1. \n Question: \nQ. What does the matrix represent? \n Answer: \n\nP. The matrix represents a system of linear equations."}, {"text": "Title: Storing One Bit P13 \n Text: {cc|cc}\n& & Q& P \n1& 1& 0& 1\n1& 1& 1& 0\n1& 0& 1& 0\n0& 1& 0& 1\n0& 0& 1& 1 \n Question: \nQ. What is the purpose of the new input in the circuit?\nQ. \nQ. The purpose of the new input is to allow the circuit to store a second bit value. When the new input is active, the circuit can store either bit value. When the new input is not active, the circuit is forced to store a 0 value. \n Answer: "}, {"text": "Title: Storing One Bit P14 \n Text:  is active low.  We extend the truth table to include a row \n with =0 and =1, which forces Q=0 and P=1. \n Question: \nQ. What is the difference between bit Q having value 1 and bit Q having value 0? \n Answer: \n\nA. Bit Q having value 1 indicates that the circuit is able to be forced into that state, while bit Q having value 0 indicates that the circuit may or may not settle into that state."}, {"text": "Title: Storing One Bit P15 \n Text: The circuit that we have drawn has a name: an \n{ {- latch}}. \nOne can also build {R-S latches} (with active high set and reset\ninputs).  The textbook also shows an\n{- latch} (labeled \nincorrectly).\nCan you figure out how to build an {R-S} latch yourself? \n Question: \nQ. What is the purpose of the input on a RS flip-flop?\nQ. \nQ. The purpose of the input is to reset bit Q to 0. \n Answer: "}, {"text": "Title: Storing One Bit P16 \n Text: Let's think a little more about\nthe {- latch}. \nWhat happens if we set =0 and =0 at\nthe same time?  Nothing bad happens immediately.  Looking at the design,\nboth gates produce 1, so Q=1 and P=1.  The bad part happens later:\nif we raise both  and  back to 1 at around\nthe same time, the stored bit may end up in either state.{Or,\nworse, in a metastable state, as mentioned earlier.} \n Question: \nQ. What does the matrix represent? \n Answer: \n\nA. The matrix represents the truth table for the logical OR operation."}, {"text": "Title: Storing One Bit P17 \n Text: We can avoid the problem by adding gates to prevent the\ntwo control inputs ( and )\nfrom ever being 1 at the same time.  A single inverter\nmight technically suffice, but let's build up the structure shown below,\nnoting that the two inverters in sequence connecting D to \nhave no practical effect at the moment. \n Question: \nQ. What is the purpose of the \"is active low\" statement?\nQ. \nQ. The \"is active low\" statement means that the signal is active when it is low, or 0. This is the opposite of the usual convention, where a signal is active when it is high, or 1. \n Answer: "}, {"text": "Title: Storing One Bit P18 \n Text: A truth table is shown to the right of the logic diagram. \n Question: \nQ. How does an R-S latch work? \n Answer: \n\nA.\n\nAn R-S latch is a simple circuit that can be used to store a single bit of information. The circuit has two inputs, a set input and a reset input, and two outputs, a Q output and a Q-bar output. The set input is used to set the value of the Q output to 1, and the reset input is used to set the value of the Q output to 0. The Q output will remain in its current state until the opposite input is activated. For example, if the Q output is 1 and the reset input is activated, the Q output will become 0."}, {"text": "Title: Storing One Bit P19 \n Text: When D=0,  is forced to 0, and the bit is reset. \n Question: \nQ. What are the consequences of setting both and to 0 at the same time? \n Answer: \n\nA. The consequences of setting both and to 0 at the same time are that Q=1 and P=1."}, {"text": "Title: Storing One Bit P20 \n Text: Similarly, when D=1,  is forced to 0, and the bit is set. \n Question: \nQ. Why is it necessary to add gates to prevent the two control inputs from being 1 at the same time? \n Answer: \n\nA. It is necessary to add gates to prevent the two control inputs from being 1 at the same time because if they are, the output of the AND gate will be 1, and the output of the OR gate will be 0."}, {"text": "Title: Storing One Bit P21 \n Text: \n{c|cccc}\nD& & & Q& P \n0& 0& 1& 0& 1\n1& 1& 0& 1& 0 \n Question: \nQ. What does a truth table show?\nQ. \nQ. A truth table shows the possible truth values of a logical expression. \n Answer: "}, {"text": "Title: Storing One Bit P22 \n Text: Unfortunately, except for some interesting timing characteristics, \nthe new design has the same functionality as a piece of wire.\nAnd, if you ask a circuit designer, thin wires also have some \ninteresting timing characteristics.  What can we do?  Rather than \nhaving Q always reflect the current value of D, let's add\nsome extra inputs to the new NAND gates that allow us to control\nwhen the value of D is copied to Q, as shown on the next page. \n Question: \nQ. What does the D in D=0 stand for?\nQ. \nQ. The D in D=0 stands for the number of days since the last bit was reset. \n Answer: "}, {"text": "Title: Storing One Bit P23 \n Text: \n{cc|cccc}\nWE& D& & & Q& P \n1& 0& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0\n0& 0& 1& 1& 0& 1\n0& 1& 1& 1& 0& 1\n0& 0& 1& 1& 1& 0\n0& 1& 1& 1& 1& 0 \n Question: \nQ. What is the meaning of the phrase \"the bit is set\"?\nQ. \nQ. This phrase means that the value of the bit is set to 1. \n Answer: "}, {"text": "Title: Storing One Bit P24 \n Text: \nThe WE (write enable) input controls whether or not Q mirrors\nthe value of D.  The first two rows in the truth table are replicated\nfrom our ``wire'' design: a value of WE=1 has no effect on the first\ntwo NAND gates, and Q=D.  A value of WE=0 forces the\nfirst two NAND gates to output 1, thus =1, =1,\nand the bit Q can\noccupy either of the two possible states, regardless of the value\nof D, as reflected in the lower four lines of the truth table. \n Question: \nQ. What does the text say about the relationship between D and Q? \n Answer: \n\nA. The text says that D and Q are in a relationship."}, {"text": "Title: Storing One Bit P25 \n Text: The circuit just shown is called a { gated D latch}, and is an\nimportant mechanism \n Question: \nQ. What does the new design have in common with a piece of wire? \n Answer: \n\nA. Both the new design and a piece of wire have interesting timing characteristics."}, {"text": "Title: Storing One Bit P26 \n Text: for storing state in sequential \nlogic.  (Random-access memory uses a slightly different \ntechnique to connect the cross-coupled inverters, but latches are\nused for nearly every other application of stored state.)\nThe ``D''\nstands for ``data,'' meaning that the bit stored is matches\nthe value of the input.  Other types of latches (including\n{S-R latches}) have been used historically, but\nD latches are used predominantly today, so we omit discussion of\nother types.\nThe ``gated'' qualifier refers to the presence of an enable input\n(we called it WE) to control\nwhen the latch copies its input into the stored bit.\nA symbol for a gated D latch appears to the right.  Note that we have\ndropped the name P in favor of , since P=\nin a gated D latch.  \n Question: \nQ. What does the letter \"P\" represent in the table? \n Answer: \n\nP represents the product of two binary numbers."}, {"text": "Title: The Clock Abstraction P0 \n Text: High-speed logic designs often use latches directly.  Engineers\nspecify the number of latches as well as combinational logic \nfunctions needed to connect one latch to the next,\nand the CAD tools optimize the combinational logic.\nThe enable inputs of successive groups of latches are then driven\nby what we call a clock signal, a single bit line distributed across\nmost of the chip that alternates between 0 and 1 with a regular\nperiod.  While the clock is 0, one set of latches holds its bit values fixed,\nand combinational logic uses those latches as inputs to produce \nbits that are copied into a\nsecond set of latches.  When the clock switches to 1, the second\nset of latches stops storing their data inputs and\nretains their bit values in order to drive other combinational logic,\nthe results of which are copied into a third set of latches.\nOf course, some of the latches in the first and third sets may be the\nsame. \n Question: \nQ. What are the basic components of a logic circuit? \n Answer: \n\nA. The basic components of a logic circuit are cross-coupled inverters, which can be used to store bits. Flip-flops are also used to store bits, and are the standard storage abstraction in most modern logic design processes. Timing diagrams can be used to illustrate the behavior of a logic circuit."}, {"text": "Title: The Clock Abstraction P1 \n Text: The timing of signals in such designs plays a critical role in their\ncorrect operation.  Fortunately, we have developed powerful abstractions \nthat allow engineers to ignore much of the complexity while thinking\nabout the Boolean logic needed for a given design. \n Question: \nQ. How does a clocked synchronous abstraction help when designing systems that store and manipulate bits? \n Answer: \n\nA clocked synchronous abstraction helps when designing systems that store and manipulate bits by abstracting away timing issues. This means that the designer does not have to worry about the timing of signal changes in the system, which makes the design process simpler and more efficient."}, {"text": "Title: The Clock Abstraction P2 \n Text: Towards that end, we make a simplifying assumption for the\nrest of our class, and for most of your career as an undergraduate:\nthe clock signal is a { square wave} delivered uniformly across a\nchip.  For example, if the period of a clock is 0.5 nanoseconds (2 GHz),\nthe clock signal is a 1 for 0.25 nanoseconds, then a 0 for\n0.25 nanoseconds.  We assume that the clock signal changes instantaneously\nand at the same time across the chip.  Such a signal can never exist in\nthe real world: voltages do not change instantaneously, and the \nphrase ``at the same time'' may not even make sense at these scales.\nHowever, circuit designers can usually provide a clock signal that\nis close enough, allowing us to forget for now that no physical signal can\nmeet our abstract definition. \n Question: \nQ. What is the author's purpose in writing this text? \n Answer: \n\nA. The author's purpose in writing this text is to provide a brief overview of the history of the English language."}, {"text": "Title: The Clock Abstraction P3 \n Text: \n SSL altered terminology on 3 Dec 21 \n Question: \nQ. What is the difference between combinational and combinatorial logic? \n Answer: \n\nThere is no difference between the two terms."}, {"text": "Title: The Clock Abstraction P4 \n Text: \nThe device shown to the right is a { master-slave} implementation of a  \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Combinational logic relies only on the current inputs to determine the output, while sequential logic also takes into account the stored state of the system."}, {"text": "Title: The Clock Abstraction P5 \n Text: The device shown to the right is a { dual-latch} implementation of a \n{ positive edge-triggered} D flip-flop.  As you can see, we have \nconstructed it from two gated D latches with opposite senses of write\nenable.  The ``D'' part of the name has the same meaning as with a\ngated D latch: the bit stored is the same as the one delivered \n Question: \nQ. What is the difference between a 1-input NAND gate and an inverter? \n Answer: \n\nA. The difference between a 1-input NAND gate and an inverter is that an inverter is drawn badly."}, {"text": "Title: The Clock Abstraction P6 \n Text: \nto the\ninput.  Other variants of flip-flops have also been built, but this \ntype dominates designs today.  Most are actually generated automatically\nfrom hardware ``design'' languages (that is, computer programming languages\nfor hardware design). \n Question: \nQ. What is the purpose of the inverters in this circuit? \n Answer: \n\nA. The inverters in this circuit form a positive feedback loop, which causes the circuit to oscillate between the two values 0 and 1."}, {"text": "Title: The Clock Abstraction P7 \n Text: When the clock is low (0), the first latch copies its value from\nthe flip-flop's D input to the midpoint (marked X in our figure, but\nnot usually given a name).  When the clock is high (1), the second\nlatch copies its value from X to the flip-flop's output Q.\nSince X can not change when the clock is high,\nthe result is that the output changes each time the clock changes\nfrom 0 to 1, which is called the { rising edge} or { positive edge}\n(the derivative) of the clock signal.  Hence the qualifier \n``positive edge-triggered,'' which describes the flip-flop's behavior. \n Question: \nQ. What does the author mean by \"P\"? \n Answer: \n\nP refers to the permutation matrix."}, {"text": "Title: The Clock Abstraction P8 \n Text: The ``master-slave'' implementation refers to the use of two latches. \n Question: \nQ. What does the author mean by \"stable state of the system\"? \n Answer: \n\nA. The stable state of the system is the state in which the system remains after the gates have reached their final values."}, {"text": "Title: The Clock Abstraction P9 \n Text: The ``dual-latch'' implementation refers to the use of two \nlatches.{Historically, this implementation was called ``master-slave,''\nbut ECE Illinois has decided to eliminate use of such terminology.}\nlatches.\nIn practice, flip-flops are almost never built this way.  To see a \ncommercial design, look up 74LS74, which uses six {3-input} NAND\ngates and allows set/reset of the flip-flop (using two\nextra inputs). \n Question: \nQ. What does it mean when the lower gate forces P to 0? \n Answer: \n\nA. That the output of the lower gate is always 0, regardless of the input."}, {"text": "Title: The Clock Abstraction P10 \n Text: The { timing diagram} to the right illustrates the operation of\nour flip-flop.  In a timing diagram, the horizontal axis represents\n(continuous) increasing time, and the individual lines represent\nvoltages for logic signals.  The relatively simple version shown here\nuses only binary values for each signal.  One can also draw \ntransitions more realistically (as taking finite time).  The dashed\nvertical lines here represent the times at which the clock rises.\nTo make the \n Question: \nQ. How can we specify which value should be stored in the cross-coupled inverters? \n Answer: \n\nA. We can specify the value to be stored by setting the input of one of the inverters to 1 and the input of the other inverter to 0."}, {"text": "Title: The Clock Abstraction P11 \n Text: example interesting, we have varied D over two clock\ncycles.  Notice that even though D rises and falls during the second\nclock cycle, its value is not copied to the output of our flip-flop.\nOne can build flip-flops that ``catch'' this kind of behavior\n(and change to output 1), but we leave such designs for later in your\ncareer. \n Question: \nQ. Why does the input use a complemented name? \n Answer: \n\nA. The input is active low, which means that it performs its intended task when its value is 0 (not 1)."}, {"text": "Title: The Clock Abstraction P12 \n Text: Circuits such as latches and flip-flops are called { sequential\nfeedback} circuits, and the process by which they are designed \nis beyond the scope of our course.  The ``feedback'' part of the\nname refers to the fact that the outputs of some gates are fed back \ninto the inputs of others.  Each cycle in a sequential feedback \ncircuit can store one bit. \n Question: \nQ. What does the matrix represent? \n Answer: \n\nP. The matrix represents a system of linear equations."}, {"text": "Title: The Clock Abstraction P13 \n Text: Circuits that merely use\nlatches and flip-flops as building blocks are called\n{ clocked synchronous sequential circuits}.  Such designs are\nstill sequential: their behavior depends on the bits currently\nstored in the latches and flip-flops.  However, their behavior\nis substantially simplified by the use of a clock signal (the ``clocked''\npart of the name) in a way that all elements change at the same\ntime (``synchronously''). \n Question: \nQ. What is the purpose of the new input in the circuit?\nQ. \nQ. The purpose of the new input is to allow the circuit to store a second bit value. When the new input is active, the circuit can store either bit value. When the new input is not active, the circuit is forced to store a 0 value. \n Answer: "}, {"text": "Title: The Clock Abstraction P14 \n Text: The value of using flip-flops and assuming a square-wave\nclock signal with uniform timing may not be clear to you yet,\nbut it bears emphasis. \n Question: \nQ. What is the difference between bit Q having value 1 and bit Q having value 0? \n Answer: \n\nA. Bit Q having value 1 indicates that the circuit is able to be forced into that state, while bit Q having value 0 indicates that the circuit may or may not settle into that state."}, {"text": "Title: The Clock Abstraction P15 \n Text: With such assumptions, { we can treat time as having \ndiscrete values.}  In other words, time ``ticks'' along discretely,\nlike integers instead of real numbers. \n Question: \nQ. What is the purpose of the input on a RS flip-flop?\nQ. \nQ. The purpose of the input is to reset bit Q to 0. \n Answer: "}, {"text": "Title: The Clock Abstraction P16 \n Text: We can look at the state of the system, calculate the inputs to our\nflip-flops through the combinational logic that drives their D\ninputs, and be confident that, when time moves to the next discrete\nvalue, we will know the new bit values stored in our flip-flops,\nallowing us to repeat the process for the next clock cycle without\nworrying about exactly when things change.  Values change only\non the rising edge of the clock! \n Question: \nQ. What does the matrix represent? \n Answer: \n\nA. The matrix represents the truth table for the logical OR operation."}, {"text": "Title: The Clock Abstraction P17 \n Text: Real systems, of course, are not so simple, and we do not have one\nclock to drive the universe, so engineers must also design systems\nthat interact even though each has its own private clock signal\n(usually with different periods). \n Question: \nQ. What is the purpose of the \"is active low\" statement?\nQ. \nQ. The \"is active low\" statement means that the signal is active when it is low, or 0. This is the opposite of the usual convention, where a signal is active when it is high, or 1. \n Answer: "}, {"text": "Title: Static Hazards: Causes and Cures* P0 \n Text: Before we forget about the fact that real designs do not provide\nperfect clocks, let's explore some of the issues that engineers\nmust sometimes face.   \n Question: \nQ. What are the basic components of a logic circuit? \n Answer: \n\nA. The basic components of a logic circuit are cross-coupled inverters, which can be used to store bits. Flip-flops are also used to store bits, and are the standard storage abstraction in most modern logic design processes. Timing diagrams can be used to illustrate the behavior of a logic circuit."}, {"text": "Title: Static Hazards: Causes and Cures* P1 \n Text: We discuss these primarily to ensure that you appreciate the power\nof the abstraction that we use in the rest of our course.\nIn later classes (probably our 298, which will absorb material \nfrom 385), you may be required to master this material.\n{ For now, we provide it simply for your interest.} \n Question: \nQ. How does a clocked synchronous abstraction help when designing systems that store and manipulate bits? \n Answer: \n\nA clocked synchronous abstraction helps when designing systems that store and manipulate bits by abstracting away timing issues. This means that the designer does not have to worry about the timing of signal changes in the system, which makes the design process simpler and more efficient."}, {"text": "Title: Static Hazards: Causes and Cures* P2 \n Text: Consider the circuit shown below, for which the output is given by \nthe equation S=AB+.  \n Question: \nQ. What is the author's purpose in writing this text? \n Answer: \n\nA. The author's purpose in writing this text is to provide a brief overview of the history of the English language."}, {"text": "Title: Static Hazards: Causes and Cures* P3 \n Text: The timing diagram on the right shows a { glitch} in the output\nwhen the input shifts from ABC=110 to 100, that is, when B falls.\nThe problem lies in the possibility that the upper AND gate, driven \nby B, might go low before the lower AND gate, driven by , goes\nhigh.  In such a case, the OR gate output S falls until the second\nAND gate rises, and the output exhibits a glitch. \n Question: \nQ. What is the difference between combinational and combinatorial logic? \n Answer: \n\nThere is no difference between the two terms."}, {"text": "Title: Static Hazards: Causes and Cures* P4 \n Text: A circuit that might exhibit a glitch in an output that functionally\nremains stable at 1 is said to have a { static-1 hazard}.  The\nqualifier ``static'' here refers to the fact that we expect the output\nto remain static, while the ``1'' refers to the expected value of the\noutput. \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Combinational logic relies only on the current inputs to determine the output, while sequential logic also takes into account the stored state of the system."}, {"text": "Title: Static Hazards: Causes and Cures* P5 \n Text: The presence of hazards in circuits can be problematic in certain\ncases.  In domino logic, for example, an output is precharged and kept\nat 1 until the output of a driving circuit pulls it to 0, at which\npoint it stays low (like a domino that has been knocked over).  If the\ndriving circuit contains {static-1} hazards, the output may fall\nin response to a glitch. \n Question: \nQ. What is the difference between a 1-input NAND gate and an inverter? \n Answer: \n\nA. The difference between a 1-input NAND gate and an inverter is that an inverter is drawn badly."}, {"text": "Title: Static Hazards: Causes and Cures* P6 \n Text: Similarly, hazards can lead to unreliable behavior in sequential\nfeedback circuits.  Consider the addition of a feedback loop to the\ncircuit just discussed, as shown in the figure below.  The output of\nthe circuit is now given by the equation S^*=AB+S,\nwhere S^* denotes the state after S feeds back through the lower\nAND gate.  In the case discussed previously, the transition from\nABC=110 to 100, the glitch in S can break the feedback, leaving\nS low or unstable.  The resulting sequential feedback circuit is\nthus unreliable.   \n Question: \nQ. What is the purpose of the inverters in this circuit? \n Answer: \n\nA. The inverters in this circuit form a positive feedback loop, which causes the circuit to oscillate between the two values 0 and 1."}, {"text": "Title: Static Hazards: Causes and Cures* P7 \n Text: \nEliminating static hazards from {two-level} circuits\nis fairly straightforward.  The Karnaugh map to the right corresponds\nto our original circuit; the solid lines indicate the \nimplicants selected by the AND gates.  A {static-1} hazard is\npresent when two adjacent 1s in the {K-map} are not covered by\na common implicant.  {Static-0} hazards do not occur in\n{two-level} SOP circuits. \n Question: \nQ. What does the author mean by \"P\"? \n Answer: \n\nP refers to the permutation matrix."}, {"text": "Title: Static Hazards: Causes and Cures* P8 \n Text: Eliminating static hazards requires merely extending the circuit with\nconsensus terms in order to ensure that some AND gate remains high\nthrough every transition between input states with\noutput 1.{Hazard elimination is not in general simple; we\nhave considered only {two-level} circuits.}  In the\n{K-map} shown, the dashed line indicates the necessary\nconsensus term, A. \n Question: \nQ. What does the author mean by \"stable state of the system\"? \n Answer: \n\nA. The stable state of the system is the state in which the system remains after the gates have reached their final values."}, {"text": "Title: Dynamic Hazards* P0 \n Text: Consider an input transition for which we expect to see a change in an\noutput.  Under certain timing conditions, the output may not\ntransition smoothly, but instead bounce between its original value\nand its new value before coming to rest at the new value.  A circuit\nthat might exhibit such behavior is said to contain a { dynamic hazard}.\nThe qualifier ``dynamic'' refers to the expected change in the output. \n Question: \nQ. What are the basic components of a logic circuit? \n Answer: \n\nA. The basic components of a logic circuit are cross-coupled inverters, which can be used to store bits. Flip-flops are also used to store bits, and are the standard storage abstraction in most modern logic design processes. Timing diagrams can be used to illustrate the behavior of a logic circuit."}, {"text": "Title: Dynamic Hazards* P1 \n Text: Dynamic hazards appear only in more complex circuits, such as the one\nshown below.  The output of this circuit is defined by the equation Q=B+++BD.  \n Question: \nQ. How does a clocked synchronous abstraction help when designing systems that store and manipulate bits? \n Answer: \n\nA clocked synchronous abstraction helps when designing systems that store and manipulate bits by abstracting away timing issues. This means that the designer does not have to worry about the timing of signal changes in the system, which makes the design process simpler and more efficient."}, {"text": "Title: Dynamic Hazards* P2 \n Text: \nConsider the transition from the input state ABCD=1111 to 1011,\nin which B falls from 1 to 0.  For simplicity, assume that each\ngate has a delay of 1 time unit.  If B goes low at time T=0, the\ntable shows the progression over time of logic levels at several\nintermediate points in the circuit and at the output Q.  Each gate\nmerely produces the appropriate output based on its inputs in the\nprevious time step.  After one delay, the three gates with B as a\ndirect input change their outputs (to stable, final values).  After\nanother delay, at T=2, the other three gates\nre-{-8pt} \n Question: \nQ. What is the author's purpose in writing this text? \n Answer: \n\nA. The author's purpose in writing this text is to provide a brief overview of the history of the English language."}, {"text": "Title: Dynamic Hazards* P3 \n Text: \n{|c|c|c|c|c|c|c|}\nT&f&g&h&i&j&Q \n0& 0& 0& 0& 1& 1& 1\n1& 1& 1& 1& 1& 1& 1\n2& 1& 1& 1& 0& 0& 0\n3& 1& 1& 1& 0& 1& 1\n4& 1& 1& 1& 0& 1& 0 \n Question: \nQ. What is the difference between combinational and combinatorial logic? \n Answer: \n\nThere is no difference between the two terms."}, {"text": "Title: Dynamic Hazards* P4 \n Text: \nspond to the initial changes and flip their outputs.  The resulting\nchanges induce another set of changes at T=3, which in turn causes\nthe output Q to change a final time at T=4. \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Combinational logic relies only on the current inputs to determine the output, while sequential logic also takes into account the stored state of the system."}, {"text": "Title: Dynamic Hazards* P5 \n Text: The output column in the table illustrates the possible impact of a\ndynamic hazard: rather than a smooth transition from 1 to 0, the\noutput drops to 0, rises back to 1, and finally falls to 0\nagain.  The dynamic hazard in this case can be attributed to the\npresence of a static hazard in the logic that produces intermediate\nvalue j. \n Question: \nQ. What is the difference between a 1-input NAND gate and an inverter? \n Answer: \n\nA. The difference between a 1-input NAND gate and an inverter is that an inverter is drawn badly."}, {"text": "Title: Essential Hazards* P0 \n Text: { Essential hazards} are inherent to the function of a circuit and\nmay appear in any implementation.  In sequential feedback circuit\ndesign, they must be addressed at a low level to ensure that\nvariations in logic path lengths ({ timing skew}) through a circuit\ndo not expose them.  With clocked synchronous circuits, essential\nhazards are abstracted into a single form: { clock skew}, or\ndisparate clock edge arrival times at a circuit's flip-flops. \n Question: \nQ. What are the basic components of a logic circuit? \n Answer: \n\nA. The basic components of a logic circuit are cross-coupled inverters, which can be used to store bits. Flip-flops are also used to store bits, and are the standard storage abstraction in most modern logic design processes. Timing diagrams can be used to illustrate the behavior of a logic circuit."}, {"text": "Title: Essential Hazards* P1 \n Text: An example demonstrates the possible effects: consider the\nconstruction of a clocked synchronous circuit to recognize {0-1}\nsequences on an input IN.  Output Q should be held high for one\ncycle after recognition, that is, until the next rising clock edge.  A\ndescription of states and a state diagram for such a circuit appear\nbelow. \n Question: \nQ. How does a clocked synchronous abstraction help when designing systems that store and manipulate bits? \n Answer: \n\nA clocked synchronous abstraction helps when designing systems that store and manipulate bits by abstracting away timing issues. This means that the designer does not have to worry about the timing of signal changes in the system, which makes the design process simpler and more efficient."}, {"text": "Title: Essential Hazards* P2 \n Text: S_1S_0 &state& meaning\n00& A& nothing, 1, or 11 seen last\n01& B& 0 seen last\n10& C& 01 recognized (output high)\n11& unused& \n Question: \nQ. What is the author's purpose in writing this text? \n Answer: \n\nA. The author's purpose in writing this text is to provide a brief overview of the history of the English language."}, {"text": "Title: Essential Hazards* P3 \n Text: For three states, we need two (=_2 3) flip-flops.\nDenote the internal state S_1S_0.  The specific internal\nstate values for each logical state (A, B, and C) simplify the\nimplementation and the example.  A { state table} \nand {K-maps} for the next-state logic appear\nbelow.  The state table uses one line per state with separate \ncolumns\nfor each input combination, making the table more compact than one\nwith one line per state/input combination.  Each column contains the\nfull next-state information, including output.  Using this form of the\nstate table, the {K-maps} can be read directly from the table. \n Question: \nQ. What is the difference between combinational and combinatorial logic? \n Answer: \n\nThere is no difference between the two terms."}, {"text": "Title: Essential Hazards* P4 \n Text: & {IN}\nS_1S_0& 0& 1 \n00& 01/0& 00/0\n01& 01/0& 10/0\n11& x& x\n10& 01/1& 00/1 \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Combinational logic relies only on the current inputs to determine the output, while sequential logic also takes into account the stored state of the system."}, {"text": "Title: Essential Hazards* P5 \n Text: Examining the {K-maps}, we see that the excitation and output\nequations are S_1^+=IN S_0, S_0^+=, and Q=S_1.\nAn implementation of the circuit using two D flip-flops appears below.\nImagine that mistakes in routing or process variations have made the\nclock signal's path to flip-flop 1 much longer than its path into\nflip-flop 0, as illustrated. \n Question: \nQ. What is the difference between a 1-input NAND gate and an inverter? \n Answer: \n\nA. The difference between a 1-input NAND gate and an inverter is that an inverter is drawn badly."}, {"text": "Title: Essential Hazards* P6 \n Text: Due to the long delays, we cannot assume that rising clock edges\narrive at the flip-flops at the same time.  The result is called clock\nskew, and can make the circuit behave improperly by exposing essential\nhazards.  In the logical B to C transition, for example, we begin in\nstate S_1S_0=01 with IN=1 and the clock edge rising.  Assume that\nthe edge reaches flip-flop 0 at time T=0.  After a flip-flop delay\n(T=1), S_0 goes low.  After another AND gate delay (T=2), input\nD_1 goes low, but the second flip-flop has yet to change state!\nFinally, at some later time, the clock edge reaches flip-flop 1.\nHowever, the output S_1 remains at 0, leaving the system in state A\nrather than state C. \n Question: \nQ. What is the purpose of the inverters in this circuit? \n Answer: \n\nA. The inverters in this circuit form a positive feedback loop, which causes the circuit to oscillate between the two values 0 and 1."}, {"text": "Title: Essential Hazards* P7 \n Text: Fortunately, in clocked synchronous sequential circuits, all essential\nhazards are related to clock skew.  This fact implies that we can\neliminate a significant amount of complexity from circuit design by\ndoing a good job of distributing the clock signal.  It also implies\nthat, as a designer, you should avoid specious addition of logic in a\nclock path, as you may regret such a decision later, as you try to\ndebug the circuit timing. \n Question: \nQ. What does the author mean by \"P\"? \n Answer: \n\nP refers to the permutation matrix."}, {"text": "Title: Proof Outline for Clocked Synchronous Design* P0 \n Text: This section outlines a proof of the claim made regarding clock\nskew being the only source of essential hazards for clocked\nsynchronous sequential circuits.  A { proof outline} suggests\nthe form that a proof might take and provides some of the logical\narguments, but is not rigorous enough to be considered a proof.\nHere we use a D flip-flop to illustrate a method for identifying\nessential hazards ({ the D flip-flop has no essential hazards, however}),\nthen argue that the method can be applied generally to collections\nof flip-flops in a clocked synchronous design to show that essential\nhazards occur only in the form of clock skew.  \n Question: \nQ. What are the basic components of a logic circuit? \n Answer: \n\nA. The basic components of a logic circuit are cross-coupled inverters, which can be used to store bits. Flip-flops are also used to store bits, and are the standard storage abstraction in most modern logic design processes. Timing diagrams can be used to illustrate the behavior of a logic circuit."}, {"text": "Title: Proof Outline for Clocked Synchronous Design* P1 \n Text: &{1-2}\nlow&L&clock low, last input low\nhigh&H&clock high, last input low\npulse low&PL&clock low, last input high (output high, too)\npulse high&PH&clock high, last input high (output high, too) \n Question: \nQ. How does a clocked synchronous abstraction help when designing systems that store and manipulate bits? \n Answer: \n\nA clocked synchronous abstraction helps when designing systems that store and manipulate bits by abstracting away timing issues. This means that the designer does not have to worry about the timing of signal changes in the system, which makes the design process simpler and more efficient."}, {"text": "Title: Proof Outline for Clocked Synchronous Design* P2 \n Text: Consider the sequential feedback state table for a positive\nedge-triggered D flip-flop, shown above.  In designing and analyzing\nsuch circuits, we assume that only one input bit changes at a time.\nThe state table consists of one row for each state and one column for\neach input combination.  Within a row, input combinations that have no\neffect on the internal state of the circuit (that is, those that do \nnot cause any change in\nthe state) are said to be stable; these states are circled.  Other\nstates are unstable, and the circuit changes state in response to\nchanges in the inputs. \n Question: \nQ. What is the author's purpose in writing this text? \n Answer: \n\nA. The author's purpose in writing this text is to provide a brief overview of the history of the English language."}, {"text": "Title: Proof Outline for Clocked Synchronous Design* P3 \n Text: For example, given an initial state L with low output, low clock,\nand high input D, the solid arcs trace the reaction of the circuit to a\nrising clock edge.  From the 01 input combination, we move along the\ncolumn to the 11 column, which indicates the new state, PH.  Moving\ndown the column to that state's row, we see that the new state is\nstable for the input combination 11, and we stop.  If PH were not\nstable, we would continue to move within the column until coming to\nrest on a stable state. \n Question: \nQ. What is the difference between combinational and combinatorial logic? \n Answer: \n\nThere is no difference between the two terms."}, {"text": "Title: Proof Outline for Clocked Synchronous Design* P4 \n Text: An essential hazard appears in such a table as a difference between\nthe final state when flipping a bit once and the final state when\nflipping a bit thrice in succession.  The dashed arcs in the figure\nillustrate the concept: after coming to rest in the PH state, we reset\nthe input to 01 and move along the PH row to find a new state of PL.\nMoving up the column, we see that the state is stable.  We then flip\nthe clock a third time and move back along the row to 11, which\nindicates that PH is again the next state.  Moving down the column, we\ncome again to rest in PH, the same state as was reached after one\nflip.  Flipping a bit three times rather than once evaluates the\nimpact of timing skew in the circuit; if a different state is reached\nafter two more flips, timing skew could cause unreliable behavior.  As\nyou can verify from the table, a D flip-flop has no essential hazards. \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Combinational logic relies only on the current inputs to determine the output, while sequential logic also takes into account the stored state of the system."}, {"text": "Title: Proof Outline for Clocked Synchronous Design* P5 \n Text: A group of flip-flops, as might appear in a clocked synchronous\ncircuit, can and usually does have essential hazards, but only dealing\nwith the clock.  As you know, the inputs to a clocked synchronous\nsequential circuit consist of a clock signal and other inputs (either\nexternal of fed back from the flip-flops).  Changing an input other\nthan the clock can change the internal state of a flip-flop (of the \n Question: \nQ. What is the difference between a 1-input NAND gate and an inverter? \n Answer: \n\nA. The difference between a 1-input NAND gate and an inverter is that an inverter is drawn badly."}, {"text": "Title: Proof Outline for Clocked Synchronous Design* P6 \n Text:  master-slave variety), but flip-flop designs do not capture the number \n Question: \nQ. What is the purpose of the inverters in this circuit? \n Answer: \n\nA. The inverters in this circuit form a positive feedback loop, which causes the circuit to oscillate between the two values 0 and 1."}, {"text": "Title: Proof Outline for Clocked Synchronous Design* P7 \n Text: dual-latch variety), but flip-flop designs do not capture the number\nof input changes in a clock cycle beyond one, and changing an input\nthree times is the same as changing it once.  Changing the clock, of\ncourse, results in a synchronous state machine transition. \n Question: \nQ. What does the author mean by \"P\"? \n Answer: \n\nP refers to the permutation matrix."}, {"text": "Title: Proof Outline for Clocked Synchronous Design* P8 \n Text: The detection of essential hazards in a clocked synchronous design\nbased on flip-flops thus reduces to examination of the state machine.\nIf the next state of the machine has any dependence on the current\nstate, an essential hazard exists, as a second rising clock edge moves\nthe system into a second new state.  For a single D flip-flop, the\nnext state is independent of the current state, and no essential\nhazards are present. \n Question: \nQ. What does the author mean by \"stable state of the system\"? \n Answer: \n\nA. The stable state of the system is the state in which the system remains after the gates have reached their final values."}, {"text": "Title: Registers P0 \n Text: This set of notes introduces registers, an abstraction used for \nstorage of groups of bits in digital systems.  We introduce some\nterminology used to describe aspects of register design and\nillustrate the idea of a shift register.  The registers shown here\nare important abstractions for digital system design. \n Question: \nQ. What is a register? \n Answer: \n\nA register is an abstraction used for storage of groups of bits in digital systems."}, {"text": "Title: Registers P1 \n Text:  { In the Fall 2012 offering of our course, we will cover this\n material on the third midterm.} \n Question: \nQ. When was the Fall 2012 offering of the course? \n Answer: \n\nA. The Fall 2012 offering of the course was in the fall of 2012."}, {"text": "Title: Registers P0 \n Text: \nA { register} is a storage element composed from one or more\nflip-flops operating on a common clock. \n Question: \nQ. What is a register? \n Answer: \n\nA register is an abstraction used for storage of groups of bits in digital systems."}, {"text": "Title: Registers P1 \n Text: In addition to the flip-flops,\nmost registers include logic to control the bits stored by the register. \n Question: \nQ. When was the Fall 2012 offering of the course? \n Answer: \n\nA. The Fall 2012 offering of the course was in the fall of 2012."}, {"text": "Title: Registers P2 \n Text: For example, D flip-flops \ncopy their inputs at the rising edge of each clock cycle,\ndiscarding whatever bits they have stored before the rising edge\n(in the previous clock cycle). \n Question: \nQ. How is a register different from a flip-flop? \n Answer: \n\nA register is a storage element composed from one or more flip-flops operating on a common clock."}, {"text": "Title: Registers P3 \n Text: To enable a flip-flop to retain its value, we might try to hide the \nrising edge of the clock from the flip-flop, as shown to the right. \n Question: \nQ. What does the text mean by \"most registers include logic to control the bits stored by the register\"? \n Answer: \n\nA. Most registers have logic circuits that can set or reset individual bits stored in the register."}, {"text": "Title: Registers P4 \n Text: The LOAD input controls the clock signals through a method known as\n{ clock gating}. \n Question: \nQ. How often do D flip-flops copy their inputs?\nQ. \nQ. D flip-flops copy their inputs at the rising edge of each clock cycle. \n Answer: "}, {"text": "Title: Registers P5 \n Text: \nWhen LOAD is high, the circuit reduces to a\nregular D flip-flop.  When LOAD is low, the flip-flop clock \ninput, c, is held high, and the flip-flop stores its \ncurrent value. \n Question: \nQ. What would happen if the flip-flop was not able to retain its value? \n Answer: \n\nA. If the flip-flop was not able to retain its value, it would lose its value on the rising edge of the clock."}, {"text": "Title: Registers P6 \n Text: The problems with clock gating are twofold.  First, adding logic to\nthe clock path introduces clock skew, which may cause timing problems\nlater in the development process (or, worse, in future projects that \n Question: \nQ. What is clock gating? \n Answer: \n\nA. Clock gating is a method of controlling the clock signals through the LOAD input."}, {"text": "Title: Registers P7 \n Text: use your circuits as components).  Second, in the design shown above,\nthe LOAD signal\ncan only be lowered while the clock is high to prevent spurious rising\nedges from causing incorrect behavior, as shown in the timing diagram. \n Question: \nQ. What is the purpose of the LOAD input?\nQ. \nQ. The LOAD input is used to load the value of the flip-flop. When LOAD is high, the circuit reduces to a regular D flip-flop. When LOAD is low, the flip-flop clock input, c, is held high, and the flip-flop stores its current value. \n Answer: "}, {"text": "Title: Registers P8 \n Text: \nA better approach is to use a mux and a feedback loop from the \nflip-flop's output, as shown in the figure to the right.   \n Question: \nQ. What are the problems with clock gating? \n Answer: \n\nA. The first problem with clock gating is that it introduces clock skew, which may cause timing problems later in the development process. The second problem is that it may increase the power consumption of the device."}, {"text": "Title: Registers P9 \n Text: When LOAD is low, the mux selects the feedback line, and the \nregister reloads its current value.   \n Question: \nQ. Why is it important to prevent spurious rising edges from causing incorrect behavior? \n Answer: \n\nA.\n\nIf spurious rising edges are allowed to cause incorrect behavior, then the overall circuit will not function correctly. In the specific case of the LOAD signal, if spurious rising edges are allowed, then the data on the bus will not be loaded correctly, and the overall circuit will not work as intended."}, {"text": "Title: Registers P10 \n Text: When LOAD is high, the mux selects the IN input, and the register \nloads a new value.   \n Question: \nQ. What is the advantage of using a mux and feedback loop from the flip-flop's output? \n Answer: \n\nA. The advantage of using a mux and feedback loop is that it allows the output of the flip-flop to be fed back to the input, which makes the system more stable."}, {"text": "Title: Registers P11 \n Text: The result is similar to a gated D latch with distinct write enable \nand clock lines. \n Question: \nQ. Q: What happens when LOAD is high?\nQ. \nQ. A: The mux selects the input line and the register loads the value on the input line. \n Answer: "}, {"text": "Title: Registers P12 \n Text: \nWe can use this extended flip-flop as a bit slice for a multi-bit register. \n Question: \nQ. Why does the mux select the IN input when LOAD is high? \n Answer: \n\nA. When LOAD is high, the mux selects the IN input so that the register can load a new value."}, {"text": "Title: Registers P13 \n Text: A four-bit register of this type\nis shown to the right.  Four data {lines---one} for each\n{bit---enter} the registers from the top of the figure.  \nWhen LOAD is low, the logic copies each flip-flop's value back to its\ninput, \n Question: \nQ. What is a gated D latch? \n Answer: \n\nA. A gated D latch is a type of flip-flop that uses a gate to control when the data is stored. The gate is usually controlled by a separate signal, such as a clock. This allows the data to be latched (stored) only when needed."}, {"text": "Title: Registers P14 \n Text: and the IN input lines are ignored.  When LOAD is high,\nthe muxes forward each IN line to the corresponding flip-flop's D\ninput, allowing the register to load the new {4-bit} value.\nThe use of one input line per bit to load a multi-bit\nregister in a single cycle is termed a { parallel load}. \n Question: \nQ. What is a bit slice?\nQ. \nQ. A bit slice is a portion of a multi-bit register. \n Answer: "}, {"text": "Title: Shift Registers P0 \n Text: \nCertain types of registers include logic to manipulate data held\nwithin the register.  A { shift register} is an important example\nof this \n Question: \nQ. What is a register? \n Answer: \n\nA register is an abstraction used for storage of groups of bits in digital systems."}, {"text": "Title: Shift Registers P1 \n Text: type.  The simplest shift register is a series of D flip-flops,\nwith the output of each attached to the input of the next, as shown to the\nright above.  In the circuit shown, a serial input SI accepts a single bit \nof data per cycle and delivers the bit four cycles later to a serial \noutput SO.  Shift registers serve many purposes in modern systems, from the\nobvious uses of providing a fixed delay and performing bit shifts for\nprocessor arithmetic to rate matching between components and reducing\nthe pin count on programmable logic devices such as field programmable\ngate arrays (FPGAs), the modern form of the programmable logic array\nmentioned in the textbook. \n Question: \nQ. When was the Fall 2012 offering of the course? \n Answer: \n\nA. The Fall 2012 offering of the course was in the fall of 2012."}, {"text": "Title: Shift Registers P2 \n Text: An example helps to illustrate the rate matching problem: \nhistorical I/O buses used fairly slow clocks, as they had to\ndrive signals and be arbitrated over relatively long distances.\nThe Peripheral Control\nInterconnect (PCI) standard, for example, provided for 33 and 66 MHz\nbus speeds.  To provide adequate data rates, such buses use many wires\nin parallel, either 32 or 64 in the case of PCI.  In contrast, a\nGigabit Ethernet (local area network) signal travelling over a fiber\nis clocked at 1.25 GHz, but sends only one bit per cycle.  Several\nlayers of shift registers sit between the fiber and the I/O bus to\nmediate between the slow, highly parallel signals that travel over the\nI/O bus and the fast, serial signals that travel over the \nfiber.  The latest variant of PCI, PCIe (e for ``express''),\nuses serial lines at much higher clock rates. \n Question: \nQ. How is a register different from a flip-flop? \n Answer: \n\nA register is a storage element composed from one or more flip-flops operating on a common clock."}, {"text": "Title: Shift Registers P3 \n Text: Returning to the figure above, imagine that the outputs Q_i feed\ninto logic clocked at 1/4^ the rate of the shift register \n(and suitably synchronized).  Every four cycles, the flip-flops fill\nup with another four bits, at which point the outputs are read in\nparallel.  The shift register shown can thus serve to transform serial\ndata to {4-bit-parallel} data at one-quarter the clock speed.\nUnlike the registers discussed earlier, the shift register above does\nnot support parallel load, which prevents it from transforming a slow,\nparallel stream of data into a high-speed serial stream.  The use of\n{ serial load} requires N cycles for an {N-bit}\nregister, but can reduce the number of wires needed to support the\noperation of the shift register.  How would you add support for\nparallel load?  How many additional inputs would be necessary? \n Question: \nQ. What does the text mean by \"most registers include logic to control the bits stored by the register\"? \n Answer: \n\nA. Most registers have logic circuits that can set or reset individual bits stored in the register."}, {"text": "Title: Shift Registers P4 \n Text: The shift register above also shifts continuously, and cannot store a \nvalue.  A set of muxes, analogous to those that we used to control \nregister loading, can be applied to control shifting, as shown \nbelow. \n Question: \nQ. How often do D flip-flops copy their inputs?\nQ. \nQ. D flip-flops copy their inputs at the rising edge of each clock cycle. \n Answer: "}, {"text": "Title: Shift Registers P5 \n Text: Using a {4-to-1} mux, we can construct a shift\nregister with additional functionality.  The bit slice at the top\nof the next page allows us to build a { bidirectional shift register} with \nparallel load capability and the ability to retain its value indefinitely.\nThe two-bit control input C uses a representation that\nwe have chosen for the four operations supported by our shift register, \nas shown in the table below the bit slice design. \n Question: \nQ. What would happen if the flip-flop was not able to retain its value? \n Answer: \n\nA. If the flip-flop was not able to retain its value, it would lose its value on the rising edge of the clock."}, {"text": "Title: Shift Registers P6 \n Text: \nThe bit slice allows us to build {N-bit} shift registers by\nreplicating the slice and adding a fixed amount of ``{ glue logic}.''\nFor example, the figure below represents a {4-bit} bidirectional \nshift register constructed in this way.  The mux\nused for the SO output logic is the glue logic needed in addition\nto the four bit slices. \n Question: \nQ. What is clock gating? \n Answer: \n\nA. Clock gating is a method of controlling the clock signals through the LOAD input."}, {"text": "Title: Shift Registers P7 \n Text: At each rising clock edge, the action specified by C_1C_0 is taken.  \nWhen C_1C_0=00, the\nregister holds its current value, with the register\nvalue appearing on\nQ[3:0] and each flip-flop feeding its output back into its input.\nFor C_1C_0=01, the shift register shifts left: the serial input,\nSI, is fed into flip-flop 0, and Q_3 is passed to the serial\noutput, SO.  Similarly, when C_1C_0=11, the shift register shifts\nright: SI is fed into flip-flop 3, and Q_0 is passed to SO.\nFinally, the case C_1C_0=10 causes all flip-flops to accept new\nvalues from IN[3:0], effecting a parallel load. \n Question: \nQ. What is the purpose of the LOAD input?\nQ. \nQ. The LOAD input is used to load the value of the flip-flop. When LOAD is high, the circuit reduces to a regular D flip-flop. When LOAD is low, the flip-flop clock input, c, is held high, and the flip-flop stores its current value. \n Answer: "}, {"text": "Title: Shift Registers P8 \n Text: {c|c}\nC_1C_0& meaning \n00& retain current value\n01& shift left (low to high)\n10& load new value (from IN)\n11& shift right (high to low) \n Question: \nQ. What are the problems with clock gating? \n Answer: \n\nA. The first problem with clock gating is that it introduces clock skew, which may cause timing problems later in the development process. The second problem is that it may increase the power consumption of the device."}, {"text": "Title: Shift Registers P9 \n Text: Several specialized shift operations are used to support data\nmanipulation in modern processors (CPUs).  Essentially, these\nspecializations dictate the glue logic for a shift\nregister as well as the serial input value.  The simplest is a {\nlogical shift}, for which SI is hardwired to 0: incoming\nbits are always 0.  A { cyclic shift} takes SO and feeds it\nback into SI, forming a circle of register bits through which the\ndata bits cycle. \n Question: \nQ. Why is it important to prevent spurious rising edges from causing incorrect behavior? \n Answer: \n\nA.\n\nIf spurious rising edges are allowed to cause incorrect behavior, then the overall circuit will not function correctly. In the specific case of the LOAD signal, if spurious rising edges are allowed, then the data on the bus will not be loaded correctly, and the overall circuit will not work as intended."}, {"text": "Title: Shift Registers P10 \n Text: Finally, an { arithmetic shift} treats the shift register contents\nas a number in 2's complement form.  For non-negative numbers and left\nshifts, an arithmetic shift is the same as a logical\nshift.  When a negative number is arithmetically shifted to\nthe right, however, the sign bit is retained, resulting in a function\nsimilar to division by two.  The difference lies in the rounding\ndirection.  Division by two rounds towards zero in most \nprocessors: -5/2 gives -2.\nArithmetic shift right rounds away from zero for negative numbers (and\ntowards zero for positive numbers): -5>>1 gives -3.  We transform our\nprevious shift register into one capable of arithmetic shifts by\neliminating the serial input and feeding the most significant bit,\nwhich represents the sign in 2's complement form, back into itself for\nright shifts, as shown below.  The bit shifted in for left shifts\nhas been hardwired to 0. \n Question: \nQ. What is the advantage of using a mux and feedback loop from the flip-flop's output? \n Answer: \n\nA. The advantage of using a mux and feedback loop is that it allows the output of the flip-flop to be fed back to the input, which makes the system more stable."}, {"text": "Title: Summary of Part 2 of the Course P0 \n Text: These notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely. \n Question: \nQ. What is the author's purpose in writing this text?\nQ. \nQ. The author's purpose in writing this text is to provide an overview of the basic concepts of computer programming. \n Answer: "}, {"text": "Title: Summary of Part 2 of the Course P1 \n Text: The difficulty of learning depends on the type of task involved.\nRemembering new terminology is relatively easy, while applying\nthe ideas underlying design decisions shown by example to new problems \nposed as human tasks is relatively hard. \n Question: \nQ. What type of tasks are relatively easy to learn? \n Answer: \n\nA. Tasks that involve remembering new terminology are relatively easy to learn."}, {"text": "Title: Summary of Part 2 of the Course P2 \n Text: In this short summary, we give you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth). \n Question: \nQ. What does the author mean when they say \"at several levels of difficulty\"? \n Answer: \n\nA. The author is referring to the different levels of ability that students may have in relation to the material being studied. The author is expecting that students will be able to demonstrate their understanding of the material at different levels, depending on their individual abilities."}, {"text": "Title: Summary of Part 2 of the Course P3 \n Text: We'll start with the skills, and leave the easy stuff for the next page. \n Question: \nQ. What is the author's purpose for writing this text?\nQ. \nQ. The author's purpose is to inform the reader about the skills needed to be a successful manager. \n Answer: "}, {"text": "Title: Summary of Part 2 of the Course P4 \n Text: We expect you to be able to exercise the following skills: \n Question: \nQ. How does the author feel about the subject?\nQ. \nQ. The author seems to feel positively about the subject. \n Answer: "}, {"text": "Title: Summary of Part 2 of the Course P5 \n Text: {Design a CMOS gate for a simple Boolean function from n-type \nand p-type transistors.} \n Question: \nQ. What is the Boolean function that the CMOS gate is designed for? \n Answer: \n\nA. The Boolean function that the CMOS gate is designed for is to invert the input signal."}, {"text": "Title: Summary of Part 2 of the Course P6 \n Text: {Apply DeMorgan's laws repeatedly to simplify the form of\nthe complement of a Boolean expression.} \n Question: \nQ. What is the difference between a Boolean expression and a Boolean algebra? \n Answer: \n\nA Boolean expression is a mathematical expression that uses the logical operations of AND, OR, and NOT to combine two or more Boolean values. A Boolean algebra is a mathematical system that uses the same logical operations to combine two or more algebraic values."}, {"text": "Title: Summary of Part 2 of the Course P7 \n Text: {Use a K-map to find a reasonable expression for a Boolean function (for\nexample, in POS or SOP form with the minimal number of terms).} \n Question: \nQ. What is a K-map? \n Answer: \n\nA K-map is a graphical tool used to simplify and visualize Boolean functions. Boolean functions can be represented in either POS or SOP form, and the K-map allows you to see which terms are essential for the function and which are not. In addition, the K-map can be used to find the minimal number of terms needed to represent a Boolean function."}, {"text": "Title: Summary of Part 2 of the Course P8 \n Text: {More generally, translate Boolean logic functions among \nconcise algebraic, truth table, K-map, and canonical (minterm/maxterm) forms.} \n Question: \nQ. What is the most efficient way to translate Boolean logic functions? \n Answer: \n\nA. The most efficient way to translate Boolean logic functions is to use the truth table method."}, {"text": "Title: Summary of Part 2 of the Course P9 \n Text: When designing combinational logic, we expect you to be able to apply\nthe following design strategies: \n Question: \nQ. What are the design strategies for combinational logic? \n Answer: \n\nA. The design strategies for combinational logic are:\n\n1. Decomposition\n2. State machine design\n3. Logic minimization\n4. Boolean algebra"}, {"text": "Title: Summary of Part 2 of the Course P10 \n Text: {Make use of human algorithms \n(for example, multiplication from addition).} \n Question: \nQ. What is an algorithm?\nQ. \nQ. An algorithm is a set of instructions that are followed in order to complete a task. \n Answer: "}, {"text": "Title: Summary of Part 2 of the Course P11 \n Text: {Determine whether a bit-sliced approach is applicable, and, if so,\nmake use of one.} \n Question: \nQ. What is a bit-sliced approach? \n Answer: \n\nA. A bit-sliced approach is an approach in which each bit of data is processed separately. This can be used to improve speed or to reduce hardware requirements."}, {"text": "Title: Summary of Part 2 of the Course P12 \n Text: {Break truth tables into parts so as to solve each part of a function \nseparately.} \n Question: \nQ. Why is it important to break truth tables into parts? \n Answer: \n\nA. Breaking truth tables into parts helps to solve each part of a function separately. This can be useful when trying to determine the output of a complex function. It also allows for easier comparison of results between different parts of the truth table."}, {"text": "Title: Summary of Part 2 of the Course P13 \n Text: {Make use of known abstractions (adders, comparators, muxes, or other\nabstractions available to you) to simplify the problem.} \n Question: \nQ. What is an abstraction? \n Answer: \n\nAn abstraction is a simplification of a complex problem or system. It can be used to represent the essential features of something without including the details. Abstractions can be created for both physical objects and concepts."}, {"text": "Title: Summary of Part 2 of the Course P14 \n Text: And, at the highest level, we expect that you will be able to do the following: \n Question: \nQ. What are the consequences of a person's words and actions? \n Answer: \n\nA.\n\nThe consequences of a person's words and actions can be significant. They can affect the person's relationships, their job, and their social standing."}, {"text": "Title: Summary of Part 2 of the Course P15 \n Text: {Understand and be able to reason at a high-level about circuit design\ntradeoffs between area/cost and performance (and to know that power is also \nimportant, but we haven't given you any quantification methods).} \n Question: \nQ. What does it mean to reason at a high-level about circuit design tradeoffs? \n Answer: \n\nA. Reasoning at a high-level about circuit design tradeoffs means understanding the implications of different design choices on the overall performance of the circuit. It also involves considering the cost and power consumption of the circuit."}, {"text": "Title: Summary of Part 2 of the Course P16 \n Text: {Understand the tradeoffs typically made to develop bit-sliced \ndesigns---typically, bit-sliced designs are simpler but bigger and \nslower---and how one can develop variants between the extremes of\nthe bit-sliced approach and optimization of functions specific\nto an {N-bit} design.} \n Question: \nQ. What are the tradeoffs typically made to develop bit-sliced designs? \n Answer: \n\nA. The tradeoffs typically made to develop bit-sliced designs include simplicity versus complexity, size versus speed, and generality versus specialization."}, {"text": "Title: Summary of Part 2 of the Course P17 \n Text: {Understand the pitfalls of marking a function's value as ``don't care'' \nfor some input combinations, and recognize that implementations do not \nproduce ``don't care.''} \n Question: \nQ. What is a pitfall of marking a function's value as \"don't care\" for some input combinations? \n Answer: \n\nA. A pitfall of marking a function's value as \"don't care\" for some input combinations is that implementations do not produce \"don't care.\""}, {"text": "Title: Summary of Part 2 of the Course P18 \n Text: {Understand the tradeoffs involved in selecting a representation for\ncommunicating information between elements in a design, such as the bit \nslices in a bit-sliced design.} \n Question: \nQ. What are some of the tradeoffs involved in selecting a representation for communicating information between elements in a design? \n Answer: \n\nA. Some of the tradeoffs involved in selecting a representation for communicating information between elements in a design include the number of bits used to represent each element, the order in which the bits are transmitted, and the amount of time required to transmit the information."}, {"text": "Title: Summary of Part 2 of the Course P19 \n Text: {Explain the operation of a latch or a flip-flop, particularly in \nterms of the bistable states used to hold a bit.} \n Question: \nQ. What is the difference between a latch and a flip-flop? \n Answer: \n\nA. A latch is a circuit that has two stable states, one that represents a \"1\" and the other that represents a \"0\". A flip-flop is a circuit that also has two stable states, but these correspond to different output logic levels, one \"high\" and the other \"low\"."}, {"text": "Title: Summary of Part 2 of the Course P20 \n Text: {Understand and be able to articulate the value of the clocked \nsynchronous design abstraction.} \n Question: \nQ. Why is the clocked synchronous design abstraction valuable? \n Answer: \n\nThe clocked synchronous design abstraction is valuable because it allows designers to reason about and verify their designs using timing diagrams. This abstractions makes it possible to predict the behavior of a design at a high level, and to verify that the design meets its timing requirements."}, {"text": "Title: Summary of Part 2 of the Course P21 \n Text: You should recognize all of these terms\nand be able to explain what they mean.  For the specific circuits, you \nshould be able to draw them and explain how they work. \n Question: \nQ. What does it mean when a circuit is \"activated\"? \n Answer: \n\nA. When a circuit is activated, it is turned on and able to conduct electricity."}, {"text": "Title: Summary of Part 2 of the Course P22 \n Text: Actually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.   \n Question: \nQ. What is the full adder? \n Answer: \n\nA. A full adder is a gate that performs the addition of two numbers."}, {"text": "Title: Summary of Part 2 of the Course P23 \n Text: {Boolean functions and logic gates}\n{-}{{}{}\n{}{}{} \n Question: \nQ. What is a Boolean function?\nQ. \nQ. A Boolean function is a mathematical function that returns a value of either 1 or 0, depending on the input. Boolean functions are used in many areas of mathematics and computer science, and are especially important in digital logic and computer programming. \n Answer: "}, {"text": "Title: Summary of Part 2 of the Course P24 \n Text: \n{specific logic circuits}\n{-}{{}{}\n{}{}{}\n{full adder}\n{half adder}\n{ripple carry adder}\n N-to-M multiplexer (mux)\n N-to-2N decoder\n{{- latch}}\n{{R-S latch}}\n{gated D latch} \n Question: \nQ. What is the difference between a full adder and a half adder? \n Answer: \n\nA full adder is a logic circuit that performs an addition operation on three input bits. A half adder is a logic circuit that performs an addition operation on two input bits."}, {"text": "Title: Summary of Part 2 of the Course P25 \n Text:  SSL altered term 3 Dec 21  \n Question: \nQ. What does SSL stand for?\nQ. \nQ. Secure Sockets Layer \n Answer: "}, {"text": "Title: Summary of Part 2 of the Course P26 \n Text: {master-slave implementation of a positive edge-triggered D flip-flop}\n{dual-latch implementation of a positive edge-triggered D flip-flop}\n{(bidirectional) shift register}\n{register supporting parallel load} \n Question: \nQ. What is a positive edge-triggered D flip-flop? \n Answer: \n\nA.\n\nA positive edge-triggered D flip-flop is a type of flip-flop that is triggered by a positive edge, or rising edge, on the clock signal. This means that the flip-flop will change state on the rising edge of the clock signal, and remain in that state for the remainder of the clock cycle."}, {"text": "Title: Summary of Part 2 of the Course P27 \n Text: \n{power, area/cost, performance}\n{computer-aided design (CAD) tools}\n{gate delay} \n Question: \nQ. What is the most important factor to consider when choosing a computer-aided design (CAD) tool? \n Answer: \n\nA. The most important factor to consider when choosing a computer-aided design (CAD) tool is the performance."}, {"text": "Title: Summary of Part 2 of the Course P28 \n Text: \n{general math concepts}\n{-}{{}{}\n{}{}{}\n{canonical form}\n{domain of a function}\n{{N-dimensional} hypercube} \n Question: \nQ. What is the canonical form of a function? \n Answer: \n\nA. The canonical form of a function is its mathematical expression."}, {"text": "Title: Summary of Part 2 of the Course P29 \n Text: \n{tools for solving logic problems}\n{-}{{}{}\n{}{}{}\n{truth table}\n{Karnaugh map (K-map)} \n Question: \nQ. What tools are used to solve logic problems? \n Answer: \n\nA. Truth tables and Karnaugh maps (K-maps) are used to solve logic problems."}, {"text": "Title: Summary of Part 2 of the Course P30 \n Text: {prime implicant}\n{bit-slicing}\n{timing diagram} \n Question: \nQ. What is a bit-slicing? \n Answer: \n\nA.\n\nA bit-slicing is a technique for reducing the number of bits required to represent a certain value. It is often used in computer architecture and digital signal processing."}, {"text": "Title: Summary of Part 2 of the Course P31 \n Text: {device technology}\n{-}{{}{}\n{}{}{}\n{complementary metal-oxide semiconductor (CMOS)}\n{field effect transistor (FET)}\n{transistor gate, source, drain} \n Question: \nQ. What is the main difference between a CMOS and a FET transistor? \n Answer: \n\nA. The main difference between a CMOS and a FET transistor is that a CMOS transistor has a gate, source, and drain, while a FET transistor only has a gate and a source."}, {"text": "Title: Summary of Part 2 of the Course P32 \n Text: \n{Boolean logic terms}\n{-}{{}{}\n{}{}{} \n Question: \nQ. What does the term \"Boolean logic\" refer to?\nQ. \nQ. In computer science, Boolean logic is the mathematical study of the logical operations of truth values, usually denoted by the values 0 (false) and 1 (true). \n Answer: "}, {"text": "Title: Summary of Part 2 of the Course P33 \n Text: {algebraic properties}\n{dual form, principle of duality}\n{sum, product}\n{minterm, maxterm}\n{sum-of-products (SOP)}\n{product-of-sums (POS)}\n{canonical sum/SOP form}\n{canonical product/POS form}\n{logical equivalence} \n Question: \nQ. What is the principle of duality? \n Answer: \n\nThe principle of duality is the algebraic property that states that the sum and product of two Boolean expressions are duals of each other. This means that the sum of two Boolean expressions is equivalent to the product of their duals."}, {"text": "Title: Summary of Part 2 of the Course P34 \n Text: \n{digital systems terms}\n{-}{{}{}\n{}{}{}\n{word size}\n{{N-bit} Gray code}\n{combinational/combinatorial logic}\n{-}{{}{}\n{}{}{}\n{two-level logic}\n{``don't care'' outputs (x's)} \n Question: \nQ. What is a Gray code? \n Answer: \n\nA Gray code is a digital code in which each successive code word differs from the previous code word by only one bit."}, {"text": "Title: Summary of Part 2 of the Course P35 \n Text: {active low input}\n{set a bit (to 1)}\n{reset a bit (to 0)} \n Question: \nQ. What is the difference between setting and resetting a bit?\nQ. \nQ. Setting a bit changes the value of the bit to 1, while resetting a bit changes the value of the bit to 0. \n Answer: "}, {"text": "Title: Summary of Part 2 of the Course P36 \n Text:  SSL altered term 3 Dec 21  \n Question: \nQ. The text describes a time when the author was very young and living in poverty.\nQ. \nQ. What is the author's opinion of their childhood poverty? \n Answer: \n\nThe author's opinion of their childhood poverty is that it was difficult and they were not able to get the things they wanted."}, {"text": "Title: Summary of Part 2 of the Course P37 \n Text: {master-slave implementation}\n{dual-latch implementation}\n{positive edge-triggered} \n Question: \nQ. What is the master-slave implementation?\nQ. \nQ. The master-slave implementation is a type of digital circuit where the output of one latch is connected to the input of another latch. The output of the first latch is the \"master\" and the output of the second latch is the \"slave\". This type of circuit is often used to create a delay between the two latches. \n Answer: "}, {"text": "Title: Summary of Part 2 of the Course P38 \n Text: {clock signal}\n{-}{{}{}\n{}{}{}\n{square wave}\n{rising/positive clock edge}\n{falling/negative clock edge}\n{clock gating} \n Question: \nQ. What is clock gating? \n Answer: \n\nA. Clock gating is a technique used to reduce power consumption by shutting off the clock signal to selected parts of a circuit. This can be done either by stopping the clock altogether, or by using a separate clock signal that is only active when needed."}, {"text": "Title: Summary of Part 2 of the Course P39 \n Text: {clocked synchronous sequential circuit}\n{parallel/serial load of register}\n FIXME?  too informal to ask them to remember it\n {glue logic}\n{logical/arithmetic/cyclic shift} \n Question: \nQ. What is the main difference between a clocked synchronous sequential circuit and a parallel/serial load of register? \n Answer: \n\nA. The main difference between a clocked synchronous sequential circuit and a parallel/serial load of register is that the former is used to store data in a memory while the latter is used to load data into a register."}, {"text": "Title: Summary of Part 2 of the Course P40 \n Text: {   }  blank 3rd page \n Question: \nQ. \nQ. What does the author say about people who are \"a little off\"?\nQ. \nQ. The author says that people who are \"a little off\" are usually the ones who are the most fun to be around. \n Answer: "}, {"text": "Title: Serialization and Finite State Machines P0 \n Text: The third part of our class builds upon the basic combinational and\nsequential logic elements that we developed in the second part. \n Question: \nQ. What is the third part of our class about?\nQ. \nQ. The third part of our class is about building upon the basic combinational and sequential logic elements that we developed in the second part. \n Answer: "}, {"text": "Title: Serialization and Finite State Machines P1 \n Text: After discussing a simple application of stored state\nto trade between area and performance, \n Question: \nQ. How can stored state be used to improve performance?\nQ. \nQ. One way to use stored state to improve performance is to trade between area and performance. This can be done by storing data in a way that uses less space, or by storing data in a way that is faster to access. \n Answer: "}, {"text": "Title: Serialization and Finite State Machines P2 \n Text: we introduce a powerful abstraction for formalizing and reasoning about\ndigital systems, the Finite State Machine (FSM). \n Question: \nQ. How does the Finite State Machine (FSM) help formalize and reason about digital systems? \n Answer: \n\nThe FSM provides a well-defined model for digital systems that can be used for formal analysis and reasoning. This abstractions helps to identify and track the states of the system, and to reason about the behavior of the system."}, {"text": "Title: Serialization and Finite State Machines P3 \n Text: General FSM models are broadly applicable in a range of engineering\ncontexts, including not only hardware and software design but also\nthe design of control systems and distributed systems.  We limit our\nmodel so as to avoid circuit timing issues in your first exposure, but\nprovide some amount of discussion as to how, when, and why you should \neventually learn the more sophisticated models. \n Question: \nQ. Why are general FSM models broadly applicable in a range of engineering contexts? \n Answer: \n\nA. The general FSM model is a simple and elegant way to describe the behavior of systems. It is easy to understand and can be applied to a wide range of engineering contexts."}, {"text": "Title: Serialization and Finite State Machines P4 \n Text: Through development a range of FSM examples, we illustrate important \ndesign issues for these systems and motivate a couple of more advanced \ncombinational logic devices that can be used as building blocks. \n Question: \nQ. What are some design issues that need to be considered when developing a FSM? \n Answer: \n\nSome design issues that need to be considered when developing a FSM include:\n-Ensuring that the FSM is correctly specified\n-Ensuring that the FSM is correctly implemented\n-Ensuring that the FSM is correctly tested\n-Ensuring that the FSM is correctly integrated into the overall system"}, {"text": "Title: Serialization and Finite State Machines P5 \n Text: Together with the idea of memory, another form of stored state,\nthese elements form the basis for development of our first computer. \n Question: \nQ. How does the idea of memory relate to stored state? \n Answer: \n\nA. Memory is a form of stored state."}, {"text": "Title: Serialization and Finite State Machines P6 \n Text: At this point we return to the textbook, in which Chapters 4 and 5\nprovide a solid introduction to the von Neumann model of computing systems\nand the {LC-3} (Little Computer, version 3) instruction set \narchitecture.  By the end of this part of the course, you will have\nseen an example of the boundary between hardware and software, and will\nbe ready to write some instructions yourself. \n Question: \nQ. What is the boundary between hardware and software? \n Answer: \n\nA. The boundary between hardware and software is the point at which instructions are executed by the CPU."}, {"text": "Title: Serialization and Finite State Machines P7 \n Text: In this set of notes, we cover the first few parts of this material.\nWe begin by describing the conversion of bit-sliced designs into \nserial designs, which store a single bit slice's output in \nflip-flops and then feed the outputs back into the bit slice in the next\ncycle.  As a specific example, we use our bit-sliced comparator \nto discuss tradeoffs in area and performance.  We introduce\nFinite State Machines and some of the tools used to design them,\nthen develop a handful of simple counter designs.  Before delving\ntoo deeply into FSM design issues, we spend a little time discussing\nother strategies for counter design and placing the material covered\nin our course in the broader context of digital system design. \n Question: \nQ. What are the tradeoffs between bit-sliced and serial designs? \n Answer: \n\nA. The tradeoffs between bit-sliced and serial designs include area, performance, and power."}, {"text": "Title: Serialization and Finite State Machines P8 \n Text: Remember that\n{ sections marked with an asterisk are provided solely for your\ninterest,} but you may need to learn this material in later\nclasses. \n Question: \nQ. Why might someone be interested in learning about the history of the\nQ. English language?\nQ. \nQ. Someone might be interested in learning about the history of the English language in order to better understand the origins of the language and how it has evolved over time. \n Answer:  Additionally, this information can help individuals better understand the relationship between English and other languages, as well as the influences that have shaped English throughout its history."}, {"text": "Title: Serialization: General Strategy P0 \n Text: In previous notes, we discussed and illustrated the development of bit-sliced\nlogic, in which one designs a logic block to handle one bit of a multi-bit\noperation, then replicates the bit slice logic to construct a design for \nthe entire operation.  We developed ripple carry adders in this way in \nNotes Set 2.3 and both unsigned and 2's complement comparators in \nNotes Set 2.4. \n Question: \nQ. What is the third part of our class about?\nQ. \nQ. The third part of our class is about building upon the basic combinational and sequential logic elements that we developed in the second part. \n Answer: "}, {"text": "Title: Serialization: General Strategy P1 \n Text: \nAnother interesting design strategy is { serialization}: rather than \nreplicating the bit slice, we can use flip-flops to store the bits passed\nfrom one bit slice to the next, then present the stored bits { to the\nsame bit slice} in the next cycle.  Thus, in a serial design, we only\nneed one copy of the bit slice logic!  The area needed for a serial design\nis usually much less than for a bit-sliced design, but such a design is\nalso usually slower.  After illustrating the general design strategy,\nwe'll consider these tradeoffs more carefully\nin the context of a detailed example. \n Question: \nQ. How can stored state be used to improve performance?\nQ. \nQ. One way to use stored state to improve performance is to trade between area and performance. This can be done by storing data in a way that uses less space, or by storing data in a way that is faster to access. \n Answer: "}, {"text": "Title: Serialization: General Strategy P2 \n Text: \nRecall the general bit-sliced design approach, as illustrated to the right. \n Question: \nQ. How does the Finite State Machine (FSM) help formalize and reason about digital systems? \n Answer: \n\nThe FSM provides a well-defined model for digital systems that can be used for formal analysis and reasoning. This abstractions helps to identify and track the states of the system, and to reason about the behavior of the system."}, {"text": "Title: Serialization: General Strategy P3 \n Text: Some number of copies of the logic for a single bit slice are connected\nin sequence.  Each bit slice accepts P bits of operand input and\nproduces Q bits of external output.  Adjacent bit slices receive\nan additional M bits of information from the previous bit slice\nand pass along M bits to the next bit slice, generally using some\nrepresentation chosen by the designer. \n Question: \nQ. Why are general FSM models broadly applicable in a range of engineering contexts? \n Answer: \n\nA. The general FSM model is a simple and elegant way to describe the behavior of systems. It is easy to understand and can be applied to a wide range of engineering contexts."}, {"text": "Title: Serialization: General Strategy P4 \n Text: The first bit slice is initialized\nby passing in constant values, and some calculation may be performed\non the final bit slice's results to produce R bits more external output. \n Question: \nQ. What are some design issues that need to be considered when developing a FSM? \n Answer: \n\nSome design issues that need to be considered when developing a FSM include:\n-Ensuring that the FSM is correctly specified\n-Ensuring that the FSM is correctly implemented\n-Ensuring that the FSM is correctly tested\n-Ensuring that the FSM is correctly integrated into the overall system"}, {"text": "Title: Serialization: General Strategy P5 \n Text: We can transform this bit-sliced design to a serial design with a single\ncopy of the bit slice logic, M+Q flip-flops, and M gates (and sometimes\nan inverter).  The strategy is illustrated on the right below.  A single\ncopy of the bit slice operates on one set of P external input bits \nand produces one set of Q external output bits each clock cycle.  In\nthe design shown, these output bits are available during the next \ncycle, after they have been stored in the flip-flops. \nThe M bits to be passed to the ``next'' bit slice are also \nstored in flip-flops,\nand in the next cycle are provided back to the same physical bit slice\nas inputs.  The first cycle of a multi-cycle operation\nmust be handled slightly differently, so we\nadd selection logic and an control signal, F.  For the first cycle, we\napply F=1, and the initial values are passed into the bit slice.  \nFor all other bits, we apply F=0, and the values stored in the flip-flops\nare returned to the bit slice's inputs.  After all bits have passed through\nthe bit slice---after N cycles for an {N-bit} design---the\nfinal M bits are stored in the flip-flops, and the results are calculated\nby the output logic.  \n Question: \nQ. How does the idea of memory relate to stored state? \n Answer: \n\nA. Memory is a form of stored state."}, {"text": "Title: Serialization: General Strategy P6 \n Text: The selection logic merits explanation.  Given that the original design\ninitialized the bits to constant values (0s or 1s), we need only simple\nlogic for selection.  The two drawings on the left above illustrate \nhow {B_i}, the complemented flip-flop output for a bit i, can be\ncombined with the first-cycle signal F to produce an appropriate input\nfor the bit slice.  Selection thus requires one extra gate for each of \nthe M inputs, and we need an inverter for F if any of the initial \nvalues is 1. \n Question: \nQ. What is the boundary between hardware and software? \n Answer: \n\nA. The boundary between hardware and software is the point at which instructions are executed by the CPU."}, {"text": "Title: Serialization: Comparator Example P0 \n Text: \nWe now apply the general strategy to a specific example, our bit-sliced\nunsigned comparator from Notes Set 2.4.  The result is shown to the right.\nIn terms of the general model, the single comparator bit slice accepts P=2\nbits of inputs each cycle, in this case a single bit from each of the two\nnumbers being compared, presented to the bit slice in increasing order\nof significance.  The bit slice produces no external output other than\nthe final result (Q=0).  Two bits (M=2) are produced each cycle by \nthe bit slice and stored \n Question: \nQ. What is the third part of our class about?\nQ. \nQ. The third part of our class is about building upon the basic combinational and sequential logic elements that we developed in the second part. \n Answer: "}, {"text": "Title: Serialization: Comparator Example P1 \n Text: into flip flops B_1 and B_0.  These bits\nrepresent the relationship between the two numbers compared so far\n(including only the bit already seen by the comparator bit slice).\nOn the first cycle, when the least significant bits of A and B are\nbeing fed into the bit slice, we set F=1, which forces the C_1 \nand C_0 inputs of the bit slice to 0 independent of the values stored\nin the flip-flops.  In all other cycles, F=0, and the NOR gates\nset C_1=B_1 and C_0=B_0.  Finally, after N cycles for an {N-bit}\ncomparison, the output logic---in this case simply wires, as shown \nin the dashed box---places the result of the comparison on \nthe Z_1 and Z_0 outputs (R=2 in the general model).  \nThe result is encoded in the \nrepresentation defined for constructing the bit slice (see Notes Set 2.4,\nbut the encoding does not matter here). \n Question: \nQ. How can stored state be used to improve performance?\nQ. \nQ. One way to use stored state to improve performance is to trade between area and performance. This can be done by storing data in a way that uses less space, or by storing data in a way that is faster to access. \n Answer: "}, {"text": "Title: Serialization: Comparator Example P2 \n Text: \nHow does the serial design compare with the bit-sliced design? \n Question: \nQ. How does the Finite State Machine (FSM) help formalize and reason about digital systems? \n Answer: \n\nThe FSM provides a well-defined model for digital systems that can be used for formal analysis and reasoning. This abstractions helps to identify and track the states of the system, and to reason about the behavior of the system."}, {"text": "Title: Serialization: Comparator Example P3 \n Text: As an estimate of area, let's count gates.  Our optimized \ndesign is replicated to the right for convenience.   \n Question: \nQ. Why are general FSM models broadly applicable in a range of engineering contexts? \n Answer: \n\nA. The general FSM model is a simple and elegant way to describe the behavior of systems. It is easy to understand and can be applied to a wide range of engineering contexts."}, {"text": "Title: Serialization: Comparator Example P4 \n Text: Each bit slice requires six {2-input} gates and two inverters.   \n Question: \nQ. What are some design issues that need to be considered when developing a FSM? \n Answer: \n\nSome design issues that need to be considered when developing a FSM include:\n-Ensuring that the FSM is correctly specified\n-Ensuring that the FSM is correctly implemented\n-Ensuring that the FSM is correctly tested\n-Ensuring that the FSM is correctly integrated into the overall system"}, {"text": "Title: Serialization: Comparator Example P5 \n Text: Assume that each \nflip-flop requires eight {2-input} gates and two inverters,\nso the serial design overall requires 24 gates \n Question: \nQ. How does the idea of memory relate to stored state? \n Answer: \n\nA. Memory is a form of stored state."}, {"text": "Title: Serialization: Comparator Example P6 \n Text: and six inverters to handle any number of bits. \n Question: \nQ. What is the boundary between hardware and software? \n Answer: \n\nA. The boundary between hardware and software is the point at which instructions are executed by the CPU."}, {"text": "Title: Serialization: Comparator Example P7 \n Text: Thus, for any number of bits N, the serial design is smaller\nthan the bit-sliced design, and the benefit grows with N. \n Question: \nQ. What are the tradeoffs between bit-sliced and serial designs? \n Answer: \n\nA. The tradeoffs between bit-sliced and serial designs include area, performance, and power."}, {"text": "Title: Serialization: Comparator Example P8 \n Text: What about performance?  In Notes Set 2.4, we counted gate delays for \nour bit-sliced design.  The path from A or B to the outputs is\nfour gate delays, but the C to Z paths are all two gate delays.\nOverall, then, the bit-sliced design requires 2N+2 gate delays \nfor N bits.  What about the serial design? \n Question: \nQ. Why might someone be interested in learning about the history of the\nQ. English language?\nQ. \nQ. Someone might be interested in learning about the history of the English language in order to better understand the origins of the language and how it has evolved over time. \n Answer:  Additionally, this information can help individuals better understand the relationship between English and other languages, as well as the influences that have shaped English throughout its history."}, {"text": "Title: Serialization: Comparator Example P9 \n Text: The performance of the serial design is likely to be much worse\nfor three reasons. \n Question:  \n Answer: \n1. What is the main advantage of using bit-sliced logic?\n\nThe main advantage of using bit-sliced logic is that it is easier to design and implement complex digital circuits. With bit-sliced logic, each bit of the circuit is handled by a separate logic block, which can be designed and tested independently. This modular approach makes it much easier to develop and debug complex digital circuits."}, {"text": "Title: Serialization: Comparator Example P10 \n Text: First, all paths in the design matter, not just the paths from bit\nslice to bit slice.  None of the inputs can be assumed to be available\nbefore the start of the clock cycle, so we must consider all paths\nfrom input to output.   \n Question: \nQ. What is the main advantage of using a serial design over a bit-sliced design?\nQ. \nQ. The main advantage of using a serial design over a bit-sliced design is that it requires less area. \n Answer: "}, {"text": "Title: Serialization: Comparator Example P11 \n Text: Second, we must also count gate delays for the\nselection logic as well as the gates embedded in the flip-flops. \n Question: \nQ. How does the general bit-sliced design approach work? \n Answer: \n\nA. The general bit-sliced design approach works by breaking down a signal into its individual bits, and then processing each bit independently. This allows for parallel processing of the signal, which can lead to faster overall processing time."}, {"text": "Title: Serialization: Comparator Example P12 \n Text: Finally, the result of these calculations may not matter, since the\nclock speed may well be limited by other logic elsewhere in the\nsystem.  If we want a common clock for all of our logic, the clock\nmust not go faster than the slowest element in the entire system, or\nsome of our logic will not work properly. \n Question: \nQ. How many bit slices are connected in sequence? \n Answer: \n\nA. The number of bit slices connected in sequence is determined by the number of bits in the operand input."}, {"text": "Title: Serialization: Comparator Example P13 \n Text: What is the longest path through our serial comparator? \n Question: \nQ. What is the purpose of the first bit slice?\nQ. \nQ. The first bit slice is used to initialize the value of the bits that will be used in the calculation. \n Answer: "}, {"text": "Title: Serialization: Comparator Example P14 \n Text: Let's assume that \nthe path through a flip-flop is eight gate delays, with four on each \nside of the clock's rising edge.  The inputs A and B are\nlikely to be driven by flip-flops elsewhere in our system, so\nwe conservatively count four gate delays to A and B and\nfive gate delays to C_1 and C_0 (the extra one comes from\nthe selection logic).  The A and B paths thus dominate inside\nthe bit slice, adding four more gate delays to the outputs Z_1\nand Z_0.  Finally, we add the last four gate delays to flip\nthe first latch in the flip-flops for a total of 12 gate delays. \n Question: \nQ. How does the serial design with a single copy of the bit slice logic work? \n Answer: \n\nA. The serial design with a single copy of the bit slice logic works by applying an input signal to the bit slice logic in the first cycle, and then storing the output of the bit slice logic in a flip-flop. In the second cycle, the bit slice logic receives the output of the flip-flop as an input, and the output of the bit slice logic is again stored in the flip-flop. This process is repeated for each bit in the input signal."}, {"text": "Title: Serialization: Comparator Example P15 \n Text: If we assume that our serial comparator limits the clock frequency\n(that is, if everything else in the system can use a faster clock),\nwe take 12 gate delays per cycle, or 12N gate delays to compare\ntwo {N-bit} numbers. \n Question: \nQ. What is the purpose of the extra gate for each of the M inputs? \n Answer: \n\nA. The extra gate is used to select the input for the bit slice."}, {"text": "Title: Serialization: Comparator Example P16 \n Text: You might also notice that adding support for 2's complement is no \nlonger free.  We need extra logic to swap the A and B inputs\nin the cycle corresponding to the sign bits of A and B.  In\nother cycles, they must remain in the usual order.  This extra\nlogic is not complex, but adds further delay to the paths. \n Question: \nQ. What is the function of the bit slice in this example?\nQ. \nQ. The bit slice in this example is used to compare two numbers. \n Answer: "}, {"text": "Title: Serialization: Comparator Example P17 \n Text: The bit-sliced and serial designs represent two extreme points in \na broad space of design possibilities.  Optimization of the entire\nN-bit logic function (for any metric) represents a third extreme.\nAs an engineer, you should realize that you can design systems\nanywhere in between these points as well.  At the end of Notes Set 2.4,\nfor example, we showed a design for a logic slice that compares \ntwo bits at a time.  In general, we can optimize logic for any \nnumber of bits and then apply multiple copies of the resulting\nlogic in space (a generalization of the bit-sliced approach),\nor in time (a generalization of the serialization approach),\nor in a combination of the two. \n Question: \nQ. How does the output logic place the result of the comparison on the Z_1 and Z_0 outputs? \n Answer: \n\nA. The output logic places the result of the comparison according to the bit slice encoding."}, {"text": "Title: Serialization: Comparator Example P18 \n Text: Sometimes these tradeoffs may happen at a higher level.\nAs mentioned in Notes Set 2.3, computer software uses \nthe carry out of an adder to perform addition of larger \ngroups of bits (over multiple clock cycles)\nthan is supported by the processor's adder hardware. \n Question: \nQ. What is the main difference between the serial and bit-sliced designs? \n Answer: \n\nA. The main difference between the serial and bit-sliced designs is that the former uses a single serial data stream while the latter uses multiple parallel data streams."}, {"text": "Title: Serialization: Comparator Example P19 \n Text: In computer system design, engineers often design hardware elements\nthat are general enough to support this kind of extension in software. \n Question: \nQ. What does the author mean by \"counting gates\"? \n Answer: \n\nA. The author means that they are using the number of gates as an estimate of the area of the design."}, {"text": "Title: Serialization: Comparator Example P20 \n Text: As a concrete example of the possible tradeoffs,\nconsider a serial comparator design based on the {2-bit} slice variant. \n Question: \nQ. What is the total number of gates required for a 32-bit slice?\nQ. \nQ. A 32-bit slice would require 72 gates. \n Answer: "}, {"text": "Title: Serialization: Comparator Example P21 \n Text: This approach leads to a serial design with 24 gates and 10 inverters, \nwhich is not much\nlarger than our earlier serial design.  In terms of gate delays,\nhowever, the new design is identical, meaning that we finish a comparison\nin half the time.  More realistic area and timing metrics show\nslightly more difference between the two designs.  These differences can \ndominate the \nresults if we blindly scale the idea to handle more bits without \nthinking carefully about the design.  Neither many-input gates\nnor gates driving many outputs work well in practice. \n Question: \nQ. What are the two inputs of a flip-flop? \n Answer: \n\nA. The two inputs of a flip-flop are the data input and the clock signal."}, {"text": "Title: Finite State Machines P0 \n Text: A { finite state machine} (or { FSM}) is a model for understanding\nthe behavior of a system by describing the system as occupying one of a\nfinite set of states, moving between these states in response to\nexternal inputs, and producing external outputs.  In any given state,\na particular input may cause the FSM to move to another state; this \ncombination is called a { transition rule}. \n Question: \nQ. What is the third part of our class about?\nQ. \nQ. The third part of our class is about building upon the basic combinational and sequential logic elements that we developed in the second part. \n Answer: "}, {"text": "Title: Finite State Machines P1 \n Text: An FSM comprises five parts: a finite set of states, a set of \npossible inputs, a set of possible outputs, a set of transition rules,\nand methods for calculating outputs.   \n Question: \nQ. How can stored state be used to improve performance?\nQ. \nQ. One way to use stored state to improve performance is to trade between area and performance. This can be done by storing data in a way that uses less space, or by storing data in a way that is faster to access. \n Answer: "}, {"text": "Title: Finite State Machines P2 \n Text: When an FSM is implemented as a digital system, all states must be\nrepresented as patterns using a fixed number of bits, all inputs must\nbe translated into bits, and all outputs must be translated into bits. \n Question: \nQ. How does the Finite State Machine (FSM) help formalize and reason about digital systems? \n Answer: \n\nThe FSM provides a well-defined model for digital systems that can be used for formal analysis and reasoning. This abstractions helps to identify and track the states of the system, and to reason about the behavior of the system."}, {"text": "Title: Finite State Machines P3 \n Text: For a digital FSM, transition rules must be { complete}; in other words,\ngiven any state of the FSM, and any pattern of input bits,\na transition must be defined from that state to another state \n(transitions from a state to itself, called { self-loops}, \nare acceptable). \n Question: \nQ. Why are general FSM models broadly applicable in a range of engineering contexts? \n Answer: \n\nA. The general FSM model is a simple and elegant way to describe the behavior of systems. It is easy to understand and can be applied to a wide range of engineering contexts."}, {"text": "Title: Finite State Machines P4 \n Text: And, of course, calculation of outputs for a digital FSM reduces\nto Boolean logic expressions. \n Question: \nQ. What are some design issues that need to be considered when developing a FSM? \n Answer: \n\nSome design issues that need to be considered when developing a FSM include:\n-Ensuring that the FSM is correctly specified\n-Ensuring that the FSM is correctly implemented\n-Ensuring that the FSM is correctly tested\n-Ensuring that the FSM is correctly integrated into the overall system"}, {"text": "Title: Finite State Machines P5 \n Text: In this class, we focus on clocked synchronous FSM implementations,\nin which the FSM's internal state bits are stored in flip-flops. \n Question: \nQ. How does the idea of memory relate to stored state? \n Answer: \n\nA. Memory is a form of stored state."}, {"text": "Title: Finite State Machines P6 \n Text: In this section, we introduce the tools used to describe, develop, and\nanalyze implementations of FSMs with digital logic.  In the next\nfew weeks, we will show you how an FSM can serve as the central control\nlogic in a computer.  At the same time, we will illustrate connections\nbetween FSMs and software and will make some connections with other\nareas of interest in ECE, such as the design and analysis of digital \ncontrol systems. \n Question: \nQ. What is the boundary between hardware and software? \n Answer: \n\nA. The boundary between hardware and software is the point at which instructions are executed by the CPU."}, {"text": "Title: Finite State Machines P7 \n Text: The table below gives a { list of abstract states} for a typical \nkeyless entry system for a car.  In this case, we have merely named the\nstates rather than specifying the bit patterns to be used for each\nstate---for this reason, we refer to them as abstract states.\nThe description of the states in the first column is an optional element\noften included in the early design stages for an FSM, when identifying\nthe states needed for the design.\nA list may also include the outputs for each state.  Again, in\nthe list below, we have specified these outputs abstractly. \n Question: \nQ. What are the tradeoffs between bit-sliced and serial designs? \n Answer: \n\nA. The tradeoffs between bit-sliced and serial designs include area, performance, and power."}, {"text": "Title: Finite State Machines P8 \n Text: By including outputs for each state,\nwe implicitly assume that outputs depend only on\nthe state of the FSM.  We discuss this assumption in more detail\nlater in these notes (see ``Machine Models''), \nbut will make the assumption throughout our class. \n Question: \nQ. Why might someone be interested in learning about the history of the\nQ. English language?\nQ. \nQ. Someone might be interested in learning about the history of the English language in order to better understand the origins of the language and how it has evolved over time. \n Answer:  Additionally, this information can help individuals better understand the relationship between English and other languages, as well as the influences that have shaped English throughout its history."}, {"text": "Title: Finite State Machines P9 \n Text: meaning& state& driver's door& other doors& alarm on \nvehicle locked& LOCKED& locked& locked& no\ndriver door unlocked& DRIVER& unlocked& locked& no\nall doors unlocked& UNLOCKED& unlocked& unlocked& no\nalarm sounding& ALARM& locked& locked& yes \n Question:  \n Answer: \n1. What is the main advantage of using bit-sliced logic?\n\nThe main advantage of using bit-sliced logic is that it is easier to design and implement complex digital circuits. With bit-sliced logic, each bit of the circuit is handled by a separate logic block, which can be designed and tested independently. This modular approach makes it much easier to develop and debug complex digital circuits."}, {"text": "Title: Finite State Machines P10 \n Text: Another tool used with FSMs is the { next-state table} (sometimes\ncalled a { state transition table}, or just a { state table}), \nwhich maps the current state and input combination into the next state of \nthe FSM.  The abstract variant shown below outlines desired behavior at\na high level, and is often ambiguous, incomplete, and even inconsistent.\nFor example, what happens if a user pushes two buttons?  What happens\nif they push unlock while the alarm is sounding?  These questions \nshould eventually be considered.  However, we can already start to see \nthe intended use of the design: starting from a locked car, a user\ncan push ``unlock'' once to gain entry to the driver's seat, or push\n``unlock'' twice to open the car fully for passengers.  To lock the\ncar, a user can push the ``lock'' button at any time.  And, if a user\nneeds help, pressing the ``panic'' button sets off an alarm.  \n Question: \nQ. What is the main advantage of using a serial design over a bit-sliced design?\nQ. \nQ. The main advantage of using a serial design over a bit-sliced design is that it requires less area. \n Answer: "}, {"text": "Title: Finite State Machines P11 \n Text: state& action/input& next state \nLOCKED& push ``unlock''& DRIVER\nDRIVER& push ``unlock''& UNLOCKED\n(any)& push ``lock''& LOCKED\n(any)& push ``panic''& ALARM \n Question: \nQ. How does the general bit-sliced design approach work? \n Answer: \n\nA. The general bit-sliced design approach works by breaking down a signal into its individual bits, and then processing each bit independently. This allows for parallel processing of the signal, which can lead to faster overall processing time."}, {"text": "Title: Finite State Machines P12 \n Text: \nA { state transition diagram} (or { transition diagram}, or\n{ state diagram}),\nas shown to the right, illustrates the\ncontents of the next-state table graphically, with each state drawn\nin a circle, and arcs between states labeled with the input combinations\nthat cause these transitions from one state to another. \n Question: \nQ. How many bit slices are connected in sequence? \n Answer: \n\nA. The number of bit slices connected in sequence is determined by the number of bits in the operand input."}, {"text": "Title: Finite State Machines P13 \n Text: Putting the FSM design into this graphical form does not solve the\nproblems with the abstract model.  The questions that we asked\nin regard to the next-state table remain unanswered. \n Question: \nQ. What is the purpose of the first bit slice?\nQ. \nQ. The first bit slice is used to initialize the value of the bits that will be used in the calculation. \n Answer: "}, {"text": "Title: Finite State Machines P14 \n Text: Implementing an FSM using digital logic requires that we translate\nthe design into bits, eliminate any ambiguity, and complete the\nspecification.  How many internal bits should we use?  What are the\npossible input values, and how are their meanings represented in bits? \nWhat are the possible output values, and how are their meanings\nrepresented in bits?  We will consider these questions for several\nexamples in the coming weeks. \n Question: \nQ. How does the serial design with a single copy of the bit slice logic work? \n Answer: \n\nA. The serial design with a single copy of the bit slice logic works by applying an input signal to the bit slice logic in the first cycle, and then storing the output of the bit slice logic in a flip-flop. In the second cycle, the bit slice logic receives the output of the flip-flop as an input, and the output of the bit slice logic is again stored in the flip-flop. This process is repeated for each bit in the input signal."}, {"text": "Title: Finite State Machines P15 \n Text: For now, we simply define answers for our example design, the keyless\nentry system.  Given four states, we need at \nleast _2(4)=2 bits of internal state, which we\nstore in two flip-flops and call S_1S_0.  The table below lists input\nand output signals and defines their meaning.  \n Question: \nQ. What is the purpose of the extra gate for each of the M inputs? \n Answer: \n\nA. The extra gate is used to select the input for the bit slice."}, {"text": "Title: Finite State Machines P16 \n Text: outputs&D& driver door; 1 means unlocked\n&R& other doors (remaining doors); 1 means unlocked\n&A& alarm; 1 means alarm is sounding\ninputs& U& unlock button; 1 means it has been pressed\n& L& lock button; 1 means it has been pressed\n& P& panic button; 1 means it has been pressed \n Question: \nQ. What is the function of the bit slice in this example?\nQ. \nQ. The bit slice in this example is used to compare two numbers. \n Answer: "}, {"text": "Title: Finite State Machines P17 \n Text: We can now choose a representation for our states and rewrite the list\nof states, using bits both for the states and for the outputs.  We\nalso include the meaning of each state for clarity in our example.\nNote that we can choose the internal representation in any way.  Here\nwe have matched the D and R outputs when possible to simplify\nthe output logic needed for the implementation.  The order of states\nin the list is not particularly important, but should be chosen for\nconvenience and clarity (including transcribing bits into to {K-maps}, \nfor example). \n Question: \nQ. How does the output logic place the result of the comparison on the Z_1 and Z_0 outputs? \n Answer: \n\nA. The output logic places the result of the comparison according to the bit slice encoding."}, {"text": "Title: Finite State Machines P18 \n Text: & & & driver's door& other doors& alarm on\nmeaning& state& S_1S_0& D& R& A \nvehicle locked& LOCKED& 00& 0& 0& 0\ndriver door unlocked& DRIVER& 10& 1& 0& 0\nall doors unlocked& UNLOCKED& 11& 1& 1& 0\nalarm sounding& ALARM& 01& 0& 0& 1 \n Question: \nQ. What is the main difference between the serial and bit-sliced designs? \n Answer: \n\nA. The main difference between the serial and bit-sliced designs is that the former uses a single serial data stream while the latter uses multiple parallel data streams."}, {"text": "Title: Finite State Machines P19 \n Text: We can also rewrite the next-state table in terms of bits.  We use\nGray code order on both axes, as these orders make it more \nconvenient to use {K-maps}.  The values represented in this table \nare the next FSM state given the current state S_1S_0 and the\ninputs U, L, and P.  Our symbols for the next-state bits\nare S_1^+ and S_0^+.  The ``+'' superscript\nis a common way of expressing\nthe next value in a discrete series, here induced by the use of\nclocked synchronous logic in implementing the FSM.  In other words,\nS_1^+ is the value of S_1 in the next clock cycle, and S_1^+\nin an FSM implemented as a digital system is a Boolean expression\nbased on the current state and the inputs.  For our example problem, we\nwant to be able to write down expressions for S_1^+(S_1,S_0,U,L,P) \nand S_1^+(S_1,S_0,U,L,P), as well as expressions for the \noutput logic D(S_1,S_0), R(S_1,S_0), and A(S_1,S_0).  \n Question: \nQ. What does the author mean by \"counting gates\"? \n Answer: \n\nA. The author means that they are using the number of gates as an estimate of the area of the design."}, {"text": "Title: Finite State Machines P20 \n Text: \n{\n{c|cccccccc}\ncurrent state & {|c}{ULP}\nS_1S_0& 000& 001& 011& 010& 110& 111& 101& 100 \n00& 00& 01& 01& 00& 00& 01& 01& 10\n01& 01& 01& 01& 00& 00& 01& 01& 01\n11& 11& 01& 01& 00& 00& 01& 01& 11\n10& 10& 01& 01& 00& 00& 01& 01& 11  \n Question: \nQ. What is the total number of gates required for a 32-bit slice?\nQ. \nQ. A 32-bit slice would require 72 gates. \n Answer: "}, {"text": "Title: Finite State Machines P21 \n Text: \nIn the process of writing out the next-state table, \nwe have made decisions for all of the questions that we asked earlier\nregarding the abstract state table.  These decisions are also reflected\nin the complete state transition diagram shown to the right.\nThe states have been extended with state bits and output bits,\nas S_1S_0/DRA.\nYou should recognize that we can\nalso leave some questions unanswered by placing x's (don't cares)\ninto our table.  However, you should also understand at this point\nthat any implementation will produce bits, not x's, so we must be\ncareful not to allow arbitrary choices unless any of the choices\nallowed is indeed acceptable for our FSM's purpose.  We will discuss\nthis process and the considerations necessary as we cover more FSM\ndesign examples. \n Question: \nQ. What are the two inputs of a flip-flop? \n Answer: \n\nA. The two inputs of a flip-flop are the data input and the clock signal."}, {"text": "Title: Finite State Machines P22 \n Text: We have deliberately omitted calculation of expressions for the\nnext-state variables S_1^+ and S_0^+,\nand for the outputs D,\nR, and A.  We expect that you are able to do so from the \ndetailed state table above, and may assign such an exercise as\npart of your homework. \n Question: \nQ. What does the text say about inverters?\nQ. \nQ. The text says that inverters are used to handle any number of bits. \n Answer: "}, {"text": "Title: Synchronous Counters P0 \n Text: A { counter} is a clocked sequential circuit with a state diagram\nconsisting of a single logical cycle. \n Question: \nQ. What is the third part of our class about?\nQ. \nQ. The third part of our class is about building upon the basic combinational and sequential logic elements that we developed in the second part. \n Answer: "}, {"text": "Title: Synchronous Counters P1 \n Text: Not all counters are synchronous.\nIn other words, not all flip-flops in a counter are required to use \nthe same clock signal.  A counter in which all flip-flops do \nutilize the same clock\nsignal is called a { synchronous counter}.   \n Question: \nQ. How can stored state be used to improve performance?\nQ. \nQ. One way to use stored state to improve performance is to trade between area and performance. This can be done by storing data in a way that uses less space, or by storing data in a way that is faster to access. \n Answer: "}, {"text": "Title: Synchronous Counters P2 \n Text: Except for a brief introduction to other types of counters in the\nnext section, our class focuses entirely on clocked synchronous designs,\nincluding counters. \n Question: \nQ. How does the Finite State Machine (FSM) help formalize and reason about digital systems? \n Answer: \n\nThe FSM provides a well-defined model for digital systems that can be used for formal analysis and reasoning. This abstractions helps to identify and track the states of the system, and to reason about the behavior of the system."}, {"text": "Title: Synchronous Counters P3 \n Text:  {The definition ignores transition\n arcs resulting from functionalit such as a reset signal that\n forces the counter back into an initial state,  \n Question: \nQ. Why are general FSM models broadly applicable in a range of engineering contexts? \n Answer: \n\nA. The general FSM model is a simple and elegant way to describe the behavior of systems. It is easy to understand and can be applied to a wide range of engineering contexts."}, {"text": "Title: Synchronous Counters P4 \n Text: \nThe design of synchronous counter circuits is a fairly straightforward\nexercise given the desired cycle of output patterns.  \n Question: \nQ. What are some design issues that need to be considered when developing a FSM? \n Answer: \n\nSome design issues that need to be considered when developing a FSM include:\n-Ensuring that the FSM is correctly specified\n-Ensuring that the FSM is correctly implemented\n-Ensuring that the FSM is correctly tested\n-Ensuring that the FSM is correctly integrated into the overall system"}, {"text": "Title: Synchronous Counters P5 \n Text: The task can be more complex if the internal state bits\nare allowed to differ from the output bits, so for now\nwe assume that output Z_i is equal to internal bit S_i.\nNote that distinction between internal states and outputs is necessary \nif any output pattern appears more than once in the desired cycle.  \n Question: \nQ. How does the idea of memory relate to stored state? \n Answer: \n\nA. Memory is a form of stored state."}, {"text": "Title: Synchronous Counters P6 \n Text: The cycle of states shown to the right corresponds to the\nstates of a {3-bit} binary counter.  The numbers in the states\nrepresent both internal state bits S_2S_1S_0 and output bits Z_2Z_1Z_0.\nWe transcribe this diagram into the next-state table shown on the left below.\nWe then write out {K-maps} for\nthe next state bits S_2^+, S_1^+, and S_0^+, as shown to the right,\nand use the {K-maps} to find expressions for these variables in\nterms of the current state. \n Question: \nQ. What is the boundary between hardware and software? \n Answer: \n\nA. The boundary between hardware and software is the point at which instructions are executed by the CPU."}, {"text": "Title: Synchronous Counters P7 \n Text: {ccc|ccc}\nS_2& S_1& S_0& S_2^+& S_1^+& S_0^+ \n0& 0& 0& 0& 0& 1\n0& 0& 1& 0& 1& 0\n0& 1& 0& 0& 1& 1\n0& 1& 1& 1& 0& 0\n1& 0& 0& 1& 0& 1\n1& 0& 1& 1& 1& 0\n1& 1& 0& 1& 1& 1\n1& 1& 1& 0& 0& 0 \n Question: \nQ. What are the tradeoffs between bit-sliced and serial designs? \n Answer: \n\nA. The tradeoffs between bit-sliced and serial designs include area, performance, and power."}, {"text": "Title: Synchronous Counters P8 \n Text:  {-10pt}\n{eqnarray*}\nS_2^+ =& {S_2}S_1S_0+S_2{S_1}+S_2{S_0} &= S_2(S_1S_0) \nS_1^+ =& S_1{S_0}+{S_1}S_0 &= S_1{S_0} \nS_0^+ =& {S_0} &= S_0 \n{eqnarray*} \n Question: \nQ. Why might someone be interested in learning about the history of the\nQ. English language?\nQ. \nQ. Someone might be interested in learning about the history of the English language in order to better understand the origins of the language and how it has evolved over time. \n Answer:  Additionally, this information can help individuals better understand the relationship between English and other languages, as well as the influences that have shaped English throughout its history."}, {"text": "Title: Synchronous Counters P9 \n Text: The first form of the expression for each next-state variable is\ntaken directly from the corresponding {K-map}.  We have rewritten\neach expression to make the emerging pattern more obvious. \n Question:  \n Answer: \n1. What is the main advantage of using bit-sliced logic?\n\nThe main advantage of using bit-sliced logic is that it is easier to design and implement complex digital circuits. With bit-sliced logic, each bit of the circuit is handled by a separate logic block, which can be designed and tested independently. This modular approach makes it much easier to develop and debug complex digital circuits."}, {"text": "Title: Synchronous Counters P10 \n Text: We can also derive the pattern intuitively by asking the following: \n Question: \nQ. What is the main advantage of using a serial design over a bit-sliced design?\nQ. \nQ. The main advantage of using a serial design over a bit-sliced design is that it requires less area. \n Answer: "}, {"text": "Title: Synchronous Counters P11 \n Text: given a binary counter in state S_{N-1}S_{N-2} S_{j+1}S_jS_{j-1}\nS_1S_0, when does S_j change in the subsequent state?  The answer,\nof course, is that S_j changes when all of the\nbits below S_j are 1.  Otherwise, S_j remains the\nsame in the next state.  We thus \nwrite S_j^+=S_j (S_{j-1} S_1S_0) and\nimplement the counter as shown below for a {4-bit} design.\nNote that the \nusual order of\noutput bits along the bottom is reversed in the figure, with the most\nsignificant bit at the right rather than the left. \n Question: \nQ. How does the general bit-sliced design approach work? \n Answer: \n\nA. The general bit-sliced design approach works by breaking down a signal into its individual bits, and then processing each bit independently. This allows for parallel processing of the signal, which can lead to faster overall processing time."}, {"text": "Title: Synchronous Counters P12 \n Text: The calculation of the left inputs to the XOR gates in the counter \nshown above is performed with a series of two-input AND gates.  Each \nof these gates AND's\nanother flip-flop value into the product.  This\napproach, called { serial gating}, implies that an {N-bit}\ncounter requires more than {N-2} gate delays\nto settle into the next state.  An alternative approach, called\n{ parallel gating}, calculates each input independently with a\nsingle logic gate, as shown below.  The blue inputs to the AND gate\nfor S_3 highlight the difference from the previous figure (note that\nthe two approaches differ only for bits S_3 and above).\nWith parallel gating, the \n{ fan-in} of the gates (the number of inputs)\nand the { fan-out} of the flip-flop outputs (number of other gates\ninto which an output feeds) grow with the size of the counter.  \nIn practice, large counters use a combination of these two approaches. \n Question: \nQ. How many bit slices are connected in sequence? \n Answer: \n\nA. The number of bit slices connected in sequence is determined by the number of bits in the operand input."}, {"text": "Title: Ripple Counters P0 \n Text: A second class of\ncounter drives some of its flip-flops with a clock signal and feeds\nflip-flop outputs into the clock inputs of its remaining flip-flops,\npossibly through additional logic.  Such a counter is called a {\nripple counter}, because the effect of a clock edge ripples through\nthe flip-flops.  The delay inherent to the ripple effect, along with\nthe complexity of ensuring that timing issues do not render the design\nunreliable, are the major drawbacks of ripple counters.  Compared with\nsynchronous counters, however, ripple counters consume less energy,\nand are sometimes used for devices with restricted energy supplies. \n Question: \nQ. What is the third part of our class about?\nQ. \nQ. The third part of our class is about building upon the basic combinational and sequential logic elements that we developed in the second part. \n Answer: "}, {"text": "Title: Ripple Counters P1 \n Text: \nGeneral ripple counters\ncan be tricky because of timing issues, but certain types are easy. \n Question: \nQ. How can stored state be used to improve performance?\nQ. \nQ. One way to use stored state to improve performance is to trade between area and performance. This can be done by storing data in a way that uses less space, or by storing data in a way that is faster to access. \n Answer: "}, {"text": "Title: Ripple Counters P2 \n Text: Consider the design of binary ripple counter.  The state diagram for \na {3-bit} binary counter is replicated to the right.\nLooking\nat the states, notice that the least-significant bit alternates with\neach state, while higher bits flip whenever the next smaller bit (to\nthe right) transitions from one to zero.  To take advantage of these\nproperties, we use positive edge-triggered D flip-flops with\ntheir complemented () outputs wired back to their inputs.\nThe clock input is fed only into the first\nflip-flop, and the complemented output of each flip-flop is also\nconnected to the clock of the next. \n Question: \nQ. How does the Finite State Machine (FSM) help formalize and reason about digital systems? \n Answer: \n\nThe FSM provides a well-defined model for digital systems that can be used for formal analysis and reasoning. This abstractions helps to identify and track the states of the system, and to reason about the behavior of the system."}, {"text": "Title: Ripple Counters P3 \n Text: \nAn implementation of a {4-bit} binary ripple counter appears to the\nright.\nThe order of bits in the figure matches the order used for our synchronous\nbinary counters: least significant on the left, most significant on the\nright.\nAs you can see from the figure, the technique generalizes to arbitrarily \nlarge binary ripple coun- \n Question: \nQ. Why are general FSM models broadly applicable in a range of engineering contexts? \n Answer: \n\nA. The general FSM model is a simple and elegant way to describe the behavior of systems. It is easy to understand and can be applied to a wide range of engineering contexts."}, {"text": "Title: Ripple Counters P4 \n Text: ters, but the\ntime required for the outputs to settle after a clock edge scales with\nthe number of flip-flops in the counter.  On the other hand, an\naverage of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n), which reduces the power requirements.{Recall that\nflip-flops record the clock state internally.  The logical activity\nrequired to record such state consumes energy.} \n Question: \nQ. What are some design issues that need to be considered when developing a FSM? \n Answer: \n\nSome design issues that need to be considered when developing a FSM include:\n-Ensuring that the FSM is correctly specified\n-Ensuring that the FSM is correctly implemented\n-Ensuring that the FSM is correctly tested\n-Ensuring that the FSM is correctly integrated into the overall system"}, {"text": "Title: Ripple Counters P5 \n Text: Beginning with the state 0000, at the rising clock edge, the left (S_0)\nflip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\nfalling clock edge and does nothing, leaving the counter in\nstate 0001.  When the next rising clock edge arrives, the left\nflip-flop toggles back to 0, which the second flip-flop sees as a\nrising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\nsees the second flip-flop's change as a falling edge and does nothing,\nand the state settles as 0010.  We leave verification of the remainder \nof the cycle as an exercise. \n Question: \nQ. How does the idea of memory relate to stored state? \n Answer: \n\nA. Memory is a form of stored state."}, {"text": "Title: Timing Issues* P0 \n Text: Ripple counters are a form of a more general strategy known as clock\ngating.{Fall 2012 students: This part may seem a little\nredundant, but we're going to remove the earlier mention of clock \ngating in future semesters.}\n{ Clock gating} uses logic to control the visibility of a\nclock signal to flip-flops (or latches).  Historically, digital system\ndesigners rarely used clock gating techniques because of the \ncomplexity introduced for the circuit designers, who must ensure \nthat clock edges are\ndelivered with little skew along a dynamically changing set of\npaths to flip-flops.  Today, however, the power benefits of hiding\nthe clock signal from flip-flops have made clock gating an attractive \nstrategy.\nNevertheless, digital logic designers and computer architects still almost\nnever use clock gating strategies directly.  In most of the industry,\nCAD tools insert logic for clock gating automatically.  \nA handful of companies (such as Intel and Apple/Samsung) design \ncustom circuits rather than relying on CAD tools to synthesize \nhardware designs from standard libraries of elements.\nIn these companies, clock gating is used widely by the circuit\ndesign teams, and some input is occasionally necessary from the \nhigher-level designers. \n Question: \nQ. What is the third part of our class about?\nQ. \nQ. The third part of our class is about building upon the basic combinational and sequential logic elements that we developed in the second part. \n Answer: "}, {"text": "Title: Timing Issues* P1 \n Text: More aggressive gating strategies are also used in modern designs, but\nthese usually require more time to transition between the on and off \nstates and can be more\ndifficult to get right automatically (with the tools), hence\nhardware designers may need to provide high-level information about\ntheir designs.  A flip-flop that does not see any change in its clock\ninput still has connections to high voltage and ground, and thus allows\na small amount of { leakage current}.  In contrast,\nwith { power gating}, the voltage difference is removed, and the \ncircuit uses no power at all.  Power gating can be tricky---as you know,\nfor example, when you turn the power on, you need to make sure that\neach latch settles into a stable state.  Latches may need to be \ninitialized to guarantee that they settle, which requires time after\nthe power is restored. \n Question: \nQ. How can stored state be used to improve performance?\nQ. \nQ. One way to use stored state to improve performance is to trade between area and performance. This can be done by storing data in a way that uses less space, or by storing data in a way that is faster to access. \n Answer: "}, {"text": "Title: Timing Issues* P2 \n Text: If you want a deeper understanding of gating issues, take ECE482, \nDigital Integrated Circuit Design, or ECE527, System-on-a-Chip Design. \n Question: \nQ. How does the Finite State Machine (FSM) help formalize and reason about digital systems? \n Answer: \n\nThe FSM provides a well-defined model for digital systems that can be used for formal analysis and reasoning. This abstractions helps to identify and track the states of the system, and to reason about the behavior of the system."}, {"text": "Title: Machine Models P0 \n Text: Before we dive fully into FSM design, we must point out that we have\nplaced a somewhat artificial restriction on the types of FSMs that\nwe use in our course. \n Question: \nQ. What is the third part of our class about?\nQ. \nQ. The third part of our class is about building upon the basic combinational and sequential logic elements that we developed in the second part. \n Answer: "}, {"text": "Title: Machine Models P1 \n Text: Historically, this restriction was given a name, and machines of the type\nthat we have discussed are called Moore machines. \n Question: \nQ. How can stored state be used to improve performance?\nQ. \nQ. One way to use stored state to improve performance is to trade between area and performance. This can be done by storing data in a way that uses less space, or by storing data in a way that is faster to access. \n Answer: "}, {"text": "Title: Machine Models P2 \n Text: However, outside of introductory classes, almost no one cares about\nthis name, nor about the name for the more general model used almost\nuniversally in hardware design, Mealy machines. \n Question: \nQ. How does the Finite State Machine (FSM) help formalize and reason about digital systems? \n Answer: \n\nThe FSM provides a well-defined model for digital systems that can be used for formal analysis and reasoning. This abstractions helps to identify and track the states of the system, and to reason about the behavior of the system."}, {"text": "Title: Machine Models P3 \n Text: What is the difference?  In a { Moore machine}, outputs depend only on\nthe internal state bits of the FSM (the values stored in the flip-flops).\nIn a { Mealy machine}, outputs\nmay be expressed as functions both of internal state and FSM inputs. \n Question: \nQ. Why are general FSM models broadly applicable in a range of engineering contexts? \n Answer: \n\nA. The general FSM model is a simple and elegant way to describe the behavior of systems. It is easy to understand and can be applied to a wide range of engineering contexts."}, {"text": "Title: Machine Models P4 \n Text: As we illustrate shortly, the benefit of using input signals to calculate\noutputs (the Mealy machine model) is that input bits effectively serve \nas additional system state, which means that the number of internal \nstate bits can be reduced. \n Question: \nQ. What are some design issues that need to be considered when developing a FSM? \n Answer: \n\nSome design issues that need to be considered when developing a FSM include:\n-Ensuring that the FSM is correctly specified\n-Ensuring that the FSM is correctly implemented\n-Ensuring that the FSM is correctly tested\n-Ensuring that the FSM is correctly integrated into the overall system"}, {"text": "Title: Machine Models P5 \n Text: The disadvantage of including input signals in the expressions for \noutput signals is that timing characteristics of input signals may not\nbe known, whereas an FSM designer may want to guarantee certain\ntiming characteristics for output signals. \n Question: \nQ. How does the idea of memory relate to stored state? \n Answer: \n\nA. Memory is a form of stored state."}, {"text": "Title: Machine Models P6 \n Text: In practice, when such timing guarantees are needed, the designer simply\nadds state to the FSM to accommodate the need, and the problem is solved. \n Question: \nQ. What is the boundary between hardware and software? \n Answer: \n\nA. The boundary between hardware and software is the point at which instructions are executed by the CPU."}, {"text": "Title: Machine Models P7 \n Text: The coin-counting FSM that we designed for our class' lab assignments,\nfor example, \nrequired that we use a Moore machine model to avoid sending the\nservo controlling the coin's path an output pulse that was too short\nto enforce the FSM's decision about which way to send the coin. \n Question: \nQ. What are the tradeoffs between bit-sliced and serial designs? \n Answer: \n\nA. The tradeoffs between bit-sliced and serial designs include area, performance, and power."}, {"text": "Title: Machine Models P8 \n Text: By adding more states to the FSM, we were able to hold the servo in\nplace, as desired. \n Question: \nQ. Why might someone be interested in learning about the history of the\nQ. English language?\nQ. \nQ. Someone might be interested in learning about the history of the English language in order to better understand the origins of the language and how it has evolved over time. \n Answer:  Additionally, this information can help individuals better understand the relationship between English and other languages, as well as the influences that have shaped English throughout its history."}, {"text": "Title: Machine Models P9 \n Text: Why are we protecting you from the model used in practice? \n Question:  \n Answer: \n1. What is the main advantage of using bit-sliced logic?\n\nThe main advantage of using bit-sliced logic is that it is easier to design and implement complex digital circuits. With bit-sliced logic, each bit of the circuit is handled by a separate logic block, which can be designed and tested independently. This modular approach makes it much easier to develop and debug complex digital circuits."}, {"text": "Title: Machine Models P10 \n Text: First, timing issues add complexity to a topic that is complex enough \nfor an introductory course. \n Question: \nQ. What is the main advantage of using a serial design over a bit-sliced design?\nQ. \nQ. The main advantage of using a serial design over a bit-sliced design is that it requires less area. \n Answer: "}, {"text": "Title: Machine Models P11 \n Text: And, second, most software FSMs are Moore machines, so the abstraction\nis a useful one in that context, too. \n Question: \nQ. How does the general bit-sliced design approach work? \n Answer: \n\nA. The general bit-sliced design approach works by breaking down a signal into its individual bits, and then processing each bit independently. This allows for parallel processing of the signal, which can lead to faster overall processing time."}, {"text": "Title: Machine Models P12 \n Text: In many design contexts, the timing issues implied by a Mealy model\ncan be relatively simple to manage.  When working in a single clock\ndomain, all of the input signals come from flip-flops in the same \ndomain, and are thus stable for most of the clock cycle.  Only rarely\ndoes one need to keep additional state to improve timing characteristics\nin these contexts.  In contrast, when interacting across clock domains,\nmore care is sometimes needed to ensure correct behavior. \n Question: \nQ. How many bit slices are connected in sequence? \n Answer: \n\nA. The number of bit slices connected in sequence is determined by the number of bits in the operand input."}, {"text": "Title: Machine Models P13 \n Text: We now illustrate the state reduction benefit of the Mealy machine\nmodel with a simple example, an FSM that recognizes the \npattern of a 0 followed by a 1 on a single input and outputs\na 1 when it observes the pattern. \n Question: \nQ. What is the purpose of the first bit slice?\nQ. \nQ. The first bit slice is used to initialize the value of the bits that will be used in the calculation. \n Answer: "}, {"text": "Title: Machine Models P14 \n Text: As already mentioned,\nMealy machines often require fewer flip-flops.\nIntuitively, the number of combinations of states and\ninputs is greater than the number of combinations of states alone, and\nallowing a function to depend on inputs reduces the number of internal\nstates needed.   \n Question: \nQ. How does the serial design with a single copy of the bit slice logic work? \n Answer: \n\nA. The serial design with a single copy of the bit slice logic works by applying an input signal to the bit slice logic in the first cycle, and then storing the output of the bit slice logic in a flip-flop. In the second cycle, the bit slice logic receives the output of the flip-flop as an input, and the output of the bit slice logic is again stored in the flip-flop. This process is repeated for each bit in the input signal."}, {"text": "Title: Machine Models P15 \n Text: A Mealy implementation of the FSM appears on the left below, and\nan example timing diagram illustrating the FSM's behavior is shown on\nthe right. \n Question: \nQ. What is the purpose of the extra gate for each of the M inputs? \n Answer: \n\nA. The extra gate is used to select the input for the bit slice."}, {"text": "Title: Machine Models P16 \n Text: The machine shown below occupies state A when the last bit seen was a 0, and\nstate B when the last bit seen was a 1. \n Question: \nQ. What is the function of the bit slice in this example?\nQ. \nQ. The bit slice in this example is used to compare two numbers. \n Answer: "}, {"text": "Title: Machine Models P17 \n Text: Notice that the transition arcs in the state diagram\nare labeled with two values instead\nof one.  Since outputs can depend on input values as well as state,\ntransitions in a Mealy machine are labeled with input/output\ncombinations, while states are labeled only with their internal bits\n(or just their names, as shown below).  Labeling states with outputs\ndoes not make sense for a Mealy machine, since outputs may vary\nwith inputs. \n Question: \nQ. How does the output logic place the result of the comparison on the Z_1 and Z_0 outputs? \n Answer: \n\nA. The output logic places the result of the comparison according to the bit slice encoding."}, {"text": "Title: Machine Models P18 \n Text: Notice that the outputs indicated on any given transition\nhold only until that transition is taken (at the rising clock edge), as is\napparent in the timing diagram.  When inputs are asynchronous, \nthat is,\nnot driven by the same clock signal, output pulses from a Mealy\nmachine can be arbitrarily short, which can lead to problems. \n Question: \nQ. What is the main difference between the serial and bit-sliced designs? \n Answer: \n\nA. The main difference between the serial and bit-sliced designs is that the former uses a single serial data stream while the latter uses multiple parallel data streams."}, {"text": "Title: Machine Models P19 \n Text: For a Moore machine, we must create a special state in which the\noutput is high.  Doing so requires that we split state B into two\nstates, a state C in which the last two bits seen were 01, and a\nstate D in which the last two bits seen were 11.  Only state C\ngenerates output 1.  State D also becomes the starting state for the\nnew state machine.  The state diagram on the left below illustrates \nthe changes, using the transition diagram style that we introduced \nearlier to represent Moore machines.\nNotice in the associated timing diagram that the output pulse lasts a\nfull clock cycle. \n Question: \nQ. What does the author mean by \"counting gates\"? \n Answer: \n\nA. The author means that they are using the number of gates as an estimate of the area of the design."}, {"text": "Title: Finite State Machine Design Examples, Part I P0 \n Text: This set of notes uses a series of examples to illustrate design principles \nfor the implementation of finite state machines (FSMs) using digital logic.\nWe begin with an overview of the design process for a digital FSM, from\nthe development of an abstract model through the implementation of\nfunctions for the next-state variables and output signals.\nOur first few examples cover only the concrete aspects:\nwe implement several counters, which illustrate the basic \nprocess of translating a concrete and complete state transition diagram\ninto an implementation based on flip-flops and logic gates.\nWe next consider a counter with a number of states that is not a power of\ntwo, with which we illustrate the need for FSM initialization.\n As part of solving the initialization problem, we also introduce \n a general form of selection logic called a multiplexer. \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Finite State Machine Design Examples, Part I P1 \n Text: We then consider the design process as a whole through a more general\nexample of a counter with multiple inputs to control its behavior. \nWe work from\nan abstract model down to an implementation, illustrating how semantic\nknowledge from the abstract model can be used to simplify the \nimplementation.  Finally, we illustrate how the choice of representation\nfor the FSM's internal state affects the complexity of the implementation.\nFortunately, designs that are more intuitive and easier for humans to\nunderstand also typically make the best designs in terms of \nother metrics, such as logic complexity. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Steps in the Design Process P0 \n Text: Before we begin exploring designs, let's talk briefly about the general\napproach that we take when designing an FSM.  We follow a six-step\nprocess:{-8pt} \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Steps in the Design Process P1 \n Text: {{}{}\n{}{}{}\n{develop an abstract model}{step-abs}\n{specify I/O behavior}{step-io}\n{complete the specification}{step-complete}\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n}\n{-8pt} \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Steps in the Design Process P2 \n Text: In Step {step-abs}, we translate our description in human language\ninto a model with states and desired behavior.  At this stage, we \nsimply try to capture the intent of the description and are not\nparticularly thorough nor exact. \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Steps in the Design Process P3 \n Text: Step {step-io} begins to formalize the model, starting with its\ninput and output behavior.  If we eventually plan to develop an\nimplementation of our FSM as a digital system (which is not the \nonly choice, of course!), all input and output\nmust consist of bits.  Often, input and/or output specifications\nmay need to match other digital systems to which we plan to connect\nour FSM.  In fact, { most problems in developing large digital systems\ntoday arise because of incompatibilities when composing two or more\nseparately designed pieces} (or { modules}) into an integrated system. \n Question: \nQ. What is the order of the steps for designing a sequential circuit? \n Answer: \n\n1) Specifying I/O behavior\n2) Developing an abstract model\n3) Completing the specification\n4) Choosing a state representation\n5) Calculating logic expressions\n6) Implementing with flip-flops and gates"}, {"text": "Title: Steps in the Design Process P4 \n Text: Once we know the I/O behavior for our FSM, in Step {step-complete}\nwe start to make\nany implicit assumptions clear and to make any other decisions\nnecessary to the design.  Occasionally, we may choose to leave\nsomething undecided in the hope of simplifying the design with\n``don't care'' entries in the logic formulation. \n Question: \nQ. What is the author's purpose for writing this text?\nQ. \nQ. The author's purpose for writing this text is to explain how to generate a model from a description in human language. \n Answer: "}, {"text": "Title: Steps in the Design Process P5 \n Text: In Step {step-repn}, we select an internal representation\nfor the bits necessary to encode the state of our FSM.  In practice,\nfor small designs, this representation can be selected by a computer \nin such a way as to optimize the implementation.  However, for large\ndesigns, such as the LC-3 instruction set architecture that we\nstudy later in this class, humans do most of the work by hand. \n Question: \nQ. What are some of the possible consequences of designing digital systems that are incompatible with each other? \n Answer: \n\nA. Possible consequences of designing digital systems that are incompatible with each other include:\n\n- Inability to use one system with another\n- Difficulty in integrating the systems\n- Loss of data\n- System crashes"}, {"text": "Title: Steps in the Design Process P6 \n Text: In the later examples in this set of notes, we show how even a \nsmall design can\nleverage meaningful information from the design when selecting\nthe representation, leading to an implementation that is simpler\nand is easier to build correctly. \n Question: \nQ. What is the purpose of making implicit assumptions clear in Step 2 of the FSM design process?\nQ. \nQ. One purpose of making implicit assumptions clear in Step 2 of the FSM design process is to avoid making incorrect assumptions that could lead to a design error. Additionally, making assumptions explicit can help to simplify the design process by allowing for easier identification and modification of assumptions as needed. \n Answer: "}, {"text": "Title: Steps in the Design Process P7 \n Text: We also show how one can\nuse abstraction to simplify an implementation. \n Question: \nQ. What is the purpose of an internal representation? \n Answer: \n\nA. The purpose of an internal representation is to provide a way to encode the state of an FSM so that it can be easily implemented."}, {"text": "Title: Steps in the Design Process P8 \n Text: By Step {step-logic}, our design is a complete specification in\nterms of bits, and we need merely derive logic expressions for the\nnext-state variables and the output signals.  This process is no\ndifferent than for combinational logic, and should already be fairly \nfamiliar to you. \n Question: \nQ. What is the purpose of the design? \n Answer: \n\nA. The purpose of the design is to create a system that can accurately and efficiently predict the results of horse races."}, {"text": "Title: Steps in the Design Process P9 \n Text: Finally, in Step {step-gates}, we translate our logic expressions\ninto gates and use flip-flops (or registers) to hold the internal\nstate bits of the FSM.  In later notes, we use more complex\nbuilding blocks when implementing an FSM, building up abstractions\nin order to simplify the design process in much the same way that\nwe have shown for combinational logic. \n Question: \nQ. What is the purpose of abstraction?\nQ. \nQ. The purpose of abstraction is to simplify an implementation. \n Answer: "}, {"text": "Title: Example: A Two-Bit Gray Code Counter P0 \n Text: Let's begin with a two-bit Gray code counter with no inputs.\nAs we mentioned in Notes Set 2.1, a Gray code is a cycle over all\nbit patterns of a certain length in which consecutive patterns differ\nin exactly one bit. \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Example: A Two-Bit Gray Code Counter P1 \n Text: For simplicity, our first few examples are based on counters and\nuse the internal state\nof the FSM as the output values.  You should already know\nhow to design combinational logic for the outputs if it were necessary. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Example: A Two-Bit Gray Code Counter P2 \n Text: The inputs to a counter, if any, are typically limited to functions\nsuch as starting and stopping the counter, controlling the counting \ndirection, and resetting the counter to a particular state. \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Example: A Two-Bit Gray Code Counter P3 \n Text: A fully-specified transition diagram for \na two-bit Gray code counter appears below.\nWith no inputs, the states simply form a loop, with\nthe counter moving from one state to the next each cycle. \n Question: \nQ. What is the order of the steps for designing a sequential circuit? \n Answer: \n\n1) Specifying I/O behavior\n2) Developing an abstract model\n3) Completing the specification\n4) Choosing a state representation\n5) Calculating logic expressions\n6) Implementing with flip-flops and gates"}, {"text": "Title: Example: A Two-Bit Gray Code Counter P4 \n Text: Each state in the diagram is marked with the internal state value S_1S_0 \n(before the ``/'') and the output Z_1Z_0 (after the ``/''), which are \nalways equal for this counter. \n Question: \nQ. What is the author's purpose for writing this text?\nQ. \nQ. The author's purpose for writing this text is to explain how to generate a model from a description in human language. \n Answer: "}, {"text": "Title: Example: A Two-Bit Gray Code Counter P5 \n Text: Based on the transition diagram, we can fill in the K-maps for the \nnext-state values S_1^+ and S_0^+ as shown to the right of the\ntransition diagram, then \nderive algebraic expressions in the usual way to obtain\nS_1^+=S_0 and S_0^+={{S_1}}. \n Question: \nQ. What are some of the possible consequences of designing digital systems that are incompatible with each other? \n Answer: \n\nA. Possible consequences of designing digital systems that are incompatible with each other include:\n\n- Inability to use one system with another\n- Difficulty in integrating the systems\n- Loss of data\n- System crashes"}, {"text": "Title: Example: A Two-Bit Gray Code Counter P6 \n Text: We then use the next-state logic to develop the implementation\nshown on the far right, completing our first counter design. \n Question: \nQ. What is the purpose of making implicit assumptions clear in Step 2 of the FSM design process?\nQ. \nQ. One purpose of making implicit assumptions clear in Step 2 of the FSM design process is to avoid making incorrect assumptions that could lead to a design error. Additionally, making assumptions explicit can help to simplify the design process by allowing for easier identification and modification of assumptions as needed. \n Answer: "}, {"text": "Title: Example: A Three-Bit Gray Code Counter P0 \n Text: \nNow we'll add a third bit to our counter, but again use a Gray code\nas the basis for the state sequence. \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Example: A Three-Bit Gray Code Counter P1 \n Text: A fully-specified transition diagram for such a counter appears to \nthe right.  As before, with no inputs, the states simply form a loop, \nwith the counter moving from one state to the next each cycle. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Example: A Three-Bit Gray Code Counter P2 \n Text: Each state in the diagram is marked with the internal state value S_2S_1S_0 \n(before ``/'') and the output Z_2Z_1Z_0 (after ``/'').  \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Example: A Three-Bit Gray Code Counter P3 \n Text: \nBased on the transition diagram, we can fill in the K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then \nderive algebraic expressions.  The results are more complex this \ntime. \n Question: \nQ. What is the order of the steps for designing a sequential circuit? \n Answer: \n\n1) Specifying I/O behavior\n2) Developing an abstract model\n3) Completing the specification\n4) Choosing a state representation\n5) Calculating logic expressions\n6) Implementing with flip-flops and gates"}, {"text": "Title: Example: A Three-Bit Gray Code Counter P4 \n Text: For our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_0 + S_1 {{S_0}} \nS_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}} \nS_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1\n{eqnarray*} \n Question: \nQ. What is the author's purpose for writing this text?\nQ. \nQ. The author's purpose for writing this text is to explain how to generate a model from a description in human language. \n Answer: "}, {"text": "Title: Example: A Three-Bit Gray Code Counter P5 \n Text: \nNotice that the equations for S_2^+ and S_1^+ share a common term,\nS_1{{S_0}}. \n Question: \nQ. What are some of the possible consequences of designing digital systems that are incompatible with each other? \n Answer: \n\nA. Possible consequences of designing digital systems that are incompatible with each other include:\n\n- Inability to use one system with another\n- Difficulty in integrating the systems\n- Loss of data\n- System crashes"}, {"text": "Title: Example: A Three-Bit Gray Code Counter P6 \n Text: This design does not allow much choice in developing good equations for\nthe next-state logic, but some designs may enable you to reduce \nthe design complexity by explicitly identifying and making use of \ncommon algebraic terms and sub-expressions for different outputs.\nIn modern design processes, identifying such opportunities is generally\nperformed by a computer program, but it's important to understand\nhow they arise.  Note that the common term becomes a single AND gate\nin the implementation of our counter, as shown to the right. \n Question: \nQ. What is the purpose of making implicit assumptions clear in Step 2 of the FSM design process?\nQ. \nQ. One purpose of making implicit assumptions clear in Step 2 of the FSM design process is to avoid making incorrect assumptions that could lead to a design error. Additionally, making assumptions explicit can help to simplify the design process by allowing for easier identification and modification of assumptions as needed. \n Answer: "}, {"text": "Title: Example: A Three-Bit Gray Code Counter P7 \n Text: Looking at the counter's implementation diagram, notice that the vertical\nlines carrying the current state values and their inverses back to the\nnext state\nlogic inputs have been carefully ordered to simplify\nunderstanding the diagram.  In particular, they are ordered from\nleft to right (on the left side of the figure) as \n{{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.\nWhen designing any logic diagram, be sure to make use of a reasonable\norder so as to make it easy for someone (including yourself!) to read \nand check the correctness of the logic. \n Question: \nQ. What is the purpose of an internal representation? \n Answer: \n\nA. The purpose of an internal representation is to provide a way to encode the state of an FSM so that it can be easily implemented."}, {"text": "Title: Example: A Color Sequencer P0 \n Text: \nEarly graphics systems used a three-bit red-green-blue (RGB) \nencoding for colors.  The color mapping for such a system is shown to\nthe right. \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Example: A Color Sequencer P1 \n Text: Imagine that you are charged with creating a counter to drive a light\nthrough a sequence of colors.  The light takes an RGB input as just\ndescribed, and the desired pattern is \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Example: A Color Sequencer P2 \n Text: {off (black)     yellow     violet     green     blue} \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Example: A Color Sequencer P3 \n Text: You immediately recognize that you merely need a counter with five\nstates.  How many flip-flops will we need?  At least three, since\n_2 (5)=3.  Given that we need three flip-flops, \nand that the colors we need to produce as \n Question: \nQ. What is the order of the steps for designing a sequential circuit? \n Answer: \n\n1) Specifying I/O behavior\n2) Developing an abstract model\n3) Completing the specification\n4) Choosing a state representation\n5) Calculating logic expressions\n6) Implementing with flip-flops and gates"}, {"text": "Title: Example: A Color Sequencer P4 \n Text: \n{c|l}\nRGB& color \n000& black\n001& blue\n010& green\n011& cyan\n100& red\n101& violet\n110& yellow\n111& white \n Question: \nQ. What is the author's purpose for writing this text?\nQ. \nQ. The author's purpose for writing this text is to explain how to generate a model from a description in human language. \n Answer: "}, {"text": "Title: Example: A Color Sequencer P5 \n Text: outputs are all unique\nbit patterns, we can again choose to use the counter's internal \nstate directly as our output values. \n Question: \nQ. What are some of the possible consequences of designing digital systems that are incompatible with each other? \n Answer: \n\nA. Possible consequences of designing digital systems that are incompatible with each other include:\n\n- Inability to use one system with another\n- Difficulty in integrating the systems\n- Loss of data\n- System crashes"}, {"text": "Title: Example: A Color Sequencer P6 \n Text: \nA fully-specified transition diagram for our color sequencer\nappears to the right.  The states again form a loop,\nand are marked with the internal state value S_2S_1S_0 \nand the output RGB. \n Question: \nQ. What is the purpose of making implicit assumptions clear in Step 2 of the FSM design process?\nQ. \nQ. One purpose of making implicit assumptions clear in Step 2 of the FSM design process is to avoid making incorrect assumptions that could lead to a design error. Additionally, making assumptions explicit can help to simplify the design process by allowing for easier identification and modification of assumptions as needed. \n Answer: "}, {"text": "Title: Example: A Color Sequencer P7 \n Text: \nAs before, we can use the transition diagram to fill in K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+, as shown to the right.\nFor each of the three states not included in our transition diagram,\nwe have inserted x's \n Question: \nQ. What is the purpose of an internal representation? \n Answer: \n\nA. The purpose of an internal representation is to provide a way to encode the state of an FSM so that it can be easily implemented."}, {"text": "Title: Example: A Color Sequencer P8 \n Text: into the K-maps to indicate ``don't care.'' \nAs you know, we can treat each x as either a 0 or a 1, whichever\nproduces better results (where ``better'' usually means simpler \nequations).  The terms that we have chosen for our algebraic \nequations are illustrated in the K-maps.  The x's within the ellipses\nbecome 1s in the implementation, and the x's outside of the ellipses\nbecome 0s. \n Question: \nQ. What is the purpose of the design? \n Answer: \n\nA. The purpose of the design is to create a system that can accurately and efficiently predict the results of horse races."}, {"text": "Title: Example: A Color Sequencer P9 \n Text: \nFor our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}} \nS_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}} \nS_0^+ &=& S_1\n{eqnarray*} \n Question: \nQ. What is the purpose of abstraction?\nQ. \nQ. The purpose of abstraction is to simplify an implementation. \n Answer: "}, {"text": "Title: Example: A Color Sequencer P10 \n Text: Again our equations for S_2^+ and S_1^+ share a common term,\nwhich becomes a single AND gate in the implementation shown to the\nright. \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Sequential logic has feedback from the output back to the input, while combinational logic does not."}, {"text": "Title: Identifying an Initial State P0 \n Text: Let's say that you go the lab and build the implementation above, \nhook it up\nto the light, and turn it on.  Does it work?  Sometimes.\nSometimes it works perfectly, but sometimes\nthe light glows cyan or red briefly first.\nAt other times, the light is an\nunchanging white. \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Identifying an Initial State P1 \n Text: \nWhat could be going wrong? \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Identifying an Initial State P2 \n Text: Let's try to understand.  We begin by deriving\nK-maps for the implementation, as shown to the right.  In these\nK-maps, each of the x's in our design has been replaced by either a 0\nor a 1.  These entries are highlighted with green italics. \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Identifying an Initial State P3 \n Text: Now let's imagine what might happen if somehow our FSM got into the\nS_2S_1S_0=111 state.  In such a state, the light would appear white,\nsince RGB=S_2S_1S_0=111. \n Question: \nQ. What is the order of the steps for designing a sequential circuit? \n Answer: \n\n1) Specifying I/O behavior\n2) Developing an abstract model\n3) Completing the specification\n4) Choosing a state representation\n5) Calculating logic expressions\n6) Implementing with flip-flops and gates"}, {"text": "Title: Identifying an Initial State P4 \n Text: What happens in the next cycle? \n Question: \nQ. What is the author's purpose for writing this text?\nQ. \nQ. The author's purpose for writing this text is to explain how to generate a model from a description in human language. \n Answer: "}, {"text": "Title: Identifying an Initial State P5 \n Text: Plugging into the equations or looking into the K-maps gives (of\ncourse) the same answer: the next state is the\nS_2^+S_1^+S_0^+=111 state.\nIn other words, the light stays white indefinitely! \n Question: \nQ. What are some of the possible consequences of designing digital systems that are incompatible with each other? \n Answer: \n\nA. Possible consequences of designing digital systems that are incompatible with each other include:\n\n- Inability to use one system with another\n- Difficulty in integrating the systems\n- Loss of data\n- System crashes"}, {"text": "Title: Identifying an Initial State P6 \n Text: As an exercise, you should check what happens \nif the light is red or cyan. \n Question: \nQ. What is the purpose of making implicit assumptions clear in Step 2 of the FSM design process?\nQ. \nQ. One purpose of making implicit assumptions clear in Step 2 of the FSM design process is to avoid making incorrect assumptions that could lead to a design error. Additionally, making assumptions explicit can help to simplify the design process by allowing for easier identification and modification of assumptions as needed. \n Answer: "}, {"text": "Title: Identifying an Initial State P7 \n Text: We can extend the transition diagram that we developed for our design\nwith the extra states possible in the implementation, as shown below.\nAs with the five states in the design, the extra states are named with\nthe color of light that they produce. \n Question: \nQ. What is the purpose of an internal representation? \n Answer: \n\nA. The purpose of an internal representation is to provide a way to encode the state of an FSM so that it can be easily implemented."}, {"text": "Title: Identifying an Initial State P8 \n Text: Notice that the FSM does not move out of the WHITE state (ever).   \n Question: \nQ. What is the purpose of the design? \n Answer: \n\nA. The purpose of the design is to create a system that can accurately and efficiently predict the results of horse races."}, {"text": "Title: Identifying an Initial State P9 \n Text: You may at this point wonder whether more careful decisions \nin selecting our next-state expressions might address this issue.\nTo some extent, yes.  For example, if we replace the \nS_2S_1 term in the equation for S_2^+ with S_2{{S_0}}, \na decision allowed\nby the ``don't care'' boxes in the K-map for our design,\nthe resulting transition diagram does not suffer from the problem\nthat we've found. \n Question: \nQ. What is the purpose of abstraction?\nQ. \nQ. The purpose of abstraction is to simplify an implementation. \n Answer: "}, {"text": "Title: Identifying an Initial State P10 \n Text: However, even if we do change our implementation slightly, we need\nto address another aspect of the problem: \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Sequential logic has feedback from the output back to the input, while combinational logic does not."}, {"text": "Title: Identifying an Initial State P11 \n Text: how can the FSM ever get into the unexpected states? \n Question: \nQ. What is the purpose of the flip-flops in an FSM?\nQ. \nQ. The flip-flops are used to hold the internal state bits of the FSM. \n Answer: "}, {"text": "Title: Identifying an Initial State P12 \n Text: \nWhat is the initial state of the three flip-flops in our implementation? \n Question: \nQ. -What is a Gray code?\nQ. -What is a two-bit Gray code counter?\nQ. -What is the cycle over all bit patterns of a certain length? \n Answer: "}, {"text": "Title: Identifying an Initial State P13 \n Text: { The initial state may not even be 0s and 1s unless we have an \nexplicit mechanism for initialization.}  \n Question: \nQ. A finite-state machine (FSM) is a model of computation used to design both computer hardware and software.\nQ. \nQ. What is a model of computation? \n Answer: \n\nA model of computation is a mathematical model that specifies how a system should behave. In the case of a finite-state machine, it specifies how the machine should transition between states in response to input."}, {"text": "Title: Identifying an Initial State P14 \n Text: Initialization can work in two ways.   \n Question: \nQ. What is the purpose of a counter?\nQ. \nQ. A counter is a device that is used to keep track of something, such as the number of times an event occurs. \n Answer: "}, {"text": "Title: Identifying an Initial State P15 \n Text: The first approach makes use of the flip-flop design.\nAs you know, a flip-flop is built from a pair of latches, and\nwe can \nmake use of the internal reset lines on these latches\nto force each flip-flop into the 0 state (or the 1 state) using an\nadditional input.  \n Question: \nQ. What does the counter do when it reaches the last state in the diagram?\nQ. \nQ. It goes back to the first state. \n Answer: "}, {"text": "Title: Identifying an Initial State P16 \n Text: Alternatively, we can add some extra logic to our design. \n Question: \nQ. What is the state diagram for a two-bit synchronous binary counter? \n Answer: \n\nA. The state diagram for a two-bit synchronous binary counter is a loop that goes from 00 to 11 and then back to 00 again."}, {"text": "Title: Identifying an Initial State P17 \n Text: Consider adding a few AND gates and a  input\n(active low), as shown in the dashed box in the figure to the right.\nIn this case, when we assert  by setting it to 0,\nthe FSM moves to state 000 in the next cycle, putting it into\nthe BLACK state.  The approach taken here is for clarity; one can\noptimize the design, if desired.  For example, we could simply connect\n as an extra input into the three AND gates on the\nleft rather than adding new ones, with the same effect. \n Question: \nQ. What does the transition diagram represent? \n Answer: \n\nA. The transition diagram represents a sequential logic circuit with two flip-flops."}, {"text": "Title: Identifying an Initial State P18 \n Text: We may sometimes want a more powerful initialization mechanism---one\nthat allows us to force the FSM into any specific state in the next\ncycle.  In such a case, we can add multiplexers to each of our \nflip-flop inputs, allowing us to use the INIT input to choose between\nnormal operation (INIT=0) of the FSM and forcing the FSM into the\nnext state given by I_2I_1I_0 (when INIT=1). \n Question: \nQ. What is the difference between the next-state logic and the implementation?\nQ. \nQ. The next-state logic is the underlying structure of the design, while the implementation is the completed design. \n Answer: "}, {"text": "Title: Developing an Abstract Model P0 \n Text: \nWe are now ready to discuss the design process for an FSM from start\nto finish. \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Developing an Abstract Model P1 \n Text: For this first abstract FSM example, we build upon something\nthat we have already seen: a two-bit Gray code counter.\nWe now want a counter that allows us to start and stop the \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Developing an Abstract Model P2 \n Text: \n{c|ccc}\nstate&    no input&  halt button& go button \ncounting& counting&      halted& \nhalted&   halted&              & counting \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Developing an Abstract Model P3 \n Text: What is the mechanism for stopping and starting?  To\nbegin our design, we could sketch out an abstract next-state\ntable such as the one shown to the right above.  In this form of the table,\nthe first column lists the states, while each of the other columns lists\nstates to which the FSM transitions after a clock cycle for a particular\ninput combination.  \n Question: \nQ. What is the order of the steps for designing a sequential circuit? \n Answer: \n\n1) Specifying I/O behavior\n2) Developing an abstract model\n3) Completing the specification\n4) Choosing a state representation\n5) Calculating logic expressions\n6) Implementing with flip-flops and gates"}, {"text": "Title: Developing an Abstract Model P4 \n Text: The table contains two states, counting and halted, and specifies\nthat the design uses two distinct buttons to move between the\nstates.\nThe table further implies that if the counter is halted,\nthe ``halt'' button has no additional effect, and if the counter\nis counting, the ``go'' button has no additional effect. \n Question: \nQ. What is the author's purpose for writing this text?\nQ. \nQ. The author's purpose for writing this text is to explain how to generate a model from a description in human language. \n Answer: "}, {"text": "Title: Developing an Abstract Model P5 \n Text: \nA counter with a single counting state, of course, does not provide\nmuch value.  We extend the table with four counting states and four\nhalted states, as shown to the right.  This version of the\ntable also introduces more formal state names, for which these notes \nuse all capital letters. \n Question: \nQ. What are some of the possible consequences of designing digital systems that are incompatible with each other? \n Answer: \n\nA. Possible consequences of designing digital systems that are incompatible with each other include:\n\n- Inability to use one system with another\n- Difficulty in integrating the systems\n- Loss of data\n- System crashes"}, {"text": "Title: Developing an Abstract Model P6 \n Text: The upper four states represent uninterrupted counting, in which \nthe counter cycles through these states indefinitely. \n Question: \nQ. What is the purpose of making implicit assumptions clear in Step 2 of the FSM design process?\nQ. \nQ. One purpose of making implicit assumptions clear in Step 2 of the FSM design process is to avoid making incorrect assumptions that could lead to a design error. Additionally, making assumptions explicit can help to simplify the design process by allowing for easier identification and modification of assumptions as needed. \n Answer: "}, {"text": "Title: Developing an Abstract Model P7 \n Text: A user can stop the counter in any state by pressing the ``halt''\nbutton, causing the counter to retain its current value until the\nuser presses the ``go'' button. \n Question: \nQ. What is the purpose of an internal representation? \n Answer: \n\nA. The purpose of an internal representation is to provide a way to encode the state of an FSM so that it can be easily implemented."}, {"text": "Title: Developing an Abstract Model P8 \n Text: Below the state table is an abstract transition diagram, which provides\nexactly the same information in graphical form.  Here circles represent\nstates (as labeled) and arcs represent transitions from one state\nto another based on an input combination (which is used to label the\narc). \n Question: \nQ. What is the purpose of the design? \n Answer: \n\nA. The purpose of the design is to create a system that can accurately and efficiently predict the results of horse races."}, {"text": "Title: Developing an Abstract Model P9 \n Text: We have already implicitly made a few choices about our counter design. \n Question: \nQ. What is the purpose of abstraction?\nQ. \nQ. The purpose of abstraction is to simplify an implementation. \n Answer: "}, {"text": "Title: Developing an Abstract Model P10 \n Text: \n{c|ccc}\nstate&    no input&  halt button& go button \n{ COUNT A}& { COUNT B}& { HALT A}& \n{ COUNT B}& { COUNT C}& { HALT B}& \n{ COUNT C}& { COUNT D}& { HALT C}& \n{ COUNT D}& { COUNT A}& { HALT D}& \n{ HALT A}&  { HALT A}&              & { COUNT B}\n{ HALT B}&  { HALT B}&              & { COUNT C}\n{ HALT C}&  { HALT C}&              & { COUNT D}\n{ HALT D}&  { HALT D}&              & { COUNT A} \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Sequential logic has feedback from the output back to the input, while combinational logic does not."}, {"text": "Title: Developing an Abstract Model P11 \n Text: \nshown retains the current state of the system when\n``halt'' is pressed.\nWe could instead reset the counter state whenever it\nis restarted, in which case we need only five states: four for\ncounting and one more for a halted counter. \n Question: \nQ. What is the purpose of the flip-flops in an FSM?\nQ. \nQ. The flip-flops are used to hold the internal state bits of the FSM. \n Answer: "}, {"text": "Title: Developing an Abstract Model P12 \n Text: Second, we've designed the counter to stop\nwhen the user presses ``halt'' and to resume counting \nwhen the user presses ``go.''  We could instead choose to delay these \neffects by a cycle.  For example, pressing ``halt'' in state { COUNT B}\ncould take the counter to state { HALT C}, and pressing ``go'' \nin state { HALT C} could take the system to state { COUNT C}. \n Question: \nQ. -What is a Gray code?\nQ. -What is a two-bit Gray code counter?\nQ. -What is the cycle over all bit patterns of a certain length? \n Answer: "}, {"text": "Title: Developing an Abstract Model P13 \n Text: In these notes, we implement only the diagrams shown. \n Question: \nQ. A finite-state machine (FSM) is a model of computation used to design both computer hardware and software.\nQ. \nQ. What is a model of computation? \n Answer: \n\nA model of computation is a mathematical model that specifies how a system should behave. In the case of a finite-state machine, it specifies how the machine should transition between states in response to input."}, {"text": "Title: Specifying I/O Behavior P0 \n Text: \nWe next start to formalize our design by specifying its input and \noutput behavior digitally.  Each of the two control buttons provides\na single bit of input.  The ``halt'' button we call H, and the\n``go'' button we call G. \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Specifying I/O Behavior P1 \n Text: For the output, we use a two-bit \nGray code.  With these choices, we can redraw the transition diagram \nas show to the right. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Specifying I/O Behavior P2 \n Text: In this figure, the states are marked with output values Z_1Z_0 and\ntransition arcs are labeled in terms of our two input buttons, G and H.  \nThe uninterrupted counting cycle is labeled with \nto indicate that it continues until we press H. \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Completing the Specification P0 \n Text: Now we need to think about how the system should behave if something \noutside of our initial expectations occurs.  Having drawn out a partial\ntransition diagram can help with this process, since we can use the\ndiagram to systematically consider all possible input conditions from\nall possible states.  The state table form can make the missing\nparts of the specification even more obvious. \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Completing the Specification P1 \n Text: \nFor our counter, the symmetry between counting states makes the problem \nsubstantially simpler.  Let's write out part of a list of states and\npart of a state table with one \ncounting state and one halt state, as shown to the right.\nFour values of the inputs HG \nare possible (recall that N bits allow 2^N possible patterns).\nWe list the columns in Gray code order, since we may want to\ntranscribe this table into K-maps later. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Completing the Specification P2 \n Text: & \nfirst counting state& { COUNT A}& counting, output Z_1Z_0=00\n  first halted state&  { HALT A}& halted, output Z_1Z_0=00 \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Completing the Specification P3 \n Text: \n{c|cccc}\n&{HG}\n        state&            00&            01&          11&           10 \n{ COUNT A}& { COUNT B}&   unspecified& unspecified& { HALT A}\n { HALT A}&  { HALT A}& { COUNT B}& unspecified&  unspecified \n Question: \nQ. What is the order of the steps for designing a sequential circuit? \n Answer: \n\n1) Specifying I/O behavior\n2) Developing an abstract model\n3) Completing the specification\n4) Choosing a state representation\n5) Calculating logic expressions\n6) Implementing with flip-flops and gates"}, {"text": "Title: Completing the Specification P4 \n Text: Let's start with the { COUNT A} state.   \n Question: \nQ. What is the author's purpose for writing this text?\nQ. \nQ. The author's purpose for writing this text is to explain how to generate a model from a description in human language. \n Answer: "}, {"text": "Title: Completing the Specification P5 \n Text: We know that if neither button is pressed (HG=00), we want \nthe counter to move to the { COUNT B} state.  And, if we press the\n``halt'' button (HG=10), we want the counter to move to the { HALT A}\nstate.  What should happen if a user presses the ``go'' button (HG=01)?\nOr if the user presses both buttons (HG=11)? \n Question: \nQ. What are some of the possible consequences of designing digital systems that are incompatible with each other? \n Answer: \n\nA. Possible consequences of designing digital systems that are incompatible with each other include:\n\n- Inability to use one system with another\n- Difficulty in integrating the systems\n- Loss of data\n- System crashes"}, {"text": "Title: Completing the Specification P6 \n Text: Answering these questions is part of fully specifying our design.  We\ncan choose to leave some parts unspecified, but { any implementation of\nour system will imply answers}, and thus we must be careful. \n Question: \nQ. What is the purpose of making implicit assumptions clear in Step 2 of the FSM design process?\nQ. \nQ. One purpose of making implicit assumptions clear in Step 2 of the FSM design process is to avoid making incorrect assumptions that could lead to a design error. Additionally, making assumptions explicit can help to simplify the design process by allowing for easier identification and modification of assumptions as needed. \n Answer: "}, {"text": "Title: Completing the Specification P7 \n Text: We choose to ignore the ``go'' button while counting, and to have the\n``halt'' button override the ``go'' button.  Thus, if HG=01 when the\ncounter is in state { COUNT A}, the counter moves to state { COUNT B}.\nAnd, if HG=11, the counter moves to state { HALT A}. \n Question: \nQ. What is the purpose of an internal representation? \n Answer: \n\nA. The purpose of an internal representation is to provide a way to encode the state of an FSM so that it can be easily implemented."}, {"text": "Title: Completing the Specification P8 \n Text: Use of explicit bit patterns for the inputs HG may help you to check \nthat all four possible input values are covered from each state.  If \nyou choose to use a transition diagram instead of a state table,\nyou might even want to add four arcs from each state, each labeled \nwith a specific\nvalue of HG.  When two arcs connect the same two states, we can either \nuse multiple labels or can indicate bits that do not matter using a\n{ don't-care} symbol, x.  For example, the arc from state { COUNT A}\nto state { COUNT B} could be labeled HG=00,01 or HG=0x.  The\narc from state { COUNT A} to state { HALT A} could be labeled\nHG=10,11 or HG=1x.  We can also use logical expressions as labels,\nbut such notation can obscure unspecified transitions. \n Question: \nQ. What is the purpose of the design? \n Answer: \n\nA. The purpose of the design is to create a system that can accurately and efficiently predict the results of horse races."}, {"text": "Title: Completing the Specification P9 \n Text: Now consider the state { HALT A}.  The transitions specified so far\nare that when we press ``go'' (HG=01), the counter moves to \nthe { COUNT B} state, and that the counter remains halted in \nstate { HALT A} if no buttons are pressed (HG=00).\nWhat if the ``halt'' button is pressed (HG=10), or\nboth buttons are pressed (HG=11)?  For consistency, we decide that\n``halt'' overrides ``go,'' but does nothing special if it alone is pressed\nwhile the counter is halted.  Thus, input patterns HG=10 and HG=11 also \ntake state { HALT A} back to itself.\nHere the arc could be labeled HG=00,10,11 or, equivalently,\nHG=00,1x or HG=x0,11. \n Question: \nQ. What is the purpose of abstraction?\nQ. \nQ. The purpose of abstraction is to simplify an implementation. \n Answer: "}, {"text": "Title: Completing the Specification P10 \n Text: \nTo complete our design, we apply the same decisions that we made for \nthe { COUNT A} state to all of the other counting states, and the \ndecisions that we made for the { HALT A} state to all of the other \nhalted states.  If we had chosen not to specify an answer, an implementation\ncould produce different behavior from the different counting\nand/or halted states, which might confuse a user. \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Sequential logic has feedback from the output back to the input, while combinational logic does not."}, {"text": "Title: Completing the Specification P11 \n Text: The resulting design appears to the right. \n Question: \nQ. What is the purpose of the flip-flops in an FSM?\nQ. \nQ. The flip-flops are used to hold the internal state bits of the FSM. \n Answer: "}, {"text": "Title: Choosing a State Representation P0 \n Text: Now we need to select a representation for the states.  Since our counter\nhas eight states, we need at least three (_2 (8)=3)\nstate bits S_2S_1S_0 to keep track of the current state. \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Choosing a State Representation P1 \n Text: As we show later, { the choice of representation for an FSM's states\ncan dramatically affect the design complexity}.  For a design as simple as \nour counter, you could just let a computer implement all possible \nrepresentations (there aren't more than 840, if we consider simple \nsymmetries) and select one according to whatever metrics are interesting. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Choosing a State Representation P2 \n Text: For bigger designs, however, the number of possibilities quickly becomes\nimpossible to explore completely. \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Choosing a State Representation P3 \n Text: Fortunately, { use of abstraction in selecting a representation \nalso tends to produce better designs} for a wide variety of metrics\n(such as design complexity, area, power consumption, and performance). \n Question: \nQ. What is the order of the steps for designing a sequential circuit? \n Answer: \n\n1) Specifying I/O behavior\n2) Developing an abstract model\n3) Completing the specification\n4) Choosing a state representation\n5) Calculating logic expressions\n6) Implementing with flip-flops and gates"}, {"text": "Title: Choosing a State Representation P4 \n Text: The right strategy is thus often to start by selecting a representation \nthat makes sense to a human, even if it requires more bits than are\nstrictly necessary.  The\nresulting implementation will be easier to\ndesign and to debug than an implementation in which only the global \nbehavior has any meaning. \n Question: \nQ. What is the author's purpose for writing this text?\nQ. \nQ. The author's purpose for writing this text is to explain how to generate a model from a description in human language. \n Answer: "}, {"text": "Title: Choosing a State Representation P5 \n Text: \nLet's return to our specific example, the counter.  We can use one bit, \nS_2, to record whether or not our counter is counting (S_2=0) or\nhalted (S_2=1).  The other two bits can then record the counter state\nin terms of the desired output.  Choosing this representation\nimplies that only wires will be necessary to compute outputs Z_1 \nand Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting\ndesign, in which states are now labeled with both internal state and\noutputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version,\nwe have changed the arc labeling to use logical expressions, which\ncan sometimes help us to think about the implementation. \n Question: \nQ. What are some of the possible consequences of designing digital systems that are incompatible with each other? \n Answer: \n\nA. Possible consequences of designing digital systems that are incompatible with each other include:\n\n- Inability to use one system with another\n- Difficulty in integrating the systems\n- Loss of data\n- System crashes"}, {"text": "Title: Choosing a State Representation P6 \n Text: The equivalent state listing and state table appear below.  We have ordered\nthe rows of the state table in Gray code order to simplify transcription\nof K-maps. \n Question: \nQ. What is the purpose of making implicit assumptions clear in Step 2 of the FSM design process?\nQ. \nQ. One purpose of making implicit assumptions clear in Step 2 of the FSM design process is to avoid making incorrect assumptions that could lead to a design error. Additionally, making assumptions explicit can help to simplify the design process by allowing for easier identification and modification of assumptions as needed. \n Answer: "}, {"text": "Title: Choosing a State Representation P7 \n Text: & S_2S_1S_0& \n{ COUNT A}& 000& counting, output Z_1Z_0=00\n{ COUNT B}& 001& counting, output Z_1Z_0=01\n{ COUNT C}& 011& counting, output Z_1Z_0=11\n{ COUNT D}& 010& counting, output Z_1Z_0=10\n { HALT A}& 100& halted, output Z_1Z_0=00\n { HALT B}& 101& halted, output Z_1Z_0=01\n { HALT C}& 111& halted, output Z_1Z_0=11\n { HALT D}& 110& halted, output Z_1Z_0=10 \n Question: \nQ. What is the purpose of an internal representation? \n Answer: \n\nA. The purpose of an internal representation is to provide a way to encode the state of an FSM so that it can be easily implemented."}, {"text": "Title: Choosing a State Representation P8 \n Text: \n{rc|cccc}\n&&{HG}\n&S_2S_1S_0& 00& 01& 11& 10 \n{ COUNT A}&000& 001& 001& 100& 100\n{ COUNT B}&001& 011& 011& 101& 101\n{ COUNT C}&011& 010& 010& 111& 111\n{ COUNT D}&010& 000& 000& 110& 110\n { HALT D}&110& 110& 000& 110& 110\n { HALT C}&111& 111& 010& 111& 111\n { HALT B}&101& 101& 011& 101& 101\n { HALT A}&100& 100& 001& 100& 100 \n Question: \nQ. What is the purpose of the design? \n Answer: \n\nA. The purpose of the design is to create a system that can accurately and efficiently predict the results of horse races."}, {"text": "Title: Choosing a State Representation P9 \n Text: Having chosen a representation, we can go ahead and implement our\ndesign in the usual way.  As shown to the right, K-maps for the \nnext-state logic are complicated, since we have five variables\nand must consider implicants that are not contiguous in the K-maps.\nThe S_2^+ logic is easy enough: we only need two terms, \nas shown. \n Question: \nQ. What is the purpose of abstraction?\nQ. \nQ. The purpose of abstraction is to simplify an implementation. \n Answer: "}, {"text": "Title: Choosing a State Representation P10 \n Text: Notice that we have used color and\nline style to distinguish different \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Sequential logic has feedback from the output back to the input, while combinational logic does not."}, {"text": "Title: Choosing a State Representation P11 \n Text: implicants in the K-maps.  Furthermore, the symmetry of the design\nproduces symmetry in the S_1^+ and S_0^+ formula, so we have\nused the same color and line style for analogous terms in these\ntwo K-maps. \n Question: \nQ. What is the purpose of the flip-flops in an FSM?\nQ. \nQ. The flip-flops are used to hold the internal state bits of the FSM. \n Answer: "}, {"text": "Title: Choosing a State Representation P12 \n Text: For S_1^+, we need four terms.  The green \nellipses in the HG=01 column are part of the same term, as are\nthe two halves of the dashed blue circle.  In S_0^+, we still\nneed four terms, but three of them are split into two pieces \nin the K-map.  As you can see, the utility of the K-map is starting\nto break down with five variables. \n Question: \nQ. -What is a Gray code?\nQ. -What is a two-bit Gray code counter?\nQ. -What is the cycle over all bit patterns of a certain length? \n Answer: "}, {"text": "Title: Abstracting Design Symmetries P0 \n Text: Rather than implementing the design as two-level logic, let's try to\ntake advantage of our design's symmetry to further simplify the\nlogic (we reduce gate count at the expense of longer, slower paths). \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Abstracting Design Symmetries P1 \n Text: Looking back to the last transition diagram, in which the arcs\nwere labeled with logical expressions, let's calculate an expression\nfor when the counter should retain its current value in the next\ncycle.  We call \nthis variable HOLD.  In the counting states, when S_2=0, \nthe counter stops (moves into a halted state without changing value) \nwhen H is true.\nIn the halted states, when S_2=1, the counter stops (stays in \na halted state) when H+ is true.  We can thus write \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Abstracting Design Symmetries P2 \n Text: {eqnarray*}\nHOLD &=& {S_2}  H + S_2  ( H +  )\nHOLD &=& {S_2} H + S_2 H + S_2 \nHOLD &=& H + S_2 \n{eqnarray*} \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Abstracting Design Symmetries P3 \n Text: In other words, the counter should hold its current \nvalue (stop counting) if we press the ``halt'' button or if the counter\nwas already halted and we didn't press the ``go'' button.  As desired,\nthe current value of the counter (S_1S_0) has no impact on this \ndecision.  You may have noticed that the expression we derived for\nHOLD also matches S_2^+, the next-state value of S_2 in the \nK-map on the previous page. \n Question: \nQ. What is the order of the steps for designing a sequential circuit? \n Answer: \n\n1) Specifying I/O behavior\n2) Developing an abstract model\n3) Completing the specification\n4) Choosing a state representation\n5) Calculating logic expressions\n6) Implementing with flip-flops and gates"}, {"text": "Title: Abstracting Design Symmetries P4 \n Text: Now let's re-write our state transition table in terms of HOLD.  The\nleft version uses state names for clarity; the right uses state values\nto help us transcribe K-maps. \n Question: \nQ. What is the author's purpose for writing this text?\nQ. \nQ. The author's purpose for writing this text is to explain how to generate a model from a description in human language. \n Answer: "}, {"text": "Title: Abstracting Design Symmetries P5 \n Text: {\n{rc|cc}\n&&{HOLD}\n&S_2S_1S_0& 0& 1 \n{ COUNT A}&000& { COUNT B}& { HALT A}\n{ COUNT B}&001& { COUNT C}& { HALT B}\n{ COUNT C}&011& { COUNT D}& { HALT C}\n{ COUNT D}&010& { COUNT A}& { HALT D}\n { HALT A}&100& { COUNT B}& { HALT A}\n { HALT B}&101& { COUNT C}& { HALT B}\n { HALT C}&111& { COUNT D}& { HALT C}\n { HALT D}&110& { COUNT A}& { HALT D} \n Question: \nQ. What are some of the possible consequences of designing digital systems that are incompatible with each other? \n Answer: \n\nA. Possible consequences of designing digital systems that are incompatible with each other include:\n\n- Inability to use one system with another\n- Difficulty in integrating the systems\n- Loss of data\n- System crashes"}, {"text": "Title: Abstracting Design Symmetries P6 \n Text: {rc|cc}\n&&{HOLD}\n&S_2S_1S_0& 0& 1 \n{ COUNT A}&000& 001& 100\n{ COUNT B}&001& 011& 101\n{ COUNT C}&011& 010& 111\n{ COUNT D}&010& 000& 110\n { HALT A}&100& 001& 100\n { HALT B}&101& 011& 101\n { HALT C}&111& 010& 111\n { HALT D}&110& 000& 110 \n Question: \nQ. What is the purpose of making implicit assumptions clear in Step 2 of the FSM design process?\nQ. \nQ. One purpose of making implicit assumptions clear in Step 2 of the FSM design process is to avoid making incorrect assumptions that could lead to a design error. Additionally, making assumptions explicit can help to simplify the design process by allowing for easier identification and modification of assumptions as needed. \n Answer: "}, {"text": "Title: Abstracting Design Symmetries P7 \n Text: The K-maps based on the HOLD abstraction are shown to the right.\nAs you can see, the necessary logic has been simplified substantially,\nrequiring only two terms each for both S_1^+ and S_0^+.  Writing\nthe next-state logic algebraically, we obtain \n Question: \nQ. What is the purpose of an internal representation? \n Answer: \n\nA. The purpose of an internal representation is to provide a way to encode the state of an FSM so that it can be easily implemented."}, {"text": "Title: Abstracting Design Symmetries P8 \n Text: {eqnarray*}\nS_2^+ &=& HOLD\nS_1^+ &=&   S_0 + HOLD  S_1\nS_0^+ &=&   {{S_1}} + HOLD  S_0\n{eqnarray*} \n Question: \nQ. What is the purpose of the design? \n Answer: \n\nA. The purpose of the design is to create a system that can accurately and efficiently predict the results of horse races."}, {"text": "Title: Abstracting Design Symmetries P9 \n Text: Notice the similarity between the equations for S_1^+S_0^+ and the \nequations for a {2-to-1} mux: when HOLD=1, the counter retains \nits state, and when HOLD=0, it counts. \n Question: \nQ. What is the purpose of abstraction?\nQ. \nQ. The purpose of abstraction is to simplify an implementation. \n Answer: "}, {"text": "Title: Abstracting Design Symmetries P10 \n Text: By using semantic meaning in our choice of representation---in\nparticular the use of S_2 to record whether\nthe counter is currently halted (S_2=1) or counting (S_2=0)---we\nhave enabled ourselves to \nseparate out the logic for deciding whether to advance the counter\nfairly cleanly from the logic for advancing the counter itself.\nOnly the HOLD bit in the diagram is used to determine\nwhether or not the counter should advance in the current cycle. \n Question: \nQ. What is the difference between combinational logic and sequential logic? \n Answer: \n\nA. Sequential logic has feedback from the output back to the input, while combinational logic does not."}, {"text": "Title: Abstracting Design Symmetries P11 \n Text: Let's check that the implementation matches our original design. \n Question: \nQ. What is the purpose of the flip-flops in an FSM?\nQ. \nQ. The flip-flops are used to hold the internal state bits of the FSM. \n Answer: "}, {"text": "Title: Abstracting Design Symmetries P12 \n Text: Start by verifying that the HOLD variable is calculated correctly,\nHOLD=H+S_2,\nthen look back at the K-map for S_2^+ in the low-level design to\nverify that the expression we used does indeed match. \n Question: \nQ. -What is a Gray code?\nQ. -What is a two-bit Gray code counter?\nQ. -What is the cycle over all bit patterns of a certain length? \n Answer: "}, {"text": "Title: Abstracting Design Symmetries P13 \n Text: Next, check the mux abstraction. \n Question: \nQ. A finite-state machine (FSM) is a model of computation used to design both computer hardware and software.\nQ. \nQ. What is a model of computation? \n Answer: \n\nA model of computation is a mathematical model that specifies how a system should behave. In the case of a finite-state machine, it specifies how the machine should transition between states in response to input."}, {"text": "Title: Abstracting Design Symmetries P14 \n Text: When HOLD=1, the next-state logic for S_1^+ and S_0^+ \nreduces to S_1^+=S_1 and S_0^+=S_0;\nin other words, the counter stops counting and simply stays in its \ncurrent state.  When HOLD=0, these equations become\nS_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating\nsequence for S_1S_0 of 00, 01, 11, 10, as desired.\nYou may want to look back at our two-bit Gray code counter design\nto compare the next-state equations. \n Question: \nQ. What is the purpose of a counter?\nQ. \nQ. A counter is a device that is used to keep track of something, such as the number of times an event occurs. \n Answer: "}, {"text": "Title: Abstracting Design Symmetries P15 \n Text: We can now verify that the implementation produces the correct transition\nbehavior.  In the counting states, S_2=0, and the HOLD value simplifies\nto HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and\nand the counter continues to count in the correct sequence.\nWhen H=1, HOLD=1, and the counter stops at its current value\n(S_2^+S_1^+S_0^+=1S_1S_0, \nwhich is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0). \n Question: \nQ. What does the counter do when it reaches the last state in the diagram?\nQ. \nQ. It goes back to the first state. \n Answer: "}, {"text": "Title: Abstracting Design Symmetries P16 \n Text: In any of the halted states, S_2=1, and we can reduce HOLD to\nHOLD=H+.  Here, so long as we press the ``halt'' button\nor do not press the ``go'' button, the counter stays in its current\nstate, because HOLD=1.  If we release ``halt'' and press ``go,''\nwe have HOLD=0, and the counter resumes counting\n(S_2^+S_1^+S_0^+=0S_0{{S_1}},\nwhich is shorthand for S_2^+=0, S_1^+=S_0, and \nS_0^+={{S_1}}). \n Question: \nQ. What is the state diagram for a two-bit synchronous binary counter? \n Answer: \n\nA. The state diagram for a two-bit synchronous binary counter is a loop that goes from 00 to 11 and then back to 00 again."}, {"text": "Title: Abstracting Design Symmetries P17 \n Text: We have now verified the implementation. \n Question: \nQ. What does the transition diagram represent? \n Answer: \n\nA. The transition diagram represents a sequential logic circuit with two flip-flops."}, {"text": "Title: Abstracting Design Symmetries P18 \n Text: What if you wanted to build a three-bit Gray code counter with the same\ncontrols for starting and stopping?  You could go back to basics and struggle \nwith six-variable {K-maps}.  Or you could simply copy the HOLD \nmechanism from the two-bit design above, insert muxes between the next \nstate logic and the flip-flops of the three-bit Gray code counter that \nwe designed earlier, and control the muxes with the HOLD bit.  \nAbstraction is a powerful tool. \n Question: \nQ. What is the difference between the next-state logic and the implementation?\nQ. \nQ. The next-state logic is the underlying structure of the design, while the implementation is the completed design. \n Answer: "}, {"text": "Title: Impact of the State Representation P0 \n Text: What happens if we choose a bad representation?  For the same FSM---the\ntwo-bit Gray code counter with start and stop inputs---the \ntable below shows a poorly chosen mapping from states to internal \nstate representation. \n Question: \nQ. What is the design process for a digital FSM? \n Answer: \n\nA. The design process for a digital FSM includes the development of an abstract model, the implementation of functions for the next-state variables and output signals, and the testing and verification of the FSM."}, {"text": "Title: Impact of the State Representation P1 \n Text: Below the table is a diagram of an implementation using that\nrepresentation. \n Question: \nQ. What is the main point of the text?\nQ. \nQ. The main point of the text is to illustrate how semantic knowledge from an abstract model can be used to simplify an implementation. \n Answer: "}, {"text": "Title: Impact of the State Representation P2 \n Text: Verifying that the implementation's behavior\nis correct is left as an exercise for the determined reader. \n Question: \nQ. What is the six-step process that the author describes for designing an FSM? \n Answer: \n\nA. The six-step process for designing an FSM is as follows:\n\n1. Define the input and output signals for the FSM.\n\n2. Define the states of the FSM.\n\n3. Define the state transition diagram for the FSM.\n\n4. Implement the FSM in hardware or software.\n\n5. Test the FSM to verify its operation.\n\n6. Documentation the FSM design."}, {"text": "Title: Impact of the State Representation P3 \n Text: {\n{|c|c|c|c|c|}{1-2}{4-5}\nstate& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5}\n{ COUNT A}& 000& & { HALT A}& 111 \n{ COUNT B}& 101& & { HALT B}& 110 \n{ COUNT C}& 011& & { HALT C}& 100 \n{ COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5} \n Question: \nQ. What is the order of the steps for designing a sequential circuit? \n Answer: \n\n1) Specifying I/O behavior\n2) Developing an abstract model\n3) Completing the specification\n4) Choosing a state representation\n5) Calculating logic expressions\n6) Implementing with flip-flops and gates"}, {"text": "Title: Design of the Finite State Machine for the Lab P0 \n Text: This set of notes explains the process that Prof. Doug Jones used to develop\nthe FSM for the lab. \n Question: \nQ. What is the FSM for the lab? \n Answer: \n\nThe FSM for the lab is a finite state machine that can be used to control the\nbehavior of the lab equipment."}, {"text": "Title: Design of the Finite State Machine for the Lab P1 \n Text: The lab simulates a vending machine mechanism for automatically \nidentifying coins (dimes and quarters only), tracking the amount \nof money entered by the user, accepting or rejecting \ncoins, and emitting a signal when a total of 35 cents has been \naccepted.  In the lab, we will only drive a light with \nthe ``paid in full'' signal.   \n Question: \nQ. How does the vending machine keep track of the amount of money entered by the user? \n Answer: \n\nA. The vending machine keeps track of the amount of money entered by the user by monitoring the coins that are inserted into it. It counts the number of dimes and quarters that are inserted and uses that information to calculate the total amount of money that has been inserted."}, {"text": "Title: Design of the Finite State Machine for the Lab P2 \n Text: Sorry, neither candy nor Dew will be distributed! \n Question: \nQ. What will be distributed? \n Answer: \n\nA. Nothing will be distributed."}, {"text": "Title: Design of the Finite State Machine for the Lab P3 \n Text: Prof. Doug Jones designed the vending machine application and the FSM,\nwhile Prof. Chris Schmitz prototyped and constructed the physical elements \nwith some help from the ECE shop. \n Question: \nQ. What is the FSM? \n Answer: \n\nA. The FSM is a vending machine application that was designed by Prof. Doug Jones. It is a physical machine that was prototyped and constructed by Prof. Chris Schmitz with some help from the ECE shop."}, {"text": "Title: Design of the Finite State Machine for the Lab P4 \n Text: Prof. Volodymyr Kindratenko together with Prof. Geoffrey Herman created \nboth the wiki documentation and the Altera Quartus portions of the lab\n(the latter were based on earlier Mentor Graphics work by Prof. Herman). \n Question: \nQ. What does the text say about the wiki documentation?\nQ. \nQ. The text says that the wiki documentation was created by Prof. Volodymyr Kindratenko and Prof. Geoffrey Herman. \n Answer: "}, {"text": "Title: Design of the Finite State Machine for the Lab P5 \n Text: Prof. Kindratenko also helped to scale the design \nin a way that made it possible to deliver to the over 400 students entering\nECE every semester.   \n Question: \nQ. Why was it important to scale the design of the course? \n Answer: \n\nA. It was important to scale the design of the course in order to be able to deliver it to the large number of students who were taking it."}, {"text": "Title: Design of the Finite State Machine for the Lab P6 \n Text: Prof. Juan Jos'e Jaramillo later identified\ncommon failure modes, including variability caused by sunshine through \nthe windows in ECEB,{No wonder people say that engineers hate \nsunlight!} and made some changes to improve robustness.  He also\ncreated the PowerPoint slides that are typically used to describe the lab in\nlecture.  Casey Smith, head guru of the ECE Instructional Labs,\ndeveloped a new debounce design and made some other hardware \nimprovements to reduce the rate of student headaches.\nFinally, Prof. Kirill Levchenko together with UA Saidivya Ashok\nstruck a blow against COVID-19 by developing an inexpensive and\nportable replacement for the physical ``vending machine'' systems\nused for testing in previous semesters. \n Question: \nQ. What are the common failure modes of the vending machine? \n Answer: \n\nA. The common failure modes of the vending machine include variability caused by sunshine through the windows in ECEB, and debounce design issues."}, {"text": "Title: Physical Design, Sensors, and Timing P0 \n Text: A user inserts a coin into a slot at one end of the device.  The coin\nthen rolls down a slope towards a gate controlled by a servo.  The gate\ncan be raised or lowered, and determines whether the coin exits from the\nother side or the bottom of the device. \n Question: \nQ. What is the FSM for the lab? \n Answer: \n\nThe FSM for the lab is a finite state machine that can be used to control the\nbehavior of the lab equipment."}, {"text": "Title: Physical Design, Sensors, and Timing P1 \n Text: As the coin rolls, it passes two optical sensors.{The full system\nactually allows four sensors to differentiate four types of coins, but\nour lab uses only two of these sensors.}  One of these sensors is \npositioned high enough above the slope that a dime passes beneath the\nsesnor, allowing the signal T produced by the sensor to tell us whether \nthe coin is a dime or a quarter.  The second sensor is positioned so\nthat all coins pass in front of it.  The sensor positions are chosen \ncarefully to ensure that, in the case of a quarter, the coin is still\nblocking the first sensor when it reaches the second sensor.  \nBlocked sensors give a signal of 1 in this design, so the rising edge \nthe signal from the second sensor can be used as a ``clock'' for our \nFSM.  When the rising edge occurs, the signal T from the first sensor \nindicates whether the coin is a quarter (T=1) or a dime (T=0). \n Question: \nQ. How does the vending machine keep track of the amount of money entered by the user? \n Answer: \n\nA. The vending machine keeps track of the amount of money entered by the user by monitoring the coins that are inserted into it. It counts the number of dimes and quarters that are inserted and uses that information to calculate the total amount of money that has been inserted."}, {"text": "Title: Physical Design, Sensors, and Timing P2 \n Text: \nA sample timing diagram for the lab appears to the right.  The clock\nsignal generated by the lab is not only not a square wave---in other words,\nthe high and low portions are not equal---but is also unlikely to be periodic.\nInstead, the ``cycle'' is defined by the time between coin insertions.\nThe T signal serves as the single input to our FSM.  In the timing \n Question: \nQ. What will be distributed? \n Answer: \n\nA. Nothing will be distributed."}, {"text": "Title: Physical Design, Sensors, and Timing P3 \n Text: diagram, T is shown as rising and falling before the clock edge.\nWe use positive edge-triggered flip-flops to implement our FSM,\nthus the aspect of the relative timing that matters to our design\nis that, when the clock rises, the value of T is stable and indicates \nthe type of coin entered.  The signal T may fall before or after\nthe clock does---the two are equivalent for our FSM's needs. \n Question: \nQ. What is the FSM? \n Answer: \n\nA. The FSM is a vending machine application that was designed by Prof. Doug Jones. It is a physical machine that was prototyped and constructed by Prof. Chris Schmitz with some help from the ECE shop."}, {"text": "Title: Physical Design, Sensors, and Timing P4 \n Text: The signal A in the timing diagram is an output from the FSM, and\nindicates whether or not the coin should be accepted.  This signal \ncontrols the servo that drives the gate, and thus determines whether\nthe coin is accepted (A=1) as payment or rejected (A=0) and returned\nto the user.   \n Question: \nQ. What does the text say about the wiki documentation?\nQ. \nQ. The text says that the wiki documentation was created by Prof. Volodymyr Kindratenko and Prof. Geoffrey Herman. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P5 \n Text: Looking at the timing diagram, you should note that our FSM makes \na decision based on its current state and the input T and enters a \nnew state at the rising clock edge.  The value of A in the next cycle\nthus determines the position of the gate when the coin eventually\nrolls to the end of the slope. \n Question: \nQ. Why was it important to scale the design of the course? \n Answer: \n\nA. It was important to scale the design of the course in order to be able to deliver it to the large number of students who were taking it."}, {"text": "Title: Physical Design, Sensors, and Timing P6 \n Text: As we said earlier, our FSM is thus a Moore machine: the output A\ndoes not depend on the input T, but only on the current internal \nstate bits of the the FSM. \n Question: \nQ. What are the common failure modes of the vending machine? \n Answer: \n\nA. The common failure modes of the vending machine include variability caused by sunshine through the windows in ECEB, and debounce design issues."}, {"text": "Title: Physical Design, Sensors, and Timing P7 \n Text: However, you should also now realize that making A depend on T\nis not adequate for this lab.  If A were to rise with T and\nfall with the rising clock edge (on entry to the next state), or\neven fall with the falling edge of T, the gate would return to the\nreject position by the time the coin reached the gate, regardless\nof our FSM's decision! \n Question: \nQ. What is the purpose of the device? \n Answer: \n\nA. The device is a coin-operated sorting machine."}, {"text": "Title: An Abstract Model\\vspace4pt P0 \n Text: \nWe start by writing down states for a user's expected behavior.\nGiven the fairly tight constraints that we have placed on our lab,\nfew combinations are pos- \n Question: \nQ. What is the FSM for the lab? \n Answer: \n\nThe FSM for the lab is a finite state machine that can be used to control the\nbehavior of the lab equipment."}, {"text": "Title: An Abstract Model\\vspace4pt P1 \n Text: \n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nSTART& DIME& QUARTER& & no\nDIME& & PAID& yes& no\nQUARTER& PAID& & yes& no\nPAID& & & yes& yes \n Question: \nQ. How does the vending machine keep track of the amount of money entered by the user? \n Answer: \n\nA. The vending machine keeps track of the amount of money entered by the user by monitoring the coins that are inserted into it. It counts the number of dimes and quarters that are inserted and uses that information to calculate the total amount of money that has been inserted."}, {"text": "Title: An Abstract Model\\vspace4pt P2 \n Text: sible.  For a total of 35 cents, a user should either insert a dime \nfollowed by a quarter, or a quarter followed by a dime. \n Question: \nQ. What will be distributed? \n Answer: \n\nA. Nothing will be distributed."}, {"text": "Title: An Abstract Model\\vspace4pt P3 \n Text: We begin in a START state, which transitions to states DIME or QUARTER\nwhen the user inserts the first coin.  With no previous coin, we need not\nspecify a value for A.  No money has been deposited, so we set \noutput P=0 in the START state. \n Question: \nQ. What is the FSM? \n Answer: \n\nA. The FSM is a vending machine application that was designed by Prof. Doug Jones. It is a physical machine that was prototyped and constructed by Prof. Chris Schmitz with some help from the ECE shop."}, {"text": "Title: An Abstract Model\\vspace4pt P4 \n Text: We next create DIME and QUARTER states corresponding to the user having\nentered one coin.  The first coin should be accepted, but more money is\nneeded, so both of these states output A=1 and P=0.\nWhen a coin of the opposite type is entered, each state moves to a\nstate called PAID, which we use for the case in which a total of 35 cents has\nbeen received.  For now, we ignore the possibility that the same type\nof coin is deposited more than once.  Finally, the PAID state accepts\nthe second coin (A=1) and indicates that the user has paid the full\nprice of 35 cents (P=1). \n Question: \nQ. What does the text say about the wiki documentation?\nQ. \nQ. The text says that the wiki documentation was created by Prof. Volodymyr Kindratenko and Prof. Geoffrey Herman. \n Answer: "}, {"text": "Title: An Abstract Model\\vspace4pt P5 \n Text: \nWe next extend our design to handle user mistakes.  If a user enters\na second dime in the DIME state, our FSM should reject the coin.  We\ncreate a REJECTD state and add it as the next state from \n Question: \nQ. Why was it important to scale the design of the course? \n Answer: \n\nA. It was important to scale the design of the course in order to be able to deliver it to the large number of students who were taking it."}, {"text": "Title: An Abstract Model\\vspace4pt P6 \n Text: \n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nSTART& DIME& QUARTER& & no\nDIME& REJECTD& PAID& yes& no\nREJECTD& REJECTD& PAID& no& no\nQUARTER& PAID& REJECTQ& yes& no\nREJECTQ& PAID& REJECTQ& no& no\nPAID& & & yes& yes \n Question: \nQ. What are the common failure modes of the vending machine? \n Answer: \n\nA. The common failure modes of the vending machine include variability caused by sunshine through the windows in ECEB, and debounce design issues."}, {"text": "Title: An Abstract Model\\vspace4pt P7 \n Text: DIME when a dime is entered.\nThe REJECTD state rejects the dime (A=0) and\ncontinues to wait for a quarter (P=0).  What should we use as next \nstates from REJECTD?  If the user enters a third dime (or a fourth, \nor a fifth, and so on), we want to reject the new dime as well.  \nIf the user enters a quarter, we want to accept the coin, at which point\nwe have received 35 cents (counting the first dime).  We use\nthis reasoning to complete the description of REJECTD.  We also create\nan analogous state, REJECTQ, to handle a user who inserts more than\none quarter. \n Question: \nQ. What is the purpose of the device? \n Answer: \n\nA. The device is a coin-operated sorting machine."}, {"text": "Title: An Abstract Model\\vspace4pt P8 \n Text: What should happen after a user has paid 35 cents and bought \none item?  The FSM at that point is in the PAID state, which delivers\nthe item by setting P=1. \n Question: \nQ. How does the system distinguish between quarters and dimes?\nQ. \nQ. The system uses two optical sensors, positioned so that a dime passes beneath the first sensor and all coins pass in front of the second sensor. When the rising edge of the signal from the second sensor occurs, the signal from the first sensor indicates whether the coin is a quarter or a dime. \n Answer: "}, {"text": "Title: An Abstract Model\\vspace4pt P9 \n Text: Given that we want the FSM to allow the user to purchase another item, \nhow should we choose the next states from PAID? \n Question: \nQ. What is the purpose of the T signal?\nQ. \nQ. The T signal is the input to the FSM. \n Answer: "}, {"text": "Title: An Abstract Model\\vspace4pt P10 \n Text: The behavior that we want from PAID is identical to the behavior that\nwe defined from START.  The 35 cents already \ndeposited was used to pay for the item delivered, so the machine is\nno longer holding any of the user's money. \n Question: \nQ. What will happen if the signal T falls before the clock rises? \n Answer: \n\nA. The value of T will be stable, indicating the type of coin entered."}, {"text": "Title: An Abstract Model\\vspace4pt P11 \n Text: We can thus simply set the next states from PAID to be DIME when a \ndime is inserted and QUARTER when a quarter is inserted. \n Question: \nQ. What does the signal A in the timing diagram represent? \n Answer: \n\nA. The signal A in the timing diagram represents the output from the FSM, which indicates whether or not the coin should be accepted as payment."}, {"text": "Title: An Abstract Model\\vspace4pt P12 \n Text: \nAt this point, we make a decision intended primarily to simplify the\nlogic needed to build the lab.  Without a physical item delivery \nmechanism with a specification for how its in- \n Question: \nQ. Why does the FSM make a decision based on its current state and the input T? \n Answer: \n\nA. The FSM makes a decision based on its current state and the input T because it needs to know when to open and close the gate in order to let the coin through."}, {"text": "Title: An Abstract Model\\vspace4pt P13 \n Text: \n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nPAID& DIME& QUARTER& yes& yes\nDIME& REJECTD& PAID& yes& no\nREJECTD& REJECTD& PAID& no& no\nQUARTER& PAID& REJECTQ& yes& no\nREJECTQ& PAID& REJECTQ& no& no \n Question: \nQ. What does T refer to in the text?\nQ. \nQ. T refers to the input to the FSM. \n Answer: "}, {"text": "Title: An Abstract Model\\vspace4pt P14 \n Text: put must be driven, \nthe behavior of the output signal P can be fairly flexible.  \nFor example, we could build a delivery mechanism that used the rising\nedge of P to open a chute.  In this case, the output P=0 in the\nstart state is not relevant, and we can merge the state START with\nthe state PAID.  The way that we handle P in the lab, we might\nfind it strange to have a ``paid'' light turn on before inserting any\nmoney, but keeping the design simple enough for a first lab exercise \nis more important.  Our final abstract state table appears above. \n Question: \nQ. What does the author mean when they say \"If A were to rise with T and fall with the rising clock edge (on entry to the next state), or even fall with the falling edge of T, the gate would return to the reject position by the time the coin reached the gate, regardless of our FSM's decision!\"? \n Answer: \n\nThe author suggests that if A were to rise and fall with T, or even just with the rising clock edge, then the gate would return to the reject position by the time the coin reached the gate. This would be regardless of our FSM's decision, since the gate would have already reset."}, {"text": "Title: Picking the Representation P0 \n Text: We are now ready to choose the state representation for the lab FSM. \n Question: \nQ. What is the FSM for the lab? \n Answer: \n\nThe FSM for the lab is a finite state machine that can be used to control the\nbehavior of the lab equipment."}, {"text": "Title: Picking the Representation P1 \n Text: With five states, we need three bits of internal state. \n Question: \nQ. How does the vending machine keep track of the amount of money entered by the user? \n Answer: \n\nA. The vending machine keeps track of the amount of money entered by the user by monitoring the coins that are inserted into it. It counts the number of dimes and quarters that are inserted and uses that information to calculate the total amount of money that has been inserted."}, {"text": "Title: Picking the Representation P2 \n Text: Prof. Jones decided to leverage human meaning in assigning the\nbit patterns, as follows: \n Question: \nQ. What will be distributed? \n Answer: \n\nA. Nothing will be distributed."}, {"text": "Title: Picking the Representation P3 \n Text: S_2& type of last coin inserted (0 for dime, 1 for quarter)\nS_1& more than one quarter inserted? (1 for yes, 0 for no)\nS_0& more than one dime inserted? (1 for yes, 0 for no) \n Question: \nQ. What is the FSM? \n Answer: \n\nA. The FSM is a vending machine application that was designed by Prof. Doug Jones. It is a physical machine that was prototyped and constructed by Prof. Chris Schmitz with some help from the ECE shop."}, {"text": "Title: Picking the Representation P4 \n Text: \nThese meanings are not easy to apply to all of our states.  For example,\nin the PAID state, the last coin inserted may have been of either type,\nor of no type at all, since we decided to start our FSM in that state as \nwell.  However, for the other four states, the meanings provide a clear\nand unique set of bit pattern assignments, as shown to the right.  We\ncan choose any of the remaining four bit patterns (010, 011, 101, or 111)\nfor the PAID state.  In fact, { we can choose all of the remaining\npatterns} for the PAID state.  We can always represent any state \n Question: \nQ. What does the text say about the wiki documentation?\nQ. \nQ. The text says that the wiki documentation was created by Prof. Volodymyr Kindratenko and Prof. Geoffrey Herman. \n Answer: "}, {"text": "Title: Picking the Representation P5 \n Text: state& S_2S_1S_0  \nPAID& ???\nDIME& 000\nREJECTD& 001\nQUARTER& 100\nREJECTQ& 110 \n Question: \nQ. Why was it important to scale the design of the course? \n Answer: \n\nA. It was important to scale the design of the course in order to be able to deliver it to the large number of students who were taking it."}, {"text": "Title: Picking the Representation P6 \n Text: with more\nthan one pattern if we have spare patterns available.  Prof. Jones\nused this freedom to simplify the logic design. \n Question: \nQ. What are the common failure modes of the vending machine? \n Answer: \n\nA. The common failure modes of the vending machine include variability caused by sunshine through the windows in ECEB, and debounce design issues."}, {"text": "Title: Picking the Representation P7 \n Text: This particular example is slightly tricky.  The four free patterns do\nnot share any single bit in common, so we cannot simply insert x's\ninto all {K-map} entries for which the next state is PAID.\nFor example, if we insert an x into the {K-map} for  S_2^+,\nand then choose a function for S_2^+ that produces a value of 1\nin place of the don't care, we must also produce a 1 in\nthe corresponding entry of the {K-map} for S_0^+.  Our options\nfor PAID include 101 and 111, but not 100 nor 110.  These latter\ntwo states have other meanings. \n Question: \nQ. What is the purpose of the device? \n Answer: \n\nA. The device is a coin-operated sorting machine."}, {"text": "Title: Picking the Representation P8 \n Text: \nLet's begin by writing a next-state table consisting mostly of bits,\nas shown to the right.  We use this table to write out a {K-map}\nfor S_2^+ as follows: any of the patterns that may be used for the\nPAID state obey the next-state rules for PAID.  Any next-state marked\nas PAID is marked as don't care in the {K-map}, \n Question: \nQ. How does the system distinguish between quarters and dimes?\nQ. \nQ. The system uses two optical sensors, positioned so that a dime passes beneath the first sensor and all coins pass in front of the second sensor. When the rising edge of the signal from the second sensor occurs, the signal from the first sensor indicates whether the coin is a quarter or a dime. \n Answer: "}, {"text": "Title: Picking the Representation P9 \n Text: \n{cc|cc}\n&&{|c}{S_2^+S_1^+S_0^+}\nstate& S_2S_1S_0& T=0& T=1 \nPAID& PAID& 000& 100\nDIME& 000& 001& PAID\nREJECTD& 001& 001& PAID\nQUARTER& 100& PAID& 110\nREJECTQ& 110& PAID& 110 \n Question: \nQ. What is the purpose of the T signal?\nQ. \nQ. The T signal is the input to the FSM. \n Answer: "}, {"text": "Title: Picking the Representation P10 \n Text: \nsince we can\nchoose patterns starting with either or both values to represent our\nPAID state.  The resulting {K-map} appears to the far right.\nAs shown, we simply set S_2^+=T, which matches our\noriginal ``meaning'' for S_2.  That is, S_2 is the type of the\nlast coin inserted. \n Question: \nQ. What will happen if the signal T falls before the clock rises? \n Answer: \n\nA. The value of T will be stable, indicating the type of coin entered."}, {"text": "Title: Picking the Representation P11 \n Text: \nBased on our choice for S_2^+, we can rewrite the {K-map} as\nshown to the right, with green italics and shading marking the\nvalues produced for the x's in the specification.  Each of these\nboxes corresponds to one transition into the PAID state.  By \nspecifying the S_2 value, we cut the number of possible choices\nfrom four to two in each case.  For those combinations in which the\nimplementation produces S_2^+=0, we must choose S_1^+=1, but\nare still free to leave S_0^+ marked as a don't care.  Similarly,\nfor those combinations in which the implementation produces S_2^+=1, \nwe must choose S_0^+=1, but\nare still free to leave S_1^+ marked as a don't care. \n Question: \nQ. What does the signal A in the timing diagram represent? \n Answer: \n\nA. The signal A in the timing diagram represents the output from the FSM, which indicates whether or not the coin should be accepted as payment."}, {"text": "Title: Picking the Representation P12 \n Text: \nThe {K-maps} for S_1^+ and S_0^+ are shown to the right.\nWe have not given algebraic expressions for either, but have indicated\nour choices by highlighting the resulting replacements of don't care\nentries with the values produced by our expressions. \n Question: \nQ. Why does the FSM make a decision based on its current state and the input T? \n Answer: \n\nA. The FSM makes a decision based on its current state and the input T because it needs to know when to open and close the gate in order to let the coin through."}, {"text": "Title: Picking the Representation P13 \n Text: At this point, we can review the state patterns actually produced by\neach of the four next-state transitions into the PAID state.  From\nthe DIME state, we move into the 101 state when the user inserts a \n Question: \nQ. What does T refer to in the text?\nQ. \nQ. T refers to the input to the FSM. \n Answer: "}, {"text": "Title: Picking the Representation P14 \n Text: \nquarter.  The result is the same from the REJECTD state.  From the\nQUARTER state, however, we move into the 010 state when the user \ninserts a dime.  The result is the same from the REJECTQ state.  We\nmust thus classify both patterns, 101 and 010, as PAID states.  The\nremaining two patterns, 011 and 111, cannot \n Question: \nQ. What does the author mean when they say \"If A were to rise with T and fall with the rising clock edge (on entry to the next state), or even fall with the falling edge of T, the gate would return to the reject position by the time the coin reached the gate, regardless of our FSM's decision!\"? \n Answer: \n\nThe author suggests that if A were to rise and fall with T, or even just with the rising clock edge, then the gate would return to the reject position by the time the coin reached the gate. This would be regardless of our FSM's decision, since the gate would have already reset."}, {"text": "Title: Picking the Representation P15 \n Text: \nbe reached from any of\nthe states in our design.  We might then try to leverage the fact\nthat the next-state patterns from these two states are not relevant \n(recall that we fixed the next-state patterns for all four of the \npossible PAID states) to further simplify our logic, but doing so \ndoes not provide any advantage (you may want to check our claim). \n Question: \nQ. What are the \"tight constraints\" mentioned in the text? \n Answer: \n\nA. The tight constraints mentioned in the text are the constraints placed on the lab by the author."}, {"text": "Title: Picking the Representation P16 \n Text: The final state table is shown to the right.  We have included the\nextra states at the bottom of the table.  We have specified the\nnext-state logic for these \n Question: \nQ. What is the state of the machine when a dime and a quarter are inserted?\nQ. \nQ. The state of the machine when a dime and a quarter are inserted is that the machine is paid and the user can accept the payment. \n Answer: "}, {"text": "Title: Picking the Representation P17 \n Text: \n{cc|cc|cc}\n&&{|c|}{S_2^+S_1^+S_0^+}\nstate& S_2S_1S_0& T=0& T=1& A& P \nPAID1& 010& 000& 100& 1& 1\nPAID2& 101& 000& 100& 1& 1\nDIME& 000& 001& 101& 1& 0\nREJECTD& 001& 001& 101& 0& 0\nQUARTER& 100& 010& 110& 1& 0\nREJECTQ& 110& 010& 110& 0& 0 \nEXTRA1& 011& 000& 100& x& x\nEXTRA2& 111& 000& 100& x& x \n Question: \nQ. What are the two combinations of coins that a user can insert to get a total of 35 cents? \n Answer: \n\nA. A user can insert a dime followed by a quarter, or a quarter followed by a dime, to get a total of 35 cents."}, {"text": "Title: Picking the Representation P18 \n Text: states, but left the output bits as don't\ncares.  A state transition diagram appears at the bottom of this page. \n Question: \nQ. What is the value of A in the START state?\nQ. \nQ. A=0 \n Answer: "}, {"text": "Title: Testing the Design P0 \n Text: Having a complete design on paper is a good step forward, but humans\nmake mistakes at all stages.  How can we know that a circuit that\nwe build in the lab correctly implements the FSM that we have outlined \nin these notes? \n Question: \nQ. What is the FSM for the lab? \n Answer: \n\nThe FSM for the lab is a finite state machine that can be used to control the\nbehavior of the lab equipment."}, {"text": "Title: Testing the Design P1 \n Text: For the lab design, we have two problems to solve. \n Question: \nQ. How does the vending machine keep track of the amount of money entered by the user? \n Answer: \n\nA. The vending machine keeps track of the amount of money entered by the user by monitoring the coins that are inserted into it. It counts the number of dimes and quarters that are inserted and uses that information to calculate the total amount of money that has been inserted."}, {"text": "Title: Testing the Design P2 \n Text: First, we have not specified an initialization scheme for the FSM.\nWe may want the FSM to start in one of the PAID states, but adding\ninitialization logic to the design may mean requiring you to wire together\nsignificantly more chips.  Second, we need a sequence of inputs that\nmanages to test that all of the next-state and output logic implementations\nare correct. \n Question: \nQ. What will be distributed? \n Answer: \n\nA. Nothing will be distributed."}, {"text": "Title: Testing the Design P3 \n Text: Testing sequential logic, including FSMs, is in general extremely difficult.\nIn fact, large sequential systems today are generally converted into \ncombinational logic by using shift registers to fill the \nflip-flops with a particular pattern, \nexecuting the logic for one clock cycle, and checking that the resulting \npattern of bits in the flip-flops is correct.  This approach is called \n{ scan-based testing}, and is discussed in ECE 543.  You \nwill make use of a similar approach\nwhen you test your combinational logic in the second week of the lab,\nbefore wiring up the flip-flops. \n Question: \nQ. What is the FSM? \n Answer: \n\nA. The FSM is a vending machine application that was designed by Prof. Doug Jones. It is a physical machine that was prototyped and constructed by Prof. Chris Schmitz with some help from the ECE shop."}, {"text": "Title: Testing the Design P4 \n Text: We have designed our FSM to be easy to test (even small FSMs\nmay be challenging) with a brute force approach.  In particular, we \nidentify two input sequences that together serve both to initialize and \nto test a correctly implemented variant of our FSM.  Our initialization\nsequence forces the FSM into a specific state regardless of its initial\nstate.  And our test sequence crosses every transition arc leaving the\nsix valid states. \n Question: \nQ. What does the text say about the wiki documentation?\nQ. \nQ. The text says that the wiki documentation was created by Prof. Volodymyr Kindratenko and Prof. Geoffrey Herman. \n Answer: "}, {"text": "Title: Testing the Design P5 \n Text: In terms of T, the coin type, we initialize the FSM with the\ninput sequence 001.  Notice that such a sequence takes any initial \nstate into PAID2. \n Question: \nQ. Why was it important to scale the design of the course? \n Answer: \n\nA. It was important to scale the design of the course in order to be able to deliver it to the large number of students who were taking it."}, {"text": "Title: Testing the Design P6 \n Text: For testing, we use the input sequence 111010010001.  You should trace \nthis sequence, starting from PAID2, on the diagram below to see how the\ntest sequence covers all of the possible arcs.  As we test, we need also\nto observe the A and P outputs in each state to check the output\nlogic. \n Question: \nQ. What are the common failure modes of the vending machine? \n Answer: \n\nA. The common failure modes of the vending machine include variability caused by sunshine through the windows in ECEB, and debounce design issues."}, {"text": "Title: Extending Keyless Entry with a Timeout P0 \n Text: This set of notes builds on the keyless entry control FSM that we\ndesigned earlier.  In particular, we use a counter to make the alarm\ntime out, turning itself off after a fixed amount of time.  The goal\nof this extension is to illustrate how we can make use of components\nsuch as registers and counters as building blocks for our FSMs\nwithout fully expanding the design to explicitly illustrate all\npossible states. \n Question: \nQ. What is the goal of this extension?\nQ. \nQ. The goal of this extension is to illustrate how we can make use of components such as registers and counters as building blocks for our FSMs without fully expanding the design to explicitly illustrate all possible states. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P0 \n Text: \nTo begin, let's review the FSM that we designed earlier for keyless \nentry.  The state transition diagram for our design is replicated to \nthe right. \n Question: \nQ. What is the goal of this extension?\nQ. \nQ. The goal of this extension is to illustrate how we can make use of components such as registers and counters as building blocks for our FSMs without fully expanding the design to explicitly illustrate all possible states. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P1 \n Text: The four states are labeled with state bits and output bits,\nS_1S_0/DRA, where D indicates that the driver's door should\nbe unlocked, R indicates that the rest of the doors should be\nunlocked, and A indicates that the alarm should be on. \n Question: \nQ. What is the advantage of using an FSM over a traditional state machine? \n Answer: \n\nA.\n\nUsing an FSM allows for a more concise and efficient description of the system's behavior. Additionally, an FSM can be easily extended to handle more complex behavior, while a traditional state machine may become unwieldy."}, {"text": "Title: Physical Design, Sensors, and Timing P2 \n Text: Transition arcs in the diagram are labeled with concise versions of \nthe inputs ULP (using don't cares), where U represents an unlock\nbutton, L represents a lock button, and P represents a panic \nbutton. \n Question: \nQ. What does the 'A' in S_1S_0/DRA stand for?\nQ. \nQ. The 'A' in S_1S_0/DRA stands for 'alarm.' \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P3 \n Text: In this design, once a user presses the panic button P, the alarm\nsounds until the user presses the \n Question: \nQ. What does ULP stand for?\nQ. \nQ. ULP stands for unlock, lock, and panic buttons. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P4 \n Text: lock button L to turn it off.\nInstead of sounding the alarm indefinitely, we might want to turn\nthe alarm off after a fixed amount of time.  In other words, after \nthe system has been in the ALARM state for, say, thirty or sixty seconds,\nwe might want to move back to the LOCKED state even if the user has\nnot pushed the lock button.  The blue annotation in the diagram indicates\nthe arc that we must adjust.  But thirty or sixty seconds is a large \nnumber of clock cycles, and our FSM must keep track of the time.\nDo we need to draw all of the states? \n Question: \nQ. What happens when a user presses the panic button?\nQ. \nQ. The alarm sounds until the user presses the button again. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P5 \n Text: Instead of following the design process that we outlined earlier, let's\nthink about how we can modify our existing design to incorporate the new\nfunctionality.  In order to keep track of time, we use a binary counter. \n Question: \nQ. What does the blue annotation in the diagram indicate? \n Answer: \n\nA. The blue annotation indicates the need to add a timer to the FSM."}, {"text": "Title: Physical Design, Sensors, and Timing P6 \n Text: Let's say that we want our timeout to be T cycles. \n Question: \nQ. How does the binary counter work? \n Answer: \n\nA.\n\nThe binary counter works by counting up in binary. So, for every tick of the clock, the binary counter will add 1 to its current value. If the counter reaches its maximum value, it will reset back to 0 and start counting again."}, {"text": "Title: Physical Design, Sensors, and Timing P7 \n Text: When we enter the alarm state, we want to set the counter's value \nto T-1, then let the counter count down until it reaches 0, at \nwhich point a timeout occurs. \n Question: \nQ. How long should our timeout be?\nQ. \nQ. T cycles. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P8 \n Text: To load the initial value, our counter should have a parallel load \ncapability that sets the counter value when input LD=1.  When LD=0,\nthe counter counts down.  \n Question: \nQ. What is the purpose of the counter in the alarm state?\nQ. \nQ. The counter is used to count down to a timeout. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P9 \n Text: The counter also has an output Z that indicates that the counter's \nvalue is currently zero, which we can use to indicate a timeout on\nthe alarm. \n Question: \nQ. If LD=1, does the counter count up or down? \n Answer: \n\nA. The counter counts up."}, {"text": "Title: Physical Design, Sensors, and Timing P10 \n Text: You should be able to build such a counter based on what you have learned\nearlier in the class.  Here, we will assume that we can just make use of\nit. \n Question: \nQ. What is the purpose of the output Z on the counter?\nQ. \nQ. The purpose of the output Z on the counter is to indicate that the counter's value is currently zero. \n Answer:  This can be used to indicate a timeout on the alarm."}, {"text": "Title: Physical Design, Sensors, and Timing P11 \n Text: How many bits do we need in our counter?  The answer depends on T.\nIf we add the counter to our design, the state of the counter is \ntechnically part of the state of our FSM, but we can treat it\nsomewhat abstractly.  For example, we only plan to make use of the \ncounter value in the ALARM state, so we ignore the counter bits in the\nthree other states.  In other words, S_1S_0=10 means that the system \nis in the LOCKED state regardless of the counter's value. \n Question: \nQ. How does the author feel about the counter?\nQ. \nQ. The author seems to feel that the counter is a helpful tool. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P12 \n Text: \nWe expand the ALARM state into T separate states based on the value\nof the counter.  As shown to the right, we name the states ALARM(1) \nthrough ALARM(T).  All of these alarm states use S_1S_0=01, but\nthey can be differentiated using a ``timer'' (the counter value). \n Question: \nQ. What is the purpose of the counter in the FSM?\nQ. \nQ. The counter is used to track the number of times the system has been in the LOCKED state. When the counter reaches a certain value, the system will transition to the ALARM state. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P13 \n Text: We need to make design decisions about how the arcs entering and\nleaving the ALARM state in our original design should be used once\nwe have incorporated the timeout.  As a first step, we decide that\nall arcs entering ALARM from other states now enter ALARM(1).  \nSimilarly, if the user presses the panic button P in any of the\nALARM(t) states, the system returns to ALARM(1).  Effectively, \npressing the panic button resets the timer. \n Question: \nQ. What is the value of T? \n Answer: \n\nT=3"}, {"text": "Title: Physical Design, Sensors, and Timing P14 \n Text: The only arc leaving the ALARM state goes to the LOCKED state on \nULP=x10.  We replicate this arc for all ALARM(t) states: the\nuser can push the lock button at any time to silence the alarm. \n Question: \nQ. What is the purpose of the timer in the alarm state?\nQ. \nQ. The timer is used to determine how long the alarm state should last. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P15 \n Text: Finally, the self-loop back to the ALARM state on ULP=x00 becomes\nthe countdown arcs in our expanded states, taking ALARM(t) to ALARM(t+1),\nand ALARM(T) to LOCKED. \n Question: \nQ. What is the purpose of the LOCKED state?\nQ. \nQ. The purpose of the LOCKED state is to silence the alarm. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P16 \n Text: Now that we have a complete specification for the extended design, we\ncan implement it.  We want to reuse our original design as much as possible,\nbut we have three new features that must be considered.  First, when\nwe enter the ALARM(1) state, we need to set the counter value to T-1.\nSecond, we need the counter value to count downward while in the ALARM\nstate.  Finally, we need to move back to the LOCKED state when a timeout\noccurs---that is, when the counter reaches zero. \n Question: \nQ. What are the expanded states in the author's model?\nQ. \nQ. The expanded states are LOCKED, COUNTING, and ALARM. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P17 \n Text: The first problem is fairly easy.  Our counter supports parallel load,\nand the only value that we need to load is T-1, so we apply the constant\nbit pattern for T-1 to the load inputs and raise the LD input whenever\nwe enter the ALARM(1) state.  In our original design, we chose to enter\nthe ALARM state whenever the user pressed P, regardless of the other\nbuttons.  Hence we can connect P directly to our counter's LD input. \n Question: \nQ. What does the author mean by \"We want to reuse our original design as much as possible\"? \n Answer: \n\nA. The author wants to minimize the changes needed to the original design in order to add the new features."}, {"text": "Title: Physical Design, Sensors, and Timing P18 \n Text: The second problem is handled by the counter's countdown functionality.\nIn the ALARM(t) states, the counter will count down each cycle, moving\nthe system from ALARM(t) to ALARM(t+1). \n Question: \nQ. What is the purpose of the counter in this design?\nQ. \nQ. The purpose of the counter is to count the number of times the user presses the P button. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P19 \n Text: The last problem is slightly trickier, since we need to change S_1S_0.\nNotice that S_1S_0=01 for the ALARM state and S_1S_0=00 for the LOCKED\nstate.  Thus, we need only force S_0 to 0 when a timeout occurs.\nWe can use a single {2-to-1} multiplexer for this purpose.  The\n``0'' input of the mux comes from the original S_0^+ logic, and the\n``1'' input is a constant 0.  All other state logic remains unchanged.  \nWhen does a timeout occur?  \n Question: \nQ. What is the difference between the first and second problems?\nQ. \nQ. The first problem is that the alarm does not sound when it is supposed to. The second problem is that the counter does not count down properly. \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing P20 \n Text: First, we must be in the ALARM(T) state, so S_1S_0=01 and the \ncounter's Z output is raised.  Second, the input combination must\nbe ULP=xx0---notice that both ULP=x00 and ULP=x10 return to \nLOCKED from ALARM(T).  A single, four-input AND gate thus suffices to\nobtain the timeout signal, {S_1}S_0Z, which we connect \nto the select input of the mux between the S_0^+ logic and the\nS_0 flip-flop. \n Question: \nQ. What is the purpose of the 2-to-1 multiplexer? \n Answer: \n\nA. The multiplexer is used to change the value of S_1S_0 from 01 to 00 when a timeout occurs."}, {"text": "Title: Physical Design, Sensors, and Timing P21 \n Text: The extension thus requires only a counter, a mux, and a gate, as shown below. \n Question: \nQ. What is the input combination that must be present in order to timeout to occur?\nQ. \nQ. The input combination that must be present in order to timeout to occur is ULP=xx0. \n Answer: "}, {"text": "Title: Finite State Machine Design Examples, Part II P0 \n Text: This set of notes provides several additional examples of FSM design.\nWe first design an FSM to control a vending machine, introducing\nencoders and decoders as components that help us to implement our\ndesign.  We then design a game controller for a logic puzzle\nimplemented as a children's game.  Finally, we analyze a digital FSM\ndesigned to control the stoplights at the intersection of two roads. \n Question: \nQ. What is the purpose of an FSM?\nQ. \nQ. An FSM is a machine that can be in one of a finite number of states. It can move from one state to another in response to some inputs, and it can perform some actions when it is in a particular state. \n Answer: "}, {"text": "Title: Design of a Vending Machine P0 \n Text: For the next example, we design an FSM to control a simple vending machine.  \nThe machine accepts {U.S. coins}{Most countries have small \nbills or coins in demoninations suitable for vending machine prices, so think \nabout some other currency if you prefer.} as payment and offers a choice\nof three items for sale. \n Question: \nQ. What is the purpose of an FSM?\nQ. \nQ. An FSM is a machine that can be in one of a finite number of states. It can move from one state to another in response to some inputs, and it can perform some actions when it is in a particular state. \n Answer: "}, {"text": "Title: Design of a Vending Machine P1 \n Text: What states does such an FSM need?  The FSM needs to keep track of how\nmuch money has been inserted in order to decide whether a user can \npurchase one of the items.  That information alone is enough for the\nsimplest machine, but let's create a machine with adjustable item\nprices.  We can use registers to hold the item prices, which \nwe denote P_1, P_2, and P_3. \n Question: \nQ. What is the output of the vending machine? \n Answer: \n\nA. The output of the vending machine is the item that is dispensed."}, {"text": "Title: Design of a Vending Machine P2 \n Text: Technically, the item prices are also part of the internal state of the \nFSM.  However,  we leave out discussion (and, indeed, methods) for setting\nthe item prices, so no state with a given combination of prices has any \ntransition to a state with a different set of item prices.\nIn other words, any given combination of item prices induces a subset \nof states that operate independently of the subset induced by a distinct \ncombination of item prices.  By abstracting away the prices in this way,\nwe can focus on a general design that allows the owner of the machine\nto set the prices dynamically. \n Question: \nQ. What is the purpose of the registers in the FSM?\nQ. \nQ. The registers hold the prices of the items in the machine. \n Answer: "}, {"text": "Title: Design of a Vending Machine P3 \n Text: \nOur machine will not accept pennies, so let's have the FSM keep track of\nhow much money has been inserted as a multiple of 5 cents (one nickel).\nThe table to the right shows five types of coins, their value in \ndollars, and their value in terms of nickels. \n Question: \nQ. How are the item prices set? \n Answer: \n\nA. The item prices are set by the owner of the machine."}, {"text": "Title: Design of a Vending Machine P4 \n Text: The most expensive item in the machine might cost a dollar or two, so\nthe FSM must track at least 20 or 40 nickels of value.  Let's \n Question: \nQ. What is the value of a dime in terms of nickels?\nQ. \nQ. 2 \n Answer: "}, {"text": "Title: Design of a Vending Machine P5 \n Text: \n{l|c|c}\n{c|}{coin type}& value& # of nickels \nnickel&      0.05& 1\ndime&        0.10& 2\nquarter&     0.25& 5\nhalf dollar& 0.50& 10\ndollar&      1.00& 20 \n Question: \nQ. What is the function of a finite state machine?\nQ. \nQ. A finite state machine is used to track the value of coins in a machine. \n Answer: "}, {"text": "Title: Design of a Vending Machine P6 \n Text: decide to\nuse six bits to record the number of nickels, which allows the machine\nto keep track of up to 3.15 (63 nickels).  We call the abstract\nstates { STATE00} through { STATE63}, and refer to a state with\nan inserted value of N nickels as { STATE{<}N{>}}. \n Question: \nQ. What is the coin type for the value of 0.25?\nQ. \nQ. The coin type for the value of 0.25 is a quarter. \n Answer: "}, {"text": "Title: Design of a Vending Machine P7 \n Text: Let's now create a next-state table, as shown at the top of the next page.\nThe user can insert one of the five coin types, or can pick one of the \nthree items.  What should happen if the user inserts more money than the \nFSM can track?  Let's make the FSM reject such coins.  Similarly, if the \nuser tries to buy an item without inserting enough money first, the FSM \nmust reject the request.  For each of the possible input events, we add a \ncondition to separate the FSM states that allow the input event to \nbe processed as the user desires from those states that do not.  For example,\nif the user inserts a quarter, those states with N<59 transition to\nstates with value N+5 and accept the quarter.  Those states with\nN reject the coin and remain in { STATE{<}N{>}}. \n Question: \nQ. Why does the machine need to keep track of the number of nickels? \n Answer: \n\nA. The machine needs to keep track of the number of nickels in order to calculate the correct amount of change to dispense."}, {"text": "Title: Design of a Vending Machine P8 \n Text: \n{c|l|c|l|c|c}\n&&& {|c}{final state}\n&&& {|c}{}& & release \ninitial state& {|c|}{input event}& condition& {|c}& & product \n{ STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none\n{ STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none\n{ STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none\n{ STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none\n{ STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none\n{ STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1\n{ STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2\n{ STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3\n{ STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none \n Question: \nQ. What should happen if the user inserts more money than the FSM can track?\nQ. \nQ. The FSM should reject such coins. \n Answer: "}, {"text": "Title: Design of a Vending Machine P9 \n Text: We can now begin to formalize the I/O for our machine.  Inputs include \ninsertion of coins and selection of items for purchase.  Outputs include\na signal to accept or reject an inserted coin as well as signals to release\neach of the three items. \n Question: \nQ. What is the initial state of the vending machine?\nQ. \nQ. The initial state of the vending machine is that there is no input and no product. \n Answer: "}, {"text": "Title: Design of a Vending Machine P10 \n Text: \nFor input to the FSM, we assume that a coin inserted in any given cycle \nis classified and delivered to our FSM using the three-bit representation \nshown to the right. \n Question: \nQ. What is the output of the machine if a customer attempts to purchase an item without inserting any coins?\nQ. \nQ. The output of the machine would be a signal to reject the purchase. \n Answer: "}, {"text": "Title: Design of a Vending Machine P11 \n Text: For item selection, we assume that the user has access to three buttons,\nB_1, B_2, and B_3, that indicate a desire to purchase the \ncorresponding item. \n Question: \nQ. What is the three-bit representation of a coin? \n Answer: \n\nA.\n\nThe three-bit representation of a coin is the coin's value in cents, represented as a binary number. For example, a quarter would be represented as 010, a dime as 001, and a penny as 000."}, {"text": "Title: Design of a Vending Machine P12 \n Text: For output, the FSM must produce a signal A indicating whether a coin\nshould be accepted.  To control the release of items that have been purchased,\nthe FSM must produce the signals R_1, R_2, and R_3, corresponding\nto the re- \n Question: \nQ. What is the user's desired purchase?\nQ. \nQ. The user's desired purchase is the item indicated by the button that they press. \n Answer: "}, {"text": "Title: Design of a Vending Machine P13 \n Text: \n{l|c}\n{c|}{coin type}& C_2C_1C_0 \nnone&        110\nnickel&      010\ndime&        000\nquarter&     011\nhalf dollar& 001\ndollar&      111 \n Question: \nQ. What is the purpose of the FSM?\nQ. \nQ. The purpose of the FSM is to control the release of items that have been purchased. \n Answer: "}, {"text": "Title: Design of a Vending Machine P14 \n Text: lease of each item.  Since outputs in our class depend only on\nstate, we extend the internal state of the FSM to include bits for each of\nthese output signals.  The output signals go high in the cycle after\nthe inputs that generate them.  Thus, for example, the accept signal A\ncorresponds to a coin inserted in the previous cycle, even if a second\ncoin is inserted in the current cycle.  This meaning must be made clear to\nwhomever builds the mechanical system to return coins. \n Question: \nQ. What is the value of a half dollar coin? \n Answer: \n\nA. The value of a half dollar coin is 50 cents."}, {"text": "Title: Design of a Vending Machine P15 \n Text: Now we are ready to complete the specification.  How many states does the\nFSM have?  With six bits to record money inserted and four bits to \ndrive output signals, we have a total of 1,024 (2^) states!\nSix different coin inputs are possible, and the selection buttons allow\neight possible combinations, giving 48 transitions from each state.\nFortunately, we can use the meaning of the bits to greatly simplify\nour analysis. \n Question: \nQ. What is the meaning of the \"accept signal A\"? \n Answer: \n\nThe accept signal A corresponds to a coin inserted in the previous cycle."}, {"text": "Title: Design of a Vending Machine P16 \n Text: First, note that the current state of the coin accept bit and item\nrelease bits---the four bits of FSM state that control the outputs---have\nno effect on the next state of the FSM.  Thus, we can consider only the\ncurrent amount of money in a given state when thinking about the \ntransitions from the state.  As you have seen, we can further abstract\nthe states using the number N, the number of nickels currently held by \nthe vending machine. \n Question: \nQ. How many coin inputs are possible?\nQ. \nQ. There are six coin inputs possible. \n Answer: "}, {"text": "Title: Design of a Vending Machine P17 \n Text: We must still consider all 48 possible transitions from { STATE{<}N{>}}.\nLooking back at our abstract next-state table, notice that we had only\neight types of input events (not counting ``no input'').  If we\nstrictly prioritize these eight possible events, we can safely ignore\ncombinations.  Recall that we adopted a similar strategy for several \nearlier designs, including the ice cream dispenser in Notes Set 2.2 and\nthe keyless entry system developed in Notes Set 3.1.3. \n Question: \nQ. What does the FSM state control? \n Answer: \n\nA. The FSM state controls the output of the coin accept bit and the item release bit."}, {"text": "Title: Design of a Vending Machine P18 \n Text: We choose to prioritize purchases over new coin insertions, and to \nprioritize item 3 over item 2 over item 1.  These prioritizations\nare strict in the sense that if the user presses B_3, both other\nbuttons are ignored, and any coin inserted is rejected, regardless of\nwhether or not the user can actually purchase item 3 (the machine\nmay not contain enough money to cover the item price). \n Question: \nQ. What is the next state table? \n Answer: \n\nA.\n\nThe next state table is a table that shows the possible transitions from one state to another for a given system. In this case, the table lists all 48 possible transitions from the state { STATE<N>} to another state, based on the eight possible input events."}, {"text": "Title: Design of a Vending Machine P19 \n Text: With the choice of strict prioritization, all transitions from all\nstates become well-defined.  We apply the transition rules in order of\ndecreasing priority,\nwith conditions, and with {don't-cares} for lower-priority inputs. \nFor example, for any of the 16 { STATE50}'s (remember that the four\ncurrent output bits do not affect transitions), the table below lists all\npossible transitions assuming that P_3=60, P_2=10, and P_1=35. \n Question: \nQ. Why do the authors choose to prioritize purchases over new coin insertions? \n Answer: \n\nA. This is so that the user can purchase an item as soon as possible."}, {"text": "Title: Design of a Vending Machine P20 \n Text: {\n{ccccc|ccccc}\n&&&&& {|c}{next state}\ninitial state& B_3& B_2& B_1& C_2C_1C_0& state& A& R_3& R_2& R_1 \n{ STATE50}& 1&x&x& xxx& { STATE50}& 0& 0&0&0\n{ STATE50}& 0&1&x& xxx& { STATE40}& 0& 0&1&0\n{ STATE50}& 0&0&1& xxx& { STATE15}& 0& 0&0&1\n{ STATE50}& 0&0&0& 010& { STATE51}& 1& 0&0&0\n{ STATE50}& 0&0&0& 000& { STATE52}& 1& 0&0&0\n{ STATE50}& 0&0&0& 011& { STATE55}& 1& 0&0&0\n{ STATE50}& 0&0&0& 001& { STATE60}& 1& 0&0&0\n{ STATE50}& 0&0&0& 111& { STATE50}& 0& 0&0&0\n{ STATE50}& 0&0&0& 110& { STATE50}& 0& 0&0&0 \n Question: \nQ. What is the meaning of \"strict prioritization\"? \n Answer: \n\nA. \"Strict prioritization\" means that all transitions from all states become well-defined. In other words, the transition rules are applied in order of decreasing priority, with conditions, and with don't-cares for lower-priority inputs."}, {"text": "Title: Design of a Vending Machine P21 \n Text: }\nNext, we need to choose a state representation.  But this task is \nessentially done: each output bit (A, R_1, R_2, and R_3) is\nrepresented with one bit in the internal representation, and the\nremaining six bits record the number of nickels held by the vending\nmachine using an unsigned representation. \n Question: \nQ. What is the next state when the initial state is STATE50 and the input is B_3B_2B_1C_2C_1C_0?\nQ. \nQ. The next state is STATE51. \n Answer: "}, {"text": "Title: Design of a Vending Machine P22 \n Text: The choice of a numeric representation for the money held is important,\nas it allows us to use an adder to compute the money held in\nthe next state. \n Question: \nQ. How is the number of nickels represented in the internal representation?\nQ. \nQ. The number of nickels is represented with six bits in the internal representation. \n Answer: "}, {"text": "Title: Encoders and Decoders P0 \n Text: Since we chose to prioritize purchases, let's begin by building logic\nto perform state transitions for purchases.  Our first task is to\nimplement prioritization among the three selection buttons.  For this\npurpose, we construct a {4-input} { priority encoder}, which \ngenerates a signal P whenever any of its four input lines is active\nand encodes the index of the highest active input as a two-bit unsigned\nnumber S.   A truth table for our priority encoder appears on the \nleft below, with {K-maps} for each of the output bits on the right. \n Question: \nQ. What is the purpose of an FSM?\nQ. \nQ. An FSM is a machine that can be in one of a finite number of states. It can move from one state to another in response to some inputs, and it can perform some actions when it is in a particular state. \n Answer: "}, {"text": "Title: Encoders and Decoders P1 \n Text: \n{cccc|cc}\nB_3& B_2& B_1& B_0& P& S \n1&x&x&x& 1& 11\n0&1&x&x& 1& 10\n0&0&1&x& 1& 01\n0&0&0&1& 1& 00\n0&0&0&0& 0& xx \n Question: \nQ. What is the output of the vending machine? \n Answer: \n\nA. The output of the vending machine is the item that is dispensed."}, {"text": "Title: Encoders and Decoders P2 \n Text: \nFrom the {K-maps}, we extract the following equations:\n{eqnarray*}\nP &=& B_3 + B_2 + B_1 + B_0\nS_1 &=& B_3 + B_2\nS_0 &=& B_3 + {B_2}B_1\n{eqnarray*}\nwhich allow us to implement our encoder as shown to the right. \n Question: \nQ. What is the purpose of the registers in the FSM?\nQ. \nQ. The registers hold the prices of the items in the machine. \n Answer: "}, {"text": "Title: Encoders and Decoders P3 \n Text: If we connect our buttons B_1, B_2, and B_3 to the priority \nencoder (and feed 0 into the fourth input), it produces a signal P \nindicating that the user is trying to make a purchase and a two-bit\nsignal S indicating which item the user wants. \n Question: \nQ. How are the item prices set? \n Answer: \n\nA. The item prices are set by the owner of the machine."}, {"text": "Title: Encoders and Decoders P4 \n Text: We also need to build logic to control the item release outputs R_1, R_2,\nand R_3.  An item should be released only when it has been selected \n(as indicated by the priority encoder signal S) and the vending machine\nhas enough money.  For now, let's leave aside calculation of the item \nrelease signal, which we call R, and focus on how we can produce the\ncorrect values of R_1, R_2, and R_3 from S and R. \n Question: \nQ. What is the value of a dime in terms of nickels?\nQ. \nQ. 2 \n Answer: "}, {"text": "Title: Encoders and Decoders P5 \n Text: The component to the right is a { decoder} with an enable input.  A \ndecoder takes an input signal---typically one coded as a binary number---and \nproduces one output for each possible value of the signal.  You may\nnotice the similarity with the structure of a mux: when the decoder\nis enabled (EN=1), each of the AND gates produces \n Question: \nQ. What is the function of a finite state machine?\nQ. \nQ. A finite state machine is used to track the value of coins in a machine. \n Answer: "}, {"text": "Title: Encoders and Decoders P6 \n Text: one minterm of the input signal S.  In\nthe mux, each of the inputs is then included in\none minterm's AND gate, and the outputs of all AND gates are ORd together.\nIn the decoder, the AND gate outputs are the outputs of the decoder.\nThus, when enabled, the decoder produces exactly one 1 bit on its outputs.\nWhen not enabled (EN=0), the decoder produces all 0 bits. \n Question: \nQ. What is the coin type for the value of 0.25?\nQ. \nQ. The coin type for the value of 0.25 is a quarter. \n Answer: "}, {"text": "Title: Encoders and Decoders P7 \n Text: We use a decoder to generate the release signals for the vending machine\nby connecting the signal S produced by the \npriority encoder to the decoder's S input and connecting the item\nrelease signal R to the decoder's EN input.  The outputs D_1,\nD_2, and D_3 then correspond to the individual item release \nsignals R_1, R_2, and R_3 for our vending machine. \n Question: \nQ. Why does the machine need to keep track of the number of nickels? \n Answer: \n\nA. The machine needs to keep track of the number of nickels in order to calculate the correct amount of change to dispense."}, {"text": "Title: Vending Machine Implementation P0 \n Text: \nWe are now ready to implement the FSM to handle purchases, as shown to the \nright.  The current number of nickels, N, is stored in a register in the\ncenter of the diagram.  Each cycle, N is fed into a {6-bit} adder,\nwhich subtracts the price of any purchase requested in that cycle.  \n Question: \nQ. What is the purpose of an FSM?\nQ. \nQ. An FSM is a machine that can be in one of a finite number of states. It can move from one state to another in response to some inputs, and it can perform some actions when it is in a particular state. \n Answer: "}, {"text": "Title: Vending Machine Implementation P1 \n Text: Recall that we chose to record item prices in registers.  We avoid the \nneed to negate prices before adding them by storing the negated prices in\nour registers.  Thus, the value of register PRICE1 is -P_1, the\nthe value of register PRICE2 is -P_2, and the\nthe value of register PRICE3 is -P_3. \n Question: \nQ. What is the output of the vending machine? \n Answer: \n\nA. The output of the vending machine is the item that is dispensed."}, {"text": "Title: Vending Machine Implementation P2 \n Text: The priority encoder's S signal is then used to select the value of \none of these three registers (using a {24-to-6} mux) as the second\ninput to the adder. \n Question: \nQ. What is the purpose of the registers in the FSM?\nQ. \nQ. The registers hold the prices of the items in the machine. \n Answer: "}, {"text": "Title: Vending Machine Implementation P3 \n Text: We use the adder to execute a subtraction, so the carry out C_ \nis 1 whenever the value of N is at least as great as the amount \nbeing subtracted.  In that case, the purchase is successful.  The AND\ngate on the left calculates the signal R indicating a successful purchase,\nwhich is then used to select the next value of N using the {12-to-6}\nmux below the adder.   \n Question: \nQ. How are the item prices set? \n Answer: \n\nA. The item prices are set by the owner of the machine."}, {"text": "Title: Vending Machine Implementation P4 \n Text: When no item selection buttons are pushed, P and thus R are both 0, \nand the mux below the adder keeps N unchanged in the next cycle.  \nSimilarly, if P=1 but N is\ninsufficient, C_ and thus R are both 0, and again N does\nnot change.  Only when P=1 and C_=1 is the purchase successful,\nin which case the price is subtracted from N in the next cycle. \n Question: \nQ. What is the value of a dime in terms of nickels?\nQ. \nQ. 2 \n Answer: "}, {"text": "Title: Vending Machine Implementation P5 \n Text: The signal R is also used to enable a decoder that generates the three\nindividual item release outputs.  The correct output is generated based on\nthe decoded S signal from the priority encoder, and all three output\nbits are latched into registers to release the purchased item in the next \ncycle. \n Question: \nQ. What is the function of a finite state machine?\nQ. \nQ. A finite state machine is used to track the value of coins in a machine. \n Answer: "}, {"text": "Title: Vending Machine Implementation P6 \n Text: One minor note on the design so far: by hardwiring C_ to 0, we created \na problem for items that cost nothing (0 nickels): in that case, C_ is\nalways 0.  We could instead store\n-P_1-1 in PRICE1 (and so forth) and feed P in to C_, but maybe\nit's better not to allow free items. \n Question: \nQ. What is the coin type for the value of 0.25?\nQ. \nQ. The coin type for the value of 0.25 is a quarter. \n Answer: "}, {"text": "Title: Vending Machine Implementation P7 \n Text: \nHow can we support coin insertion?  Let's use the same adder to add\neach inserted coin's value to N.  The table at the right shows\nthe value of each coin as a {5-bit} unsigned number of nickels.\nUsing this table, we can fill in {K-maps} for each bit of V, as\nshown below.  Notice that we have marked the two undefined bit patterns\nfor the coin type C as don't cares in the {K-maps}. \n Question: \nQ. Why does the machine need to keep track of the number of nickels? \n Answer: \n\nA. The machine needs to keep track of the number of nickels in order to calculate the correct amount of change to dispense."}, {"text": "Title: Vending Machine Implementation P8 \n Text: \n{l|c|c}\n{c|}{coin type}& C_2C_1C_0& V_4V_3V_2V_1V_0 \nnone&        110& 00000\nnickel&      010& 00001\ndime&        000& 00010\nquarter&     011& 00101\nhalf dollar& 001& 01010\ndollar&      111& 10100 \n Question: \nQ. What should happen if the user inserts more money than the FSM can track?\nQ. \nQ. The FSM should reject such coins. \n Answer: "}, {"text": "Title: Vending Machine Implementation P9 \n Text: Solving the {K-maps} gives the following equations, which we\nimplement as shown to the right. \n Question: \nQ. What is the initial state of the vending machine?\nQ. \nQ. The initial state of the vending machine is that there is no input and no product. \n Answer: "}, {"text": "Title: Vending Machine Implementation P10 \n Text: {eqnarray*}\nV_4 &=& C_2C_0\nV_3 &=& {C_1}C_0\nV_2 &=& C_1C_0\nV_1 &=& {C_1}\nV_0 &=& {C_2}C_1\n{eqnarray*} \n Question: \nQ. What is the output of the machine if a customer attempts to purchase an item without inserting any coins?\nQ. \nQ. The output of the machine would be a signal to reject the purchase. \n Answer: "}, {"text": "Title: Vending Machine Implementation P11 \n Text: \nNow we can extend the design to handle coin insertion, as shown to the right\nwith new elements highlighted in blue.  The output of the coin value \ncalculator is extended with a leading 0 and then fed into a {12-to-6} mux.\nWhen a purchase is requested, P=1 and the mux forwards the item price to \nthe adder---recall that we chose to give purchases priority over coin\ninsertion.  When no purchase is requested, the value of any coin inserted\n(or 0 when no coin is inserted) is passed to the adder. \n Question: \nQ. What is the three-bit representation of a coin? \n Answer: \n\nA.\n\nThe three-bit representation of a coin is the coin's value in cents, represented as a binary number. For example, a quarter would be represented as 010, a dime as 001, and a penny as 000."}, {"text": "Title: Vending Machine Implementation P12 \n Text: Two new gates have been added on the lower left.  First, let's verify that\npurchases work as before.  When a purchase is requested, P=1, so the\nNOR gate outputs 0, and the OR gate simple forwards R to control the\nmux that decides whether the purchase was successful, just as in our\noriginal design. \n Question: \nQ. What is the user's desired purchase?\nQ. \nQ. The user's desired purchase is the item indicated by the button that they press. \n Answer: "}, {"text": "Title: Vending Machine Implementation P13 \n Text: When no purchase is made (P=0, and R=0), the adder adds the value of \nany inserted coin to N.  If the addition overflows, C_=1, and\nthe output of the NOR gate is 0.  Note that the NOR gate output is stored\nas the output A in the next cycle, so a coin that causes overflow in the\namount of money stored is rejected.  The OR gate also outputs 0, and N\nremains unchanged.  If the addition does not overflow, the NOR gate\noutputs a 1, the coin is accepted (A=1 in the next cycle), and \nthe mux allows the sum N+V to be written back as the new value of N. \n Question: \nQ. What is the purpose of the FSM?\nQ. \nQ. The purpose of the FSM is to control the release of items that have been purchased. \n Answer: "}, {"text": "Title: Vending Machine Implementation P14 \n Text: The tables at the top of the next page define all of the state variables,\ninputs, outputs, and internal signals used in the design, and list the\nnumber of bits for each variable. \n Question: \nQ. What is the value of a half dollar coin? \n Answer: \n\nA. The value of a half dollar coin is 50 cents."}, {"text": "Title: Vending Machine Implementation P15 \n Text: \n{|ccl|}\n{|r|}{{ FSM state}}\nPRICE1& 6& negated price of item 1 (-P_1)\nPRICE2& 6& negated price of item 2 (-P_2)\nPRICE3& 6& negated price of item 3 (-P_3)\nN& 6& value of money in machine (in nickels)\nA& 1& stored value of accept coin output\nR_1& 1& stored value of release item 1 output\nR_2& 1& stored value of release item 2 output\nR_3& 1& stored value of release item 3 output \n{|r|}{{ internal signals}}\nV& 5& inserted coin value in nickels\nP& 1& purchase requested (from priority encoder)\nS& 2& item # requested (from priority encoder)\nR& 1& release item (purchase approved)  \n Question: \nQ. What is the meaning of the \"accept signal A\"? \n Answer: \n\nThe accept signal A corresponds to a coin inserted in the previous cycle."}, {"text": "Title: Vending Machine Implementation P16 \n Text: \n{|ccl|}\n{|r|}{{ inputs}}\nB_1& 1& item 1 selected for purchase\nB_2& 1& item 2 selected for purchase\nB_3& 1& item 3 selected for purchase\nC& 3& coin inserted (see earlier table \n& & for meaning) \n{|r|}{{ outputs}}\nA& 1& accept inserted coin (last cycle)\nR_1& 1& release item 1\nR_2& 1& release item 2\nR_3& 1& release item 3\n{|l|}{{ Note that outputs correspond one-to-one}}\n{|l|}{{ with four bits of FSM state.}}  \n Question: \nQ. How many coin inputs are possible?\nQ. \nQ. There are six coin inputs possible. \n Answer: "}, {"text": "Title: Design of a Game Controller P0 \n Text: For the next example, imagine that you are part of a team building a\ngame for children to play at Engineering Open House. \n Question: \nQ. What is the purpose of an FSM?\nQ. \nQ. An FSM is a machine that can be in one of a finite number of states. It can move from one state to another in response to some inputs, and it can perform some actions when it is in a particular state. \n Answer: "}, {"text": "Title: Design of a Game Controller P1 \n Text: The game revolves around an old logic problem in which a farmer must\ncross a river in order to reach the market.  The farmer is traveling\nto the market to sell a fox, a goose, and some corn.  The farmer has\na boat, but the boat is only large enough to carry the fox, the goose,\nor the corn along with the farmer.  The farmer knows that if he leaves\nthe fox alone with the goose, the fox will eat the goose.  Similarly,\nif the farmer leaves the goose alone with the corn, the goose will \neat the corn. \n Question: \nQ. What is the output of the vending machine? \n Answer: \n\nA. The output of the vending machine is the item that is dispensed."}, {"text": "Title: Design of a Game Controller P2 \n Text: How can the farmer cross the river? \n Question: \nQ. What is the purpose of the registers in the FSM?\nQ. \nQ. The registers hold the prices of the items in the machine. \n Answer: "}, {"text": "Title: Design of a Game Controller P3 \n Text: Your team decides to build a board illustrating the problem with\na river from top to bottom and lights illustrating the positions of \nthe farmer (always with the boat), the fox, the goose, and the\ncorn on either the left bank or the right bank of the river.\nEverything starts on the left bank, and the children can play \nthe game until they win by getting everything to the right bank or\nuntil they make a mistake. \n Question: \nQ. How are the item prices set? \n Answer: \n\nA. The item prices are set by the owner of the machine."}, {"text": "Title: Design of a Game Controller P4 \n Text: As the ECE major on your team, you get to design the FSM! \n Question: \nQ. What is the value of a dime in terms of nickels?\nQ. \nQ. 2 \n Answer: "}, {"text": "Title: Design of a Game Controller P5 \n Text: {choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates} \n Question: \nQ. What is the function of a finite state machine?\nQ. \nQ. A finite state machine is used to track the value of coins in a machine. \n Answer: "}, {"text": "Title: Design of a Game Controller P6 \n Text: Since the four entities (farmer, fox, goose, and corn) can be only\non one bank or the other, we can use one bit to represent the location\nof each entity.  Rather than giving the states names, let's just\ncall a state FXGC.  The value of F represents the location of the farmer,\neither on the left bank (F=0) or the right bank (F=1).  Using\nthe same representation (0 for the left bank, 1 for the right bank),\nthe value of X represents the location of the fox, G represents the\nlocation of the goose, and C represents the location of the corn. \n Question: \nQ. What is the coin type for the value of 0.25?\nQ. \nQ. The coin type for the value of 0.25 is a quarter. \n Answer: "}, {"text": "Title: Design of a Game Controller P7 \n Text: \nWe can now put together an abstract next-state table, as shown to\nthe right.  Once the player wins or loses, let's have the game indicate\ntheir final status and stop accepting requests to have the farmer cross\nthe river.  We can use a reset button to force the game back into the\noriginal state for the next player. \n Question: \nQ. Why does the machine need to keep track of the number of nickels? \n Answer: \n\nA. The machine needs to keep track of the number of nickels in order to calculate the correct amount of change to dispense."}, {"text": "Title: Design of a Game Controller P8 \n Text: Note that we have included conditions for some of the input events, as \nwe did previously \n Question: \nQ. What should happen if the user inserts more money than the FSM can track?\nQ. \nQ. The FSM should reject such coins. \n Answer: "}, {"text": "Title: Design of a Game Controller P9 \n Text: \n{c|l|c|c}\ninitial state& {|c|}{input event}& condition& final state \nFXGC& no input& always& FXGC\nFXGC& reset & always& 0000\nFXGC& cross alone& always& XGC\nFXGC& cross with fox& F=X& GC\nFXGC& cross with goose& F=G& XC\nFXGC& cross with corn& F=C& XG \n Question: \nQ. What is the initial state of the vending machine?\nQ. \nQ. The initial state of the vending machine is that there is no input and no product. \n Answer: "}, {"text": "Title: Design of a Game Controller P10 \n Text: with the vending machine design. \n Question: \nQ. What is the output of the machine if a customer attempts to purchase an item without inserting any coins?\nQ. \nQ. The output of the machine would be a signal to reject the purchase. \n Answer: "}, {"text": "Title: Design of a Game Controller P11 \n Text: The conditions here require that the farmer be on the same bank as any\nentity that the player wants the farmer to carry across the river. \n Question: \nQ. What is the three-bit representation of a coin? \n Answer: \n\nA.\n\nThe three-bit representation of a coin is the coin's value in cents, represented as a binary number. For example, a quarter would be represented as 010, a dime as 001, and a penny as 000."}, {"text": "Title: Design of a Game Controller P12 \n Text: Next, we specify the I/O interface.  \n Question: \nQ. What is the user's desired purchase?\nQ. \nQ. The user's desired purchase is the item indicated by the button that they press. \n Answer: "}, {"text": "Title: Design of a Game Controller P13 \n Text: For input, the game has five buttons.  A reset button R forces the\nFSM back into the initial state.  The other four buttons cause the\nfarmer to cross the river: B_F crosses alone, B_X with the fox,\nB_G with the goose, and B_C with the corn. \n Question: \nQ. What is the purpose of the FSM?\nQ. \nQ. The purpose of the FSM is to control the release of items that have been purchased. \n Answer: "}, {"text": "Title: Design of a Game Controller P14 \n Text: For output, we need position indicators for the four entities, but let's\nassume that we can simply output the current state FXGC and have\nappropriate images or lights appear on the correct banks of the \nriver.  We also need two more indicators: W for reaching the winning\nstate, and L for reaching a losing state. \n Question: \nQ. What is the value of a half dollar coin? \n Answer: \n\nA. The value of a half dollar coin is 50 cents."}, {"text": "Title: Design of a Game Controller P15 \n Text: Now we are ready to complete the specification.  We could use a strict\nprioritization of input events, as we did with earlier examples.  Instead,\nin order to vary the designs a bit, we use a strict prioritization among \nallowed inputs.  The reset button R has the highest priority, followed\nby B_F, B_C, B_G, and finally B_X.  However, only those buttons \nthat result in an allowed move are considered when selecting one button \namong several pressed in a single clock cycle. \n Question: \nQ. What is the meaning of the \"accept signal A\"? \n Answer: \n\nThe accept signal A corresponds to a coin inserted in the previous cycle."}, {"text": "Title: Design of a Game Controller P16 \n Text: \nAs an example, consider the \nstate FXGC=0101.  The farmer is not on the same bank as the fox, nor\nas the corn, so the B_X and B_C buttons are ignored, leading to the\nnext-state table to the right.  Notice that B_G is accepted even if B_C\nis pressed because the farmer is not on the same \n Question: \nQ. How many coin inputs are possible?\nQ. \nQ. There are six coin inputs possible. \n Answer: "}, {"text": "Title: Design of a Game Controller P17 \n Text: \n{cccccc|c}\nFXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+ \n0101& 1& x& x& x& x& 0000\n0101& 0& 1& x& x& x& 1101\n0101& 0& 0& x& 1& x& 1111\n0101& 0& 0& x& 0& x& 0101 \n Question: \nQ. What does the FSM state control? \n Answer: \n\nA. The FSM state controls the output of the coin accept bit and the item release bit."}, {"text": "Title: Design of a Game Controller P18 \n Text: bank as the corn.\nAs shown later, this approach to prioritization\nof inputs is equally simple in terms of implementation.   \n Question: \nQ. What is the next state table? \n Answer: \n\nA.\n\nThe next state table is a table that shows the possible transitions from one state to another for a given system. In this case, the table lists all 48 possible transitions from the state { STATE<N>} to another state, based on the eight possible input events."}, {"text": "Title: Design of a Game Controller P19 \n Text: \nRecall that we want to stop the game when the player wins or loses.\nIn these states, only the reset button is accepted.  For example, the \nstate FXGC=0110 is a losing state because \n Question: \nQ. Why do the authors choose to prioritize purchases over new coin insertions? \n Answer: \n\nA. This is so that the user can purchase an item as soon as possible."}, {"text": "Title: Design of a Game Controller P20 \n Text: \n{cccccc|c}\nFXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+ \n0110& 1& x& x& x& x& 0000\n0110& 0& x& x& x& x& 0110 \n Question: \nQ. What is the meaning of \"strict prioritization\"? \n Answer: \n\nA. \"Strict prioritization\" means that all transitions from all states become well-defined. In other words, the transition rules are applied in order of decreasing priority, with conditions, and with don't-cares for lower-priority inputs."}, {"text": "Title: Design of a Game Controller P21 \n Text: the farmer\nhas left the fox with the goose on the opposite side of the river.\nIn this case, the player can reset the game, but other buttons\nare ignored. \n Question: \nQ. What is the next state when the initial state is STATE50 and the input is B_3B_2B_1C_2C_1C_0?\nQ. \nQ. The next state is STATE51. \n Answer: "}, {"text": "Title: Design of a Game Controller P22 \n Text: As we have already chosen a representation, we now move on to implement\nthe FSM.  Let's begin by calculating the next state ignoring the\nreset button and the winning and losing states, as shown in the logic \ndiagram below. \n Question: \nQ. How is the number of nickels represented in the internal representation?\nQ. \nQ. The number of nickels is represented with six bits in the internal representation. \n Answer: "}, {"text": "Title: Design of a Game Controller P23 \n Text: The left column of XOR gates determines whether the farmer is on the same\nbank as the corn (top gate), goose (middle gate), and fox (bottom gate).\nThe output of each gate is then used to mask out the corresponding button:\nonly when the farmer is on the same bank are these buttons considered.\nThe adjusted button values are then fed into a priority encoder, which\nselects the highest priority input event according to the scheme that\nwe outlined earlier (from highest to lowest, B_F, B_C, B_G, and\nB_X, ignoring the reset button).   \n Question: \nQ. What is the purpose of the numeric representation of money?\nQ. \nQ. The numeric representation of money is important because it allows us to use an adder to compute the money held in the next state. \n Answer: "}, {"text": "Title: Design of a Game Controller P24 \n Text: The output of the priority encoder is then used to drive another column\nof XOR gates on the right in order to calculate the next state.\nIf any of the allowed buttons is pressed, the priority encoder \noutputs P=1, and the farmer's bank is changed.  If B_C is allowed\nand selected by the priority encoder (only when B_F is not pressed),\nboth the farmer and the corn's banks are flipped.  The goose and the fox\nare handled in the same way. \n Question: \nQ. What is a priority encoder?\nQ. \nQ. A priority encoder is a device that generates a signal whenever any of its input lines is active, and encodes the index of the highest active input as a two-bit unsigned number. \n Answer: "}, {"text": "Title: Design of a Game Controller P25 \n Text: \nNext, let's build a component to produce the win and lose signals.\nThe one winning state is FXGC=1111, so we simply need an AND gate.\nFor the lose signal L, we can fill in a {K-map} and derive an \nexpression, as shown to the right, then implement as shown in the logic\ndiagram to the far right.  For the {K-map}, remember that the player\nloses whenever the fox and the goose are on the same side of the river,\nbut opposite from the farmer, or whenever the goose and the corn are on\nthe same side of the river, but opposite from the farmer. \n Question: \nQ. How does the parity bit work in this code? \n Answer: \n\nParity bit P is used to check whether the number of 1s in the code is even or odd. If the number of 1s is even, then P = 1. If the number of 1s is odd, then P = 0."}, {"text": "Title: Design of a Game Controller P26 \n Text: {eqnarray*}\nL &=& F   +  X G +\n&& F   +  G C\n{eqnarray*} \n Question: \nQ. What are the Boolean equations for the S_1 and S_0 output signals? \n Answer: \n\nA. S_1 = B_3 + B_2, S_0 = B_3 + B_2B_1"}, {"text": "Title: Design of a Game Controller P27 \n Text: \nFinally, we complete our design by integrating the next-state\ncalculation and the win-lose calculation with a couple of muxes, as\nshown to the right. \n Question: \nQ. Why does the priority encoder only need three inputs? \n Answer: \n\nA. The priority encoder only needs three inputs because it is only trying to determine which button the user pressed. It does not need to know about the fourth button."}, {"text": "Title: Design of a Game Controller P28 \n Text: The lower mux controls the final value of the next state: note that\nit selects between the constant state FXGC=0000 when the reset button R\nis pressed and the output of the upper mux when R=0. \n Question: \nQ. What signal controls the release of items from the vending machine? \n Answer: \n\nA. The signal R, which indicates that an item has been selected and that the vending machine has enough money, controls the release of items from the vending machine."}, {"text": "Title: Design of a Game Controller P29 \n Text: The upper mux is controlled by W+L, and retains the current state\nwhenever either signal is 1.  In other words, once the player has won\nor lost, the upper mux prevents further state changes until the reset\nbutton is pressed. \n Question: \nQ. What is a decoder? \n Answer: \n\nA. A decoder is a component that takes an input signal and produces one output for each possible value of the signal."}, {"text": "Title: Design of a Game Controller P30 \n Text: When R, W, and L are all 0, the next state is calculated\naccording to whatever buttons have been pressed. \n Question: \nQ. Why does the decoder produce all 0 bits when the enable signal is not asserted? \n Answer: \n\nA. When the enable signal is not asserted, none of the inputs to the decoder are active, and therefore the output of all AND gates in the decoder will be 0."}, {"text": "Title: Analysis of a Stoplight Controller P0 \n Text: In this example, we begin with a digital FSM design and analyze it to\nunderstand how it works and to verify that its behavior is appropriate. \n Question: \nQ. What is the purpose of an FSM?\nQ. \nQ. An FSM is a machine that can be in one of a finite number of states. It can move from one state to another in response to some inputs, and it can perform some actions when it is in a particular state. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P1 \n Text: The FSM that we analyze has been designed to control the stoplights\nat the intersection of two roads.  For naming purposes, we assume that one\nof the roads runs East and West (EW), and the second runs North and \nSouth (NS).   \n Question: \nQ. What is the output of the vending machine? \n Answer: \n\nA. The output of the vending machine is the item that is dispensed."}, {"text": "Title: Analysis of a Stoplight Controller P2 \n Text: The stoplight controller has two inputs, each of which \nsenses vehicles approaching from either direction on one of the two roads.\nThe input V^=1 when a vehicle approaches from either the East or the\nWest, and the input V^=1 when a vehicle approaches from either the\nNorth or the South.  These inputs are also active when vehicles are stopped\nwaiting at the corresponding lights. \n Question: \nQ. What is the purpose of the registers in the FSM?\nQ. \nQ. The registers hold the prices of the items in the machine. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P3 \n Text: Another three inputs, A, B, and C, control the timing behavior of the\nsystem; we do not discuss them here except as variables. \n Question: \nQ. How are the item prices set? \n Answer: \n\nA. The item prices are set by the owner of the machine."}, {"text": "Title: Analysis of a Stoplight Controller P4 \n Text: \nThe outputs of the controller consist of two {2-bit} values, \nL^ and L^, that specify the light colors for the two roads.\nIn particular, L^ controls the lights facing East and West, and\nL^ controls the lights facing North and South.  The meaning of these\noutputs is given in the table to the right. \n Question: \nQ. What is the value of a dime in terms of nickels?\nQ. \nQ. 2 \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P5 \n Text: \n{c|c}\nL& light color \n0x& red\n10& yellow\n11& green \n Question: \nQ. What is the function of a finite state machine?\nQ. \nQ. A finite state machine is used to track the value of coins in a machine. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P6 \n Text: Let's think about the basic operation of the controller. \n Question: \nQ. What is the coin type for the value of 0.25?\nQ. \nQ. The coin type for the value of 0.25 is a quarter. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P7 \n Text: For safety reasons, the controller must ensure that the lights on one\nor both roads are red at all times.   \n Question: \nQ. Why does the machine need to keep track of the number of nickels? \n Answer: \n\nA. The machine needs to keep track of the number of nickels in order to calculate the correct amount of change to dispense."}, {"text": "Title: Analysis of a Stoplight Controller P8 \n Text: Similarly, if a road has a green light, the controller should \nshow a yellow light before showing a red light to give drivers some\nwarning and allow them to slow down. \n Question: \nQ. What should happen if the user inserts more money than the FSM can track?\nQ. \nQ. The FSM should reject such coins. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P9 \n Text: Finally, for fairness, the controller should alternate green lights\nbetween the two roads. \n Question: \nQ. What is the initial state of the vending machine?\nQ. \nQ. The initial state of the vending machine is that there is no input and no product. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P10 \n Text: Now take a look at the logic diagram below. \n Question: \nQ. What is the output of the machine if a customer attempts to purchase an item without inserting any coins?\nQ. \nQ. The output of the machine would be a signal to reject the purchase. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P11 \n Text: The state of the FSM has been split into two pieces: a {3-bit} \nregister S and a {6-bit} timer.  The timer is simply a binary \ncounter that counts downward and produces an output of Z=1 when it \nreaches 0.  Notice that the register S only takes a new value\nwhen the timer reaches 0, and that the Z signal from the timer\nalso forces a new value to be loaded into the timer in the next \ncycle.  We can thus think of transitions in the FSM on a cycle by \ncycle basis as consisting of two types.  The first type simply\ncounts downward for a number of cycles while holding the register S\nconstant, while the second changes the value of S and sets the\ntimer in order to maintain the new value of S \nfor some number of cycles. \n Question: \nQ. What is the three-bit representation of a coin? \n Answer: \n\nA.\n\nThe three-bit representation of a coin is the coin's value in cents, represented as a binary number. For example, a quarter would be represented as 010, a dime as 001, and a penny as 000."}, {"text": "Title: Analysis of a Stoplight Controller P12 \n Text: \nLet's look at the next-state logic for S, which feeds into the IN\ninputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice \nthat none of the inputs to the FSM directly affect these values.  The\nstates of S thus act like a counter.  By examining the connections,\nwe can derive equations for the next state and draw a transition\ndiagram, as shown to the right. \n Question: \nQ. What is the user's desired purchase?\nQ. \nQ. The user's desired purchase is the item indicated by the button that they press. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P13 \n Text: As the figure shows, there are six states in the loop defined by the \nnext-state logic, with the two remaining states converging into the\nloop after a single cycle. \n Question: \nQ. What is the purpose of the FSM?\nQ. \nQ. The purpose of the FSM is to control the release of items that have been purchased. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P14 \n Text: Let's now examine the outputs for each\nstate in order to understand how the stoplight sequencing\nworks. \n Question: \nQ. What is the value of a half dollar coin? \n Answer: \n\nA. The value of a half dollar coin is 50 cents."}, {"text": "Title: Analysis of a Stoplight Controller P15 \n Text: We derive equations for the outputs that control the lights, as shown\nto the right, then calculate values and colors for each\nstate, as shown to the far right.  For completeness, the table \nincludes the states outside of the desired loop.  The \nlights are all red in both of these states, which is necessary for safety. \n Question: \nQ. What is the meaning of the \"accept signal A\"? \n Answer: \n\nThe accept signal A corresponds to a coin inserted in the previous cycle."}, {"text": "Title: Analysis of a Stoplight Controller P16 \n Text: S_2^+ &=& {S_2} + S_0\nS_1^+ &=& {S_2}  S_1\nS_0^+ &=& {S_2}\n{eqnarray*} \n Question: \nQ. How many coin inputs are possible?\nQ. \nQ. There are six coin inputs possible. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P17 \n Text: {eqnarray*}\nL_1^ &=& S_2 S_1\nL_0^ &=& S_0\nL_1^ &=& S_2 {S_1}\nL_0^ &=& S_0\n{eqnarray*} \n Question: \nQ. What does the FSM state control? \n Answer: \n\nA. The FSM state controls the output of the coin accept bit and the item release bit."}, {"text": "Title: Analysis of a Stoplight Controller P18 \n Text: {c|cc|cc}\n&&& EW& NS\n&&& light& light\nS& L^& L^& color& color \n000& 00& 00&    red&    red\n111& 11& 01&  green&    red\n110& 10& 00& yellow&    red\n010& 00& 00&    red&    red\n101& 01& 11&    red&  green\n100& 00& 10&    red& yellow \n001& 01& 01&    red&    red\n011& 01& 01&    red&    red \n Question: \nQ. What is the next state table? \n Answer: \n\nA.\n\nThe next state table is a table that shows the possible transitions from one state to another for a given system. In this case, the table lists all 48 possible transitions from the state { STATE<N>} to another state, based on the eight possible input events."}, {"text": "Title: Analysis of a Stoplight Controller P19 \n Text: \nNow let's think about how the timer works.  As we already noted, the\ntimer value is set whenever S enters a new state, but it can also be\nset under other conditions---in particular, by the signal F calculated\nat the bottom of the FSM logic diagram.   \n Question: \nQ. Why do the authors choose to prioritize purchases over new coin insertions? \n Answer: \n\nA. This is so that the user can purchase an item as soon as possible."}, {"text": "Title: Analysis of a Stoplight Controller P20 \n Text: \nFor now, assume that F=0.  In this case, the timer is set only when\nthe state S changes, and we can find the duration of each state by\nanalyzing the muxes.  The bottom mux selects A when S_2=0, and \nselects the output of the top mux when S_2=1.  The top mux selects B\nwhen S_0=1, and selects C when S_0=0.  Combining these results,\nwe can calculate the duration of the next states of S when F=0, \nas shown in the table to the right.  We can then combine the next\nstate duration with our previous calculation of the state sequencing \n(also the order in the table) to obtain the durations of each state, also\nshown in the rightmost column of the table. \n Question: \nQ. What is the meaning of \"strict prioritization\"? \n Answer: \n\nA. \"Strict prioritization\" means that all transitions from all states become well-defined. In other words, the transition rules are applied in order of decreasing priority, with conditions, and with don't-cares for lower-priority inputs."}, {"text": "Title: Analysis of a Stoplight Controller P21 \n Text: \n{c|cc|cc}\n& EW& NS& next& current\n& light& light& state& state\nS& color& color& duration& duration \n000&    red&    red& A& C\n111&  green&    red& B& A\n110& yellow&    red& C& B\n010&    red&    red& A& C\n101&    red&  green& B& A\n100&    red& yellow& C& B \n001&    red&    red& A& ---\n011&    red&    red& A& --- \n Question: \nQ. What is the next state when the initial state is STATE50 and the input is B_3B_2B_1C_2C_1C_0?\nQ. \nQ. The next state is STATE51. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P22 \n Text: What does F do?  Analyzing the gates that produce it gives \nF=S_1S_0{V^+{S_1}S_0{V^.  If we \nignore the two states outside of the main loop for S, the first term \nis 1 only when the lights are green on the East and West roads and the \ndetector for the North and South roads indicates that no vehicles are \napproaching.  Similarly, the second term is 1 only when the lights are \ngreen on the North and South roads and the detector for the East and \nWest roads indicates that no vehicles are approaching. \n Question: \nQ. How is the number of nickels represented in the internal representation?\nQ. \nQ. The number of nickels is represented with six bits in the internal representation. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P23 \n Text: What happens when F=1?  First, the OR gate feeding into the timer's\nLD input produces a 1, meaning that the timer loads a new value\ninstead of counting down.  Second, the OR gate controlling the lower\nmux selects the A input.  In other words, the timer is reset to A\ncycles, corresponding to the initial value for the green light states.\nIn other words, the light stays green until vehicles approach on \nthe other road, plus A more cycles. \n Question: \nQ. What is the purpose of the numeric representation of money?\nQ. \nQ. The numeric representation of money is important because it allows us to use an adder to compute the money held in the next state. \n Answer: "}, {"text": "Title: Analysis of a Stoplight Controller P24 \n Text: Unfortunately, the signal F may also be 1 in the unused states of S,\nin which case the lights on both roads may remain red even though cars\nare waiting on one of the roads.  To avoid this behavior, we must be \nsure to initialize the state S to one of the six states in the\ndesired loop. \n Question: \nQ. What is a priority encoder?\nQ. \nQ. A priority encoder is a device that generates a signal whenever any of its input lines is active, and encodes the index of the highest active input as a two-bit unsigned number. \n Answer: "}, {"text": "Title: Random Access Memories P0 \n Text: This set of notes describes random access memories (RAMs), providing slightly\nmore detail than is available in the textbook.  We begin with a discussion\nof the memory abstraction and the types of memory most commonly used in\ndigital systems, then examine how one can build memories (static RAMs) \nusing logic.  We next introduce tri-state buffers as a way of simplifying\nouput connections, and illustrate how memory chips can be combined to\nprovide larger and wider memories.  A more detailed description of dynamic \nRAMs finishes this set. \n Question: \nQ. What is the difference between static and dynamic RAMs? \n Answer: \n\nA. Static RAMs are built using logic gates and require a power supply to maintain their data, while dynamic RAMs use capacitors to store data and only require a power supply when they are being accessed."}, {"text": "Title: Random Access Memories P1 \n Text: { Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.} \n Question: \nQ. What is the difference between a \"solar eclipse\" and a \"lunar eclipse\"? \n Answer: \n\nA. A \"solar eclipse\" is when the moon passes in front of the sun, and a \"lunar eclipse\" is when the earth passes in front of the moon."}, {"text": "Title: Memory P0 \n Text: A computer { memory} is a group of storage elements and the logic\nnecessary to move data in and out of the elements.  The size of the\nelements in a memory---called the { addressability} of the \nmemory---varies from a single binary digit, or { bit},\nto a { byte} (8 bits) or more.  Typically, we refer to data\nelements larger than a byte as { words}, but the size of a word\ndepends on context.  \n Question: \nQ. What is the difference between static and dynamic RAMs? \n Answer: \n\nA. Static RAMs are built using logic gates and require a power supply to maintain their data, while dynamic RAMs use capacitors to store data and only require a power supply when they are being accessed."}, {"text": "Title: Memory P1 \n Text: Each element in a memory is assigned a unique name, called an {\naddress}, that allows an external circuit to identify the particular\nelement of interest.  These addresses are not unlike the street\naddresses that you use when you send a letter.  Unlike street\naddresses, however, memory addresses usually have little or no\nredundancy; each possible combination of bits in an address identifies\na distinct set of bits in the memory.  The figure on the right below \nillustrates the concept.  Each house represents a storage element and \nis associated with a unique address. \n Question: \nQ. What is the difference between a \"solar eclipse\" and a \"lunar eclipse\"? \n Answer: \n\nA. A \"solar eclipse\" is when the moon passes in front of the sun, and a \"lunar eclipse\" is when the earth passes in front of the moon."}, {"text": "Title: Memory P2 \n Text: The memories that we consider in this class have several properties in\ncommon.  These memories support two operations: { write} places a\nword of data into an element, and { read} retrieves a copy of a\nword of data from an element.  The memories are also { volatile},\nwhich means that the data held by a memory are erased when electrical\npower is turned off or fails.  { Non-volatile} forms of memory\ninclude magnetic and optical storage media such as DVDs, CD-ROMs, disks, \nand tapes, capacitive storage media such as Flash drives,\nand some programmable logic devices.\nFinally, the memories considered in this class are { random access\nmemories (RAMs)}, which means that the time required to access an\nelement in the memory is independent of the element being accessed.\nIn contrast, { serial memories} such as magnetic tape require much\nless time to access data near the current location in the tape than\ndata far away from the current location. \n Question: \nQ. What is the difference between a memory and storage? \n Answer: \n\nA. Memory typically refers to data elements that can be accessed quickly, while storage usually refers to data that is stored for longer periods of time."}, {"text": "Title: Memory P3 \n Text: The figure on the left above shows a generic RAM structure.  The\nmemory contains 2^k elements of N bits each.  A {k-bit}\naddress input, ADDR, identifies the memory element of interest for\nany particular operation.  The write enable\ninput, WE, selects the operation to be performed: if\nWE is high, the operation is a write; if it is low, the\noperation is a read.  Data to be written into an element are provided\nthrough N inputs at the top, and data read from an element appear on\nN outputs at the bottom.  Finally, a { chip select} input, CS,\nfunctions as an enable control for the memory; when CS is low, the\nmemory neither reads nor writes any location. \n Question: \nQ. How are memory addresses different from street addresses?\nQ. \nQ. One key difference is that memory addresses usually have little or no redundancy, meaning each possible combination of bits in an address identifies a distinct set of bits in memory. In contrast, street addresses often have some degree of redundancy built in (for example, using directional indicators or abbreviations), which can help reduce errors when entering an address. \n Answer: "}, {"text": "Title: Memory P4 \n Text: Random access memory further divides into two important types: {\nstatic RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}.\nSRAM employs active logic in the form of a two-inverter loop to\nmaintain stored values.  DRAM uses a charged capacitor to store a bit;\nthe charge drains over time and must be replaced, giving rise to the\nqualifier ``dynamic.''  ``Static'' thus serves only to differentiate\nmemories with active logic elements from those with capacitive\nelements.  Both types are volatile, that is, both lose all data when the\npower supply is removed.  We consider both SRAM and DRAM \nin this course, but the details of DRAM operation are beyond our scope.  \n Question: \nQ. What is the difference between volatile and non-volatile memories? \n Answer: \n\nVolatile memories are memories that are erased when electrical power is turned off or fails. Non-volatile memories are memories that are not erased when electrical power is turned off or fails."}, {"text": "Title: Static Random Access Memory P0 \n Text: Static random access memory is used for high-speed applications such\nas processor caches and some embedded designs.  As SRAM bit\n{density---the} number of bits in a given chip {area---is}\nsignificantly lower than DRAM bit density, most applications with less\ndemanding speed requirements use DRAM.  The main memory in most\ncomputers, for example, is DRAM, whereas the memory on the same chip\nas a processor is SRAM.{Chips combining both DRAM and processor\nlogic are available, and are used by some processor manufacturers (such \nas IBM).  Research is underway to couple such logic types more efficiently\nby building 3D stacks of chips.}  DRAM is also unavailable\nwhen recharging its capacitors, which can be a problem for\napplications with stringent real-time needs. \n Question: \nQ. What is the difference between static and dynamic RAMs? \n Answer: \n\nA. Static RAMs are built using logic gates and require a power supply to maintain their data, while dynamic RAMs use capacitors to store data and only require a power supply when they are being accessed."}, {"text": "Title: Static Random Access Memory P1 \n Text: \nA diagram of an SRAM { cell} (a single bit) appears to\nthe right.  A dual-inverter loop stores the bit, and is connected\nto opposing BIT lines through transistors controlled by a SELECT\nline.   \n Question: \nQ. What is the difference between a \"solar eclipse\" and a \"lunar eclipse\"? \n Answer: \n\nA. A \"solar eclipse\" is when the moon passes in front of the sun, and a \"lunar eclipse\" is when the earth passes in front of the moon."}, {"text": "Title: Static Random Access Memory P2 \n Text: The cell works as follows.  When SELECT is high, the\ntransistors connect the inverter loop to the bit lines.  When writing\na cell, the bit lines are held at opposite logic values, forcing the\ninverters to match the values on the lines and storing the value from\nthe BIT input.  When reading a cell, the bit lines are disconnected\nfrom other logic, allowing the inverters to drive the lines with\ntheir current outputs.   \n Question: \nQ. What is the difference between a memory and storage? \n Answer: \n\nA. Memory typically refers to data elements that can be accessed quickly, while storage usually refers to data that is stored for longer periods of time."}, {"text": "Title: Static Random Access Memory P3 \n Text: The value stored previously is thus copied onto\nthe BIT line as an output, and the opposite value is placed on the\n line.  When SELECT is low, the transistors\ndisconnect the inverters from the bit lines, and the cell\nholds its current value until SELECT goes high again. \n Question: \nQ. How are memory addresses different from street addresses?\nQ. \nQ. One key difference is that memory addresses usually have little or no redundancy, meaning each possible combination of bits in an address identifies a distinct set of bits in memory. In contrast, street addresses often have some degree of redundancy built in (for example, using directional indicators or abbreviations), which can help reduce errors when entering an address. \n Answer: "}, {"text": "Title: Static Random Access Memory P4 \n Text: The actual operation of an SRAM cell is more complicated than we\nhave described.  For example, when writing a bit, the BIT lines \ncan temporarily connect high voltage to ground (a short).  The \ncircuit must be designed carefully to minimize the power consumed\nduring this process.  When reading a bit, the BIT lines\nare pre-charged halfway between high-voltage and ground, and \nanalog devices called sense amplifiers are used to detect the\nvoltage changes on the BIT lines (driven by the inverter loop)\nas quickly as possible.  These analog design issues are outside of \nthe scope of our class. \n Question: \nQ. What is the difference between volatile and non-volatile memories? \n Answer: \n\nVolatile memories are memories that are erased when electrical power is turned off or fails. Non-volatile memories are memories that are not erased when electrical power is turned off or fails."}, {"text": "Title: Static Random Access Memory P5 \n Text: \nA number of cells are combined into a { bit slice}, as shown to\nthe right. \n Question: \nQ. What does the chip select input do? \n Answer: \n\nA. The CS input enables or disables the memory. When CS is low, the memory is active and can read or write data. When CS is high, the memory is inactive and cannot perform any operations."}, {"text": "Title: Static Random Access Memory P6 \n Text: The labels along the bottom of the figure are external inputs to the \nbit slice, and match the labels for the abstract \n Question: \nQ. What is the difference between SRAM and DRAM? \n Answer: \n\nA. SRAM uses active logic in the form of a two-inverter loop to maintain stored values, while DRAM uses a charged capacitor to store a bit."}, {"text": "Title: Static Random Access Memory P7 \n Text: memory discussed earlier.  The \nbit slice in the figure can be thought of as a {16-address},\n{1-bit-addressable} memory (2^4b). \n Question: \nQ. What are some applications that use SRAM? \n Answer: \n\nA. SRAM is used for high-speed applications such as processor caches and some embedded designs."}, {"text": "Title: Static Random Access Memory P8 \n Text: The cells in a bit slice\nshare bit lines and analog read and write logic, which appears to the\nright in the figure.  Based on the ADDR input, a decoder sets one\ncell's SELECT line high to enable a read or write operation to the\ncell.   \n Question: \nQ. How does the SRAM cell work? \n Answer: \n\nA.\n\nThe SRAM cell works by storing a bit in a dual-inverter loop. The loop is connected to opposing BIT lines through transistors controlled by a SELECT line."}, {"text": "Title: Static Random Access Memory P9 \n Text: The chip select input CS drives the enable input of\nthe decoder, so none of the memory cells is active when chip select is\nlow (CS=0), and exactly one of the memory cells is active when\nchip select is high (CS=1). \n Question: \nQ. When writing a cell, what forces the inverters to match the values on the lines? \n Answer: \n\nA. The bit lines are held at opposite logic values."}, {"text": "Title: Static Random Access Memory P10 \n Text: Actual bit slices can contain many more cells than are shown in the \nfigure---more cells means less extra logic per cell, but slower memory,\nsince longer wires have higher capacitance. \n Question: \nQ. What is the value stored in the cell when SELECT is high? \n Answer: \n\nA. The value stored previously is copied onto the BIT line as an output."}, {"text": "Title: Static Random Access Memory P11 \n Text: A read operation is performed as follows.  We set CS=1 and WE=0,\nand place the address of the cell to be read on the ADDR input.\nThe decoder outputs a 1 on the appropriate cell's SELECT line,\nand the read logic reads the bit from the cell and delivers it\nto its Q output, which is then available on the bit \nslice's {DATA-OUT} output. \n Question: \nQ. What is the function of the sense amplifiers in an SRAM cell? \n Answer: \n\nA. The sense amplifiers are responsible for quickly detecting voltage changes on the BIT lines, which are caused by the inverter loop."}, {"text": "Title: Static Random Access Memory P12 \n Text: For a write operation, we set CS=1 and WE=1.  We again place the\naddress of the cell to be written on the ADDR input and set the\nvalue of the bit slice's {DATA-IN} input to the value to be written\ninto the memory cell.  When the decoder activates the cell's SELECT line,\nthe write logic writes the new value from its D input into\nthe memory cell.  Later reads from that cell then produce the new value. \n Question: \nQ. What is a bit slice? \n Answer: \n\nA bit slice is a number of cells that are combined into a single unit. This unit can then be used to perform various operations on data."}, {"text": "Title: Static Random Access Memory P13 \n Text: The outputs of the cell selection decoder can be used to control\nmultiple bit slices, as shown in the figure above of a {2^6b}\nmemory.  Selection between bit slices is\nthen based on other bits from the address (ADDR).  In the figure\nabove, a {2-to-4} decoder is used to deliver write requests to\none of four bit slices, and a {4-to-1} mux is used to choose\nthe appropriate output bit for read requests. \n Question: \nQ. What are the external inputs to the bit slice? \n Answer: \n\nA. External inputs to the bit slice include the clock signal, data signal, and reset signal."}, {"text": "Title: Static Random Access Memory P14 \n Text: The {4-to-16} decoder now activates one cell in each of the four \nbit slices.  For a read operation, WE=0, and the {2-to-4} decoder \nis not enabled, so it outputs all 0s.  All four bit slices thus perform\nreads, and the desired result bit is forwarded to {DATA-OUT} by the \n{4-to-1} mux.  The tri-state buffer between the mux \nand {DATA-OUT} is explained in a later section. \n Question: \nQ. What is a bit slice? \n Answer: \n\nA bit slice is a section of a computer's memory that can be addressed as a single unit. A bit slice can be thought of as a 16-address, 1-bit-addressable memory."}, {"text": "Title: Static Random Access Memory P15 \n Text: For a write operation, exactly one of the bit\nslices has its WE input set to 1 by the {2-to-4} decoder.\nThat bit slice writes the bit value delivered to all bit slices\nfrom {DATA-IN}.  The other three bit slices perform reads, but their \nresults are simply discarded. \n Question: \nQ. Why are bit slices used? \n Answer: \n\nA. Bit slices are used because they allow for easy read and write operations to a cell."}, {"text": "Title: Static Random Access Memory P16 \n Text: The approach shown above, in which a cell is selected\nthrough a two-dimensional indexing scheme, is known as { coincident\nselection}.  The qualifier ``coincident'' arises from the notion that\nthe desired cell coincides with the intersection of the active row and\ncolumn outputs from the decoders. \n Question: \nQ. What is the function of the chip select input?\nQ. \nQ. The chip select input determines which of the memory cells is active. When chip select is low (CS=0), none of the memory cells are active. When chip select is high (CS=1), exactly one of the memory cells is active. \n Answer: "}, {"text": "Title: Static Random Access Memory P17 \n Text: The benefit of coincident selection is easily calculated in terms of\nthe number of gates required for the decoders.  Decoder complexity is\nroughly equal to the number of outputs, as each output is a minterm\nand requires a unique gate to calculate it.   \n Question: \nQ. How many cells can a bit slice contain?\nQ. \nQ. There is no definite answer to this question since it depends on the size of the cells and the length of the wires. However, it is stated in the text that a bit slice can contain many more cells than are shown in the figure. \n Answer:  Therefore, we can infer that a bit slice can contain at least several hundred cells."}, {"text": "Title: Static Random Access Memory P18 \n Text:  Fanout trees for input terms and inverted terms add relatively few gates.   \n Question: \nQ. What is the effect of setting CS=1 and WE=0? \n Answer: \n\nA.\n\nSetting CS=1 and WE=0 will cause the decoder to output a 1 on the appropriate cell's SELECT line, and the read logic to read the bit from the cell and deliver it to its Q output."}, {"text": "Title: Static Random Access Memory P19 \n Text: Consider a 1M8b RAM chip.  The number of addresses is 2^,\nand the total number of memory cells is 8,388,608 (2^).\nOne option is to use eight bit slices and a {20-to-1,048,576}\ndecoder, or about 2^ gates.  Alternatively, we can use 8,192 bit\nslices of 1,024 cells.  For the second implementation, we need \ntwo {10-to-1024} decoders, or about 2^ gates.  As chip \narea is roughly proportional to the number of gates, the savings are \nsubstantial.  Other schemes are possible as well: if we want a more \nsquare chip area, we might choose to use 4,096 bit slices of 2,048 \ncells along with one {11-to-2048} decoder and\none {9-to-512} decoder.  This approach requires roughly 25 more\ndecoder gates than our previous example, but is still far superior to\nthe eight-bit-slice implementation. \n Question: \nQ. What is the value of the bit slice's {DATA-IN} input? \n Answer: \n\nA. The value of the bit slice's {DATA-IN} input is the value to be written into the memory cell."}, {"text": "Title: Static Random Access Memory P20 \n Text: Memories are typically unclocked devices.  However, as you have seen,\nthe circuits are highly structured, which enables engineers to cope\nwith the complexity of sequential feedback design.  Devices used to\ncontrol memories are typically clocked, and the interaction between\nthe two can be fairly complex.   \n Question: \nQ. What is the purpose of the cell selection decoder?\nQ. \nQ. The cell selection decoder is used to select between different bit slices in a memory. \n Answer:  This allows the memory to be divided into multiple sections, each of which can be independently accessed."}, {"text": "Title: Static Random Access Memory P21 \n Text: \nTiming diagrams for reads and writes\nto SRAM are shown to the right.  A write operation\nappears on the left.  In the first cycle, the controller raises the\nchip select signal and places the memory address to be written on the\naddress inputs.  Once the memory has had time to set up the \nappropriate \n Question: \nQ. What is the purpose of the 4-to-16 decoder?\nQ. \nQ. The 4-to-16 decoder is used to select one of the four bit slices for a read operation. \n Answer: "}, {"text": "Title: Static Random Access Memory P22 \n Text: select lines\ninternally, the WE input is raised, and data are placed\non the data inputs.  The delay, which is specified by the memory\nmanufacturer, is necessary to avoid writing data to the incorrect\nelement within the memory.  The timing shown in the\nfigure rounds this delay up to a single clock cycle, but the\nactual delay needed depends on the clock speed and the memory's \nspecification.  At some point after new data have been\ndelivered to the memory, the write operation completes within the\nmemory.  The time from the application of the address until the\n(worst-case) completion of the write operation is called the {\nwrite cycle} of the memory, and is also specified by the memory \nmanufacturer.  Once the write cycle has passed, the controlling logic \nlowers WE, waits for the change to settle within the memory,\nthen removes the address and lowers the chip select signal.  The\nreason for the delay between these signal changes is the same: to \navoid mistakenly overwriting another memory location. \n Question: \nQ. What is the purpose of the 2-to-4 decoder in this circuit?\nQ. \nQ. The 2-to-4 decoder is used to select one of the four bit slices for a write operation. The other three bit slices perform reads, but their results are discarded. \n Answer: "}, {"text": "Title: Static Random Access Memory P23 \n Text: A read operation is quite similar.  As shown on the right, the\ncontrolling logic places the address on the input lines and raises the\nchip select signal.  No races need be considered, as read operations\non SRAM do not affect the stored data.  After a delay called the {\nread cycle}, the data can be read from the data outputs.  The address\ncan then be removed and the chip select signal lowered. \n Question: \nQ. What is coincident selection? \n Answer: \n\nA. Coincident selection is a method of selecting a cell through a two-dimensional indexing scheme."}, {"text": "Title: Static Random Access Memory P24 \n Text: For both reads and writes, the number of cycles required for an\noperation depends on a combination of the clock cycle of the\ncontroller and the cycle time of the memory.  For example, with a\n25 nanosecond write cycle and a 10 nanosecond clock cycle, a write\nrequires three cycles.  In general, the number of cycles required is\ngiven by the formula {memory cycle time}/{clock cycle\ntime}. \n Question: \nQ. What is the benefit of coincident selection? \n Answer: \n\nA. The benefit of coincident selection is that it reduces the number of gates required for the decoders."}, {"text": "Title: Tri-State Buffers and Combining Chips P0 \n Text: Recall the buffer symbol---a triangle like an inverter, but with no\ninversion bubble---between the mux and the {DATA-OUT} \nsignal of the {2^6b} memory shown earlier.  This \n{ tri-state buffer} serves to disconnect the memory logic \nfrom the output line when the memory is not performing a read.  \n Question: \nQ. What is the difference between static and dynamic RAMs? \n Answer: \n\nA. Static RAMs are built using logic gates and require a power supply to maintain their data, while dynamic RAMs use capacitors to store data and only require a power supply when they are being accessed."}, {"text": "Title: Tri-State Buffers and Combining Chips P1 \n Text: \nAn implementation diagram for a tri-state buffer appears to the right \nalong with the symbolic\nform and a truth table.  The ``Z'' in the truth table output means \nhigh impedance (and is sometimes written ``hi-Z'').  In other words,\nthere is effectively no electrical connection between the tri-state \nbuffer and the output OUT. \n Question: \nQ. What is the difference between a \"solar eclipse\" and a \"lunar eclipse\"? \n Answer: \n\nA. A \"solar eclipse\" is when the moon passes in front of the sun, and a \"lunar eclipse\" is when the earth passes in front of the moon."}, {"text": "Title: Tri-State Buffers and Combining Chips P2 \n Text: This logical disconnection is achieved by using the outer \n Question: \nQ. What is the difference between a memory and storage? \n Answer: \n\nA. Memory typically refers to data elements that can be accessed quickly, while storage usually refers to data that is stored for longer periods of time."}, {"text": "Title: Tri-State Buffers and Combining Chips P3 \n Text: {cc|c}\nEN& IN& OUT \n0& x& Z\n1& 0& 0\n1& 1& 1 \n Question: \nQ. How are memory addresses different from street addresses?\nQ. \nQ. One key difference is that memory addresses usually have little or no redundancy, meaning each possible combination of bits in an address identifies a distinct set of bits in memory. In contrast, street addresses often have some degree of redundancy built in (for example, using directional indicators or abbreviations), which can help reduce errors when entering an address. \n Answer: "}, {"text": "Title: Tri-State Buffers and Combining Chips P4 \n Text: (upper and lower)\npair of transistors in the logic diagram.  When EN=0, both transistors\nturn off, meaning that regardless of the value of IN, OUT is connected\nneither to high voltage nor to ground. \n Question: \nQ. What is the difference between volatile and non-volatile memories? \n Answer: \n\nVolatile memories are memories that are erased when electrical power is turned off or fails. Non-volatile memories are memories that are not erased when electrical power is turned off or fails."}, {"text": "Title: Tri-State Buffers and Combining Chips P5 \n Text: When EN=1, both transistors turn on, and the tri-state buffer acts as\na pair of back-to-back inverters, copying the signal from IN to OUT,\nas shown in the truth table. \n Question: \nQ. What does the chip select input do? \n Answer: \n\nA. The CS input enables or disables the memory. When CS is low, the memory is active and can read or write data. When CS is high, the memory is inactive and cannot perform any operations."}, {"text": "Title: Tri-State Buffers and Combining Chips P6 \n Text: What benefit does this logical disconnection provide? \n Question: \nQ. What is the difference between SRAM and DRAM? \n Answer: \n\nA. SRAM uses active logic in the form of a two-inverter loop to maintain stored values, while DRAM uses a charged capacitor to store a bit."}, {"text": "Title: Tri-State Buffers and Combining Chips P7 \n Text: So long as only one memory's chip select input is high at any time,\nthe same output line can be shared by more than one memory\nwithout the need for additional multiplexers. \n Question: \nQ. What are some applications that use SRAM? \n Answer: \n\nA. SRAM is used for high-speed applications such as processor caches and some embedded designs."}, {"text": "Title: Tri-State Buffers and Combining Chips P8 \n Text: Memory chips were often combined in this way to produce larger memories. \n Question: \nQ. How does the SRAM cell work? \n Answer: \n\nA.\n\nThe SRAM cell works by storing a bit in a dual-inverter loop. The loop is connected to opposing BIT lines through transistors controlled by a SELECT line."}, {"text": "Title: Tri-State Buffers and Combining Chips P9 \n Text: \nThe figure to the right illustrates how larger memories can be constructed\nusing multiple chips.  In the case shown, two {2^-bit} \nmemories are used to implement a {2^{k+1}-bit} memory.\nOne of the address bits---in the case shown, the most significant bit---is\nused to drive a decoder that determines which of the two chips is \nactive (CS=1).  The decoder is enabled with the chip select signal for\nthe larger memory, so neither chip is enabled when the external CS is\nlow, as desired.  The \n Question: \nQ. When writing a cell, what forces the inverters to match the values on the lines? \n Answer: \n\nA. The bit lines are held at opposite logic values."}, {"text": "Title: Tri-State Buffers and Combining Chips P10 \n Text: rest of the address bits, as well as the external\ndata inputs and write enable signal, are simply delivered to both memories.\nThe external data outputs are also connected to both memories.  \nEnsuring that at most one chip select signal is high at any time\nguarantees that at most one of the two memory chips drives logic values\non the data outputs. \n Question: \nQ. What is the value stored in the cell when SELECT is high? \n Answer: \n\nA. The value stored previously is copied onto the BIT line as an output."}, {"text": "Title: Tri-State Buffers and Combining Chips P11 \n Text: \nMultiple chips can also be used to construct wider memories, as shown to\nthe right.  In the case shown, two {2^-bit} \nmemories are used to implement a {2^-bit} memory.\nBoth chips are either active or inactive at the same time, so the external \naddress, write enable, and chip select inputs are routed to both chips.\nIn contrast, the data inputs and outputs are separate: the left chip\nhandles the high N bits of input on writes and produces the high N\nbits of output on reads, while the right chip handles the low N bits of \ninput and produces the low N bits of output. \n Question: \nQ. What is the function of the sense amplifiers in an SRAM cell? \n Answer: \n\nA. The sense amplifiers are responsible for quickly detecting voltage changes on the BIT lines, which are caused by the inverter loop."}, {"text": "Title: Tri-State Buffers and Combining Chips P12 \n Text: Historically, tri-state buffers were also used to reduce the number of\npins needed on chips.  Pins have long been a scarce resource, and the \namount of data that can cross a chip's pins in a second (the product of the\nnumber of pins and the data rate per pin) has not grown nearly as rapidly \nas the number of transistors packed into a fixed area. \n Question: \nQ. What is a bit slice? \n Answer: \n\nA bit slice is a number of cells that are combined into a single unit. This unit can then be used to perform various operations on data."}, {"text": "Title: Tri-State Buffers and Combining Chips P13 \n Text: By combining inputs and outputs, chip designers were able to halve the\nnumber of pins needed.  For example, data inputs and outputs of memory\nwere often combined into a single set of data wires, with bidirectional\nsignals.  When performing a read from a memory chip, the memory chip\ndrove the data pins with the bits being read (tri-state buffers on the\nmemory chip were enabled).  When performing a write, other logic such as \na processor wrote the value to be stored onto the data pins (tri-state \nbuffers were not enabled). \n Question: \nQ. What are the external inputs to the bit slice? \n Answer: \n\nA. External inputs to the bit slice include the clock signal, data signal, and reset signal."}, {"text": "Title: Dynamic Random Access Memory* P0 \n Text: Dynamic random access memory, or DRAM, is used for main memory in\ncomputers and for other applications in which size is more important\nthan speed.  While slower than SRAM, DRAM is denser (has\nmore bits per chip area).  A substantial part of DRAM density is\ndue to transistor count: typical SRAM cells use six transistors\n(two for each inverter, and two more to connect the inverters to the \nbit lines), while DRAM cells use only a single transistor.\nHowever, memory designers have also made significant advances in\nfurther miniaturizing DRAM cells to improve density beyond the \nbenefit available from simple transistor count. \n Question: \nQ. What is the difference between static and dynamic RAMs? \n Answer: \n\nA. Static RAMs are built using logic gates and require a power supply to maintain their data, while dynamic RAMs use capacitors to store data and only require a power supply when they are being accessed."}, {"text": "Title: Dynamic Random Access Memory* P1 \n Text: \nA diagram of a DRAM cell appears to the right.  \nDRAM storage is capacitive: a bit is stored by charging or not charging \na capacitor.  The capacitor is attached to a BIT line \nthrough a transistor controlled by a SELECT line.   \n Question: \nQ. What is the difference between a \"solar eclipse\" and a \"lunar eclipse\"? \n Answer: \n\nA. A \"solar eclipse\" is when the moon passes in front of the sun, and a \"lunar eclipse\" is when the earth passes in front of the moon."}, {"text": "Title: Dynamic Random Access Memory* P2 \n Text: When SELECT is low, the capacitor is isolated and \nholds its charge.  However, the transistor's resistance is\nfinite, and some charge leaks out onto the bit line.  Charge also\nleaks into the substrate on which the transistor is constructed.  After\nsome amount of time, all of the charge dissipates, and the bit is\nlost.  To avoid such loss, the cell must be { refreshed}\nperiodically by reading the contents and writing them back with active\nlogic. \n Question: \nQ. What is the difference between a memory and storage? \n Answer: \n\nA. Memory typically refers to data elements that can be accessed quickly, while storage usually refers to data that is stored for longer periods of time."}, {"text": "Title: Dynamic Random Access Memory* P3 \n Text: When the SELECT line is high during a write operation, logic driving\nthe bit line forces charge onto the capacitor or removes all charge\nfrom it.  For a read operation, the bit line is first brought to an\nintermediate voltage level (a voltage level between 0 and 1), then\nSELECT is raised, allowing the capacitor to either pull a small\namount of charge from the bit line or to push a small amount of charge\nonto the bit line.  The resulting change in voltage is then detected\nby a { sense amplifier} at the end of the bit line.  A sense amp \nis analogous to a marble on a mountaintop: a small push causes the\nmarble to roll rapidly downhill in the direction of the push.\nSimilarly, a small change in voltage causes a sense amp's output to\nmove rapidly to a logical 0 or 1, depending on the direction of the\nsmall change.  As mentioned earlier, sense amplifiers also appear in \nSRAM implementations.\nWhile not technically necessary, as they are with DRAM, the use of a\nsense amp to react to small changes in voltage makes reads faster. \n Question: \nQ. How are memory addresses different from street addresses?\nQ. \nQ. One key difference is that memory addresses usually have little or no redundancy, meaning each possible combination of bits in an address identifies a distinct set of bits in memory. In contrast, street addresses often have some degree of redundancy built in (for example, using directional indicators or abbreviations), which can help reduce errors when entering an address. \n Answer: "}, {"text": "Title: Dynamic Random Access Memory* P4 \n Text: Each read operation on a DRAM cell brings the voltage on its capacitor\ncloser to the intermediate voltage level, in effect destroying the\ndata in the cell.  DRAM is thus said to have { destructive reads}.\nTo preserve data during a read, the bits must be written back\ninto the cells after a read.  For example, the output of the sense \namplifiers can\nbe used to drive the bit lines, rewriting the cells with the\nappropriate data. \n Question: \nQ. What is the difference between volatile and non-volatile memories? \n Answer: \n\nVolatile memories are memories that are erased when electrical power is turned off or fails. Non-volatile memories are memories that are not erased when electrical power is turned off or fails."}, {"text": "Title: Dynamic Random Access Memory* P5 \n Text: At the chip level, typical DRAM inputs and outputs differ from those\nof SRAM.   \n Question: \nQ. What does the chip select input do? \n Answer: \n\nA. The CS input enables or disables the memory. When CS is low, the memory is active and can read or write data. When CS is high, the memory is inactive and cannot perform any operations."}, {"text": "Title: Dynamic Random Access Memory* P6 \n Text: Due to the large size and high density of DRAM,\naddresses are split into row and column components and provided\nthrough a common set of pins.  The DRAM stores the components in\nregisters to support this approach.  Additional inputs, known as the\n{ row} and { column address} {{ strobes}---RAS} and\nCAS, {respectively---are} used to indicate when address\ncomponents are available.  As\nyou might guess from the structure of coincident selection, DRAM\nrefresh occurs on a row-by-row basis (across bit slices---on columns\nrather than rows in the figures earlier in these notes, but the terminology\nof DRAM is a row).  Raising the SELECT line for a\nrow destructively reads the contents of all cells on that row, forcing\nthe cells to be rewritten and effecting a refresh.  The row is thus a\nnatural basis for the refresh cycle.  The DRAM data pins provide\nbidirectional signals for reading and writing elements of the DRAM.\nAn { output enable} input, OE, controls tri-state buffers with\nthe DRAM to determine whether or not the DRAM drives the data pins.\nThe WE input, which controls the type of operation, is\nalso present. \n Question: \nQ. What is the difference between SRAM and DRAM? \n Answer: \n\nA. SRAM uses active logic in the form of a two-inverter loop to maintain stored values, while DRAM uses a charged capacitor to store a bit."}, {"text": "Title: Dynamic Random Access Memory* P7 \n Text: \nTiming diagrams for writes and reads on a historical DRAM implementation\nappear to the right.  In both cases, the row component of the address is \nfirst applied to the address pins, then RAS is raised.  In the\nnext cycle of the controlling logic, the column component is applied\nto the address pins, and CAS is raised.   \n Question: \nQ. What are some applications that use SRAM? \n Answer: \n\nA. SRAM is used for high-speed applications such as processor caches and some embedded designs."}, {"text": "Title: Dynamic Random Access Memory* P8 \n Text: For a write, as shown on the left, the WE signal and the\ndata can \n Question: \nQ. How does the SRAM cell work? \n Answer: \n\nA.\n\nThe SRAM cell works by storing a bit in a dual-inverter loop. The loop is connected to opposing BIT lines through transistors controlled by a SELECT line."}, {"text": "Title: Dynamic Random Access Memory* P9 \n Text: also be applied in the second cycle.  The DRAM has internal\ntiming and control logic that prevent races from overwriting an\nincorrect element (remember that the row and column addresses have to\nbe stored in registers).  The DRAM again specifies a write cycle,\nafter which the operation is guaranteed to be complete.  In order, the\nWE, CAS, and RAS signals are then lowered.   \n Question: \nQ. When writing a cell, what forces the inverters to match the values on the lines? \n Answer: \n\nA. The bit lines are held at opposite logic values."}, {"text": "Title: Dynamic Random Access Memory* P10 \n Text: For a read operation, the output enable signal, OE, is raised after\nCAS is raised.  The DATA pins, which should be floating (in other\nwords, not driven by any logic), are then driven by the DRAM.  After the \nread cycle, valid data appear on the DATA pins, and OE, CAS, and\nRAS are lowered in order after the data are read. \n Question: \nQ. What is the value stored in the cell when SELECT is high? \n Answer: \n\nA. The value stored previously is copied onto the BIT line as an output."}, {"text": "Title: Dynamic Random Access Memory* P11 \n Text: Modern DRAM chips are substantially more sophisticated than those\ndiscussed here, and many of the functions that used to be provided\nby external logic are now integrated onto the chips themselves. \n Question: \nQ. What is the function of the sense amplifiers in an SRAM cell? \n Answer: \n\nA. The sense amplifiers are responsible for quickly detecting voltage changes on the BIT lines, which are caused by the inverter loop."}, {"text": "Title: Dynamic Random Access Memory* P12 \n Text: As an example of modern DRAMs, one can obtain\nthe data sheet for Micron Semiconductor's 8Gb ({2^b},\nfor example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016. \n Question: \nQ. What is a bit slice? \n Answer: \n\nA bit slice is a number of cells that are combined into a single unit. This unit can then be used to perform various operations on data."}, {"text": "Title: Dynamic Random Access Memory* P13 \n Text: The ability to synchronize to an external clock has become prevalent in \nthe industry, leading to the somewhat confusing term SDRAM, which stands \nfor { synchronous DRAM}.  The memory structures themselves\nare still unclocked, but logic is provided on the chip to synchronize \naccesses to the external clock without the need for additional \nlogic. \n Question: \nQ. What are the external inputs to the bit slice? \n Answer: \n\nA. External inputs to the bit slice include the clock signal, data signal, and reset signal."}, {"text": "Title: Dynamic Random Access Memory* P14 \n Text: The clock provided to the Micron chip just mentioned\ncan be as fast as 1.6 GHz, and data can be\ntransferred on both the rising and falling edges of the clock\n(hence the name DDR, or { double data rate}).   \n Question: \nQ. What is a bit slice? \n Answer: \n\nA bit slice is a section of a computer's memory that can be addressed as a single unit. A bit slice can be thought of as a 16-address, 1-bit-addressable memory."}, {"text": "Title: Dynamic Random Access Memory* P15 \n Text: In addition to row and\ncolumn components of the address, these chips further separate cells into\n{ banks} and groups of banks.  These allow a user to exploit parallelism\nby starting reads or writes to separate banks at the same time, thus\nimproving the speed at which data can move in and out of the memory. \n Question: \nQ. Why are bit slices used? \n Answer: \n\nA. Bit slices are used because they allow for easy read and write operations to a cell."}, {"text": "Title: Dynamic Random Access Memory* P16 \n Text: For the {2^b} version of the Micron chip,\nthe cells are structured into 4 groups of 4 banks (16 banks total),\neach with 131,072 rows and 1,024 columns.   \n Question: \nQ. What is the function of the chip select input?\nQ. \nQ. The chip select input determines which of the memory cells is active. When chip select is low (CS=0), none of the memory cells are active. When chip select is high (CS=1), exactly one of the memory cells is active. \n Answer: "}, {"text": "Title: Dynamic Random Access Memory* P17 \n Text: DRAM implementations provide interfaces for specifying\nrefresh operations in addition to reads and writes.\nManaging refresh timing and execution is\ngenerally left to an external DRAM controller. \n Question: \nQ. How many cells can a bit slice contain?\nQ. \nQ. There is no definite answer to this question since it depends on the size of the cells and the length of the wires. However, it is stated in the text that a bit slice can contain many more cells than are shown in the figure. \n Answer:  Therefore, we can infer that a bit slice can contain at least several hundred cells."}, {"text": "Title: Dynamic Random Access Memory* P18 \n Text: For the Micron chip, refresh commands must be issued every \n7.8 microseconds at normal temperatures.  Each\ncommand refreshes about 2^ cells, so 8,192 commands refresh\nthe whole chip in less than 64 milliseconds. \n Question: \nQ. What is the effect of setting CS=1 and WE=0? \n Answer: \n\nA.\n\nSetting CS=1 and WE=0 will cause the decoder to output a 1 on the appropriate cell's SELECT line, and the read logic to read the bit from the cell and deliver it to its Q output."}, {"text": "Title: Dynamic Random Access Memory* P19 \n Text: Alternatively, the chip can handle refresh on-chip in\norder to maintain memory contents when the rest of the system is \npowered down. \n Question: \nQ. What is the value of the bit slice's {DATA-IN} input? \n Answer: \n\nA. The value of the bit slice's {DATA-IN} input is the value to be written into the memory cell."}, {"text": "Title: From FSM to Computer P0 \n Text: The FSM designs we have explored so far have started with a human-based\ndesign process in which someone writes down the desired behavior in\nterms of states, inputs, outputs, and transitions.  Such an approach\nmakes it easier to build a digital FSM, since the abstraction used\ncorresponds almost directly to the implementation. \n Question: \nQ. The text states that \"such an approach makes it easier to build a digital FSM.\" What is a FSM? \n Answer: \n\nA. A FSM is a finite state machine."}, {"text": "Title: From FSM to Computer P1 \n Text: As an alternative, one can start by mapping the desired task into a\nhigh-level programming language, then using components such as registers,\ncounters, and memories to implement the variables needed.  In this approach,\nthe control structure of the code maps into a high-level FSM design.\nOf course, in order to implement our FSM with digital logic, we eventually\nstill need to map down to bits and gates. \n Question: \nQ. How do you map the control structure of code into a high-level FSM design? \n Answer: \n\nA. By using components such as registers, counters, and memories to implement the variables needed."}, {"text": "Title: From FSM to Computer P2 \n Text: In this set of notes, we show how one can transform a piece of code\nwritten in a high-level language into an FSM.  This process is meant to\nhelp you understand how we can design an FSM that executes simple\npieces of a flow chart such as assignments, { if} statements, and \nloops.  Later, we generalize this concept and build an FSM that allows\nthe pieces to be executed to be specified after the FSM is built---in \nother words, the FSM executes a program specified by bits stored in \nmemory.  This more general model, as you might have already guessed, \nis a computer.   \n Question: \nQ. What is the purpose of this process? \n Answer: \n\nA. The purpose of this process is to help you understand how to design an FSM that executes simple pieces of a flow chart."}, {"text": "Title: Specifying the Problem P0 \n Text: Let's begin by specifying the problem that we want to solve.\nSay that we want to find the minimum value in a set of 10 integers.\nUsing the C programming language, we can write the following fragment of \ncode: \n Question: \nQ. The text states that \"such an approach makes it easier to build a digital FSM.\" What is a FSM? \n Answer: \n\nA. A FSM is a finite state machine."}, {"text": "Title: Specifying the Problem P1 \n Text: aaaa=aaaa=\nint >values[10];    /* 10 integers--filled in by other code */\nint >idx;\nint >min \n Question: \nQ. How do you map the control structure of code into a high-level FSM design? \n Answer: \n\nA. By using components such as registers, counters, and memories to implement the variables needed."}, {"text": "Title: Specifying the Problem P2 \n Text: min = values[0];\nfor (idx = 1; 10 > idx; idx = idx + 1) {\n>  if (min > values[idx]) {\n>  >  min = values[idx];\n>  }\n}\n/* The minimum value from array is now in min. */ \n Question: \nQ. What is the purpose of this process? \n Answer: \n\nA. The purpose of this process is to help you understand how to design an FSM that executes simple pieces of a flow chart."}, {"text": "Title: Specifying the Problem P3 \n Text: \nThe code uses array notation, which we have not used previously in our \nclass, so let's first discuss the meaning of the code. \n Question: \nQ. What is the C programming language? \n Answer: \n\nThe C programming language is a high-level, general-purpose programming language."}, {"text": "Title: Specifying the Problem P4 \n Text: The code uses three variables. \n Question: \nQ. What is the value of the variable \"min\"? \n Answer: \n\nA. The value of the variable \"min\" is the smallest integer in the array \"values\"."}, {"text": "Title: Specifying the Problem P5 \n Text: The variable { values} represents the 10 values in our set.\nThe suffix ``[10]'' after the variable name tells the compiler that\nwe want an array of 10 integers ({ int}) indexed from 0 to 9.\nThese integers can be treated as 10 separate variables, but can be\naccessed using the single name ``{ values}'' along with an index\n(again, from 0 to 9 in this case). \n Question: \nQ. What is the minimum value from array? \n Answer: \n\nA. The minimum value from array is the value in min."}, {"text": "Title: Specifying the Problem P6 \n Text: The variable { idx} holds a loop index that we use to examine each\nof the values one by one in order to find the minimum value in the set. \n Question: \nQ. What does the code mean? \n Answer: \n\nA. The code means that the person's name is stored in an array, and the person's age is stored in an array."}, {"text": "Title: Specifying the Problem P7 \n Text: Finally, the variable { min} holds the smallest known value as \nthe program examines each of the values in the set. \n Question: \nQ. \nQ. What are the three variables in the text? \n Answer: \n\nA. The three variables in the text are \"code,\" \"question,\" and \"answer.\""}, {"text": "Title: Specifying the Problem P8 \n Text: The program body consists of two statements.   \n Question: \nQ. What is the range of indices for the array \"values\"?\nQ. \nQ. The range of indices for the array \"values\" is 0 to 9. \n Answer: "}, {"text": "Title: Specifying the Problem P9 \n Text: We assume that some other piece of code---one not shown here---has \ninitialized the 10 values in our set before the code above executes. \n Question: \nQ. What is the purpose of the idx variable?\nQ. \nQ. The idx variable is used to loop through the values in order to find the minimum value. \n Answer: "}, {"text": "Title: Specifying the Problem P10 \n Text: The first statement initializes the\nminimum known value ({ min}) to the value stored at index 0 in the \narray ({ values[0]}).\nThe second statement is a loop in which the variable { index} \ntakes on values from 1 to 9.  For each value, an { if} statement\ncompares the current\nknown minimum with the value stored in the array at index given by the\n{ idx} variable.  If the stored value is smaller, the current known \nvalue (again, { min}) is updated to reflect the program's\nhaving found a smaller value.  When the loop finishes all nine iterations,\nthe variable { min} holds the smallest value among the set of 10 \nintegers stored in the { values} array. \n Question: \nQ. 1. What is the purpose of the variable \"count\"?\nQ. \nQ. 2. What is the purpose of the variable \"min\"? \n Answer: \n\n1. The purpose of the variable \"count\" is to keep track of how many values have been examined so far.\n\n2. The purpose of the variable \"min\" is to hold the smallest known value as the program examines each of the values in the set."}, {"text": "Title: Specifying the Problem P11 \n Text: \nAs a first step towards designing an FSM to implement the code, we transform\nthe code into a flow chart, as shown to the right.  The program again begins\nwith initialization, which appears in the second column of the flow chart.  \nThe loop in the program translates to the third column of the flow chart, \nand the { if} statement to the middle comparison and update \nof { min}. \n Question: \nQ. What does the program body consist of?\nQ. \nQ. The program body consists of two statements. \n Answer: "}, {"text": "Title: Specifying the Problem P12 \n Text: Our goal is now to design an FSM to implement the flow chart.  In order\nto do so, we want to leverage the same kind of abstraction that we used\nearlier, when extending our keyless entry system with a timer.  Although the\ntimer's value was technically also \n Question: \nQ. What does the code above do?\nQ. \nQ. The code above initializes the 10 values in our set. \n Answer: "}, {"text": "Title: Specifying the Problem P13 \n Text: part of the FSM's state, we treated it\nas data and integrated it into our next-state decisions in only a couple\nof cases. \n Question: \nQ. What are the values of the variables min and index after the first iteration of the loop?\nQ. \nQ. The value of min after the first iteration of the loop is 1, and the value of index is 1. \n Answer: "}, {"text": "Title: Specifying the Problem P14 \n Text: For our minimum value problem, we have two sources of data.  First, an\nexternal program supplies data in the form of a set of 10 integers.  If\nwe assume {32-bit} integers, these data technically form 320 input bits!\nSecond, as with the keyless entry system timer, we have data used internally\nby our FSM, such as the loop index and the current minimum value.  These\nare technically state bits.  For both types of data, we treat them\nabstractly as values rather than thinking of them individually as bits,\nallowing us to develop our FSM at a high-level and then to implement it \nusing the components that we have developed earlier in our course. \n Question: \nQ. How does the program find the smallest number in the list? \n Answer: \n\nA. The program finds the smallest number in the list by comparing all of the numbers in the list and finding the smallest number."}, {"text": "Title: Choosing Components and Identifying States P0 \n Text: Now we are ready to design an FSM that implements the flow chart.\nWhat components do we need, other than our state logic?\nWe use registers and counters to implement the variables { idx}\nand { min} in the program.\nFor the array { values}, we use a {1632-bit} \nmemory.{We technically only need a {1032-bit} \nmemory, but we round up the size of the address space to reflect more\nrealistic memory designs; one can always optimize later.}\nWe need a comparator to implement the test for the { if} statement.\nWe choose to use a serial comparator, which allows us to illustrate again\nhow one logical high-level state can be subdivided into many actual states.\nTo operate the serial comparator, we make use of two shift registers that \npresent the comparator with one bit per cycle on each input, and a counter\nto keep track of the comparator's progress. \n Question: \nQ. The text states that \"such an approach makes it easier to build a digital FSM.\" What is a FSM? \n Answer: \n\nA. A FSM is a finite state machine."}, {"text": "Title: Choosing Components and Identifying States P1 \n Text: How do we identify high-level states from our flow chart?  Although\nthe flow chart attempts to break down the program into `simple' steps,\none step of a flow chart may sometimes require more than one state\nin an FSM.  Similarly, one FSM state may be able to implement several\nsteps in a flow chart, if those steps can be performed simultaneously.\nOur design illustrates both possibilities. \n Question: \nQ. How do you map the control structure of code into a high-level FSM design? \n Answer: \n\nA. By using components such as registers, counters, and memories to implement the variables needed."}, {"text": "Title: Choosing Components and Identifying States P2 \n Text: How we map flow chart elements into FSM states also depends to some \ndegree on what components we use, which is why we began with some discussion\nof components.  In practice, one can go back and forth between the two, \nadjusting components to better match the high-level states, and adjusting \nstates to better match the desired components. \n Question: \nQ. What is the purpose of this process? \n Answer: \n\nA. The purpose of this process is to help you understand how to design an FSM that executes simple pieces of a flow chart."}, {"text": "Title: Choosing Components and Identifying States P3 \n Text: Finally, note that we are only concerned with high-level states, so we do \nnot need to provide details (yet) down to the level of individual clock \ncycles, but we do want to define high-level states that can be implemented\nin a fixed number of cycles, or at least a controllable number of cycles.\nIf we cannot specify clearly when transitions occur from an FSM state, we\nmay not be able to implement the state. \n Question: \nQ. What is the C programming language? \n Answer: \n\nThe C programming language is a high-level, general-purpose programming language."}, {"text": "Title: Choosing Components and Identifying States P4 \n Text: Now let's go through the flow chart and identify states.  Initialization of\n{ min} and { idx} need not occur serially, and the result of the\nfirst comparison between { idx} and the constant 10 is known in advance,\nso we can merge all three operations into a single state, which we \ncall { INIT}. \n Question: \nQ. What is the value of the variable \"min\"? \n Answer: \n\nA. The value of the variable \"min\" is the smallest integer in the array \"values\"."}, {"text": "Title: Choosing Components and Identifying States P5 \n Text: We can also merge the updates of { min} and { idx} into a second\nFSM state, which we call { COPY}.  However, the update to { min} \noccurs only when the comparison ({ min > value[idx]}) is true.  \nWe can use logic to predicate execution of the update.  In other words, we \ncan use the output of the comparator, which is available after the comparator \nhas finished comparing the two values (in a high-level FSM state that we \nhave yet to define), to determine whether or not the register holding \n{ min} loads a new value in the { COPY} state. \n Question: \nQ. What is the minimum value from array? \n Answer: \n\nA. The minimum value from array is the value in min."}, {"text": "Title: Choosing Components and Identifying States P6 \n Text: Our model of use for this FSM involves external logic filling the memory\n(the array of integer values), executing the FSM ``code,'' and then\nchecking the answer.  To support this use model, we create a FSM state \ncalled { WAIT} for cycles in which the FSM has no work to do.\nLater, we also make use of an external input signal { START} \nto start the FSM\nexecution.  The { WAIT} state logically corresponds to the ``START'' \nbubble in the flow chart. \n Question: \nQ. What does the code mean? \n Answer: \n\nA. The code means that the person's name is stored in an array, and the person's age is stored in an array."}, {"text": "Title: Choosing Components and Identifying States P7 \n Text: \nOnly the test for the { if} statement remains.  Using a serial\ncomparator to compare two {32-bit} values requires 32 cycles.\nHowever, we need an additional cycle to move values into our shift \nregisters so that the comparator can see the first bit.  Thus our\nsingle comparison operation breaks into two high-level states.  In the\nfirst state, which we call { PREP}, we copy { min} to one\nof the shift registers, copy { values[idx]} to the other shift\nregister, and reset the counter that measures the cycles needed for\nour serial comparator.  We then move to a second high-level state,\nwhich we call { COMPARE}, in which we feed one bit per cycle\nfrom each shift register to the serial comparator.  The { COMPARE} \nstate \n Question: \nQ. \nQ. What are the three variables in the text? \n Answer: \n\nA. The three variables in the text are \"code,\" \"question,\" and \"answer.\""}, {"text": "Title: Choosing Components and Identifying States P8 \n Text: executes for 32 cycles, after which the comparator\nproduces the one-bit answer that we need, and we can move to the\n{ COPY} state.  The association between the flow chart and the\nhigh-level FSM states is illustrated in the figure shown to the right\nabove. \n Question: \nQ. What is the range of indices for the array \"values\"?\nQ. \nQ. The range of indices for the array \"values\" is 0 to 9. \n Answer: "}, {"text": "Title: Choosing Components and Identifying States P9 \n Text: \nWe can now also draw an abstract state diagram for our FSM, as shown\nto the right.  The FSM begins in the { WAIT} state.  After external\nlogic fills the { values} array, it signals the FSM to begin by\nraising the { START} signal.  The FSM transitions into the \n{ INIT} state, and in the next cycle into the { PREP} state.\nFrom { PREP}, the FSM always moves to { COMPARE}, where it\nremains for 32 cycles while the serial comparator executes a comparison.\nAfter { COMPARE}, the FSM moves to the { COPY} \n Question: \nQ. What is the purpose of the idx variable?\nQ. \nQ. The idx variable is used to loop through the values in order to find the minimum value. \n Answer: "}, {"text": "Title: Choosing Components and Identifying States P10 \n Text: state, where\nit remains for one cycle.  The transition from { COPY} depends on\nhow many loop iterations have executed.  If more loop iterations remain,\nthe FSM moves to { PREP} to execute the next iteration.  If the\nloop is done, the FSM returns to { WAIT} to allow external logic\nto read the result of the computation. \n Question: \nQ. 1. What is the purpose of the variable \"count\"?\nQ. \nQ. 2. What is the purpose of the variable \"min\"? \n Answer: \n\n1. The purpose of the variable \"count\" is to keep track of how many values have been examined so far.\n\n2. The purpose of the variable \"min\" is to hold the smallest known value as the program examines each of the values in the set."}, {"text": "Title: Laying Out Components P0 \n Text: \nOur high-level FSM design tells us what our components need to be able to\ndo in any given cycle.  For example, when we load new values into the shift\nregisters that provide bits to the serial comparator, we always copy \n{ min} into one shift register and { values[idx]} into the second.\nUsing this information, we can put together our components and simplify our\ndesign by fixing the way in which bits flow between them. \n Question: \nQ. The text states that \"such an approach makes it easier to build a digital FSM.\" What is a FSM? \n Answer: \n\nA. A FSM is a finite state machine."}, {"text": "Title: Laying Out Components P1 \n Text: The figure at the right shows how we can organize our components.\nAgain, in practice, one goes back and forth thinking about states,\ncomponents, and flow from state to state.  In these notes, we \npresent only a completed design. \n Question: \nQ. How do you map the control structure of code into a high-level FSM design? \n Answer: \n\nA. By using components such as registers, counters, and memories to implement the variables needed."}, {"text": "Title: Laying Out Components P2 \n Text: Let's take a detailed look at each of the components. \n Question: \nQ. What is the purpose of this process? \n Answer: \n\nA. The purpose of this process is to help you understand how to design an FSM that executes simple pieces of a flow chart."}, {"text": "Title: Laying Out Components P3 \n Text: At the upper left of the figure is a {4-bit} binary counter called\n{ IDX} to hold the { idx}\nvariable.  The counter can be reset to 0 using the { RST} input.\nOtherwise, the { CNT} input controls whether or not the counter\nincrements its value.  With this counter design, we can force { idx} \nto 0 in the { WAIT} state and then count upwards in the { INIT}\nand { COPY} states. \n Question: \nQ. What is the C programming language? \n Answer: \n\nThe C programming language is a high-level, general-purpose programming language."}, {"text": "Title: Laying Out Components P4 \n Text: A memory labeled { VALUES} to hold the array { values} appears \nin the upper right of\nthe figure.  The read/write control for the memory is hardwired to 1 (read)\nin the figure, and the data input lines are unattached.  To integrate \nwith other logic that can operate our FSM, we need to add more \ncontrol logic to allow writing into the memory and to attach the data\ninputs to something that provides the data bits.  The address input of\nthe memory comes always from the { IDX} counter value; in other words,\nwhenever we access this memory by making use of the data output lines,\nwe read { values[idx]}. \n Question: \nQ. What is the value of the variable \"min\"? \n Answer: \n\nA. The value of the variable \"min\" is the smallest integer in the array \"values\"."}, {"text": "Title: Laying Out Components P5 \n Text: In the middle left of the figure is a {32-bit} register for the \n{ min} variable.  It has a control input { LD} that \ndetermines whether or not it loads a new value at the end of the clock\ncycle.  If a new value is loaded, the new value always corresponds to\nthe output of the { VALUES} memory, { values[idx]}.  Recall\nthat { min} always changes in the { INIT} state, and may change\nin the { COPY} state.  But the new value stored in { min}\nis always { values[idx]}. \nNote also that when the FSM completes its task, the result of the \ncomputation is left in the { MIN} register for external logic to\nread (connections for this purpose are not shown in the figure). \n Question: \nQ. What is the minimum value from array? \n Answer: \n\nA. The minimum value from array is the value in min."}, {"text": "Title: Laying Out Components P6 \n Text: Continuing downward in the figure, we see two right shift registers\nlabeled { A} and { B}.  Each has a control input { LD}\nthat enables a parallel load.  Register { A} loads from register\n{ MIN}, and register { B} loads from the memory data output\n({ values[idx]}).  These loads are needed in the { PREP}\nstate of our FSM.  When { LD} is low, the shift registers\nsimply shifts to the right.  The serial output { SO} makes the\nleast significant bit of each shift register available.  Shifting\nis necessary to feed the serial comparator in the { COMPARE} state. \n Question: \nQ. What does the code mean? \n Answer: \n\nA. The code means that the person's name is stored in an array, and the person's age is stored in an array."}, {"text": "Title: Laying Out Components P7 \n Text: Below register { A} is a {5-bit} binary counter called { CNT}.\nThe counter is used to control the serial comparator in the { COMPARE}\nstate.  A reset input { RST} allows it to be forced to 0 in the \n{ PREP} state.  When the counter value is exactly zero, the \noutput { Z} is high. \n Question: \nQ. \nQ. What are the three variables in the text? \n Answer: \n\nA. The three variables in the text are \"code,\" \"question,\" and \"answer.\""}, {"text": "Title: Laying Out Components P8 \n Text: The last major component is the serial comparator, which is based on the\ndesign developed in Notes Set 3.1.  The two bits to be compared in \na cycle come from shift registers { A} and { B}.  The first\nbit indicator comes from the zero indicator of counter { CNT}.\nThe comparator actually produces two outputs ({ Z1} and { Z0}),\nbut the meaning of the\n{ Z1} output by itself is { A > B}.  In the diagram,\nthis signal has been labeled { THEN}. \n Question: \nQ. What is the range of indices for the array \"values\"?\nQ. \nQ. The range of indices for the array \"values\" is 0 to 9. \n Answer: "}, {"text": "Title: Laying Out Components P9 \n Text: There are two additional elements in the figure that we have yet to discuss.\nEach simply compares the value in a register with a fixed constant and \nproduces a {1-bit} signal.  When the FSM finishes an iteration of\nthe loop in the { COPY} state, it must check the loop condition\n({ 10 > idx}) and move either to the { PREP} state or, when\nthe loop finishes, to the { WAIT} state to let the external logic\nread the answer from the { MIN} register.  The loop is done when the\ncurrent iteration count is nine, so we compare { IDX} with nine to\nproduce the { DONE} signal.  The other constant comparison is between\nthe counter { CNT} and the value 31 to produce the { LAST} signal,\nwhich indicates that the serial comparator is on its last cycle of \ncomparison.  In the cycle after { LAST} is high, the { THEN} \noutput of the comparator indicates whether or not { A > B}. \n Question: \nQ. What is the purpose of the idx variable?\nQ. \nQ. The idx variable is used to loop through the values in order to find the minimum value. \n Answer: "}, {"text": "Title: Control and Data P0 \n Text: One can think of the components and the interconnections between them\nas enabling the movement of data between registers, while the high-level \nFSM controls which data move from register to register in each cycle.   \n Question: \nQ. The text states that \"such an approach makes it easier to build a digital FSM.\" What is a FSM? \n Answer: \n\nA. A FSM is a finite state machine."}, {"text": "Title: Control and Data P1 \n Text: With this model in mind, we call the components and interconnections\nfor our design the { datapath}---a term that we will see again when\nwe examine the parts of a computer in the coming weeks. \n Question: \nQ. How do you map the control structure of code into a high-level FSM design? \n Answer: \n\nA. By using components such as registers, counters, and memories to implement the variables needed."}, {"text": "Title: Control and Data P2 \n Text: The datapath requires several inputs to control the operation of the\ncomponents---these we can treat as outputs of the FSM.\nThese signals allow the FSM to control the motion of data in the \ndatapath, so we call them { control signals}. \n Question: \nQ. What is the purpose of this process? \n Answer: \n\nA. The purpose of this process is to help you understand how to design an FSM that executes simple pieces of a flow chart."}, {"text": "Title: Control and Data P3 \n Text: Similarly, the datapath produces several outputs that we can treat\nas inputs to the FSM. \n Question: \nQ. What is the C programming language? \n Answer: \n\nThe C programming language is a high-level, general-purpose programming language."}, {"text": "Title: Control and Data P4 \n Text: The tables below summarize the control signals (left) and outputs (right)\nfrom the datapath for our FSM. \n Question: \nQ. What is the value of the variable \"min\"? \n Answer: \n\nA. The value of the variable \"min\" is the smallest integer in the array \"values\"."}, {"text": "Title: Control and Data P5 \n Text: [t]\n{|c|l|}\ndatapath& \ninput& {|c|} \n{ IDX.RST}& reset { IDX} counter to 0\n{ IDX.CNT}& increment { IDX} counter\n{ MIN.LD}& load new value into { MIN} register\n{ A.LD}& load new value into shift register { A}\n{ B.LD}& load new value into shift register { B}\n{ CNT.RST}& reset { CNT} counter  \n Question: \nQ. What is the minimum value from array? \n Answer: \n\nA. The minimum value from array is the value in min."}, {"text": "Title: Control and Data P6 \n Text: \n[t]\n{|c|l|c|}\ndatapath& &\noutput& {|c|}& based on \n{ DONE}& last loop iteration finished& { IDX = 9}\n{ LAST}& serial comparator executing& { CNT = 31}\n&    last cycle&\n{ THEN}& { if} statement condition true& { A > B} \n{}\n{} \n Question: \nQ. What does the code mean? \n Answer: \n\nA. The code means that the person's name is stored in an array, and the person's age is stored in an array."}, {"text": "Title: Control and Data P7 \n Text: Using the datapath controls signals and outputs, we can now write a more\nformal state transition table for the FSM, as shown below. \n Question: \nQ. \nQ. What are the three variables in the text? \n Answer: \n\nA. The three variables in the text are \"code,\" \"question,\" and \"answer.\""}, {"text": "Title: Control and Data P8 \n Text: The ``actions'' column of the table\nlists the changes to register and counter values\nthat are made in each of the FSM states.  The notation used to represent\nthe actions is called { register transfer language} ({ RTL}).\nThe meaning of an individual action is similar to the meaning of the \ncorresponding statement from our C code or from the flow chart.\nFor example, in the { WAIT} state, ``{ IDX  0}''\nmeans the same thing as ``{ idx = 0;}''.  In particular, both mean\nthat the value currently stored in the { IDX} counter is overwritten \nwith the number 0 (all 0 bits). \n Question: \nQ. What is the range of indices for the array \"values\"?\nQ. \nQ. The range of indices for the array \"values\" is 0 to 9. \n Answer: "}, {"text": "Title: Control and Data P9 \n Text: The meaning of RTL is slightly different from the usual interpretation of\nhigh-level programming languages, however, in terms of when the actions\nhappen.  A list of C statements is generally executed one at a time. \n Question: \nQ. What is the purpose of the idx variable?\nQ. \nQ. The idx variable is used to loop through the values in order to find the minimum value. \n Answer: "}, {"text": "Title: Control and Data P10 \n Text: In contrast, the entire list of RTL actions \n Question: \nQ. 1. What is the purpose of the variable \"count\"?\nQ. \nQ. 2. What is the purpose of the variable \"min\"? \n Answer: \n\n1. The purpose of the variable \"count\" is to keep track of how many values have been examined so far.\n\n2. The purpose of the variable \"min\" is to hold the smallest known value as the program examines each of the values in the set."}, {"text": "Title: Control and Data P11 \n Text: \n{|c|l|c|c|}\nstate& {|c|}{actions (simultaneous)}& condition& next state \n{ WAIT}& { IDX}  0 (to read { VALUES[0]} in { INIT})& { START}& { INIT} \n&& {{ START}}& { WAIT} \n{ INIT}& { MIN}  { VALUES[IDX]} ({ IDX} is 0 in this state)& (always)& { PREP} \n& { IDX}  { IDX} + 1& & \n{ PREP}& { A}  { MIN}& (always)& { COMPARE}\n& { B}  { VALUES[IDX]}& &\n& { CNT}  0& & \n{ COMPARE}& run serial comparator& { LAST}& { COPY}\n&& {{ LAST}}& { COMPARE} \n{ COPY}& { THEN}: { MIN}  { VALUES[IDX]}& { DONE}& { WAIT} \n& { IDX}  { IDX} + 1& {{ DONE}}& { PREP}  \n Question: \nQ. What does the program body consist of?\nQ. \nQ. The program body consists of two statements. \n Answer: "}, {"text": "Title: Control and Data P12 \n Text: for an FSM state is executed\nsimultaneously, at the end of the clock cycle.  As you know, an FSM moves \nfrom its current state into a new state at the end of every clock cycle,\nso actions during different cycles usually are associated with different \nstates.\nWe can, however, change the value in more than one register at the end \nof the same clock cycle, so we can execute more than one RTL action in\nthe same state, so long as the actions do not exceed the capabilities\nof our datapath (the components must be able to support the simultaneous \nexecution of the actions).  Some care must be taken with states that \nexecute for more than one cycle to ensure that repeating the \nRTL actions is appropriate.  In our design, only the { WAIT} and\n{ COMPARE} states execute for more than one cycle.  The { WAIT}\nstate resets the { IDX} counter repeatedly, which causes no problems.\nThe { COMPARE} statement has no RTL actions---all of the shifting,\ncomparison, and counting activity needed to do its work occurs within \nthe datapath itself. \n Question: \nQ. What does the code above do?\nQ. \nQ. The code above initializes the 10 values in our set. \n Answer: "}, {"text": "Title: Control and Data P13 \n Text: One additional piece of RTL syntax needs explanation.  In the { COPY}\nstate, the first action begins with ``{ THEN}:,'' which means that the \nprefixed RTL action occurs only when the { THEN} signal is high.\nRecall that the { THEN} signal indicates that the comparator has\nfound { A > B}, so the equivalent C code is ``{ if (A > B) \n{min = values[idx]}}''. \n Question: \nQ. What are the values of the variables min and index after the first iteration of the loop?\nQ. \nQ. The value of min after the first iteration of the loop is 1, and the value of index is 1. \n Answer: "}, {"text": "Title: State Representation and Logic Expressions P0 \n Text: Let's think about the representation for the FSM states.  The FSM has \nfive states, so we could use as few as three flip-flops.  Instead, we choose\nto use a { one-hot encoding}, in which any valid bit pattern has exactly\none 1 bit.  In other words, we use five flip-flops instead of three,\nand our states are represented with the bit patterns 10000, 01000, 00100,\n00010, and 00001. \n Question: \nQ. The text states that \"such an approach makes it easier to build a digital FSM.\" What is a FSM? \n Answer: \n\nA. A FSM is a finite state machine."}, {"text": "Title: State Representation and Logic Expressions P1 \n Text: The table below shows the mapping from each high-level state to \nboth the five-bit encoding for the state as well as the six control signals \nneeded for the datapath.  For each state, the values of the control signals\ncan be found by examining the actions necessary in that state. \n Question: \nQ. How do you map the control structure of code into a high-level FSM design? \n Answer: \n\nA. By using components such as registers, counters, and memories to implement the variables needed."}, {"text": "Title: State Representation and Logic Expressions P2 \n Text: {\n{|c|c|cccccc|}\nstate& { S_4S_3S_2S_1S_0}& { IDX.RST}& { IDX.CNT}& { MIN.LD}& { A.LD}& { B.LD}& { CNT.RST} \n{ WAIT}& { 1 0 0 0 0}& 1& 0& 0& 0& 0& 0\n{ INIT}& { 0 1 0 0 0}& 0& 1& 1& 0& 0& 0\n{ PREP}& { 0 0 1 0 0}& 0& 0& 0& 1& 1& 1\n{ COMPARE}& { 0 0 0 1 0}& 0& 0& 0& 0& 0& 0\n{ COPY}& { 0 0 0 0 1}& 0& 1& { THEN}& 0& 0& 0 \n{} \n Question: \nQ. What is the purpose of this process? \n Answer: \n\nA. The purpose of this process is to help you understand how to design an FSM that executes simple pieces of a flow chart."}, {"text": "Title: State Representation and Logic Expressions P3 \n Text: The { WAIT} state needs to set { IDX} to 0 but need not affect \nother register or counter values, so { WAIT} produces a 1 only for\n{ IDX.RST}.  The { INIT} state needs to load { values[0]} into \nthe { MIN} register while simultaneously incrementing the { IDX}\ncounter (from 0 to 1), so { INIT} produces 1s for { IDX.CNT}\nand { MIN.LD}.  The { PREP} state loads both shift registers\nand resets the counter { CNT} by producing 1s for { A.LD},\n{ B.LD}, and { CNT.RST}.  The { COMPARE} state does not\nchange any register values, so it produces all 0s.  Finally, the { COPY}\nstate increments the { IDX} counter while simultaneously loading a\nnew value into the { MIN} register.  The { COPY} state produces 1\nfor { IDX.CNT}, but must use the signal { THEN} coming from the\ndatapath to decide whether or not { MIN} is loaded. \n Question: \nQ. What is the C programming language? \n Answer: \n\nThe C programming language is a high-level, general-purpose programming language."}, {"text": "Title: State Representation and Logic Expressions P4 \n Text: The advantage of a one-hot encoding becomes obvious when we write\nequations for the six control signals and the next-state logic, as shown\nto the right.   \n Question: \nQ. What is the value of the variable \"min\"? \n Answer: \n\nA. The value of the variable \"min\" is the smallest integer in the array \"values\"."}, {"text": "Title: State Representation and Logic Expressions P5 \n Text: Implementing the logic to complete our design now requires only a handful \nof small logic gates. \n Question: \nQ. What is the minimum value from array? \n Answer: \n\nA. The minimum value from array is the value in min."}, {"text": "Title: State Representation and Logic Expressions P6 \n Text: \n{eqnarray*}\n{ IDX.RST} & = & { S}_4\n{ IDX.CNT} & = & { S}_3 + { S}_0\n{ MIN.LD} & = & { S}_3 + { S}_0  { THEN}\n{ A.LD} & = & { S}_2\n{ B.LD} & = & { S}_2\n{ CNT.RST} & = & { S}_2\n{eqnarray*} \n Question: \nQ. What does the code mean? \n Answer: \n\nA. The code means that the person's name is stored in an array, and the person's age is stored in an array."}, {"text": "Title: State Representation and Logic Expressions P7 \n Text: \n{eqnarray*}\n{ S}_4^+ & = & { S}_4  { START} + { S}_0  { DONE}\n{ S}_3^+ & = & { S}_4  { START}\n{ S}_2^+ & = & { S}_3 + { S}_0  { DONE}\n{ S}_1^+ & = & { S}_2 + { S}_1  { LAST}\n{ S}_0^+ & = & { S}_1  { LAST}\n{eqnarray*} \n Question: \nQ. \nQ. What are the three variables in the text? \n Answer: \n\nA. The three variables in the text are \"code,\" \"question,\" and \"answer.\""}, {"text": "Title: State Representation and Logic Expressions P8 \n Text: \nNotice that the terms in each control signal can be read directly from \nthe rows of the state table and OR'd together.  The terms in each of the\nnext-state equations represent the incoming arcs for the corresponding\nstate.  For example, the { WAIT} state has one self-loop (the first\nterm) and a transition arc coming from the { COPY} state when the\nloop is done. \n Question: \nQ. What is the range of indices for the array \"values\"?\nQ. \nQ. The range of indices for the array \"values\" is 0 to 9. \n Answer: "}, {"text": "Title: State Representation and Logic Expressions P9 \n Text: These expressions complete our design. \n Question: \nQ. What is the purpose of the idx variable?\nQ. \nQ. The idx variable is used to loop through the values in order to find the minimum value. \n Answer: "}, {"text": "Title: Summary of Part 3 of the Course P0 \n Text: \nIn this short summary, we \ngive you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth). \n Question: \nQ. How can I improve my understanding of the text? \n Answer: \n\nA.\n\nThere are a few things you can do to improve your understanding of the text:\n\n-Read the text carefully and slowly, making sure to understand each word and sentence.\n\n-Try to identify the main ideas and key points in the text.\n\n-summarize the text in your own words.\n\n- discuss the text with your classmates or a tutor, and ask questions about anything you don't understand."}, {"text": "Title: Summary of Part 3 of the Course P1 \n Text: Students often find this part of the course more challenging than the\nearlier parts of the course. \n Question: \nQ. What is the author's opinion of the students in the course?\nQ. \nQ. The author's opinion of the students in the course is not stated in the text. \n Answer: "}, {"text": "Title: Summary of Part 3 of the Course P2 \n Text: In addition to these notes, you should read Chapters 4 and 5 of the \nPatt and Patel textbook, which cover the von Neumann\nmodel, instruction processing, and ISAs.   \n Question: \nQ. What is an ISA? \n Answer: \n\nA. An ISA is an instruction set architecture, which is a set of commands that can be executed by a computer."}, {"text": "Title: Summary of Part 3 of the Course P3 \n Text: Students typically find that the homeworks in this part of the course\nrequire more time than did those in earlier parts of the course.\nProblems on the exam will be similar in nature but designed to require \nless actual time to solve (assuming that you have been doing the homeworks).   \n Question: \nQ. What does the author mean when they say \"Problems on the exam will be similar in nature but designed to require less actual time to solve\"?\nQ. \nQ. The author means that the problems on the exam will require less time to solve than the problems on the homeworks. \n Answer: "}, {"text": "Title: Summary of Part 3 of the Course P4 \n Text: We'll start with the easy stuff.   \n Question: \nQ. What does the author mean by \"the easy stuff\"?\nQ. \nQ. The author means the easy questions. \n Answer: "}, {"text": "Title: Summary of Part 3 of the Course P5 \n Text: You should recognize all of these terms and be able\nto explain what they mean.  For the specific circuits, you should be able \nto draw them and explain how they work.  Actually, we don't care whether \nyou can draw something from memory---a mux, for example---provided that \nyou know what a mux does and can derive a gate diagram correctly for one \nin a few minutes.  Higher-level skills are much more valuable. \n Question: \nQ. What is a mux? \n Answer: \n\nA. A mux is a multiplexer. It is a device that takes multiple input signals and outputs a single signal. The output signal is a function of the input signals and the control signal."}, {"text": "Title: Summary of Part 3 of the Course P6 \n Text: {digital systems terms\n{--}{{}{}\n{}{}{}\n module\n fan-in\n fan-out\n machine models: Moore and Mealy \n Question: \nQ. What is the difference between Moore and Mealy machine models? \n Answer: \n\nA. The main difference between Moore and Mealy machine models is that in Moore machine models, the output of the machine is a function of only the present state, while in Mealy machine models, the output of the machine is a function of both the present state and the present input."}, {"text": "Title: Summary of Part 3 of the Course P7 \n Text: {simple state machines\n{--}{{}{}\n{}{}{}\n synchronous counter\n ripple counter\n serialization (of bit-sliced design) \n Question: \nQ. What are some of the differences between a synchronous counter and a ripple counter? \n Answer: \n\nA.\n\nSynchronous counters are typically faster than ripple counters, since the clock signal is only propagated through the flip-flops on each clock cycle. This can make synchronous counters more difficult to design and debug, however. Ripple counters, on the other hand, propagate the clock signal through the flip-flops on each clock cycle, which can make them slower but easier to design and debug."}, {"text": "Title: Summary of Part 3 of the Course P8 \n Text: {finite state machines (FSMs)\n{--}{{}{}\n{}{}{}\n states and state representation\n transition rule\n self-loop\n next state (+) notation\n meaning of don't care in input  combination\n meaning of don't care in output\n unused states and initialization\n completeness (with regard to  FSM specification)\n list of (abstract) states\n next-state table/state transition table/state table\n state transition diagram/transition  diagram/state diagram \n Question: \nQ. What is the meaning of don't care in input combination? \n Answer: \n\nThe don't care in input combination means that the output of the FSM does not depend on the input."}, {"text": "Title: Summary of Part 3 of the Course P9 \n Text: {memory\n{--}{{}{}\n{}{}{}\n number of addresses\n addressability\n read/write logic\n serial/random access memory (RAM)\n volatile/non-volatile (N-V)\n static/dynamic RAM (SRAM/DRAM)\n SRAM cell\n DRAM cell\n design as a collection of cells\n coincident selection\n bit lines and sense amplifiers \n Question: \nQ. How does the read/write logic work in a RAM? \n Answer: \n\nThe read/write logic in a RAM allows the user to read from and write to the memory. This is done by using the address lines to select the desired location in the memory, and then using the data lines to read or write the data."}, {"text": "Title: Summary of Part 3 of the Course P10 \n Text: {von Neumann model\n{--}{{}{}\n{}{}{}\n{processing unit\n{--}{{}{}\n{}{}{}\n register file\n arithmetic logic unit (ALU)\n word size \n Question: \nQ. What is the word size of the ALU in the von Neumann model? \n Answer: \n\nThe word size of the ALU is the same as the word size of the register file."}, {"text": "Title: Summary of Part 3 of the Course P11 \n Text: }\n{control unit\n{--}{{}{}\n{}{}{}\n program counter (PC)\n instruction register (IR)\n implementation as FSM \n Question: \nQ. What is the control unit?\nQ. \nQ. The control unit is a portion of a computer's central processing unit (CPU) that directs the operation of the processor. It tells the computer's memory, arithmetic/logic unit, and input/output devices what to do. \n Answer: "}, {"text": "Title: Summary of Part 3 of the Course P12 \n Text: }\n input and output units\n{memory\n{--}{{}{}\n{}{}{}\n memory address register (MAR)\n memory data register (MDR) \n Question: \nQ. What are the three units in a computer's memory?\nQ. \nQ. The three units in a computer's memory are the memory address register (MAR), the memory data register (MDR), and the memory buffer register (MBR). \n Answer: "}, {"text": "Title: Summary of Part 3 of the Course P13 \n Text: {tri-state buffer\n{--}{{}{}\n{}{}{}\n meaning of Z/hi-Z output\n use in distributed mux \n Question: \nQ. What does the Z/hi-Z output of a tri-state buffer mean?\nQ. \nQ. The Z/hi-Z output of a tri-state buffer indicates that the buffer is in a high impedance state. This means that the buffer is not driving any current and is effectively disconnected from the circuit. This can be used in a distributed multiplexer to allow multiple signals to share the same line. \n Answer: "}, {"text": "Title: Summary of Part 3 of the Course P14 \n Text: \n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (of an encoded instruction)}\n{operation code (opcode)}\n{types of instructions}\n{-}{{}{}\n{}{}{} \n Question: \nQ. What is an ISA? \n Answer: \n\nA. An ISA is an instruction set architecture. It is the interface between the hardware and software of a computer."}, {"text": "Title: Summary of Part 3 of the Course P15 \n Text: We expect you to be able to exercise the following skills: \n Question: \nQ. What is the author's purpose in writing this text?\nQ. \nQ. The author's purpose in writing this text is to inform the reader about the benefits of meditation. \n Answer: "}, {"text": "Title: Summary of Part 3 of the Course P16 \n Text: {Transform a bit-sliced design into a serial design, and explain the \ntradeoffs involved in terms of area and time required to compute a result.}\n{Based on a transition diagram, implement a synchronous counter from \nflip-flops and logic gates.}\n{Implement a binary ripple counter (but not necessarily a more general \ntype of ripple counter) from flip-flops and logic gates.}\n{Given an FSM implemented as digital logic, analyze the FSM to produce \na state transition diagram.}\n{Design an FSM to meet an abstract specification for a task, including \nproduction of specified output signals, and possibly including selection \nof appropriate inputs.}\n{Complete the specification of an FSM by ensuring that each state \nincludes a transition rule for every possible input combination.}\n{Compose memory chips into larger memory systems, using additional\ndecoders when necessary.}\n{Encode {LC-3} instructions into machine code.}\n{Read and understand programs written in {LC-3} assembly/machine code.} \n Question: \nQ. What is the difference between a synchronous and asynchronous counter? \n Answer: \n\nA. A synchronous counter is a counter in which the flip-flops are all clocked by the same clock signal. An asynchronous counter is a counter in which the flip-flops are not all clocked by the same clock signal."}, {"text": "Title: Summary of Part 3 of the Course P17 \n Text: \nAt a higher level, we expect that you understand the concepts and ideas \nsufficiently well to do the following: \n Question: \nQ. 1. What is the author's purpose in writing this text?\nQ. 2. What does the author mean by \"the world of ideas\"?\nQ. 3. What is the author's opinion of the current state of the world?\nQ. 4. What does the author believe is the cause of the world's problems?\nQ. 5. What does the author think is the solution to the world's problems? \n Answer: \n\n1. The author's purpose in writing this text is to explain his belief that the world is in a state of decline because it has lost touch with the world of ideas.\n2. The author means by \"the world of ideas\" the realm of abstract thought and knowledge, as opposed to the physical world.\n3. The author's opinion of the current state of the world is that it is in a state of decline because it has lost touch with the world of ideas.\n4. The author believes that the cause of the world's problems is that it has lost touch with the world of ideas.\n5. The author thinks that the solution to the world's problems is to reconnect with the world of ideas."}, {"text": "Title: Summary of Part 3 of the Course P18 \n Text: {Abstract design symmetries from an FSM specification in order to \nsimplify the implementation.}\n{Make use of a high-level state design, possibly with many sub-states \nin each high-level state, to simplify the implementation.}\n{Use counters to insert time-based transitions between states (such \nas timeouts).}\n{Implement an FSM using logic components such as registers, \ncounters, comparators, and adders as building blocks.}\n{Explain the basic organization of a computer's microarchitecture\nas well as the role played by elements of a von Neumann design in the\nprocessing of instructions.}\n{Identify the stages of processing an instruction (such as fetch,\ndecode, getting operands, execution, and writing back results) in a \nprocessor control unit state machine diagram.} \n Question: \nQ. What are abstract design symmetries? \n Answer: \n\nA. Abstract design symmetries are relationships between states in an FSM specification that can be used to simplify the implementation."}, {"text": "Title: Summary of Part 3 of the Course P19 \n Text: \nAnd, at the highest level, we expect that you will be able to do the following: \n Question: \nQ. How does the author feel about the subject?\nQ. \nQ. The author seems to be very passionate about the subject. \n Answer: "}, {"text": "Title: Summary of Part 3 of the Course P20 \n Text: {Explain the difference between the Moore and Mealy machine models, \nas well as why you might find each of them useful when designing an FSM.}\n{Understand the need for initialization of an FSM, be able to analyze \nand identify potential problems arising from lack of initialization, and \nbe able to extend an implementation to include initialization to an \nappropriate state when necessary.}\n{Understand how the choice of internal state bits for an FSM can \naffect the complexity of the implementation of next-state and output \nlogic, and be able to select a reasonable state assignment.}\n{Identify and fix design flaws in simple FSMs by analyzing an existing \nimplementation, comparing it with the specification, and removing any \ndifferences by making any necessary changes to the implementation.} \n Question: \nQ. What are the differences between Moore and Mealy machines? \n Answer: \n\n\n\nA. Moore machines have outputs that only depend on the current state, while Mealy machines have outputs that depend on both the current state and the current input. Moore machines are therefore easier to design and implement, but Mealy machines can be more efficient in terms of the number of states required."}, {"text": "Title: Summary of Part 3 of the Course P21 \n Text: {   }  empty 3rd page \n Question: \nQ. What is the title of the book?\nQ. \nQ. The Catcher in the Rye \n Answer: "}, {"text": "Title: Control Unit Design P0 \n Text: Appendix C of the Patt and Patel textbook describes a microarchitecture for\nthe LC-3 ISA, including a control unit implementation. \n Question: \nQ. What is the control unit implementation in the LC-3 microarchitecture? \n Answer: \n\nA. The control unit implementation in the LC-3 microarchitecture is a control unit that interprets the LC-3 instruction set and performs the operations specified by those instructions."}, {"text": "Title: Control Unit Design P1 \n Text: In this set of notes, we introduce a few concepts and strategies for\ncontrol unit design, using the textbook's {LC-3} microarchitecture \nto help illustrate them.  Several figures from the textbook are\nreproduced with permission in these notes as an aid to understanding. \n Question: \nQ. What is the LC-3 microarchitecture? \n Answer: \n\nThe LC-3 microarchitecture is a set of rules and conventions for designing and implementing a microprocessor. It is based on the von Neumann model of computation, with a central processing unit (CPU) that contains a control unit and an arithmetic logic unit (ALU). The LC-3 microarchitecture is an enhanced version of the original LC-3, which was developed at the University of California, Berkeley."}, {"text": "Title: Control Unit Design P2 \n Text: The control unit of a computer based on the von Neumann model can be viewed\nas an FSM that fetches instructions from memory and executes them.  Many\npossible implementations exist both for the control unit itself and for the\nresources that it controls, the other components in the von Neumann model,\nwhich we collectively call the { datapath}. \n Question: \nQ. What does the control unit of a computer based on the von Neumann model do? \n Answer: \n\nA. The control unit of a computer based on the von Neumann model fetches instructions from memory and executes them."}, {"text": "Title: Control Unit Design P3 \n Text: In this set of notes, we discuss two strategies for structured control\nunit design and introduce the idea of using memories to encode logic \nfunctions. \n Question: \nQ. What is the difference between the two strategies for structured control unit design? \n Answer: \n\nThe first strategy is to use a hierarchy of control units, each responsible for a specific task. The second strategy is to use a single control unit that is responsible for all tasks."}, {"text": "Title: Control Unit Design P4 \n Text: Let's begin by recalling that the control unit consists of three parts: a \nhigh-level FSM that controls instruction processing, a program counter (PC)\nregister that holds the address of the next instruction to be executed,\nand an instruction register (IR) that holds the current instruction as\nit executes. \n Question: \nQ. What is the purpose of the instruction register? \n Answer: \n\nA. The instruction register holds the current instruction as it executes."}, {"text": "Title: Control Unit Design P5 \n Text: Other von Neumann components provide inputs to the control unit.  The\nmemory unit, for example, contains the instructions and data on which \nthe program executes.   \n Question: \nQ. What other components does the von Neumann model have, and what are their functions? \n Answer: \n\nA. Other components in the von Neumann model include the memory unit and the input/output unit. The memory unit stores instructions and data for the program, while the input/output unit handles communication with external devices."}, {"text": "Title: Control Unit Design P6 \n Text: The processing unit contains a register file and condition \ncodes (N, Z, and P for the {LC-3} ISA).   \n Question: \nQ. What does the register file in the processing unit store? \n Answer: \n\nA. The register file stores the data that the processing unit uses to carry out its operations. This data includes the values of the operands used in the operations, as well as the values of the condition codes."}, {"text": "Title: Control Unit Design P7 \n Text: The outputs of the control unit are signals that control operation of the\ndatapath: the processing unit, the memory, and the I/O interfaces. \n Question: \nQ. What is the control unit? \n Answer: \n\nA.\n\nThe control unit is the component of the CPU that controls the overall operation of the processor. It coordinates the activities of the datapath, the processing unit, the memory, and the I/O interfaces."}, {"text": "Title: Control Unit Design P8 \n Text: The basic problem that we must solve, then, for control unit design, is to \nmap instruction processing and the state of the FSM (including the PC and\nthe IR) into appropriate sequences of { control signals} for the datapath. \n Question: \nQ. What is the basic problem that must be solved for control unit design? \n Answer: \n\nThe basic problem that must be solved for control unit design is to map instruction processing and the state of the FSM (including the PC and the IR) into appropriate sequences of control signals for the datapath."}, {"text": "Title: LC-3 Datapath Control Signals P0 \n Text: As we have skipped over the implementation details of interrupts and \nprivilege of the {LC-3} in our class, let's consider a \nmicroarchitecture and datapath without those capabilities.   \n Question: \nQ. What is the control unit implementation in the LC-3 microarchitecture? \n Answer: \n\nA. The control unit implementation in the LC-3 microarchitecture is a control unit that interprets the LC-3 instruction set and performs the operations specified by those instructions."}, {"text": "Title: LC-3 Datapath Control Signals P1 \n Text: The figure on the next page (Patt and Patel Figure C.3) shows an {LC-3}\ndatapath and control signals without support for interrupts and privilege. \n Question: \nQ. What is the LC-3 microarchitecture? \n Answer: \n\nThe LC-3 microarchitecture is a set of rules and conventions for designing and implementing a microprocessor. It is based on the von Neumann model of computation, with a central processing unit (CPU) that contains a control unit and an arithmetic logic unit (ALU). The LC-3 microarchitecture is an enhanced version of the original LC-3, which was developed at the University of California, Berkeley."}, {"text": "Title: LC-3 Datapath Control Signals P2 \n Text: \nSome of the datapath control signals mentioned\nin the textbook are no longer necessary in the simplified design. \n Question: \nQ. What does the control unit of a computer based on the von Neumann model do? \n Answer: \n\nA. The control unit of a computer based on the von Neumann model fetches instructions from memory and executes them."}, {"text": "Title: LC-3 Datapath Control Signals P3 \n Text: Let's discuss the signals that remain and give some examples of how they\nare used.  A list appears to the right. \n Question: \nQ. What is the difference between the two strategies for structured control unit design? \n Answer: \n\nThe first strategy is to use a hierarchy of control units, each responsible for a specific task. The second strategy is to use a single control unit that is responsible for all tasks."}, {"text": "Title: LC-3 Datapath Control Signals P4 \n Text: First, we have a set of seven {1-bit} control signals (starting \nwith ``LD.'') that specifies whether registers in the datapath load \nnew values. \n Question: \nQ. What is the purpose of the instruction register? \n Answer: \n\nA. The instruction register holds the current instruction as it executes."}, {"text": "Title: LC-3 Datapath Control Signals P5 \n Text: Next, there are four {1-bit} signals (starting with ``Gate'') for \ntri-state buffers that control access to the bus.  These four implement \na distributed mux for the bus.  Only one value can \n Question: \nQ. What other components does the von Neumann model have, and what are their functions? \n Answer: \n\nA. Other components in the von Neumann model include the memory unit and the input/output unit. The memory unit stores instructions and data for the program, while the input/output unit handles communication with external devices."}, {"text": "Title: LC-3 Datapath Control Signals P6 \n Text: signal& meaning \nLD.MAR& load new value into memory address register\nLD.MDR& load new value into memory data register\nLD.IR& load new value into instruction register\nLD.BEN& load new value into branch enable register\nLD.REG& load new value into register file\nLD.CC& load new values into condition code registers (N,Z,P)\nLD.PC& load new value into program counter \nGatePC& write program counter value onto bus\nGateMDR& write memory data register onto bus\nGateALU& write arithmetic logic unit result onto bus\nGateMARMUX& write memory address register mux output onto bus \nPCMUX& select value to write to program counter (2 bits)\nDRMUX& select value to write to destination register (2 bits)\nSR1MUX& select register to read from register file (2 bits)\nADDR1MUX& select register component of address (1 bit)\nADDR2MUX& select offset component of address (2 bits)\nMARMUX& select type of address generation (1 bit) \nALUK& select arithmetic logic unit operation (2 bits) \nMIO.EN& enable memory \nR.W& read or write from memory  \n Question: \nQ. What does the register file in the processing unit store? \n Answer: \n\nA. The register file stores the data that the processing unit uses to carry out its operations. This data includes the values of the operands used in the operations, as well as the values of the condition codes."}, {"text": "Title: LC-3 Datapath Control Signals P7 \n Text: appear on the bus in \nany cycle, so at most one of these signals can be 1; others must all \nbe 0 to avoid creating a short. \n Question: \nQ. What is the control unit? \n Answer: \n\nA.\n\nThe control unit is the component of the CPU that controls the overall operation of the processor. It coordinates the activities of the datapath, the processing unit, the memory, and the I/O interfaces."}, {"text": "Title: LC-3 Datapath Control Signals P8 \n Text: The third group (ending with ``MUX'') of signals\ncontrols multiplexers in the datapath.\nThe number of bits for each depends on the number of inputs to the mux;\nthe total number of signals is 10.\nThe last two groups of signals control the ALU and the memory, \nrequiring a total of 4 more signals.  The total of all groups is thus\n25 control signals for the datapath without support for privilege\nand interrupts. \n Question: \nQ. What is the basic problem that must be solved for control unit design? \n Answer: \n\nThe basic problem that must be solved for control unit design is to map instruction processing and the state of the FSM (including the PC and the IR) into appropriate sequences of control signals for the datapath."}, {"text": "Title: Example Control Word: ADD P0 \n Text: Before we begin to discuss control unit design in more detail, let's\nwork through a couple of examples of implementing specific RTL with\nthe control signals available.  The figure below (Patt and Patel \nFigure C.2) shows a state machine for the {LC-3} ISA (again \nwithout detail on interrupts and privilege). \n Question: \nQ. What is the control unit implementation in the LC-3 microarchitecture? \n Answer: \n\nA. The control unit implementation in the LC-3 microarchitecture is a control unit that interprets the LC-3 instruction set and performs the operations specified by those instructions."}, {"text": "Title: Example Control Word: ADD P1 \n Text: Consider now the state that implements the ADD instruction---state\nnumber 1 in the figure on the previous page, just below and to the \nleft of the decode state. \n Question: \nQ. What is the LC-3 microarchitecture? \n Answer: \n\nThe LC-3 microarchitecture is a set of rules and conventions for designing and implementing a microprocessor. It is based on the von Neumann model of computation, with a central processing unit (CPU) that contains a control unit and an arithmetic logic unit (ALU). The LC-3 microarchitecture is an enhanced version of the original LC-3, which was developed at the University of California, Berkeley."}, {"text": "Title: Example Control Word: ADD P2 \n Text: The RTL for the state is: DR  SR + OP2, set CC. \n Question: \nQ. What does the control unit of a computer based on the von Neumann model do? \n Answer: \n\nA. The control unit of a computer based on the von Neumann model fetches instructions from memory and executes them."}, {"text": "Title: Example Control Word: ADD P3 \n Text: We can think of the 25 control signals that implement the desired RTL\nas a { control word} for the datapath.  \n Question: \nQ. What is the difference between the two strategies for structured control unit design? \n Answer: \n\nThe first strategy is to use a hierarchy of control units, each responsible for a specific task. The second strategy is to use a single control unit that is responsible for all tasks."}, {"text": "Title: Example Control Word: ADD P4 \n Text: Let's begin with the register load control signals.  The RTL writes\nto two types of registers: the register file and the condition codes.\nTo accomplish these simultaneous writes, LD.REG and LD.CC must be high.\nNo other registers in the datapath should change, so the other five\nLD signals---LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC---should be low. \n Question: \nQ. What is the purpose of the instruction register? \n Answer: \n\nA. The instruction register holds the current instruction as it executes."}, {"text": "Title: Example Control Word: ADD P5 \n Text: What about the bus?  In order to write the result of the add operation\ninto the register file, the control signals must allow the ALU to write\nits result on to the bus.  So we need GateALU=1.  And the other Gate\nsignals---GatePC, GateMDR, and GateMARMUX---must all be 0.  The \ncondition codes are also calculated from the value on the bus, but\nthey are calculated on the same value as is written to the register\nfile (by the definition of the {LC-3} ISA). \n Question: \nQ. What other components does the von Neumann model have, and what are their functions? \n Answer: \n\nA. Other components in the von Neumann model include the memory unit and the input/output unit. The memory unit stores instructions and data for the program, while the input/output unit handles communication with external devices."}, {"text": "Title: Example Control Word: ADD P6 \n Text: If the RTL for an FSM state implicitly requires more than one value \nto appear on the bus in the same cycle, that state is impossible to \nimplement using the given datapath.  Either the datapath or the state \nmachine must be changed in such a case.  The textbook's design has been \nfairly thoroughly tested and debugged. \n Question: \nQ. What does the register file in the processing unit store? \n Answer: \n\nA. The register file stores the data that the processing unit uses to carry out its operations. This data includes the values of the operands used in the operations, as well as the values of the condition codes."}, {"text": "Title: Example Control Word: ADD P7 \n Text: \nThe earlier figure of the datapath does not \nshow all of the muxes.  The remaining muxes appear in the figure \nto the right (Patt and Patel Figure C.6). \n Question: \nQ. What is the control unit? \n Answer: \n\nA.\n\nThe control unit is the component of the CPU that controls the overall operation of the processor. It coordinates the activities of the datapath, the processing unit, the memory, and the I/O interfaces."}, {"text": "Title: Example Control Word: ADD P8 \n Text: Some of the muxes in the datapath must be used to enable the\naddition needed for ADD to occur.  The\nDRMUX must select its IR[11:9] \ninput in order to write to the destination register specified by the \nADD instruction.  Similarly, the SR1MUX must select its IR[8:6] input \nin order to \n Question: \nQ. What is the basic problem that must be solved for control unit design? \n Answer: \n\nThe basic problem that must be solved for control unit design is to map instruction processing and the state of the FSM (including the PC and the IR) into appropriate sequences of control signals for the datapath."}, {"text": "Title: Example Control Word: ADD P9 \n Text: pass the first source register specified by the ADD to the ALU as input A\n(see the datapath figure).  SR2MUX is always controlled by the \nmode bit IR[5], so the control unit does not need to generate anything\n(note that this signal was not in the list given earlier). \n Question: \nQ. What are interrupts and privilege? \n Answer: \n\nA.\n\nInterrupts are a type of signal that can be sent to a microprocessor to indicate that an event has occurred that needs attention. Privilege is a term used to describe the different levels of access that a user may have to a computer system."}, {"text": "Title: Example Control Word: ADD P10 \n Text: The rest of the muxes in the datapath---PCMUX, ADDR1MUX, ADDR2MUX, and\nMARMUX---do not matter, and the signals controlling them are don't cares.\nFor example, since the PC does not change, the output of the PCMUX\nis simply discarded, thus which input the PCMUX forwards to its output\ncannot matter. \n Question: \nQ. What is the LC-3? \n Answer: \n\nA. LC-3 is a computer architecture."}, {"text": "Title: Example Control Word: ADD P11 \n Text: The ALU must perform an addition, so we must set the operation type\nALUK appropriately.  And memory should not be enabled (MIO.EN=0),\nin which case the read/write control for memory, R.W, is a don't\ncare.  These 25 signal values together (including seven don't cares)\nimplement the RTL for the single state of execution for an \nADD instruction. \n Question: \nQ. Some of the datapath control signals mentioned in the textbook are no longer necessary in the simplified design.\nQ. \nQ. Why are some of the datapath control signals mentioned in the textbook no longer necessary in the simplified design? \n Answer: \n\nThe simplified design requires fewer datapath control signals because it uses a smaller and more efficient datapath."}, {"text": "Title: Example Control Word: LDR P0 \n Text: As a second example, consider the first state in the sequence that \nimplements the LDR instruction---state number 6 in the figure on the \nprevious page. \n Question: \nQ. What is the control unit implementation in the LC-3 microarchitecture? \n Answer: \n\nA. The control unit implementation in the LC-3 microarchitecture is a control unit that interprets the LC-3 instruction set and performs the operations specified by those instructions."}, {"text": "Title: Example Control Word: LDR P1 \n Text: The RTL for the state is: MAR  BaseR + off6, but BaseR is\nabbreviated to ``B'' in the state diagram. \n Question: \nQ. What is the LC-3 microarchitecture? \n Answer: \n\nThe LC-3 microarchitecture is a set of rules and conventions for designing and implementing a microprocessor. It is based on the von Neumann model of computation, with a central processing unit (CPU) that contains a control unit and an arithmetic logic unit (ALU). The LC-3 microarchitecture is an enhanced version of the original LC-3, which was developed at the University of California, Berkeley."}, {"text": "Title: Example Control Word: LDR P2 \n Text: What is the control word for this state? \n Question: \nQ. What does the control unit of a computer based on the von Neumann model do? \n Answer: \n\nA. The control unit of a computer based on the von Neumann model fetches instructions from memory and executes them."}, {"text": "Title: Example Control Word: LDR P3 \n Text: Let's again begin with the register load control signals.  Only the\nMAR is written by the RTL, so we need LD.MAR=1 and the other load\nsignals all equal to 0. \n Question: \nQ. What is the difference between the two strategies for structured control unit design? \n Answer: \n\nThe first strategy is to use a hierarchy of control units, each responsible for a specific task. The second strategy is to use a single control unit that is responsible for all tasks."}, {"text": "Title: Example Control Word: LDR P4 \n Text: The address (BaseR + off6) is generated by the address adder, then \npasses through the MARMUX to the bus, from which it can be written into\nthe MAR.  To allow the MARMUX to write to the bus, we set GateMARMUX\nhigh and set the other three Gate control signals low. \n Question: \nQ. What is the purpose of the instruction register? \n Answer: \n\nA. The instruction register holds the current instruction as it executes."}, {"text": "Title: Example Control Word: LDR P5 \n Text: More of the muxes are needed for this state's RTL than we needed for\nthe ADD execution state's RTL. \n Question: \nQ. What other components does the von Neumann model have, and what are their functions? \n Answer: \n\nA. Other components in the von Neumann model include the memory unit and the input/output unit. The memory unit stores instructions and data for the program, while the input/output unit handles communication with external devices."}, {"text": "Title: Example Control Word: LDR P6 \n Text: The SR1MUX must again select its IR[8:6] input, this time in order to \npass the BaseR specified by the instruction to ADDR1MUX.  ADDR1MUX\nmust then select the output of the register file in order to pass the \nBaseR to the address adder.  The other input of the address adder should\nbe off6, which corresponds to the sign-\nextended version of IR[5:0].\nADDR2MUX must select this input to pass to the address adder. \nFinally, the MARMUX must select the output of the address adder.\nThe PCMUX and DRMUX do not matter for this case, and can be left as\ndon't cares.  Neither the PC nor any register in the register file\nis written. \n Question: \nQ. What does the register file in the processing unit store? \n Answer: \n\nA. The register file stores the data that the processing unit uses to carry out its operations. This data includes the values of the operands used in the operations, as well as the values of the condition codes."}, {"text": "Title: Example Control Word: LDR P7 \n Text: The output of the ALU is not used, so which operation it performs is \nirrelevant, and the ALUK controls are also don't cares.  Memory is\nalso not used, so again we set MIO.EN=0.  And, as before, the R.W\ncontrol for memory is a don't care. \n Question: \nQ. What is the control unit? \n Answer: \n\nA.\n\nThe control unit is the component of the CPU that controls the overall operation of the processor. It coordinates the activities of the datapath, the processing unit, the memory, and the I/O interfaces."}, {"text": "Title: Example Control Word: LDR P8 \n Text: These 25 signal values together (again including seven don't cares)\nimplement the RTL for the first state of LDR execution. \n Question: \nQ. What is the basic problem that must be solved for control unit design? \n Answer: \n\nThe basic problem that must be solved for control unit design is to map instruction processing and the state of the FSM (including the PC and the IR) into appropriate sequences of control signals for the datapath."}, {"text": "Title: Hardwired Control P0 \n Text: \nNow we are ready to think about how control signals can be generated. \n Question: \nQ. What is the control unit implementation in the LC-3 microarchitecture? \n Answer: \n\nA. The control unit implementation in the LC-3 microarchitecture is a control unit that interprets the LC-3 instruction set and performs the operations specified by those instructions."}, {"text": "Title: Hardwired Control P1 \n Text: As illustrated to the right, instruction processing consists of two \nsteps repeated infinitely: fetch an instruction, then execute the \ninstruction. \n Question: \nQ. What is the LC-3 microarchitecture? \n Answer: \n\nThe LC-3 microarchitecture is a set of rules and conventions for designing and implementing a microprocessor. It is based on the von Neumann model of computation, with a central processing unit (CPU) that contains a control unit and an arithmetic logic unit (ALU). The LC-3 microarchitecture is an enhanced version of the original LC-3, which was developed at the University of California, Berkeley."}, {"text": "Title: Hardwired Control P2 \n Text: Let's say that we choose a fixed number of cycles for each of these two\nsteps.  We can then control our system with a \n Question: \nQ. What does the control unit of a computer based on the von Neumann model do? \n Answer: \n\nA. The control unit of a computer based on the von Neumann model fetches instructions from memory and executes them."}, {"text": "Title: Hardwired Control P3 \n Text: counter, using the counter's value and the IR to generate the\ncontrol signals through combinational\nlogic.  The PC is used only as data and has little or no direct effect on \nhow the system fetches and processes instructions.{Some ISAs do \nsplit the address space into privileged and non-privileged regions, but \nwe ignore that possibility here.} \n Question: \nQ. What is the difference between the two strategies for structured control unit design? \n Answer: \n\nThe first strategy is to use a hierarchy of control units, each responsible for a specific task. The second strategy is to use a single control unit that is responsible for all tasks."}, {"text": "Title: Hardwired Control P4 \n Text: This approach in general is called { hardwired control}. \n Question: \nQ. What is the purpose of the instruction register? \n Answer: \n\nA. The instruction register holds the current instruction as it executes."}, {"text": "Title: Hardwired Control P5 \n Text: How many cycles do we need for instruction fetch?  How many cycles do\nwe need for instruction processing?  The answers depend on the factors:\nthe complexity of the ISA, and the complexity of the datapath. \n Question: \nQ. What other components does the von Neumann model have, and what are their functions? \n Answer: \n\nA. Other components in the von Neumann model include the memory unit and the input/output unit. The memory unit stores instructions and data for the program, while the input/output unit handles communication with external devices."}, {"text": "Title: Hardwired Control P6 \n Text: Given a simple ISA, we can design a datapath that\nis powerful enough to process any instruction in a single cycle.  \nThe control unit for such a design is\nan example of { single-cycle, hardwired control}.  While this\napproach simplifies the control unit, the cycle time\nfor the clock is limited by the slowest instruction.\nThe clock must also be slow enough to let memory \noperations complete in single cycle, both for instruction fetch and\nfor instructions that require a memory access. \n Question: \nQ. What does the register file in the processing unit store? \n Answer: \n\nA. The register file stores the data that the processing unit uses to carry out its operations. This data includes the values of the operands used in the operations, as well as the values of the condition codes."}, {"text": "Title: Hardwired Control P7 \n Text: Such a low clock rate is usually not acceptable. \n Question: \nQ. What is the control unit? \n Answer: \n\nA.\n\nThe control unit is the component of the CPU that controls the overall operation of the processor. It coordinates the activities of the datapath, the processing unit, the memory, and the I/O interfaces."}, {"text": "Title: Hardwired Control P8 \n Text: More generally, we can use a simpler datapath and break both instruction\nfetch and instruction processing into multiple steps.  Using the datapath\nin the figure from Patt and Patel, for example, instruction fetch requires \nthree steps, and the number of steps for instruction processing depends \non the type of instruction being processed.  The state diagram shown\nearlier illustrates the steps for each opcode. \n Question: \nQ. What is the basic problem that must be solved for control unit design? \n Answer: \n\nThe basic problem that must be solved for control unit design is to map instruction processing and the state of the FSM (including the PC and the IR) into appropriate sequences of control signals for the datapath."}, {"text": "Title: Hardwired Control P9 \n Text: Although the datapath is not powerful enough to complete instructions\nin a single cycle, we can build a { multi-cycle, hardwired control} \nunit (one based on combinational logic).   \n Question: \nQ. What are interrupts and privilege? \n Answer: \n\nA.\n\nInterrupts are a type of signal that can be sent to a microprocessor to indicate that an event has occurred that needs attention. Privilege is a term used to describe the different levels of access that a user may have to a computer system."}, {"text": "Title: Hardwired Control P10 \n Text: In fact, this type of control unit is not much more complex than the \nsingle-cycle version.  For the control unit's FSM, we can use a binary \ncounter to enumerate first the steps of fetch, then the steps\nof processing.  The counter value along with the IR register can \ndrive combinational logic to produce control signals.  And, to avoid\nprocessing at the speed of the slowest instruction (the opcode that\nrequires the largest number of steps; LDI and STI in the {LC-3}\nstate diagram), we can add a reset signal to the counter to force it \nback to instruction fetch.  The FSM counter reset signal is simply \nanother control signal.  Finally, we can add one more signal that \npauses the counter while waiting for a memory operation to complete.  \nThe system clock can then run at the speed of the logic rather than \nat the speed of memory. \n Question: \nQ. What is the LC-3? \n Answer: \n\nA. LC-3 is a computer architecture."}, {"text": "Title: Hardwired Control P11 \n Text: The control unit implementation discussed in Appendix C of Patt and Patel \nis not hardwired, but it does make use of a memory ready signal to \nachieve this decoupling between the clock speed of the processor and the\naccess time of the memory. \n Question: \nQ. Some of the datapath control signals mentioned in the textbook are no longer necessary in the simplified design.\nQ. \nQ. Why are some of the datapath control signals mentioned in the textbook no longer necessary in the simplified design? \n Answer: \n\nThe simplified design requires fewer datapath control signals because it uses a smaller and more efficient datapath."}, {"text": "Title: Hardwired Control P12 \n Text: \nThe figure to the right illustrates a general \nmulti-cycle, hardwired control unit.  The three blocks on\nthe left are the control unit state.  The combinational logic in\nthe middle uses the control unit state along with some\ndatapath status bits to compute the control signals for the datapath\nand the extra controls needed for the FSM counter, IR, and PC.\nThe datapath appears to the right in the figure. \n Question: \nQ. What does the author mean by \"signals that remain\"? \n Answer: \n\nA. The author means the signals that are still used after the update."}, {"text": "Title: Hardwired Control P13 \n Text: How complex is the combinational logic?  As mentioned earlier, we \nassume that the PC does not directly affect control.  But we still \nhave 16 bits of IR, the FSM counter state, and the datapath status\nsignals.  Perhaps we need 24-variable {K-maps}?   \n Question: \nQ. What is the purpose of the control signals?\nQ. \nQ. The control signals are used to specify whether or not registers in the datapath should load new values. \n Answer: "}, {"text": "Title: Hardwired Control P14 \n Text: Here's where engineering and human design come to \nthe rescue: by careful design of the ISA and the encoding, the\nauthors have made many of the datapath control signals for the {LC-3}\nISA quite simple.\nFor example, the register that appears on the register file's SR2\noutput is always specified by IR[2:0].  The SR1 output \nrequires a mux, but the choices are limited to IR[11:9] and IR[8:6]\n(and R6 in the design with support for interrupts).\nSimilarly, the destination register in the register file\nis always R7 or IR[11:9] (or, again, R6 when supporting interrupts). \n Question: \nQ. What is the purpose of the four 1-bit signals that start with \"Gate\"?\nQ. \nQ. The four 1-bit signals that start with \"Gate\" control access to the bus. They implement a distributed mux for the bus, allowing only one value to be placed on the bus at a time. \n Answer: "}, {"text": "Title: Hardwired Control P15 \n Text: The control signals for an {LC-3}\ndatapath depend almost entirely on the state of the control unit \nFSM (counter bits in a hardwired design) and the opcode IR[15:12]. \n Question: \nQ. What is the meaning of the LD.MAR signal?\nQ. \nQ. The LD.MAR signal loads a new value into the memory address register. \n Answer: "}, {"text": "Title: Hardwired Control P16 \n Text: The control signals are thus reduced to fairly simple functions. \n Question: \nQ. Why can only one of the signals be 1? \n Answer: \n\nA. If more than one of the signals were 1, it would create a short."}, {"text": "Title: Hardwired Control P17 \n Text: Let's imagine building a hardwired control unit for the {LC-3}. \n Question: \nQ. What is the purpose of the control signals?\nQ. \nQ. The control signals are used to control various components in the datapath, such as multiplexers, the ALU, and memory. \n Answer: "}, {"text": "Title: Hardwired Control P18 \n Text: Let's start by being more precise about the number of inputs to\nthe combinational logic. \n Question: \nQ. What is the purpose of the control signals available? \n Answer: \n\nA. The control signals available are used to implement specific RTL."}, {"text": "Title: Hardwired Control P19 \n Text: Although most decisions are based on the opcode, the datapath\nand state diagram shown earlier for the {LC-3} ISA do have\none instance of using another instruction bit to determine behavior.\nSpecifically, the JSR instruction has two modes, and the control\nunit uses IR[11] to choose between them.  So we need to have\nfive bits of IR instead of four as input to our logic. \n Question: \nQ. What is state number 1 in the figure on the previous page?\nQ. \nQ. State number 1 is the state that implements the ADD instruction. \n Answer: "}, {"text": "Title: Hardwired Control P20 \n Text: How many datapath status signals are needed? \n Question: \nQ. What is the RTL for the state?\nQ. \nQ. The RTL for the state is: DR  SR + OP2, set CC. \n Answer: "}, {"text": "Title: Hardwired Control P21 \n Text: When the control unit accesses memory, it must wait until the \nmemory finishes the access, as indicated by a memory ready signal R.\nAnd the control unit must implement the conditional part of conditional\nbranches, for which it uses the datapath's branch enable signal BEN. \n Question: \nQ. What are the 25 control signals that implement the desired RTL? \n Answer: \n\nThe 25 control signals that implement the desired RTL are the control signals that determine the operation of the datapath. These signals specify the sequence of operations that the datapath should perform in order to implement the desired RTL."}, {"text": "Title: Hardwired Control P22 \n Text: These two datapath status signals suffice for our design. \n Question: \nQ. What are the five LD signals that should be low? \n Answer: \n\nA. LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC."}, {"text": "Title: Hardwired Control P23 \n Text: How many bits do we need for the counter?\nInstruction fetch requires three cycles: one to move\nthe PC to the MAR and increment the PC, a second to read from\nmemory into MDR, and a third to move the instruction bits across\nthe bus from MDR into IR.  Instruction decoding in a hardwired design\nis implicit and \nrequires no cycles: since all of our control signals can depend on\nthe IR, we do not need a cycle to change the FSM state to reflect the opcode.\nLooking at the {LC-3} state diagram, we see\nthat processing an instruction requires at most five cycles.  In\ntotal, at\nmost eight steps are needed to fetch and process any {LC-3} \ninstruction, so we can use a {3-bit} binary counter. \n Question: \nQ. What is the value that is written to the register file? \n Answer: \n\nA. The result of the add operation."}, {"text": "Title: Hardwired Control P24 \n Text: We thus have a total of ten bits of input: IR[15:11], R, BEN, and\na {3-bit} counter. \n Question: \nQ. How do you change the state machine so that it requires only one value on the bus in the same cycle? \n Answer: \n\nA. Change the state machine so that it requires only one value on the bus in the same cycle."}, {"text": "Title: Hardwired Control P25 \n Text: Adding the RESET and PAUSE controls for our FSM counter to the \n25 control signals listed earlier, we need to find 27 functions\non 10 variables. \n Question: \nQ. What is the purpose of the muxes? \n Answer: \n\nA. The muxes are used to select which data to send to the ALU."}, {"text": "Title: Hardwired Control P26 \n Text: That's still a lot of big {K-maps} to solve.  Is there an\neasier way? \n Question: \nQ. Q: What is the purpose of the DRMUX and SR1MUX in the datapath?\nQ. \nQ. A: The DRMUX and SR1MUX are used to enable the addition needed for ADD to occur. \n Answer: "}, {"text": "Title: Using a Memory for Logic Functions P0 \n Text: Consider a case in which you need to compute many functions on a small \nnumber of bits, such as we just described for the multi-cycle, hardwired\ncontrol unit.  One strategy is to use a memory (possibly a read-only memory).\nA {2^m} memory can be viewed as computing N arbitrary\nfunctions on m variables.  The functions to be computed are specified\nby filling in the bits of the memory.  So long as the value of m is fairly\nsmall, the memory (especially SRAM) can be fast. \n Question: \nQ. What is the control unit implementation in the LC-3 microarchitecture? \n Answer: \n\nA. The control unit implementation in the LC-3 microarchitecture is a control unit that interprets the LC-3 instruction set and performs the operations specified by those instructions."}, {"text": "Title: Using a Memory for Logic Functions P1 \n Text: Synthesis tools (or hard work) can, of course, produce smaller designs\nthat use fewer gates.  Actually, tools may be able to optimize a fixed\ndesign expressed as read-only memory, too.  But designing the functions\nwith a memory makes them easier to modify later.  If we make a mistake,\nfor example, in computing one of the functions, we need only change a bit\nor two in the memory instead of solving equations and reoptimizing and\nreplacing logic gates.  We can also extend our design if we have space\nremaining (that is, if the functions are undefined for some combinations \nof the m inputs).  The\nCray T3D supercomputer, for example, used a similar approach to add\nnew instructions to the Alpha processors on which it was based. \n Question: \nQ. What is the LC-3 microarchitecture? \n Answer: \n\nThe LC-3 microarchitecture is a set of rules and conventions for designing and implementing a microprocessor. It is based on the von Neumann model of computation, with a central processing unit (CPU) that contains a control unit and an arithmetic logic unit (ALU). The LC-3 microarchitecture is an enhanced version of the original LC-3, which was developed at the University of California, Berkeley."}, {"text": "Title: Using a Memory for Logic Functions P2 \n Text: This strategy is effective in many contexts, so let's briefly discuss two \nanalogous cases.  In software, a memory becomes a lookup table.  Before\nhandheld calculators, lookup tables were used by humans to compute \ntranscendental functions such as sines, cosines, logarithms.  Computer \ngraphics hardware and software used a similar approach for transcendental\nfunctions in order to reduce cost and improve \nspeed.  Functions such as counting 1 bits in a word are useful for \nprocessor scheduling and networking, but not all ISAs provide this type\nof instruction.  In such cases, lookup tables in software are often the\nbest solution. \n Question: \nQ. What does the control unit of a computer based on the von Neumann model do? \n Answer: \n\nA. The control unit of a computer based on the von Neumann model fetches instructions from memory and executes them."}, {"text": "Title: Using a Memory for Logic Functions P3 \n Text: In programmable hardware such as Field Programmable Gate Arrays (FPGAs),\nlookup tables (called LUTs in this context) have played an important role\nin implementing arbitrary logic functions. \n Question: \nQ. What is the difference between the two strategies for structured control unit design? \n Answer: \n\nThe first strategy is to use a hierarchy of control units, each responsible for a specific task. The second strategy is to use a single control unit that is responsible for all tasks."}, {"text": "Title: Using a Memory for Logic Functions P4 \n Text: The FPGA is the modern form of the programmable logic array (PLA)\nmentioned in the textbook, and will be your main tool \nfor developing digital hardware in ECE385. \n Question: \nQ. What is the purpose of the instruction register? \n Answer: \n\nA. The instruction register holds the current instruction as it executes."}, {"text": "Title: Using a Memory for Logic Functions P5 \n Text: For many years, FPGAs served as a hardware prototyping platform, but\nmany companies today ship their first round products using \ndesigns mapped to FPGAs.  Why?  Chips are more and more expensive\nto design, and mistakes are costly to fix.  In contrast, while\ncompanies pay more to buy an FPGA than to produce a chip (after the \nfirst chip!), errors in the design can usually be fixed by sending \ncustomers a new version through the Internet. \n Question: \nQ. What other components does the von Neumann model have, and what are their functions? \n Answer: \n\nA. Other components in the von Neumann model include the memory unit and the input/output unit. The memory unit stores instructions and data for the program, while the input/output unit handles communication with external devices."}, {"text": "Title: Using a Memory for Logic Functions P6 \n Text: Let's return to our {LC-3} example. \n Question: \nQ. What does the register file in the processing unit store? \n Answer: \n\nA. The register file stores the data that the processing unit uses to carry out its operations. This data includes the values of the operands used in the operations, as well as the values of the condition codes."}, {"text": "Title: Using a Memory for Logic Functions P7 \n Text: Instead of solving the {K-maps}, we can use a small memory:\n2^ bits (27,648 bits total). \n Question: \nQ. What is the control unit? \n Answer: \n\nA.\n\nThe control unit is the component of the CPU that controls the overall operation of the processor. It coordinates the activities of the datapath, the processing unit, the memory, and the I/O interfaces."}, {"text": "Title: Using a Memory for Logic Functions P8 \n Text: We just need calculate the bits, put\nthem into the memory, and use the memory to produce the control signals. \n Question: \nQ. What is the basic problem that must be solved for control unit design? \n Answer: \n\nThe basic problem that must be solved for control unit design is to map instruction processing and the state of the FSM (including the PC and the IR) into appropriate sequences of control signals for the datapath."}, {"text": "Title: Using a Memory for Logic Functions P9 \n Text: The ``address'' input to the memory are the same 10 bits that we\nneeded for our combinational logic: IR[15:11], R, BEN, and the FSM counter.\nThe data outputs of the memory are the control signals and the\nRESET and PAUSE inputs to the FSM counter.  And we're done. \n Question: \nQ. What are interrupts and privilege? \n Answer: \n\nA.\n\nInterrupts are a type of signal that can be sent to a microprocessor to indicate that an event has occurred that needs attention. Privilege is a term used to describe the different levels of access that a user may have to a computer system."}, {"text": "Title: Using a Memory for Logic Functions P10 \n Text: We can do a little better, though.  The datapath in the textbook was\ndesigned to work with the textbook's control unit.  If we add a little\nlogic, we can significantly simplify our memory-based, hardwired implementation.\nFor example, we only need to pause the FSM counter when waiting for memory.\nIf we can produce a control signal that indicates a need to wait for\nmemory, say {WAIT-MEM}, we can use a couple of gates to compute the\nFSM counter's PAUSE signal as {WAIT-MEM} AND (NOT R).  Making this change\nshrinks our memory to 2^9 bits.  The extra two control\nsignals in this case are RESET and {WAIT-MEM}. \n Question: \nQ. What is the LC-3? \n Answer: \n\nA. LC-3 is a computer architecture."}, {"text": "Title: Using a Memory for Logic Functions P11 \n Text: Next, look at how BEN is used in the state diagram:\nthe only use is to terminate the\nprocessing of branch instructions when no branch should occur (when \nBEN=0).  We can fold that functionality into the FSM counter's RESET\nsignal by producing a branch reset signal, {BR-RESET}, to reset\nthe counter to end a branch and a second signal, {INST-DONE},\nwhen an instruction is done.  The RESET input for the FSM counter\nis then ({BR-RESET} AND (NOT BEN)) OR {INST-DONE}. \n Question: \nQ. Some of the datapath control signals mentioned in the textbook are no longer necessary in the simplified design.\nQ. \nQ. Why are some of the datapath control signals mentioned in the textbook no longer necessary in the simplified design? \n Answer: \n\nThe simplified design requires fewer datapath control signals because it uses a smaller and more efficient datapath."}, {"text": "Title: Using a Memory for Logic Functions P12 \n Text: And our memory further shrinks to 2^8 bits, where the extra\nthree control signals are {WAIT-MEM}, {BR-RESET}, and \n{INST-DONE}. \n Question: \nQ. What does the author mean by \"signals that remain\"? \n Answer: \n\nA. The author means the signals that are still used after the update."}, {"text": "Title: Using a Memory for Logic Functions P13 \n Text: Finally, recall that the only need for IR[11] is to implement the two\nforms of JSR.  But we can add wires to connect SR1 to PCMUX's fourth \ninput, then control the PCMUX output selection using IR[11] when appropriate\n(using another control signal).  With this extension, we can implement\nboth forms with a single state, writing to both R7 and PC in the same\ncycle. \n Question: \nQ. What is the purpose of the control signals?\nQ. \nQ. The control signals are used to specify whether or not registers in the datapath should load new values. \n Answer: "}, {"text": "Title: Using a Memory for Logic Functions P14 \n Text: Our final memory can then be 2^7 bits (3,712 bits total),\nwhich is less than one-seventh the number of bits that we needed before \nmodifying the datapath. \n Question: \nQ. What is the purpose of the four 1-bit signals that start with \"Gate\"?\nQ. \nQ. The four 1-bit signals that start with \"Gate\" control access to the bus. They implement a distributed mux for the bus, allowing only one value to be placed on the bus at a time. \n Answer: "}, {"text": "Title: Microprogrammed Control P0 \n Text: We are now ready to discuss the second approach to control unit design.\nTake another look at the state diagram for the the {LC-3} ISA.  Does it \nremind you of anything?  Like a flowchart, it has relatively few arcs \nleaving each state---usually only one or two. \n Question: \nQ. What is the control unit implementation in the LC-3 microarchitecture? \n Answer: \n\nA. The control unit implementation in the LC-3 microarchitecture is a control unit that interprets the LC-3 instruction set and performs the operations specified by those instructions."}, {"text": "Title: Microprogrammed Control P1 \n Text: What if we treat the state diagram as a program?  We can use a small\nmemory to hold { microinstructions} (another name for control words) \nand use the FSM state number as the memory address.   \n Question: \nQ. What is the LC-3 microarchitecture? \n Answer: \n\nThe LC-3 microarchitecture is a set of rules and conventions for designing and implementing a microprocessor. It is based on the von Neumann model of computation, with a central processing unit (CPU) that contains a control unit and an arithmetic logic unit (ALU). The LC-3 microarchitecture is an enhanced version of the original LC-3, which was developed at the University of California, Berkeley."}, {"text": "Title: Microprogrammed Control P2 \n Text: Without support for interrupts or privilege, and with the datapath \nextension for JSR mentioned for hardwired control, the {LC-3} state \nmachine requires fewer than 32 states. \n Question: \nQ. What does the control unit of a computer based on the von Neumann model do? \n Answer: \n\nA. The control unit of a computer based on the von Neumann model fetches instructions from memory and executes them."}, {"text": "Title: Microprogrammed Control P3 \n Text: The datapath has 25 control signals, but we need one more for the\ndatapath extension for JSR. \n Question: \nQ. What is the difference between the two strategies for structured control unit design? \n Answer: \n\nThe first strategy is to use a hierarchy of control units, each responsible for a specific task. The second strategy is to use a single control unit that is responsible for all tasks."}, {"text": "Title: Microprogrammed Control P4 \n Text: We thus start with {5-bit} state number (in a register)\nand a {2^5 bit} memory, which we call\nour control ROM (read-only memory) to distinguish\nit from the big, slow, von Neumann memory. \n Question: \nQ. What is the purpose of the instruction register? \n Answer: \n\nA. The instruction register holds the current instruction as it executes."}, {"text": "Title: Microprogrammed Control P5 \n Text: Each cycle, the { microprogrammed control} unit applies the FSM state \nnumber to the control ROM (no IR bits, \njust the state number), gets back a set of control signals, and\nuses them to drive the datapath. \n Question: \nQ. What other components does the von Neumann model have, and what are their functions? \n Answer: \n\nA. Other components in the von Neumann model include the memory unit and the input/output unit. The memory unit stores instructions and data for the program, while the input/output unit handles communication with external devices."}, {"text": "Title: Microprogrammed Control P6 \n Text: \nTo write our microprogram, we need to calculate the control signals\nfor each microinstruction and put them in the control ROM, but we also\nneed to have a way to decide which microinstruction should execute \nnext.  We call the latter problem { sequencing} or microsequencing. \n Question: \nQ. What does the register file in the processing unit store? \n Answer: \n\nA. The register file stores the data that the processing unit uses to carry out its operations. This data includes the values of the operands used in the operations, as well as the values of the condition codes."}, {"text": "Title: Microprogrammed Control P7 \n Text: Notice that most of the time there's no choice: we have only { one}\nnext microinstruction.  One simple approach is then to add the address\n(the {5-bit} state ID) of the next microinstruction to the control ROM.\nInstead of 26 bits per FSM state, we now have 31 bits per FSM state. \n Question: \nQ. What is the control unit? \n Answer: \n\nA.\n\nThe control unit is the component of the CPU that controls the overall operation of the processor. It coordinates the activities of the datapath, the processing unit, the memory, and the I/O interfaces."}, {"text": "Title: Microprogrammed Control P8 \n Text: Sometimes we do need to have two possible next states.  When waiting\nfor memory (the von Neumann memory, not the control ROM) to\nfinish an access, for example, we want our FSM to stay\nin the same state, then move to the next state when the access completes.\nLet's add a second address to each microinstruction, and add\na branch control signal for the microprogram to decide whether we\nshould use the first address or the second for the next\nmicroinstruction.  This design, using a 2^5 bit memory\n(1,152 bits total), appears to the right. \n Question: \nQ. What is the basic problem that must be solved for control unit design? \n Answer: \n\nThe basic problem that must be solved for control unit design is to map instruction processing and the state of the FSM (including the PC and the IR) into appropriate sequences of control signals for the datapath."}, {"text": "Title: Microprogrammed Control P9 \n Text: \nThe microprogram branch control signal is a Boolean logic expression\nbased on the memory ready signal R and \nIR[11].  We can implement it with a state ID comparison and\na mux, as shown to the right.  For the branch instruction execution state, \nthe mux selects input 1, BEN.  For all other states, the mux selects \ninput 0, R.  When an FSM state has only a single next state, we set both \nIDs in the control ROM to the ID for that state, so the value of R \nhas no effect. \n Question: \nQ. What are interrupts and privilege? \n Answer: \n\nA.\n\nInterrupts are a type of signal that can be sent to a microprocessor to indicate that an event has occurred that needs attention. Privilege is a term used to describe the different levels of access that a user may have to a computer system."}, {"text": "Title: Microprogrammed Control P10 \n Text:  We can simplify the implementation of the microprogram branch control\n by setting both next-state addresses to the same value when a state\n does not branch.  Then the branch control becomes a don't care for that\n state. \n Question: \nQ. What is the LC-3? \n Answer: \n\nA. LC-3 is a computer architecture."}, {"text": "Title: Microprogrammed Control P11 \n Text: \nWhat's missing?  Decode!  We do have one FSM state in which we need to be\nable to branch to one of sixteen possible next states, one for each\nopcode.  Let's just add another mux and choose the state IDs for starting\nto process each opcode in an easy way, as shown to the right with\nextensions highlighted in blue.  The textbook assigns the first state \nfor processing each opcode the IDs IR[15:12] preceeded by two 0s (the\ntextbook's design requires {6-bit} state IDs).  We adopt the same\nstrategy.  For example, the first FSM state for processing an ADD is 00001, \nand the first state for a TRAP is 01111.  In each case, the opcode \nencoding specifies the last four bits. \n Question: \nQ. Some of the datapath control signals mentioned in the textbook are no longer necessary in the simplified design.\nQ. \nQ. Why are some of the datapath control signals mentioned in the textbook no longer necessary in the simplified design? \n Answer: \n\nThe simplified design requires fewer datapath control signals because it uses a smaller and more efficient datapath."}, {"text": "Title: Microprogrammed Control P12 \n Text: Now we can pick the remaining state IDs arbitrarily and fill in the \ncontrol ROM with control \nsignals that implement each state's RTL and the two possible next states.\nTransitions from the decode state are handled by the extra mux. \n Question: \nQ. What does the author mean by \"signals that remain\"? \n Answer: \n\nA. The author means the signals that are still used after the update."}, {"text": "Title: Microprogrammed Control P13 \n Text: The microprogrammed control unit implementation in Appendix C of \nPatt and Patel is similar to the one that we have developed here,\nbut is slightly more complex so that it can handle interrupts\nand privilege.    \n Question: \nQ. What is the purpose of the control signals?\nQ. \nQ. The control signals are used to specify whether or not registers in the datapath should load new values. \n Answer: "}, {"text": "Title: Redundancy and Coding P0 \n Text: This set of notes introduces the idea of using sparsely populated\nrepresentations to protect against accidental changes to bits.\nToday, such representations are used in almost every type of storage\nsystem, from bits on a chip to main memory to disk to archival tapes. \n Question: \nQ. How does using a sparsely populated representation protect against accidental changes to bits? \n Answer: \n\nA. When using a sparsely populated representation, there are fewer bits that can be accidentally changed. This is because there are fewer bits in total, and because the bits are spread out more evenly. This makes it less likely that a change in one bit will affect the others."}, {"text": "Title: Redundancy and Coding P1 \n Text: We begin our discussion with examples of representations in which some \nbit patterns have no meaning, then consider what happens when a bit \nchanges accidentally.  We next outline a general scheme \nthat allows a digital system to detect a single bit error. \n Question: \nQ. What is the general scheme that allows a digital system to detect a single bit error? \n Answer: \n\nA.\n\nThere is no single general scheme that allows a digital system to detect a single bit error. However, there are some common methods that can be used to detect errors, such as parity checks and checksums."}, {"text": "Title: Redundancy and Coding P2 \n Text: Building on the mechanism underlying this scheme,\nwe describe a distance metric that enables us to think more broadly about \nboth detecting and correcting such errors, and then show a general\napproach that allows correction of a single bit error. \n Question: \nQ. How does the scheme work? \n Answer: \n\nA. The scheme relies on the fact that a single bit error will produce a\nuniquely identifiable pattern of incorrect parity bits. By XORing the\nreceived parity bits with the expected parity bits, the position of\nthe error can be determined. The error can then be corrected by\nflipping the corresponding bit in the message."}, {"text": "Title: Redundancy and Coding P3 \n Text: We leave discussion of more sophisticated schemes to classes on\ncoding and information theory. \n Question: \nQ. What is a more sophisticated scheme for information theory? \n Answer: \n\nA. A more sophisticated scheme for information theory would be one that takes into account the relationships between different pieces of information, and uses this to encode and decode messages more efficiently."}, {"text": "Title: Sparse Representations P0 \n Text: Representations used by computers must avoid ambiguity: a single\nbit pattern in a representation cannot be used to represent more than\none value.  However, the converse need not be true.  A representation can\nhave several bit patterns representing the same value, and\nnot all bit patterns in a representation need be\nused to represent values. \n Question: \nQ. How does using a sparsely populated representation protect against accidental changes to bits? \n Answer: \n\nA. When using a sparsely populated representation, there are fewer bits that can be accidentally changed. This is because there are fewer bits in total, and because the bits are spread out more evenly. This makes it less likely that a change in one bit will affect the others."}, {"text": "Title: Sparse Representations P1 \n Text: Let's consider a few example of representations with unused patterns.\nHistorically, one common class of representations of this type was \nthose used to represent individual decimal digits.  We examine three\nexamples from this class. \n Question: \nQ. What is the general scheme that allows a digital system to detect a single bit error? \n Answer: \n\nA.\n\nThere is no single general scheme that allows a digital system to detect a single bit error. However, there are some common methods that can be used to detect errors, such as parity checks and checksums."}, {"text": "Title: Sparse Representations P2 \n Text: The first is Binary-coded Decimal (BCD), in which\ndecimal digits are encoded individually \nusing their representations in the unsigned\n(binary) representation.  Since we have 10 decimal digits, we need \n10 patterns, and four bits for each digit.  But four bits allow \n2^4=16 bit patterns.  In BCD, the patterns 1010,1011,,1111\nare unused.  It is important to note that BCD is not the same as the\nunsigned representation.  The decimal number 732, for example, requires 12 bits\nwhen encoded as BCD: 0111 0011 0010.  When written using a {12-bit}\nunsigned representation, 732 is written 001011011100.  Operations\non BCD values were implemented in early processors, including the\n8086, and are thus still available in the x86 instruction set architecture\ntoday! \n Question: \nQ. How does the scheme work? \n Answer: \n\nA. The scheme relies on the fact that a single bit error will produce a\nuniquely identifiable pattern of incorrect parity bits. By XORing the\nreceived parity bits with the expected parity bits, the position of\nthe error can be determined. The error can then be corrected by\nflipping the corresponding bit in the message."}, {"text": "Title: Sparse Representations P3 \n Text: \nThe second example is an {Excess-3} code, in which each decimal\ndigit d is represented by the pattern corresponding to the \n{4-bit} unsigned pattern for d+3.  For example, the digit 4\nis represented as 0111, and the digit 7 is represented as 1010.\nThe {Excess-3} code has some attractive aspects when using \nsimple hardware.  For example, we can use a {4-bit} binary \nadder to add two digits c and d represented in the {Excess-3} code, \nand the carry out signal produced by the adder is the same as the carry\nout for the decimal addition, since c+d is equivalent to \n(c+3)+(d+3). \n Question: \nQ. What is a more sophisticated scheme for information theory? \n Answer: \n\nA. A more sophisticated scheme for information theory would be one that takes into account the relationships between different pieces of information, and uses this to encode and decode messages more efficiently."}, {"text": "Title: Sparse Representations P4 \n Text: The third example of decimal digit representations is a {2-out-of-5}\ncode.  In such a code, five bits are used to encode each digit.   \n Question: \nQ. Why is it important that a representation used by computers avoid ambiguity? \n Answer: \n\nA. It is important that a representation used by computers avoid ambiguity because a single bit pattern in a representation cannot be used to represent more than one value."}, {"text": "Title: Sparse Representations P5 \n Text: Only patterns with exactly two 1s are used.  There are exactly ten \nsuch patterns, and an example representation is shown to the right (more\nthan one assignment of values to patterns has been used in real systems). \n Question: \nQ. What is the purpose of unused patterns in representations? \n Answer: \n\nA. The purpose of unused patterns in representations is to allow for representations of numbers that are not limited to a particular base. For example, in the decimal system, there are ten digits, 0 through 9, and each digit can be represented by a pattern of four bits. However, there are also numbers that can be represented in other bases, such as binary (base 2) or hexadecimal (base 16). In these cases, the unused patterns can be used to represent the other digits in the number."}, {"text": "Title: Sparse Representations P6 \n Text: \n{c|c}\n& a 2-out-of-5\ndigit& representation \n1& 00011\n2& 00101\n3& 00110\n4& 01001\n5& 01010\n6& 01100\n7& 10001\n8& 10010\n9& 10100\n0& 11000 \n Question: \nQ. Why are some bit patterns unused in BCD encoding? \n Answer: \n\nA. Some bit patterns are unused in BCD encoding because they are not needed to represent decimal digits."}, {"text": "Title: Error Detection P0 \n Text: Errors in digital systems\ncan occur for many reasons, ranging from cosmic ray strikes to\ndefects in chip fabrication to errors in the design of the digital system. \n Question: \nQ. How does using a sparsely populated representation protect against accidental changes to bits? \n Answer: \n\nA. When using a sparsely populated representation, there are fewer bits that can be accidentally changed. This is because there are fewer bits in total, and because the bits are spread out more evenly. This makes it less likely that a change in one bit will affect the others."}, {"text": "Title: Error Detection P1 \n Text: As a simple model, we assume that an error takes the form of changes to\nsome number of bits.  In other words, a bit that should have the value 0\ninstead has the value 1, or a bit that should have the value 1 instead\nhas the value 0.  Such an error is called a { bit error}. \n Question: \nQ. What is the general scheme that allows a digital system to detect a single bit error? \n Answer: \n\nA.\n\nThere is no single general scheme that allows a digital system to detect a single bit error. However, there are some common methods that can be used to detect errors, such as parity checks and checksums."}, {"text": "Title: Error Detection P2 \n Text: Digital systems can be designed with or without tolerance to errors.\nWhen an error occurs, no notification nor identification of the error\nis provided.  Rather, if error tolerance is needed,\nthe system must be designed to be able to \nrecognize and identify errors automatically. \n Question: \nQ. How does the scheme work? \n Answer: \n\nA. The scheme relies on the fact that a single bit error will produce a\nuniquely identifiable pattern of incorrect parity bits. By XORing the\nreceived parity bits with the expected parity bits, the position of\nthe error can be determined. The error can then be corrected by\nflipping the corresponding bit in the message."}, {"text": "Title: Error Detection P3 \n Text: Often, we assume that each of the bits may be in error independently\nof all of the others, each with some low probability.  With such\nan assumption, multiple bit errors\nare much less likely than single bit errors, and we can focus on \ndesigns that tolerate a single bit error. \n Question: \nQ. What is a more sophisticated scheme for information theory? \n Answer: \n\nA. A more sophisticated scheme for information theory would be one that takes into account the relationships between different pieces of information, and uses this to encode and decode messages more efficiently."}, {"text": "Title: Error Detection P4 \n Text: When a bit error occurs, however, we must assume that \nit can happen to any of the bits. \n Question: \nQ. Why is it important that a representation used by computers avoid ambiguity? \n Answer: \n\nA. It is important that a representation used by computers avoid ambiguity because a single bit pattern in a representation cannot be used to represent more than one value."}, {"text": "Title: Error Detection P5 \n Text: The use of many patterns to represent a smaller number of values, as\nis the case in a {2-out-of-5} code, enables a system\nto perform { error detection}. \n Question: \nQ. What is the purpose of unused patterns in representations? \n Answer: \n\nA. The purpose of unused patterns in representations is to allow for representations of numbers that are not limited to a particular base. For example, in the decimal system, there are ten digits, 0 through 9, and each digit can be represented by a pattern of four bits. However, there are also numbers that can be represented in other bases, such as binary (base 2) or hexadecimal (base 16). In these cases, the unused patterns can be used to represent the other digits in the number."}, {"text": "Title: Error Detection P6 \n Text: Let's consider what happens when a value represented using a \n{2-out-of-5} code is subjected to a single bit error.\nImagine that we have the digit 7.  In the table on the previous page,\nnotice that the digit 7 is represented with the pattern 10001. \n Question: \nQ. Why are some bit patterns unused in BCD encoding? \n Answer: \n\nA. Some bit patterns are unused in BCD encoding because they are not needed to represent decimal digits."}, {"text": "Title: Error Detection P7 \n Text: As we mentioned, we must assume that the bit error can occur in any \nof the five bits, thus we have five possible bit patterns after the error \noccurs.  If the error occurs in the first bit, we have the pattern 00001.\nIf the error occurs in the second bit, we have the pattern 11001.\nThe complete set of possible error patterns is 00001, 11001, 10101,\n10011, and 10000.   \n Question: \nQ. What is the binary pattern for the decimal digit 9 in the Excess-3 code? \n Answer:  \n\nA. The binary pattern for the decimal digit 9 in the Excess-3 code is 1111."}, {"text": "Title: Error Detection P8 \n Text: Notice that none of the possible error patterns has exactly two 1s, and \nthus none of them is a meaningful pattern in our {2-out-of-5} code.\nIn other words, whenever a digital system represents the digit 7 and \na single bit error occurs, the system will be able to detect that an \nerror has occurred. \n Question: \nQ. What is a 2-out-of-5 code?\nQ. \nQ. A 2-out-of-5 code is a code in which five bits are used to encode each digit. \n Answer: "}, {"text": "Title: Error Detection P9 \n Text: What if the system needs to represent a different digit?  \nRegardless of which digit is represented, the pattern with no errors \nhas exactly two 1s,\nby the definition of our representation.  If we then flip one of the \nfive bits by subjecting it to a bit error, the resulting error pattern\nhas either one 1 (if the bit error changes a 1 to a 0) or three 1s\n(if the bit error changes a 0 to a 1).  In other words, regardless of\nwhich digit is represented, and regardless of which bit has an error,\nthe resulting error pattern never has a meaning in \nthe {2-out-of-5} code.  So this representation enables a digital\nsystem to detect any single bit error! \n Question: \nQ. How many patterns with exactly two 1s are there? \n Answer: \n\nA. There are ten patterns with exactly two 1s."}, {"text": "Title: Parity P0 \n Text: The ability to detect any single bit error is certainly useful.\nHowever, so far we have only shown how to protect ourselves when we\nwant to represent decimal digits.  Do we need to develop a separate\nerror-tolerant representation for every type of information that\nwe might want to represent?  Or can we instead come up with a more\ngeneral approach? \n Question: \nQ. How does using a sparsely populated representation protect against accidental changes to bits? \n Answer: \n\nA. When using a sparsely populated representation, there are fewer bits that can be accidentally changed. This is because there are fewer bits in total, and because the bits are spread out more evenly. This makes it less likely that a change in one bit will affect the others."}, {"text": "Title: Parity P1 \n Text: The answer to the second question is yes: we can, in fact, systematically\ntransform any representation into a representation that allows detection of a\nsingle bit error.  The key to this transformation is the idea of\n{ parity}. \n Question: \nQ. What is the general scheme that allows a digital system to detect a single bit error? \n Answer: \n\nA.\n\nThere is no single general scheme that allows a digital system to detect a single bit error. However, there are some common methods that can be used to detect errors, such as parity checks and checksums."}, {"text": "Title: Parity P2 \n Text: \nConsider an arbitrary representation for some type of information.\nBy way of example, we use the {3-bit} unsigned representation.\nFor each pattern used in the representation, we can count the number\nof 1s.  The resulting count is either odd or even.  By adding an extra\nbit---called a { parity bit}---to the representation, and \nselecting the parity bit's value \nappropriately for each bit pattern, we can ensure that the count of 1s\nis odd (called { odd parity}) or even (called { even parity})\nfor all values represented.  The idea is \n Question: \nQ. How does the scheme work? \n Answer: \n\nA. The scheme relies on the fact that a single bit error will produce a\nuniquely identifiable pattern of incorrect parity bits. By XORing the\nreceived parity bits with the expected parity bits, the position of\nthe error can be determined. The error can then be corrected by\nflipping the corresponding bit in the message."}, {"text": "Title: Parity P3 \n Text: illustrated in the table\nto the right for the {3-bit} unsigned representation.  The parity\nbits are shown in bold. \n Question: \nQ. What is a more sophisticated scheme for information theory? \n Answer: \n\nA. A more sophisticated scheme for information theory would be one that takes into account the relationships between different pieces of information, and uses this to encode and decode messages more efficiently."}, {"text": "Title: Parity P4 \n Text: \n{c|c|c|c|c}\nvalue      &   3-bit & number& with odd& with even \nrepresented& unsigned& of 1s & parity  & parity  \n0& 000& 0& 000{ 1}& 000{ 0}\n1& 001& 1& 001{ 0}& 001{ 1}\n2& 010& 1& 010{ 0}& 010{ 1}\n3& 011& 2& 011{ 1}& 011{ 0}\n4& 100& 1& 100{ 0}& 100{ 1}\n5& 101& 2& 101{ 1}& 101{ 0}\n6& 110& 2& 110{ 1}& 110{ 0}\n7& 111& 3& 111{ 0}& 111{ 1} \n Question: \nQ. Why is it important that a representation used by computers avoid ambiguity? \n Answer: \n\nA. It is important that a representation used by computers avoid ambiguity because a single bit pattern in a representation cannot be used to represent more than one value."}, {"text": "Title: Parity P5 \n Text: Either approach to selecting the parity bits ensures that any single\nbit error can be detected.  For example, if we choose to use odd\nparity, a single\nbit error changes either a 0 into a 1 or a 1 into a 0.\nThe number of 1s in the resulting error pattern thus\ndiffers by exactly one from the original pattern, and the parity of\nthe error pattern is even.  But all valid patterns have odd parity,\nso any single bit error can be detected by simply counting the number\nof 1s. \n Question: \nQ. What is the purpose of unused patterns in representations? \n Answer: \n\nA. The purpose of unused patterns in representations is to allow for representations of numbers that are not limited to a particular base. For example, in the decimal system, there are ten digits, 0 through 9, and each digit can be represented by a pattern of four bits. However, there are also numbers that can be represented in other bases, such as binary (base 2) or hexadecimal (base 16). In these cases, the unused patterns can be used to represent the other digits in the number."}, {"text": "Title: Hamming Distance P0 \n Text: Next, let's think about how we might use representations---we might\nalso think of them as { codes}---to protect a system against \nmultiple bit errors.  As we have seen with parity, one strategy that\nwe can use to provide such error tolerance is the use of representations\nin which only some of the patterns actually represent values.\nLet's call such patterns { code words}.  In other words, the\ncode words in a representation are those patterns that correspond to \nreal values of information.  Other patterns in the representation have no \nmeaning. \n Question: \nQ. How does using a sparsely populated representation protect against accidental changes to bits? \n Answer: \n\nA. When using a sparsely populated representation, there are fewer bits that can be accidentally changed. This is because there are fewer bits in total, and because the bits are spread out more evenly. This makes it less likely that a change in one bit will affect the others."}, {"text": "Title: Hamming Distance P1 \n Text: As a tool to help us understand error tolerance, let's define\na measure of the distance between code words in a representation.\nGiven two code words X and Y, we can calculate the number N_ of \nbits that must change to transform X into Y.  Such a calculation\nmerely requires that we compare the patterns bit by bit and count\nthe number of places in which they differ.  Notice that this\nrelationship is symmetric: the same number of changes are required\nto transform Y into X, so N_=N_.   \n Question: \nQ. What is the general scheme that allows a digital system to detect a single bit error? \n Answer: \n\nA.\n\nThere is no single general scheme that allows a digital system to detect a single bit error. However, there are some common methods that can be used to detect errors, such as parity checks and checksums."}, {"text": "Title: Hamming Distance P2 \n Text: We refer to this number N_ as the { Hamming distance} between\ncode word X and code word Y.   \n Question: \nQ. How does the scheme work? \n Answer: \n\nA. The scheme relies on the fact that a single bit error will produce a\nuniquely identifiable pattern of incorrect parity bits. By XORing the\nreceived parity bits with the expected parity bits, the position of\nthe error can be determined. The error can then be corrected by\nflipping the corresponding bit in the message."}, {"text": "Title: Hamming Distance P3 \n Text: The metric is named after Richard Hamming, a computing pioneer and an \nalumnus of the UIUC Math department. \n Question: \nQ. What is a more sophisticated scheme for information theory? \n Answer: \n\nA. A more sophisticated scheme for information theory would be one that takes into account the relationships between different pieces of information, and uses this to encode and decode messages more efficiently."}, {"text": "Title: Hamming Distance P4 \n Text: The Hamming distance between two code words tells us how many bit errors\nare necessary in order for a digital system to mistake one code word\nfor the other.  Given a representation, we can calculate the minimum\nHamming distance between any pair of code words used by the \nrepresentation.  The result is called the { Hamming distance of the \nrepresentation}, and represents the minimum of bit errors that must\noccur before a system might fail to detect errors in a stored value. \n Question: \nQ. Why is it important that a representation used by computers avoid ambiguity? \n Answer: \n\nA. It is important that a representation used by computers avoid ambiguity because a single bit pattern in a representation cannot be used to represent more than one value."}, {"text": "Title: Hamming Distance P5 \n Text: The Hamming distance for nearly all of the representations that we \nintroduced in earlier sections is 1.  Since more than half of the\npatterns (and often all of the patterns!) correspond to meaningful\nvalues, some pairs of code words must differ in only one bit, and\nthese representations cannot tolerate any errors.  For example,\nthe decimal value 42 is stored as 101010 using a {6-bit} unsigned\nrepresentation, but any bit error in that pattern produces another \nvalid pattern corresponding to one of the following \ndecimal numbers: 10, 58, 34, 46, 40, 43.  Note that the Hamming distance\nbetween any two patterns is not necessarily 1.  Rather, the Hamming \ndistance of the unsigned representation, which corresponds to the \nminimum between any pair of valid patterns, is 1. \n Question: \nQ. What is the purpose of unused patterns in representations? \n Answer: \n\nA. The purpose of unused patterns in representations is to allow for representations of numbers that are not limited to a particular base. For example, in the decimal system, there are ten digits, 0 through 9, and each digit can be represented by a pattern of four bits. However, there are also numbers that can be represented in other bases, such as binary (base 2) or hexadecimal (base 16). In these cases, the unused patterns can be used to represent the other digits in the number."}, {"text": "Title: Hamming Distance P6 \n Text: In contrast, the Hamming distance of the {2-out-of-5} code that\nwe discussed earlier is 2.  Similarly, the Hamming distance of any\nrepresentation extended with a parity bit is at least 2. \n Question: \nQ. Why are some bit patterns unused in BCD encoding? \n Answer: \n\nA. Some bit patterns are unused in BCD encoding because they are not needed to represent decimal digits."}, {"text": "Title: Hamming Distance P7 \n Text: Now let's think about the problem slightly differently. \n Question: \nQ. What is the binary pattern for the decimal digit 9 in the Excess-3 code? \n Answer:  \n\nA. The binary pattern for the decimal digit 9 in the Excess-3 code is 1111."}, {"text": "Title: Hamming Distance P8 \n Text: how many bit errors can we detect in values using that representation? \n Question: \nQ. What is a 2-out-of-5 code?\nQ. \nQ. A 2-out-of-5 code is a code in which five bits are used to encode each digit. \n Answer: "}, {"text": "Title: Hamming Distance P9 \n Text: { A representation with Hamming distance d can detect up to d-1 bit errors.} \n Question: \nQ. How many patterns with exactly two 1s are there? \n Answer: \n\nA. There are ten patterns with exactly two 1s."}, {"text": "Title: Hamming Distance P10 \n Text: To understand this claim, start by selecting a code word from the\nrepresentation and changing up to d-1 of the bits.  No matter\nhow one chooses to change the bits, these changes cannot result in\nanother code word, since we know that any other code word has to \nrequire at least d changes from our original code word, by the\ndefinition of the representation's Hamming distance. \n Question: \nQ. How many digits does the 2-out-of-5 digit representation have?\nQ. \nQ. The 2-out-of-5 digit representation has 5 digits. \n Answer: "}, {"text": "Title: Hamming Distance P11 \n Text: A digital system using the representation can thus detect up to d-1\nerrors.  However, if d or more errors occur, the system might sometimes\nfail to detect any error in the stored value. \n Question: \nQ. What are the consequences of digital system errors? \n Answer: \n\nA. The consequences of digital system errors can be very serious, ranging from data corruption to system crashes."}, {"text": "Title: Error Correction P0 \n Text: Detection of errors is important, but may sometimes not be enough.\nWhat can a digital system do when it detects an error?  In some\ncases, the system may be able to find the original value elsewhere, or\nmay be able to re-compute the value from other values.  In other \ncases, the value is simply lost, and the digital system may need\nto reboot or even shut down until a human can attend to it. \n Question: \nQ. How does using a sparsely populated representation protect against accidental changes to bits? \n Answer: \n\nA. When using a sparsely populated representation, there are fewer bits that can be accidentally changed. This is because there are fewer bits in total, and because the bits are spread out more evenly. This makes it less likely that a change in one bit will affect the others."}, {"text": "Title: Error Correction P1 \n Text: Many real systems cannot afford such a luxury.  Life-critical systems\nsuch as medical equipment and airplanes should not turn themselves off\nand wait for a human's attention.  Space vehicles face a similar dilemma,\nsince no human may be able to reach them. \n Question: \nQ. What is the general scheme that allows a digital system to detect a single bit error? \n Answer: \n\nA.\n\nThere is no single general scheme that allows a digital system to detect a single bit error. However, there are some common methods that can be used to detect errors, such as parity checks and checksums."}, {"text": "Title: Error Correction P2 \n Text: Can we use a strategy similar to the one that we have developed for error\ndetection in order to try to perform { error correction}, recovering\nthe original value?  Yes, but the overhead---the\nextra bits that we need to provide such functionality---is higher. \n Question: \nQ. How does the scheme work? \n Answer: \n\nA. The scheme relies on the fact that a single bit error will produce a\nuniquely identifiable pattern of incorrect parity bits. By XORing the\nreceived parity bits with the expected parity bits, the position of\nthe error can be determined. The error can then be corrected by\nflipping the corresponding bit in the message."}, {"text": "Title: Error Correction P3 \n Text: Let's start by thinking about a code with Hamming distance 2, such\nas {4-bit} 2's complement with odd parity.  We know that such a \ncode can detect one bit error.  Can it correct such a bit error, too? \n Question: \nQ. What is a more sophisticated scheme for information theory? \n Answer: \n\nA. A more sophisticated scheme for information theory would be one that takes into account the relationships between different pieces of information, and uses this to encode and decode messages more efficiently."}, {"text": "Title: Error Correction P4 \n Text: Imagine that a system has stored the decimal value 6 using the \npattern 0110{ 1}, where the last bit is the odd parity bit.\nA bit error occurs, changing the stored pattern to 0111{ 1}, which is\nnot a valid pattern, since it has an even number of 1s.  But can the\nsystem know that the original value stored was 6?  No, it cannot.\nThe original value may also have been 7, in which case the original\npattern was 0111{ 0}, and the bit error occurred in the final\nbit.  The original value may also have been -1, 3, or 5.  The system\nhas no way of resolving this ambiguity. \n Question: \nQ. Why is it important that a representation used by computers avoid ambiguity? \n Answer: \n\nA. It is important that a representation used by computers avoid ambiguity because a single bit pattern in a representation cannot be used to represent more than one value."}, {"text": "Title: Error Correction P5 \n Text: The same problem arises if a digital system uses a code with\nHamming distance d to detect up to d-1 errors. \n Question: \nQ. What is the purpose of unused patterns in representations? \n Answer: \n\nA. The purpose of unused patterns in representations is to allow for representations of numbers that are not limited to a particular base. For example, in the decimal system, there are ten digits, 0 through 9, and each digit can be represented by a pattern of four bits. However, there are also numbers that can be represented in other bases, such as binary (base 2) or hexadecimal (base 16). In these cases, the unused patterns can be used to represent the other digits in the number."}, {"text": "Title: Error Correction P6 \n Text: \nError correction is possible, however, if we assume that fewer bit\nerrors occur (or if we instead use a representation with a larger Hamming\ndistance). \n Question: \nQ. Why are some bit patterns unused in BCD encoding? \n Answer: \n\nA. Some bit patterns are unused in BCD encoding because they are not needed to represent decimal digits."}, {"text": "Title: Error Correction P7 \n Text: As a simple example, let's create a representation for the numbers 0\nthrough 3 by making three copies of the {2-bit} unsigned \nrepresentation, as shown to the right.  The Hamming distance of the\nresulting code is 3, so any two bit errors can be detected.  However,\nthis code also enables us to correct a single bit error.  Intuitively, \nthink of the three copies as voting on the right answer. \n Question: \nQ. What is the binary pattern for the decimal digit 9 in the Excess-3 code? \n Answer:  \n\nA. The binary pattern for the decimal digit 9 in the Excess-3 code is 1111."}, {"text": "Title: Error Correction P8 \n Text: \n{c|c}\nvalue      & three-copy\nrepresented& code \n0& 000000\n1& 010101\n2& 101010\n3& 111111 \n Question: \nQ. What is a 2-out-of-5 code?\nQ. \nQ. A 2-out-of-5 code is a code in which five bits are used to encode each digit. \n Answer: "}, {"text": "Title: Error Correction P9 \n Text: Since a\nsingle bit error can only corrupt one copy, a majority vote always\ngives the right answer!\nTripling the number of bits needed in a representation is not a good\ngeneral strategy, however. \nNotice also that ``correcting'' a pattern with two bit errors can produce\nthe wrong result. \n Question: \nQ. How many patterns with exactly two 1s are there? \n Answer: \n\nA. There are ten patterns with exactly two 1s."}, {"text": "Title: Error Correction P10 \n Text: Let's think about the problem in terms\nof Hamming distance.  Assume that we use a code with Hamming distance d\nand imagine that up to k bit errors affect a stored value.\nThe resulting pattern then falls within a neighborhood of distance k\nfrom the original code word.  This neighborhood contains all bit \npatterns within Hamming distance k of the original pattern.\nWe can define such a neighborhood around each code word.  Now, \nsince d bit errors are needed to transform a code word into\nany other code word, these neighborhoods are disjoint so long\nas 2k{d-1}.  In other words, if the inequality holds,\nany bit pattern in the representation can be in at most one code word's \nneighborhood.  The digital system can then correct the errors by \nselecting the unique value identified by the associated neighborhood.\nNote that patterns encountered as a result of up to k bit errors\nalways fall within the original code word's neighborhood; the inequality\nensures that the neighborhood identified in this way is unique.\nWe can manipulate the inequality to express the number of errors k that\ncan be corrected in terms of the Hamming distance d of the code.\n{ A code with Hamming distance d allows up to {d-1}\nerrors to be corrected}, where  represents the\ninteger floor function on x, or rounding x down to the nearest \ninteger. \n Question: \nQ. How many digits does the 2-out-of-5 digit representation have?\nQ. \nQ. The 2-out-of-5 digit representation has 5 digits. \n Answer: "}, {"text": "Title: Hamming Codes P0 \n Text: Hamming also developed a general and efficient approach for \nextending an arbitrary representation to allow correction of\na single bit error.  The approach yields codes with Hamming distance 3. \n Question: \nQ. How does using a sparsely populated representation protect against accidental changes to bits? \n Answer: \n\nA. When using a sparsely populated representation, there are fewer bits that can be accidentally changed. This is because there are fewer bits in total, and because the bits are spread out more evenly. This makes it less likely that a change in one bit will affect the others."}, {"text": "Title: Hamming Codes P1 \n Text: To understand how a { Hamming code} works, think of the bits in the\nrepresentation as being numbered starting from 1.  For example, if\nwe have seven bits in the code, we might write a bit pattern X \nas x_7x_6x_5x_4x_3x_2x_1. \n Question: \nQ. What is the general scheme that allows a digital system to detect a single bit error? \n Answer: \n\nA.\n\nThere is no single general scheme that allows a digital system to detect a single bit error. However, there are some common methods that can be used to detect errors, such as parity checks and checksums."}, {"text": "Title: Hamming Codes P2 \n Text: The bits with indices that are powers of two are parity check bits.\nThese include x_1, x_2, x_4, x_8, and so forth.  The remaining\nbits can be used to hold data.  For example, we could use a {7-bit}\nHamming code and map the bits from a {4-bit} unsigned representation \ninto bits x_7, x_6, x_5, and x_3.  Notice that Hamming codes are\nnot so useful for small numbers of bits, but require only logarithmic\noverhead for large numbers of bits.  That is, in an {N-bit}\nHamming code, only _2(N+1) bits are used for parity\nchecks. \n Question: \nQ. How does the scheme work? \n Answer: \n\nA. The scheme relies on the fact that a single bit error will produce a\nuniquely identifiable pattern of incorrect parity bits. By XORing the\nreceived parity bits with the expected parity bits, the position of\nthe error can be determined. The error can then be corrected by\nflipping the corresponding bit in the message."}, {"text": "Title: Hamming Codes P3 \n Text: How are the parity checks defined?  Each parity bit is used to\nprovide even parity for those bits with indices for which the index,\nwhen written in binary, includes a 1 in the single position in which\nthe parity bit's index contains a 1.  The x_1 bit, for example,\nprovides even parity on all bits with odd indices.  The x_2 bit\nprovides even parity on x_2, x_3, x_6, x_7, x_, and so\nforth. \n Question: \nQ. What is a more sophisticated scheme for information theory? \n Answer: \n\nA. A more sophisticated scheme for information theory would be one that takes into account the relationships between different pieces of information, and uses this to encode and decode messages more efficiently."}, {"text": "Title: Hamming Codes P4 \n Text: In a {7-bit} Hamming code, for example, \nx_1 is chosen so that\nit has even parity together with x_3, x_5, and x_7. \n Question: \nQ. Why is it important that a representation used by computers avoid ambiguity? \n Answer: \n\nA. It is important that a representation used by computers avoid ambiguity because a single bit pattern in a representation cannot be used to represent more than one value."}, {"text": "Title: Hamming Codes P5 \n Text: Similarly, x_2 is chosen so that\nit has even parity together with x_3, x_6, and x_7. \n Question: \nQ. What is the purpose of unused patterns in representations? \n Answer: \n\nA. The purpose of unused patterns in representations is to allow for representations of numbers that are not limited to a particular base. For example, in the decimal system, there are ten digits, 0 through 9, and each digit can be represented by a pattern of four bits. However, there are also numbers that can be represented in other bases, such as binary (base 2) or hexadecimal (base 16). In these cases, the unused patterns can be used to represent the other digits in the number."}, {"text": "Title: Hamming Codes P6 \n Text: Finally, x_4 is chosen so that\nit has even parity together with x_5, x_6, and x_7. \n Question: \nQ. Why are some bit patterns unused in BCD encoding? \n Answer: \n\nA. Some bit patterns are unused in BCD encoding because they are not needed to represent decimal digits."}, {"text": "Title: Hamming Codes P7 \n Text: \nThe table to the right shows the result of embedding \na {4-bit} unsigned representation into a {7-bit}\nHamming code. \n Question: \nQ. What is the binary pattern for the decimal digit 9 in the Excess-3 code? \n Answer:  \n\nA. The binary pattern for the decimal digit 9 in the Excess-3 code is 1111."}, {"text": "Title: Hamming Codes P8 \n Text: A Hamming code provides a convenient way to identify\nwhich bit should be corrected when a single bit error occurs.  \nNotice that each bit is protected by a unique subset of the parity bits\ncorresponding to the binary form of the bit's index.  Bit x_6, for\nexample, is protected by bits x_4 and x_2, because the number 6\nis written 110 in binary.\nIf a bit is affected by an error, the parity bits\nthat register the error are those corresponding to 1s in the binary\nnumber of the index.  So if we calculate check bits as 1 to represent\nan error (odd parity) and 0 to represent no error (even parity),\nthen concatenate those bits into a binary number, we obtain the\nbinary value of the index of the single bit affected by an error (or \nthe number 0\nif no error has occurred). \n Question: \nQ. What is a 2-out-of-5 code?\nQ. \nQ. A 2-out-of-5 code is a code in which five bits are used to encode each digit. \n Answer: "}, {"text": "Title: Hamming Codes P9 \n Text: \n{c|c|c|c|c|c}\n           &  4-bit         &     &     &     &7-bit\nvalue      & unsigned       &     &     &     &Hamming\nrepresented&(x_7x_6x_5x_3)&x_4&x_2&x_1&code \n 0& 0000& 0& 0& 0& 0000000\n 1& 0001& 0& 1& 1& 0000111\n 2& 0010& 1& 0& 1& 0011001\n 3& 0011& 1& 1& 0& 0011110\n 4& 0100& 1& 1& 0& 0101010\n 5& 0101& 1& 0& 1& 0101101\n 6& 0110& 0& 1& 1& 0110011\n 7& 0111& 0& 0& 0& 0110100\n 8& 1000& 1& 1& 1& 1001011\n 9& 1001& 1& 0& 0& 1001100\n10& 1010& 0& 1& 0& 1010010\n11& 1011& 0& 0& 1& 1010101\n12& 1100& 0& 0& 1& 1100001\n13& 1101& 0& 1& 0& 1100110\n14& 1110& 1& 0& 0& 1111000\n15& 1111& 1& 1& 1& 1111111 \n Question: \nQ. How many patterns with exactly two 1s are there? \n Answer: \n\nA. There are ten patterns with exactly two 1s."}, {"text": "Title: Hamming Codes P10 \n Text: Let's do a couple of examples based on the pattern for the decimal\nnumber 9, 1001100.  First, assume that no error occurs.  We calculate\ncheck bit c_4 by checking whether x_4, x_5, x_6, and x_7 together\nhave even parity.  Since no error occurred, they do, so c_4=0.\nSimilarly, for c_2 we consider x_2, x_3, x_6, and x_7.\nThese also have even parity, so c_2=0.  Finally, for c_1, we \nconsider x_1, x_3, x_5, and x_7.  As with the others, these\ntogether have even parity, so c_1=0.  Writing c_4c_2c_1, we obtain 000,\nand conclude that no error has occurred. \n Question: \nQ. How many digits does the 2-out-of-5 digit representation have?\nQ. \nQ. The 2-out-of-5 digit representation has 5 digits. \n Answer: "}, {"text": "Title: Hamming Codes P11 \n Text: Next assume that bit 3 has an error, giving us the pattern 1001000.\nIn this case, we have again that c_4=0, but the bits corresponding to\nboth c_2 and c_1 have odd parity, so c2=1 and c_1=1.  Now when\nwe write the check bits c_4c_2c_1, we obtain 011,\nand we are able to recognize that bit 3 has been changed. \n Question: \nQ. What are the consequences of digital system errors? \n Answer: \n\nA. The consequences of digital system errors can be very serious, ranging from data corruption to system crashes."}, {"text": "Title: Hamming Codes P12 \n Text: A Hamming code can only correct one bit error, however.  If two bit\nerrors occur, correction will produce the wrong answer.  Let's imagine\nthat both bits 3 and 5 have been flipped in our example pattern for\nthe decimal number 9, producing the pattern 1011000.  Calculating the\ncheck bits as before and writing them as c_4c_2c_1, we obtain 110,\nwhich leads us to incorrectly conclude that bit 6 has been flipped.\nAs a result, we ``correct'' the pattern to 1111000, which represents\nthe decimal number 14. \n Question: \nQ. What does it mean when a bit that should have the value 0 instead has the value 1? \n Answer: \n\nA. A bit that should have the value 0 instead has the value 1 means that an error has occurred, and the bit has been flipped from 0 to 1."}, {"text": "Title: SEC-DED Codes P0 \n Text: We now consider one final extension of Hamming codes to enable a system\nto perform single error correction while also detecting any two bit errors.\nSuch codes are known as { Single Error Correction, Double Error \nDetection (SEC-DED)} codes.  Creating such a code from a Hamming code is\ntrivial: add a parity bit covering the entire Hamming code.  The extra\nparity bit increases the Hamming distance to 4.  A Hamming distance of 4\nstill allows only single bit error correction, but avoids the problem\nof Hamming distance 3 codes when two bit errors occur, since patterns\nat Hamming distance 2 from a valid code word cannot be within distance 1\nof another code word, and thus cannot be ``corrected'' to the wrong\nresult. \n Question: \nQ. How does using a sparsely populated representation protect against accidental changes to bits? \n Answer: \n\nA. When using a sparsely populated representation, there are fewer bits that can be accidentally changed. This is because there are fewer bits in total, and because the bits are spread out more evenly. This makes it less likely that a change in one bit will affect the others."}, {"text": "Title: SEC-DED Codes P1 \n Text: In fact, one can add a parity bit to any representation with an odd\nHamming distance to create a new representation with Hamming distance\none greater than the original representation.  To proof this convenient\nfact, begin with a representation with Hamming distance d, where d\nis odd.  If we choose two code words from the representation, and their \nHamming distance is already greater than d, their distance in the \nnew representation will also be greater than d.  Adding a parity\nbit cannot decrease the distance.  On the other hand, if the two code\nwords are exactly distance d apart, they must have opposite parity,\nsince they differ by an odd number of bits.  Thus the new parity bit\nwill be a 0 for one of the code words and a 1 for the other, increasing\nthe Hamming distance to d+1 in the new representation.  Since all\npairs of code words have Hamming distance of at least d+1, the\nnew representation also has Hamming distance d+1. \n Question: \nQ. What is the general scheme that allows a digital system to detect a single bit error? \n Answer: \n\nA.\n\nThere is no single general scheme that allows a digital system to detect a single bit error. However, there are some common methods that can be used to detect errors, such as parity checks and checksums."}, {"text": "Title: Instruction Set Architecture* P0 \n Text: This set of notes discusses \ntradeoffs and design elements of instruction set architectures (ISAs).\n{ The material is beyond the scope of our class, and is provided purely\nfor your interest.}  Those who find these topics interesting\nmay also want to read the ECE391 notes, which describe similar material\nwith a focus on the x86 ISA. \n Question: \nQ. What is the purpose of an ISA? \n Answer: \n\nThe purpose of an ISA is to define the interface between a computer's software and hardware."}, {"text": "Title: Instruction Set Architecture* P1 \n Text: As you know, the ISA\ndefines the interface between software and hardware, abstracting the\ncapabilities of a computer's datapath and standardizing the format of\ninstructions to utilize those capabilities.  Successful ISAs are\nrarely discarded, as success implies the existence of large amounts of\nsoftware built to use the ISA.  Rather, they are extended, and their\noriginal forms must be supported for decades (consider, for example,\nthe IBM 360 and the Intel x86).  Employing sound design principles\nis thus imperative in an ISA. \n Question: \nQ. What is the significance of an ISA? \n Answer: \n\nAn ISA is important because it defines the interface between software and hardware. This abstraction allows for different types of hardware to be compatible with the same software. In addition, the ISA standardizes the format of instructions so that they can be executed by different types of hardware."}, {"text": "Title: Formats and Fields* P0 \n Text: The LC-3 ISA \nemploys fixed-length instructions and a load-store architecture, two\naspects that help to reduce the design space to a manageable set of\nchoices.  In a general ISA design, many other options exist for\ninstruction formats. \n Question: \nQ. What is the purpose of an ISA? \n Answer: \n\nThe purpose of an ISA is to define the interface between a computer's software and hardware."}, {"text": "Title: Formats and Fields* P1 \n Text: Recall the idea of separating the bits of an instruction into\n(possibly non-contiguous) fields.  One of the fields must contain an\nopcode, which specifies the type of operation to be performed by the\ninstruction.  In the {LC-3} ISA, most opcodes specify both\nthe type of operation and the types of arguments to the operation.  \nMore generally, many addressing modes are possible for\neach operand, and we can think of the bits\nthat specify the addressing mode as a separate field, known as the\n{ mode} field.   \n Question: \nQ. What is the significance of an ISA? \n Answer: \n\nAn ISA is important because it defines the interface between software and hardware. This abstraction allows for different types of hardware to be compatible with the same software. In addition, the ISA standardizes the format of instructions so that they can be executed by different types of hardware."}, {"text": "Title: Formats and Fields* P2 \n Text: As a simple example, the {LC-3's} ADD and AND instructions \ncontain a {1-bit} mode field that specifies whether the second\noperand of the ADD/AND comes from a register or is an immediate value. \n Question: \nQ. What are some of the other options for instruction formats in a general ISA design? \n Answer: \n\nA. Some other options for instruction formats in a general ISA design include variable-length instructions and an immediate-addressing architecture."}, {"text": "Title: Formats and Fields* P3 \n Text: Several questions must be answered in order to define the possible\ninstruction formats for an ISA.  First, are instructions fixed-length\nor variable-length?  Second, how many addresses are needed for each\ninstruction, and how many of the addresses can be memory addresses?\nFinally, what forms of addresses are possible for each operand?\nFor example, can one use\nfull memory addresses or only limited offsets relative to a register? \n Question: \nQ. What is the LC-3 ISA? \n Answer: \n\nA. The LC-3 ISA is an instruction set architecture that specifies both the type of operation to be performed by the instruction and the types of arguments to the operation."}, {"text": "Title: Formats and Fields* P4 \n Text: The answer to the first question depends on many factors, but several\nclear advantages exist for both answers.  { Fixed-length\ninstructions} are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions.  As an example of this\nsimplification, note that the {LC-3} ISA does not support \naddressing for individual bytes, only for {16-bit} words. \n Question: \nQ. Why does the ADD and AND instruction in the LC-3 contain a 1-bit mode field?\nQ. \nQ. The 1-bit mode field in the ADD and AND instruction of the LC-3 specifies whether the second operand of the ADD/AND comes from a register or is an immediate value. \n Answer: "}, {"text": "Title: Formats and Fields* P5 \n Text: { Variable-length instructions} also have benefits, however.\nVariable-length encodings allow more efficient encodings, saving both\nmemory and disk space.  A register transfer operation, for example,\nclearly requires fewer bits than addition of values at two direct\nmemory addresses for storage at a third.  Fixed-length instructions\nmust be fixed at the length of the longest possible instruction,\nwhereas variable-length instructions can use lengths appropriate to\neach mode.  The same tradeoff has another form in the sense that\nfixed-length ISAs typically eliminate many addressing modes in order\nto limit the size of the instructions.  Variable-length instructions\nthus allow more flexibility; indeed, extensions to a variable-length\nISA can incorporate new addressing modes that require longer\ninstructions without affecting the original ISA. \n Question: \nQ. How many operands are needed for each instruction? \n Answer: \n\nA. Each instruction needs at least one operand."}, {"text": "Title: Formats and Fields* P6 \n Text: For example, the maximum length of x86 instructions has grown from six \nbytes in 1978 (the 8086 ISA) to fifteen bytes in today's version of the\nISA. \n Question: \nQ. How does fetching the opcode and mode fields in order to decide how many more\nQ. bits are necessary to complete the instruction impact performance? \n Answer: \n\nFetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction can impact performance by causing the processor to stall while it fetches the additional bits. This stall can cause the processor to fall behind in its execution, leading to reduced performance."}, {"text": "Title: Formats and Fields* P7 \n Text: \nMoving to the last of the three questions posed for instruction format\ndefinition, operand address specification,\nwe explore a range of answers developed over the last few\ndecades.  Answers are usually chosen based on the number of bits\nnecessary, and we use this metric to organize the possibilities.  The\nfigure below separates approaches into two dimensions: the vertical\ndimension divides addressing into registers and memory, and the\nhorizontal dimension into varieties within each type. \n Question: \nQ. What is the advantage of fixed-length instructions?\nQ. \nQ. Fixed-length instructions are easier to decode. \n Answer: "}, {"text": "Title: Formats and Fields* P8 \n Text: As a register file contains fewer registers than a memory does words,\nthe use of register operands rather than memory addresses reduces the\nnumber of bits required to specify an operand.  The {LC-3} ISA\nuses a restricted set of addressing modes to stay within the limit\nimposed by the use of {16-bit} instructions.  Both\nregister and memory addresses, however, admit a wide range of\nimplementations. \n Question: \nQ. What is the maximum length of an x86 instruction?\nQ. \nQ. The maximum length of an x86 instruction is fifteen bytes. \n Answer: "}, {"text": "Title: Formats and Fields* P9 \n Text: { Implicit operands} of either type require no additional bits for\nthe implicit address.  The {LC-3} procedure call instruction, JSR,\nfor example, stores the return address in R7.  No bits in the JSR \nencoding name the R7 register; R7 is used implicitly for every JSR\nexecuted. \n Question: \nQ. What are the benefits of using a register-based addressing scheme? \n Answer: \n\nA. Register-based addressing schemes have a number of benefits over memory-based schemes. \n\nFirst, because registers are much smaller than memory, a register-based scheme can use a smaller number of bits to encode an operand address. This can save space in the instruction format, and can also allow for a smaller and faster instruction fetch unit. \n\nSecond, registers can be accessed much faster than memory. This is because registers are located on the CPU die, while memory is located off-chip. Register-based addressing can therefore result in faster instruction execution. \n\nThird, register-based addressing can be more flexible than memory-based addressing. This is because the set of registers available for use can be changed dynamically, while the memory map is fixed. This can allow for more efficient use of resources, as well as more complex addressing modes."}, {"text": "Title: Formats and Fields* P10 \n Text: Similarly, the procedure call instructions in many ISAs push the \nreturn address onto a stack using an implicit register for the top \nof stack pointer. \n Question: \nQ. What is the advantage of using register operands over memory addresses? \n Answer: \n\nA. The advantage of using register operands over memory addresses is that register operands require fewer bits to specify an operand. This allows for a more compact instruction set and more efficient encoding."}, {"text": "Title: Formats and Fields* P11 \n Text: Memory addresses can also be implicitly equated to other memory\naddresses.  An increment instruction operating on a memory address, for\nexample, implicitly writes the result back to the same address.   \n Question: \nQ. Why does the author mention the LC-3 procedure call instruction, JSR?\nQ. \nQ. The author mentions the LC-3 procedure call instruction, JSR, because it is an example of an instruction that uses an implicit operand. \n Answer: "}, {"text": "Title: Formats and Fields* P12 \n Text: At the opposite extreme, an instruction may include a full address,\neither to any register in the register file or to any address in the \nmemory.  The term { general-purpose registers} indicates that \nregisters are used in any operation. \n Question: \nQ. What is an implicit register?\nQ. \nQ. An implicit register is a register that is not explicitly mentioned, but is implied by the context. In the context of a stack, an implicit register is typically the top of stack pointer, which is not explicitly mentioned, but is implied by the context of the stack. \n Answer: "}, {"text": "Title: Formats and Fields* P13 \n Text: { Special-purpose registers}, in contrast, split the register file\nand allow only certain registers to be used in each operation.  For\nexample, the Motorola 680x0 series, used in early Apple\nMacintosh computers, provides distinct sets of address and data\nregisters.  Loads and stores use the address registers; arithmetic,\nlogic, and shift operations use the data registers.  As a result, \neach instruction\nselects from a smaller set of registers and thus requires fewer bits\nin the instruction to name the register for use. \n Question: \nQ. What is an increment instruction? \n Answer: \n\nA. An increment instruction is an instruction that increases the value of a memory address by one."}, {"text": "Title: Formats and Fields* P14 \n Text: As full memory addresses require many more bits than full register\naddresses, a wider range of techniques has been employed to reduce the\nlength.  ``Zero page'' addresses, as defined in the 6510 (6502) ISA\nused by Commodore PET's,{My computer in junior high school.}\nC64's,{My computer in high school.} and VIC 20's, prefixed a\none-byte address with a zero byte, allowing shorter instructions when\nmemory addresses fell within the first 256 memory locations.  Assembly\nand machine language programmers made heavy use of these locations to\nproduce shorter programs.   \n Question: \nQ. What is the difference between a register and an address?\nQ. \nQ. A register is a small amount of storage within the CPU that is used to hold data and instructions. An address is a location in memory where data is stored. \n Answer: "}, {"text": "Title: Formats and Fields* P15 \n Text: Relative addressing is quite common in the {LC-3} ISA, in which many\naddresses are PC-relative.  Typical commerical ISAs also make use of\nrelative addressing.  The Alpha ISA, for example, has a PC-relative form of\nprocedure call with a {21-bit} offset (plus or minus a megabyte),\nand the x86 ISA has a ``short'' form of branch instructions that\nuses an {8-bit} offset. \n Question: \nQ. Why does using distinct sets of address and data registers require fewer bits in the instruction to name the register for use?\nQ. \nQ. Using distinct sets of address and data registers requires fewer bits in the instruction to name the register for use because each instruction only has to select from a smaller set of registers. \n Answer: "}, {"text": "Title: Formats and Fields* P16 \n Text: Segmented memory is a form of relative addressing that uses a register\n(usually implicit) to provide the high bits of an address and an\nexplicit memory address (or another register) to provide the low bits.\nIn the early x86 ISAs, for example, {20-bit} addresses are\nfound by adding a {16-bit} segment register extended with four\nzero bits to a {16-bit} offset. \n Question: \nQ. How do zero page addresses work?\nQ. \nQ. Zero page addresses are one byte long and are prefixed with a zero byte. This allows shorter instructions when memory addresses fall within the first 256 memory locations. \n Answer: "}, {"text": "Title: Addressing Architectures* P0 \n Text: One question remains for the definition of instruction formats: how\nmany addresses are needed for each instruction, and how many of the\naddresses can be memory addresses?  The first part of this question\nusually ranges from zero to three, and is rarely allowed to go\nbeyond three.  The answer to the second part determines the {\naddressing architecture} implemented by an ISA.  We now illustrate the\ntradeoffs between five distinct addressing architectures through the\nuse of a running example, the assignment X=AB+C/D. \n Question: \nQ. What is the purpose of an ISA? \n Answer: \n\nThe purpose of an ISA is to define the interface between a computer's software and hardware."}, {"text": "Title: Addressing Architectures* P1 \n Text: A binary operator requires two source operands and one destination\noperand, for a total of three addresses.  The ADD instruction, for\nexample, has a { {3-address}} format: \n Question: \nQ. What is the significance of an ISA? \n Answer: \n\nAn ISA is important because it defines the interface between software and hardware. This abstraction allows for different types of hardware to be compatible with the same software. In addition, the ISA standardizes the format of instructions so that they can be executed by different types of hardware."}, {"text": "Title: Addressing Architectures* P2 \n Text: {-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B,C>; M[A]  M[B] + M[C]\n>or>ADD>R1,R2,R3>; R1  R2 + R3\n{-6pt} \n Question: \nQ. What are some of the other options for instruction formats in a general ISA design? \n Answer: \n\nA. Some other options for instruction formats in a general ISA design include variable-length instructions and an immediate-addressing architecture."}, {"text": "Title: Addressing Architectures* P3 \n Text: If all three addresses can be memory addresses, the ISA is dubbed a\n{ memory-to-memory architecture}.  Such architectures may have\nsmall register sets or even lack a register file completely.  To\nimplement the assignment, we assume the availability of two memory\nlocations, T1 and T2, for temporary storage: \n Question: \nQ. What is the LC-3 ISA? \n Answer: \n\nA. The LC-3 ISA is an instruction set architecture that specifies both the type of operation to be performed by the instruction and the types of arguments to the operation."}, {"text": "Title: Addressing Architectures* P4 \n Text: {-6pt}\n=WWW=WW,WW,WW=\n>MUL>T1,A,B>; T1  M[A] * M[B]\n>DIV>T2,C,D>; T2  M[C] / M[D]\n>ADD>X,T1,T2>; X  M[T1] + M[T2]\n{-6pt} \n Question: \nQ. Why does the ADD and AND instruction in the LC-3 contain a 1-bit mode field?\nQ. \nQ. The 1-bit mode field in the ADD and AND instruction of the LC-3 specifies whether the second operand of the ADD/AND comes from a register or is an immediate value. \n Answer: "}, {"text": "Title: Addressing Architectures* P5 \n Text: The assignment requires only three instructions to implement, but each\ninstruction contains three full memory addresses, and is thus quite long. \n Question: \nQ. How many operands are needed for each instruction? \n Answer: \n\nA. Each instruction needs at least one operand."}, {"text": "Title: Addressing Architectures* P6 \n Text: At the other extreme is the { load-store architecture} used by the\n{LC-3} ISA.  In a load-store architecture, only\nloads and stores can use memory addresses; all other operations use\nonly registers.  As most instructions use only registers, this type of\naddressing architecture is also called a { register-to-register\narchitecture}.  The example assignment translates to the code shown below,\nwhich assumes that R1, R2, and R3 are free for use (the instructions\nare { NOT} {LC-3} instructions, but rather a generic assembly\nlanguage for a load-store architecture).  \n Question: \nQ. How does fetching the opcode and mode fields in order to decide how many more\nQ. bits are necessary to complete the instruction impact performance? \n Answer: \n\nFetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction can impact performance by causing the processor to stall while it fetches the additional bits. This stall can cause the processor to fall behind in its execution, leading to reduced performance."}, {"text": "Title: Addressing Architectures* P7 \n Text: {-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>LD>R2,B>; R2  M[B]\n>MUL>R1,R1,R2>; R1  R1 * R2\n>LD>R2,C>; R2  M[C]\n>LD>R3,D>; R3  M[D]\n>DIV>R2,R2,R3>; R2  R2 / R3\n>ADD>R1,R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt} \n Question: \nQ. What is the advantage of fixed-length instructions?\nQ. \nQ. Fixed-length instructions are easier to decode. \n Answer: "}, {"text": "Title: Addressing Architectures* P8 \n Text: Eight instructions are necessary, but no instruction requires more\nthan one full memory address, and several use only register addresses,\nallowing the use of shorter instructions.  The need to move data in\nand out of memory explicitly, however, also requires a reasonably\nlarge register set, as is available in the ARM, Sparc, Alpha, and IA-64\nISAs.   \n Question: \nQ. What is the maximum length of an x86 instruction?\nQ. \nQ. The maximum length of an x86 instruction is fifteen bytes. \n Answer: "}, {"text": "Title: Addressing Architectures* P9 \n Text: Architectures that use other combinations of memory and register\naddresses with {3-address} formats are not named.  Unary\noperators and transfer operators require only one source operand, thus\ncan use a {2-address} format (for example, NOT A,B).  Binary operations\ncan also use { {2-address}} format if one operand is implicit,\nas in the following instructions: \n Question: \nQ. What are the benefits of using a register-based addressing scheme? \n Answer: \n\nA. Register-based addressing schemes have a number of benefits over memory-based schemes. \n\nFirst, because registers are much smaller than memory, a register-based scheme can use a smaller number of bits to encode an operand address. This can save space in the instruction format, and can also allow for a smaller and faster instruction fetch unit. \n\nSecond, registers can be accessed much faster than memory. This is because registers are located on the CPU die, while memory is located off-chip. Register-based addressing can therefore result in faster instruction execution. \n\nThird, register-based addressing can be more flexible than memory-based addressing. This is because the set of registers available for use can be changed dynamically, while the memory map is fixed. This can allow for more efficient use of resources, as well as more complex addressing modes."}, {"text": "Title: Addressing Architectures* P10 \n Text: {-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B>; M[A]  M[A] + M[B]\n>or>ADD>R1,B>; R1  R1 + M[B]\n{-6pt} \n Question: \nQ. What is the advantage of using register operands over memory addresses? \n Answer: \n\nA. The advantage of using register operands over memory addresses is that register operands require fewer bits to specify an operand. This allows for a more compact instruction set and more efficient encoding."}, {"text": "Title: Addressing Architectures* P11 \n Text: The second instruction, in which one address is a register and the\nsecond is a memory address, defines a { register-memory\narchitecture}.  As shown by the code on the next page, \nsuch architectures strike a balance\nbetween the two architectures just discussed. \n Question: \nQ. Why does the author mention the LC-3 procedure call instruction, JSR?\nQ. \nQ. The author mentions the LC-3 procedure call instruction, JSR, because it is an example of an instruction that uses an implicit operand. \n Answer: "}, {"text": "Title: Addressing Architectures* P12 \n Text: \n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>MUL>R1,B>; R1  R1 * M[B]\n>LD>R2,C>; R2  M[C]\n>DIV>R2,D>; R2  R2 / M[D]\n>ADD>R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt} \n Question: \nQ. What is an implicit register?\nQ. \nQ. An implicit register is a register that is not explicitly mentioned, but is implied by the context. In the context of a stack, an implicit register is typically the top of stack pointer, which is not explicitly mentioned, but is implied by the context of the stack. \n Answer: "}, {"text": "Title: Addressing Architectures* P13 \n Text: The assignment now requires six instructions using at most one memory\naddress each; like memory-to-memory architectures, register-memory\narchitectures use relatively few registers.  Note that two-register\noperations are also allowed.  Intel's x86 ISA is a register-memory\narchitecture. \n Question: \nQ. What is an increment instruction? \n Answer: \n\nA. An increment instruction is an instruction that increases the value of a memory address by one."}, {"text": "Title: Addressing Architectures* P14 \n Text: Several ISAs of the past{The 6510/6502 as well, if memory\nserves, as the 8080, Z80, and Z8000, which used to drive parlor video\ngames.}  used a special-purpose register called the accumulator for\nALU operations, and are called { accumulator architectures}.  The\naccumulator in such architectures is implicitly both a source and the\ndestination for any such operation, allowing a { {1-address}}\nformat for instructions, as shown below. \n Question: \nQ. What is the difference between a register and an address?\nQ. \nQ. A register is a small amount of storage within the CPU that is used to hold data and instructions. An address is a location in memory where data is stored. \n Answer: "}, {"text": "Title: Addressing Architectures* P15 \n Text: {-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>B>; ACC  ACC + M[B]\n>or>ST>E>; M[E]  ACC\n{-6pt} \n Question: \nQ. Why does using distinct sets of address and data registers require fewer bits in the instruction to name the register for use?\nQ. \nQ. Using distinct sets of address and data registers requires fewer bits in the instruction to name the register for use because each instruction only has to select from a smaller set of registers. \n Answer: "}, {"text": "Title: Addressing Architectures* P16 \n Text: Accumulator architectures strike the same balance as register-memory\narchitectures, but use fewer registers.  Note that memory location X\nis used as a temporary storage location as well as the final storage\nlocation in the following code: \n Question: \nQ. How do zero page addresses work?\nQ. \nQ. Zero page addresses are one byte long and are prefixed with a zero byte. This allows shorter instructions when memory addresses fall within the first 256 memory locations. \n Answer: "}, {"text": "Title: Addressing Architectures* P17 \n Text: {-6pt}\n=WWW=WW,WW,WW=\n>LD>A>; ACC  M[A]\n>MUL>B>; ACC  ACC * M[B]\n>ST>X>; M[X]  ACC\n>LD>C>; ACC  M[C]\n>DIV>D>; ACC  ACC / M[D]\n>ADD>X>; ACC  ACC + M[X]\n>ST>X>; M[X]  ACC\n{-6pt} \n Question: \nQ. What is the advantage of using relative addressing?\nQ. \nQ. Relative addressing can be more efficient than absolute addressing because it requires less memory to store the address. It can also be more flexible because it can be used to access data that is stored in different locations. \n Answer: "}, {"text": "Title: Addressing Architectures* P18 \n Text: The last addressing architecture that we discuss is rarely used for\nmodern general-purpose processors, but may be familiar to\nyou because of its historical use in scientific and engineering calculators. \n Question: \nQ. What is the purpose of segmentation in memory?\nQ. \nQ. Segmentation is a form of memory management that allows different areas of memory to be used for different purposes. This can be helpful for optimizing performance or for security purposes. \n Answer: "}, {"text": "Title: Addressing Architectures* P19 \n Text: A { stack architecture}\nmaintains a stack of values and draws all ALU operands from this\nstack, allowing these instructions to use a { {0-address}}\nformat.  A special-purpose stack pointer (SP) register points to the\ntop of the stack in memory, and operations analogous to load ({\npush}) and store ({ pop}) are provided to move values on and off\nthe stack.  To implement our example assignment, we first transform it\ninto postfix notation (also called reverse Polish notation): \n Question: \nQ. How many addresses can an instruction have in this ISA? \n Answer: \n\nA. An instruction in this ISA can have up to three addresses."}, {"text": "Title: Addressing Architectures* P20 \n Text: {-6pt}\n=\n>A  B  *  C  D  /  +\n{-6pt} \n Question: \nQ. What does the term \"3-address\" refer to in this context?\nQ. \nQ. A 3-address instruction is an instruction that uses three addresses: two source operands and one destination operand. \n Answer: "}, {"text": "Title: Addressing Architectures* P21 \n Text: The resulting sequence of symbols transforms on a one-to-one basis\ninto instructions for a stack architecture: \n Question: \nQ. What does the text say about ADD?\nQ. \nQ. The text says that ADD is a command that can be used to add two numbers or registers together. \n Answer: "}, {"text": "Title: Addressing Architectures* P22 \n Text: {-6pt}\n=WWWW=WW=; M[SP+1]  M[SP] + M[SP + 1], SP  SP + 1WW=AB+C/DW=AB+C/DW=\n>PUSH>A>; SP  SP - 1, M[SP]  M[A]>A\n>PUSH>B>; SP  SP - 1, M[SP]  M[B]>B>A\n>MUL>>; M[SP+1]  M[SP+1] * M[SP], SP  SP + 1>AB\n>PUSH>C>; SP  SP - 1, M[SP]  M[C]>C>AB\n>PUSH>D>; SP  SP - 1, M[SP]  M[D]>D>C>AB\n>DIV>>; M[SP+1]  M[SP+1] / M[SP], SP  SP + 1>C/D>AB\n>ADD>>; M[SP+1]  M[SP+1] + M[SP], SP  SP + 1>AB+C/D\n>POP>X>; M[X]  M[SP], SP  SP + 1\n{-6pt} \n Question: \nQ. How many memory locations are required for this architecture?\nQ. \nQ. Two memory locations are required for this architecture. \n Answer: "}, {"text": "Title: Addressing Architectures* P23 \n Text: The values to the right are the values on the stack, starting with the\ntop value on the left and progressing downwards, { after the\ncompletion of each instruction}. \n Question: \nQ. What does the '>' symbol signify in this text?\nQ. \nQ. The '>' symbol is used to signify the end of a line of code. \n Answer: "}, {"text": "Title: Common Special-Purpose Registers* P0 \n Text: This section illustrates the uses of special-purpose registers through\na few examples.  \n Question: \nQ. What is the purpose of an ISA? \n Answer: \n\nThe purpose of an ISA is to define the interface between a computer's software and hardware."}, {"text": "Title: Common Special-Purpose Registers* P1 \n Text: The { stack pointer (SP)} points to the top of the stack in memory.\nMost older architectures support push and pop operations that\nimplicitly use the stack pointer.  Modern architectures assign a\ngeneral-purpose register to be the stack pointer and reference it\nexplicitly, although an assembler may support\ninstructions that appear to use implicit operands but in fact\ntranslate to machine instructions with explicit reference to the\nregister defined to be the SP. \n Question: \nQ. What is the significance of an ISA? \n Answer: \n\nAn ISA is important because it defines the interface between software and hardware. This abstraction allows for different types of hardware to be compatible with the same software. In addition, the ISA standardizes the format of instructions so that they can be executed by different types of hardware."}, {"text": "Title: Common Special-Purpose Registers* P2 \n Text: The { program counter (PC)} points to the next instruction to be\nexecuted.  Some modern architectures expose it as a general-purpose\nregister, although its distinct role in the implementation keeps such\na model from becoming as common as the use of a general-purpose\nregister for the SP. \n Question: \nQ. What are some of the other options for instruction formats in a general ISA design? \n Answer: \n\nA. Some other options for instruction formats in a general ISA design include variable-length instructions and an immediate-addressing architecture."}, {"text": "Title: Common Special-Purpose Registers* P3 \n Text: The { processor status register (PSR)}, also known as the {\nprocessor status word (PSW)}, contains all status bits as well as a\nmode bit indicating whether the processor is operating in user mode or\nprivileged (operating system) mode.  Having a register with this\ninformation allows more general access than is possible solely through\nthe use of control flow instructions. \n Question: \nQ. What is the LC-3 ISA? \n Answer: \n\nA. The LC-3 ISA is an instruction set architecture that specifies both the type of operation to be performed by the instruction and the types of arguments to the operation."}, {"text": "Title: Common Special-Purpose Registers* P4 \n Text: The { zero register} appears in modern architectures of the RISC\nvariety (defined in the next section of these notes).  The register is\nread-only and serves both as a useful constant and as a destination\nfor operations performed only for their side-effects (for example, setting\nstatus bits).  The availability of a zero register also allows certain\nopcodes to serve double duty.  A register-to-register add instruction\nbecomes a register move instruction when one source operand is zero.\nSimilarly, an immediate add instruction becomes an immediate load\ninstruction when one source operand is zero. \n Question: \nQ. Why does the ADD and AND instruction in the LC-3 contain a 1-bit mode field?\nQ. \nQ. The 1-bit mode field in the ADD and AND instruction of the LC-3 specifies whether the second operand of the ADD/AND comes from a register or is an immediate value. \n Answer: "}, {"text": "Title: Reduced Instruction Set Computers* P0 \n Text: By the mid-1980's, the VAX architecture dominated the workstation and\nminicomputer markets, which included most universities.  Digital\nEquipment Corporation, the creator of the VAX, was second only to IBM\nin terms of computer sales.  VAXen, as the machines were called, used\nmicroprogrammed control units and supported numerous addressing modes\nas well as complex instructions ranging from ``square root'' to\n``find roots of polynomial equation.'' \n Question: \nQ. What is the purpose of an ISA? \n Answer: \n\nThe purpose of an ISA is to define the interface between a computer's software and hardware."}, {"text": "Title: Reduced Instruction Set Computers* P1 \n Text: The impact of increasingly dense integrated circuit technology had\nbegun to have its effect, however, and in view of increasing processor\nclock speeds, more and more programmers were using high-level\nlanguages rather than writing assembly code.  Although assembly\nprogrammers often made use of the complex VAX instructions, compilers\nwere usually unable to recognize the corresponding high-level language\nconstructs and thus were unable to make use of the instructions. \n Question: \nQ. What is the significance of an ISA? \n Answer: \n\nAn ISA is important because it defines the interface between software and hardware. This abstraction allows for different types of hardware to be compatible with the same software. In addition, the ISA standardizes the format of instructions so that they can be executed by different types of hardware."}, {"text": "Title: Reduced Instruction Set Computers* P2 \n Text: Increasing density also led to rapid growth in memory sizes, to the\npoint that researchers began to question the need for variable-length\ninstructions.  Recall that variable-length instructions allow shorter\ncodes by providing more efficient instruction encodings.  With the\ntrend toward larger memories, code length was less important.  The\nperformance advantage of fixed-length instructions, which simplifies\nthe datapath and enables pipelining, on the other hand, was \nattractive. \n Question: \nQ. What are some of the other options for instruction formats in a general ISA design? \n Answer: \n\nA. Some other options for instruction formats in a general ISA design include variable-length instructions and an immediate-addressing architecture."}, {"text": "Title: Reduced Instruction Set Computers* P3 \n Text: Researchers leveraged these ideas, which had been floating around the\nresearch community (and had appeared in some commercial architectures)\nto create { reduced instruction set computers}, or { RISC}\nmachines.  The competing VAXen were labeled { CISC} machines, which\nstands for { complex instruction set computers}. \n Question: \nQ. What is the LC-3 ISA? \n Answer: \n\nA. The LC-3 ISA is an instruction set architecture that specifies both the type of operation to be performed by the instruction and the types of arguments to the operation."}, {"text": "Title: Reduced Instruction Set Computers* P4 \n Text: RISC machines employ fixed-length instructions and a load-store\narchitecture, allowing only a few addressing modes and small offsets.\nThis combination of design decisions enables deep pipelines and\nmultiple instruction issues in a single cycle (termed superscalar\nimplementations), and for years, RISC machines were viewed by many\nresearchers as the proper design for future ISAs.  However, companies\nsuch as Intel soon learned to pipeline microoperations after decoding\ninstructions, and CISC architectures now offer competitive if not\nsuperior performance in comparison with RISC machines.  The VAXen are\ndead, of course,{Unless you talk with customer support\nemployees, for whom no machine ever dies.}  having been replaced by\nthe Alpha, which in turn fell to x86, which is now struggling with ARM\nto enter the mobile market.   \n Question: \nQ. Why does the ADD and AND instruction in the LC-3 contain a 1-bit mode field?\nQ. \nQ. The 1-bit mode field in the ADD and AND instruction of the LC-3 specifies whether the second operand of the ADD/AND comes from a register or is an immediate value. \n Answer: "}, {"text": "Title: Procedure and System Calls* P0 \n Text: A { procedure} is a sequence of instructions that executes a\nparticular task.  Procedures are used as building blocks for multiple,\nlarger tasks.  The concept of a procedure is fundamental to\nprogramming, and appears in some form in every high-level language as\nwell as in most ISAs. \n Question: \nQ. What is the purpose of an ISA? \n Answer: \n\nThe purpose of an ISA is to define the interface between a computer's software and hardware."}, {"text": "Title: Procedure and System Calls* P1 \n Text: For our purposes, the terms procedure, subroutine,\nfunction, and method are synonymous, although they usually have\nslightly different meanings from the linguistic point of view.\nProcedure calls are supported through { call} and { return}\ncontrol flow instructions.  The first instruction in the code below,\nfor example, transfers control to the procedure ``DoSomeWork,'' which\npresumably does some work, then returns control to the instruction\nfollowing the call. \n Question: \nQ. What is the significance of an ISA? \n Answer: \n\nAn ISA is important because it defines the interface between software and hardware. This abstraction allows for different types of hardware to be compatible with the same software. In addition, the ISA standardizes the format of instructions so that they can be executed by different types of hardware."}, {"text": "Title: Procedure and System Calls* P2 \n Text: =DoSomeWork:WW=WWWW=DoSomeWorkWW= \n>loop:>CALL>DoSomeWork\n>>CMP>R6,#1>; compare return value in R6 to 1\n>>BEQ>loop>; keep doing work until R6 is not 1 \n Question: \nQ. What are some of the other options for instruction formats in a general ISA design? \n Answer: \n\nA. Some other options for instruction formats in a general ISA design include variable-length instructions and an immediate-addressing architecture."}, {"text": "Title: Procedure and System Calls* P3 \n Text: >DoSomeWork:>>> ; set R6 to 0 when all work is done, 1 otherwise \n>>RETN \n Question: \nQ. What is the LC-3 ISA? \n Answer: \n\nA. The LC-3 ISA is an instruction set architecture that specifies both the type of operation to be performed by the instruction and the types of arguments to the operation."}, {"text": "Title: Procedure and System Calls* P4 \n Text: The procedure also places a return value in R6, which the instruction\nfollowing the call compares with immediate value 1.  Until the two are\nnot equal (when all work is done), the branch returns control to the\ncall and executes the procedure again. \n Question: \nQ. Why does the ADD and AND instruction in the LC-3 contain a 1-bit mode field?\nQ. \nQ. The 1-bit mode field in the ADD and AND instruction of the LC-3 specifies whether the second operand of the ADD/AND comes from a register or is an immediate value. \n Answer: "}, {"text": "Title: Procedure and System Calls* P5 \n Text: As you may recall, the call and return use the stack pointer to keep\ntrack of nested calls.  Sample RTL for these operations appears below. \n Question: \nQ. How many operands are needed for each instruction? \n Answer: \n\nA. Each instruction needs at least one operand."}, {"text": "Title: Procedure and System Calls* P6 \n Text: \n{eqnarray*}\n{call RTL}&&SP  SP - 1\n&& M[SP]  PC\n&& PC  {procedure start}\n{eqnarray*} \n Question: \nQ. How does fetching the opcode and mode fields in order to decide how many more\nQ. bits are necessary to complete the instruction impact performance? \n Answer: \n\nFetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction can impact performance by causing the processor to stall while it fetches the additional bits. This stall can cause the processor to fall behind in its execution, leading to reduced performance."}, {"text": "Title: Procedure and System Calls* P7 \n Text: \n{eqnarray*}\n{return RTL}&&PC  M[SP]\n&&SP  SP + 1\n{eqnarray*} \n Question: \nQ. What is the advantage of fixed-length instructions?\nQ. \nQ. Fixed-length instructions are easier to decode. \n Answer: "}, {"text": "Title: Procedure and System Calls* P8 \n Text: \nWhile an ISA provides the call and return instructions necessary to\nsupport procedures, it does not specify how information is passed to\nor returned from a procedure.  A standard for such decisions is\nusually developed and included in descriptions of the architecture,\nhowever.  This { calling convention} specifies how information is\npassed between a caller and a callee.  In particular, it specifies the\nfollowing: where arguments must be placed, either in registers or in\nspecific stack memory locations; which registers can be used or\nchanged by the procedure; and where any return value must be placed. \n Question: \nQ. What is the maximum length of an x86 instruction?\nQ. \nQ. The maximum length of an x86 instruction is fifteen bytes. \n Answer: "}, {"text": "Title: Procedure and System Calls* P9 \n Text: The term ``calling convention'' is also used in the programming\nlanguage community to describe the convention for deciding what\ninformation is passed for a given call operation.  For example, are\nvariables passed by value, by pointers to values, or in some other\nway?  However, once the things to be sent are decided, the\narchitectural calling convention that we discuss here is used\nto determine where to put the data in order for the callee to be able\nto find it. \n Question: \nQ. What are the benefits of using a register-based addressing scheme? \n Answer: \n\nA. Register-based addressing schemes have a number of benefits over memory-based schemes. \n\nFirst, because registers are much smaller than memory, a register-based scheme can use a smaller number of bits to encode an operand address. This can save space in the instruction format, and can also allow for a smaller and faster instruction fetch unit. \n\nSecond, registers can be accessed much faster than memory. This is because registers are located on the CPU die, while memory is located off-chip. Register-based addressing can therefore result in faster instruction execution. \n\nThird, register-based addressing can be more flexible than memory-based addressing. This is because the set of registers available for use can be changed dynamically, while the memory map is fixed. This can allow for more efficient use of resources, as well as more complex addressing modes."}, {"text": "Title: Procedure and System Calls* P10 \n Text: Calling conventions for architectures with large register sets\ntypically pass arguments in registers, and nearly all conventions\nplace the return value in a register.  A calling convention also\ndivides the register set into { caller-saved} and \n{ callee-saved} registers.  Caller-saved registers can be modified \narbitrarily\nby the called procedure, whereas any value in a callee-saved register\nmust be preserved.  Similarly, before calling a procedure, a caller\nmust preserve the values of any caller saved registers that are needed\nafter the call.  Registers of both types usually saved on the stack by\nthe appropriate code (caller or callee). \n Question: \nQ. What is the advantage of using register operands over memory addresses? \n Answer: \n\nA. The advantage of using register operands over memory addresses is that register operands require fewer bits to specify an operand. This allows for a more compact instruction set and more efficient encoding."}, {"text": "Title: Procedure and System Calls* P11 \n Text: A typical stack structure appears in the figure to the right.  In\npreparation for a call, a caller first stores any caller-saved\nregisters on the stack.  Arguments to the procedure to be called are\npushed next.  The procedure is called next, implicitly pushing the\nreturn address (the address of the instruction following the call\ninstruction).  Finally, the called procedure may allocate space on the\nstack for storage of callee-saved registers as well as local\nvariables. \n Question: \nQ. Why does the author mention the LC-3 procedure call instruction, JSR?\nQ. \nQ. The author mentions the LC-3 procedure call instruction, JSR, because it is an example of an instruction that uses an implicit operand. \n Answer: "}, {"text": "Title: Procedure and System Calls* P12 \n Text: As an example, the following calling convention can be applied to \nan {8-register} load-store architecture similar to the {LC-3}\nISA: the first three arguments must be placed in R0 through R2 (in order), \nwith any remaining arguments on the stack; the return value must be placed \nin R6; R0 through R2 are caller-saved, as\nis R6, while R3 through R5 are callee-saved; R7 is used as the stack\npointer.  The code fragments below use this calling convention to\nimplement a procedure and a call of that procedure. \n Question: \nQ. What is an implicit register?\nQ. \nQ. An implicit register is a register that is not explicitly mentioned, but is implied by the context. In the context of a stack, an implicit register is typically the top of stack pointer, which is not explicitly mentioned, but is implied by the context of the stack. \n Answer: "}, {"text": "Title: Procedure and System Calls* P13 \n Text: int =add3 (int n1, int n2, int n3) {\n>return (n1 + n2 + n3);\n} \n Question: \nQ. What is an increment instruction? \n Answer: \n\nA. An increment instruction is an instruction that increases the value of a memory address by one."}, {"text": "Title: Procedure and System Calls* P14 \n Text: printf (``d'', add3 (10, 20, 30)); \n Question: \nQ. What is the difference between a register and an address?\nQ. \nQ. A register is a small amount of storage within the CPU that is used to hold data and instructions. An address is a location in memory where data is stored. \n Answer: "}, {"text": "Title: Procedure and System Calls* P15 \n Text: by convention:\n= n1 is in R0\n>n2 is in R1\n>n3 is in R2\n>return value is in R6 \n Question: \nQ. Why does using distinct sets of address and data registers require fewer bits in the instruction to name the register for use?\nQ. \nQ. Using distinct sets of address and data registers requires fewer bits in the instruction to name the register for use because each instruction only has to select from a smaller set of registers. \n Answer: "}, {"text": "Title: Procedure and System Calls* P16 \n Text: add3:  = WWWW= WWWWW= \nadd3:>ADD>R0,R0,R1\n>ADD>R6,R0,R2\n>RETN\n>\n>PUSH>R1>; save the value in R1\n>LDI>R0,#10>; marshal arguments\n>LDI>R1,#20\n>LDI>R2,#30\n>CALL>add3\n>MOV>R1,R6>; return value becomes 2nd argument\n>LDI>R0,``d''>; load a pointer to the string\n>CALL>printf\n>POP>R1>; restore R1 \n Question: \nQ. How do zero page addresses work?\nQ. \nQ. Zero page addresses are one byte long and are prefixed with a zero byte. This allows shorter instructions when memory addresses fall within the first 256 memory locations. \n Answer: "}, {"text": "Title: Procedure and System Calls* P17 \n Text: The add3 procedure takes three integers as arguments, adds them\ntogether, and returns the sum.  The procedure is called with the\nconstants 10, 20, and 30, and the result is printed.  By the calling\nconvention, when the call is made, R0 must contain the value 10, R1\nthe value 20, and R2 the value 30.  We assume that the caller wants to\npreserve the value of R1, but does not care about R3 or R5.  In the\nassembly language version on the right, R1 is first saved to the\nstack, then the arguments are marshaled into position, and finally the\ncall is made.  The procedure itself needs no local storage and does\nnot change any callee-saved registers, thus must simply add the\nnumbers together and place the result in R6.  After add3 returns, its\nreturn value is moved from R6 to R1 in preparation for the call to\nprintf.  After loading a pointer to the format string into R0, the\nsecond call is made, and R1 is restored, completing the translation. \n Question: \nQ. What is the advantage of using relative addressing?\nQ. \nQ. Relative addressing can be more efficient than absolute addressing because it requires less memory to store the address. It can also be more flexible because it can be used to access data that is stored in different locations. \n Answer: "}, {"text": "Title: Procedure and System Calls* P18 \n Text: { System calls} are almost identical to procedure calls.  As with\nprocedure calls, a calling convention is used: before invoking a\nsystem call, arguments are marshaled into the appropriate registers or\nlocations in the stack; after a system call returns, any result\nappears in a pre-specified register.  The calling convention used for\nsystem calls need not be the same as that used for procedure calls.\nRather than a call instruction, system calls are usually initiated\nwith a { trap} instruction, and system calls are also known as\ntraps.  With many architectures, a system call places the processor in\nprivileged or kernel mode, and the instructions that implement the\ncall are considered to be part of the operating system.  The term\nsystem call arises from this fact. \n Question: \nQ. What is the purpose of segmentation in memory?\nQ. \nQ. Segmentation is a form of memory management that allows different areas of memory to be used for different purposes. This can be helpful for optimizing performance or for security purposes. \n Answer: "}, {"text": "Title: Interrupts and Exceptions* P0 \n Text: Unexpected processor interruptions arise both from interactions\nbetween a processor and external devices and from errors or unexpected\nbehavior in the program being executed.  The term { interrupt} is\nreserved for asynchronous interruptions generated by other devices,\nincluding disk drives, printers, network cards, video cards,\nkeyboards, mice, and any number of other possibilities.  {\nExceptions} occur when a processor encounters an unexpected opcode or\noperand.  An undefined instruction, for example, gives rise to an\nexception, as does an attempt to divide by zero.  Exceptions usually\ncause the current program to terminate, although many operating\nsystems will allow the program to catch the exception and to handle it\nmore intelligently.  The table below summarizes the characteristics of\nthe two types and compares them to system calls. \n Question: \nQ. What is the purpose of an ISA? \n Answer: \n\nThe purpose of an ISA is to define the interface between a computer's software and hardware."}, {"text": "Title: Interrupts and Exceptions* P1 \n Text: {\n{|l|l|l|c|c|}\n{|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected \ninterrupt& external device& packet arrived at network card& yes& yes\nexception& invalid opcode or operand& divide by zero& no& yes\ntrap/system call& deliberate, via trap instruction& print character to console& no& no  \n Question: \nQ. What is the significance of an ISA? \n Answer: \n\nAn ISA is important because it defines the interface between software and hardware. This abstraction allows for different types of hardware to be compatible with the same software. In addition, the ISA standardizes the format of instructions so that they can be executed by different types of hardware."}, {"text": "Title: Interrupts and Exceptions* P2 \n Text: Interrupts occur asynchronously with respect to the program.  Most\ndesigns only recognize interrupts between instructions.  In other words, \nthe presence of interrupts is checked only after completing an instruction\nrather than in every cycle.  In pipelined designs, however,\ninstructions execute simultaneously, and the decision as to which\ninstructions occur ``before'' an interrupt and which occur ``after''\nmust be made by the processor.  Exceptions are not asynchronous in the\nsense that they occur for a particular instruction, thus no decision\nneed be made as to instruction ordering.  After determining which\ninstructions were before an interrupt, a pipelined processor discards\nthe state of any partially executed instructions that occur ``after''\nthe interrupt and completes all instructions that occur ``before.''\nThe terminated instructions are simply restarted after the interrupt\ncompletes.  Handling the decision, the termination, and the\ncompletion, however, significantly increases the design complexity of\nthe system. \n Question: \nQ. What are some of the other options for instruction formats in a general ISA design? \n Answer: \n\nA. Some other options for instruction formats in a general ISA design include variable-length instructions and an immediate-addressing architecture."}, {"text": "Title: Interrupts and Exceptions* P3 \n Text: The code associated with an interrupt, an exception, or a system call\nis a form of procedure called a { handler}, and is found by looking\nup the interrupt number, exception number, or trap number in a table\nof functions called a { vector table}.  Vector tables\nfor each type (interrupts, exceptions, and system calls) may be separate,\nor may be combined into a single table.\nInterrupts and exceptions share a need to save all registers and\nstatus bits before execution of the corresponding handler code (and to\nrestore those values afterward).  Generally, the\n{values---including} the status word {register---are} placed\non the stack.  With system calls, saving and restoring any necessary\nstate is part of the calling convention.  A special return from\ninterrupt instruction is used to return control from the interrupt\nhandler to the interrupted code; a similar instruction forces the\nprocessor back into user mode when returning from a system call. \n Question: \nQ. What is the LC-3 ISA? \n Answer: \n\nA. The LC-3 ISA is an instruction set architecture that specifies both the type of operation to be performed by the instruction and the types of arguments to the operation."}, {"text": "Title: Interrupts and Exceptions* P4 \n Text: Interrupts are also interesting in the sense that typical computers\noften have many interrupt-generating devices but only a few\ninterrupts.  Interrupts are prioritized by number, and only an\ninterrupt with higher priority can interrupt another interrupt.\nInterrupts with equal or lower priority are blocked while an interrupt\nexecutes.  Some interrupts can also be blocked in some architectures\nby setting bits in a special-purpose register called an interrupt\nmask.  While an interrupt number is masked, interrupts of that type\nare blocked, and can not occur. \n Question: \nQ. Why does the ADD and AND instruction in the LC-3 contain a 1-bit mode field?\nQ. \nQ. The 1-bit mode field in the ADD and AND instruction of the LC-3 specifies whether the second operand of the ADD/AND comes from a register or is an immediate value. \n Answer: "}, {"text": "Title: Interrupts and Exceptions* P5 \n Text: As several devices may generate interrupts with the same interrupt\nnumber, interrupt handlers can be { chained} together.  Each\nhandler corresponds to a particular device.  When an interrupt occurs,\ncontrol is passed to the handler for the first device, which accesses\ndevice registers to determine whether or not that device generated an\ninterrupt.  If it did, the appropriate service is provided.  If not,\nor after the service is complete, control is passed to the next\nhandler in the chain, which handles interrupts from the second device,\nand so forth until the last handler in the chain completes.  At this\npoint, registers and processor state are restored and control is\nreturned to the point at which the interrupt occurred. \n Question: \nQ. How many operands are needed for each instruction? \n Answer: \n\nA. Each instruction needs at least one operand."}, {"text": "Title: Control Flow Conditions* P0 \n Text: Control flow instructions may change the PC, loading it with an\naddress specified by the instruction.  Although any addressing mode\ncan be supported, the most common specify an address directly in the\ninstruction, use a register as an address, or use an address relative\nto a register.   \n Question: \nQ. What is the purpose of an ISA? \n Answer: \n\nThe purpose of an ISA is to define the interface between a computer's software and hardware."}, {"text": "Title: Control Flow Conditions* P1 \n Text: Unconditional control flow instructions typically provided by an ISA\ninclude procedure calls and returns, traps, and jumps.  Conditional\ncontrol flow instructions are branches, and are logically based on\nstatus bits set by two types of instructions: { comparisons} and\n{ bit tests}.  Comparisons subtract one value from another to set\nthe status bits, whereas bit tests use an AND operation to\ncheck whether certain bits are set or not in a value. \n Question: \nQ. What is the significance of an ISA? \n Answer: \n\nAn ISA is important because it defines the interface between software and hardware. This abstraction allows for different types of hardware to be compatible with the same software. In addition, the ISA standardizes the format of instructions so that they can be executed by different types of hardware."}, {"text": "Title: Control Flow Conditions* P2 \n Text: Many ISAs implement\nstatus bits as special-purpose registers and implicitly set them when\nexecuting\ncertain instructions.  A branch based on R2 being less or equal to R3\ncan then be written as shown below.  The status bits are set by\nsubtracting R3 from R2 with the ALU. \n Question: \nQ. What are some of the other options for instruction formats in a general ISA design? \n Answer: \n\nA. Some other options for instruction formats in a general ISA design include variable-length instructions and an immediate-addressing architecture."}, {"text": "Title: Control Flow Conditions* P3 \n Text: \n=WWWWW=WW,WW,WW=\n>CMP>R2,R3>; R2 < R3: CNZ  110, R2 = R3: CNZ  001,\n>>>;     R2 > R3: CNZ  000\n>BLE>R1>; Z  C = 1: PC  R1\n{-6pt} \n Question: \nQ. What is the LC-3 ISA? \n Answer: \n\nA. The LC-3 ISA is an instruction set architecture that specifies both the type of operation to be performed by the instruction and the types of arguments to the operation."}, {"text": "Title: Control Flow Conditions* P4 \n Text: The status bits are not always implemented as special-purpose\nregisters; instead, they may be kept in general-purpose registers or\nnot kept at all.  For example, the Alpha ISA stores the results of\ncomparisons in general-purpose registers, and the same branch is\ninstead implemented as follows: \n Question: \nQ. Why does the ADD and AND instruction in the LC-3 contain a 1-bit mode field?\nQ. \nQ. The 1-bit mode field in the ADD and AND instruction of the LC-3 specifies whether the second operand of the ADD/AND comes from a register or is an immediate value. \n Answer: "}, {"text": "Title: Control Flow Conditions* P5 \n Text: {-6pt}\n=WWWWW=WW,WW,WW=\n>CMPLE>R4,R2,R3>; R2  R3: R4  1, R2 > R3: R4  0\n>BNE>R4,R1>; R4  0: PC  R1\n{-6pt} \n Question: \nQ. How many operands are needed for each instruction? \n Answer: \n\nA. Each instruction needs at least one operand."}, {"text": "Title: Control Flow Conditions* P6 \n Text: Finally, status bits can be calculated, used, and discarded within\na single instruction, in which case the branch is written as follows: \n Question: \nQ. How does fetching the opcode and mode fields in order to decide how many more\nQ. bits are necessary to complete the instruction impact performance? \n Answer: \n\nFetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction can impact performance by causing the processor to stall while it fetches the additional bits. This stall can cause the processor to fall behind in its execution, leading to reduced performance."}, {"text": "Title: Control Flow Conditions* P7 \n Text: {-6pt}\n=WWWWW=WW,WW,WW=\n>BLE>R1,R2,R3>; R2  R3: PC  R1\n{-6pt} \n Question: \nQ. What is the advantage of fixed-length instructions?\nQ. \nQ. Fixed-length instructions are easier to decode. \n Answer: "}, {"text": "Title: Control Flow Conditions* P8 \n Text: The three approaches have advantages and disadvantages similar to\nthose discussed in the section on addressing architectures: the first\nhas the shortest instructions, the second is the most general and\nsimplest to implement, and the third requires the fewest instructions. \n Question: \nQ. What is the maximum length of an x86 instruction?\nQ. \nQ. The maximum length of an x86 instruction is fifteen bytes. \n Answer: "}, {"text": "Title: Stack Operations* P0 \n Text: Two types of stack operations are commonly supported.  Push and pop\nare the basic operations in many older architectures, and values can\nbe placed upon or removed from the stack using these instructions.  In\nmore modern architectures, in which the SP becomes a general-purpose\nregister, push and pop are replaced with indexed loads and stores,\nthat is, loads and stores using the stack pointer and an offset as the\naddress for the memory operation.  Stack updates are performed using\nthe ALU, subtracting and adding immediate values from the SP as\nnecessary to allocate and deallocate local storage. \n Question: \nQ. What is the purpose of an ISA? \n Answer: \n\nThe purpose of an ISA is to define the interface between a computer's software and hardware."}, {"text": "Title: Stack Operations* P1 \n Text: Stack operations serve three purposes in a typical architecture.  The\nfirst is to support procedure calls, as illustrated in a previous\nsection.  The second is to provide temporary storage during\ninterrupts, which was also mentioned earlier.   \n Question: \nQ. What is the significance of an ISA? \n Answer: \n\nAn ISA is important because it defines the interface between software and hardware. This abstraction allows for different types of hardware to be compatible with the same software. In addition, the ISA standardizes the format of instructions so that they can be executed by different types of hardware."}, {"text": "Title: Stack Operations* P2 \n Text: The third use of stack operations is to support { spill code}\ngenerated by compilers.  Compilers first translate high-level\nlanguages into an intermediate representation much like assembly code\nbut with an extremely large (theoretically infinite) register set.\nThe final translation step translates this intermediate representation\ninto assembly code for the target architecture, assigning\narchitectural registers as necessary.  However, as real ISAs support\nonly a finite number of registers, the compiler must occasionally\nspill values into memory.  For example, if ten values are in use at\nsome point in the code, but the architecture has only eight registers,\nspill code must be generated to store the remaining two values on the\nstack and to restore them when they are needed. \n Question: \nQ. What are some of the other options for instruction formats in a general ISA design? \n Answer: \n\nA. Some other options for instruction formats in a general ISA design include variable-length instructions and an immediate-addressing architecture."}, {"text": "Title: I/O* P0 \n Text: As a final topic, we now consider how a processor\nconnects to other devices to allow input and output.  We have already\ndiscussed interrupts, which are a special form of I/O in which only\nthe signal requesting attention is conveyed to the processor.\nCommunication of data occurs through instructions similar to loads and\nstores.  A processor is designed with a number of {{ I/O\nports}---usually} read-only or write-only registers to which devices\ncan be attached with opposite semantics.  That is, a port is usually\nwritten by the processor and read by a device or written by a device\nand read by the processor. \n Question: \nQ. What is the purpose of an ISA? \n Answer: \n\nThe purpose of an ISA is to define the interface between a computer's software and hardware."}, {"text": "Title: I/O* P1 \n Text: The question of exactly how I/O ports are accessed is an interesting\none.  One option is to create special instructions, such as the {\nin} and { out} instructions of the x86 architecture.  Port\naddresses can then be specified in the same way that memory addresses\nare specified, but use a distinct address space.  Just as two sets of\nspecial-purpose registers can be separated by the ISA, such an {\nindependent I/O} system separates I/O ports from memory addresses by\nusing distinct instructions for each class of operation. \n Question: \nQ. What is the significance of an ISA? \n Answer: \n\nAn ISA is important because it defines the interface between software and hardware. This abstraction allows for different types of hardware to be compatible with the same software. In addition, the ISA standardizes the format of instructions so that they can be executed by different types of hardware."}, {"text": "Title: I/O* P2 \n Text: Alternatively, device registers can be accessed using the same load and store\ninstructions as are used to access memory.  This approach, known as\n{ memory-mapped I/O}, requires no new instructions for I/O, but\ndemands that a region of the memory address space be set aside for\nI/O.  The memory words with those addresses, if they exist, can not be\naccessed during normal processor operations. \n Question: \nQ. What are some of the other options for instruction formats in a general ISA design? \n Answer: \n\nA. Some other options for instruction formats in a general ISA design include variable-length instructions and an immediate-addressing architecture."}, {"text": "Title: Summary of Part 4 of the Course P0 \n Text: With the exception of control unit design strategies and redundancy \nand coding, most of the material in this part of the course is drawn from\nPatt and Patel Chapters 4 through 7.  You may also want to read Patt and \nPatel's Appendix C for details of their control unit design. \n Question: \nQ. What is the role of the control unit in a computer?\nQ. \nQ. The control unit is responsible for fetching and decoding instructions, as well as for controlling the overall operation of the computer. \n Answer: "}, {"text": "Title: Summary of Part 4 of the Course P1 \n Text: In this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth). \n Question: \nQ. What is the difference between the \"I\" and \"you\" in the text? \n Answer: \n\nA. The \"I\" in the text is the narrator, who is telling the story. The \"you\" in the text is the reader, who is hearing the story."}, {"text": "Title: Summary of Part 4 of the Course P2 \n Text: We'll start with the easy stuff.  You should recognize all of these terms\nand be able to explain what they mean.  \n For the specific circuits, you \n should be able to draw them and explain how they work.\n(You may skip the *'d terms in Fall 2012.) \n Question: \nQ. How does a transistor work? \n Answer: \n\nA. The transistor is the basic building block of modern electronic devices, and is used to amplify or switch electronic signals. A transistor consists of a semiconductor material with three terminals: the base, the collector, and the emitter. The base–collector junction is forward biased, and the base–emitter junction is reverse biased. When a small current is flowing into the base, it controls a much larger current between the collector and emitter."}, {"text": "Title: Summary of Part 4 of the Course P3 \n Text: {von Neumann elements}\n{-}{{}{}\n{}{}{}\n{program counter (PC)}\n{instruction register (IR)}\n{memory address register (MAR)}\n{memory data register (MDR)}\n{processor datapath} \n Question: \nQ. What is the function of the instruction register?\nQ. \nQ. The instruction register stores the code for the instruction that is currently being executed. \n Answer: "}, {"text": "Title: Summary of Part 4 of the Course P4 \n Text: \n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (in an encoded instruction)}\n{operation code (opcode)} \n Question: \nQ. What is the definition of an instruction set architecture (ISA)? \n Answer: \n\nA. The instruction set architecture (ISA) is the specification of the set of instructions that a processor can execute."}, {"text": "Title: Summary of Part 4 of the Course P5 \n Text: \n{assemblers and assembly code}\n{-}{{}{}\n{}{}{}\n{opcode mnemonic (such as ADD, JMP)}\n{two-pass process} \n Question: \nQ. What is a two-pass process?\nQ. \nQ. A two-pass process is a process in which two passes are made through the data in order to complete the desired task. \n Answer: "}, {"text": "Title: Summary of Part 4 of the Course P6 \n Text: {symbol table}\n{pseudo-op / directive} \n Question: \nQ. This text is a question about a symbol table.\nQ. \nQ. What is a symbol table?\nQ. \nQ. A symbol table is a table that lists all of the symbols used in a program, along with their corresponding addresses. \n Answer: "}, {"text": "Title: Summary of Part 4 of the Course P7 \n Text:  no documentation, and advanced topics ... no testing \n Question: \nQ. What is the purpose of this text?\nQ. \nQ. This text does not seem to have a purpose. \n Answer: "}, {"text": "Title: Summary of Part 4 of the Course P8 \n Text:  {logic design optimization}\n {-}{{}{}\n {}{}{}\n {bit-sliced (including multiple bits per slice)}\n \n {pipelined logic}\n {tree-based}\n  \n Question: \nQ. What is the difference between bit-sliced and pipelined logic? \n Answer: \n\nA. Bit-sliced logic is a type of logic design where each bit of data is processed by a separate slice of logic. Pipelined logic is a type of logic design where each stage of logic is implemented as a separate pipeline stage."}, {"text": "Title: Summary of Part 4 of the Course P9 \n Text: {control unit design strategies}\n{-}{{}{}\n{}{}{}\n{control word / microinstruction}\n{sequencing / microsequencing}\n{hardwired control}\n{-}{{}{}\n{}{}{}\n{single-cycle}\n{multi-cycle} \n Question: \nQ. How does the control unit design strategy affect the control word/microinstruction sequencing? \n Answer: \n\nA. The control unit design strategy affects the control word/microinstruction sequencing in that it determines how the control word is used to generate the microinstruction. If the control unit is hardwired, the control word is used to directly generate the microinstruction. If the control unit is microprogrammed, the control word is used to address a microinstruction in a control store."}, {"text": "Title: Summary of Part 4 of the Course P10 \n Text: {microprogrammed control}\n {pipelining (of instruction processing)} \n Question: \nQ. What is microprogrammed control? \n Answer: \n\nA. Microprogrammed control is a method of controlling a processor using a microprogram. A microprogram is a set of instructions that are stored in a special memory and that tell the processor how to carry out a specific task."}, {"text": "Title: Summary of Part 4 of the Course P11 \n Text: \n{error detection and correction\n{--}{{}{}\n{}{}{}\n code/sparse representation\n code word\n bit error\n odd/even parity bit\n Hamming distance between code words\n Hamming distance of a code\n Hamming code\n SEC-DED \n Question: \nQ. What is the Hamming code?\nQ. \nQ. The Hamming code is a type of error-correcting code that can be used to detect and correct certain types of errors in digital data. \n Answer: "}, {"text": "Title: Summary of Part 4 of the Course P12 \n Text: \nWe expect you to be able to exercise the following skills: \n Question: \nQ. How does the author feel about the subject? \n Answer: \n\nA. The author feels that the subject is very important and that it should be taught in schools."}, {"text": "Title: Summary of Part 4 of the Course P13 \n Text:  FIXME ... should write something about critical path, but expecting\n them to do much with it isn't reasonable \n Question: \nQ. What is the difference between a system call and a library call?\nQ. \nQ. A system call is a low-level interface to the kernel that allows a user program to request a service from the operating system. A library call is a higher-level interface that is implemented as a set of functions in a library. \n Answer: "}, {"text": "Title: Summary of Part 4 of the Course P14 \n Text:  {Implement arbitrary Boolean logic, and be able to reason about\n alternative designs in terms of their critical path delays and number\n of gates required to implement.} \n Question: \nQ. How can you implement arbitrary Boolean logic? \n Answer: \n\nA. You can use a truth table to implement arbitrary Boolean logic."}, {"text": "Title: Summary of Part 4 of the Course P15 \n Text: {Map RTL (register transfer language) operations into control words\nfor a given processor datapath.} \n Question: \nQ. Map RTL (register transfer language) operations into control words\nQ. for a given processor datapath.\nQ. \nQ. What is the register transfer language? \n Answer: \n\nThe register transfer language is a language that specifies the transfer of data between registers in a digital circuit."}, {"text": "Title: Summary of Part 4 of the Course P16 \n Text: {Systematically decompose a (simple enough) problem to the level \nof {LC-3} instructions.} \n Question: \nQ. How do you systematically decompose a problem to the level of LC-3 instructions? \n Answer: \n\nA. By breaking the problem down into smaller, more manageable pieces and then identifying the LC-3 instructions that are needed to solve each piece."}, {"text": "Title: Summary of Part 4 of the Course P17 \n Text: {Encode {LC-3} instructions into machine code.} \n Question: \nQ. How would you encode an LC-3 instruction into machine code? \n Answer: \n\nA. You would use a machine code translator to convert the LC-3 instruction into the corresponding machine code."}, {"text": "Title: Summary of Part 4 of the Course P18 \n Text: {Read and understand programs written in {LC-3} assembly/machine code.} \n Question: \nQ. What is the LC-3 assembly/machine code? \n Answer: \n\nLC-3 assembly/machine code is a low-level programming language used to program the LC-3 microprocessor."}, {"text": "Title: Summary of Part 4 of the Course P19 \n Text: {Test and debug a small program in {LC-3} assembly/machine code.} \n Question: \nQ. What is the {LC-3}?\nQ. \nQ. The LC-3 is a 16-bit microprocessor developed by Moodle in the early 1980s. \n Answer: "}, {"text": "Title: Summary of Part 4 of the Course P20 \n Text: {Be able to calculate the Hamming distance of a code/representation.} \n Question: \nQ. What is the Hamming distance of a code/representation? \n Answer: \n\nA. The Hamming distance of a code/representation is the number of bits that differ between two code words."}, {"text": "Title: Summary of Part 4 of the Course P21 \n Text: {Know the relationships between Hamming distance and the abilities\nto detect and to correct bit errors.} \n Question: \nQ. What is Hamming distance? \n Answer: \n\nA. Hamming distance is a measure of the number of bit errors that can be detected and corrected in a message."}, {"text": "Title: Summary of Part 4 of the Course P22 \n Text: We expect that you will understand the concepts and ideas to the extent\nthat you can do the following: \n Question: \nQ. What are the consequences of not understanding the concepts and ideas in the text? \n Answer: \n\nA. The consequences of not understanding the concepts and ideas in the text are that you will not be able to do the following:\n\n- Understand the main points of the text\n- Understand the implications of the text\n- Engage with the text on a deeper level"}, {"text": "Title: Summary of Part 4 of the Course P23 \n Text: {Explain the role of different types of instructions in allowing\na programmer to express a computation.} \n Question: \nQ. What is the role of different types of instructions in allowing a programmer to express a computation? \n Answer: \n\nA. Different types of instructions allow a programmer to express a computation in different ways. For example, some instructions may allow a programmer to express a computation as a series of steps, while other instructions may allow a programmer to express a computation as a single operation."}, {"text": "Title: Summary of Part 4 of the Course P24 \n Text:  FIXME: ISA design is beyond the scope of this course; just include\n for interest, if at all \n Question: \nQ. The text discusses the design of the ISA.\nQ. \nQ. What is the ISA? \n Answer: \n\nThe ISA is the instruction set architecture. It is the set of instructions that a processor can execute."}, {"text": "Title: Summary of Part 4 of the Course P25 \n Text:  {Explain the tradeoffs in different addressing modes so as to motivate\n inclusion of multiple addressing modes in an ISA.} \n Question: \nQ. What are the benefits of having multiple addressing modes in an ISA? \n Answer: \n\nThere are several benefits to having multiple addressing modes in an ISA. First, it allows the programmer to more easily express the intent of their code. Second, it can improve code density, which can save on memory and storage requirements. Finally, it can improve performance by allowing the processor to more directly access the data it needs."}, {"text": "Title: Summary of Part 4 of the Course P26 \n Text: {Explain the importance of the three types of subdivisions in systematic\ndecomposition (sequential, conditional, and iterative).} \n Question: \nQ. What is the importance of the three types of subdivisions in systematic decomposition? \n Answer: \n\nA. The three types of subdivisions in systematic decomposition are important because they help to break down a problem into smaller, more manageable pieces. This makes it easier to understand the problem and to find a solution."}, {"text": "Title: Summary of Part 4 of the Course P27 \n Text: {Explain the process of transforming assembly code into machine code\n(that is, explain how an assembler works, including describing the use of\nthe symbol table).} \n Question: \nQ. What is the process of transforming assembly code into machine code? \n Answer: \n\nThe process of transforming assembly code into machine code is called assembly. An assembler is a program that transforms assembly code into machine code. The assembler uses a symbol table to map the assembly code to the machine code."}, {"text": "Title: Summary of Part 4 of the Course P28 \n Text: {Be able to use parity for error detection, and Hamming codes for\nerror correction.} \n Question: \nQ. What is the difference between parity and Hamming codes? \n Answer: \n\nParity is a technique that can be used for error detection, while Hamming codes can be used for error correction. Parity checks whether the number of 1s in a given sequence is even or odd, while Hamming codes compare the parity of different bits in a sequence to detect errors."}, {"text": "Title: Summary of Part 4 of the Course P29 \n Text: At the highest level, \nwe hope that, while you do not have direct substantial experience in \nthis regard from our class (and should not expect to be tested on these\nskills), that you will nonetheless be able to begin\nto do the following when designing combinational logic: \n Question: \nQ. How does this text hope that students will be able to design combinational logic? \n Answer: \n\nA. The text hopes that students will be able to design combinational logic by understanding the principles of Boolean algebra and logic gates."}, {"text": "Title: Summary of Part 4 of the Course P30 \n Text: {Design and compare implementations using gates, decoders, muxes, and/or memories \nas appropriate, and including reasoning about the relevant design tradeoffs \nin terms of area and delay.} \n Question: \nQ. What are some relevant design tradeoffs to consider when implementing a design using gates, decoders, muxes, and/or memories? \n Answer: \n\nSome relevant design tradeoffs to consider when implementing a design using gates, decoders, muxes, and/or memories include area, delay, power consumption, and reliability."}, {"text": "Title: Summary of Part 4 of the Course P31 \n Text: {Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based \ndesign, again including reasoning about the relevant design tradeoffs in\nterms of area and delay.} \n Question: \nQ. What is the primary tradeoff that designers must consider when choosing between a bit-sliced, serial, pipelined, or tree-based design?\nQ. \nQ. The primary tradeoff that designers must consider when choosing between a bit-sliced, serial, pipelined, or tree-based design is area versus delay. \n Answer: "}, {"text": "Title: Summary of Part 4 of the Course P32 \n Text: {Design and compare implementations of processor control units\nusing both hardwired and microprogrammed strategies,\nand again including reasoning about the relevant design tradeoffs in\nterms of area and delay.} \n Question: \nQ. What are some of the tradeoffs between hardwired and microprogrammed control units? \n Answer: \n\nA. The main tradeoff between hardwired and microprogrammed control units is area. Hardwired control units require more area to implement than microprogrammed control units. However, microprogrammed control units may require more area if they are more complex. In terms of delay, hardwired control units are typically faster than microprogrammed control units."}, {"text": "Title: Summary of Part 4 of the Course P33 \n Text: {Understand basic tradeoffs in the sparsity of code words with error \ndetection and correction capabilities.} \n Question: \nQ. How can tradeoffs in the sparsity of code words be used to improve error detection and correction capabilities? \n Answer: \n\nA. One way to improve error detection and correction capabilities is to use sparser code words. This can be done by using a larger code word size, or by using a code with a higher minimum distance."}, {"text": "Title: Summary of Part 4 of the Course P34 \n Text: {   }   blank 3rd page \n Question: \nQ. \nQ. What is the author's purpose for writing this text? \n Answer: \n\nA. To inform the reader about the author's experience with anxiety and to provide helpful tips for managing anxiety."}], "end2end": [{"text": "Title: Finite State Machine Design Examples, Part I \n Text: {Finite State Machine Design Examples, Part I}\n\nThis set of notes uses a series of examples to illustrate design principles \nfor the implementation of finite state machines (FSMs) using digital logic.\nWe begin with an overview of the design process for a digital FSM, from\nthe development of an abstract model through the implementation of\nfunctions for the next-state variables and output signals.\nOur first few examples cover only the concrete aspects:\nwe implement several counters, which illustrate the basic \nprocess of translating a concrete and complete state transition diagram\ninto an implementation based on flip-flops and logic gates.\nWe next consider a counter with a number of states that is not a power of\ntwo, with which we illustrate the need for FSM initialization.\n As part of solving the initialization problem, we also introduce \n a general form of selection logic called a multiplexer.\n\nWe then consider the design process as a whole through a more general\nexample of a counter with multiple inputs to control its behavior. \nWe work from\nan abstract model down to an implementation, illustrating how semantic\nknowledge from the abstract model can be used to simplify the \nimplementation.  Finally, we illustrate how the choice of representation\nfor the FSM's internal state affects the complexity of the implementation.\nFortunately, designs that are more intuitive and easier for humans to\nunderstand also typically make the best designs in terms of \nother metrics, such as logic complexity.\n\n\n \n Question: What is the implementation of the finite state machine? \n Answer: \n\nA. The design process for a digital FSM typically begins with the development of an abstract model. This model is then used to generate the functions for the next-state variables and output signals. Finally, the design is implemented using flip-flops and logic gates."}, {"text": "Title: Steps in the Design Process \n Text: {Steps in the Design Process}\n\nBefore we begin exploring designs, let's talk briefly about the general\napproach that we take when designing an FSM.  We follow a six-step\nprocess:{-8pt}\n\n{{}{}\n{}{}{}\n{develop an abstract model}{step-abs}\n{specify I/O behavior}{step-io}\n{complete the specification}{step-complete}\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n}\n{-8pt}\n\nIn Step {step-abs}, we translate our description in human language\ninto a model with states and desired behavior.  At this stage, we \nsimply try to capture the intent of the description and are not\nparticularly thorough nor exact.\n\nStep {step-io} begins to formalize the model, starting with its\ninput and output behavior.  If we eventually plan to develop an\nimplementation of our FSM as a digital system (which is not the \nonly choice, of course!), all input and output\nmust consist of bits.  Often, input and/or output specifications\nmay need to match other digital systems to which we plan to connect\nour FSM.  In fact, { most problems in developing large digital systems\ntoday arise because of incompatibilities when composing two or more\nseparately designed pieces} (or { modules}) into an integrated system.\n\nOnce we know the I/O behavior for our FSM, in Step {step-complete}\nwe start to make\nany implicit assumptions clear and to make any other decisions\nnecessary to the design.  Occasionally, we may choose to leave\nsomething undecided in the hope of simplifying the design with\n``don't care'' entries in the logic formulation.\n\nIn Step {step-repn}, we select an internal representation\nfor the bits necessary to encode the state of our FSM.  In practice,\nfor small designs, this representation can be selected by a computer \nin such a way as to optimize the implementation.  However, for large\ndesigns, such as the LC-3 instruction set architecture that we\nstudy later in this class, humans do most of the work by hand.\n\nIn the later examples in this set of notes, we show how even a \nsmall design can\nleverage meaningful information from the design when selecting\nthe representation, leading to an implementation that is simpler\nand is easier to build correctly.\n\nWe also show how one can\nuse abstraction to simplify an implementation.\n\nBy Step {step-logic}, our design is a complete specification in\nterms of bits, and we need merely derive logic expressions for the\nnext-state variables and the output signals.  This process is no\ndifferent than for combinational logic, and should already be fairly \nfamiliar to you.\n\n\n\nFinally, in Step {step-gates}, we translate our logic expressions\ninto gates and use flip-flops (or registers) to hold the internal\nstate bits of the FSM.  In later notes, we use more complex\nbuilding blocks when implementing an FSM, building up abstractions\nin order to simplify the design process in much the same way that\nwe have shown for combinational logic.\n\n\n \n Question: Why do most problems arise in developing large digital systems today? \n Answer: \n\nA. Most problems in developing large digital systems today arise because of incompatibilities when composing two or more\nseparately designed pieces into an integrated system."}, {"text": "Title: Example: A Two-Bit Gray Code Counter \n Text: {Example: A Two-Bit Gray Code Counter}\n\nLet's begin with a two-bit Gray code counter with no inputs.\nAs we mentioned in Notes Set 2.1, a Gray code is a cycle over all\nbit patterns of a certain length in which consecutive patterns differ\nin exactly one bit.\n\nFor simplicity, our first few examples are based on counters and\nuse the internal state\nof the FSM as the output values.  You should already know\nhow to design combinational logic for the outputs if it were necessary.\n\nThe inputs to a counter, if any, are typically limited to functions\nsuch as starting and stopping the counter, controlling the counting \ndirection, and resetting the counter to a particular state.\n\nA fully-specified transition diagram for \na two-bit Gray code counter appears below.\nWith no inputs, the states simply form a loop, with\nthe counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_1S_0 \n(before the ``/'') and the output Z_1Z_0 (after the ``/''), which are \nalways equal for this counter.\n\nBased on the transition diagram, we can fill in the K-maps for the \nnext-state values S_1^+ and S_0^+ as shown to the right of the\ntransition diagram, then \nderive algebraic expressions in the usual way to obtain\nS_1^+=S_0 and S_0^+={{S_1}}.\n\nWe then use the next-state logic to develop the implementation\nshown on the far right, completing our first counter design.\n\n\n\n\n\n\n\n\n\n\n\n\n \n Question: What is a Gray code? \n Answer: \n\nA Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit."}, {"text": "Title: Example: A Three-Bit Gray Code Counter \n Text: {Example: A Three-Bit Gray Code Counter}\n\n\nNow we'll add a third bit to our counter, but again use a Gray code\nas the basis for the state sequence.\n\nA fully-specified transition diagram for such a counter appears to \nthe right.  As before, with no inputs, the states simply form a loop, \nwith the counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_2S_1S_0 \n(before ``/'') and the output Z_2Z_1Z_0 (after ``/''). \n\n\n\n\n\n\nBased on the transition diagram, we can fill in the K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then \nderive algebraic expressions.  The results are more complex this \ntime.\n\n\n\n\n\n\n\n\n\nFor our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_0 + S_1 {{S_0}} \nS_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}} \nS_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1\n{eqnarray*}\n\n\nNotice that the equations for S_2^+ and S_1^+ share a common term,\nS_1{{S_0}}.\n\nThis design does not allow much choice in developing good equations for\nthe next-state logic, but some designs may enable you to reduce \nthe design complexity by explicitly identifying and making use of \ncommon algebraic terms and sub-expressions for different outputs.\nIn modern design processes, identifying such opportunities is generally\nperformed by a computer program, but it's important to understand\nhow they arise.  Note that the common term becomes a single AND gate\nin the implementation of our counter, as shown to the right.\n\nLooking at the counter's implementation diagram, notice that the vertical\nlines carrying the current state values and their inverses back to the\nnext state\nlogic inputs have been carefully ordered to simplify\nunderstanding the diagram.  In particular, they are ordered from\nleft to right (on the left side of the figure) as \n{{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.\nWhen designing any logic diagram, be sure to make use of a reasonable\norder so as to make it easy for someone (including yourself!) to read \nand check the correctness of the logic.\n\n\n\n\n\n\n \n Question: What is the output code of a three-bit gray code counter? \n Answer: \n\nA.\n\nA three-bit gray code counter works by cycling through a sequence of states, with each state corresponding to a different output code. The output code is determined by the state of the three flip-flops in the counter. The output code is a gray code, which means that only one bit changes between successive states."}, {"text": "Title: Example: A Color Sequencer \n Text: {Example: A Color Sequencer}\n\n\nEarly graphics systems used a three-bit red-green-blue (RGB) \nencoding for colors.  The color mapping for such a system is shown to\nthe right.\n\nImagine that you are charged with creating a counter to drive a light\nthrough a sequence of colors.  The light takes an RGB input as just\ndescribed, and the desired pattern is\n\n{off (black)     yellow     violet     green     blue}\n\nYou immediately recognize that you merely need a counter with five\nstates.  How many flip-flops will we need?  At least three, since\n_2 (5)=3.  Given that we need three flip-flops, \nand that the colors we need to produce as\n\n\n{c|l}\nRGB& color \n000& black\n001& blue\n010& green\n011& cyan\n100& red\n101& violet\n110& yellow\n111& white\n\n\n\noutputs are all unique\nbit patterns, we can again choose to use the counter's internal \nstate directly as our output values.\n\n\nA fully-specified transition diagram for our color sequencer\nappears to the right.  The states again form a loop,\nand are marked with the internal state value S_2S_1S_0 \nand the output RGB.\n\n\n\n\n\n\nAs before, we can use the transition diagram to fill in K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+, as shown to the right.\nFor each of the three states not included in our transition diagram,\nwe have inserted x's\n\n\n\n\n\n\n\n\n\ninto the K-maps to indicate ``don't care.'' \nAs you know, we can treat each x as either a 0 or a 1, whichever\nproduces better results (where ``better'' usually means simpler \nequations).  The terms that we have chosen for our algebraic \nequations are illustrated in the K-maps.  The x's within the ellipses\nbecome 1s in the implementation, and the x's outside of the ellipses\nbecome 0s.\n\n\nFor our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}} \nS_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}} \nS_0^+ &=& S_1\n{eqnarray*}\n\nAgain our equations for S_2^+ and S_1^+ share a common term,\nwhich becomes a single AND gate in the implementation shown to the\nright.\n\n\n\n\n\n \n Question: Is it possible to create a color sequencer? \n Answer: \n\nA. No, it is not possible."}, {"text": "Title: Identifying an Initial State \n Text: {Identifying an Initial State}\n\nLet's say that you go the lab and build the implementation above, \nhook it up\nto the light, and turn it on.  Does it work?  Sometimes.\nSometimes it works perfectly, but sometimes\nthe light glows cyan or red briefly first.\nAt other times, the light is an\nunchanging white.\n\n\nWhat could be going wrong?\n\nLet's try to understand.  We begin by deriving\nK-maps for the implementation, as shown to the right.  In these\nK-maps, each of the x's in our design has been replaced by either a 0\nor a 1.  These entries are highlighted with green italics.\n\n\n{file=part3/figs/colS2-bad.eps,width=1.00in}\n\n{file=part3/figs/colS1-bad.eps,width=1.00in}\n\n{file=part3/figs/colS0-bad.eps,width=1.00in}\n\n\nNow let's imagine what might happen if somehow our FSM got into the\nS_2S_1S_0=111 state.  In such a state, the light would appear white,\nsince RGB=S_2S_1S_0=111.\n\nWhat happens in the next cycle?\n\nPlugging into the equations or looking into the K-maps gives (of\ncourse) the same answer: the next state is the\nS_2^+S_1^+S_0^+=111 state.\nIn other words, the light stays white indefinitely!\n\nAs an exercise, you should check what happens \nif the light is red or cyan.\n\nWe can extend the transition diagram that we developed for our design\nwith the extra states possible in the implementation, as shown below.\nAs with the five states in the design, the extra states are named with\nthe color of light that they produce.\n\n{{file=part3/figs/colors-full.eps,width=5.8in}}\n\nNotice that the FSM does not move out of the WHITE state (ever).  \n\nYou may at this point wonder whether more careful decisions \nin selecting our next-state expressions might address this issue.\nTo some extent, yes.  For example, if we replace the \nS_2S_1 term in the equation for S_2^+ with S_2{{S_0}}, \na decision allowed\nby the ``don't care'' boxes in the K-map for our design,\nthe resulting transition diagram does not suffer from the problem\nthat we've found.\n\nHowever, even if we do change our implementation slightly, we need\nto address another aspect of the problem:\n\nhow can the FSM ever get into the unexpected states?\n\n\nWhat is the initial state of the three flip-flops in our implementation?\n\n{ The initial state may not even be 0s and 1s unless we have an \nexplicit mechanism for initialization.} \n\nInitialization can work in two ways.  \n\nThe first approach makes use of the flip-flop design.\nAs you know, a flip-flop is built from a pair of latches, and\nwe can \nmake use of the internal reset lines on these latches\nto force each flip-flop into the 0 state (or the 1 state) using an\nadditional input. \n\nAlternatively, we can add some extra logic to our design.\n\nConsider adding a few AND gates and a  input\n(active low), as shown in the dashed box in the figure to the right.\nIn this case, when we assert  by setting it to 0,\nthe FSM moves to state 000 in the next cycle, putting it into\nthe BLACK state.  The approach taken here is for clarity; one can\noptimize the design, if desired.  For example, we could simply connect\n as an extra input into the three AND gates on the\nleft rather than adding new ones, with the same effect.\n\nWe may sometimes want a more powerful initialization mechanism---one\nthat allows us to force the FSM into any specific state in the next\ncycle.  In such a case, we can add multiplexers to each of our \nflip-flop inputs, allowing us to use the INIT input to choose between\nnormal operation (INIT=0) of the FSM and forcing the FSM into the\nnext state given by I_2I_1I_0 (when INIT=1).\n\n\n\n\n\n\n\n\n \n Question: What is the INIT input? \n Answer: \n\nA. The INIT input allows us to force the FSM into a specific state in the next cycle."}, {"text": "Title: Developing an Abstract Model \n Text: {Developing an Abstract Model}\n\n\nWe are now ready to discuss the design process for an FSM from start\nto finish.\n\nFor this first abstract FSM example, we build upon something\nthat we have already seen: a two-bit Gray code counter.\nWe now want a counter that allows us to start and stop the\n\n\n{c|ccc}\nstate&    no input&  halt button& go button \ncounting& counting&      halted& \nhalted&   halted&              & counting\n\n\n\ncount.\n\nWhat is the mechanism for stopping and starting?  To\nbegin our design, we could sketch out an abstract next-state\ntable such as the one shown to the right above.  In this form of the table,\nthe first column lists the states, while each of the other columns lists\nstates to which the FSM transitions after a clock cycle for a particular\ninput combination. \n\nThe table contains two states, counting and halted, and specifies\nthat the design uses two distinct buttons to move between the\nstates.\nThe table further implies that if the counter is halted,\nthe ``halt'' button has no additional effect, and if the counter\nis counting, the ``go'' button has no additional effect.\n\n\nA counter with a single counting state, of course, does not provide\nmuch value.  We extend the table with four counting states and four\nhalted states, as shown to the right.  This version of the\ntable also introduces more formal state names, for which these notes \nuse all capital letters.\n\nThe upper four states represent uninterrupted counting, in which \nthe counter cycles through these states indefinitely.\n\nA user can stop the counter in any state by pressing the ``halt''\nbutton, causing the counter to retain its current value until the\nuser presses the ``go'' button.\n\nBelow the state table is an abstract transition diagram, which provides\nexactly the same information in graphical form.  Here circles represent\nstates (as labeled) and arcs represent transitions from one state\nto another based on an input combination (which is used to label the\narc).\n\nWe have already implicitly made a few choices about our counter design.\n\nFirst, the counter\n\n\n{c|ccc}\nstate&    no input&  halt button& go button \n{ COUNT A}& { COUNT B}& { HALT A}& \n{ COUNT B}& { COUNT C}& { HALT B}& \n{ COUNT C}& { COUNT D}& { HALT C}& \n{ COUNT D}& { COUNT A}& { HALT D}& \n{ HALT A}&  { HALT A}&              & { COUNT B}\n{ HALT B}&  { HALT B}&              & { COUNT C}\n{ HALT C}&  { HALT C}&              & { COUNT D}\n{ HALT D}&  { HALT D}&              & { COUNT A}\n\n\n\n\nshown retains the current state of the system when\n``halt'' is pressed.\nWe could instead reset the counter state whenever it\nis restarted, in which case we need only five states: four for\ncounting and one more for a halted counter.\n\nSecond, we've designed the counter to stop\nwhen the user presses ``halt'' and to resume counting \nwhen the user presses ``go.''  We could instead choose to delay these \neffects by a cycle.  For example, pressing ``halt'' in state { COUNT B}\ncould take the counter to state { HALT C}, and pressing ``go'' \nin state { HALT C} could take the system to state { COUNT C}.\n\nIn these notes, we implement only the diagrams shown.\n\n \n Question: What is the mechanism for stopping and starting? \n Answer: \n\nA. The mechanism for stopping and starting is that a user can stop the counter in any state by pressing the \"halt\" button, causing the counter to retain its current value until the user presses the \"go\" button."}, {"text": "Title: Specifying I/O Behavior \n Text: {Specifying I/O Behavior}\n\n\nWe next start to formalize our design by specifying its input and \noutput behavior digitally.  Each of the two control buttons provides\na single bit of input.  The ``halt'' button we call H, and the\n``go'' button we call G.\n\nFor the output, we use a two-bit \nGray code.  With these choices, we can redraw the transition diagram \nas show to the right.\n\nIn this figure, the states are marked with output values Z_1Z_0 and\ntransition arcs are labeled in terms of our two input buttons, G and H.  \nThe uninterrupted counting cycle is labeled with \nto indicate that it continues until we press H.\n\n\n\n \n\n \n Question: What is the purpose of the two-bit Gray code? \n Answer: \n\nA. The purpose of the two-bit Gray code is to allow for easy determination of the output value when the input value changes."}, {"text": "Title: Completing the Specification \n Text: {Completing the Specification}\n\nNow we need to think about how the system should behave if something \noutside of our initial expectations occurs.  Having drawn out a partial\ntransition diagram can help with this process, since we can use the\ndiagram to systematically consider all possible input conditions from\nall possible states.  The state table form can make the missing\nparts of the specification even more obvious.\n\n\n\n\nFor our counter, the symmetry between counting states makes the problem \nsubstantially simpler.  Let's write out part of a list of states and\npart of a state table with one \ncounting state and one halt state, as shown to the right.\nFour values of the inputs HG \nare possible (recall that N bits allow 2^N possible patterns).\nWe list the columns in Gray code order, since we may want to\ntranscribe this table into K-maps later.\n\n\n{\n\n& \nfirst counting state& { COUNT A}& counting, output Z_1Z_0=00\n  first halted state&  { HALT A}& halted, output Z_1Z_0=00\n\n\n{c|cccc}\n&{HG}\n        state&            00&            01&          11&           10 \n{ COUNT A}& { COUNT B}&   unspecified& unspecified& { HALT A}\n { HALT A}&  { HALT A}& { COUNT B}& unspecified&  unspecified\n\n\n\nLet's start with the { COUNT A} state.  \n\nWe know that if neither button is pressed (HG=00), we want \nthe counter to move to the { COUNT B} state.  And, if we press the\n``halt'' button (HG=10), we want the counter to move to the { HALT A}\nstate.  What should happen if a user presses the ``go'' button (HG=01)?\nOr if the user presses both buttons (HG=11)?\n\nAnswering these questions is part of fully specifying our design.  We\ncan choose to leave some parts unspecified, but { any implementation of\nour system will imply answers}, and thus we must be careful.\n\nWe choose to ignore the ``go'' button while counting, and to have the\n``halt'' button override the ``go'' button.  Thus, if HG=01 when the\ncounter is in state { COUNT A}, the counter moves to state { COUNT B}.\nAnd, if HG=11, the counter moves to state { HALT A}.\n\nUse of explicit bit patterns for the inputs HG may help you to check \nthat all four possible input values are covered from each state.  If \nyou choose to use a transition diagram instead of a state table,\nyou might even want to add four arcs from each state, each labeled \nwith a specific\nvalue of HG.  When two arcs connect the same two states, we can either \nuse multiple labels or can indicate bits that do not matter using a\n{ don't-care} symbol, x.  For example, the arc from state { COUNT A}\nto state { COUNT B} could be labeled HG=00,01 or HG=0x.  The\narc from state { COUNT A} to state { HALT A} could be labeled\nHG=10,11 or HG=1x.  We can also use logical expressions as labels,\nbut such notation can obscure unspecified transitions.\n\nNow consider the state { HALT A}.  The transitions specified so far\nare that when we press ``go'' (HG=01), the counter moves to \nthe { COUNT B} state, and that the counter remains halted in \nstate { HALT A} if no buttons are pressed (HG=00).\nWhat if the ``halt'' button is pressed (HG=10), or\nboth buttons are pressed (HG=11)?  For consistency, we decide that\n``halt'' overrides ``go,'' but does nothing special if it alone is pressed\nwhile the counter is halted.  Thus, input patterns HG=10 and HG=11 also \ntake state { HALT A} back to itself.\nHere the arc could be labeled HG=00,10,11 or, equivalently,\nHG=00,1x or HG=x0,11.\n\n\nTo complete our design, we apply the same decisions that we made for \nthe { COUNT A} state to all of the other counting states, and the \ndecisions that we made for the { HALT A} state to all of the other \nhalted states.  If we had chosen not to specify an answer, an implementation\ncould produce different behavior from the different counting\nand/or halted states, which might confuse a user.\n\nThe resulting design appears to the right.\n\n\n\n \n\n\n \n Question: What is important to specify all possible inputs for each state? \n Answer: \n\nAns. It is important to specify all possible inputs for each state because any implementation of the system will imply answers to these inputs, and thus we must be careful."}, {"text": "Title: Choosing a State Representation \n Text: {Choosing a State Representation}\n\nNow we need to select a representation for the states.  Since our counter\nhas eight states, we need at least three (_2 (8)=3)\nstate bits S_2S_1S_0 to keep track of the current state.\n\nAs we show later, { the choice of representation for an FSM's states\ncan dramatically affect the design complexity}.  For a design as simple as \nour counter, you could just let a computer implement all possible \nrepresentations (there aren't more than 840, if we consider simple \nsymmetries) and select one according to whatever metrics are interesting.\n\nFor bigger designs, however, the number of possibilities quickly becomes\nimpossible to explore completely.\n\nFortunately, { use of abstraction in selecting a representation \nalso tends to produce better designs} for a wide variety of metrics\n(such as design complexity, area, power consumption, and performance).\n\nThe right strategy is thus often to start by selecting a representation \nthat makes sense to a human, even if it requires more bits than are\nstrictly necessary.  The\nresulting implementation will be easier to\ndesign and to debug than an implementation in which only the global \nbehavior has any meaning.\n\n\nLet's return to our specific example, the counter.  We can use one bit, \nS_2, to record whether or not our counter is counting (S_2=0) or\nhalted (S_2=1).  The other two bits can then record the counter state\nin terms of the desired output.  Choosing this representation\nimplies that only wires will be necessary to compute outputs Z_1 \nand Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting\ndesign, in which states are now labeled with both internal state and\noutputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version,\nwe have changed the arc labeling to use logical expressions, which\ncan sometimes help us to think about the implementation.\n\n\n\n\n\nThe equivalent state listing and state table appear below.  We have ordered\nthe rows of the state table in Gray code order to simplify transcription\nof K-maps.\n\n\n\n& S_2S_1S_0& \n{ COUNT A}& 000& counting, output Z_1Z_0=00\n{ COUNT B}& 001& counting, output Z_1Z_0=01\n{ COUNT C}& 011& counting, output Z_1Z_0=11\n{ COUNT D}& 010& counting, output Z_1Z_0=10\n { HALT A}& 100& halted, output Z_1Z_0=00\n { HALT B}& 101& halted, output Z_1Z_0=01\n { HALT C}& 111& halted, output Z_1Z_0=11\n { HALT D}& 110& halted, output Z_1Z_0=10\n\n\n{rc|cccc}\n&&{HG}\n&S_2S_1S_0& 00& 01& 11& 10 \n{ COUNT A}&000& 001& 001& 100& 100\n{ COUNT B}&001& 011& 011& 101& 101\n{ COUNT C}&011& 010& 010& 111& 111\n{ COUNT D}&010& 000& 000& 110& 110\n { HALT D}&110& 110& 000& 110& 110\n { HALT C}&111& 111& 010& 111& 111\n { HALT B}&101& 101& 011& 101& 101\n { HALT A}&100& 100& 001& 100& 100\n\n\n\nHaving chosen a representation, we can go ahead and implement our\ndesign in the usual way.  As shown to the right, K-maps for the \nnext-state logic are complicated, since we have five variables\nand must consider implicants that are not contiguous in the K-maps.\nThe S_2^+ logic is easy enough: we only need two terms, \nas shown.\n\nNotice that we have used color and\nline style to distinguish different\n\n\n\n\n\n\n\n\n\nimplicants in the K-maps.  Furthermore, the symmetry of the design\nproduces symmetry in the S_1^+ and S_0^+ formula, so we have\nused the same color and line style for analogous terms in these\ntwo K-maps.\n\nFor S_1^+, we need four terms.  The green \nellipses in the HG=01 column are part of the same term, as are\nthe two halves of the dashed blue circle.  In S_0^+, we still\nneed four terms, but three of them are split into two pieces \nin the K-map.  As you can see, the utility of the K-map is starting\nto break down with five variables.\n\n\n \n Question: What is the right strategy for a counter? \n Answer: \n\nA. The right strategy is thus often to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning."}, {"text": "Title: Abstracting Design Symmetries \n Text: {Abstracting Design Symmetries}\n\nRather than implementing the design as two-level logic, let's try to\ntake advantage of our design's symmetry to further simplify the\nlogic (we reduce gate count at the expense of longer, slower paths).\n\nLooking back to the last transition diagram, in which the arcs\nwere labeled with logical expressions, let's calculate an expression\nfor when the counter should retain its current value in the next\ncycle.  We call \nthis variable HOLD.  In the counting states, when S_2=0, \nthe counter stops (moves into a halted state without changing value) \nwhen H is true.\nIn the halted states, when S_2=1, the counter stops (stays in \na halted state) when H+ is true.  We can thus write\n\n{eqnarray*}\nHOLD &=& {S_2}  H + S_2  ( H +  )\nHOLD &=& {S_2} H + S_2 H + S_2 \nHOLD &=& H + S_2 \n{eqnarray*}\n\nIn other words, the counter should hold its current \nvalue (stop counting) if we press the ``halt'' button or if the counter\nwas already halted and we didn't press the ``go'' button.  As desired,\nthe current value of the counter (S_1S_0) has no impact on this \ndecision.  You may have noticed that the expression we derived for\nHOLD also matches S_2^+, the next-state value of S_2 in the \nK-map on the previous page.\n\nNow let's re-write our state transition table in terms of HOLD.  The\nleft version uses state names for clarity; the right uses state values\nto help us transcribe K-maps.\n\n{\n{rc|cc}\n&&{HOLD}\n&S_2S_1S_0& 0& 1 \n{ COUNT A}&000& { COUNT B}& { HALT A}\n{ COUNT B}&001& { COUNT C}& { HALT B}\n{ COUNT C}&011& { COUNT D}& { HALT C}\n{ COUNT D}&010& { COUNT A}& { HALT D}\n { HALT A}&100& { COUNT B}& { HALT A}\n { HALT B}&101& { COUNT C}& { HALT B}\n { HALT C}&111& { COUNT D}& { HALT C}\n { HALT D}&110& { COUNT A}& { HALT D}\n\n{rc|cc}\n&&{HOLD}\n&S_2S_1S_0& 0& 1 \n{ COUNT A}&000& 001& 100\n{ COUNT B}&001& 011& 101\n{ COUNT C}&011& 010& 111\n{ COUNT D}&010& 000& 110\n { HALT A}&100& 001& 100\n { HALT B}&101& 011& 101\n { HALT C}&111& 010& 111\n { HALT D}&110& 000& 110\n\n\n\nThe K-maps based on the HOLD abstraction are shown to the right.\nAs you can see, the necessary logic has been simplified substantially,\nrequiring only two terms each for both S_1^+ and S_0^+.  Writing\nthe next-state logic algebraically, we obtain\n\n{eqnarray*}\nS_2^+ &=& HOLD\nS_1^+ &=&   S_0 + HOLD  S_1\nS_0^+ &=&   {{S_1}} + HOLD  S_0\n{eqnarray*}\n\n\n\n\n\n\n\n\n\nNotice the similarity between the equations for S_1^+S_0^+ and the \nequations for a {2-to-1} mux: when HOLD=1, the counter retains \nits state, and when HOLD=0, it counts.\n\n\n\n\n\nAn implementation appears below.\n\nBy using semantic meaning in our choice of representation---in\nparticular the use of S_2 to record whether\nthe counter is currently halted (S_2=1) or counting (S_2=0)---we\nhave enabled ourselves to \nseparate out the logic for deciding whether to advance the counter\nfairly cleanly from the logic for advancing the counter itself.\nOnly the HOLD bit in the diagram is used to determine\nwhether or not the counter should advance in the current cycle.\n\nLet's check that the implementation matches our original design.\n\nStart by verifying that the HOLD variable is calculated correctly,\nHOLD=H+S_2,\nthen look back at the K-map for S_2^+ in the low-level design to\nverify that the expression we used does indeed match.\n\n\n\nNext, check the mux abstraction.\n\nWhen HOLD=1, the next-state logic for S_1^+ and S_0^+ \nreduces to S_1^+=S_1 and S_0^+=S_0;\nin other words, the counter stops counting and simply stays in its \ncurrent state.  When HOLD=0, these equations become\nS_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating\nsequence for S_1S_0 of 00, 01, 11, 10, as desired.\nYou may want to look back at our two-bit Gray code counter design\nto compare the next-state equations.\n\nWe can now verify that the implementation produces the correct transition\nbehavior.  In the counting states, S_2=0, and the HOLD value simplifies\nto HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and\nand the counter continues to count in the correct sequence.\nWhen H=1, HOLD=1, and the counter stops at its current value\n(S_2^+S_1^+S_0^+=1S_1S_0, \nwhich is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).\n\nIn any of the halted states, S_2=1, and we can reduce HOLD to\nHOLD=H+.  Here, so long as we press the ``halt'' button\nor do not press the ``go'' button, the counter stays in its current\nstate, because HOLD=1.  If we release ``halt'' and press ``go,''\nwe have HOLD=0, and the counter resumes counting\n(S_2^+S_1^+S_0^+=0S_0{{S_1}},\nwhich is shorthand for S_2^+=0, S_1^+=S_0, and \nS_0^+={{S_1}}).\n\nWe have now verified the implementation.\n\nWhat if you wanted to build a three-bit Gray code counter with the same\ncontrols for starting and stopping?  You could go back to basics and struggle \nwith six-variable {K-maps}.  Or you could simply copy the HOLD \nmechanism from the two-bit design above, insert muxes between the next \nstate logic and the flip-flops of the three-bit Gray code counter that \nwe designed earlier, and control the muxes with the HOLD bit.  \nAbstraction is a powerful tool.\n\n\n\n \n Question: What is the value of HOLD? \n Answer: \n\nA. The value of HOLD is 1."}, {"text": "Title: Impact of the State Representation \n Text: {Impact of the State Representation}\n\nWhat happens if we choose a bad representation?  For the same FSM---the\ntwo-bit Gray code counter with start and stop inputs---the \ntable below shows a poorly chosen mapping from states to internal \nstate representation.\n\nBelow the table is a diagram of an implementation using that\nrepresentation.\n\nVerifying that the implementation's behavior\nis correct is left as an exercise for the determined reader.\n\n{\n{|c|c|c|c|c|}{1-2}{4-5}\nstate& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5}\n{ COUNT A}& 000& & { HALT A}& 111 \n{ COUNT B}& 101& & { HALT B}& 110 \n{ COUNT C}& 011& & { HALT C}& 100 \n{ COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}\n\n}\n\n\n\n\n\n\n\n\n \n Question: What is the impact of changing the state representation? \n Answer: \n\nA. Changing the state representation can impact the design of the FSM in several ways. For example, if the states are represented by numbers instead of by letters, the transition table and state diagram will be different. In addition, the implementation of the FSM may be more complex if the states are represented by numbers instead of by letters."}, {"text": "Title: Random Access Memories \n Text: {Random Access Memories}\n\nThis set of notes describes random access memories (RAMs), providing slightly\nmore detail than is available in the textbook.  We begin with a discussion\nof the memory abstraction and the types of memory most commonly used in\ndigital systems, then examine how one can build memories (static RAMs) \nusing logic.  We next introduce tri-state buffers as a way of simplifying\nouput connections, and illustrate how memory chips can be combined to\nprovide larger and wider memories.  A more detailed description of dynamic \nRAMs finishes this set.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}\n\n \n Question: What type of RAMs are dynamic? \n Answer: \n\nA. Static RAMs store data in a flip-flop, while dynamic RAMs store data in a capacitor."}, {"text": "Title: Memory \n Text: \n\nA computer { memory} is a group of storage elements and the logic\nnecessary to move data in and out of the elements.  The size of the\nelements in a memory---called the { addressability} of the \nmemory---varies from a single binary digit, or { bit},\nto a { byte} (8 bits) or more.  Typically, we refer to data\nelements larger than a byte as { words}, but the size of a word\ndepends on context. \n\nEach element in a memory is assigned a unique name, called an {\naddress}, that allows an external circuit to identify the particular\nelement of interest.  These addresses are not unlike the street\naddresses that you use when you send a letter.  Unlike street\naddresses, however, memory addresses usually have little or no\nredundancy; each possible combination of bits in an address identifies\na distinct set of bits in the memory.  The figure on the right below \nillustrates the concept.  Each house represents a storage element and \nis associated with a unique address.\n\n{{file=part3/figs/lec18-1.eps,width=4in}}\n\nThe memories that we consider in this class have several properties in\ncommon.  These memories support two operations: { write} places a\nword of data into an element, and { read} retrieves a copy of a\nword of data from an element.  The memories are also { volatile},\nwhich means that the data held by a memory are erased when electrical\npower is turned off or fails.  { Non-volatile} forms of memory\ninclude magnetic and optical storage media such as DVDs, CD-ROMs, disks, \nand tapes, capacitive storage media such as Flash drives,\nand some programmable logic devices.\nFinally, the memories considered in this class are { random access\nmemories (RAMs)}, which means that the time required to access an\nelement in the memory is independent of the element being accessed.\nIn contrast, { serial memories} such as magnetic tape require much\nless time to access data near the current location in the tape than\ndata far away from the current location.\n\nThe figure on the left above shows a generic RAM structure.  The\nmemory contains 2^k elements of N bits each.  A {k-bit}\naddress input, ADDR, identifies the memory element of interest for\nany particular operation.  The write enable\ninput, WE, selects the operation to be performed: if\nWE is high, the operation is a write; if it is low, the\noperation is a read.  Data to be written into an element are provided\nthrough N inputs at the top, and data read from an element appear on\nN outputs at the bottom.  Finally, a { chip select} input, CS,\nfunctions as an enable control for the memory; when CS is low, the\nmemory neither reads nor writes any location.\n\nRandom access memory further divides into two important types: {\nstatic RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}.\nSRAM employs active logic in the form of a two-inverter loop to\nmaintain stored values.  DRAM uses a charged capacitor to store a bit;\nthe charge drains over time and must be replaced, giving rise to the\nqualifier ``dynamic.''  ``Static'' thus serves only to differentiate\nmemories with active logic elements from those with capacitive\nelements.  Both types are volatile, that is, both lose all data when the\npower supply is removed.  We consider both SRAM and DRAM \nin this course, but the details of DRAM operation are beyond our scope. \n\n\n \n Question: What type of memory does SRAM use? \n Answer: \n\nSRAM uses active logic elements to store data, while DRAM uses capacitive elements."}, {"text": "Title: Static Random Access Memory \n Text: {Static Random Access Memory}\n\nStatic random access memory is used for high-speed applications such\nas processor caches and some embedded designs.  As SRAM bit\n{density---the} number of bits in a given chip {area---is}\nsignificantly lower than DRAM bit density, most applications with less\ndemanding speed requirements use DRAM.  The main memory in most\ncomputers, for example, is DRAM, whereas the memory on the same chip\nas a processor is SRAM.{Chips combining both DRAM and processor\nlogic are available, and are used by some processor manufacturers (such \nas IBM).  Research is underway to couple such logic types more efficiently\nby building 3D stacks of chips.}  DRAM is also unavailable\nwhen recharging its capacitors, which can be a problem for\napplications with stringent real-time needs.\n\n\nA diagram of an SRAM { cell} (a single bit) appears to\nthe right.  A dual-inverter loop stores the bit, and is connected\nto opposing BIT lines through transistors controlled by a SELECT\nline.  \n\nThe cell works as follows.  When SELECT is high, the\ntransistors connect the inverter loop to the bit lines.  When writing\na cell, the bit lines are held at opposite logic values, forcing the\ninverters to match the values on the lines and storing the value from\nthe BIT input.  When reading a cell, the bit lines are disconnected\nfrom other logic, allowing the inverters to drive the lines with\ntheir current outputs.  \n\n\n{file=part3/figs/lec18-2.eps,width=2.20in}\n\n\nThe value stored previously is thus copied onto\nthe BIT line as an output, and the opposite value is placed on the\n line.  When SELECT is low, the transistors\ndisconnect the inverters from the bit lines, and the cell\nholds its current value until SELECT goes high again.\n\nThe actual operation of an SRAM cell is more complicated than we\nhave described.  For example, when writing a bit, the BIT lines \ncan temporarily connect high voltage to ground (a short).  The \ncircuit must be designed carefully to minimize the power consumed\nduring this process.  When reading a bit, the BIT lines\nare pre-charged halfway between high-voltage and ground, and \nanalog devices called sense amplifiers are used to detect the\nvoltage changes on the BIT lines (driven by the inverter loop)\nas quickly as possible.  These analog design issues are outside of \nthe scope of our class.\n\n\nA number of cells are combined into a { bit slice}, as shown to\nthe right.\n\nThe labels along the bottom of the figure are external inputs to the \nbit slice, and match the labels for the abstract\n\n\n{file=part3/figs/lec18-3.eps,width=5in}\n\n\nmemory discussed earlier.  The \nbit slice in the figure can be thought of as a {16-address},\n{1-bit-addressable} memory (2^4b).\n\nThe cells in a bit slice\nshare bit lines and analog read and write logic, which appears to the\nright in the figure.  Based on the ADDR input, a decoder sets one\ncell's SELECT line high to enable a read or write operation to the\ncell.  \n\nThe chip select input CS drives the enable input of\nthe decoder, so none of the memory cells is active when chip select is\nlow (CS=0), and exactly one of the memory cells is active when\nchip select is high (CS=1).\n\nActual bit slices can contain many more cells than are shown in the \nfigure---more cells means less extra logic per cell, but slower memory,\nsince longer wires have higher capacitance.\n\nA read operation is performed as follows.  We set CS=1 and WE=0,\nand place the address of the cell to be read on the ADDR input.\nThe decoder outputs a 1 on the appropriate cell's SELECT line,\nand the read logic reads the bit from the cell and delivers it\nto its Q output, which is then available on the bit \nslice's {DATA-OUT} output.\n\nFor a write operation, we set CS=1 and WE=1.  We again place the\naddress of the cell to be written on the ADDR input and set the\nvalue of the bit slice's {DATA-IN} input to the value to be written\ninto the memory cell.  When the decoder activates the cell's SELECT line,\nthe write logic writes the new value from its D input into\nthe memory cell.  Later reads from that cell then produce the new value.\n\n{{file=part3/figs/lec18-4.eps,width=6.15in}}\n\nThe outputs of the cell selection decoder can be used to control\nmultiple bit slices, as shown in the figure above of a {2^6b}\nmemory.  Selection between bit slices is\nthen based on other bits from the address (ADDR).  In the figure\nabove, a {2-to-4} decoder is used to deliver write requests to\none of four bit slices, and a {4-to-1} mux is used to choose\nthe appropriate output bit for read requests.\n\nThe {4-to-16} decoder now activates one cell in each of the four \nbit slices.  For a read operation, WE=0, and the {2-to-4} decoder \nis not enabled, so it outputs all 0s.  All four bit slices thus perform\nreads, and the desired result bit is forwarded to {DATA-OUT} by the \n{4-to-1} mux.  The tri-state buffer between the mux \nand {DATA-OUT} is explained in a later section.\n\nFor a write operation, exactly one of the bit\nslices has its WE input set to 1 by the {2-to-4} decoder.\nThat bit slice writes the bit value delivered to all bit slices\nfrom {DATA-IN}.  The other three bit slices perform reads, but their \nresults are simply discarded.\n\nThe approach shown above, in which a cell is selected\nthrough a two-dimensional indexing scheme, is known as { coincident\nselection}.  The qualifier ``coincident'' arises from the notion that\nthe desired cell coincides with the intersection of the active row and\ncolumn outputs from the decoders.\n\nThe benefit of coincident selection is easily calculated in terms of\nthe number of gates required for the decoders.  Decoder complexity is\nroughly equal to the number of outputs, as each output is a minterm\nand requires a unique gate to calculate it.  \n\n Fanout trees for input terms and inverted terms add relatively few gates.  \n\nConsider a 1M8b RAM chip.  The number of addresses is 2^,\nand the total number of memory cells is 8,388,608 (2^).\nOne option is to use eight bit slices and a {20-to-1,048,576}\ndecoder, or about 2^ gates.  Alternatively, we can use 8,192 bit\nslices of 1,024 cells.  For the second implementation, we need \ntwo {10-to-1024} decoders, or about 2^ gates.  As chip \narea is roughly proportional to the number of gates, the savings are \nsubstantial.  Other schemes are possible as well: if we want a more \nsquare chip area, we might choose to use 4,096 bit slices of 2,048 \ncells along with one {11-to-2048} decoder and\none {9-to-512} decoder.  This approach requires roughly 25 more\ndecoder gates than our previous example, but is still far superior to\nthe eight-bit-slice implementation.\n\nMemories are typically unclocked devices.  However, as you have seen,\nthe circuits are highly structured, which enables engineers to cope\nwith the complexity of sequential feedback design.  Devices used to\ncontrol memories are typically clocked, and the interaction between\nthe two can be fairly complex.  \n\n\nTiming diagrams for reads and writes\nto SRAM are shown to the right.  A write operation\nappears on the left.  In the first cycle, the controller raises the\nchip select signal and places the memory address to be written on the\naddress inputs.  Once the memory has had time to set up the \nappropriate\n\n\n{file=part3/figs/lec18-6.eps,width=4in}\n\n\nselect lines\ninternally, the WE input is raised, and data are placed\non the data inputs.  The delay, which is specified by the memory\nmanufacturer, is necessary to avoid writing data to the incorrect\nelement within the memory.  The timing shown in the\nfigure rounds this delay up to a single clock cycle, but the\nactual delay needed depends on the clock speed and the memory's \nspecification.  At some point after new data have been\ndelivered to the memory, the write operation completes within the\nmemory.  The time from the application of the address until the\n(worst-case) completion of the write operation is called the {\nwrite cycle} of the memory, and is also specified by the memory \nmanufacturer.  Once the write cycle has passed, the controlling logic \nlowers WE, waits for the change to settle within the memory,\nthen removes the address and lowers the chip select signal.  The\nreason for the delay between these signal changes is the same: to \navoid mistakenly overwriting another memory location.\n\nA read operation is quite similar.  As shown on the right, the\ncontrolling logic places the address on the input lines and raises the\nchip select signal.  No races need be considered, as read operations\non SRAM do not affect the stored data.  After a delay called the {\nread cycle}, the data can be read from the data outputs.  The address\ncan then be removed and the chip select signal lowered.\n\nFor both reads and writes, the number of cycles required for an\noperation depends on a combination of the clock cycle of the\ncontroller and the cycle time of the memory.  For example, with a\n25 nanosecond write cycle and a 10 nanosecond clock cycle, a write\nrequires three cycles.  In general, the number of cycles required is\ngiven by the formula {memory cycle time}/{clock cycle\ntime}.\n\n\n\n \n Question: What is the name of the type of memory used for high speed applications? \n Answer: \n\nA. SRAM is a type of static random access memory."}, {"text": "Title: Tri-State Buffers and Combining Chips \n Text: {Tri-State Buffers and Combining Chips}\n\nRecall the buffer symbol---a triangle like an inverter, but with no\ninversion bubble---between the mux and the {DATA-OUT} \nsignal of the {2^6b} memory shown earlier.  This \n{ tri-state buffer} serves to disconnect the memory logic \nfrom the output line when the memory is not performing a read. \n\n\nAn implementation diagram for a tri-state buffer appears to the right \nalong with the symbolic\nform and a truth table.  The ``Z'' in the truth table output means \nhigh impedance (and is sometimes written ``hi-Z'').  In other words,\nthere is effectively no electrical connection between the tri-state \nbuffer and the output OUT.\n\nThis logical disconnection is achieved by using the outer\n\n\n{file=part3/figs/tri-state.eps,width=3in}\n\n\n{cc|c}\nEN& IN& OUT \n0& x& Z\n1& 0& 0\n1& 1& 1\n\n\n\n(upper and lower)\npair of transistors in the logic diagram.  When EN=0, both transistors\nturn off, meaning that regardless of the value of IN, OUT is connected\nneither to high voltage nor to ground.\n\nWhen EN=1, both transistors turn on, and the tri-state buffer acts as\na pair of back-to-back inverters, copying the signal from IN to OUT,\nas shown in the truth table.\n\nWhat benefit does this logical disconnection provide?\n\nSo long as only one memory's chip select input is high at any time,\nthe same output line can be shared by more than one memory\nwithout the need for additional multiplexers.\n\nMemory chips were often combined in this way to produce larger memories.\n\n\nThe figure to the right illustrates how larger memories can be constructed\nusing multiple chips.  In the case shown, two {2^-bit} \nmemories are used to implement a {2^{k+1}-bit} memory.\nOne of the address bits---in the case shown, the most significant bit---is\nused to drive a decoder that determines which of the two chips is \nactive (CS=1).  The decoder is enabled with the chip select signal for\nthe larger memory, so neither chip is enabled when the external CS is\nlow, as desired.  The\n\n\n{file=part3/figs/mem-larger.eps,width=4.75in}\n\n\nrest of the address bits, as well as the external\ndata inputs and write enable signal, are simply delivered to both memories.\nThe external data outputs are also connected to both memories.  \nEnsuring that at most one chip select signal is high at any time\nguarantees that at most one of the two memory chips drives logic values\non the data outputs.\n\n\nMultiple chips can also be used to construct wider memories, as shown to\nthe right.  In the case shown, two {2^-bit} \nmemories are used to implement a {2^-bit} memory.\nBoth chips are either active or inactive at the same time, so the external \naddress, write enable, and chip select inputs are routed to both chips.\nIn contrast, the data inputs and outputs are separate: the left chip\nhandles the high N bits of input on writes and produces the high N\nbits of output on reads, while the right chip handles the low N bits of \ninput and produces the low N bits of output.\n\n\n{file=part3/figs/mem-wider.eps,width=4.05in}\n\n\nHistorically, tri-state buffers were also used to reduce the number of\npins needed on chips.  Pins have long been a scarce resource, and the \namount of data that can cross a chip's pins in a second (the product of the\nnumber of pins and the data rate per pin) has not grown nearly as rapidly \nas the number of transistors packed into a fixed area.\n\nBy combining inputs and outputs, chip designers were able to halve the\nnumber of pins needed.  For example, data inputs and outputs of memory\nwere often combined into a single set of data wires, with bidirectional\nsignals.  When performing a read from a memory chip, the memory chip\ndrove the data pins with the bits being read (tri-state buffers on the\nmemory chip were enabled).  When performing a write, other logic such as \na processor wrote the value to be stored onto the data pins (tri-state \nbuffers were not enabled).\n\n\n \n Question: What is the effect of a tri-state buffer on a circuit? \n Answer: \n\nA. A tri-state buffer is a device that can be used to disconnect a circuit from an output line. The buffer has three states: high, low, and high impedance. When the buffer is in the high impedance state, the circuit is effectively disconnected from the output line."}, {"text": "Title: Dynamic Random Access Memory* \n Text: {Dynamic Random Access Memory*}\n\nDynamic random access memory, or DRAM, is used for main memory in\ncomputers and for other applications in which size is more important\nthan speed.  While slower than SRAM, DRAM is denser (has\nmore bits per chip area).  A substantial part of DRAM density is\ndue to transistor count: typical SRAM cells use six transistors\n(two for each inverter, and two more to connect the inverters to the \nbit lines), while DRAM cells use only a single transistor.\nHowever, memory designers have also made significant advances in\nfurther miniaturizing DRAM cells to improve density beyond the \nbenefit available from simple transistor count.\n\n\nA diagram of a DRAM cell appears to the right.  \nDRAM storage is capacitive: a bit is stored by charging or not charging \na capacitor.  The capacitor is attached to a BIT line \nthrough a transistor controlled by a SELECT line.  \n\nWhen SELECT is low, the capacitor is isolated and \nholds its charge.  However, the transistor's resistance is\nfinite, and some charge leaks out onto the bit line.  Charge also\nleaks into the substrate on which the transistor is constructed.  After\nsome amount of time, all of the charge dissipates, and the bit is\nlost.  To avoid such loss, the cell must be { refreshed}\nperiodically by reading the contents and writing them back with active\nlogic.\n\n\n{file=part3/figs/lec18-8.eps,width=1.1in}\n\n\nWhen the SELECT line is high during a write operation, logic driving\nthe bit line forces charge onto the capacitor or removes all charge\nfrom it.  For a read operation, the bit line is first brought to an\nintermediate voltage level (a voltage level between 0 and 1), then\nSELECT is raised, allowing the capacitor to either pull a small\namount of charge from the bit line or to push a small amount of charge\nonto the bit line.  The resulting change in voltage is then detected\nby a { sense amplifier} at the end of the bit line.  A sense amp \nis analogous to a marble on a mountaintop: a small push causes the\nmarble to roll rapidly downhill in the direction of the push.\nSimilarly, a small change in voltage causes a sense amp's output to\nmove rapidly to a logical 0 or 1, depending on the direction of the\nsmall change.  As mentioned earlier, sense amplifiers also appear in \nSRAM implementations.\nWhile not technically necessary, as they are with DRAM, the use of a\nsense amp to react to small changes in voltage makes reads faster.\n\nEach read operation on a DRAM cell brings the voltage on its capacitor\ncloser to the intermediate voltage level, in effect destroying the\ndata in the cell.  DRAM is thus said to have { destructive reads}.\nTo preserve data during a read, the bits must be written back\ninto the cells after a read.  For example, the output of the sense \namplifiers can\nbe used to drive the bit lines, rewriting the cells with the\nappropriate data.\n\nAt the chip level, typical DRAM inputs and outputs differ from those\nof SRAM.  \n\nDue to the large size and high density of DRAM,\naddresses are split into row and column components and provided\nthrough a common set of pins.  The DRAM stores the components in\nregisters to support this approach.  Additional inputs, known as the\n{ row} and { column address} {{ strobes}---RAS} and\nCAS, {respectively---are} used to indicate when address\ncomponents are available.  As\nyou might guess from the structure of coincident selection, DRAM\nrefresh occurs on a row-by-row basis (across bit slices---on columns\nrather than rows in the figures earlier in these notes, but the terminology\nof DRAM is a row).  Raising the SELECT line for a\nrow destructively reads the contents of all cells on that row, forcing\nthe cells to be rewritten and effecting a refresh.  The row is thus a\nnatural basis for the refresh cycle.  The DRAM data pins provide\nbidirectional signals for reading and writing elements of the DRAM.\nAn { output enable} input, OE, controls tri-state buffers with\nthe DRAM to determine whether or not the DRAM drives the data pins.\nThe WE input, which controls the type of operation, is\nalso present.\n\n\nTiming diagrams for writes and reads on a historical DRAM implementation\nappear to the right.  In both cases, the row component of the address is \nfirst applied to the address pins, then RAS is raised.  In the\nnext cycle of the controlling logic, the column component is applied\nto the address pins, and CAS is raised.  \n\nFor a write, as shown on the left, the WE signal and the\ndata can\n\n\n{file=part3/figs/lec18-9.eps,width=4in}\n\n\nalso be applied in the second cycle.  The DRAM has internal\ntiming and control logic that prevent races from overwriting an\nincorrect element (remember that the row and column addresses have to\nbe stored in registers).  The DRAM again specifies a write cycle,\nafter which the operation is guaranteed to be complete.  In order, the\nWE, CAS, and RAS signals are then lowered.  \n\nFor a read operation, the output enable signal, OE, is raised after\nCAS is raised.  The DATA pins, which should be floating (in other\nwords, not driven by any logic), are then driven by the DRAM.  After the \nread cycle, valid data appear on the DATA pins, and OE, CAS, and\nRAS are lowered in order after the data are read.\n\nModern DRAM chips are substantially more sophisticated than those\ndiscussed here, and many of the functions that used to be provided\nby external logic are now integrated onto the chips themselves.\n\nAs an example of modern DRAMs, one can obtain\nthe data sheet for Micron Semiconductor's 8Gb ({2^b},\nfor example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.\n\nThe ability to synchronize to an external clock has become prevalent in \nthe industry, leading to the somewhat confusing term SDRAM, which stands \nfor { synchronous DRAM}.  The memory structures themselves\nare still unclocked, but logic is provided on the chip to synchronize \naccesses to the external clock without the need for additional \nlogic.\n\nThe clock provided to the Micron chip just mentioned\ncan be as fast as 1.6 GHz, and data can be\ntransferred on both the rising and falling edges of the clock\n(hence the name DDR, or { double data rate}).  \n\nIn addition to row and\ncolumn components of the address, these chips further separate cells into\n{ banks} and groups of banks.  These allow a user to exploit parallelism\nby starting reads or writes to separate banks at the same time, thus\nimproving the speed at which data can move in and out of the memory.\n\nFor the {2^b} version of the Micron chip,\nthe cells are structured into 4 groups of 4 banks (16 banks total),\neach with 131,072 rows and 1,024 columns.  \n\nDRAM implementations provide interfaces for specifying\nrefresh operations in addition to reads and writes.\nManaging refresh timing and execution is\ngenerally left to an external DRAM controller.\n\nFor the Micron chip, refresh commands must be issued every \n7.8 microseconds at normal temperatures.  Each\ncommand refreshes about 2^ cells, so 8,192 commands refresh\nthe whole chip in less than 64 milliseconds.\n\nAlternatively, the chip can handle refresh on-chip in\norder to maintain memory contents when the rest of the system is \npowered down.\n\n\n\n \n Question: What is the main difference between DRAM and SRAM? \n Answer: \n\nA. DRAM is denser (has more bits per chip area) than SRAM."}, {"text": "Title: Design of the Finite State Machine for the Lab \n Text: {Design of the Finite State Machine for the Lab}\n\nThis set of notes explains the process that Prof. Doug Jones used to develop\nthe FSM for the lab.\n\nThe lab simulates a vending machine mechanism for automatically \nidentifying coins (dimes and quarters only), tracking the amount \nof money entered by the user, accepting or rejecting \ncoins, and emitting a signal when a total of 35 cents has been \naccepted.  In the lab, we will only drive a light with \nthe ``paid in full'' signal.  \n\nSorry, neither candy nor Dew will be distributed!\n\nProf. Doug Jones designed the vending machine application and the FSM,\nwhile Prof. Chris Schmitz prototyped and constructed the physical elements \nwith some help from the ECE shop.\n\nProf. Volodymyr Kindratenko together with Prof. Geoffrey Herman created \nboth the wiki documentation and the Altera Quartus portions of the lab\n(the latter were based on earlier Mentor Graphics work by Prof. Herman).\n\nProf. Kindratenko also helped to scale the design \nin a way that made it possible to deliver to the over 400 students entering\nECE every semester.  \n\nProf. Juan Jos'e Jaramillo later identified\ncommon failure modes, including variability caused by sunshine through \nthe windows in ECEB,{No wonder people say that engineers hate \nsunlight!} and made some changes to improve robustness.  He also\ncreated the PowerPoint slides that are typically used to describe the lab in\nlecture.  Casey Smith, head guru of the ECE Instructional Labs,\ndeveloped a new debounce design and made some other hardware \nimprovements to reduce the rate of student headaches.\nFinally, Prof. Kirill Levchenko together with UA Saidivya Ashok\nstruck a blow against COVID-19 by developing an inexpensive and\nportable replacement for the physical ``vending machine'' systems\nused for testing in previous semesters.\n\n \n Question: What is the FSM? \n Answer: \n\nA. The vending machine uses a FSM to automatically identify coins, track the amount of money entered by the user, accept or reject coins, and emit a signal when a total of 35 cents has been accepted."}, {"text": "Title: Physical Design, Sensors, and Timing \n Text: {Physical Design, Sensors, and Timing}\n\nA user inserts a coin into a slot at one end of the device.  The coin\nthen rolls down a slope towards a gate controlled by a servo.  The gate\ncan be raised or lowered, and determines whether the coin exits from the\nother side or the bottom of the device.\n\nAs the coin rolls, it passes two optical sensors.{The full system\nactually allows four sensors to differentiate four types of coins, but\nour lab uses only two of these sensors.}  One of these sensors is \npositioned high enough above the slope that a dime passes beneath the\nsesnor, allowing the signal T produced by the sensor to tell us whether \nthe coin is a dime or a quarter.  The second sensor is positioned so\nthat all coins pass in front of it.  The sensor positions are chosen \ncarefully to ensure that, in the case of a quarter, the coin is still\nblocking the first sensor when it reaches the second sensor.  \nBlocked sensors give a signal of 1 in this design, so the rising edge \nthe signal from the second sensor can be used as a ``clock'' for our \nFSM.  When the rising edge occurs, the signal T from the first sensor \nindicates whether the coin is a quarter (T=1) or a dime (T=0).\n\n\nA sample timing diagram for the lab appears to the right.  The clock\nsignal generated by the lab is not only not a square wave---in other words,\nthe high and low portions are not equal---but is also unlikely to be periodic.\nInstead, the ``cycle'' is defined by the time between coin insertions.\nThe T signal serves as the single input to our FSM.  In the timing\n\n\n{file=part3/figs/lab-timing.eps,width=2.55in}\n\n\ndiagram, T is shown as rising and falling before the clock edge.\nWe use positive edge-triggered flip-flops to implement our FSM,\nthus the aspect of the relative timing that matters to our design\nis that, when the clock rises, the value of T is stable and indicates \nthe type of coin entered.  The signal T may fall before or after\nthe clock does---the two are equivalent for our FSM's needs.\n\nThe signal A in the timing diagram is an output from the FSM, and\nindicates whether or not the coin should be accepted.  This signal \ncontrols the servo that drives the gate, and thus determines whether\nthe coin is accepted (A=1) as payment or rejected (A=0) and returned\nto the user.  \n\nLooking at the timing diagram, you should note that our FSM makes \na decision based on its current state and the input T and enters a \nnew state at the rising clock edge.  The value of A in the next cycle\nthus determines the position of the gate when the coin eventually\nrolls to the end of the slope.\n\nAs we said earlier, our FSM is thus a Moore machine: the output A\ndoes not depend on the input T, but only on the current internal \nstate bits of the the FSM.\n\nHowever, you should also now realize that making A depend on T\nis not adequate for this lab.  If A were to rise with T and\nfall with the rising clock edge (on entry to the next state), or\neven fall with the falling edge of T, the gate would return to the\nreject position by the time the coin reached the gate, regardless\nof our FSM's decision!\n\n \n Question: What is the signal that determines whether or not a coin should be accepted? \n Answer: \n\nA. The signal A in the timing diagram serves as an output from the FSM, and\nindicates whether or not the coin should be accepted."}, {"text": "Title: An Abstract Model\\vspace4pt \n Text: {An Abstract Model}\n\n\nWe start by writing down states for a user's expected behavior.\nGiven the fairly tight constraints that we have placed on our lab,\nfew combinations are pos-\n\n\n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nSTART& DIME& QUARTER& & no\nDIME& & PAID& yes& no\nQUARTER& PAID& & yes& no\nPAID& & & yes& yes\n\n\n\nsible.  For a total of 35 cents, a user should either insert a dime \nfollowed by a quarter, or a quarter followed by a dime.\n\nWe begin in a START state, which transitions to states DIME or QUARTER\nwhen the user inserts the first coin.  With no previous coin, we need not\nspecify a value for A.  No money has been deposited, so we set \noutput P=0 in the START state.\n\nWe next create DIME and QUARTER states corresponding to the user having\nentered one coin.  The first coin should be accepted, but more money is\nneeded, so both of these states output A=1 and P=0.\nWhen a coin of the opposite type is entered, each state moves to a\nstate called PAID, which we use for the case in which a total of 35 cents has\nbeen received.  For now, we ignore the possibility that the same type\nof coin is deposited more than once.  Finally, the PAID state accepts\nthe second coin (A=1) and indicates that the user has paid the full\nprice of 35 cents (P=1).\n\n\nWe next extend our design to handle user mistakes.  If a user enters\na second dime in the DIME state, our FSM should reject the coin.  We\ncreate a REJECTD state and add it as the next state from\n\n\n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nSTART& DIME& QUARTER& & no\nDIME& REJECTD& PAID& yes& no\nREJECTD& REJECTD& PAID& no& no\nQUARTER& PAID& REJECTQ& yes& no\nREJECTQ& PAID& REJECTQ& no& no\nPAID& & & yes& yes\n\n\n\nDIME when a dime is entered.\nThe REJECTD state rejects the dime (A=0) and\ncontinues to wait for a quarter (P=0).  What should we use as next \nstates from REJECTD?  If the user enters a third dime (or a fourth, \nor a fifth, and so on), we want to reject the new dime as well.  \nIf the user enters a quarter, we want to accept the coin, at which point\nwe have received 35 cents (counting the first dime).  We use\nthis reasoning to complete the description of REJECTD.  We also create\nan analogous state, REJECTQ, to handle a user who inserts more than\none quarter.\n\nWhat should happen after a user has paid 35 cents and bought \none item?  The FSM at that point is in the PAID state, which delivers\nthe item by setting P=1.\n\nGiven that we want the FSM to allow the user to purchase another item, \nhow should we choose the next states from PAID?\n\nThe behavior that we want from PAID is identical to the behavior that\nwe defined from START.  The 35 cents already \ndeposited was used to pay for the item delivered, so the machine is\nno longer holding any of the user's money.\n\nWe can thus simply set the next states from PAID to be DIME when a \ndime is inserted and QUARTER when a quarter is inserted.\n\n\n\n\nAt this point, we make a decision intended primarily to simplify the\nlogic needed to build the lab.  Without a physical item delivery \nmechanism with a specification for how its in-\n\n\n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nPAID& DIME& QUARTER& yes& yes\nDIME& REJECTD& PAID& yes& no\nREJECTD& REJECTD& PAID& no& no\nQUARTER& PAID& REJECTQ& yes& no\nREJECTQ& PAID& REJECTQ& no& no\n\n\n\nput must be driven, \nthe behavior of the output signal P can be fairly flexible.  \nFor example, we could build a delivery mechanism that used the rising\nedge of P to open a chute.  In this case, the output P=0 in the\nstart state is not relevant, and we can merge the state START with\nthe state PAID.  The way that we handle P in the lab, we might\nfind it strange to have a ``paid'' light turn on before inserting any\nmoney, but keeping the design simple enough for a first lab exercise \nis more important.  Our final abstract state table appears above.\n\n \n Question: What is the purpose of the state that we created? \n Answer: "}, {"text": "Title: Picking the Representation \n Text: {Picking the Representation}\n\nWe are now ready to choose the state representation for the lab FSM.\n\nWith five states, we need three bits of internal state.\n\nProf. Jones decided to leverage human meaning in assigning the\nbit patterns, as follows:\n\n{\n\nS_2& type of last coin inserted (0 for dime, 1 for quarter)\nS_1& more than one quarter inserted? (1 for yes, 0 for no)\nS_0& more than one dime inserted? (1 for yes, 0 for no)\n\n}\n\n\nThese meanings are not easy to apply to all of our states.  For example,\nin the PAID state, the last coin inserted may have been of either type,\nor of no type at all, since we decided to start our FSM in that state as \nwell.  However, for the other four states, the meanings provide a clear\nand unique set of bit pattern assignments, as shown to the right.  We\ncan choose any of the remaining four bit patterns (010, 011, 101, or 111)\nfor the PAID state.  In fact, { we can choose all of the remaining\npatterns} for the PAID state.  We can always represent any state\n\n\n\nstate& S_2S_1S_0  \nPAID& ???\nDIME& 000\nREJECTD& 001\nQUARTER& 100\nREJECTQ& 110\n\n\n\nwith more\nthan one pattern if we have spare patterns available.  Prof. Jones\nused this freedom to simplify the logic design.\n\nThis particular example is slightly tricky.  The four free patterns do\nnot share any single bit in common, so we cannot simply insert x's\ninto all {K-map} entries for which the next state is PAID.\nFor example, if we insert an x into the {K-map} for  S_2^+,\nand then choose a function for S_2^+ that produces a value of 1\nin place of the don't care, we must also produce a 1 in\nthe corresponding entry of the {K-map} for S_0^+.  Our options\nfor PAID include 101 and 111, but not 100 nor 110.  These latter\ntwo states have other meanings.\n\n\nLet's begin by writing a next-state table consisting mostly of bits,\nas shown to the right.  We use this table to write out a {K-map}\nfor S_2^+ as follows: any of the patterns that may be used for the\nPAID state obey the next-state rules for PAID.  Any next-state marked\nas PAID is marked as don't care in the {K-map},\n\n\n{cc|cc}\n&&{|c}{S_2^+S_1^+S_0^+}\nstate& S_2S_1S_0& T=0& T=1 \nPAID& PAID& 000& 100\nDIME& 000& 001& PAID\nREJECTD& 001& 001& PAID\nQUARTER& 100& PAID& 110\nREJECTQ& 110& PAID& 110\n\n\n\n\n\n\nsince we can\nchoose patterns starting with either or both values to represent our\nPAID state.  The resulting {K-map} appears to the far right.\nAs shown, we simply set S_2^+=T, which matches our\noriginal ``meaning'' for S_2.  That is, S_2 is the type of the\nlast coin inserted.\n\n\nBased on our choice for S_2^+, we can rewrite the {K-map} as\nshown to the right, with green italics and shading marking the\nvalues produced for the x's in the specification.  Each of these\nboxes corresponds to one transition into the PAID state.  By \nspecifying the S_2 value, we cut the number of possible choices\nfrom four to two in each case.  For those combinations in which the\nimplementation produces S_2^+=0, we must choose S_1^+=1, but\nare still free to leave S_0^+ marked as a don't care.  Similarly,\nfor those combinations in which the implementation produces S_2^+=1, \nwe must choose S_0^+=1, but\nare still free to leave S_1^+ marked as a don't care.\n\n\n\n\n\n\nThe {K-maps} for S_1^+ and S_0^+ are shown to the right.\nWe have not given algebraic expressions for either, but have indicated\nour choices by highlighting the resulting replacements of don't care\nentries with the values produced by our expressions.\n\nAt this point, we can review the state patterns actually produced by\neach of the four next-state transitions into the PAID state.  From\nthe DIME state, we move into the 101 state when the user inserts a\n\n\n\n\n\n\n\n\nquarter.  The result is the same from the REJECTD state.  From the\nQUARTER state, however, we move into the 010 state when the user \ninserts a dime.  The result is the same from the REJECTQ state.  We\nmust thus classify both patterns, 101 and 010, as PAID states.  The\nremaining two patterns, 011 and 111, cannot\n\n\nbe reached from any of\nthe states in our design.  We might then try to leverage the fact\nthat the next-state patterns from these two states are not relevant \n(recall that we fixed the next-state patterns for all four of the \npossible PAID states) to further simplify our logic, but doing so \ndoes not provide any advantage (you may want to check our claim).\n\nThe final state table is shown to the right.  We have included the\nextra states at the bottom of the table.  We have specified the\nnext-state logic for these\n\n\n{cc|cc|cc}\n&&{|c|}{S_2^+S_1^+S_0^+}\nstate& S_2S_1S_0& T=0& T=1& A& P \nPAID1& 010& 000& 100& 1& 1\nPAID2& 101& 000& 100& 1& 1\nDIME& 000& 001& 101& 1& 0\nREJECTD& 001& 001& 101& 0& 0\nQUARTER& 100& 010& 110& 1& 0\nREJECTQ& 110& 010& 110& 0& 0 \nEXTRA1& 011& 000& 100& x& x\nEXTRA2& 111& 000& 100& x& x\n\n\n\nstates, but left the output bits as don't\ncares.  A state transition diagram appears at the bottom of this page.\n\n \n Question: What is the state of the FSM? \n Answer: \n\nThe bit pattern S_2S_1S_0 represents the internal state of the FSM."}, {"text": "Title: Testing the Design \n Text: {Testing the Design}\n\nHaving a complete design on paper is a good step forward, but humans\nmake mistakes at all stages.  How can we know that a circuit that\nwe build in the lab correctly implements the FSM that we have outlined \nin these notes?\n\nFor the lab design, we have two problems to solve.\n\nFirst, we have not specified an initialization scheme for the FSM.\nWe may want the FSM to start in one of the PAID states, but adding\ninitialization logic to the design may mean requiring you to wire together\nsignificantly more chips.  Second, we need a sequence of inputs that\nmanages to test that all of the next-state and output logic implementations\nare correct.\n\nTesting sequential logic, including FSMs, is in general extremely difficult.\nIn fact, large sequential systems today are generally converted into \ncombinational logic by using shift registers to fill the \nflip-flops with a particular pattern, \nexecuting the logic for one clock cycle, and checking that the resulting \npattern of bits in the flip-flops is correct.  This approach is called \n{ scan-based testing}, and is discussed in ECE 543.  You \nwill make use of a similar approach\nwhen you test your combinational logic in the second week of the lab,\nbefore wiring up the flip-flops.\n\nWe have designed our FSM to be easy to test (even small FSMs\nmay be challenging) with a brute force approach.  In particular, we \nidentify two input sequences that together serve both to initialize and \nto test a correctly implemented variant of our FSM.  Our initialization\nsequence forces the FSM into a specific state regardless of its initial\nstate.  And our test sequence crosses every transition arc leaving the\nsix valid states.\n\n\n\nIn terms of T, the coin type, we initialize the FSM with the\ninput sequence 001.  Notice that such a sequence takes any initial \nstate into PAID2.\n\nFor testing, we use the input sequence 111010010001.  You should trace \nthis sequence, starting from PAID2, on the diagram below to see how the\ntest sequence covers all of the possible arcs.  As we test, we need also\nto observe the A and P outputs in each state to check the output\nlogic.\n\n{{file=part3/figs/lab-diag-notes.eps,width=4.25in}}\n\n\n\n \n Question: What is the first step in testing the circuit? \n Answer: "}, {"text": "Title: Finite State Machine Design Examples, Part II \n Text: {Finite State Machine Design Examples, Part II}\n\nThis set of notes provides several additional examples of FSM design.\nWe first design an FSM to control a vending machine, introducing\nencoders and decoders as components that help us to implement our\ndesign.  We then design a game controller for a logic puzzle\nimplemented as a children's game.  Finally, we analyze a digital FSM\ndesigned to control the stoplights at the intersection of two roads.\n\n\n \n Question: What is an encoder? \n Answer: \n\nA. An encoder is a component that helps to implement an FSM design. It converts input signals into a code that can be read by a machine."}, {"text": "Title: Design of a Vending Machine \n Text: {Design of a Vending Machine}\n\nFor the next example, we design an FSM to control a simple vending machine.  \nThe machine accepts {U.S. coins}{Most countries have small \nbills or coins in demoninations suitable for vending machine prices, so think \nabout some other currency if you prefer.} as payment and offers a choice\nof three items for sale.\n\nWhat states does such an FSM need?  The FSM needs to keep track of how\nmuch money has been inserted in order to decide whether a user can \npurchase one of the items.  That information alone is enough for the\nsimplest machine, but let's create a machine with adjustable item\nprices.  We can use registers to hold the item prices, which \nwe denote P_1, P_2, and P_3.\n\nTechnically, the item prices are also part of the internal state of the \nFSM.  However,  we leave out discussion (and, indeed, methods) for setting\nthe item prices, so no state with a given combination of prices has any \ntransition to a state with a different set of item prices.\nIn other words, any given combination of item prices induces a subset \nof states that operate independently of the subset induced by a distinct \ncombination of item prices.  By abstracting away the prices in this way,\nwe can focus on a general design that allows the owner of the machine\nto set the prices dynamically.\n\n\nOur machine will not accept pennies, so let's have the FSM keep track of\nhow much money has been inserted as a multiple of 5 cents (one nickel).\nThe table to the right shows five types of coins, their value in \ndollars, and their value in terms of nickels.\n\nThe most expensive item in the machine might cost a dollar or two, so\nthe FSM must track at least 20 or 40 nickels of value.  Let's\n\n\n{l|c|c}\n{c|}{coin type}& value& # of nickels \nnickel&      0.05& 1\ndime&        0.10& 2\nquarter&     0.25& 5\nhalf dollar& 0.50& 10\ndollar&      1.00& 20\n\n\n\ndecide to\nuse six bits to record the number of nickels, which allows the machine\nto keep track of up to 3.15 (63 nickels).  We call the abstract\nstates { STATE00} through { STATE63}, and refer to a state with\nan inserted value of N nickels as { STATE{<}N{>}}.\n\nLet's now create a next-state table, as shown at the top of the next page.\nThe user can insert one of the five coin types, or can pick one of the \nthree items.  What should happen if the user inserts more money than the \nFSM can track?  Let's make the FSM reject such coins.  Similarly, if the \nuser tries to buy an item without inserting enough money first, the FSM \nmust reject the request.  For each of the possible input events, we add a \ncondition to separate the FSM states that allow the input event to \nbe processed as the user desires from those states that do not.  For example,\nif the user inserts a quarter, those states with N<59 transition to\nstates with value N+5 and accept the quarter.  Those states with\nN reject the coin and remain in { STATE{<}N{>}}.\n\n\n{c|l|c|l|c|c}\n&&& {|c}{final state}\n&&& {|c}{}& & release \ninitial state& {|c|}{input event}& condition& {|c}& & product \n{ STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none\n{ STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none\n{ STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none\n{ STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none\n{ STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none\n{ STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1\n{ STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2\n{ STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3\n{ STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none\n\n\n\nWe can now begin to formalize the I/O for our machine.  Inputs include \ninsertion of coins and selection of items for purchase.  Outputs include\na signal to accept or reject an inserted coin as well as signals to release\neach of the three items.\n\n\nFor input to the FSM, we assume that a coin inserted in any given cycle \nis classified and delivered to our FSM using the three-bit representation \nshown to the right.\n\nFor item selection, we assume that the user has access to three buttons,\nB_1, B_2, and B_3, that indicate a desire to purchase the \ncorresponding item.\n\nFor output, the FSM must produce a signal A indicating whether a coin\nshould be accepted.  To control the release of items that have been purchased,\nthe FSM must produce the signals R_1, R_2, and R_3, corresponding\nto the re-\n\n\n{l|c}\n{c|}{coin type}& C_2C_1C_0 \nnone&        110\nnickel&      010\ndime&        000\nquarter&     011\nhalf dollar& 001\ndollar&      111\n\n\n\nlease of each item.  Since outputs in our class depend only on\nstate, we extend the internal state of the FSM to include bits for each of\nthese output signals.  The output signals go high in the cycle after\nthe inputs that generate them.  Thus, for example, the accept signal A\ncorresponds to a coin inserted in the previous cycle, even if a second\ncoin is inserted in the current cycle.  This meaning must be made clear to\nwhomever builds the mechanical system to return coins.\n\nNow we are ready to complete the specification.  How many states does the\nFSM have?  With six bits to record money inserted and four bits to \ndrive output signals, we have a total of 1,024 (2^) states!\nSix different coin inputs are possible, and the selection buttons allow\neight possible combinations, giving 48 transitions from each state.\nFortunately, we can use the meaning of the bits to greatly simplify\nour analysis.\n\nFirst, note that the current state of the coin accept bit and item\nrelease bits---the four bits of FSM state that control the outputs---have\nno effect on the next state of the FSM.  Thus, we can consider only the\ncurrent amount of money in a given state when thinking about the \ntransitions from the state.  As you have seen, we can further abstract\nthe states using the number N, the number of nickels currently held by \nthe vending machine.\n\nWe must still consider all 48 possible transitions from { STATE{<}N{>}}.\nLooking back at our abstract next-state table, notice that we had only\neight types of input events (not counting ``no input'').  If we\nstrictly prioritize these eight possible events, we can safely ignore\ncombinations.  Recall that we adopted a similar strategy for several \nearlier designs, including the ice cream dispenser in Notes Set 2.2 and\nthe keyless entry system developed in Notes Set 3.1.3.\n\nWe choose to prioritize purchases over new coin insertions, and to \nprioritize item 3 over item 2 over item 1.  These prioritizations\nare strict in the sense that if the user presses B_3, both other\nbuttons are ignored, and any coin inserted is rejected, regardless of\nwhether or not the user can actually purchase item 3 (the machine\nmay not contain enough money to cover the item price).\n\nWith the choice of strict prioritization, all transitions from all\nstates become well-defined.  We apply the transition rules in order of\ndecreasing priority,\nwith conditions, and with {don't-cares} for lower-priority inputs. \nFor example, for any of the 16 { STATE50}'s (remember that the four\ncurrent output bits do not affect transitions), the table below lists all\npossible transitions assuming that P_3=60, P_2=10, and P_1=35.\n\n{\n{ccccc|ccccc}\n&&&&& {|c}{next state}\ninitial state& B_3& B_2& B_1& C_2C_1C_0& state& A& R_3& R_2& R_1 \n{ STATE50}& 1&x&x& xxx& { STATE50}& 0& 0&0&0\n{ STATE50}& 0&1&x& xxx& { STATE40}& 0& 0&1&0\n{ STATE50}& 0&0&1& xxx& { STATE15}& 0& 0&0&1\n{ STATE50}& 0&0&0& 010& { STATE51}& 1& 0&0&0\n{ STATE50}& 0&0&0& 000& { STATE52}& 1& 0&0&0\n{ STATE50}& 0&0&0& 011& { STATE55}& 1& 0&0&0\n{ STATE50}& 0&0&0& 001& { STATE60}& 1& 0&0&0\n{ STATE50}& 0&0&0& 111& { STATE50}& 0& 0&0&0\n{ STATE50}& 0&0&0& 110& { STATE50}& 0& 0&0&0\n\n}\nNext, we need to choose a state representation.  But this task is \nessentially done: each output bit (A, R_1, R_2, and R_3) is\nrepresented with one bit in the internal representation, and the\nremaining six bits record the number of nickels held by the vending\nmachine using an unsigned representation.\n\nThe choice of a numeric representation for the money held is important,\nas it allows us to use an adder to compute the money held in\nthe next state.\n\n \n Question: What is the output of the vending machine? \n Answer: \n\nA. The output of the vending machine would be \"1\" for the release of item 2, and \"0\" for everything else."}, {"text": "Title: Encoders and Decoders \n Text: {Encoders and Decoders}\n\nSince we chose to prioritize purchases, let's begin by building logic\nto perform state transitions for purchases.  Our first task is to\nimplement prioritization among the three selection buttons.  For this\npurpose, we construct a {4-input} { priority encoder}, which \ngenerates a signal P whenever any of its four input lines is active\nand encodes the index of the highest active input as a two-bit unsigned\nnumber S.   A truth table for our priority encoder appears on the \nleft below, with {K-maps} for each of the output bits on the right.\n\n\n{cccc|cc}\nB_3& B_2& B_1& B_0& P& S \n1&x&x&x& 1& 11\n0&1&x&x& 1& 10\n0&0&1&x& 1& 01\n0&0&0&1& 1& 00\n0&0&0&0& 0& xx\n\n\n\n\n\n\n\n\n\n\nFrom the {K-maps}, we extract the following equations:\n{eqnarray*}\nP &=& B_3 + B_2 + B_1 + B_0\nS_1 &=& B_3 + B_2\nS_0 &=& B_3 + {B_2}B_1\n{eqnarray*}\nwhich allow us to implement our encoder as shown to the right.\n\nIf we connect our buttons B_1, B_2, and B_3 to the priority \nencoder (and feed 0 into the fourth input), it produces a signal P \nindicating that the user is trying to make a purchase and a two-bit\nsignal S indicating which item the user wants.\n\n\n\n\n\nWe also need to build logic to control the item release outputs R_1, R_2,\nand R_3.  An item should be released only when it has been selected \n(as indicated by the priority encoder signal S) and the vending machine\nhas enough money.  For now, let's leave aside calculation of the item \nrelease signal, which we call R, and focus on how we can produce the\ncorrect values of R_1, R_2, and R_3 from S and R.\n\nThe component to the right is a { decoder} with an enable input.  A \ndecoder takes an input signal---typically one coded as a binary number---and \nproduces one output for each possible value of the signal.  You may\nnotice the similarity with the structure of a mux: when the decoder\nis enabled (EN=1), each of the AND gates produces\n\n\n\n\n\none minterm of the input signal S.  In\nthe mux, each of the inputs is then included in\none minterm's AND gate, and the outputs of all AND gates are ORd together.\nIn the decoder, the AND gate outputs are the outputs of the decoder.\nThus, when enabled, the decoder produces exactly one 1 bit on its outputs.\nWhen not enabled (EN=0), the decoder produces all 0 bits.\n\nWe use a decoder to generate the release signals for the vending machine\nby connecting the signal S produced by the \npriority encoder to the decoder's S input and connecting the item\nrelease signal R to the decoder's EN input.  The outputs D_1,\nD_2, and D_3 then correspond to the individual item release \nsignals R_1, R_2, and R_3 for our vending machine.\n\n \n Question: What is the purpose of a priority encoder? \n Answer: \n\nA. The purpose of a priority encoder is to generate a signal P whenever any of its four input lines is active and encode the index of the highest active input as a two-bit unsigned number S."}, {"text": "Title: Vending Machine Implementation \n Text: {Vending Machine Implementation}\n\n\nWe are now ready to implement the FSM to handle purchases, as shown to the \nright.  The current number of nickels, N, is stored in a register in the\ncenter of the diagram.  Each cycle, N is fed into a {6-bit} adder,\nwhich subtracts the price of any purchase requested in that cycle. \n\nRecall that we chose to record item prices in registers.  We avoid the \nneed to negate prices before adding them by storing the negated prices in\nour registers.  Thus, the value of register PRICE1 is -P_1, the\nthe value of register PRICE2 is -P_2, and the\nthe value of register PRICE3 is -P_3.\n\nThe priority encoder's S signal is then used to select the value of \none of these three registers (using a {24-to-6} mux) as the second\ninput to the adder.\n\nWe use the adder to execute a subtraction, so the carry out C_ \nis 1 whenever the value of N is at least as great as the amount \nbeing subtracted.  In that case, the purchase is successful.  The AND\ngate on the left calculates the signal R indicating a successful purchase,\nwhich is then used to select the next value of N using the {12-to-6}\nmux below the adder.  \n\nWhen no item selection buttons are pushed, P and thus R are both 0, \nand the mux below the adder keeps N unchanged in the next cycle.  \nSimilarly, if P=1 but N is\ninsufficient, C_ and thus R are both 0, and again N does\nnot change.  Only when P=1 and C_=1 is the purchase successful,\nin which case the price is subtracted from N in the next cycle.\n\n\n\n\n\nThe signal R is also used to enable a decoder that generates the three\nindividual item release outputs.  The correct output is generated based on\nthe decoded S signal from the priority encoder, and all three output\nbits are latched into registers to release the purchased item in the next \ncycle.\n\nOne minor note on the design so far: by hardwiring C_ to 0, we created \na problem for items that cost nothing (0 nickels): in that case, C_ is\nalways 0.  We could instead store\n-P_1-1 in PRICE1 (and so forth) and feed P in to C_, but maybe\nit's better not to allow free items.\n\n\nHow can we support coin insertion?  Let's use the same adder to add\neach inserted coin's value to N.  The table at the right shows\nthe value of each coin as a {5-bit} unsigned number of nickels.\nUsing this table, we can fill in {K-maps} for each bit of V, as\nshown below.  Notice that we have marked the two undefined bit patterns\nfor the coin type C as don't cares in the {K-maps}.\n\n\n{l|c|c}\n{c|}{coin type}& C_2C_1C_0& V_4V_3V_2V_1V_0 \nnone&        110& 00000\nnickel&      010& 00001\ndime&        000& 00010\nquarter&     011& 00101\nhalf dollar& 001& 01010\ndollar&      111& 10100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the {K-maps} gives the following equations, which we\nimplement as shown to the right.\n\n{eqnarray*}\nV_4 &=& C_2C_0\nV_3 &=& {C_1}C_0\nV_2 &=& C_1C_0\nV_1 &=& {C_1}\nV_0 &=& {C_2}C_1\n{eqnarray*}\n\n\n\n\n\n\nNow we can extend the design to handle coin insertion, as shown to the right\nwith new elements highlighted in blue.  The output of the coin value \ncalculator is extended with a leading 0 and then fed into a {12-to-6} mux.\nWhen a purchase is requested, P=1 and the mux forwards the item price to \nthe adder---recall that we chose to give purchases priority over coin\ninsertion.  When no purchase is requested, the value of any coin inserted\n(or 0 when no coin is inserted) is passed to the adder.\n\nTwo new gates have been added on the lower left.  First, let's verify that\npurchases work as before.  When a purchase is requested, P=1, so the\nNOR gate outputs 0, and the OR gate simple forwards R to control the\nmux that decides whether the purchase was successful, just as in our\noriginal design.\n\nWhen no purchase is made (P=0, and R=0), the adder adds the value of \nany inserted coin to N.  If the addition overflows, C_=1, and\nthe output of the NOR gate is 0.  Note that the NOR gate output is stored\nas the output A in the next cycle, so a coin that causes overflow in the\namount of money stored is rejected.  The OR gate also outputs 0, and N\nremains unchanged.  If the addition does not overflow, the NOR gate\noutputs a 1, the coin is accepted (A=1 in the next cycle), and \nthe mux allows the sum N+V to be written back as the new value of N.\n\n\n\n\n\nThe tables at the top of the next page define all of the state variables,\ninputs, outputs, and internal signals used in the design, and list the\nnumber of bits for each variable.\n\n\n\n\n{|ccl|}\n{|r|}{{ FSM state}}\nPRICE1& 6& negated price of item 1 (-P_1)\nPRICE2& 6& negated price of item 2 (-P_2)\nPRICE3& 6& negated price of item 3 (-P_3)\nN& 6& value of money in machine (in nickels)\nA& 1& stored value of accept coin output\nR_1& 1& stored value of release item 1 output\nR_2& 1& stored value of release item 2 output\nR_3& 1& stored value of release item 3 output \n{|r|}{{ internal signals}}\nV& 5& inserted coin value in nickels\nP& 1& purchase requested (from priority encoder)\nS& 2& item # requested (from priority encoder)\nR& 1& release item (purchase approved) \n\n\n{|ccl|}\n{|r|}{{ inputs}}\nB_1& 1& item 1 selected for purchase\nB_2& 1& item 2 selected for purchase\nB_3& 1& item 3 selected for purchase\nC& 3& coin inserted (see earlier table \n& & for meaning) \n{|r|}{{ outputs}}\nA& 1& accept inserted coin (last cycle)\nR_1& 1& release item 1\nR_2& 1& release item 2\nR_3& 1& release item 3\n{|l|}{{ Note that outputs correspond one-to-one}}\n{|l|}{{ with four bits of FSM state.}} \n\n\n\n\n \n Question: What is the value of an inserted coin stored as? \n Answer: \n\nA. The vending machine uses the coin type input C and the coin value input V to determine the value of an inserted coin."}, {"text": "Title: Design of a Game Controller \n Text: {Design of a Game Controller}\n\nFor the next example, imagine that you are part of a team building a\ngame for children to play at Engineering Open House.\n\nThe game revolves around an old logic problem in which a farmer must\ncross a river in order to reach the market.  The farmer is traveling\nto the market to sell a fox, a goose, and some corn.  The farmer has\na boat, but the boat is only large enough to carry the fox, the goose,\nor the corn along with the farmer.  The farmer knows that if he leaves\nthe fox alone with the goose, the fox will eat the goose.  Similarly,\nif the farmer leaves the goose alone with the corn, the goose will \neat the corn.\n\nHow can the farmer cross the river?\n\nYour team decides to build a board illustrating the problem with\na river from top to bottom and lights illustrating the positions of \nthe farmer (always with the boat), the fox, the goose, and the\ncorn on either the left bank or the right bank of the river.\nEverything starts on the left bank, and the children can play \nthe game until they win by getting everything to the right bank or\nuntil they make a mistake.\n\nAs the ECE major on your team, you get to design the FSM!\n\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n\nSince the four entities (farmer, fox, goose, and corn) can be only\non one bank or the other, we can use one bit to represent the location\nof each entity.  Rather than giving the states names, let's just\ncall a state FXGC.  The value of F represents the location of the farmer,\neither on the left bank (F=0) or the right bank (F=1).  Using\nthe same representation (0 for the left bank, 1 for the right bank),\nthe value of X represents the location of the fox, G represents the\nlocation of the goose, and C represents the location of the corn.\n\n\nWe can now put together an abstract next-state table, as shown to\nthe right.  Once the player wins or loses, let's have the game indicate\ntheir final status and stop accepting requests to have the farmer cross\nthe river.  We can use a reset button to force the game back into the\noriginal state for the next player.\n\nNote that we have included conditions for some of the input events, as \nwe did previously\n\n\n{c|l|c|c}\ninitial state& {|c|}{input event}& condition& final state \nFXGC& no input& always& FXGC\nFXGC& reset & always& 0000\nFXGC& cross alone& always& XGC\nFXGC& cross with fox& F=X& GC\nFXGC& cross with goose& F=G& XC\nFXGC& cross with corn& F=C& XG\n\n\n\nwith the vending machine design.\n\nThe conditions here require that the farmer be on the same bank as any\nentity that the player wants the farmer to carry across the river.\n\nNext, we specify the I/O interface. \n\nFor input, the game has five buttons.  A reset button R forces the\nFSM back into the initial state.  The other four buttons cause the\nfarmer to cross the river: B_F crosses alone, B_X with the fox,\nB_G with the goose, and B_C with the corn.\n\nFor output, we need position indicators for the four entities, but let's\nassume that we can simply output the current state FXGC and have\nappropriate images or lights appear on the correct banks of the \nriver.  We also need two more indicators: W for reaching the winning\nstate, and L for reaching a losing state.\n\nNow we are ready to complete the specification.  We could use a strict\nprioritization of input events, as we did with earlier examples.  Instead,\nin order to vary the designs a bit, we use a strict prioritization among \nallowed inputs.  The reset button R has the highest priority, followed\nby B_F, B_C, B_G, and finally B_X.  However, only those buttons \nthat result in an allowed move are considered when selecting one button \namong several pressed in a single clock cycle.\n\n\nAs an example, consider the \nstate FXGC=0101.  The farmer is not on the same bank as the fox, nor\nas the corn, so the B_X and B_C buttons are ignored, leading to the\nnext-state table to the right.  Notice that B_G is accepted even if B_C\nis pressed because the farmer is not on the same\n\n\n{cccccc|c}\nFXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+ \n0101& 1& x& x& x& x& 0000\n0101& 0& 1& x& x& x& 1101\n0101& 0& 0& x& 1& x& 1111\n0101& 0& 0& x& 0& x& 0101\n\n\n\nbank as the corn.\nAs shown later, this approach to prioritization\nof inputs is equally simple in terms of implementation.  \n\n\nRecall that we want to stop the game when the player wins or loses.\nIn these states, only the reset button is accepted.  For example, the \nstate FXGC=0110 is a losing state because\n\n\n{cccccc|c}\nFXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+ \n0110& 1& x& x& x& x& 0000\n0110& 0& x& x& x& x& 0110\n\n\n\nthe farmer\nhas left the fox with the goose on the opposite side of the river.\nIn this case, the player can reset the game, but other buttons\nare ignored.\n\nAs we have already chosen a representation, we now move on to implement\nthe FSM.  Let's begin by calculating the next state ignoring the\nreset button and the winning and losing states, as shown in the logic \ndiagram below.\n\n\n\nThe left column of XOR gates determines whether the farmer is on the same\nbank as the corn (top gate), goose (middle gate), and fox (bottom gate).\nThe output of each gate is then used to mask out the corresponding button:\nonly when the farmer is on the same bank are these buttons considered.\nThe adjusted button values are then fed into a priority encoder, which\nselects the highest priority input event according to the scheme that\nwe outlined earlier (from highest to lowest, B_F, B_C, B_G, and\nB_X, ignoring the reset button).  \n\nThe output of the priority encoder is then used to drive another column\nof XOR gates on the right in order to calculate the next state.\nIf any of the allowed buttons is pressed, the priority encoder \noutputs P=1, and the farmer's bank is changed.  If B_C is allowed\nand selected by the priority encoder (only when B_F is not pressed),\nboth the farmer and the corn's banks are flipped.  The goose and the fox\nare handled in the same way.\n\n\nNext, let's build a component to produce the win and lose signals.\nThe one winning state is FXGC=1111, so we simply need an AND gate.\nFor the lose signal L, we can fill in a {K-map} and derive an \nexpression, as shown to the right, then implement as shown in the logic\ndiagram to the far right.  For the {K-map}, remember that the player\nloses whenever the fox and the goose are on the same side of the river,\nbut opposite from the farmer, or whenever the goose and the corn are on\nthe same side of the river, but opposite from the farmer.\n\n\n\n{eqnarray*}\nL &=& F   +  X G +\n&& F   +  G C\n{eqnarray*}\n\n\n\n\n\n\nFinally, we complete our design by integrating the next-state\ncalculation and the win-lose calculation with a couple of muxes, as\nshown to the right.\n\nThe lower mux controls the final value of the next state: note that\nit selects between the constant state FXGC=0000 when the reset button R\nis pressed and the output of the upper mux when R=0.\n\nThe upper mux is controlled by W+L, and retains the current state\nwhenever either signal is 1.  In other words, once the player has won\nor lost, the upper mux prevents further state changes until the reset\nbutton is pressed.\n\nWhen R, W, and L are all 0, the next state is calculated\naccording to whatever buttons have been pressed.\n\n\n\n\n\n\n\n \n Question: What is the output of the priority encoder? \n Answer: \n\nA. The output of the priority encoder is 011."}, {"text": "Title: Analysis of a Stoplight Controller \n Text: {Analysis of a Stoplight Controller}\n\nIn this example, we begin with a digital FSM design and analyze it to\nunderstand how it works and to verify that its behavior is appropriate.\n\nThe FSM that we analyze has been designed to control the stoplights\nat the intersection of two roads.  For naming purposes, we assume that one\nof the roads runs East and West (EW), and the second runs North and \nSouth (NS).  \n\nThe stoplight controller has two inputs, each of which \nsenses vehicles approaching from either direction on one of the two roads.\nThe input V^=1 when a vehicle approaches from either the East or the\nWest, and the input V^=1 when a vehicle approaches from either the\nNorth or the South.  These inputs are also active when vehicles are stopped\nwaiting at the corresponding lights.\n\nAnother three inputs, A, B, and C, control the timing behavior of the\nsystem; we do not discuss them here except as variables.\n\n\nThe outputs of the controller consist of two {2-bit} values, \nL^ and L^, that specify the light colors for the two roads.\nIn particular, L^ controls the lights facing East and West, and\nL^ controls the lights facing North and South.  The meaning of these\noutputs is given in the table to the right.\n\n\n{c|c}\nL& light color \n0x& red\n10& yellow\n11& green\n\n\n\nLet's think about the basic operation of the controller.\n\nFor safety reasons, the controller must ensure that the lights on one\nor both roads are red at all times.  \n\nSimilarly, if a road has a green light, the controller should \nshow a yellow light before showing a red light to give drivers some\nwarning and allow them to slow down.\n\nFinally, for fairness, the controller should alternate green lights\nbetween the two roads.\n\nNow take a look at the logic diagram below.\n\nThe state of the FSM has been split into two pieces: a {3-bit} \nregister S and a {6-bit} timer.  The timer is simply a binary \ncounter that counts downward and produces an output of Z=1 when it \nreaches 0.  Notice that the register S only takes a new value\nwhen the timer reaches 0, and that the Z signal from the timer\nalso forces a new value to be loaded into the timer in the next \ncycle.  We can thus think of transitions in the FSM on a cycle by \ncycle basis as consisting of two types.  The first type simply\ncounts downward for a number of cycles while holding the register S\nconstant, while the second changes the value of S and sets the\ntimer in order to maintain the new value of S \nfor some number of cycles.\n\n\n\n3.45\n\n\nLet's look at the next-state logic for S, which feeds into the IN\ninputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice \nthat none of the inputs to the FSM directly affect these values.  The\nstates of S thus act like a counter.  By examining the connections,\nwe can derive equations for the next state and draw a transition\ndiagram, as shown to the right.\n\nAs the figure shows, there are six states in the loop defined by the \nnext-state logic, with the two remaining states converging into the\nloop after a single cycle.\n\nLet's now examine the outputs for each\nstate in order to understand how the stoplight sequencing\nworks.\n\nWe derive equations for the outputs that control the lights, as shown\nto the right, then calculate values and colors for each\nstate, as shown to the far right.  For completeness, the table \nincludes the states outside of the desired loop.  The \nlights are all red in both of these states, which is necessary for safety.\n\n\n{eqnarray*}\n\nS_2^+ &=& {S_2} + S_0\nS_1^+ &=& {S_2}  S_1\nS_0^+ &=& {S_2}\n{eqnarray*}\n\n{eqnarray*}\nL_1^ &=& S_2 S_1\nL_0^ &=& S_0\nL_1^ &=& S_2 {S_1}\nL_0^ &=& S_0\n{eqnarray*}\n\n\n\n{c|cc|cc}\n&&& EW& NS\n&&& light& light\nS& L^& L^& color& color \n000& 00& 00&    red&    red\n111& 11& 01&  green&    red\n110& 10& 00& yellow&    red\n010& 00& 00&    red&    red\n101& 01& 11&    red&  green\n100& 00& 10&    red& yellow \n001& 01& 01&    red&    red\n011& 01& 01&    red&    red\n\n\n\n\nNow let's think about how the timer works.  As we already noted, the\ntimer value is set whenever S enters a new state, but it can also be\nset under other conditions---in particular, by the signal F calculated\nat the bottom of the FSM logic diagram.  \n\n\nFor now, assume that F=0.  In this case, the timer is set only when\nthe state S changes, and we can find the duration of each state by\nanalyzing the muxes.  The bottom mux selects A when S_2=0, and \nselects the output of the top mux when S_2=1.  The top mux selects B\nwhen S_0=1, and selects C when S_0=0.  Combining these results,\nwe can calculate the duration of the next states of S when F=0, \nas shown in the table to the right.  We can then combine the next\nstate duration with our previous calculation of the state sequencing \n(also the order in the table) to obtain the durations of each state, also\nshown in the rightmost column of the table.\n\n\n{c|cc|cc}\n& EW& NS& next& current\n& light& light& state& state\nS& color& color& duration& duration \n000&    red&    red& A& C\n111&  green&    red& B& A\n110& yellow&    red& C& B\n010&    red&    red& A& C\n101&    red&  green& B& A\n100&    red& yellow& C& B \n001&    red&    red& A& ---\n011&    red&    red& A& ---\n\n\n\nWhat does F do?  Analyzing the gates that produce it gives \nF=S_1S_0{V^+{S_1}S_0{V^.  If we \nignore the two states outside of the main loop for S, the first term \nis 1 only when the lights are green on the East and West roads and the \ndetector for the North and South roads indicates that no vehicles are \napproaching.  Similarly, the second term is 1 only when the lights are \ngreen on the North and South roads and the detector for the East and \nWest roads indicates that no vehicles are approaching.\n\nWhat happens when F=1?  First, the OR gate feeding into the timer's\nLD input produces a 1, meaning that the timer loads a new value\ninstead of counting down.  Second, the OR gate controlling the lower\nmux selects the A input.  In other words, the timer is reset to A\ncycles, corresponding to the initial value for the green light states.\nIn other words, the light stays green until vehicles approach on \nthe other road, plus A more cycles.\n\nUnfortunately, the signal F may also be 1 in the unused states of S,\nin which case the lights on both roads may remain red even though cars\nare waiting on one of the roads.  To avoid this behavior, we must be \nsure to initialize the state S to one of the six states in the\ndesired loop.\n\n\n\n \n Question: What is the main purpose of the FSM? \n Answer: \n\nA. The FSM always has at least one red light on at all times by design. This is to ensure safety at the intersection."}, {"text": "Title: From FSM to Computer \n Text: {From FSM to Computer}\n\nThe FSM designs we have explored so far have started with a human-based\ndesign process in which someone writes down the desired behavior in\nterms of states, inputs, outputs, and transitions.  Such an approach\nmakes it easier to build a digital FSM, since the abstraction used\ncorresponds almost directly to the implementation.\n\nAs an alternative, one can start by mapping the desired task into a\nhigh-level programming language, then using components such as registers,\ncounters, and memories to implement the variables needed.  In this approach,\nthe control structure of the code maps into a high-level FSM design.\nOf course, in order to implement our FSM with digital logic, we eventually\nstill need to map down to bits and gates.\n\nIn this set of notes, we show how one can transform a piece of code\nwritten in a high-level language into an FSM.  This process is meant to\nhelp you understand how we can design an FSM that executes simple\npieces of a flow chart such as assignments, { if} statements, and \nloops.  Later, we generalize this concept and build an FSM that allows\nthe pieces to be executed to be specified after the FSM is built---in \nother words, the FSM executes a program specified by bits stored in \nmemory.  This more general model, as you might have already guessed, \nis a computer.  \n\n \n Question: What is the difference between an FSM and a computer? \n Answer: \n\nA. The difference between an FSM and a computer is that an FSM is a specific type of finite state machine that is used to execute simple pieces of a flow chart, while a computer is a more general model that allows the pieces to be executed to be specified after the FSM is built."}, {"text": "Title: Specifying the Problem \n Text: {Specifying the Problem}\n\nLet's begin by specifying the problem that we want to solve.\nSay that we want to find the minimum value in a set of 10 integers.\nUsing the C programming language, we can write the following fragment of \ncode:\n\n\n{\n\naaaa=aaaa=\nint >values[10];    /* 10 integers--filled in by other code */\nint >idx;\nint >min\n\nmin = values[0];\nfor (idx = 1; 10 > idx; idx = idx + 1) {\n>  if (min > values[idx]) {\n>  >  min = values[idx];\n>  }\n}\n/* The minimum value from array is now in min. */\n\n}\n\n\nThe code uses array notation, which we have not used previously in our \nclass, so let's first discuss the meaning of the code.\n\nThe code uses three variables.\n\nThe variable { values} represents the 10 values in our set.\nThe suffix ``[10]'' after the variable name tells the compiler that\nwe want an array of 10 integers ({ int}) indexed from 0 to 9.\nThese integers can be treated as 10 separate variables, but can be\naccessed using the single name ``{ values}'' along with an index\n(again, from 0 to 9 in this case).\n\nThe variable { idx} holds a loop index that we use to examine each\nof the values one by one in order to find the minimum value in the set.\n\nFinally, the variable { min} holds the smallest known value as \nthe program examines each of the values in the set.\n\nThe program body consists of two statements.  \n\nWe assume that some other piece of code---one not shown here---has \ninitialized the 10 values in our set before the code above executes.\n\nThe first statement initializes the\nminimum known value ({ min}) to the value stored at index 0 in the \narray ({ values[0]}).\nThe second statement is a loop in which the variable { index} \ntakes on values from 1 to 9.  For each value, an { if} statement\ncompares the current\nknown minimum with the value stored in the array at index given by the\n{ idx} variable.  If the stored value is smaller, the current known \nvalue (again, { min}) is updated to reflect the program's\nhaving found a smaller value.  When the loop finishes all nine iterations,\nthe variable { min} holds the smallest value among the set of 10 \nintegers stored in the { values} array.\n\n\nAs a first step towards designing an FSM to implement the code, we transform\nthe code into a flow chart, as shown to the right.  The program again begins\nwith initialization, which appears in the second column of the flow chart.  \nThe loop in the program translates to the third column of the flow chart, \nand the { if} statement to the middle comparison and update \nof { min}.\n\nOur goal is now to design an FSM to implement the flow chart.  In order\nto do so, we want to leverage the same kind of abstraction that we used\nearlier, when extending our keyless entry system with a timer.  Although the\ntimer's value was technically also\n\n\n{{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}\n\n\npart of the FSM's state, we treated it\nas data and integrated it into our next-state decisions in only a couple\nof cases.\n\nFor our minimum value problem, we have two sources of data.  First, an\nexternal program supplies data in the form of a set of 10 integers.  If\nwe assume {32-bit} integers, these data technically form 320 input bits!\nSecond, as with the keyless entry system timer, we have data used internally\nby our FSM, such as the loop index and the current minimum value.  These\nare technically state bits.  For both types of data, we treat them\nabstractly as values rather than thinking of them individually as bits,\nallowing us to develop our FSM at a high-level and then to implement it \nusing the components that we have developed earlier in our course.\n\n \n Question: What is the code used for? \n Answer: \n\nA. The code uses three variables."}, {"text": "Title: Choosing Components and Identifying States \n Text: {Choosing Components and Identifying States}\n\nNow we are ready to design an FSM that implements the flow chart.\nWhat components do we need, other than our state logic?\nWe use registers and counters to implement the variables { idx}\nand { min} in the program.\nFor the array { values}, we use a {1632-bit} \nmemory.{We technically only need a {1032-bit} \nmemory, but we round up the size of the address space to reflect more\nrealistic memory designs; one can always optimize later.}\nWe need a comparator to implement the test for the { if} statement.\nWe choose to use a serial comparator, which allows us to illustrate again\nhow one logical high-level state can be subdivided into many actual states.\nTo operate the serial comparator, we make use of two shift registers that \npresent the comparator with one bit per cycle on each input, and a counter\nto keep track of the comparator's progress.\n\nHow do we identify high-level states from our flow chart?  Although\nthe flow chart attempts to break down the program into `simple' steps,\none step of a flow chart may sometimes require more than one state\nin an FSM.  Similarly, one FSM state may be able to implement several\nsteps in a flow chart, if those steps can be performed simultaneously.\nOur design illustrates both possibilities.\n\nHow we map flow chart elements into FSM states also depends to some \ndegree on what components we use, which is why we began with some discussion\nof components.  In practice, one can go back and forth between the two, \nadjusting components to better match the high-level states, and adjusting \nstates to better match the desired components.\n\nFinally, note that we are only concerned with high-level states, so we do \nnot need to provide details (yet) down to the level of individual clock \ncycles, but we do want to define high-level states that can be implemented\nin a fixed number of cycles, or at least a controllable number of cycles.\nIf we cannot specify clearly when transitions occur from an FSM state, we\nmay not be able to implement the state.\n\n\n\nNow let's go through the flow chart and identify states.  Initialization of\n{ min} and { idx} need not occur serially, and the result of the\nfirst comparison between { idx} and the constant 10 is known in advance,\nso we can merge all three operations into a single state, which we \ncall { INIT}.\n\nWe can also merge the updates of { min} and { idx} into a second\nFSM state, which we call { COPY}.  However, the update to { min} \noccurs only when the comparison ({ min > value[idx]}) is true.  \nWe can use logic to predicate execution of the update.  In other words, we \ncan use the output of the comparator, which is available after the comparator \nhas finished comparing the two values (in a high-level FSM state that we \nhave yet to define), to determine whether or not the register holding \n{ min} loads a new value in the { COPY} state.\n\nOur model of use for this FSM involves external logic filling the memory\n(the array of integer values), executing the FSM ``code,'' and then\nchecking the answer.  To support this use model, we create a FSM state \ncalled { WAIT} for cycles in which the FSM has no work to do.\nLater, we also make use of an external input signal { START} \nto start the FSM\nexecution.  The { WAIT} state logically corresponds to the ``START'' \nbubble in the flow chart.\n\n\nOnly the test for the { if} statement remains.  Using a serial\ncomparator to compare two {32-bit} values requires 32 cycles.\nHowever, we need an additional cycle to move values into our shift \nregisters so that the comparator can see the first bit.  Thus our\nsingle comparison operation breaks into two high-level states.  In the\nfirst state, which we call { PREP}, we copy { min} to one\nof the shift registers, copy { values[idx]} to the other shift\nregister, and reset the counter that measures the cycles needed for\nour serial comparator.  We then move to a second high-level state,\nwhich we call { COMPARE}, in which we feed one bit per cycle\nfrom each shift register to the serial comparator.  The { COMPARE} \nstate\n\n\n{{file=part3/figs/part3-min-flow-chart-states.eps,width=3.96in}}\n\n\nexecutes for 32 cycles, after which the comparator\nproduces the one-bit answer that we need, and we can move to the\n{ COPY} state.  The association between the flow chart and the\nhigh-level FSM states is illustrated in the figure shown to the right\nabove.\n\n\nWe can now also draw an abstract state diagram for our FSM, as shown\nto the right.  The FSM begins in the { WAIT} state.  After external\nlogic fills the { values} array, it signals the FSM to begin by\nraising the { START} signal.  The FSM transitions into the \n{ INIT} state, and in the next cycle into the { PREP} state.\nFrom { PREP}, the FSM always moves to { COMPARE}, where it\nremains for 32 cycles while the serial comparator executes a comparison.\nAfter { COMPARE}, the FSM moves to the { COPY}\n\n\n{{file=part3/figs/part3-min-state-diag.eps,width=3in}}\n\n\nstate, where\nit remains for one cycle.  The transition from { COPY} depends on\nhow many loop iterations have executed.  If more loop iterations remain,\nthe FSM moves to { PREP} to execute the next iteration.  If the\nloop is done, the FSM returns to { WAIT} to allow external logic\nto read the result of the computation.\n\n\n\n \n Question: What do we need to implement the test for the if statement? \n Answer: \n\nWe need registers and counters to implement the variables idx and min in the program. For the array values, we use a 1632-bit memory. We need a comparator to implement the test for the if statement."}, {"text": "Title: Laying Out Components \n Text: {Laying Out Components}\n\n\nOur high-level FSM design tells us what our components need to be able to\ndo in any given cycle.  For example, when we load new values into the shift\nregisters that provide bits to the serial comparator, we always copy \n{ min} into one shift register and { values[idx]} into the second.\nUsing this information, we can put together our components and simplify our\ndesign by fixing the way in which bits flow between them.\n\nThe figure at the right shows how we can organize our components.\nAgain, in practice, one goes back and forth thinking about states,\ncomponents, and flow from state to state.  In these notes, we \npresent only a completed design.\n\nLet's take a detailed look at each of the components.\n\nAt the upper left of the figure is a {4-bit} binary counter called\n{ IDX} to hold the { idx}\nvariable.  The counter can be reset to 0 using the { RST} input.\nOtherwise, the { CNT} input controls whether or not the counter\nincrements its value.  With this counter design, we can force { idx} \nto 0 in the { WAIT} state and then count upwards in the { INIT}\nand { COPY} states.\n\n\n{{file=part3/figs/part3-min-components.eps,width=3.84in}}\n\n\nA memory labeled { VALUES} to hold the array { values} appears \nin the upper right of\nthe figure.  The read/write control for the memory is hardwired to 1 (read)\nin the figure, and the data input lines are unattached.  To integrate \nwith other logic that can operate our FSM, we need to add more \ncontrol logic to allow writing into the memory and to attach the data\ninputs to something that provides the data bits.  The address input of\nthe memory comes always from the { IDX} counter value; in other words,\nwhenever we access this memory by making use of the data output lines,\nwe read { values[idx]}.\n\nIn the middle left of the figure is a {32-bit} register for the \n{ min} variable.  It has a control input { LD} that \ndetermines whether or not it loads a new value at the end of the clock\ncycle.  If a new value is loaded, the new value always corresponds to\nthe output of the { VALUES} memory, { values[idx]}.  Recall\nthat { min} always changes in the { INIT} state, and may change\nin the { COPY} state.  But the new value stored in { min}\nis always { values[idx]}. \nNote also that when the FSM completes its task, the result of the \ncomputation is left in the { MIN} register for external logic to\nread (connections for this purpose are not shown in the figure).\n\nContinuing downward in the figure, we see two right shift registers\nlabeled { A} and { B}.  Each has a control input { LD}\nthat enables a parallel load.  Register { A} loads from register\n{ MIN}, and register { B} loads from the memory data output\n({ values[idx]}).  These loads are needed in the { PREP}\nstate of our FSM.  When { LD} is low, the shift registers\nsimply shifts to the right.  The serial output { SO} makes the\nleast significant bit of each shift register available.  Shifting\nis necessary to feed the serial comparator in the { COMPARE} state.\n\nBelow register { A} is a {5-bit} binary counter called { CNT}.\nThe counter is used to control the serial comparator in the { COMPARE}\nstate.  A reset input { RST} allows it to be forced to 0 in the \n{ PREP} state.  When the counter value is exactly zero, the \noutput { Z} is high.\n\n\n\nThe last major component is the serial comparator, which is based on the\ndesign developed in Notes Set 3.1.  The two bits to be compared in \na cycle come from shift registers { A} and { B}.  The first\nbit indicator comes from the zero indicator of counter { CNT}.\nThe comparator actually produces two outputs ({ Z1} and { Z0}),\nbut the meaning of the\n{ Z1} output by itself is { A > B}.  In the diagram,\nthis signal has been labeled { THEN}.\n\nThere are two additional elements in the figure that we have yet to discuss.\nEach simply compares the value in a register with a fixed constant and \nproduces a {1-bit} signal.  When the FSM finishes an iteration of\nthe loop in the { COPY} state, it must check the loop condition\n({ 10 > idx}) and move either to the { PREP} state or, when\nthe loop finishes, to the { WAIT} state to let the external logic\nread the answer from the { MIN} register.  The loop is done when the\ncurrent iteration count is nine, so we compare { IDX} with nine to\nproduce the { DONE} signal.  The other constant comparison is between\nthe counter { CNT} and the value 31 to produce the { LAST} signal,\nwhich indicates that the serial comparator is on its last cycle of \ncomparison.  In the cycle after { LAST} is high, the { THEN} \noutput of the comparator indicates whether or not { A > B}.\n\n \n Question: What are the DONE and LAST signals used for? \n Answer: \n\n\nThe DONE and LAST signals are used to compare the value in a register with a fixed constant. The DONE signal compares the value in the IDX register with the constant 9, and the LAST signal compares the value in the CNT register with the constant 31. These comparisons are used to determine when the loop is finished and when the serial comparator is on its last cycle of comparison, respectively."}, {"text": "Title: Control and Data \n Text: {Control and Data}\n\nOne can think of the components and the interconnections between them\nas enabling the movement of data between registers, while the high-level \nFSM controls which data move from register to register in each cycle.  \n\nWith this model in mind, we call the components and interconnections\nfor our design the { datapath}---a term that we will see again when\nwe examine the parts of a computer in the coming weeks.\n\nThe datapath requires several inputs to control the operation of the\ncomponents---these we can treat as outputs of the FSM.\nThese signals allow the FSM to control the motion of data in the \ndatapath, so we call them { control signals}.\n\nSimilarly, the datapath produces several outputs that we can treat\nas inputs to the FSM.\n\nThe tables below summarize the control signals (left) and outputs (right)\nfrom the datapath for our FSM.\n\n[t]\n{|c|l|}\ndatapath& \ninput& {|c|} \n{ IDX.RST}& reset { IDX} counter to 0\n{ IDX.CNT}& increment { IDX} counter\n{ MIN.LD}& load new value into { MIN} register\n{ A.LD}& load new value into shift register { A}\n{ B.LD}& load new value into shift register { B}\n{ CNT.RST}& reset { CNT} counter \n\n\n[t]\n{|c|l|c|}\ndatapath& &\noutput& {|c|}& based on \n{ DONE}& last loop iteration finished& { IDX = 9}\n{ LAST}& serial comparator executing& { CNT = 31}\n&    last cycle&\n{ THEN}& { if} statement condition true& { A > B} \n{}\n{}\n\n\n\nUsing the datapath controls signals and outputs, we can now write a more\nformal state transition table for the FSM, as shown below.\n\nThe ``actions'' column of the table\nlists the changes to register and counter values\nthat are made in each of the FSM states.  The notation used to represent\nthe actions is called { register transfer language} ({ RTL}).\nThe meaning of an individual action is similar to the meaning of the \ncorresponding statement from our C code or from the flow chart.\nFor example, in the { WAIT} state, ``{ IDX  0}''\nmeans the same thing as ``{ idx = 0;}''.  In particular, both mean\nthat the value currently stored in the { IDX} counter is overwritten \nwith the number 0 (all 0 bits).\n\n\n\nThe meaning of RTL is slightly different from the usual interpretation of\nhigh-level programming languages, however, in terms of when the actions\nhappen.  A list of C statements is generally executed one at a time.\n\nIn contrast, the entire list of RTL actions\n\n\n{|c|l|c|c|}\nstate& {|c|}{actions (simultaneous)}& condition& next state \n{ WAIT}& { IDX}  0 (to read { VALUES[0]} in { INIT})& { START}& { INIT} \n&& {{ START}}& { WAIT} \n{ INIT}& { MIN}  { VALUES[IDX]} ({ IDX} is 0 in this state)& (always)& { PREP} \n& { IDX}  { IDX} + 1& & \n{ PREP}& { A}  { MIN}& (always)& { COMPARE}\n& { B}  { VALUES[IDX]}& &\n& { CNT}  0& & \n{ COMPARE}& run serial comparator& { LAST}& { COPY}\n&& {{ LAST}}& { COMPARE} \n{ COPY}& { THEN}: { MIN}  { VALUES[IDX]}& { DONE}& { WAIT} \n& { IDX}  { IDX} + 1& {{ DONE}}& { PREP} \n\n\n\nfor an FSM state is executed\nsimultaneously, at the end of the clock cycle.  As you know, an FSM moves \nfrom its current state into a new state at the end of every clock cycle,\nso actions during different cycles usually are associated with different \nstates.\nWe can, however, change the value in more than one register at the end \nof the same clock cycle, so we can execute more than one RTL action in\nthe same state, so long as the actions do not exceed the capabilities\nof our datapath (the components must be able to support the simultaneous \nexecution of the actions).  Some care must be taken with states that \nexecute for more than one cycle to ensure that repeating the \nRTL actions is appropriate.  In our design, only the { WAIT} and\n{ COMPARE} states execute for more than one cycle.  The { WAIT}\nstate resets the { IDX} counter repeatedly, which causes no problems.\nThe { COMPARE} statement has no RTL actions---all of the shifting,\ncomparison, and counting activity needed to do its work occurs within \nthe datapath itself.\n\nOne additional piece of RTL syntax needs explanation.  In the { COPY}\nstate, the first action begins with ``{ THEN}:,'' which means that the \nprefixed RTL action occurs only when the { THEN} signal is high.\nRecall that the { THEN} signal indicates that the comparator has\nfound { A > B}, so the equivalent C code is ``{ if (A > B) \n{min = values[idx]}}''.\n\n \n Question: What is the state of the FSM? \n Answer: \n\nA. The FSM moves from its current state into a new state at the end of every clock cycle."}, {"text": "Title: State Representation and Logic Expressions \n Text: {State Representation and Logic Expressions}\n\nLet's think about the representation for the FSM states.  The FSM has \nfive states, so we could use as few as three flip-flops.  Instead, we choose\nto use a { one-hot encoding}, in which any valid bit pattern has exactly\none 1 bit.  In other words, we use five flip-flops instead of three,\nand our states are represented with the bit patterns 10000, 01000, 00100,\n00010, and 00001.\n\nThe table below shows the mapping from each high-level state to \nboth the five-bit encoding for the state as well as the six control signals \nneeded for the datapath.  For each state, the values of the control signals\ncan be found by examining the actions necessary in that state.\n\n{\n{|c|c|cccccc|}\nstate& { S_4S_3S_2S_1S_0}& { IDX.RST}& { IDX.CNT}& { MIN.LD}& { A.LD}& { B.LD}& { CNT.RST} \n{ WAIT}& { 1 0 0 0 0}& 1& 0& 0& 0& 0& 0\n{ INIT}& { 0 1 0 0 0}& 0& 1& 1& 0& 0& 0\n{ PREP}& { 0 0 1 0 0}& 0& 0& 0& 1& 1& 1\n{ COMPARE}& { 0 0 0 1 0}& 0& 0& 0& 0& 0& 0\n{ COPY}& { 0 0 0 0 1}& 0& 1& { THEN}& 0& 0& 0 \n{}\n\n}\n\nThe { WAIT} state needs to set { IDX} to 0 but need not affect \nother register or counter values, so { WAIT} produces a 1 only for\n{ IDX.RST}.  The { INIT} state needs to load { values[0]} into \nthe { MIN} register while simultaneously incrementing the { IDX}\ncounter (from 0 to 1), so { INIT} produces 1s for { IDX.CNT}\nand { MIN.LD}.  The { PREP} state loads both shift registers\nand resets the counter { CNT} by producing 1s for { A.LD},\n{ B.LD}, and { CNT.RST}.  The { COMPARE} state does not\nchange any register values, so it produces all 0s.  Finally, the { COPY}\nstate increments the { IDX} counter while simultaneously loading a\nnew value into the { MIN} register.  The { COPY} state produces 1\nfor { IDX.CNT}, but must use the signal { THEN} coming from the\ndatapath to decide whether or not { MIN} is loaded.\n\n\n\nThe advantage of a one-hot encoding becomes obvious when we write\nequations for the six control signals and the next-state logic, as shown\nto the right.  \n\nImplementing the logic to complete our design now requires only a handful \nof small logic gates.\n\n\n{eqnarray*}\n{ IDX.RST} & = & { S}_4\n{ IDX.CNT} & = & { S}_3 + { S}_0\n{ MIN.LD} & = & { S}_3 + { S}_0  { THEN}\n{ A.LD} & = & { S}_2\n{ B.LD} & = & { S}_2\n{ CNT.RST} & = & { S}_2\n{eqnarray*}\n\n\n{eqnarray*}\n{ S}_4^+ & = & { S}_4  { START} + { S}_0  { DONE}\n{ S}_3^+ & = & { S}_4  { START}\n{ S}_2^+ & = & { S}_3 + { S}_0  { DONE}\n{ S}_1^+ & = & { S}_2 + { S}_1  { LAST}\n{ S}_0^+ & = & { S}_1  { LAST}\n{eqnarray*}\n\n\nNotice that the terms in each control signal can be read directly from \nthe rows of the state table and OR'd together.  The terms in each of the\nnext-state equations represent the incoming arcs for the corresponding\nstate.  For example, the { WAIT} state has one self-loop (the first\nterm) and a transition arc coming from the { COPY} state when the\nloop is done.\n\nThese expressions complete our design.\n\n\n\n\n\n \n Question: What is the advantage of using a one-hot encoding? \n Answer: \n\nA. The advantage of using a one-hot encoding is that it is easy to write equations for the six control signals and the next-state logic."}, {"text": "Title: Extending Keyless Entry with a Timeout \n Text: {Extending Keyless Entry with a Timeout}\n\nThis set of notes builds on the keyless entry control FSM that we\ndesigned earlier.  In particular, we use a counter to make the alarm\ntime out, turning itself off after a fixed amount of time.  The goal\nof this extension is to illustrate how we can make use of components\nsuch as registers and counters as building blocks for our FSMs\nwithout fully expanding the design to explicitly illustrate all\npossible states.\n\n \n Question: What is the purpose of the extension? \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing \n Text: {Physical Design, Sensors, and Timing}\n\n\nTo begin, let's review the FSM that we designed earlier for keyless \nentry.  The state transition diagram for our design is replicated to \nthe right.\n\nThe four states are labeled with state bits and output bits,\nS_1S_0/DRA, where D indicates that the driver's door should\nbe unlocked, R indicates that the rest of the doors should be\nunlocked, and A indicates that the alarm should be on.\n\nTransition arcs in the diagram are labeled with concise versions of \nthe inputs ULP (using don't cares), where U represents an unlock\nbutton, L represents a lock button, and P represents a panic \nbutton.\n\nIn this design, once a user presses the panic button P, the alarm\nsounds until the user presses the\n\n\n{file=part3/figs/ke-trans-diag-brief.eps,width=4.2in}\n\n\nlock button L to turn it off.\nInstead of sounding the alarm indefinitely, we might want to turn\nthe alarm off after a fixed amount of time.  In other words, after \nthe system has been in the ALARM state for, say, thirty or sixty seconds,\nwe might want to move back to the LOCKED state even if the user has\nnot pushed the lock button.  The blue annotation in the diagram indicates\nthe arc that we must adjust.  But thirty or sixty seconds is a large \nnumber of clock cycles, and our FSM must keep track of the time.\nDo we need to draw all of the states?\n\nInstead of following the design process that we outlined earlier, let's\nthink about how we can modify our existing design to incorporate the new\nfunctionality.  In order to keep track of time, we use a binary counter.\n\nLet's say that we want our timeout to be T cycles.\n\nWhen we enter the alarm state, we want to set the counter's value \nto T-1, then let the counter count down until it reaches 0, at \nwhich point a timeout occurs.\n\nTo load the initial value, our counter should have a parallel load \ncapability that sets the counter value when input LD=1.  When LD=0,\nthe counter counts down. \n\nThe counter also has an output Z that indicates that the counter's \nvalue is currently zero, which we can use to indicate a timeout on\nthe alarm.\n\nYou should be able to build such a counter based on what you have learned\nearlier in the class.  Here, we will assume that we can just make use of\nit.\n\nHow many bits do we need in our counter?  The answer depends on T.\nIf we add the counter to our design, the state of the counter is \ntechnically part of the state of our FSM, but we can treat it\nsomewhat abstractly.  For example, we only plan to make use of the \ncounter value in the ALARM state, so we ignore the counter bits in the\nthree other states.  In other words, S_1S_0=10 means that the system \nis in the LOCKED state regardless of the counter's value.\n\n\nWe expand the ALARM state into T separate states based on the value\nof the counter.  As shown to the right, we name the states ALARM(1) \nthrough ALARM(T).  All of these alarm states use S_1S_0=01, but\nthey can be differentiated using a ``timer'' (the counter value).\n\nWe need to make design decisions about how the arcs entering and\nleaving the ALARM state in our original design should be used once\nwe have incorporated the timeout.  As a first step, we decide that\nall arcs entering ALARM from other states now enter ALARM(1).  \nSimilarly, if the user presses the panic button P in any of the\nALARM(t) states, the system returns to ALARM(1).  Effectively, \npressing the panic button resets the timer.\n\nThe only arc leaving the ALARM state goes to the LOCKED state on \nULP=x10.  We replicate this arc for all ALARM(t) states: the\nuser can push the lock button at any time to silence the alarm.\n\nFinally, the self-loop back to the ALARM state on ULP=x00 becomes\nthe countdown arcs in our expanded states, taking ALARM(t) to ALARM(t+1),\nand ALARM(T) to LOCKED.\n\nNow that we have a complete specification for the extended design, we\ncan implement it.  We want to reuse our original design as much as possible,\nbut we have three new features that must be considered.  First, when\nwe enter the ALARM(1) state, we need to set the counter value to T-1.\nSecond, we need the counter value to count downward while in the ALARM\nstate.  Finally, we need to move back to the LOCKED state when a timeout\noccurs---that is, when the counter reaches zero.\n\n\n{file=part3/figs/ke-alarm-expansion.eps,width=1.75in}\n\n\nThe first problem is fairly easy.  Our counter supports parallel load,\nand the only value that we need to load is T-1, so we apply the constant\nbit pattern for T-1 to the load inputs and raise the LD input whenever\nwe enter the ALARM(1) state.  In our original design, we chose to enter\nthe ALARM state whenever the user pressed P, regardless of the other\nbuttons.  Hence we can connect P directly to our counter's LD input.\n\nThe second problem is handled by the counter's countdown functionality.\nIn the ALARM(t) states, the counter will count down each cycle, moving\nthe system from ALARM(t) to ALARM(t+1).\n\nThe last problem is slightly trickier, since we need to change S_1S_0.\nNotice that S_1S_0=01 for the ALARM state and S_1S_0=00 for the LOCKED\nstate.  Thus, we need only force S_0 to 0 when a timeout occurs.\nWe can use a single {2-to-1} multiplexer for this purpose.  The\n``0'' input of the mux comes from the original S_0^+ logic, and the\n``1'' input is a constant 0.  All other state logic remains unchanged.  \nWhen does a timeout occur? \n\nFirst, we must be in the ALARM(T) state, so S_1S_0=01 and the \ncounter's Z output is raised.  Second, the input combination must\nbe ULP=xx0---notice that both ULP=x00 and ULP=x10 return to \nLOCKED from ALARM(T).  A single, four-input AND gate thus suffices to\nobtain the timeout signal, {S_1}S_0Z, which we connect \nto the select input of the mux between the S_0^+ logic and the\nS_0 flip-flop.\n\nThe extension thus requires only a counter, a mux, and a gate, as shown below.\n\n{{file=part3/figs/ke-alarm-exp-impl.eps,width=2.65in}}\n\n\n\n\n\n \n Question: What is the function of the counter in the extended design? \n Answer: \nWhat is the function of the counter in the extended design?\n\nThe counter is used to keep track of time so that the alarm can sound for a fixed amount of time."}, {"text": "Title: Serialization and Finite State Machines \n Text: {Serialization and Finite State Machines}\n\nThe third part of our class builds upon the basic combinational and\nsequential logic elements that we developed in the second part.\n\nAfter discussing a simple application of stored state\nto trade between area and performance,\n\nwe introduce a powerful abstraction for formalizing and reasoning about\ndigital systems, the Finite State Machine (FSM).\n\nGeneral FSM models are broadly applicable in a range of engineering\ncontexts, including not only hardware and software design but also\nthe design of control systems and distributed systems.  We limit our\nmodel so as to avoid circuit timing issues in your first exposure, but\nprovide some amount of discussion as to how, when, and why you should \neventually learn the more sophisticated models.\n\nThrough development a range of FSM examples, we illustrate important \ndesign issues for these systems and motivate a couple of more advanced \ncombinational logic devices that can be used as building blocks.\n\nTogether with the idea of memory, another form of stored state,\nthese elements form the basis for development of our first computer.\n\nAt this point we return to the textbook, in which Chapters 4 and 5\nprovide a solid introduction to the von Neumann model of computing systems\nand the {LC-3} (Little Computer, version 3) instruction set \narchitecture.  By the end of this part of the course, you will have\nseen an example of the boundary between hardware and software, and will\nbe ready to write some instructions yourself.\n\nIn this set of notes, we cover the first few parts of this material.\nWe begin by describing the conversion of bit-sliced designs into \nserial designs, which store a single bit slice's output in \nflip-flops and then feed the outputs back into the bit slice in the next\ncycle.  As a specific example, we use our bit-sliced comparator \nto discuss tradeoffs in area and performance.  We introduce\nFinite State Machines and some of the tools used to design them,\nthen develop a handful of simple counter designs.  Before delving\ntoo deeply into FSM design issues, we spend a little time discussing\nother strategies for counter design and placing the material covered\nin our course in the broader context of digital system design.\n\nRemember that\n{ sections marked with an asterisk are provided solely for your\ninterest,} but you may need to learn this material in later\nclasses.\n\n\n \n Question: What is the difference between a finite state machine and a counter? \n Answer: \n\nThe design of a finite state machine differs from that of a counter in a few ways. For one, a finite state machine has a finite number of states that it can be in, while a counter can have an infinite number of states. Additionally, a finite state machine's output is determined by its current state, while a counter's output is determined by its current state and the inputs it receives. Finally, a finite state machine can have multiple inputs and outputs, while a counter usually has a single input and a single output."}, {"text": "Title: Serialization: General Strategy \n Text: {Serialization: General Strategy}\n\nIn previous notes, we discussed and illustrated the development of bit-sliced\nlogic, in which one designs a logic block to handle one bit of a multi-bit\noperation, then replicates the bit slice logic to construct a design for \nthe entire operation.  We developed ripple carry adders in this way in \nNotes Set 2.3 and both unsigned and 2's complement comparators in \nNotes Set 2.4.\n\n\nAnother interesting design strategy is { serialization}: rather than \nreplicating the bit slice, we can use flip-flops to store the bits passed\nfrom one bit slice to the next, then present the stored bits { to the\nsame bit slice} in the next cycle.  Thus, in a serial design, we only\nneed one copy of the bit slice logic!  The area needed for a serial design\nis usually much less than for a bit-sliced design, but such a design is\nalso usually slower.  After illustrating the general design strategy,\nwe'll consider these tradeoffs more carefully\nin the context of a detailed example.\n\n\nRecall the general bit-sliced design approach, as illustrated to the right.\n\nSome number of copies of the logic for a single bit slice are connected\nin sequence.  Each bit slice accepts P bits of operand input and\nproduces Q bits of external output.  Adjacent bit slices receive\nan additional M bits of information from the previous bit slice\nand pass along M bits to the next bit slice, generally using some\nrepresentation chosen by the designer.\n\n\n{file=part3/figs/gen-slice-comp.eps,width=3.8in}\n\n\nThe first bit slice is initialized\nby passing in constant values, and some calculation may be performed\non the final bit slice's results to produce R bits more external output.\n\n\n\nWe can transform this bit-sliced design to a serial design with a single\ncopy of the bit slice logic, M+Q flip-flops, and M gates (and sometimes\nan inverter).  The strategy is illustrated on the right below.  A single\ncopy of the bit slice operates on one set of P external input bits \nand produces one set of Q external output bits each clock cycle.  In\nthe design shown, these output bits are available during the next \ncycle, after they have been stored in the flip-flops. \nThe M bits to be passed to the ``next'' bit slice are also \nstored in flip-flops,\nand in the next cycle are provided back to the same physical bit slice\nas inputs.  The first cycle of a multi-cycle operation\nmust be handled slightly differently, so we\nadd selection logic and an control signal, F.  For the first cycle, we\napply F=1, and the initial values are passed into the bit slice.  \nFor all other bits, we apply F=0, and the values stored in the flip-flops\nare returned to the bit slice's inputs.  After all bits have passed through\nthe bit slice---after N cycles for an {N-bit} design---the\nfinal M bits are stored in the flip-flops, and the results are calculated\nby the output logic. \n\n{\n\n{file=part3/figs/init-ser-slice.eps,width=1.4in}\n\n\n{file=part3/figs/ser-slice-comp.eps,width=3.25in}\n\n}\n\nThe selection logic merits explanation.  Given that the original design\ninitialized the bits to constant values (0s or 1s), we need only simple\nlogic for selection.  The two drawings on the left above illustrate \nhow {B_i}, the complemented flip-flop output for a bit i, can be\ncombined with the first-cycle signal F to produce an appropriate input\nfor the bit slice.  Selection thus requires one extra gate for each of \nthe M inputs, and we need an inverter for F if any of the initial \nvalues is 1.\n\n\n \n Question: What is the main tradeoff of a bit-sliced design? \n Answer: \n\nThe main tradeoff of a serial design is that it is usually slower than a bit-sliced design. However, it requires less area, which may be important in some applications."}, {"text": "Title: Serialization: Comparator Example \n Text: {Serialization: Comparator Example}\n\n\nWe now apply the general strategy to a specific example, our bit-sliced\nunsigned comparator from Notes Set 2.4.  The result is shown to the right.\nIn terms of the general model, the single comparator bit slice accepts P=2\nbits of inputs each cycle, in this case a single bit from each of the two\nnumbers being compared, presented to the bit slice in increasing order\nof significance.  The bit slice produces no external output other than\nthe final result (Q=0).  Two bits (M=2) are produced each cycle by \nthe bit slice and stored\n\n\n{file=part3/figs/ser-compare.eps,width=3.5in}\n\n\ninto flip flops B_1 and B_0.  These bits\nrepresent the relationship between the two numbers compared so far\n(including only the bit already seen by the comparator bit slice).\nOn the first cycle, when the least significant bits of A and B are\nbeing fed into the bit slice, we set F=1, which forces the C_1 \nand C_0 inputs of the bit slice to 0 independent of the values stored\nin the flip-flops.  In all other cycles, F=0, and the NOR gates\nset C_1=B_1 and C_0=B_0.  Finally, after N cycles for an {N-bit}\ncomparison, the output logic---in this case simply wires, as shown \nin the dashed box---places the result of the comparison on \nthe Z_1 and Z_0 outputs (R=2 in the general model).  \nThe result is encoded in the \nrepresentation defined for constructing the bit slice (see Notes Set 2.4,\nbut the encoding does not matter here).\n\n\nHow does the serial design compare with the bit-sliced design?\n\nAs an estimate of area, let's count gates.  Our optimized \ndesign is replicated to the right for convenience.  \n\nEach bit slice requires six {2-input} gates and two inverters.  \n\nAssume that each \nflip-flop requires eight {2-input} gates and two inverters,\nso the serial design overall requires 24 gates\n\n\n{file=part3/figs/comparator-opt-nn.eps,width=4.1in}\n\n\nand six inverters to handle any number of bits.\n\nThus, for any number of bits N, the serial design is smaller\nthan the bit-sliced design, and the benefit grows with N.\n\nWhat about performance?  In Notes Set 2.4, we counted gate delays for \nour bit-sliced design.  The path from A or B to the outputs is\nfour gate delays, but the C to Z paths are all two gate delays.\nOverall, then, the bit-sliced design requires 2N+2 gate delays \nfor N bits.  What about the serial design?\n\nThe performance of the serial design is likely to be much worse\nfor three reasons.\n\nFirst, all paths in the design matter, not just the paths from bit\nslice to bit slice.  None of the inputs can be assumed to be available\nbefore the start of the clock cycle, so we must consider all paths\nfrom input to output.  \n\nSecond, we must also count gate delays for the\nselection logic as well as the gates embedded in the flip-flops.\n\nFinally, the result of these calculations may not matter, since the\nclock speed may well be limited by other logic elsewhere in the\nsystem.  If we want a common clock for all of our logic, the clock\nmust not go faster than the slowest element in the entire system, or\nsome of our logic will not work properly.\n\nWhat is the longest path through our serial comparator?\n\nLet's assume that \nthe path through a flip-flop is eight gate delays, with four on each \nside of the clock's rising edge.  The inputs A and B are\nlikely to be driven by flip-flops elsewhere in our system, so\nwe conservatively count four gate delays to A and B and\nfive gate delays to C_1 and C_0 (the extra one comes from\nthe selection logic).  The A and B paths thus dominate inside\nthe bit slice, adding four more gate delays to the outputs Z_1\nand Z_0.  Finally, we add the last four gate delays to flip\nthe first latch in the flip-flops for a total of 12 gate delays.\n\nIf we assume that our serial comparator limits the clock frequency\n(that is, if everything else in the system can use a faster clock),\nwe take 12 gate delays per cycle, or 12N gate delays to compare\ntwo {N-bit} numbers.\n\nYou might also notice that adding support for 2's complement is no \nlonger free.  We need extra logic to swap the A and B inputs\nin the cycle corresponding to the sign bits of A and B.  In\nother cycles, they must remain in the usual order.  This extra\nlogic is not complex, but adds further delay to the paths.\n\nThe bit-sliced and serial designs represent two extreme points in \na broad space of design possibilities.  Optimization of the entire\nN-bit logic function (for any metric) represents a third extreme.\nAs an engineer, you should realize that you can design systems\nanywhere in between these points as well.  At the end of Notes Set 2.4,\nfor example, we showed a design for a logic slice that compares \ntwo bits at a time.  In general, we can optimize logic for any \nnumber of bits and then apply multiple copies of the resulting\nlogic in space (a generalization of the bit-sliced approach),\nor in time (a generalization of the serialization approach),\nor in a combination of the two.\n\nSometimes these tradeoffs may happen at a higher level.\nAs mentioned in Notes Set 2.3, computer software uses \nthe carry out of an adder to perform addition of larger \ngroups of bits (over multiple clock cycles)\nthan is supported by the processor's adder hardware.\n\nIn computer system design, engineers often design hardware elements\nthat are general enough to support this kind of extension in software.\n\nAs a concrete example of the possible tradeoffs,\nconsider a serial comparator design based on the {2-bit} slice variant.\n\nThis approach leads to a serial design with 24 gates and 10 inverters, \nwhich is not much\nlarger than our earlier serial design.  In terms of gate delays,\nhowever, the new design is identical, meaning that we finish a comparison\nin half the time.  More realistic area and timing metrics show\nslightly more difference between the two designs.  These differences can \ndominate the \nresults if we blindly scale the idea to handle more bits without \nthinking carefully about the design.  Neither many-input gates\nnor gates driving many outputs work well in practice.\n\n\n\n \n Question: What is the longest path through the serial comparator? \n Answer: \n\nA. The serial comparator design uses a flip-flop to store the result of each comparison bit slice. The selection logic is used to determine when the flip-flop should be updated based on the inputs A and B. The gate delays for the selection logic must be counted in order to accurately determine the longest path through the serial comparator."}, {"text": "Title: Finite State Machines \n Text: {Finite State Machines}\n\nA { finite state machine} (or { FSM}) is a model for understanding\nthe behavior of a system by describing the system as occupying one of a\nfinite set of states, moving between these states in response to\nexternal inputs, and producing external outputs.  In any given state,\na particular input may cause the FSM to move to another state; this \ncombination is called a { transition rule}.\n\nAn FSM comprises five parts: a finite set of states, a set of \npossible inputs, a set of possible outputs, a set of transition rules,\nand methods for calculating outputs.  \n\nWhen an FSM is implemented as a digital system, all states must be\nrepresented as patterns using a fixed number of bits, all inputs must\nbe translated into bits, and all outputs must be translated into bits.\n\nFor a digital FSM, transition rules must be { complete}; in other words,\ngiven any state of the FSM, and any pattern of input bits,\na transition must be defined from that state to another state \n(transitions from a state to itself, called { self-loops}, \nare acceptable).\n\nAnd, of course, calculation of outputs for a digital FSM reduces\nto Boolean logic expressions.\n\nIn this class, we focus on clocked synchronous FSM implementations,\nin which the FSM's internal state bits are stored in flip-flops.\n\nIn this section, we introduce the tools used to describe, develop, and\nanalyze implementations of FSMs with digital logic.  In the next\nfew weeks, we will show you how an FSM can serve as the central control\nlogic in a computer.  At the same time, we will illustrate connections\nbetween FSMs and software and will make some connections with other\nareas of interest in ECE, such as the design and analysis of digital \ncontrol systems.\n\nThe table below gives a { list of abstract states} for a typical \nkeyless entry system for a car.  In this case, we have merely named the\nstates rather than specifying the bit patterns to be used for each\nstate---for this reason, we refer to them as abstract states.\nThe description of the states in the first column is an optional element\noften included in the early design stages for an FSM, when identifying\nthe states needed for the design.\nA list may also include the outputs for each state.  Again, in\nthe list below, we have specified these outputs abstractly.\n\nBy including outputs for each state,\nwe implicitly assume that outputs depend only on\nthe state of the FSM.  We discuss this assumption in more detail\nlater in these notes (see ``Machine Models''), \nbut will make the assumption throughout our class.\n\n\n\n{\n\nmeaning& state& driver's door& other doors& alarm on \nvehicle locked& LOCKED& locked& locked& no\ndriver door unlocked& DRIVER& unlocked& locked& no\nall doors unlocked& UNLOCKED& unlocked& unlocked& no\nalarm sounding& ALARM& locked& locked& yes\n\n}\n\n\n\nAnother tool used with FSMs is the { next-state table} (sometimes\ncalled a { state transition table}, or just a { state table}), \nwhich maps the current state and input combination into the next state of \nthe FSM.  The abstract variant shown below outlines desired behavior at\na high level, and is often ambiguous, incomplete, and even inconsistent.\nFor example, what happens if a user pushes two buttons?  What happens\nif they push unlock while the alarm is sounding?  These questions \nshould eventually be considered.  However, we can already start to see \nthe intended use of the design: starting from a locked car, a user\ncan push ``unlock'' once to gain entry to the driver's seat, or push\n``unlock'' twice to open the car fully for passengers.  To lock the\ncar, a user can push the ``lock'' button at any time.  And, if a user\nneeds help, pressing the ``panic'' button sets off an alarm. \n\n\n\n{\n\nstate& action/input& next state \nLOCKED& push ``unlock''& DRIVER\nDRIVER& push ``unlock''& UNLOCKED\n(any)& push ``lock''& LOCKED\n(any)& push ``panic''& ALARM\n\n}\n\n\n\n\nA { state transition diagram} (or { transition diagram}, or\n{ state diagram}),\nas shown to the right, illustrates the\ncontents of the next-state table graphically, with each state drawn\nin a circle, and arcs between states labeled with the input combinations\nthat cause these transitions from one state to another.\n\nPutting the FSM design into this graphical form does not solve the\nproblems with the abstract model.  The questions that we asked\nin regard to the next-state table remain unanswered.\n\nImplementing an FSM using digital logic requires that we translate\nthe design into bits, eliminate any ambiguity, and complete the\nspecification.  How many internal bits should we use?  What are the\npossible input values, and how are their meanings represented in bits? \nWhat are the possible output values, and how are their meanings\nrepresented in bits?  We will consider these questions for several\nexamples in the coming weeks.\n\n\n{file=part3/figs/ke-abs-trans-diag.eps,width=3.45in}\n\n\nFor now, we simply define answers for our example design, the keyless\nentry system.  Given four states, we need at \nleast _2(4)=2 bits of internal state, which we\nstore in two flip-flops and call S_1S_0.  The table below lists input\nand output signals and defines their meaning. \n\n{\n\noutputs&D& driver door; 1 means unlocked\n&R& other doors (remaining doors); 1 means unlocked\n&A& alarm; 1 means alarm is sounding\ninputs& U& unlock button; 1 means it has been pressed\n& L& lock button; 1 means it has been pressed\n& P& panic button; 1 means it has been pressed\n\n}\n\nWe can now choose a representation for our states and rewrite the list\nof states, using bits both for the states and for the outputs.  We\nalso include the meaning of each state for clarity in our example.\nNote that we can choose the internal representation in any way.  Here\nwe have matched the D and R outputs when possible to simplify\nthe output logic needed for the implementation.  The order of states\nin the list is not particularly important, but should be chosen for\nconvenience and clarity (including transcribing bits into to {K-maps}, \nfor example).\n\n{\n\n& & & driver's door& other doors& alarm on\nmeaning& state& S_1S_0& D& R& A \nvehicle locked& LOCKED& 00& 0& 0& 0\ndriver door unlocked& DRIVER& 10& 1& 0& 0\nall doors unlocked& UNLOCKED& 11& 1& 1& 0\nalarm sounding& ALARM& 01& 0& 0& 1\n\n}\n\n\n\nWe can also rewrite the next-state table in terms of bits.  We use\nGray code order on both axes, as these orders make it more \nconvenient to use {K-maps}.  The values represented in this table \nare the next FSM state given the current state S_1S_0 and the\ninputs U, L, and P.  Our symbols for the next-state bits\nare S_1^+ and S_0^+.  The ``+'' superscript\nis a common way of expressing\nthe next value in a discrete series, here induced by the use of\nclocked synchronous logic in implementing the FSM.  In other words,\nS_1^+ is the value of S_1 in the next clock cycle, and S_1^+\nin an FSM implemented as a digital system is a Boolean expression\nbased on the current state and the inputs.  For our example problem, we\nwant to be able to write down expressions for S_1^+(S_1,S_0,U,L,P) \nand S_1^+(S_1,S_0,U,L,P), as well as expressions for the \noutput logic D(S_1,S_0), R(S_1,S_0), and A(S_1,S_0). \n\n\n{\n{c|cccccccc}\ncurrent state & {|c}{ULP}\nS_1S_0& 000& 001& 011& 010& 110& 111& 101& 100 \n00& 00& 01& 01& 00& 00& 01& 01& 10\n01& 01& 01& 01& 00& 00& 01& 01& 01\n11& 11& 01& 01& 00& 00& 01& 01& 11\n10& 10& 01& 01& 00& 00& 01& 01& 11 \n\n}\n\n\nIn the process of writing out the next-state table, \nwe have made decisions for all of the questions that we asked earlier\nregarding the abstract state table.  These decisions are also reflected\nin the complete state transition diagram shown to the right.\nThe states have been extended with state bits and output bits,\nas S_1S_0/DRA.\nYou should recognize that we can\nalso leave some questions unanswered by placing x's (don't cares)\ninto our table.  However, you should also understand at this point\nthat any implementation will produce bits, not x's, so we must be\ncareful not to allow arbitrary choices unless any of the choices\nallowed is indeed acceptable for our FSM's purpose.  We will discuss\nthis process and the considerations necessary as we cover more FSM\ndesign examples.\n\n\n{file=part3/figs/ke-trans-diag.eps,width=4.2in}\n\n\nWe have deliberately omitted calculation of expressions for the\nnext-state variables S_1^+ and S_0^+,\nand for the outputs D,\nR, and A.  We expect that you are able to do so from the \ndetailed state table above, and may assign such an exercise as\npart of your homework.\n\n\n \n Question: What is a finite state machine? \n Answer: \n\nA finite state machine is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs."}, {"text": "Title: Synchronous Counters \n Text: {Synchronous Counters}\n\nA { counter} is a clocked sequential circuit with a state diagram\nconsisting of a single logical cycle.\n\nNot all counters are synchronous.\nIn other words, not all flip-flops in a counter are required to use \nthe same clock signal.  A counter in which all flip-flops do \nutilize the same clock\nsignal is called a { synchronous counter}.  \n\nExcept for a brief introduction to other types of counters in the\nnext section, our class focuses entirely on clocked synchronous designs,\nincluding counters.\n\n {The definition ignores transition\n arcs resulting from functionalit such as a reset signal that\n forces the counter back into an initial state, \n\n\nThe design of synchronous counter circuits is a fairly straightforward\nexercise given the desired cycle of output patterns. \n\nThe task can be more complex if the internal state bits\nare allowed to differ from the output bits, so for now\nwe assume that output Z_i is equal to internal bit S_i.\nNote that distinction between internal states and outputs is necessary \nif any output pattern appears more than once in the desired cycle. \n\nThe cycle of states shown to the right corresponds to the\nstates of a {3-bit} binary counter.  The numbers in the states\nrepresent both internal state bits S_2S_1S_0 and output bits Z_2Z_1Z_0.\nWe transcribe this diagram into the next-state table shown on the left below.\nWe then write out {K-maps} for\nthe next state bits S_2^+, S_1^+, and S_0^+, as shown to the right,\nand use the {K-maps} to find expressions for these variables in\nterms of the current state.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n{\n\n{ccc|ccc}\nS_2& S_1& S_0& S_2^+& S_1^+& S_0^+ \n0& 0& 0& 0& 0& 1\n0& 0& 1& 0& 1& 0\n0& 1& 0& 0& 1& 1\n0& 1& 1& 1& 0& 0\n1& 0& 0& 1& 0& 1\n1& 0& 1& 1& 1& 0\n1& 1& 0& 1& 1& 1\n1& 1& 1& 0& 0& 0\n\n\n\n{file=part3/figs/sbin3-s2.eps,width=1in}\n{file=part3/figs/sbin3-s1.eps,width=1in}\n{file=part3/figs/sbin3-s0.eps,width=1in} {-10pt}\n{eqnarray*}\nS_2^+ =& {S_2}S_1S_0+S_2{S_1}+S_2{S_0} &= S_2(S_1S_0) \nS_1^+ =& S_1{S_0}+{S_1}S_0 &= S_1{S_0} \nS_0^+ =& {S_0} &= S_0 \n{eqnarray*}\n\n}\n\nThe first form of the expression for each next-state variable is\ntaken directly from the corresponding {K-map}.  We have rewritten\neach expression to make the emerging pattern more obvious.\n\nWe can also derive the pattern intuitively by asking the following:\n\ngiven a binary counter in state S_{N-1}S_{N-2} S_{j+1}S_jS_{j-1}\nS_1S_0, when does S_j change in the subsequent state?  The answer,\nof course, is that S_j changes when all of the\nbits below S_j are 1.  Otherwise, S_j remains the\nsame in the next state.  We thus \nwrite S_j^+=S_j (S_{j-1} S_1S_0) and\nimplement the counter as shown below for a {4-bit} design.\nNote that the \nusual order of\noutput bits along the bottom is reversed in the figure, with the most\nsignificant bit at the right rather than the left.\n\n{{file=part3/figs/ser-gating.eps,width=4.5in}}\n\nThe calculation of the left inputs to the XOR gates in the counter \nshown above is performed with a series of two-input AND gates.  Each \nof these gates AND's\nanother flip-flop value into the product.  This\napproach, called { serial gating}, implies that an {N-bit}\ncounter requires more than {N-2} gate delays\nto settle into the next state.  An alternative approach, called\n{ parallel gating}, calculates each input independently with a\nsingle logic gate, as shown below.  The blue inputs to the AND gate\nfor S_3 highlight the difference from the previous figure (note that\nthe two approaches differ only for bits S_3 and above).\nWith parallel gating, the \n{ fan-in} of the gates (the number of inputs)\nand the { fan-out} of the flip-flop outputs (number of other gates\ninto which an output feeds) grow with the size of the counter.  \nIn practice, large counters use a combination of these two approaches.\n\n{{file=part3/figs/par-gating.eps,width=4.5in}}\n\n\n \n Question: What is the main difference between a synchronous and an asynchronous counter? \n Answer: \n\nA. The main difference between a synchronous and an asynchronous counter is that all flip-flops in a synchronous counter are required to use the same clock signal, while this is not the case for an asynchronous counter."}, {"text": "Title: Ripple Counters \n Text: {Ripple Counters}\n\nA second class of\ncounter drives some of its flip-flops with a clock signal and feeds\nflip-flop outputs into the clock inputs of its remaining flip-flops,\npossibly through additional logic.  Such a counter is called a {\nripple counter}, because the effect of a clock edge ripples through\nthe flip-flops.  The delay inherent to the ripple effect, along with\nthe complexity of ensuring that timing issues do not render the design\nunreliable, are the major drawbacks of ripple counters.  Compared with\nsynchronous counters, however, ripple counters consume less energy,\nand are sometimes used for devices with restricted energy supplies.\n\n\nGeneral ripple counters\ncan be tricky because of timing issues, but certain types are easy.\n\nConsider the design of binary ripple counter.  The state diagram for \na {3-bit} binary counter is replicated to the right.\nLooking\nat the states, notice that the least-significant bit alternates with\neach state, while higher bits flip whenever the next smaller bit (to\nthe right) transitions from one to zero.  To take advantage of these\nproperties, we use positive edge-triggered D flip-flops with\ntheir complemented () outputs wired back to their inputs.\nThe clock input is fed only into the first\nflip-flop, and the complemented output of each flip-flop is also\nconnected to the clock of the next.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n\nAn implementation of a {4-bit} binary ripple counter appears to the\nright.\nThe order of bits in the figure matches the order used for our synchronous\nbinary counters: least significant on the left, most significant on the\nright.\nAs you can see from the figure, the technique generalizes to arbitrarily \nlarge binary ripple coun-\n\n\n\n\n\nters, but the\ntime required for the outputs to settle after a clock edge scales with\nthe number of flip-flops in the counter.  On the other hand, an\naverage of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n), which reduces the power requirements.{Recall that\nflip-flops record the clock state internally.  The logical activity\nrequired to record such state consumes energy.}\n\n\n\nBeginning with the state 0000, at the rising clock edge, the left (S_0)\nflip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\nfalling clock edge and does nothing, leaving the counter in\nstate 0001.  When the next rising clock edge arrives, the left\nflip-flop toggles back to 0, which the second flip-flop sees as a\nrising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\nsees the second flip-flop's change as a falling edge and does nothing,\nand the state settles as 0010.  We leave verification of the remainder \nof the cycle as an exercise.\n\n \n Question: What is the ripple effect? \n Answer: \n\nA. A ripple counter is a type of digital counter which uses a clock signal to trigger the flip-flops and feed the outputs of those flip-flops back into the clock inputs of other flip-flops. This type of counter is called a ripple counter because the effect of a clock edge ripples through the flip-flops."}, {"text": "Title: Timing Issues* \n Text: {Timing Issues*}\n\nRipple counters are a form of a more general strategy known as clock\ngating.{Fall 2012 students: This part may seem a little\nredundant, but we're going to remove the earlier mention of clock \ngating in future semesters.}\n{ Clock gating} uses logic to control the visibility of a\nclock signal to flip-flops (or latches).  Historically, digital system\ndesigners rarely used clock gating techniques because of the \ncomplexity introduced for the circuit designers, who must ensure \nthat clock edges are\ndelivered with little skew along a dynamically changing set of\npaths to flip-flops.  Today, however, the power benefits of hiding\nthe clock signal from flip-flops have made clock gating an attractive \nstrategy.\nNevertheless, digital logic designers and computer architects still almost\nnever use clock gating strategies directly.  In most of the industry,\nCAD tools insert logic for clock gating automatically.  \nA handful of companies (such as Intel and Apple/Samsung) design \ncustom circuits rather than relying on CAD tools to synthesize \nhardware designs from standard libraries of elements.\nIn these companies, clock gating is used widely by the circuit\ndesign teams, and some input is occasionally necessary from the \nhigher-level designers.\n\nMore aggressive gating strategies are also used in modern designs, but\nthese usually require more time to transition between the on and off \nstates and can be more\ndifficult to get right automatically (with the tools), hence\nhardware designers may need to provide high-level information about\ntheir designs.  A flip-flop that does not see any change in its clock\ninput still has connections to high voltage and ground, and thus allows\na small amount of { leakage current}.  In contrast,\nwith { power gating}, the voltage difference is removed, and the \ncircuit uses no power at all.  Power gating can be tricky---as you know,\nfor example, when you turn the power on, you need to make sure that\neach latch settles into a stable state.  Latches may need to be \ninitialized to guarantee that they settle, which requires time after\nthe power is restored.\n\nIf you want a deeper understanding of gating issues, take ECE482, \nDigital Integrated Circuit Design, or ECE527, System-on-a-Chip Design.\n\n\n \n Question: What is the purpose of power gating? \n Answer: \n\nA. Clock gating uses logic to control the visibility of a clock signal to flip-flops (or latches). Power gating uses logic to control the voltage difference between the flip-flops (or latches)."}, {"text": "Title: Machine Models \n Text: {Machine Models}\n\nBefore we dive fully into FSM design, we must point out that we have\nplaced a somewhat artificial restriction on the types of FSMs that\nwe use in our course.\n\nHistorically, this restriction was given a name, and machines of the type\nthat we have discussed are called Moore machines.\n\nHowever, outside of introductory classes, almost no one cares about\nthis name, nor about the name for the more general model used almost\nuniversally in hardware design, Mealy machines.\n\nWhat is the difference?  In a { Moore machine}, outputs depend only on\nthe internal state bits of the FSM (the values stored in the flip-flops).\nIn a { Mealy machine}, outputs\nmay be expressed as functions both of internal state and FSM inputs.\n\nAs we illustrate shortly, the benefit of using input signals to calculate\noutputs (the Mealy machine model) is that input bits effectively serve \nas additional system state, which means that the number of internal \nstate bits can be reduced.\n\nThe disadvantage of including input signals in the expressions for \noutput signals is that timing characteristics of input signals may not\nbe known, whereas an FSM designer may want to guarantee certain\ntiming characteristics for output signals.\n\nIn practice, when such timing guarantees are needed, the designer simply\nadds state to the FSM to accommodate the need, and the problem is solved.\n\nThe coin-counting FSM that we designed for our class' lab assignments,\nfor example, \nrequired that we use a Moore machine model to avoid sending the\nservo controlling the coin's path an output pulse that was too short\nto enforce the FSM's decision about which way to send the coin.\n\nBy adding more states to the FSM, we were able to hold the servo in\nplace, as desired.\n\nWhy are we protecting you from the model used in practice?\n\nFirst, timing issues add complexity to a topic that is complex enough \nfor an introductory course.\n\nAnd, second, most software FSMs are Moore machines, so the abstraction\nis a useful one in that context, too.\n\nIn many design contexts, the timing issues implied by a Mealy model\ncan be relatively simple to manage.  When working in a single clock\ndomain, all of the input signals come from flip-flops in the same \ndomain, and are thus stable for most of the clock cycle.  Only rarely\ndoes one need to keep additional state to improve timing characteristics\nin these contexts.  In contrast, when interacting across clock domains,\nmore care is sometimes needed to ensure correct behavior.\n\nWe now illustrate the state reduction benefit of the Mealy machine\nmodel with a simple example, an FSM that recognizes the \npattern of a 0 followed by a 1 on a single input and outputs\na 1 when it observes the pattern.\n\nAs already mentioned,\nMealy machines often require fewer flip-flops.\nIntuitively, the number of combinations of states and\ninputs is greater than the number of combinations of states alone, and\nallowing a function to depend on inputs reduces the number of internal\nstates needed.  \n\nA Mealy implementation of the FSM appears on the left below, and\nan example timing diagram illustrating the FSM's behavior is shown on\nthe right.\n\nThe machine shown below occupies state A when the last bit seen was a 0, and\nstate B when the last bit seen was a 1.\n\nNotice that the transition arcs in the state diagram\nare labeled with two values instead\nof one.  Since outputs can depend on input values as well as state,\ntransitions in a Mealy machine are labeled with input/output\ncombinations, while states are labeled only with their internal bits\n(or just their names, as shown below).  Labeling states with outputs\ndoes not make sense for a Mealy machine, since outputs may vary\nwith inputs.\n\nNotice that the outputs indicated on any given transition\nhold only until that transition is taken (at the rising clock edge), as is\napparent in the timing diagram.  When inputs are asynchronous, \nthat is,\nnot driven by the same clock signal, output pulses from a Mealy\nmachine can be arbitrarily short, which can lead to problems.\n\n\n\n{{file=part3/figs/lec17-3.eps,width=5in}}\n\n\n\nFor a Moore machine, we must create a special state in which the\noutput is high.  Doing so requires that we split state B into two\nstates, a state C in which the last two bits seen were 01, and a\nstate D in which the last two bits seen were 11.  Only state C\ngenerates output 1.  State D also becomes the starting state for the\nnew state machine.  The state diagram on the left below illustrates \nthe changes, using the transition diagram style that we introduced \nearlier to represent Moore machines.\nNotice in the associated timing diagram that the output pulse lasts a\nfull clock cycle.\n\n\n\n{{file=part3/figs/lec17-4.eps,width=5in}}\n\n\n\n\n\n \n Question: Why is the Mealy machine model more reliable in practice? \n Answer: \n\nA. There are several reasons for this. The main reason is that the Mealy machine model can be implemented with fewer flip-flops than the Moore machine model, which reduces the cost of hardware implementation. Additionally, the Mealy machine model is more flexible than the Moore machine model, and can be more easily adapted to changes in the input or output requirements. Finally, the Mealy machine model is more resistant to timing issues than the Moore machine model, which makes it more reliable in practice."}, {"text": "Title: Summary of Part 3 of the Course \n Text: {Summary of Part 3 of the Course}\n\n\nIn this short summary, we \ngive you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nStudents often find this part of the course more challenging than the\nearlier parts of the course.\n\nIn addition to these notes, you should read Chapters 4 and 5 of the \nPatt and Patel textbook, which cover the von Neumann\nmodel, instruction processing, and ISAs.  \n\nStudents typically find that the homeworks in this part of the course\nrequire more time than did those in earlier parts of the course.\nProblems on the exam will be similar in nature but designed to require \nless actual time to solve (assuming that you have been doing the homeworks).  \n\nWe'll start with the easy stuff.  \n\nYou should recognize all of these terms and be able\nto explain what they mean.  For the specific circuits, you should be able \nto draw them and explain how they work.  Actually, we don't care whether \nyou can draw something from memory---a mux, for example---provided that \nyou know what a mux does and can derive a gate diagram correctly for one \nin a few minutes.  Higher-level skills are much more valuable.\n\n[t]\n{}{{}{}\n{}{}{}\n\n{digital systems terms\n{--}{{}{}\n{}{}{}\n module\n fan-in\n fan-out\n machine models: Moore and Mealy\n\n}\n\n{simple state machines\n{--}{{}{}\n{}{}{}\n synchronous counter\n ripple counter\n serialization (of bit-sliced design)\n\n}\n\n{finite state machines (FSMs)\n{--}{{}{}\n{}{}{}\n states and state representation\n transition rule\n self-loop\n next state (+) notation\n meaning of don't care in input  combination\n meaning of don't care in output\n unused states and initialization\n completeness (with regard to  FSM specification)\n list of (abstract) states\n next-state table/state transition table/state table\n state transition diagram/transition  diagram/state diagram\n\n}\n\n{memory\n{--}{{}{}\n{}{}{}\n number of addresses\n addressability\n read/write logic\n serial/random access memory (RAM)\n volatile/non-volatile (N-V)\n static/dynamic RAM (SRAM/DRAM)\n SRAM cell\n DRAM cell\n design as a collection of cells\n coincident selection\n bit lines and sense amplifiers\n\n}\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann model\n{--}{{}{}\n{}{}{}\n{processing unit\n{--}{{}{}\n{}{}{}\n register file\n arithmetic logic unit (ALU)\n word size\n\n}\n{control unit\n{--}{{}{}\n{}{}{}\n program counter (PC)\n instruction register (IR)\n implementation as FSM\n\n}\n input and output units\n{memory\n{--}{{}{}\n{}{}{}\n memory address register (MAR)\n memory data register (MDR)\n\n}\n{processor datapath}\n\n{control signal}\n\n}\n\n{tri-state buffer\n{--}{{}{}\n{}{}{}\n meaning of Z/hi-Z output\n use in distributed mux\n\n}\n\n{instruction processing}\n{-}{{}{}\n{}{}{}\n\n\n\n{register transfer language (RTL)}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (of an encoded instruction)}\n{operation code (opcode)}\n{types of instructions}\n{-}{{}{}\n{}{}{}\n\n{data movement}\n{control flow}\n\n{addressing modes}\n{-}{{}{}\n{}{}{}\n\n\n{PC-relative}\n\n{base + offset}\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Transform a bit-sliced design into a serial design, and explain the \ntradeoffs involved in terms of area and time required to compute a result.}\n{Based on a transition diagram, implement a synchronous counter from \nflip-flops and logic gates.}\n{Implement a binary ripple counter (but not necessarily a more general \ntype of ripple counter) from flip-flops and logic gates.}\n{Given an FSM implemented as digital logic, analyze the FSM to produce \na state transition diagram.}\n{Design an FSM to meet an abstract specification for a task, including \nproduction of specified output signals, and possibly including selection \nof appropriate inputs.}\n{Complete the specification of an FSM by ensuring that each state \nincludes a transition rule for every possible input combination.}\n{Compose memory chips into larger memory systems, using additional\ndecoders when necessary.}\n{Encode {LC-3} instructions into machine code.}\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n\nAt a higher level, we expect that you understand the concepts and ideas \nsufficiently well to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Abstract design symmetries from an FSM specification in order to \nsimplify the implementation.}\n{Make use of a high-level state design, possibly with many sub-states \nin each high-level state, to simplify the implementation.}\n{Use counters to insert time-based transitions between states (such \nas timeouts).}\n{Implement an FSM using logic components such as registers, \ncounters, comparators, and adders as building blocks.}\n{Explain the basic organization of a computer's microarchitecture\nas well as the role played by elements of a von Neumann design in the\nprocessing of instructions.}\n{Identify the stages of processing an instruction (such as fetch,\ndecode, getting operands, execution, and writing back results) in a \nprocessor control unit state machine diagram.}\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the difference between the Moore and Mealy machine models, \nas well as why you might find each of them useful when designing an FSM.}\n{Understand the need for initialization of an FSM, be able to analyze \nand identify potential problems arising from lack of initialization, and \nbe able to extend an implementation to include initialization to an \nappropriate state when necessary.}\n{Understand how the choice of internal state bits for an FSM can \naffect the complexity of the implementation of next-state and output \nlogic, and be able to select a reasonable state assignment.}\n{Identify and fix design flaws in simple FSMs by analyzing an existing \nimplementation, comparing it with the specification, and removing any \ndifferences by making any necessary changes to the implementation.}\n\n\n\n\n\n{   }  empty 3rd page\n\n\n\n\n\n \n Question: What are the differences between the two models? \n Answer: \n\nA. The Moore and Mealy machine models differ in how they generate output signals. Moore machines generate outputs only on state transitions, while Mealy machines generate outputs on every clock cycle, based on the current input and state."}, {"text": "Title: Instruction Set Architecture* \n Text: {Instruction Set Architecture*}\n\nThis set of notes discusses \ntradeoffs and design elements of instruction set architectures (ISAs).\n{ The material is beyond the scope of our class, and is provided purely\nfor your interest.}  Those who find these topics interesting\nmay also want to read the ECE391 notes, which describe similar material\nwith a focus on the x86 ISA.\n\nAs you know, the ISA\ndefines the interface between software and hardware, abstracting the\ncapabilities of a computer's datapath and standardizing the format of\ninstructions to utilize those capabilities.  Successful ISAs are\nrarely discarded, as success implies the existence of large amounts of\nsoftware built to use the ISA.  Rather, they are extended, and their\noriginal forms must be supported for decades (consider, for example,\nthe IBM 360 and the Intel x86).  Employing sound design principles\nis thus imperative in an ISA.\n\n \n Question: What are some design principles that are important to consider when designing an ISA? \n Answer: \n\nA. Some design principles that are important to consider when designing an ISA include:\n\n-Ease of use: the ISA should be easy for programmers to use and understand.\n\n-Efficiency: the ISA should be designed to be efficient in terms of both execution time and memory usage.\n\n-Compatibility: the ISA should be compatible with existing software and hardware."}, {"text": "Title: Formats and Fields* \n Text: {Formats and Fields*}\n\nThe LC-3 ISA \nemploys fixed-length instructions and a load-store architecture, two\naspects that help to reduce the design space to a manageable set of\nchoices.  In a general ISA design, many other options exist for\ninstruction formats.\n\nRecall the idea of separating the bits of an instruction into\n(possibly non-contiguous) fields.  One of the fields must contain an\nopcode, which specifies the type of operation to be performed by the\ninstruction.  In the {LC-3} ISA, most opcodes specify both\nthe type of operation and the types of arguments to the operation.  \nMore generally, many addressing modes are possible for\neach operand, and we can think of the bits\nthat specify the addressing mode as a separate field, known as the\n{ mode} field.  \n\nAs a simple example, the {LC-3's} ADD and AND instructions \ncontain a {1-bit} mode field that specifies whether the second\noperand of the ADD/AND comes from a register or is an immediate value.\n\nSeveral questions must be answered in order to define the possible\ninstruction formats for an ISA.  First, are instructions fixed-length\nor variable-length?  Second, how many addresses are needed for each\ninstruction, and how many of the addresses can be memory addresses?\nFinally, what forms of addresses are possible for each operand?\nFor example, can one use\nfull memory addresses or only limited offsets relative to a register?\n\nThe answer to the first question depends on many factors, but several\nclear advantages exist for both answers.  { Fixed-length\ninstructions} are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions.  As an example of this\nsimplification, note that the {LC-3} ISA does not support \naddressing for individual bytes, only for {16-bit} words.\n\n{ Variable-length instructions} also have benefits, however.\nVariable-length encodings allow more efficient encodings, saving both\nmemory and disk space.  A register transfer operation, for example,\nclearly requires fewer bits than addition of values at two direct\nmemory addresses for storage at a third.  Fixed-length instructions\nmust be fixed at the length of the longest possible instruction,\nwhereas variable-length instructions can use lengths appropriate to\neach mode.  The same tradeoff has another form in the sense that\nfixed-length ISAs typically eliminate many addressing modes in order\nto limit the size of the instructions.  Variable-length instructions\nthus allow more flexibility; indeed, extensions to a variable-length\nISA can incorporate new addressing modes that require longer\ninstructions without affecting the original ISA.\n\nFor example, the maximum length of x86 instructions has grown from six \nbytes in 1978 (the 8086 ISA) to fifteen bytes in today's version of the\nISA.\n\n\n\n\nMoving to the last of the three questions posed for instruction format\ndefinition, operand address specification,\nwe explore a range of answers developed over the last few\ndecades.  Answers are usually chosen based on the number of bits\nnecessary, and we use this metric to organize the possibilities.  The\nfigure below separates approaches into two dimensions: the vertical\ndimension divides addressing into registers and memory, and the\nhorizontal dimension into varieties within each type.\n\n{{file=part4/figs/lec23-1.eps,width=4in}}\n\nAs a register file contains fewer registers than a memory does words,\nthe use of register operands rather than memory addresses reduces the\nnumber of bits required to specify an operand.  The {LC-3} ISA\nuses a restricted set of addressing modes to stay within the limit\nimposed by the use of {16-bit} instructions.  Both\nregister and memory addresses, however, admit a wide range of\nimplementations.\n\n{ Implicit operands} of either type require no additional bits for\nthe implicit address.  The {LC-3} procedure call instruction, JSR,\nfor example, stores the return address in R7.  No bits in the JSR \nencoding name the R7 register; R7 is used implicitly for every JSR\nexecuted.\n\nSimilarly, the procedure call instructions in many ISAs push the \nreturn address onto a stack using an implicit register for the top \nof stack pointer.\n\nMemory addresses can also be implicitly equated to other memory\naddresses.  An increment instruction operating on a memory address, for\nexample, implicitly writes the result back to the same address.  \n\nAt the opposite extreme, an instruction may include a full address,\neither to any register in the register file or to any address in the \nmemory.  The term { general-purpose registers} indicates that \nregisters are used in any operation.\n\n{ Special-purpose registers}, in contrast, split the register file\nand allow only certain registers to be used in each operation.  For\nexample, the Motorola 680x0 series, used in early Apple\nMacintosh computers, provides distinct sets of address and data\nregisters.  Loads and stores use the address registers; arithmetic,\nlogic, and shift operations use the data registers.  As a result, \neach instruction\nselects from a smaller set of registers and thus requires fewer bits\nin the instruction to name the register for use.\n\nAs full memory addresses require many more bits than full register\naddresses, a wider range of techniques has been employed to reduce the\nlength.  ``Zero page'' addresses, as defined in the 6510 (6502) ISA\nused by Commodore PET's,{My computer in junior high school.}\nC64's,{My computer in high school.} and VIC 20's, prefixed a\none-byte address with a zero byte, allowing shorter instructions when\nmemory addresses fell within the first 256 memory locations.  Assembly\nand machine language programmers made heavy use of these locations to\nproduce shorter programs.  \n\nRelative addressing is quite common in the {LC-3} ISA, in which many\naddresses are PC-relative.  Typical commerical ISAs also make use of\nrelative addressing.  The Alpha ISA, for example, has a PC-relative form of\nprocedure call with a {21-bit} offset (plus or minus a megabyte),\nand the x86 ISA has a ``short'' form of branch instructions that\nuses an {8-bit} offset.\n\nSegmented memory is a form of relative addressing that uses a register\n(usually implicit) to provide the high bits of an address and an\nexplicit memory address (or another register) to provide the low bits.\nIn the early x86 ISAs, for example, {20-bit} addresses are\nfound by adding a {16-bit} segment register extended with four\nzero bits to a {16-bit} offset.\n\n\n\n\n\n \n Question: What is one advantage of using fixed-length instructions? \n Answer: \n\nA. Fixed-length instructions are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions."}, {"text": "Title: Addressing Architectures* \n Text: {Addressing Architectures*}\n\nOne question remains for the definition of instruction formats: how\nmany addresses are needed for each instruction, and how many of the\naddresses can be memory addresses?  The first part of this question\nusually ranges from zero to three, and is rarely allowed to go\nbeyond three.  The answer to the second part determines the {\naddressing architecture} implemented by an ISA.  We now illustrate the\ntradeoffs between five distinct addressing architectures through the\nuse of a running example, the assignment X=AB+C/D.\n\nA binary operator requires two source operands and one destination\noperand, for a total of three addresses.  The ADD instruction, for\nexample, has a { {3-address}} format:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B,C>; M[A]  M[B] + M[C]\n>or>ADD>R1,R2,R3>; R1  R2 + R3\n{-6pt}\n\nIf all three addresses can be memory addresses, the ISA is dubbed a\n{ memory-to-memory architecture}.  Such architectures may have\nsmall register sets or even lack a register file completely.  To\nimplement the assignment, we assume the availability of two memory\nlocations, T1 and T2, for temporary storage:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>MUL>T1,A,B>; T1  M[A] * M[B]\n>DIV>T2,C,D>; T2  M[C] / M[D]\n>ADD>X,T1,T2>; X  M[T1] + M[T2]\n{-6pt}\n\nThe assignment requires only three instructions to implement, but each\ninstruction contains three full memory addresses, and is thus quite long.\n\nAt the other extreme is the { load-store architecture} used by the\n{LC-3} ISA.  In a load-store architecture, only\nloads and stores can use memory addresses; all other operations use\nonly registers.  As most instructions use only registers, this type of\naddressing architecture is also called a { register-to-register\narchitecture}.  The example assignment translates to the code shown below,\nwhich assumes that R1, R2, and R3 are free for use (the instructions\nare { NOT} {LC-3} instructions, but rather a generic assembly\nlanguage for a load-store architecture). \n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>LD>R2,B>; R2  M[B]\n>MUL>R1,R1,R2>; R1  R1 * R2\n>LD>R2,C>; R2  M[C]\n>LD>R3,D>; R3  M[D]\n>DIV>R2,R2,R3>; R2  R2 / R3\n>ADD>R1,R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nEight instructions are necessary, but no instruction requires more\nthan one full memory address, and several use only register addresses,\nallowing the use of shorter instructions.  The need to move data in\nand out of memory explicitly, however, also requires a reasonably\nlarge register set, as is available in the ARM, Sparc, Alpha, and IA-64\nISAs.  \n\nArchitectures that use other combinations of memory and register\naddresses with {3-address} formats are not named.  Unary\noperators and transfer operators require only one source operand, thus\ncan use a {2-address} format (for example, NOT A,B).  Binary operations\ncan also use { {2-address}} format if one operand is implicit,\nas in the following instructions:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B>; M[A]  M[A] + M[B]\n>or>ADD>R1,B>; R1  R1 + M[B]\n{-6pt}\n\nThe second instruction, in which one address is a register and the\nsecond is a memory address, defines a { register-memory\narchitecture}.  As shown by the code on the next page, \nsuch architectures strike a balance\nbetween the two architectures just discussed.\n\n\n\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>MUL>R1,B>; R1  R1 * M[B]\n>LD>R2,C>; R2  M[C]\n>DIV>R2,D>; R2  R2 / M[D]\n>ADD>R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nThe assignment now requires six instructions using at most one memory\naddress each; like memory-to-memory architectures, register-memory\narchitectures use relatively few registers.  Note that two-register\noperations are also allowed.  Intel's x86 ISA is a register-memory\narchitecture.\n\nSeveral ISAs of the past{The 6510/6502 as well, if memory\nserves, as the 8080, Z80, and Z8000, which used to drive parlor video\ngames.}  used a special-purpose register called the accumulator for\nALU operations, and are called { accumulator architectures}.  The\naccumulator in such architectures is implicitly both a source and the\ndestination for any such operation, allowing a { {1-address}}\nformat for instructions, as shown below.\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>B>; ACC  ACC + M[B]\n>or>ST>E>; M[E]  ACC\n{-6pt}\n\nAccumulator architectures strike the same balance as register-memory\narchitectures, but use fewer registers.  Note that memory location X\nis used as a temporary storage location as well as the final storage\nlocation in the following code:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>A>; ACC  M[A]\n>MUL>B>; ACC  ACC * M[B]\n>ST>X>; M[X]  ACC\n>LD>C>; ACC  M[C]\n>DIV>D>; ACC  ACC / M[D]\n>ADD>X>; ACC  ACC + M[X]\n>ST>X>; M[X]  ACC\n{-6pt}\n\nThe last addressing architecture that we discuss is rarely used for\nmodern general-purpose processors, but may be familiar to\nyou because of its historical use in scientific and engineering calculators.\n\nA { stack architecture}\nmaintains a stack of values and draws all ALU operands from this\nstack, allowing these instructions to use a { {0-address}}\nformat.  A special-purpose stack pointer (SP) register points to the\ntop of the stack in memory, and operations analogous to load ({\npush}) and store ({ pop}) are provided to move values on and off\nthe stack.  To implement our example assignment, we first transform it\ninto postfix notation (also called reverse Polish notation):\n\n{-6pt}\n=\n>A  B  *  C  D  /  +\n{-6pt}\n\nThe resulting sequence of symbols transforms on a one-to-one basis\ninto instructions for a stack architecture:\n\n{-6pt}\n=WWWW=WW=; M[SP+1]  M[SP] + M[SP + 1], SP  SP + 1WW=AB+C/DW=AB+C/DW=\n>PUSH>A>; SP  SP - 1, M[SP]  M[A]>A\n>PUSH>B>; SP  SP - 1, M[SP]  M[B]>B>A\n>MUL>>; M[SP+1]  M[SP+1] * M[SP], SP  SP + 1>AB\n>PUSH>C>; SP  SP - 1, M[SP]  M[C]>C>AB\n>PUSH>D>; SP  SP - 1, M[SP]  M[D]>D>C>AB\n>DIV>>; M[SP+1]  M[SP+1] / M[SP], SP  SP + 1>C/D>AB\n>ADD>>; M[SP+1]  M[SP+1] + M[SP], SP  SP + 1>AB+C/D\n>POP>X>; M[X]  M[SP], SP  SP + 1\n{-6pt}\n\nThe values to the right are the values on the stack, starting with the\ntop value on the left and progressing downwards, { after the\ncompletion of each instruction}.\n\n\n\n\n\n \n Question: What is the difference between a memory-to-memory and a load-store architecture? \n Answer: \n\n\nA. In a memory-to-memory architecture, all operations can use memory addresses, whereas in a load-store architecture, only loads and stores can use memory addresses."}, {"text": "Title: Common Special-Purpose Registers* \n Text: {Common Special-Purpose Registers*}\n\nThis section illustrates the uses of special-purpose registers through\na few examples. \n\nThe { stack pointer (SP)} points to the top of the stack in memory.\nMost older architectures support push and pop operations that\nimplicitly use the stack pointer.  Modern architectures assign a\ngeneral-purpose register to be the stack pointer and reference it\nexplicitly, although an assembler may support\ninstructions that appear to use implicit operands but in fact\ntranslate to machine instructions with explicit reference to the\nregister defined to be the SP.\n\nThe { program counter (PC)} points to the next instruction to be\nexecuted.  Some modern architectures expose it as a general-purpose\nregister, although its distinct role in the implementation keeps such\na model from becoming as common as the use of a general-purpose\nregister for the SP.\n\nThe { processor status register (PSR)}, also known as the {\nprocessor status word (PSW)}, contains all status bits as well as a\nmode bit indicating whether the processor is operating in user mode or\nprivileged (operating system) mode.  Having a register with this\ninformation allows more general access than is possible solely through\nthe use of control flow instructions.\n\nThe { zero register} appears in modern architectures of the RISC\nvariety (defined in the next section of these notes).  The register is\nread-only and serves both as a useful constant and as a destination\nfor operations performed only for their side-effects (for example, setting\nstatus bits).  The availability of a zero register also allows certain\nopcodes to serve double duty.  A register-to-register add instruction\nbecomes a register move instruction when one source operand is zero.\nSimilarly, an immediate add instruction becomes an immediate load\ninstruction when one source operand is zero.\n\n\n \n Question: What is the purpose of the zero register? \n Answer: \n\nA. The zero register is used as a constant and as a destination for operations performed only for their side-effects."}, {"text": "Title: Reduced Instruction Set Computers* \n Text: {Reduced Instruction Set Computers*}\n\nBy the mid-1980's, the VAX architecture dominated the workstation and\nminicomputer markets, which included most universities.  Digital\nEquipment Corporation, the creator of the VAX, was second only to IBM\nin terms of computer sales.  VAXen, as the machines were called, used\nmicroprogrammed control units and supported numerous addressing modes\nas well as complex instructions ranging from ``square root'' to\n``find roots of polynomial equation.''\n\nThe impact of increasingly dense integrated circuit technology had\nbegun to have its effect, however, and in view of increasing processor\nclock speeds, more and more programmers were using high-level\nlanguages rather than writing assembly code.  Although assembly\nprogrammers often made use of the complex VAX instructions, compilers\nwere usually unable to recognize the corresponding high-level language\nconstructs and thus were unable to make use of the instructions.\n\nIncreasing density also led to rapid growth in memory sizes, to the\npoint that researchers began to question the need for variable-length\ninstructions.  Recall that variable-length instructions allow shorter\ncodes by providing more efficient instruction encodings.  With the\ntrend toward larger memories, code length was less important.  The\nperformance advantage of fixed-length instructions, which simplifies\nthe datapath and enables pipelining, on the other hand, was \nattractive.\n\nResearchers leveraged these ideas, which had been floating around the\nresearch community (and had appeared in some commercial architectures)\nto create { reduced instruction set computers}, or { RISC}\nmachines.  The competing VAXen were labeled { CISC} machines, which\nstands for { complex instruction set computers}.\n\nRISC machines employ fixed-length instructions and a load-store\narchitecture, allowing only a few addressing modes and small offsets.\nThis combination of design decisions enables deep pipelines and\nmultiple instruction issues in a single cycle (termed superscalar\nimplementations), and for years, RISC machines were viewed by many\nresearchers as the proper design for future ISAs.  However, companies\nsuch as Intel soon learned to pipeline microoperations after decoding\ninstructions, and CISC architectures now offer competitive if not\nsuperior performance in comparison with RISC machines.  The VAXen are\ndead, of course,{Unless you talk with customer support\nemployees, for whom no machine ever dies.}  having been replaced by\nthe Alpha, which in turn fell to x86, which is now struggling with ARM\nto enter the mobile market.  \n\n\n \n Question: Why were RISC machines viewed as the proper design for future ISAs? \n Answer: \n\nA. RISC machines were viewed as the proper design for future ISAs because they employed fixed-length instructions and a load-store architecture, which allowed only a few addressing modes and small offsets. This combination of design decisions enabled deep pipelines and multiple instruction issues in a single cycle (termed superscalar implementations), and for years, RISC machines were viewed by many researchers as the proper design for future ISAs."}, {"text": "Title: Procedure and System Calls* \n Text: {Procedure and System Calls*}\n\nA { procedure} is a sequence of instructions that executes a\nparticular task.  Procedures are used as building blocks for multiple,\nlarger tasks.  The concept of a procedure is fundamental to\nprogramming, and appears in some form in every high-level language as\nwell as in most ISAs.\n\nFor our purposes, the terms procedure, subroutine,\nfunction, and method are synonymous, although they usually have\nslightly different meanings from the linguistic point of view.\nProcedure calls are supported through { call} and { return}\ncontrol flow instructions.  The first instruction in the code below,\nfor example, transfers control to the procedure ``DoSomeWork,'' which\npresumably does some work, then returns control to the instruction\nfollowing the call.\n\n{-6pt}\n\n=DoSomeWork:WW=WWWW=DoSomeWorkWW= \n>loop:>CALL>DoSomeWork\n>>CMP>R6,#1>; compare return value in R6 to 1\n>>BEQ>loop>; keep doing work until R6 is not 1\n\n>DoSomeWork:>>> ; set R6 to 0 when all work is done, 1 otherwise \n>>RETN\n\n{-6pt}\n\nThe procedure also places a return value in R6, which the instruction\nfollowing the call compares with immediate value 1.  Until the two are\nnot equal (when all work is done), the branch returns control to the\ncall and executes the procedure again.\n\nAs you may recall, the call and return use the stack pointer to keep\ntrack of nested calls.  Sample RTL for these operations appears below.\n\n\n{eqnarray*}\n{call RTL}&&SP  SP - 1\n&& M[SP]  PC\n&& PC  {procedure start}\n{eqnarray*}\n\n\n{eqnarray*}\n{return RTL}&&PC  M[SP]\n&&SP  SP + 1\n{eqnarray*}\n\n\nWhile an ISA provides the call and return instructions necessary to\nsupport procedures, it does not specify how information is passed to\nor returned from a procedure.  A standard for such decisions is\nusually developed and included in descriptions of the architecture,\nhowever.  This { calling convention} specifies how information is\npassed between a caller and a callee.  In particular, it specifies the\nfollowing: where arguments must be placed, either in registers or in\nspecific stack memory locations; which registers can be used or\nchanged by the procedure; and where any return value must be placed.\n\nThe term ``calling convention'' is also used in the programming\nlanguage community to describe the convention for deciding what\ninformation is passed for a given call operation.  For example, are\nvariables passed by value, by pointers to values, or in some other\nway?  However, once the things to be sent are decided, the\narchitectural calling convention that we discuss here is used\nto determine where to put the data in order for the callee to be able\nto find it.\n\nCalling conventions for architectures with large register sets\ntypically pass arguments in registers, and nearly all conventions\nplace the return value in a register.  A calling convention also\ndivides the register set into { caller-saved} and \n{ callee-saved} registers.  Caller-saved registers can be modified \narbitrarily\nby the called procedure, whereas any value in a callee-saved register\nmust be preserved.  Similarly, before calling a procedure, a caller\nmust preserve the values of any caller saved registers that are needed\nafter the call.  Registers of both types usually saved on the stack by\nthe appropriate code (caller or callee).\n\n\n\n\n\nA typical stack structure appears in the figure to the right.  In\npreparation for a call, a caller first stores any caller-saved\nregisters on the stack.  Arguments to the procedure to be called are\npushed next.  The procedure is called next, implicitly pushing the\nreturn address (the address of the instruction following the call\ninstruction).  Finally, the called procedure may allocate space on the\nstack for storage of callee-saved registers as well as local\nvariables.\n\nAs an example, the following calling convention can be applied to \nan {8-register} load-store architecture similar to the {LC-3}\nISA: the first three arguments must be placed in R0 through R2 (in order), \nwith any remaining arguments on the stack; the return value must be placed \nin R6; R0 through R2 are caller-saved, as\nis R6, while R3 through R5 are callee-saved; R7 is used as the stack\npointer.  The code fragments below use this calling convention to\nimplement a procedure and a call of that procedure.\n\n\n{file=part4/figs/lec23-2.eps,width=1.25in}\n\n\n[t]\n{\n[t]\n\nint =add3 (int n1, int n2, int n3) {\n>return (n1 + n2 + n3);\n}\n\nprintf (``d'', add3 (10, 20, 30));\n\n\n\nby convention:\n= n1 is in R0\n>n2 is in R1\n>n3 is in R2\n>return value is in R6\n\n\n\n[t]\n\nadd3:  = WWWW= WWWWW= \nadd3:>ADD>R0,R0,R1\n>ADD>R6,R0,R2\n>RETN\n>\n>PUSH>R1>; save the value in R1\n>LDI>R0,#10>; marshal arguments\n>LDI>R1,#20\n>LDI>R2,#30\n>CALL>add3\n>MOV>R1,R6>; return value becomes 2nd argument\n>LDI>R0,``d''>; load a pointer to the string\n>CALL>printf\n>POP>R1>; restore R1\n\n\n\nThe add3 procedure takes three integers as arguments, adds them\ntogether, and returns the sum.  The procedure is called with the\nconstants 10, 20, and 30, and the result is printed.  By the calling\nconvention, when the call is made, R0 must contain the value 10, R1\nthe value 20, and R2 the value 30.  We assume that the caller wants to\npreserve the value of R1, but does not care about R3 or R5.  In the\nassembly language version on the right, R1 is first saved to the\nstack, then the arguments are marshaled into position, and finally the\ncall is made.  The procedure itself needs no local storage and does\nnot change any callee-saved registers, thus must simply add the\nnumbers together and place the result in R6.  After add3 returns, its\nreturn value is moved from R6 to R1 in preparation for the call to\nprintf.  After loading a pointer to the format string into R0, the\nsecond call is made, and R1 is restored, completing the translation.\n\n{ System calls} are almost identical to procedure calls.  As with\nprocedure calls, a calling convention is used: before invoking a\nsystem call, arguments are marshaled into the appropriate registers or\nlocations in the stack; after a system call returns, any result\nappears in a pre-specified register.  The calling convention used for\nsystem calls need not be the same as that used for procedure calls.\nRather than a call instruction, system calls are usually initiated\nwith a { trap} instruction, and system calls are also known as\ntraps.  With many architectures, a system call places the processor in\nprivileged or kernel mode, and the instructions that implement the\ncall are considered to be part of the operating system.  The term\nsystem call arises from this fact.\n\n\n\n\n\n \n Question: What is the name of the procedure that is used to describe its operation? \n Answer: \nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. ["}, {"text": "Title: Interrupts and Exceptions* \n Text: {Interrupts and Exceptions*}\n\nUnexpected processor interruptions arise both from interactions\nbetween a processor and external devices and from errors or unexpected\nbehavior in the program being executed.  The term { interrupt} is\nreserved for asynchronous interruptions generated by other devices,\nincluding disk drives, printers, network cards, video cards,\nkeyboards, mice, and any number of other possibilities.  {\nExceptions} occur when a processor encounters an unexpected opcode or\noperand.  An undefined instruction, for example, gives rise to an\nexception, as does an attempt to divide by zero.  Exceptions usually\ncause the current program to terminate, although many operating\nsystems will allow the program to catch the exception and to handle it\nmore intelligently.  The table below summarizes the characteristics of\nthe two types and compares them to system calls.\n\n{\n{|l|l|l|c|c|}\n{|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected \ninterrupt& external device& packet arrived at network card& yes& yes\nexception& invalid opcode or operand& divide by zero& no& yes\ntrap/system call& deliberate, via trap instruction& print character to console& no& no \n\n}\n\nInterrupts occur asynchronously with respect to the program.  Most\ndesigns only recognize interrupts between instructions.  In other words, \nthe presence of interrupts is checked only after completing an instruction\nrather than in every cycle.  In pipelined designs, however,\ninstructions execute simultaneously, and the decision as to which\ninstructions occur ``before'' an interrupt and which occur ``after''\nmust be made by the processor.  Exceptions are not asynchronous in the\nsense that they occur for a particular instruction, thus no decision\nneed be made as to instruction ordering.  After determining which\ninstructions were before an interrupt, a pipelined processor discards\nthe state of any partially executed instructions that occur ``after''\nthe interrupt and completes all instructions that occur ``before.''\nThe terminated instructions are simply restarted after the interrupt\ncompletes.  Handling the decision, the termination, and the\ncompletion, however, significantly increases the design complexity of\nthe system.\n\nThe code associated with an interrupt, an exception, or a system call\nis a form of procedure called a { handler}, and is found by looking\nup the interrupt number, exception number, or trap number in a table\nof functions called a { vector table}.  Vector tables\nfor each type (interrupts, exceptions, and system calls) may be separate,\nor may be combined into a single table.\nInterrupts and exceptions share a need to save all registers and\nstatus bits before execution of the corresponding handler code (and to\nrestore those values afterward).  Generally, the\n{values---including} the status word {register---are} placed\non the stack.  With system calls, saving and restoring any necessary\nstate is part of the calling convention.  A special return from\ninterrupt instruction is used to return control from the interrupt\nhandler to the interrupted code; a similar instruction forces the\nprocessor back into user mode when returning from a system call.\n\nInterrupts are also interesting in the sense that typical computers\noften have many interrupt-generating devices but only a few\ninterrupts.  Interrupts are prioritized by number, and only an\ninterrupt with higher priority can interrupt another interrupt.\nInterrupts with equal or lower priority are blocked while an interrupt\nexecutes.  Some interrupts can also be blocked in some architectures\nby setting bits in a special-purpose register called an interrupt\nmask.  While an interrupt number is masked, interrupts of that type\nare blocked, and can not occur.\n\nAs several devices may generate interrupts with the same interrupt\nnumber, interrupt handlers can be { chained} together.  Each\nhandler corresponds to a particular device.  When an interrupt occurs,\ncontrol is passed to the handler for the first device, which accesses\ndevice registers to determine whether or not that device generated an\ninterrupt.  If it did, the appropriate service is provided.  If not,\nor after the service is complete, control is passed to the next\nhandler in the chain, which handles interrupts from the second device,\nand so forth until the last handler in the chain completes.  At this\npoint, registers and processor state are restored and control is\nreturned to the point at which the interrupt occurred.\n\n\n\n\n\n \n Question: What is an asynchronous signal from an external device indicating that it needs attention? \n Answer: \n\nAn interrupt is an asynchronous signal from an external device indicating that it requires attention."}, {"text": "Title: Control Flow Conditions* \n Text: {Control Flow Conditions*}\n\nControl flow instructions may change the PC, loading it with an\naddress specified by the instruction.  Although any addressing mode\ncan be supported, the most common specify an address directly in the\ninstruction, use a register as an address, or use an address relative\nto a register.  \n\nUnconditional control flow instructions typically provided by an ISA\ninclude procedure calls and returns, traps, and jumps.  Conditional\ncontrol flow instructions are branches, and are logically based on\nstatus bits set by two types of instructions: { comparisons} and\n{ bit tests}.  Comparisons subtract one value from another to set\nthe status bits, whereas bit tests use an AND operation to\ncheck whether certain bits are set or not in a value.\n\nMany ISAs implement\nstatus bits as special-purpose registers and implicitly set them when\nexecuting\ncertain instructions.  A branch based on R2 being less or equal to R3\ncan then be written as shown below.  The status bits are set by\nsubtracting R3 from R2 with the ALU.\n\n\n=WWWWW=WW,WW,WW=\n>CMP>R2,R3>; R2 < R3: CNZ  110, R2 = R3: CNZ  001,\n>>>;     R2 > R3: CNZ  000\n>BLE>R1>; Z  C = 1: PC  R1\n{-6pt}\n\nThe status bits are not always implemented as special-purpose\nregisters; instead, they may be kept in general-purpose registers or\nnot kept at all.  For example, the Alpha ISA stores the results of\ncomparisons in general-purpose registers, and the same branch is\ninstead implemented as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>CMPLE>R4,R2,R3>; R2  R3: R4  1, R2 > R3: R4  0\n>BNE>R4,R1>; R4  0: PC  R1\n{-6pt}\n\nFinally, status bits can be calculated, used, and discarded within\na single instruction, in which case the branch is written as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>BLE>R1,R2,R3>; R2  R3: PC  R1\n{-6pt}\n\nThe three approaches have advantages and disadvantages similar to\nthose discussed in the section on addressing architectures: the first\nhas the shortest instructions, the second is the most general and\nsimplest to implement, and the third requires the fewest instructions.\n\n\n \n Question: What is the disadvantage of using special-purpose registers to store status bits? \n Answer: \n\nA. The advantage of using special-purpose registers to store status bits is that it is the shortest instruction. The disadvantage is that it is the most general and simplest to implement."}, {"text": "Title: Stack Operations* \n Text: {Stack Operations*}\n\nTwo types of stack operations are commonly supported.  Push and pop\nare the basic operations in many older architectures, and values can\nbe placed upon or removed from the stack using these instructions.  In\nmore modern architectures, in which the SP becomes a general-purpose\nregister, push and pop are replaced with indexed loads and stores,\nthat is, loads and stores using the stack pointer and an offset as the\naddress for the memory operation.  Stack updates are performed using\nthe ALU, subtracting and adding immediate values from the SP as\nnecessary to allocate and deallocate local storage.\n\nStack operations serve three purposes in a typical architecture.  The\nfirst is to support procedure calls, as illustrated in a previous\nsection.  The second is to provide temporary storage during\ninterrupts, which was also mentioned earlier.  \n\nThe third use of stack operations is to support { spill code}\ngenerated by compilers.  Compilers first translate high-level\nlanguages into an intermediate representation much like assembly code\nbut with an extremely large (theoretically infinite) register set.\nThe final translation step translates this intermediate representation\ninto assembly code for the target architecture, assigning\narchitectural registers as necessary.  However, as real ISAs support\nonly a finite number of registers, the compiler must occasionally\nspill values into memory.  For example, if ten values are in use at\nsome point in the code, but the architecture has only eight registers,\nspill code must be generated to store the remaining two values on the\nstack and to restore them when they are needed.\n\n\n \n Question: What is the third use of stack operations? \n Answer: \n\nA. When the compiler encounters a situation where there are more values in use than there are registers available, it generates code to store the excess values on the stack and to restore them when they are needed."}, {"text": "Title: I/O* \n Text: {I/O*}\n\nAs a final topic, we now consider how a processor\nconnects to other devices to allow input and output.  We have already\ndiscussed interrupts, which are a special form of I/O in which only\nthe signal requesting attention is conveyed to the processor.\nCommunication of data occurs through instructions similar to loads and\nstores.  A processor is designed with a number of {{ I/O\nports}---usually} read-only or write-only registers to which devices\ncan be attached with opposite semantics.  That is, a port is usually\nwritten by the processor and read by a device or written by a device\nand read by the processor.\n\nThe question of exactly how I/O ports are accessed is an interesting\none.  One option is to create special instructions, such as the {\nin} and { out} instructions of the x86 architecture.  Port\naddresses can then be specified in the same way that memory addresses\nare specified, but use a distinct address space.  Just as two sets of\nspecial-purpose registers can be separated by the ISA, such an {\nindependent I/O} system separates I/O ports from memory addresses by\nusing distinct instructions for each class of operation.\n\nAlternatively, device registers can be accessed using the same load and store\ninstructions as are used to access memory.  This approach, known as\n{ memory-mapped I/O}, requires no new instructions for I/O, but\ndemands that a region of the memory address space be set aside for\nI/O.  The memory words with those addresses, if they exist, can not be\naccessed during normal processor operations.\n\n\n\n\n\n \n Question: What is the difference between independent and memory-mapped I/O? \n Answer: \n\n\n\nA. Independent I/O uses separate instructions to access I/O ports, while memory-mapped I/O uses the same load and store instructions as are used to access memory."}, {"text": "Title: Control Unit Design \n Text: {Control Unit Design}\n\nAppendix C of the Patt and Patel textbook describes a microarchitecture for\nthe LC-3 ISA, including a control unit implementation.\n\nIn this set of notes, we introduce a few concepts and strategies for\ncontrol unit design, using the textbook's {LC-3} microarchitecture \nto help illustrate them.  Several figures from the textbook are\nreproduced with permission in these notes as an aid to understanding.\n\nThe control unit of a computer based on the von Neumann model can be viewed\nas an FSM that fetches instructions from memory and executes them.  Many\npossible implementations exist both for the control unit itself and for the\nresources that it controls, the other components in the von Neumann model,\nwhich we collectively call the { datapath}.\n\nIn this set of notes, we discuss two strategies for structured control\nunit design and introduce the idea of using memories to encode logic \nfunctions.\n\nLet's begin by recalling that the control unit consists of three parts: a \nhigh-level FSM that controls instruction processing, a program counter (PC)\nregister that holds the address of the next instruction to be executed,\nand an instruction register (IR) that holds the current instruction as\nit executes.\n\nOther von Neumann components provide inputs to the control unit.  The\nmemory unit, for example, contains the instructions and data on which \nthe program executes.  \n\nThe processing unit contains a register file and condition \ncodes (N, Z, and P for the {LC-3} ISA).  \n\nThe outputs of the control unit are signals that control operation of the\ndatapath: the processing unit, the memory, and the I/O interfaces.\n\nThe basic problem that we must solve, then, for control unit design, is to \nmap instruction processing and the state of the FSM (including the PC and\nthe IR) into appropriate sequences of { control signals} for the datapath.\n\n \n Question: What are the inputs to the control unit? \n Answer: \n\nA. The inputs to the control unit are the instructions and data from memory, and the condition codes from the processing unit."}, {"text": "Title: LC-3 Datapath Control Signals \n Text: {LC-3 Datapath Control Signals}\n\nAs we have skipped over the implementation details of interrupts and \nprivilege of the {LC-3} in our class, let's consider a \nmicroarchitecture and datapath without those capabilities.  \n\nThe figure on the next page (Patt and Patel Figure C.3) shows an {LC-3}\ndatapath and control signals without support for interrupts and privilege.\n\n\nSome of the datapath control signals mentioned\nin the textbook are no longer necessary in the simplified design.\n\nLet's discuss the signals that remain and give some examples of how they\nare used.  A list appears to the right.\n\nFirst, we have a set of seven {1-bit} control signals (starting \nwith ``LD.'') that specifies whether registers in the datapath load \nnew values.\n\nNext, there are four {1-bit} signals (starting with ``Gate'') for \ntri-state buffers that control access to the bus.  These four implement \na distributed mux for the bus.  Only one value can\n\n\n\nsignal& meaning \nLD.MAR& load new value into memory address register\nLD.MDR& load new value into memory data register\nLD.IR& load new value into instruction register\nLD.BEN& load new value into branch enable register\nLD.REG& load new value into register file\nLD.CC& load new values into condition code registers (N,Z,P)\nLD.PC& load new value into program counter \nGatePC& write program counter value onto bus\nGateMDR& write memory data register onto bus\nGateALU& write arithmetic logic unit result onto bus\nGateMARMUX& write memory address register mux output onto bus \nPCMUX& select value to write to program counter (2 bits)\nDRMUX& select value to write to destination register (2 bits)\nSR1MUX& select register to read from register file (2 bits)\nADDR1MUX& select register component of address (1 bit)\nADDR2MUX& select offset component of address (2 bits)\nMARMUX& select type of address generation (1 bit) \nALUK& select arithmetic logic unit operation (2 bits) \nMIO.EN& enable memory \nR.W& read or write from memory \n\n\n\nappear on the bus in \nany cycle, so at most one of these signals can be 1; others must all \nbe 0 to avoid creating a short.\n\n\n\n{file=part4/patt-patel-appendix-C/APPC03.eps,width=6.5in}\n\n\n\nThe third group (ending with ``MUX'') of signals\ncontrols multiplexers in the datapath.\nThe number of bits for each depends on the number of inputs to the mux;\nthe total number of signals is 10.\nThe last two groups of signals control the ALU and the memory, \nrequiring a total of 4 more signals.  The total of all groups is thus\n25 control signals for the datapath without support for privilege\nand interrupts.\n\n \n Question: What is the number of signals in the LC-3 datapath? \n Answer: \nQ. 23\nQ. 26"}, {"text": "Title: Example Control Word: ADD \n Text: {Example Control Word: ADD}\n\nBefore we begin to discuss control unit design in more detail, let's\nwork through a couple of examples of implementing specific RTL with\nthe control signals available.  The figure below (Patt and Patel \nFigure C.2) shows a state machine for the {LC-3} ISA (again \nwithout detail on interrupts and privilege).\n\n{{file=part4/patt-patel-appendix-C/APPC02.eps,width=6.25in}}\n\nConsider now the state that implements the ADD instruction---state\nnumber 1 in the figure on the previous page, just below and to the \nleft of the decode state.\n\nThe RTL for the state is: DR  SR + OP2, set CC.\n\nWe can think of the 25 control signals that implement the desired RTL\nas a { control word} for the datapath. \n\nLet's begin with the register load control signals.  The RTL writes\nto two types of registers: the register file and the condition codes.\nTo accomplish these simultaneous writes, LD.REG and LD.CC must be high.\nNo other registers in the datapath should change, so the other five\nLD signals---LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC---should be low.\n\nWhat about the bus?  In order to write the result of the add operation\ninto the register file, the control signals must allow the ALU to write\nits result on to the bus.  So we need GateALU=1.  And the other Gate\nsignals---GatePC, GateMDR, and GateMARMUX---must all be 0.  The \ncondition codes are also calculated from the value on the bus, but\nthey are calculated on the same value as is written to the register\nfile (by the definition of the {LC-3} ISA).\n\nIf the RTL for an FSM state implicitly requires more than one value \nto appear on the bus in the same cycle, that state is impossible to \nimplement using the given datapath.  Either the datapath or the state \nmachine must be changed in such a case.  The textbook's design has been \nfairly thoroughly tested and debugged.\n\n\nThe earlier figure of the datapath does not \nshow all of the muxes.  The remaining muxes appear in the figure \nto the right (Patt and Patel Figure C.6).\n\nSome of the muxes in the datapath must be used to enable the\naddition needed for ADD to occur.  The\nDRMUX must select its IR[11:9] \ninput in order to write to the destination register specified by the \nADD instruction.  Similarly, the SR1MUX must select its IR[8:6] input \nin order to\n\n\n{file=part4/patt-patel-appendix-C/APPC06.eps,width=4.45in}\n\n\npass the first source register specified by the ADD to the ALU as input A\n(see the datapath figure).  SR2MUX is always controlled by the \nmode bit IR[5], so the control unit does not need to generate anything\n(note that this signal was not in the list given earlier).\n\nThe rest of the muxes in the datapath---PCMUX, ADDR1MUX, ADDR2MUX, and\nMARMUX---do not matter, and the signals controlling them are don't cares.\nFor example, since the PC does not change, the output of the PCMUX\nis simply discarded, thus which input the PCMUX forwards to its output\ncannot matter.\n\nThe ALU must perform an addition, so we must set the operation type\nALUK appropriately.  And memory should not be enabled (MIO.EN=0),\nin which case the read/write control for memory, R.W, is a don't\ncare.  These 25 signal values together (including seven don't cares)\nimplement the RTL for the single state of execution for an \nADD instruction.\n\n\n \n Question: What is the value of the gateALU? \n Answer: \n\nA. LD.REG = 1, LD.CC = 1, GateALU = 1, ALUK = 0, and MIO.EN = 0"}, {"text": "Title: Example Control Word: LDR \n Text: {Example Control Word: LDR}\n\nAs a second example, consider the first state in the sequence that \nimplements the LDR instruction---state number 6 in the figure on the \nprevious page.\n\nThe RTL for the state is: MAR  BaseR + off6, but BaseR is\nabbreviated to ``B'' in the state diagram.\n\nWhat is the control word for this state?\n\nLet's again begin with the register load control signals.  Only the\nMAR is written by the RTL, so we need LD.MAR=1 and the other load\nsignals all equal to 0.\n\nThe address (BaseR + off6) is generated by the address adder, then \npasses through the MARMUX to the bus, from which it can be written into\nthe MAR.  To allow the MARMUX to write to the bus, we set GateMARMUX\nhigh and set the other three Gate control signals low.\n\nMore of the muxes are needed for this state's RTL than we needed for\nthe ADD execution state's RTL.\n\nThe SR1MUX must again select its IR[8:6] input, this time in order to \npass the BaseR specified by the instruction to ADDR1MUX.  ADDR1MUX\nmust then select the output of the register file in order to pass the \nBaseR to the address adder.  The other input of the address adder should\nbe off6, which corresponds to the sign-\nextended version of IR[5:0].\nADDR2MUX must select this input to pass to the address adder. \nFinally, the MARMUX must select the output of the address adder.\nThe PCMUX and DRMUX do not matter for this case, and can be left as\ndon't cares.  Neither the PC nor any register in the register file\nis written.\n\nThe output of the ALU is not used, so which operation it performs is \nirrelevant, and the ALUK controls are also don't cares.  Memory is\nalso not used, so again we set MIO.EN=0.  And, as before, the R.W\ncontrol for memory is a don't care.\n\nThese 25 signal values together (again including seven don't cares)\nimplement the RTL for the first state of LDR execution.\n\n\n \n Question: What is the control word for the first state of LDR? \n Answer: \n\nThe control word for this state is:\n\nLD.MAR=1, GateMARMUX=1, ADDR1MUX=IR[8:6], ADDR2MUX=off6, MARMUX=ADDR+, PCMUX=X, DRMUX=X, ALUK=X, MIO.EN=0, R.W=X."}, {"text": "Title: Hardwired Control \n Text: {Hardwired Control}\n\n\nNow we are ready to think about how control signals can be generated.\n\nAs illustrated to the right, instruction processing consists of two \nsteps repeated infinitely: fetch an instruction, then execute the \ninstruction.\n\nLet's say that we choose a fixed number of cycles for each of these two\nsteps.  We can then control our system with a\n\n\n{file=part4/figs/inst-loop.eps,width=2.5in}\n\n\ncounter, using the counter's value and the IR to generate the\ncontrol signals through combinational\nlogic.  The PC is used only as data and has little or no direct effect on \nhow the system fetches and processes instructions.{Some ISAs do \nsplit the address space into privileged and non-privileged regions, but \nwe ignore that possibility here.}\n\nThis approach in general is called { hardwired control}.\n\nHow many cycles do we need for instruction fetch?  How many cycles do\nwe need for instruction processing?  The answers depend on the factors:\nthe complexity of the ISA, and the complexity of the datapath.\n\nGiven a simple ISA, we can design a datapath that\nis powerful enough to process any instruction in a single cycle.  \nThe control unit for such a design is\nan example of { single-cycle, hardwired control}.  While this\napproach simplifies the control unit, the cycle time\nfor the clock is limited by the slowest instruction.\nThe clock must also be slow enough to let memory \noperations complete in single cycle, both for instruction fetch and\nfor instructions that require a memory access.\n\nSuch a low clock rate is usually not acceptable.\n\nMore generally, we can use a simpler datapath and break both instruction\nfetch and instruction processing into multiple steps.  Using the datapath\nin the figure from Patt and Patel, for example, instruction fetch requires \nthree steps, and the number of steps for instruction processing depends \non the type of instruction being processed.  The state diagram shown\nearlier illustrates the steps for each opcode.\n\nAlthough the datapath is not powerful enough to complete instructions\nin a single cycle, we can build a { multi-cycle, hardwired control} \nunit (one based on combinational logic).  \n\nIn fact, this type of control unit is not much more complex than the \nsingle-cycle version.  For the control unit's FSM, we can use a binary \ncounter to enumerate first the steps of fetch, then the steps\nof processing.  The counter value along with the IR register can \ndrive combinational logic to produce control signals.  And, to avoid\nprocessing at the speed of the slowest instruction (the opcode that\nrequires the largest number of steps; LDI and STI in the {LC-3}\nstate diagram), we can add a reset signal to the counter to force it \nback to instruction fetch.  The FSM counter reset signal is simply \nanother control signal.  Finally, we can add one more signal that \npauses the counter while waiting for a memory operation to complete.  \nThe system clock can then run at the speed of the logic rather than \nat the speed of memory.\n\nThe control unit implementation discussed in Appendix C of Patt and Patel \nis not hardwired, but it does make use of a memory ready signal to \nachieve this decoupling between the clock speed of the processor and the\naccess time of the memory.\n\n\nThe figure to the right illustrates a general \nmulti-cycle, hardwired control unit.  The three blocks on\nthe left are the control unit state.  The combinational logic in\nthe middle uses the control unit state along with some\ndatapath status bits to compute the control signals for the datapath\nand the extra controls needed for the FSM counter, IR, and PC.\nThe datapath appears to the right in the figure.\n\n\n\n\n\nHow complex is the combinational logic?  As mentioned earlier, we \nassume that the PC does not directly affect control.  But we still \nhave 16 bits of IR, the FSM counter state, and the datapath status\nsignals.  Perhaps we need 24-variable {K-maps}?  \n\nHere's where engineering and human design come to \nthe rescue: by careful design of the ISA and the encoding, the\nauthors have made many of the datapath control signals for the {LC-3}\nISA quite simple.\nFor example, the register that appears on the register file's SR2\noutput is always specified by IR[2:0].  The SR1 output \nrequires a mux, but the choices are limited to IR[11:9] and IR[8:6]\n(and R6 in the design with support for interrupts).\nSimilarly, the destination register in the register file\nis always R7 or IR[11:9] (or, again, R6 when supporting interrupts).\n\nThe control signals for an {LC-3}\ndatapath depend almost entirely on the state of the control unit \nFSM (counter bits in a hardwired design) and the opcode IR[15:12].\n\nThe control signals are thus reduced to fairly simple functions.\n\nLet's imagine building a hardwired control unit for the {LC-3}.\n\nLet's start by being more precise about the number of inputs to\nthe combinational logic.\n\nAlthough most decisions are based on the opcode, the datapath\nand state diagram shown earlier for the {LC-3} ISA do have\none instance of using another instruction bit to determine behavior.\nSpecifically, the JSR instruction has two modes, and the control\nunit uses IR[11] to choose between them.  So we need to have\nfive bits of IR instead of four as input to our logic.\n\nHow many datapath status signals are needed?\n\nWhen the control unit accesses memory, it must wait until the \nmemory finishes the access, as indicated by a memory ready signal R.\nAnd the control unit must implement the conditional part of conditional\nbranches, for which it uses the datapath's branch enable signal BEN.\n\nThese two datapath status signals suffice for our design.\n\nHow many bits do we need for the counter?\nInstruction fetch requires three cycles: one to move\nthe PC to the MAR and increment the PC, a second to read from\nmemory into MDR, and a third to move the instruction bits across\nthe bus from MDR into IR.  Instruction decoding in a hardwired design\nis implicit and \nrequires no cycles: since all of our control signals can depend on\nthe IR, we do not need a cycle to change the FSM state to reflect the opcode.\nLooking at the {LC-3} state diagram, we see\nthat processing an instruction requires at most five cycles.  In\ntotal, at\nmost eight steps are needed to fetch and process any {LC-3} \ninstruction, so we can use a {3-bit} binary counter.\n\nWe thus have a total of ten bits of input: IR[15:11], R, BEN, and\na {3-bit} counter.\n\nAdding the RESET and PAUSE controls for our FSM counter to the \n25 control signals listed earlier, we need to find 27 functions\non 10 variables.\n\nThat's still a lot of big {K-maps} to solve.  Is there an\neasier way?\n\n\n \n Question: How many datapath status signals are needed for a hardwired control unit? \n Answer: \n\nA. Two datapath status signals are needed for a hardwired control unit."}, {"text": "Title: Using a Memory for Logic Functions \n Text: {Using a Memory for Logic Functions}\n\nConsider a case in which you need to compute many functions on a small \nnumber of bits, such as we just described for the multi-cycle, hardwired\ncontrol unit.  One strategy is to use a memory (possibly a read-only memory).\nA {2^m} memory can be viewed as computing N arbitrary\nfunctions on m variables.  The functions to be computed are specified\nby filling in the bits of the memory.  So long as the value of m is fairly\nsmall, the memory (especially SRAM) can be fast.\n\nSynthesis tools (or hard work) can, of course, produce smaller designs\nthat use fewer gates.  Actually, tools may be able to optimize a fixed\ndesign expressed as read-only memory, too.  But designing the functions\nwith a memory makes them easier to modify later.  If we make a mistake,\nfor example, in computing one of the functions, we need only change a bit\nor two in the memory instead of solving equations and reoptimizing and\nreplacing logic gates.  We can also extend our design if we have space\nremaining (that is, if the functions are undefined for some combinations \nof the m inputs).  The\nCray T3D supercomputer, for example, used a similar approach to add\nnew instructions to the Alpha processors on which it was based.\n\nThis strategy is effective in many contexts, so let's briefly discuss two \nanalogous cases.  In software, a memory becomes a lookup table.  Before\nhandheld calculators, lookup tables were used by humans to compute \ntranscendental functions such as sines, cosines, logarithms.  Computer \ngraphics hardware and software used a similar approach for transcendental\nfunctions in order to reduce cost and improve \nspeed.  Functions such as counting 1 bits in a word are useful for \nprocessor scheduling and networking, but not all ISAs provide this type\nof instruction.  In such cases, lookup tables in software are often the\nbest solution.\n\nIn programmable hardware such as Field Programmable Gate Arrays (FPGAs),\nlookup tables (called LUTs in this context) have played an important role\nin implementing arbitrary logic functions.\n\nThe FPGA is the modern form of the programmable logic array (PLA)\nmentioned in the textbook, and will be your main tool \nfor developing digital hardware in ECE385.\n\nFor many years, FPGAs served as a hardware prototyping platform, but\nmany companies today ship their first round products using \ndesigns mapped to FPGAs.  Why?  Chips are more and more expensive\nto design, and mistakes are costly to fix.  In contrast, while\ncompanies pay more to buy an FPGA than to produce a chip (after the \nfirst chip!), errors in the design can usually be fixed by sending \ncustomers a new version through the Internet.\n\nLet's return to our {LC-3} example.\n\nInstead of solving the {K-maps}, we can use a small memory:\n2^ bits (27,648 bits total).\n\nWe just need calculate the bits, put\nthem into the memory, and use the memory to produce the control signals.\n\nThe ``address'' input to the memory are the same 10 bits that we\nneeded for our combinational logic: IR[15:11], R, BEN, and the FSM counter.\nThe data outputs of the memory are the control signals and the\nRESET and PAUSE inputs to the FSM counter.  And we're done.\n\nWe can do a little better, though.  The datapath in the textbook was\ndesigned to work with the textbook's control unit.  If we add a little\nlogic, we can significantly simplify our memory-based, hardwired implementation.\nFor example, we only need to pause the FSM counter when waiting for memory.\nIf we can produce a control signal that indicates a need to wait for\nmemory, say {WAIT-MEM}, we can use a couple of gates to compute the\nFSM counter's PAUSE signal as {WAIT-MEM} AND (NOT R).  Making this change\nshrinks our memory to 2^9 bits.  The extra two control\nsignals in this case are RESET and {WAIT-MEM}.\n\nNext, look at how BEN is used in the state diagram:\nthe only use is to terminate the\nprocessing of branch instructions when no branch should occur (when \nBEN=0).  We can fold that functionality into the FSM counter's RESET\nsignal by producing a branch reset signal, {BR-RESET}, to reset\nthe counter to end a branch and a second signal, {INST-DONE},\nwhen an instruction is done.  The RESET input for the FSM counter\nis then ({BR-RESET} AND (NOT BEN)) OR {INST-DONE}.\n\nAnd our memory further shrinks to 2^8 bits, where the extra\nthree control signals are {WAIT-MEM}, {BR-RESET}, and \n{INST-DONE}.\n\nFinally, recall that the only need for IR[11] is to implement the two\nforms of JSR.  But we can add wires to connect SR1 to PCMUX's fourth \ninput, then control the PCMUX output selection using IR[11] when appropriate\n(using another control signal).  With this extension, we can implement\nboth forms with a single state, writing to both R7 and PC in the same\ncycle.\n\nOur final memory can then be 2^7 bits (3,712 bits total),\nwhich is less than one-seventh the number of bits that we needed before \nmodifying the datapath.\n\n\n\n\n \n Question: What is the benefit of using a memory to compute logic functions? \n Answer: \n\nA. Using a memory to compute logic functions allows for easy modification of the design if a mistake is made. Additionally, it allows for easy extension of the design if more space is needed."}, {"text": "Title: Microprogrammed Control \n Text: {Microprogrammed Control}\n\nWe are now ready to discuss the second approach to control unit design.\nTake another look at the state diagram for the the {LC-3} ISA.  Does it \nremind you of anything?  Like a flowchart, it has relatively few arcs \nleaving each state---usually only one or two.\n\nWhat if we treat the state diagram as a program?  We can use a small\nmemory to hold { microinstructions} (another name for control words) \nand use the FSM state number as the memory address.  \n\nWithout support for interrupts or privilege, and with the datapath \nextension for JSR mentioned for hardwired control, the {LC-3} state \nmachine requires fewer than 32 states.\n\nThe datapath has 25 control signals, but we need one more for the\ndatapath extension for JSR.\n\nWe thus start with {5-bit} state number (in a register)\nand a {2^5 bit} memory, which we call\nour control ROM (read-only memory) to distinguish\nit from the big, slow, von Neumann memory.\n\nEach cycle, the { microprogrammed control} unit applies the FSM state \nnumber to the control ROM (no IR bits, \njust the state number), gets back a set of control signals, and\nuses them to drive the datapath.\n\n\nTo write our microprogram, we need to calculate the control signals\nfor each microinstruction and put them in the control ROM, but we also\nneed to have a way to decide which microinstruction should execute \nnext.  We call the latter problem { sequencing} or microsequencing.\n\nNotice that most of the time there's no choice: we have only { one}\nnext microinstruction.  One simple approach is then to add the address\n(the {5-bit} state ID) of the next microinstruction to the control ROM.\nInstead of 26 bits per FSM state, we now have 31 bits per FSM state.\n\nSometimes we do need to have two possible next states.  When waiting\nfor memory (the von Neumann memory, not the control ROM) to\nfinish an access, for example, we want our FSM to stay\nin the same state, then move to the next state when the access completes.\nLet's add a second address to each microinstruction, and add\na branch control signal for the microprogram to decide whether we\nshould use the first address or the second for the next\nmicroinstruction.  This design, using a 2^5 bit memory\n(1,152 bits total), appears to the right.\n\n\n{file=part4/figs/microprogrammed-no-decode.eps,width=2in}\n\n\n\nThe microprogram branch control signal is a Boolean logic expression\nbased on the memory ready signal R and \nIR[11].  We can implement it with a state ID comparison and\na mux, as shown to the right.  For the branch instruction execution state, \nthe mux selects input 1, BEN.  For all other states, the mux selects \ninput 0, R.  When an FSM state has only a single next state, we set both \nIDs in the control ROM to the ID for that state, so the value of R \nhas no effect.\n\n\n\n\n\n We can simplify the implementation of the microprogram branch control\n by setting both next-state addresses to the same value when a state\n does not branch.  Then the branch control becomes a don't care for that\n state.\n\n\nWhat's missing?  Decode!  We do have one FSM state in which we need to be\nable to branch to one of sixteen possible next states, one for each\nopcode.  Let's just add another mux and choose the state IDs for starting\nto process each opcode in an easy way, as shown to the right with\nextensions highlighted in blue.  The textbook assigns the first state \nfor processing each opcode the IDs IR[15:12] preceeded by two 0s (the\ntextbook's design requires {6-bit} state IDs).  We adopt the same\nstrategy.  For example, the first FSM state for processing an ADD is 00001, \nand the first state for a TRAP is 01111.  In each case, the opcode \nencoding specifies the last four bits.\n\nNow we can pick the remaining state IDs arbitrarily and fill in the \ncontrol ROM with control \nsignals that implement each state's RTL and the two possible next states.\nTransitions from the decode state are handled by the extra mux.\n\nThe microprogrammed control unit implementation in Appendix C of \nPatt and Patel is similar to the one that we have developed here,\nbut is slightly more complex so that it can handle interrupts\nand privilege.   \n\n\n\n\n\n\n\n\n \n Question: What is the second approach to control unit design? \n Answer: programmed control} unit applies the FSM state \nQ. number to the control ROM (no IR bits, \nQ. just the state number), gets back a set of control signals, and\nQ. uses them to drive the datapath.\nQ. \nQ. \nQ. To write our microprogram, we need to calculate the control signals\nQ. for each microinstruction and put them in the control ROM, but we also\nQ. need to have a way to decide which microinstruction should execute \nQ. next.  We call the latter problem { sequencing} or microsequencing.\nQ. \nQ. Notice that most of the time there's no choice: we have only { one}\nQ. next microinstruction.  One simple approach is then to add the address\nQ. (the {5-bit} state ID) of the next microinstruction to the control ROM.\nQ. Instead of 26 bits per FSM state, we now have 31 bits per FSM state.\nQ. \nQ. Sometimes we do need to have two possible next states.  When waiting\nQ. for memory (the von Neumann memory, not the"}, {"text": "Title: Redundancy and Coding \n Text: {Redundancy and Coding}\n\nThis set of notes introduces the idea of using sparsely populated\nrepresentations to protect against accidental changes to bits.\nToday, such representations are used in almost every type of storage\nsystem, from bits on a chip to main memory to disk to archival tapes.\n\nWe begin our discussion with examples of representations in which some \nbit patterns have no meaning, then consider what happens when a bit \nchanges accidentally.  We next outline a general scheme \nthat allows a digital system to detect a single bit error.\n\nBuilding on the mechanism underlying this scheme,\nwe describe a distance metric that enables us to think more broadly about \nboth detecting and correcting such errors, and then show a general\napproach that allows correction of a single bit error.\n\nWe leave discussion of more sophisticated schemes to classes on\ncoding and information theory.\n\n\n \n Question: What is the purpose of sparsely populated representations? \n Answer: \n\nA. The purpose of using a sparsely populated representation is to\nprotect against accidental changes to bits."}, {"text": "Title: Sparse Representations \n Text: {Sparse Representations}\n\nRepresentations used by computers must avoid ambiguity: a single\nbit pattern in a representation cannot be used to represent more than\none value.  However, the converse need not be true.  A representation can\nhave several bit patterns representing the same value, and\nnot all bit patterns in a representation need be\nused to represent values.\n\nLet's consider a few example of representations with unused patterns.\nHistorically, one common class of representations of this type was \nthose used to represent individual decimal digits.  We examine three\nexamples from this class.\n\nThe first is Binary-coded Decimal (BCD), in which\ndecimal digits are encoded individually \nusing their representations in the unsigned\n(binary) representation.  Since we have 10 decimal digits, we need \n10 patterns, and four bits for each digit.  But four bits allow \n2^4=16 bit patterns.  In BCD, the patterns 1010,1011,,1111\nare unused.  It is important to note that BCD is not the same as the\nunsigned representation.  The decimal number 732, for example, requires 12 bits\nwhen encoded as BCD: 0111 0011 0010.  When written using a {12-bit}\nunsigned representation, 732 is written 001011011100.  Operations\non BCD values were implemented in early processors, including the\n8086, and are thus still available in the x86 instruction set architecture\ntoday!\n\n\nThe second example is an {Excess-3} code, in which each decimal\ndigit d is represented by the pattern corresponding to the \n{4-bit} unsigned pattern for d+3.  For example, the digit 4\nis represented as 0111, and the digit 7 is represented as 1010.\nThe {Excess-3} code has some attractive aspects when using \nsimple hardware.  For example, we can use a {4-bit} binary \nadder to add two digits c and d represented in the {Excess-3} code, \nand the carry out signal produced by the adder is the same as the carry\nout for the decimal addition, since c+d is equivalent to \n(c+3)+(d+3).\n\nThe third example of decimal digit representations is a {2-out-of-5}\ncode.  In such a code, five bits are used to encode each digit.  \n\nOnly patterns with exactly two 1s are used.  There are exactly ten \nsuch patterns, and an example representation is shown to the right (more\nthan one assignment of values to patterns has been used in real systems).\n\n\n{c|c}\n& a 2-out-of-5\ndigit& representation \n1& 00011\n2& 00101\n3& 00110\n4& 01001\n5& 01010\n6& 01100\n7& 10001\n8& 10010\n9& 10100\n0& 11000\n\n\n\n\n\n \n Question: What is the difference between a 2-out-of-5 code and an unsigned representation? \n Answer: \n\nA. A 2-out-of-5 code is a code in which five bits are used to encode each digit, and only patterns with exactly two 1s are used."}, {"text": "Title: Error Detection \n Text: {Error Detection}\n\nErrors in digital systems\ncan occur for many reasons, ranging from cosmic ray strikes to\ndefects in chip fabrication to errors in the design of the digital system.\n\nAs a simple model, we assume that an error takes the form of changes to\nsome number of bits.  In other words, a bit that should have the value 0\ninstead has the value 1, or a bit that should have the value 1 instead\nhas the value 0.  Such an error is called a { bit error}.\n\n\n\nDigital systems can be designed with or without tolerance to errors.\nWhen an error occurs, no notification nor identification of the error\nis provided.  Rather, if error tolerance is needed,\nthe system must be designed to be able to \nrecognize and identify errors automatically.\n\nOften, we assume that each of the bits may be in error independently\nof all of the others, each with some low probability.  With such\nan assumption, multiple bit errors\nare much less likely than single bit errors, and we can focus on \ndesigns that tolerate a single bit error.\n\nWhen a bit error occurs, however, we must assume that \nit can happen to any of the bits.\n\nThe use of many patterns to represent a smaller number of values, as\nis the case in a {2-out-of-5} code, enables a system\nto perform { error detection}.\n\nLet's consider what happens when a value represented using a \n{2-out-of-5} code is subjected to a single bit error.\nImagine that we have the digit 7.  In the table on the previous page,\nnotice that the digit 7 is represented with the pattern 10001.\n\nAs we mentioned, we must assume that the bit error can occur in any \nof the five bits, thus we have five possible bit patterns after the error \noccurs.  If the error occurs in the first bit, we have the pattern 00001.\nIf the error occurs in the second bit, we have the pattern 11001.\nThe complete set of possible error patterns is 00001, 11001, 10101,\n10011, and 10000.  \n\nNotice that none of the possible error patterns has exactly two 1s, and \nthus none of them is a meaningful pattern in our {2-out-of-5} code.\nIn other words, whenever a digital system represents the digit 7 and \na single bit error occurs, the system will be able to detect that an \nerror has occurred.\n\nWhat if the system needs to represent a different digit?  \nRegardless of which digit is represented, the pattern with no errors \nhas exactly two 1s,\nby the definition of our representation.  If we then flip one of the \nfive bits by subjecting it to a bit error, the resulting error pattern\nhas either one 1 (if the bit error changes a 1 to a 0) or three 1s\n(if the bit error changes a 0 to a 1).  In other words, regardless of\nwhich digit is represented, and regardless of which bit has an error,\nthe resulting error pattern never has a meaning in \nthe {2-out-of-5} code.  So this representation enables a digital\nsystem to detect any single bit error!\n\n\n \n Question: What is the meaning of the resulting pattern? \n Answer: \n\nA. If the system needs to represent a different digit, the pattern with no errors will still have exactly two 1s. If a bit error occurs, the resulting error pattern will have either one 1 or three 1s, which will not have a meaning in the 2-out-of-5 code. So this representation will still enable the digital system to detect any single bit error."}, {"text": "Title: Parity \n Text: \n\nThe ability to detect any single bit error is certainly useful.\nHowever, so far we have only shown how to protect ourselves when we\nwant to represent decimal digits.  Do we need to develop a separate\nerror-tolerant representation for every type of information that\nwe might want to represent?  Or can we instead come up with a more\ngeneral approach?\n\nThe answer to the second question is yes: we can, in fact, systematically\ntransform any representation into a representation that allows detection of a\nsingle bit error.  The key to this transformation is the idea of\n{ parity}.\n\n\nConsider an arbitrary representation for some type of information.\nBy way of example, we use the {3-bit} unsigned representation.\nFor each pattern used in the representation, we can count the number\nof 1s.  The resulting count is either odd or even.  By adding an extra\nbit---called a { parity bit}---to the representation, and \nselecting the parity bit's value \nappropriately for each bit pattern, we can ensure that the count of 1s\nis odd (called { odd parity}) or even (called { even parity})\nfor all values represented.  The idea is\n\nillustrated in the table\nto the right for the {3-bit} unsigned representation.  The parity\nbits are shown in bold.\n\n\n{c|c|c|c|c}\nvalue      &   3-bit & number& with odd& with even \nrepresented& unsigned& of 1s & parity  & parity  \n0& 000& 0& 000{ 1}& 000{ 0}\n1& 001& 1& 001{ 0}& 001{ 1}\n2& 010& 1& 010{ 0}& 010{ 1}\n3& 011& 2& 011{ 1}& 011{ 0}\n4& 100& 1& 100{ 0}& 100{ 1}\n5& 101& 2& 101{ 1}& 101{ 0}\n6& 110& 2& 110{ 1}& 110{ 0}\n7& 111& 3& 111{ 0}& 111{ 1}\n\n\n\nEither approach to selecting the parity bits ensures that any single\nbit error can be detected.  For example, if we choose to use odd\nparity, a single\nbit error changes either a 0 into a 1 or a 1 into a 0.\nThe number of 1s in the resulting error pattern thus\ndiffers by exactly one from the original pattern, and the parity of\nthe error pattern is even.  But all valid patterns have odd parity,\nso any single bit error can be detected by simply counting the number\nof 1s.\n\n\n \n Question: What is the key to systematically transform any representation into a representation that allows detection of a single bit error? \n Answer: \n\nA. A parity bit is used to detect single bit errors in a representation."}, {"text": "Title: Hamming Distance \n Text: {Hamming Distance}\n\nNext, let's think about how we might use representations---we might\nalso think of them as { codes}---to protect a system against \nmultiple bit errors.  As we have seen with parity, one strategy that\nwe can use to provide such error tolerance is the use of representations\nin which only some of the patterns actually represent values.\nLet's call such patterns { code words}.  In other words, the\ncode words in a representation are those patterns that correspond to \nreal values of information.  Other patterns in the representation have no \nmeaning.\n\nAs a tool to help us understand error tolerance, let's define\na measure of the distance between code words in a representation.\nGiven two code words X and Y, we can calculate the number N_ of \nbits that must change to transform X into Y.  Such a calculation\nmerely requires that we compare the patterns bit by bit and count\nthe number of places in which they differ.  Notice that this\nrelationship is symmetric: the same number of changes are required\nto transform Y into X, so N_=N_.  \n\nWe refer to this number N_ as the { Hamming distance} between\ncode word X and code word Y.  \n\nThe metric is named after Richard Hamming, a computing pioneer and an \nalumnus of the UIUC Math department.\n\nThe Hamming distance between two code words tells us how many bit errors\nare necessary in order for a digital system to mistake one code word\nfor the other.  Given a representation, we can calculate the minimum\nHamming distance between any pair of code words used by the \nrepresentation.  The result is called the { Hamming distance of the \nrepresentation}, and represents the minimum of bit errors that must\noccur before a system might fail to detect errors in a stored value.\n\nThe Hamming distance for nearly all of the representations that we \nintroduced in earlier sections is 1.  Since more than half of the\npatterns (and often all of the patterns!) correspond to meaningful\nvalues, some pairs of code words must differ in only one bit, and\nthese representations cannot tolerate any errors.  For example,\nthe decimal value 42 is stored as 101010 using a {6-bit} unsigned\nrepresentation, but any bit error in that pattern produces another \nvalid pattern corresponding to one of the following \ndecimal numbers: 10, 58, 34, 46, 40, 43.  Note that the Hamming distance\nbetween any two patterns is not necessarily 1.  Rather, the Hamming \ndistance of the unsigned representation, which corresponds to the \nminimum between any pair of valid patterns, is 1.\n\nIn contrast, the Hamming distance of the {2-out-of-5} code that\nwe discussed earlier is 2.  Similarly, the Hamming distance of any\nrepresentation extended with a parity bit is at least 2.\n\nNow let's think about the problem slightly differently.\n\nGiven a particular representation, \n\nhow many bit errors can we detect in values using that representation?\n\n{ A representation with Hamming distance d can detect up to d-1 bit errors.}\n\nTo understand this claim, start by selecting a code word from the\nrepresentation and changing up to d-1 of the bits.  No matter\nhow one chooses to change the bits, these changes cannot result in\nanother code word, since we know that any other code word has to \nrequire at least d changes from our original code word, by the\ndefinition of the representation's Hamming distance.\n\nA digital system using the representation can thus detect up to d-1\nerrors.  However, if d or more errors occur, the system might sometimes\nfail to detect any error in the stored value.\n\n\n \n Question: What is the Hamming distance of the unsigned representation? \n Answer: \n\nA. A representation with Hamming distance 2 can detect 1 bit error."}, {"text": "Title: Error Correction \n Text: {Error Correction}\n\nDetection of errors is important, but may sometimes not be enough.\nWhat can a digital system do when it detects an error?  In some\ncases, the system may be able to find the original value elsewhere, or\nmay be able to re-compute the value from other values.  In other \ncases, the value is simply lost, and the digital system may need\nto reboot or even shut down until a human can attend to it.\n\nMany real systems cannot afford such a luxury.  Life-critical systems\nsuch as medical equipment and airplanes should not turn themselves off\nand wait for a human's attention.  Space vehicles face a similar dilemma,\nsince no human may be able to reach them.\n\nCan we use a strategy similar to the one that we have developed for error\ndetection in order to try to perform { error correction}, recovering\nthe original value?  Yes, but the overhead---the\nextra bits that we need to provide such functionality---is higher.\n\n\n\nLet's start by thinking about a code with Hamming distance 2, such\nas {4-bit} 2's complement with odd parity.  We know that such a \ncode can detect one bit error.  Can it correct such a bit error, too?\n\nImagine that a system has stored the decimal value 6 using the \npattern 0110{ 1}, where the last bit is the odd parity bit.\nA bit error occurs, changing the stored pattern to 0111{ 1}, which is\nnot a valid pattern, since it has an even number of 1s.  But can the\nsystem know that the original value stored was 6?  No, it cannot.\nThe original value may also have been 7, in which case the original\npattern was 0111{ 0}, and the bit error occurred in the final\nbit.  The original value may also have been -1, 3, or 5.  The system\nhas no way of resolving this ambiguity.\n\nThe same problem arises if a digital system uses a code with\nHamming distance d to detect up to d-1 errors.\n\n\nError correction is possible, however, if we assume that fewer bit\nerrors occur (or if we instead use a representation with a larger Hamming\ndistance).\n\nAs a simple example, let's create a representation for the numbers 0\nthrough 3 by making three copies of the {2-bit} unsigned \nrepresentation, as shown to the right.  The Hamming distance of the\nresulting code is 3, so any two bit errors can be detected.  However,\nthis code also enables us to correct a single bit error.  Intuitively, \nthink of the three copies as voting on the right answer.\n\n\n{c|c}\nvalue      & three-copy\nrepresented& code \n0& 000000\n1& 010101\n2& 101010\n3& 111111\n\n\n\nSince a\nsingle bit error can only corrupt one copy, a majority vote always\ngives the right answer!\nTripling the number of bits needed in a representation is not a good\ngeneral strategy, however. \nNotice also that ``correcting'' a pattern with two bit errors can produce\nthe wrong result.\n\nLet's think about the problem in terms\nof Hamming distance.  Assume that we use a code with Hamming distance d\nand imagine that up to k bit errors affect a stored value.\nThe resulting pattern then falls within a neighborhood of distance k\nfrom the original code word.  This neighborhood contains all bit \npatterns within Hamming distance k of the original pattern.\nWe can define such a neighborhood around each code word.  Now, \nsince d bit errors are needed to transform a code word into\nany other code word, these neighborhoods are disjoint so long\nas 2k{d-1}.  In other words, if the inequality holds,\nany bit pattern in the representation can be in at most one code word's \nneighborhood.  The digital system can then correct the errors by \nselecting the unique value identified by the associated neighborhood.\nNote that patterns encountered as a result of up to k bit errors\nalways fall within the original code word's neighborhood; the inequality\nensures that the neighborhood identified in this way is unique.\nWe can manipulate the inequality to express the number of errors k that\ncan be corrected in terms of the Hamming distance d of the code.\n{ A code with Hamming distance d allows up to {d-1}\nerrors to be corrected}, where  represents the\ninteger floor function on x, or rounding x down to the nearest \ninteger.\n\n\n \n Question: What is error correction? \n Answer: \n\nError correction is the process of detecting and correcting errors in digital data."}, {"text": "Title: Hamming Codes \n Text: {Hamming Codes}\n\nHamming also developed a general and efficient approach for \nextending an arbitrary representation to allow correction of\na single bit error.  The approach yields codes with Hamming distance 3.\n\nTo understand how a { Hamming code} works, think of the bits in the\nrepresentation as being numbered starting from 1.  For example, if\nwe have seven bits in the code, we might write a bit pattern X \nas x_7x_6x_5x_4x_3x_2x_1.\n\nThe bits with indices that are powers of two are parity check bits.\nThese include x_1, x_2, x_4, x_8, and so forth.  The remaining\nbits can be used to hold data.  For example, we could use a {7-bit}\nHamming code and map the bits from a {4-bit} unsigned representation \ninto bits x_7, x_6, x_5, and x_3.  Notice that Hamming codes are\nnot so useful for small numbers of bits, but require only logarithmic\noverhead for large numbers of bits.  That is, in an {N-bit}\nHamming code, only _2(N+1) bits are used for parity\nchecks.\n\nHow are the parity checks defined?  Each parity bit is used to\nprovide even parity for those bits with indices for which the index,\nwhen written in binary, includes a 1 in the single position in which\nthe parity bit's index contains a 1.  The x_1 bit, for example,\nprovides even parity on all bits with odd indices.  The x_2 bit\nprovides even parity on x_2, x_3, x_6, x_7, x_, and so\nforth.\n\nIn a {7-bit} Hamming code, for example, \nx_1 is chosen so that\nit has even parity together with x_3, x_5, and x_7.\n\nSimilarly, x_2 is chosen so that\nit has even parity together with x_3, x_6, and x_7.\n\nFinally, x_4 is chosen so that\nit has even parity together with x_5, x_6, and x_7.\n\n\nThe table to the right shows the result of embedding \na {4-bit} unsigned representation into a {7-bit}\nHamming code.\n\nA Hamming code provides a convenient way to identify\nwhich bit should be corrected when a single bit error occurs.  \nNotice that each bit is protected by a unique subset of the parity bits\ncorresponding to the binary form of the bit's index.  Bit x_6, for\nexample, is protected by bits x_4 and x_2, because the number 6\nis written 110 in binary.\nIf a bit is affected by an error, the parity bits\nthat register the error are those corresponding to 1s in the binary\nnumber of the index.  So if we calculate check bits as 1 to represent\nan error (odd parity) and 0 to represent no error (even parity),\nthen concatenate those bits into a binary number, we obtain the\nbinary value of the index of the single bit affected by an error (or \nthe number 0\nif no error has occurred).\n\n\n{c|c|c|c|c|c}\n           &  4-bit         &     &     &     &7-bit\nvalue      & unsigned       &     &     &     &Hamming\nrepresented&(x_7x_6x_5x_3)&x_4&x_2&x_1&code \n 0& 0000& 0& 0& 0& 0000000\n 1& 0001& 0& 1& 1& 0000111\n 2& 0010& 1& 0& 1& 0011001\n 3& 0011& 1& 1& 0& 0011110\n 4& 0100& 1& 1& 0& 0101010\n 5& 0101& 1& 0& 1& 0101101\n 6& 0110& 0& 1& 1& 0110011\n 7& 0111& 0& 0& 0& 0110100\n 8& 1000& 1& 1& 1& 1001011\n 9& 1001& 1& 0& 0& 1001100\n10& 1010& 0& 1& 0& 1010010\n11& 1011& 0& 0& 1& 1010101\n12& 1100& 0& 0& 1& 1100001\n13& 1101& 0& 1& 0& 1100110\n14& 1110& 1& 0& 0& 1111000\n15& 1111& 1& 1& 1& 1111111\n\n\n\nLet's do a couple of examples based on the pattern for the decimal\nnumber 9, 1001100.  First, assume that no error occurs.  We calculate\ncheck bit c_4 by checking whether x_4, x_5, x_6, and x_7 together\nhave even parity.  Since no error occurred, they do, so c_4=0.\nSimilarly, for c_2 we consider x_2, x_3, x_6, and x_7.\nThese also have even parity, so c_2=0.  Finally, for c_1, we \nconsider x_1, x_3, x_5, and x_7.  As with the others, these\ntogether have even parity, so c_1=0.  Writing c_4c_2c_1, we obtain 000,\nand conclude that no error has occurred.\n\nNext assume that bit 3 has an error, giving us the pattern 1001000.\nIn this case, we have again that c_4=0, but the bits corresponding to\nboth c_2 and c_1 have odd parity, so c2=1 and c_1=1.  Now when\nwe write the check bits c_4c_2c_1, we obtain 011,\nand we are able to recognize that bit 3 has been changed.\n\nA Hamming code can only correct one bit error, however.  If two bit\nerrors occur, correction will produce the wrong answer.  Let's imagine\nthat both bits 3 and 5 have been flipped in our example pattern for\nthe decimal number 9, producing the pattern 1011000.  Calculating the\ncheck bits as before and writing them as c_4c_2c_1, we obtain 110,\nwhich leads us to incorrectly conclude that bit 6 has been flipped.\nAs a result, we ``correct'' the pattern to 1111000, which represents\nthe decimal number 14.\n\n\n \n Question: What is the Hamming distance of a Hamming code? \n Answer: \n\nA. The Hamming distance of a Hamming code is 3."}, {"text": "Title: SEC-DED Codes \n Text: {SEC-DED Codes}\n\nWe now consider one final extension of Hamming codes to enable a system\nto perform single error correction while also detecting any two bit errors.\nSuch codes are known as { Single Error Correction, Double Error \nDetection (SEC-DED)} codes.  Creating such a code from a Hamming code is\ntrivial: add a parity bit covering the entire Hamming code.  The extra\nparity bit increases the Hamming distance to 4.  A Hamming distance of 4\nstill allows only single bit error correction, but avoids the problem\nof Hamming distance 3 codes when two bit errors occur, since patterns\nat Hamming distance 2 from a valid code word cannot be within distance 1\nof another code word, and thus cannot be ``corrected'' to the wrong\nresult.\n\nIn fact, one can add a parity bit to any representation with an odd\nHamming distance to create a new representation with Hamming distance\none greater than the original representation.  To proof this convenient\nfact, begin with a representation with Hamming distance d, where d\nis odd.  If we choose two code words from the representation, and their \nHamming distance is already greater than d, their distance in the \nnew representation will also be greater than d.  Adding a parity\nbit cannot decrease the distance.  On the other hand, if the two code\nwords are exactly distance d apart, they must have opposite parity,\nsince they differ by an odd number of bits.  Thus the new parity bit\nwill be a 0 for one of the code words and a 1 for the other, increasing\nthe Hamming distance to d+1 in the new representation.  Since all\npairs of code words have Hamming distance of at least d+1, the\nnew representation also has Hamming distance d+1.\n\n\n\n\n\n \n Question: What is the Hamming distance of a SEC-DED code? \n Answer: \n\nA. The Hamming distance of a SEC-DED code is 4."}, {"text": "Title: Summary of Part 4 of the Course \n Text: {Summary of Part 4 of the Course}\n\nWith the exception of control unit design strategies and redundancy \nand coding, most of the material in this part of the course is drawn from\nPatt and Patel Chapters 4 through 7.  You may also want to read Patt and \nPatel's Appendix C for details of their control unit design.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nWe'll start with the easy stuff.  You should recognize all of these terms\nand be able to explain what they mean.  \n For the specific circuits, you \n should be able to draw them and explain how they work.\n(You may skip the *'d terms in Fall 2012.)\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann elements}\n{-}{{}{}\n{}{}{}\n{program counter (PC)}\n{instruction register (IR)}\n{memory address register (MAR)}\n{memory data register (MDR)}\n{processor datapath}\n\n{control signal}\n{instruction processing}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (in an encoded instruction)}\n{operation code (opcode)}\n\n\n{assemblers and assembly code}\n{-}{{}{}\n{}{}{}\n{opcode mnemonic (such as ADD, JMP)}\n{two-pass process}\n\n{symbol table}\n{pseudo-op / directive}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{systematic decomposition}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n no documentation, and advanced topics ... no testing\n\n {logic design optimization}\n {-}{{}{}\n {}{}{}\n {bit-sliced (including multiple bits per slice)}\n \n {pipelined logic}\n {tree-based}\n \n\n{control unit design strategies}\n{-}{{}{}\n{}{}{}\n{control word / microinstruction}\n{sequencing / microsequencing}\n{hardwired control}\n{-}{{}{}\n{}{}{}\n{single-cycle}\n{multi-cycle}\n\n{microprogrammed control}\n {pipelining (of instruction processing)}\n\n\n{error detection and correction\n{--}{{}{}\n{}{}{}\n code/sparse representation\n code word\n bit error\n odd/even parity bit\n Hamming distance between code words\n Hamming distance of a code\n Hamming code\n SEC-DED\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n FIXME ... should write something about critical path, but expecting\n them to do much with it isn't reasonable\n\n {Implement arbitrary Boolean logic, and be able to reason about\n alternative designs in terms of their critical path delays and number\n of gates required to implement.}\n\n{Map RTL (register transfer language) operations into control words\nfor a given processor datapath.}\n\n{Systematically decompose a (simple enough) problem to the level \nof {LC-3} instructions.}\n\n{Encode {LC-3} instructions into machine code.}\n\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n{Test and debug a small program in {LC-3} assembly/machine code.}\n\n{Be able to calculate the Hamming distance of a code/representation.}\n\n{Know the relationships between Hamming distance and the abilities\nto detect and to correct bit errors.}\n\n\n\nWe expect that you will understand the concepts and ideas to the extent\nthat you can do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the role of different types of instructions in allowing\na programmer to express a computation.}\n\n FIXME: ISA design is beyond the scope of this course; just include\n for interest, if at all\n\n {Explain the tradeoffs in different addressing modes so as to motivate\n inclusion of multiple addressing modes in an ISA.}\n\n{Explain the importance of the three types of subdivisions in systematic\ndecomposition (sequential, conditional, and iterative).}\n\n{Explain the process of transforming assembly code into machine code\n(that is, explain how an assembler works, including describing the use of\nthe symbol table).}\n\n{Be able to use parity for error detection, and Hamming codes for\nerror correction.}\n\n\n\nAt the highest level, \nwe hope that, while you do not have direct substantial experience in \nthis regard from our class (and should not expect to be tested on these\nskills), that you will nonetheless be able to begin\nto do the following when designing combinational logic:\n\n{}{{}{}\n{}{}{}\n\n{Design and compare implementations using gates, decoders, muxes, and/or memories \nas appropriate, and including reasoning about the relevant design tradeoffs \nin terms of area and delay.}\n\n{Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based \ndesign, again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Design and compare implementations of processor control units\nusing both hardwired and microprogrammed strategies,\nand again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Understand basic tradeoffs in the sparsity of code words with error \ndetection and correction capabilities.}\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n \n Question: What is the purpose of the assembler course? \n Answer: \n\nAn assembler is a program that takes assembly code, which is code written in a human-readable form, and translates it into machine code, which is code that can be run by a computer."}, {"text": "Title: Example: Bit-Sliced Addition \n Text: {Example: Bit-Sliced Addition}\n\nIn this set of notes, we illustrate basic logic design using integer\naddition as an example.  By recognizing and mimicking the structured \napproach used by humans to perform addition, we introduce an important \nabstraction for logic design.  We follow this approach to design an\nadder known as a ripple-carry adder, then discuss some of the \nimplications of the approach and highlight how the same approach can \nbe used in software.  In the next set of notes, we use the same\ntechnique to design a comparator for two integers.\n\n \n Question: What is the traditional adder's approach to logic design? \n Answer: \n\nA. A ripple-carry adder is a type of adder that uses a ripple-carry technique to propagate the carry bit through each stage of the addition process. A traditional adder, on the other hand, does not use this technique and instead propagates the carry bit through each stage in a different way."}, {"text": "Title: One Bit at a Time \n Text: {One Bit at a Time}\n\nMany of the operations that we want to perform on groups of bits can \nbe broken down into repeated operations on individual bits.\n\nWhen we add two binary numbers, for example, we first add the least\nsignificant bits, then move to the second least significant, and so on.\nAs we go, we may need to carry from lower bits into higher bits.\n\nWhen we compare two (unsigned) binary numbers with the same number of bits,\nwe usually start with the most significant bits and move downward in\nsignificance until we find a difference or reach the end of the two \nnumbers.  In the latter case, the two numbers are equal.\n\nWhen we build combinational logic to implement this kind of calculation,\nour approach as humans can be leveraged as an abstraction technique.\nRather than building and optimizing a different Boolean function for \nan 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size\nthat we might want, we can instead design a circuit that adds a single\nbit and passes any necessary information into another copy of itself.\nBy using copies of this { bit-sliced} adder circuit, we can\nmimic our approach as humans and build adders of any size, just as\nwe expect that a human could add two binary numbers of any size.\n\nThe resulting designs are, of course, slightly less efficient than\ndesigns that are optimized for their specific purpose (such as adding\ntwo 17-bit numbers), but the simplicity of the approach makes the\ntradeoff an interesting one.\n\n\n \n Question: How can we mimic our approach as humans? \n Answer: "}, {"text": "Title: Abstracting the Human Process \n Text: {Abstracting the Human Process}\n\n\nThink about how we as humans add two {N-bit}\nnumbers, A and B.\n\nAn illustration appears to the right, using N=8.\n\nFor now, let's assume that our numbers are stored in an unsigned \nrepresentation.\n\nAs you know, addition for 2's complement is identical except for the\ncalculation of overflow.\n\nWe start adding from the least significant bit and move to the left.\nSince adding two 1s can overflow a single bit, we carry a 1 when\nnecessary into the next column.  Thus, in general, we are actually\nadding three input bits.  The carry from the previous column is usually\nnot written explicitly by humans, but in a digital system\nwe need to write a 0 instead of leaving the value blank.\n\nFocus now on the addition of a single column.  Except for the\nfirst and last bits, which we might choose to handle slightly \ndifferently, the addition process is identical \nfor any column.  We add a carry in bit (possibly 0) with one\nbit from each of our numbers to produce a sum bit and a carry\nout bit for the next column.  Column addition is the task\nthat our bit slice logic must perform.\n\nThe diagram to the right shows an abstract model of our \nadder bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming\nfrom the top or left \nand outputs going to the bottom or right.  Outside\nof the bit slice logic, we index the carry bits using the\n\n\n\n{{file=part2/figs/add-abs.eps,width=1.90in}}\n\n\n\n\nbit number.  The bit slice has C^M provided as an input and \nproduces C^{M+1} as an output.\n\nInternally, we use C_ to denote the carry input,\nand C_ to denote the carry output.\n\nSimilarly, the\nbits A_M and B_M from the numbers A and B are\nrepresented internally as A and B, and the bit S_M produced for\nthe sum S is represented internally as S.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.\n\nThe abstract device for adding three inputs bits and producing\ntwo output bits is called a { full adder}.  You may \nalso encounter the term { half adder}, which adds only two\ninput bits.  To form an {N-bit} adder, we integrate N\ncopies of the full adder---the bit slice that we design next---as \nshown below.  The result is called a { ripple carry adder}\nbecause the carry information moves from the low bits to the high\nbits slowly, like a ripple on the surface of a pond.\n\n{{file=part2/figs/add-integrated.eps,width=5.5in}}\n\n\n \n Question: What is the name of the abstract device for adding three inputs and producing two output bits? \n Answer: \n\nA. A full adder works by adding three input bits and producing two output bits."}, {"text": "Title: Designing the Logic \n Text: {Designing the Logic}\n\nNow we are ready to design our adder bit slice.\nLet's start by writing a truth table for C_ and S,\nas shown on the left below.\n\nTo the right of the truth tables are {K-maps} for each output,\nand equations for each output are then shown to the right of the \n{K-maps}.\n\nWe suggest that you work through identification of the prime implicants \nin the {K-maps} and check your work with the equations.\n\n{\n[t]\n{ccc|cc}\nA& B& C_& C_& S \n0& 0& 0& 0& 0\n0& 1& 0& 0& 1\n0& 0& 1& 0& 1\n0& 1& 1& 1& 0\n1& 0& 0& 0& 1\n1& 0& 1& 1& 0\n1& 1& 0& 1& 0\n1& 1& 1& 1& 1\n\n\n\n{file=part2/figs/add-cin.eps,width=1in}\n{file=part2/figs/add-s.eps,width=1in}\n\n\n{eqnarray*}\nC_&=&A B+A C_+B C_   \nS&=&A B C_+A  {C_+\n&& B {C_+  C_\n&=&A{C_\n{eqnarray*}\n\n}\n\nThe equation for C_ implements a { majority function}\non three bits.  In particular, a carry is produced whenever at least two \nout of the three input bits (a majority) are 1s.  Why do we mention\nthis name?  Although we know that we can build any logic function from\nNAND gates, common functions such as those used to add numbers may\nbenefit from optimization.  Imagine that in some technology, creating\na majority function directly may produce a better result than implementing\nsuch a function from logic gates.  In such a case, we want the \nperson designing the circuit to know that can make use of such an \nimprovement.\n\nWe rewrote the equation for S to make use of the XOR operation for \na similar reason: the implementation of XOR gates from transistors\nmay be slightly better than the implementation of XOR based on NAND gates.\nIf a circuit designer provides an optimized variant of XOR, we want\nour design to make use of the optimized version.\n\n{{file=part2/figs/adder-bit.eps,width=2.55in}\n{file=part2/figs/adder-bit-nn.eps,width=2.55in}}\n\nThe gate diagrams above implement a single bit slice for an adder.\nThe version on\nthe left uses AND and OR gates (and an XOR for the sum), while the\nversion on the right uses NAND gates, leaving the XOR as an XOR.\n\nLet's discuss the design in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\nFor each bit, we need three {2-input} NAND gates, one {3-input}\nNAND gate, and a {3-input} XOR gate (a big gate; around 30 transistors).\n\nFor speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.  \n\nWe can thus estimate our design's\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer.\nHere we have two gate delays from any of the inputs\nto the C_ output.  The XOR gate may be a little slower, but\nnone of its inputs come from other gates anyway.\n\nWhen we connect multiple copies of our bit slice logic together to \nform an adder, the A and B inputs\nto the outputs is not as important as the delay from C_\nto the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis---this propagation delay\ngives rise to the name ``ripple carry.''\nLooking again at the diagram, \nnotice that we have two gate delays from C_\nto C_.\nThe total delay for an {N-bit} comparator based on this\nimplementation is thus two gate delays per bit, \nfor a total of 2N gate delays.\n\n\n \n Question: What are the inputs to a bit slice? \n Answer: \n\nA. The inputs to a bit slice are A and B, and the outputs are S and C_."}, {"text": "Title: Adders and Word Size\\vspace12pt \n Text: {Adders and Word Size}\n\n\nNow that we know how to build an {N-bit} adder, we can add\nsome detail to the diagram that we drew when we \nintroduced 2's complement back in Notes Set 1.2, as shown to the right.\n\nThe adder is important enough to computer systems to merit its own\nsymbol in logic diagrams, which is shown to the right with the inputs\nand outputs from our design added as labels.  The text in the middle\nmarking the symbol as an adder is only included for clarity: { any time \nyou see a symbol of the shape shown to the right, it is an adder} (or \nsometimes a device that can add and do other operations).  The width \nof the operand input and output lines then tells you the size of the \nadder.\n\n\n{file=part2/figs/adder-trad.eps,width=1.3in}\n\n\nYou may already know that most computers have a { word size}\nspecified as part of the Instruction Set Architecture.  The word\nsize specifies the number of bits in each operand when the computer\nadds two numbers, and is often used widely within the \nmicroarchitecture as well (for example, to decide the number of \nwires to use when moving bits around).  Most desktop and laptop machines\nnow have a word size of 64 bits, but many phone processors (and\ndesktops/laptops a few years ago) use a {32-bit} word size.\nEmbedded microcontrollers may use a {16-bit} or even \nan {8-bit} word size.\n\n\nHaving seen how we can build an {N-bit} adder from simple\nchunks of logic operating on each pair of bits, you should not have\nmuch difficulty in understanding the diagram to the right.\n\nIf we start with a design for an {N-bit} adder---even if that\ndesign is not built from bit slices, but is instead optimized for\nthat particular size---we can create a {2N-bit} adder by \nsimply connecting two copies of the {N-bit} adder.  We give\nthe adder for the less significant bits (the one on the right\nin the figure) an initial carry of 0,\nand pass the carry produced by the adder for the less significant\nbits into the carry input of the adder for the more significant\nbits.  We calculate overflow based on the results of the adder\nfor more significant bits (the one on the left in the figure), \nusing the method appropriate to the \ntype of operands we are adding (either unsigned or 2's complement).\n\n\n{file=part2/figs/adder-x2.eps,width=2.15in}\n\n\nYou should also realize that this connection need not be physical.\nIn other words, if a computer has an {N-bit} adder, it can\nhandle operands with 2N bits (or 3N, or 10N, or 42N) by\nusing the {N-bit} adder repeatedly, starting with the\nleast significant bits and working upward until all of the bits\nhave been added.  The computer must of course arrange to have the\noperands routed to the adder a few bits at a time, and must\nensure that the carry produced by each addition is then delivered to\nthe carry input (of the same adder!) for the next addition.\nIn the coming months, you will learn how to design hardware that\nallows you to manage bits in this way, so that by the end of our\nclass, you will be able to design a simple computer on your own.\n\n\n\n\n\n \n Question: What is the symbol of an adder? \n Answer: \n\nA.\n\nAn adder is a device that can add two numbers.  The adder is important enough to computer systems to merit its own symbol in logic diagrams, which is shown to the right with the inputs and outputs from our design added as labels.  The width of the operand input and output lines then tells you the size of the adder."}, {"text": "Title: Summary of Part 2 of the Course \n Text: {Summary of Part 2 of the Course}\n\nThese notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.\n\nThe difficulty of learning depends on the type of task involved.\nRemembering new terminology is relatively easy, while applying\nthe ideas underlying design decisions shown by example to new problems \nposed as human tasks is relatively hard.\n\nIn this short summary, we give you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nWe'll start with the skills, and leave the easy stuff for the next page.\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Design a CMOS gate for a simple Boolean function from n-type \nand p-type transistors.}\n\n{Apply DeMorgan's laws repeatedly to simplify the form of\nthe complement of a Boolean expression.}\n\n{Use a K-map to find a reasonable expression for a Boolean function (for\nexample, in POS or SOP form with the minimal number of terms).}\n\n{More generally, translate Boolean logic functions among \nconcise algebraic, truth table, K-map, and canonical (minterm/maxterm) forms.}\n\n\n\nWhen designing combinational logic, we expect you to be able to apply\nthe following design strategies:\n\n{}{{}{}\n{}{}{}\n\n{Make use of human algorithms \n(for example, multiplication from addition).}\n\n{Determine whether a bit-sliced approach is applicable, and, if so,\nmake use of one.}\n\n{Break truth tables into parts so as to solve each part of a function \nseparately.}\n\n{Make use of known abstractions (adders, comparators, muxes, or other\nabstractions available to you) to simplify the problem.}\n\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Understand and be able to reason at a high-level about circuit design\ntradeoffs between area/cost and performance (and to know that power is also \nimportant, but we haven't given you any quantification methods).}\n\n{Understand the tradeoffs typically made to develop bit-sliced \ndesigns---typically, bit-sliced designs are simpler but bigger and \nslower---and how one can develop variants between the extremes of\nthe bit-sliced approach and optimization of functions specific\nto an {N-bit} design.}\n\n{Understand the pitfalls of marking a function's value as ``don't care'' \nfor some input combinations, and recognize that implementations do not \nproduce ``don't care.''}\n\n{Understand the tradeoffs involved in selecting a representation for\ncommunicating information between elements in a design, such as the bit \nslices in a bit-sliced design.}\n\n{Explain the operation of a latch or a flip-flop, particularly in \nterms of the bistable states used to hold a bit.}\n\n{Understand and be able to articulate the value of the clocked \nsynchronous design abstraction.}\n\n\n\n\n\n\n\nYou should recognize all of these terms\nand be able to explain what they mean.  For the specific circuits, you \nshould be able to draw them and explain how they work.\n\nActually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  \n\n[t]\n{}{{}{}\n{}{}{}\n\n{Boolean functions and logic gates}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n\n\n{majority function}\n\n\n{specific logic circuits}\n{-}{{}{}\n{}{}{}\n{full adder}\n{half adder}\n{ripple carry adder}\n N-to-M multiplexer (mux)\n N-to-2N decoder\n{{- latch}}\n{{R-S latch}}\n{gated D latch}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation of a positive edge-triggered D flip-flop}\n{dual-latch implementation of a positive edge-triggered D flip-flop}\n{(bidirectional) shift register}\n{register supporting parallel load}\n\n\n{design metrics}\n{-}{{}{}\n{}{}{}\n\n\n\n\n{power, area/cost, performance}\n{computer-aided design (CAD) tools}\n{gate delay}\n\n\n{general math concepts}\n{-}{{}{}\n{}{}{}\n{canonical form}\n{domain of a function}\n{{N-dimensional} hypercube}\n\n\n{tools for solving logic problems}\n{-}{{}{}\n{}{}{}\n{truth table}\n{Karnaugh map (K-map)}\n\n{prime implicant}\n{bit-slicing}\n{timing diagram}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{device technology}\n{-}{{}{}\n{}{}{}\n{complementary metal-oxide semiconductor (CMOS)}\n{field effect transistor (FET)}\n{transistor gate, source, drain}\n\n\n{Boolean logic terms}\n{-}{{}{}\n{}{}{}\n\n{algebraic properties}\n{dual form, principle of duality}\n{sum, product}\n{minterm, maxterm}\n{sum-of-products (SOP)}\n{product-of-sums (POS)}\n{canonical sum/SOP form}\n{canonical product/POS form}\n{logical equivalence}\n\n\n{digital systems terms}\n{-}{{}{}\n{}{}{}\n{word size}\n{{N-bit} Gray code}\n{combinational/combinatorial logic}\n{-}{{}{}\n{}{}{}\n{two-level logic}\n{``don't care'' outputs (x's)}\n\n{sequential logic}\n{-}{{}{}\n{}{}{}\n\n{active low input}\n{set a bit (to 1)}\n{reset a bit (to 0)}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation}\n{dual-latch implementation}\n{positive edge-triggered}\n\n{clock signal}\n{-}{{}{}\n{}{}{}\n{square wave}\n{rising/positive clock edge}\n{falling/negative clock edge}\n{clock gating}\n\n{clocked synchronous sequential circuit}\n{parallel/serial load of register}\n FIXME?  too informal to ask them to remember it\n {glue logic}\n{logical/arithmetic/cyclic shift}\n\n\n\n\n\n\n\n\n\n{   }  blank 3rd page\n\n\n\n\n\n\n\n \n Question: What is the basic concept of a flip-flop? \n Answer: \n\nA. A latch is a circuit that is used to store a bit of information, while a flip-flop is a type of latch that is used to store a bit of information in a way that allows it to be changed."}, {"text": "Title: Registers \n Text: \n\nThis set of notes introduces registers, an abstraction used for \nstorage of groups of bits in digital systems.  We introduce some\nterminology used to describe aspects of register design and\nillustrate the idea of a shift register.  The registers shown here\nare important abstractions for digital system design.\n\n { In the Fall 2012 offering of our course, we will cover this\n material on the third midterm.}\n\n\n \n Question: What is the purpose of the registers? \n Answer: "}, {"text": "Title: Registers \n Text: \n\n\nA { register} is a storage element composed from one or more\nflip-flops operating on a common clock.\n\nIn addition to the flip-flops,\nmost registers include logic to control the bits stored by the register.\n\nFor example, D flip-flops \ncopy their inputs at the rising edge of each clock cycle,\ndiscarding whatever bits they have stored before the rising edge\n(in the previous clock cycle).\n\nTo enable a flip-flop to retain its value, we might try to hide the \nrising edge of the clock from the flip-flop, as shown to the right.\n\nThe LOAD input controls the clock signals through a method known as\n{ clock gating}.\n\n\n{{file=part2/figs/lec16-1a.eps,width=2.45in}}\n{file=part2/figs/lec16-1b.eps,width=3.4in}\n\n\nWhen LOAD is high, the circuit reduces to a\nregular D flip-flop.  When LOAD is low, the flip-flop clock \ninput, c, is held high, and the flip-flop stores its \ncurrent value.\n\nThe problems with clock gating are twofold.  First, adding logic to\nthe clock path introduces clock skew, which may cause timing problems\nlater in the development process (or, worse, in future projects that\n\nuse your circuits as components).  Second, in the design shown above,\nthe LOAD signal\ncan only be lowered while the clock is high to prevent spurious rising\nedges from causing incorrect behavior, as shown in the timing diagram.\n\n\nA better approach is to use a mux and a feedback loop from the \nflip-flop's output, as shown in the figure to the right.  \n\nWhen LOAD is low, the mux selects the feedback line, and the \nregister reloads its current value.  \n\nWhen LOAD is high, the mux selects the IN input, and the register \nloads a new value.  \n\nThe result is similar to a gated D latch with distinct write enable \nand clock lines.\n\n\n{file=part2/figs/lec16-2.eps,width=2in}\n\n\n\nWe can use this extended flip-flop as a bit slice for a multi-bit register.\n\nA four-bit register of this type\nis shown to the right.  Four data {lines---one} for each\n{bit---enter} the registers from the top of the figure.  \nWhen LOAD is low, the logic copies each flip-flop's value back to its\ninput,\n\n\n{file=part2/figs/lec16-3.eps,width=5in}\n\n\nand the IN input lines are ignored.  When LOAD is high,\nthe muxes forward each IN line to the corresponding flip-flop's D\ninput, allowing the register to load the new {4-bit} value.\nThe use of one input line per bit to load a multi-bit\nregister in a single cycle is termed a { parallel load}.\n\n \n Question: What is the difference between a gated D latch and a LOAD low register? \n Answer: \n\nA. The extended flip-flop uses a mux and a feedback loop from the flip-flop's output to reload its current value when LOAD is low. When LOAD is high, the mux selects the IN input, and the register loads a new value."}, {"text": "Title: Shift Registers \n Text: {Shift Registers}\n\n\nCertain types of registers include logic to manipulate data held\nwithin the register.  A { shift register} is an important example\nof this\n\n\n{file=part2/figs/lec16-4.eps,width=5in}\n\n\ntype.  The simplest shift register is a series of D flip-flops,\nwith the output of each attached to the input of the next, as shown to the\nright above.  In the circuit shown, a serial input SI accepts a single bit \nof data per cycle and delivers the bit four cycles later to a serial \noutput SO.  Shift registers serve many purposes in modern systems, from the\nobvious uses of providing a fixed delay and performing bit shifts for\nprocessor arithmetic to rate matching between components and reducing\nthe pin count on programmable logic devices such as field programmable\ngate arrays (FPGAs), the modern form of the programmable logic array\nmentioned in the textbook.\n\nAn example helps to illustrate the rate matching problem: \nhistorical I/O buses used fairly slow clocks, as they had to\ndrive signals and be arbitrated over relatively long distances.\nThe Peripheral Control\nInterconnect (PCI) standard, for example, provided for 33 and 66 MHz\nbus speeds.  To provide adequate data rates, such buses use many wires\nin parallel, either 32 or 64 in the case of PCI.  In contrast, a\nGigabit Ethernet (local area network) signal travelling over a fiber\nis clocked at 1.25 GHz, but sends only one bit per cycle.  Several\nlayers of shift registers sit between the fiber and the I/O bus to\nmediate between the slow, highly parallel signals that travel over the\nI/O bus and the fast, serial signals that travel over the \nfiber.  The latest variant of PCI, PCIe (e for ``express''),\nuses serial lines at much higher clock rates.\n\nReturning to the figure above, imagine that the outputs Q_i feed\ninto logic clocked at 1/4^ the rate of the shift register \n(and suitably synchronized).  Every four cycles, the flip-flops fill\nup with another four bits, at which point the outputs are read in\nparallel.  The shift register shown can thus serve to transform serial\ndata to {4-bit-parallel} data at one-quarter the clock speed.\nUnlike the registers discussed earlier, the shift register above does\nnot support parallel load, which prevents it from transforming a slow,\nparallel stream of data into a high-speed serial stream.  The use of\n{ serial load} requires N cycles for an {N-bit}\nregister, but can reduce the number of wires needed to support the\noperation of the shift register.  How would you add support for\nparallel load?  How many additional inputs would be necessary?\n\nThe shift register above also shifts continuously, and cannot store a \nvalue.  A set of muxes, analogous to those that we used to control \nregister loading, can be applied to control shifting, as shown \nbelow.\n\n{{file=part2/figs/lec16-5.eps,width=5.3in}}\n\nUsing a {4-to-1} mux, we can construct a shift\nregister with additional functionality.  The bit slice at the top\nof the next page allows us to build a { bidirectional shift register} with \nparallel load capability and the ability to retain its value indefinitely.\nThe two-bit control input C uses a representation that\nwe have chosen for the four operations supported by our shift register, \nas shown in the table below the bit slice design.\n\n\nThe bit slice allows us to build {N-bit} shift registers by\nreplicating the slice and adding a fixed amount of ``{ glue logic}.''\nFor example, the figure below represents a {4-bit} bidirectional \nshift register constructed in this way.  The mux\nused for the SO output logic is the glue logic needed in addition\nto the four bit slices.\n\nAt each rising clock edge, the action specified by C_1C_0 is taken.  \nWhen C_1C_0=00, the\nregister holds its current value, with the register\nvalue appearing on\nQ[3:0] and each flip-flop feeding its output back into its input.\nFor C_1C_0=01, the shift register shifts left: the serial input,\nSI, is fed into flip-flop 0, and Q_3 is passed to the serial\noutput, SO.  Similarly, when C_1C_0=11, the shift register shifts\nright: SI is fed into flip-flop 3, and Q_0 is passed to SO.\nFinally, the case C_1C_0=10 causes all flip-flops to accept new\nvalues from IN[3:0], effecting a parallel load.\n\n\n{file=part2/figs/lec16-6.eps,width=2.3in}\n{c|c}\nC_1C_0& meaning \n00& retain current value\n01& shift left (low to high)\n10& load new value (from IN)\n11& shift right (high to low)\n\n{-4pt}\n\n{{file=part2/figs/lec16-7.eps,width=5.2in}}\n\nSeveral specialized shift operations are used to support data\nmanipulation in modern processors (CPUs).  Essentially, these\nspecializations dictate the glue logic for a shift\nregister as well as the serial input value.  The simplest is a {\nlogical shift}, for which SI is hardwired to 0: incoming\nbits are always 0.  A { cyclic shift} takes SO and feeds it\nback into SI, forming a circle of register bits through which the\ndata bits cycle.\n\nFinally, an { arithmetic shift} treats the shift register contents\nas a number in 2's complement form.  For non-negative numbers and left\nshifts, an arithmetic shift is the same as a logical\nshift.  When a negative number is arithmetically shifted to\nthe right, however, the sign bit is retained, resulting in a function\nsimilar to division by two.  The difference lies in the rounding\ndirection.  Division by two rounds towards zero in most \nprocessors: -5/2 gives -2.\nArithmetic shift right rounds away from zero for negative numbers (and\ntowards zero for positive numbers): -5>>1 gives -3.  We transform our\nprevious shift register into one capable of arithmetic shifts by\neliminating the serial input and feeding the most significant bit,\nwhich represents the sign in 2's complement form, back into itself for\nright shifts, as shown below.  The bit shifted in for left shifts\nhas been hardwired to 0.\n\n{{file=part2/figs/lec16-8.eps,width=5.2in}}\n\n\n\n \n Question: What type of register is an important example of? \n Answer: \n\nA shift register is a register that includes logic to manipulate data held within the register. A shift register is an important example of this type."}, {"text": "Title: Boolean Properties and Don't Care Simplification \n Text: {Boolean Properties and Don't Care Simplification}\n\nThis set of notes begins with a brief illustration of a few properties\nof Boolean logic, which may be of use to you in manipulating algebraic\nexpressions and in identifying equivalent logic functions without resorting \nto truth tables.\n\nWe then discuss the value of underspecifying a logic function so as\nto allow for selection of the simplest possible implementation.\n\nThis technique must be used carefully to avoid incorrect behavior,\nso we illustrate the possibility of misuse with an example, then\ntalk about several ways of solving the example correctly.\n\nWe conclude by generalizing the ideas in the example to several\nimportant application areas and talking about related problems.\n\n \n Question: What is the value of underspecifying a logic function? \n Answer: \n\nA. The value of underspecifying a logic function is that it allows for selection of the simplest possible implementation."}, {"text": "Title: Logic Properties \n Text: {Logic Properties}\n\nTable  (on the next page)\nlists a number of properties of Boolean logic.\nMost of these are easy to derive from our earlier definitions, but\na few may be surprising to you.  In particular, in the algebra of\nreal numbers, multiplication\ndistributes over addition, but addition does not distribute over\nmultiplication.  For example, 3{(4+7)}=(3)+(3),\nbut 3+(4)=(3+4){(3+7)}.  In Boolean algebra,\nboth operators distribute over one another, as indicated in\nTable .  The consensus properties may also be \nnonintuitive.  Drawing a {K-map} may help you understand the\nconsensus property on the right side of the table.  For the \nconsensus variant on the left side of the table,\nconsider that since either A or  must be 0,\neither B or C or both must be 1 for the first two factors on the left\nto be 1 when ANDed together.  But in that case, the third factor \nis also 1, and is thus redundant.\n\nAs mentioned previously, Boolean\nalgebra has an elegant symmetry known as a duality, in which any\nlogic statement (an expression or an equation) is related to a\nsecond logic statement.\n\nTo calculate the { dual form} of a Boolean expression or equation, \nreplace 0 with 1, replace 1 with 0, \nreplace AND with OR, and\nreplace OR with AND.\n\n{ Variables are not changed when finding the dual form.}\n\nThe dual form of a dual form is the original logic statement.\n\nBe careful when calculating a dual form: our convention for ordering \narithmetic operations is broken by the exchange, so you may want\nto add explicit parentheses before calculating the dual.  For\nexample, the dual of AB+C is not A+BC.\nRather, the dual of AB+C is (A+B)C.\n\n{ Add parentheses as necessary when calculating a dual form to ensure\nthat the order of operations does not change.}\n\nDuality has several useful practical applications.  First, the \n{ principle of duality} states that any theorem or identity has \nthe same truth value in dual form (we do not prove the principle\nhere).\n\nThe rows of Table  are organized according to this\nprinciple: each row contains two equations that are the duals \nof one another.  \n However, we rename variables freely to\n simplify the dual form.  For example, taking the dual of 1+A=1,\n we obtain 0=0.  But we created variable A \n solely for the purpose of expressing the property, so we might\n instead have called the complementary value A.  We rename\n (using the same name!) to obtain 0=0, as shown in the table.\n\nSecond, the dual form is useful when designing certain types of logic,\nsuch as the networks of transistors connecting the output of a CMOS\ngate to high voltage and ground.  If you look at the gate designs in\nthe textbook (and particularly those in the exercises),\nyou will notice that these networks are duals.  \n\nA function/expression is not a theorem nor an identity,\nthus the principle of duality does not apply to the dual of an\nexpression.\n\nHowever, if you treat the value 0 as ``true,'' the dual form of an\nexpression has the same truth values as the original (operating with\nvalue 1 as ``true'').\n\nFinally, you can calculate the complement of a Boolean function\n(any expression) by calculating the dual form and then complementing\neach variable.\n\n \n Question: What is the principle of duality? \n Answer: \n\nA. The principle of duality states that any theorem or identity has the same truth value in dual form. This means that any theorem or identity has the same truth value in its dual form."}, {"text": "Title: Choosing the Best Function \n Text: {Choosing the Best Function}\n\nWhen we specify how something works using a human language, we\nleave out details.  Sometimes we do so deliberately, assuming that a\nreader or listener can provide the details themselves: ``Take me to the \nairport!''\nrather than ``Please bend your right arm at the elbow and shift your\nright upper arm forward so as to place your hand near the ignition key.\nNext, ...''  \n\n\n{\n{|lll|}\n1+A=1& 0=0&\n1=A& 0+A=A&\nA+A=A& A=A&\nA=0& A+=1&\n{A+B}= &\n=+& DeMorgan's laws\n(A+B)C=AC+BC&\nA B+C=(A+C)(B+C)&distribution\n(A+B)(+C)(B+C)=(A+B)(+C)&\nA B+ C+B C=A B+ C& consensus\n\n}\n{Boolean logic properties.  The two columns are dual forms of\none another.}\n\n\n\nYou know the basic technique for implementing a Boolean function\nusing { combinational logic}: use a {K-map} to identify a\nreasonable SOP or POS form, draw the resulting design, and perhaps\nconvert to NAND/NOR gates.\n\n\n\nWhen we develop combinational logic designs, we may also choose to\nleave some aspects unspecified.  In particular, the value of a\nBoolean logic function to be implemented may not matter for some\ninput combinations.  If we express the function as a truth table,\nwe may choose to mark the function's value for some input combinations \nas ``{ don't care},'' which is written as ``x'' (no quotes).\n\nWhat is the benefit of using ``don't care'' values?  \n\nUsing ``don't care'' values allows you to choose from among several\npossible logic functions, all of which produce the desired results\n(as well as some combination of 0s and 1s in place of the ``don't\ncare'' values).  \n\nEach input combination marked as ``don't care'' doubles the number\nof functions that can be chosen to implement the design, often enabling \nthe logic needed for implementation to be simpler.\n\n\nFor example, the {K-map} to the right specifies a function F(A,B,C)\nwith two ``don't care'' entries.  \n\nIf you are asked to design combinational logic for this function,\nyou can\nchoose any values for the two ``don't care'' entries.  When identifying\nprime implicants, each ``x'' can either be a 0 or a 1.\n\n\n\n\n\n\nDepending on the choices made for the x's, we obtain one of \nthe following four functions:\n\n{eqnarray*}\nF&=& B+B C\nF&=& B+B C+A  \nF&=&B\nF&=&B+A \n{eqnarray*}\n\n\n\n\n\nGiven this set of choices, a designer typically chooses the third: F=B,\nwhich corresponds to the {K-map} shown to the right of the \nequations.  The design then \nproduces F=1 when A=1, B=1, and C=0 (ABC=110), \nand produces F=0 when A=1, B=0, and C=0 (ABC=100).\nThese differences are marked with shading and green italics in the new\n{K-map}.  No implementation ever produces an ``x.''\n\n \n Question: What is the benefit of using 'don't care' values? \n Answer: \n\nA. Using \"don't care\" values allows you to choose from among several possible logic functions, all of which produce the desired results. This can often enable the logic needed for implementation to be simpler."}, {"text": "Title: Caring about Don't Cares \n Text: {Caring about Don't Cares}\n\nWhat can go wrong?\n\nIn the context of a digital system, unspecified details\nmay or may not be important.  However, { any implementation of \na specification \nimplies decisions} about these details, so decisions should only be left \nunspecified if any of the possible answers is indeed acceptable.\n\nAs a concrete example, let's design logic to control an ice cream \ndispenser.  The dispenser has two flavors, lychee and mango, but\nalso allows us to create a blend of the two flavors.\nFor each of the two flavors, our logic must output two bits\nto control the amount of ice cream that comes out of the dispenser.\nThe two-bit C_L[1:0] output of our logic must specify the number \nof half-servings of lychee ice cream as a binary number, and\nthe two-bit C_M[1:0] output must specify the number of \nhalf-servings of mango ice cream.  Thus, for either flavor,\n00 indicates none of that flavor,\n01 indicates one-half of a serving, and \n10 indicates a full serving.\n\nInputs to our logic will consist of three buttons: an L button to\nrequest a serving of lychee ice cream, a B button to request a\nblend---half a serving of each flavor, and an M button to request\na serving of mango ice cream.  Each button produces a 1 \nwhen pressed and a 0 when not pressed.\n\n\n\nLet's start with the assumption that the user only presses one button\nat a time.  In this case, we can treat input combinations in which\nmore than one button is pressed as ``don't care'' values in the truth\ntables for the outputs.  K-maps for all four output bits appear below.\nThe x's indicate ``don't care'' values.\n\n\n\nWhen we calculate the logic function for an output, each ``don't care''\nvalue can be treated as either 0 or 1, whichever is more convenient\nin terms of creating the logic.  In the case of C_M[1], for \nexample, we can treat the three x's in the ellipse as 1s, treat the x\noutside of the ellipse as a 0, and simply\nuse M (the implicant represented by the ellipse) for C_M[1].  The\nother three output bits are left as an exercise, although the result \nappears momentarily.\n\n\nThe implementation at right takes full advantage of the ``don't care''\nparts of our specification.  In this case, we require no logic at all;\nwe need merely connect the inputs to the correct outputs.  Let's verify\nthe operation.  We have four cases to consider.  First, if none of the \nbuttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00 \nand C_L=00).  Second, if we request lychee ice cream (LBM=100), the\noutputs are C_L=10 and C_M=00, so we get a full serving of lychee\nand no mango.  Third, if we request a blend (LBM=010), the outputs\nare C_L=01 and C_M=01, giving us half a serving of each flavor.\nFinally, if we request mango ice cream (LBM=001), we get no lychee\nbut a full serving of mango.\n\n\n\n\n\nThe K-maps for this implementation appear below.  Each of\nthe ``don't care'' x's from the original design has been replaced with\neither a 0 or a 1 and highlighted with shading and green italics.\nAny implementation produces \neither 0 or 1 for every output bit for every possible input combination.\n\n{{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}\n\nAs you can see, leveraging ``don't care'' output bits can sometimes\nsignificantly simplify our logic.  In the case of this example, we were\nable to completely eliminate any need for gates!  Unfortunately, \nthe resulting implementation may sometimes produce unexpected results. \n\nBased on the implementation, what happens if a user presses more\nthan one button?  The ice cream cup overflows!\n\nLet's see why.\n\nConsider the case LBM=101, in which we've pressed both the lychee and\nmango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a \nfull serving of each flavor, or two servings total.  Pressing other \ncombinations may have other repercussions as well.\nConsider pressing lychee and blend (LBM=110).  The outputs are then\nC_L=11 and C_M=01.  Hopefully the dispenser simply gives us one\nand a half servings of lychee and a half serving of mango.  However,\nif the person who designed the dispenser assumed that no one would ever\nask for more than one serving, something worse might happen.  In other\nwords, giving an input of C_L=11 to the ice cream dispenser may lead to\nother unexpected behavior if its designer decided that that input \npattern was a ``don't care.''\n\nThe root of the problem is that { while we don't care about the value of\nany particular output marked ``x'' for any particular input combination,\nwe do actually care about the relationship between the outputs}.  \n\nWhat can we do?  When in doubt, it is safest to make \nchoices and to add the new decisions to the specification rather than \nleaving output values specified as ``don't care.''\n\nFor our ice cream dispenser logic, rather than leaving the outputs \nunspecified whenever a user presses more than one button, we could \nchoose an acceptable outcome for each input combination and \nreplace the x's with 0s and 1s.  We might, for example, decide to\nproduce lychee ice cream whenever the lychee button is pressed, regardless\nof other buttons (LBM=1xx, which means that we don't care about the\ninputs B and M, so LBM=100, LBM=101, LBM=110, or LBM=111).  \nThat decision alone covers three of the\nfour unspecified input patterns.  We might also decide that when the \nblend and mango buttons are pushed together (but without the lychee\nbutton, LBM=011), our logic produces a blend.  \n\nThe resulting K-maps are shown below, again with shading and green italics \nidentifying\nthe combinations in which our original design specified ``don't care.''\n\n{{file=part2/figs/CLhigh-priority.eps,width=1.00in}{file=part2/figs/CLlow-priority.eps,width=1.00in}{file=part2/figs/CMhigh-priority.eps,width=1.00in}{file=part2/figs/CMlow-priority.eps,width=1.00in}}\n\n\nThe logic in the dashed box to the right implements the set of choices\njust discussed, and matches the K-maps above.\n\nBased on our additional choices, this implementation enforces\na strict priority scheme on the user's button presses.\n\nIf a user requests lychee, they can also press either or\nboth of the other buttons with no effect.  The lychee button has\npriority.  Similarly, if the user does not press lychee, but press-\n\n\n\n\n\nes the blend button, pressing the mango button at the same time has no\neffect.  Choosing mango requires that no other buttons be pressed.\nWe have thus chosen a prioritization order for the buttons and imposed \nthis order on the design.\n\nWe can view this same implementation in another way.\n\nNote the one-to-one correspondence between inputs (on the left) and \noutputs (on the right) for the dashed box.\n\nThis logic takes the user's button presses and chooses at most one of\nthe buttons to pass along to our original controller implementation \n(to the right of the dashed box).\n\nIn other words, rather than thinking of the logic in the dashed\nbox as implementing a specific set of decisions, we can think of the\nlogic as cleaning up the inputs to ensure that only valid\ncombinations are passed to our original implementation.\n\nOnce the inputs are cleaned up, the original implementation is \nacceptable, because input combinations containing more than a \nsingle 1 are in fact impossible.\n\nStrict prioritization is one useful way to clean up our inputs.\n\nIn general, we can design logic to map each\nof the four undesirable input patterns into one of the permissible \ncombinations (the\nfour that we specified explicitly in our original design, with LBM \nin the set ).  Selecting a prioritization scheme\nis just one approach for making these choices in a way\nthat is easy for a user to understand and \nis fairly easy to implement.\n\n\nA second simple approach is to ignore illegal\ncombinations by mapping them into the ``no buttons pressed'' \ninput pattern.\n\nSuch an implementation appears to the right, laid out to show that\none can again view the logic in the dashed box either as cleaning up \nthe inputs (by mentally grouping the logic with the inputs) or as a specific \nset of choices for our ``don't care'' output values (by grouping the \nlogic with the outputs).\n\nIn either case, the logic shown \nenforces our as-\n\n\n\n\n\nsumptions in a fairly conservative way:\nif a user presses more than one button, the logic squashes all button\npresses.  Only a single 1 value at a time can pass through to \nthe wires on the right of the figure.\n\n\n\nFor completeness, the K-maps corresponding to this implementation are given\nhere.\n\n{{file=part2/figs/CLhigh-conserve.eps,width=1.00in}{file=part2/figs/CLlow-conserve.eps,width=1.00in}{file=part2/figs/CMhigh-conserve.eps,width=1.00in}{file=part2/figs/CMlow-conserve.eps,width=1.00in}}\n\n\n \n Question: What is the difference between the two output bits? \n Answer: \n\nA. A \"don't care\" output bit can be either 0 or 1, while an output bit that is specified as 0 or 1 must be one or the other."}, {"text": "Title: Generalizations and Applications* \n Text: {Generalizations and Applications*}\n\nThe approaches that we illustrated to clean up the input signals to\nour design have application in many areas.  The ideas in this \nsection are drawn from the field and are sometimes the subjects of \nlater classes, but { are not exam material for our class.}\n\nPrioritization of distinct\ninputs is used to arbitrate between devices attached to a\nprocessor.  Processors typically execute much more quickly than do devices.\nWhen a device needs attention, the device signals the processor\nby changing the voltage on an interrupt line (the name comes from the\nidea that the device interrupts the processor's current activity, such\nas running a user program).  However, more than\none device may need the attention of the processor simultaneously, so\na priority encoder is used to impose a strict order on the devices and\nto tell the processor about their needs one at a time.\n\nIf you want to learn more about this application, take ECE391.\n\nWhen components are designed together, assuming that some input patterns\ndo not occur is common practice, since such assumptions can dramatically\nreduce the number of gates required, improve performance, reduce power\nconsumption, and so forth.  As a side effect, when we want to test a\nchip to make sure that no defects or other problems prevent the chip\nfrom operating correctly, we have to be careful so as not to ``test''\nbit patterns that should never occur in practice.  Making up random bit\npatterns is easy, but can produce bad results or even destroy the chip\nif some parts of the design have assumed that a combination produced \nrandomly can never occur.  To avoid these problems, designers add extra\nlogic that changes the disallowed patterns into allowed patterns, just\nas we did with our design.  The use of random bit patterns is common in\nBuilt-In Self Test (BIST), and so the process of inserting extra logic\nto avoid problems is called BIST hardening.  BIST hardening can add\n{10-20} additional logic to a design.\n\nOur graduate class on digital system testing, ECE543, covers this\nmaterial, but has not been offered recently.\n\n\n\n\n\n \n Question: Who is the target audience for this text? \n Answer: \n\nA. The audience for this text is primarily students in an introductory digital design class."}, {"text": "Title: Optimizing Logic Expressions \n Text: {Optimizing Logic Expressions}\n\nThe second part of the course covers digital design more\ndeeply than does the textbook.  The lecture notes will explain the\nadditional material, and we will provide further examples in lectures\nand in discussion sections.  Please let us know if you need further\nmaterial for study.\n\nIn the last notes, we introduced Boolean logic operations and showed\nthat with AND, OR, and NOT, we can express any Boolean function on any\nnumber of variables.\n\nBefore you begin these notes, please read the first two sections \nin Chapter 3 of the textbook,\nwhich discuss the operation of { complementary metal-oxide semiconductor}\n({ CMOS}) transistors, illustrate how gates implementing the\nAND, OR, and NOT operations can be built using transistors,\nand introduce DeMorgan's laws. \n\nThis set of notes exposes you to a mix of techniques,\nterminology, tools, and philosophy.  Some of the material is not\ncritical to our class (and will not be tested), but is useful for\nyour broader education, and may help you in later classes.  The value\nof this material has changed substantially in the last couple of decades,\nand particularly in the last few years, as algorithms for tools\nthat help with hardware design have undergone rapid advances.  We\ntalk about these issues as we introduce the ideas.\n\nThe notes begin with a discussion of the ``best'' way to\nexpress a Boolean function and some techniques used historically\nto evaluate such decisions.\n\nWe next introduce the terminology necessary to understand manipulation\nof expressions, and use these terms to explain the Karnaugh map, or\n{K-map}, a tool that we will use for many purposes this semester.\n\nWe illustrate the use of {K-maps} with a couple of\nexamples, then touch on a few important questions and useful\nways of thinking about Boolean logic.\n\nWe conclude with a discussion of the general problem of \nmulti-metric optimization, introducing some ideas and approaches\nof general use to engineers.\n\n\n \n Question: What is the best way to express a Boolean function? \n Answer: \n\nA. The best way to express a Boolean function is to use a truth table."}, {"text": "Title: Defining Optimality \n Text: {Defining Optimality}\n\nIn the notes on logic operations, you learned how to express an\narbitrary function on bits as an OR of minterms (ANDs with one\ninput per variable on which the function operates). \nAlthough this approach demonstrates logical completeness, the results\noften seem inefficient, as you can see by comparing the following \nexpressions for the carry out C from the\naddition of two {2-bit} unsigned numbers, A=A_1A_0 and B=B_1B_0.\n\n\nC &=& A_1B_1 + (A_1+B_1)A_0B_0  \n&=& A_1B_1 + A_1A_0B_0 + A_0B_1B_0  \n&=& \n {A_1} A_0 B_1 B_0 +\nA_1 {A_0} B_1 {B_0} +\nA_1 {A_0} B_1 B_0 +\n&&A_1 A_0 {B_1} B_0 +\nA_1 A_0 B_1 {B_0} +\nA_1 A_0 B_1 B_0 \n\n\nThese three expressions are identical in the sense that they have\nthe same truth tables---they are the same mathematical function.\n\nEquation () is the form that we gave when we introduced\nthe idea of using logic to calculate overflow.  In this form, we were\nable to explain the terms intuitively.\n\nEquation () results from distributing the parenthesized OR\nin Equation ().\n\nEquation () is the result of our logical completeness\nconstruction.  \n\nSince the functions are identical, does the form actually matter at all?\nCertainly either of the first two forms is easier for us to write \nthan is the third.  If we think\nof the form of an expression as a mapping from the function that we\nare trying to calculate into the AND, OR, and NOT functions that we\nuse as logical building blocks, we might also say that the first two\nversions use fewer building blocks.  That observation does have some\ntruth, but let's try to be more precise by framing a question.\n\nFor any given function, there are an infinite number of ways that we can\nexpress the function (for example, given one variable A on which the\nfunction depends, you can OR together any number of copies \nof A without changing the function).\n\n{ What exactly makes one expression better than another?}\n\n\n\nIn 1952, Edward Veitch wrote an article on simplifying truth functions.\nIn the introduction, he said, ``This general problem can be very complicated\nand difficult.  Not only does the complexity increase greatly with the\nnumber of inputs and outputs, but the criteria of the best circuit will\nvary with the equipment involved.''\n\nSixty years later, the answer is largely the same: the criteria depend\nstrongly on the underlying technology (the gates and the devices used\nto construct the gates), and no single { metric}, or way of measuring,\nis sufficient to capture the important differences between expressions \nin all cases.\n\nThree high-level metrics commonly used to evaluate chip\ndesigns are cost, power, and performance.  Cost usually represents the\nmanufacturing cost, which is closely related to the physical silicon\narea required for the design: the larger the chip, the more expensive\nthe chip is to produce.  Power measures energy consumption over\ntime.  A chip that consumes more power means that a user's energy bill\nis higher and, in a portable device, either that the device is heavier or has\na shorter battery life.  Performance measures the speed at which the\ndesign operates.  A faster design can offer more functionality, such as\nsupporting the latest games, or can just finish the same work in less\ntime than a slower design.  These metrics are sometimes related: if\na chip finishes its work, the chip can turn itself off, saving energy.\n\nHow do such high-level metrics relate to the problem at hand?  Only\nindirectly in practice.  There are too many factors involved to make\ndirect calculations of cost, power, or performance at the level of\nlogic expressions.  \n\nFinding an { optimal} solution---the best formulation of a specific\nlogic function for a given metric---is often impossible using the \ncomputational resources and algorithms available to us.\n\nInstead, tools typically use heuristic approaches\nto find solutions that strike a balance between these metrics.\nA { heuristic} approach is one that is\nbelieved to yield fairly good solutions to a problem, but does\nnot necessarily find an optimal solution.\n\nA human engineer can typically impose { constraints}, such as\nlimits on the chip area or limits on the minimum performance,\nin order to guide the process.\n\nHuman engineers may also restructure the implementation of a \nlarger design, such as a design to perform floating-point arithmetic,\nso as to change the logic functions used in the design.\n\n{ Today, manipulation of logic expressions for the purposes of \noptimization is performed almost entirely by computers.}  Humans must\nsupply the logic functions of interest, and must program the acceptable \ntransformations between equivalent forms, but computers do the grunt\nwork of comparing alternative formulations and deciding which one is\nbest to use in context.\n\nAlthough we believe that hand optimization of Boolean expressions is no\nlonger an important skill for our graduates, we do think that you\nshould be exposed to the ideas and metrics historically used for\nsuch optimization.  The rationale for retaining this exposure is \nthreefold. \n\nFirst, we believe that you still need to be able to perform basic\nlogic reformulations (slowly is acceptable)\nand logical equivalence checking (answering the question, ``Do two \nexpressions represent the same function?'').\n\nSecond, the complexity of the problem is a good way to introduce you\nto real engineering.\n\nFinally, the contextual information will help you to develop a better\nunderstanding of finite state machines and higher-level abstractions\nthat form the core of digital systems and are still defined directly\nby humans today.\n\nTowards that end, we conclude this introduction by discussing two\nmetrics that engineers traditionally used to optimize \nlogic expressions.  These metrics are now embedded in { computer-aided \ndesign} ({ CAD}) tools and tuned to specific underlying technologies,\nbut the reasons for their use are still interesting.\n\nThe first metric of interest is a heuristic for the area needed for\na design.\n\nThe measurement is simple: count the number of variable occurrences in\nan expression.  Simply go through and add up how many variables you see.\nUsing our example function C, \nEquation () gives a count of 6,\nEquation () gives a count of 8, and\nEquation () gives a count of 24.\nSmaller numbers represent better expressions, so \nEquation () is the best choice by this metric.\n\nWhy is this metric interesting?\nRecall how gates are built from transistors.\nAn {N-input} gate requires roughly 2N transistors, so if you \ncount up the number of variables in the\nexpression, you get an estimate of the number of transistors needed,\nwhich is in turn an estimate for the area required for the design.\n\nA variation on variable counting is to add the number of operations,\nsince each gate also takes space for wiring (within as well as between\ngates).  Note that we ignore the number of inputs to the operations,\nso a {2-input} AND counts as 1, but a {10-input} AND also counts\nas 1.  We do not usually count complementing variables as an operation\nfor this metric because the complements of variables are sometimes \navailable at no extra cost in gates or wires.\n\nIf we add the number of operations in our example, we get\na count of 10 for Equation ()---two ANDs, two ORs,\nand 6 variables,\n\na count of 12 for Equation ()---three ANDS, one OR,\nand 8 variables,\n\nand a count of 31 for Equation ()---six ANDs, one OR,\nand 24 variables.\n\nThe relative differences between these equations \nare reduced when one counts operations.\n\nA second metric of interest is a heuristic for the performance of a design.\n\nPerformance is inversely related to the delay necessary for a design\nto produce an output once its inputs are available.  For example, if\nyou know how many seconds it takes to produce a result, you can easily\ncalculate the number of results that can be produced per second, which \nmeasures performance.\n\nThe measurement needed is the longest chain of operations\nperformed on any instance of a variable.  The complement of\na variable is included if the variable's complement is not \navailable without using an inverter.\n\nThe rationale for this metric is that gate outputs do not change \ninstantaneously when their inputs\nchange. Once an input to a gate has reached an appropriate voltage to represent \na 0 or a 1, the transistors in the gate switch (on or off) and electrons \nstart to move.\nOnly when the output of the gate reaches the appropriate new voltage \ncan the gates driven by the output start to change. \nIf we count each\nfunction/gate as one delay (we call this time a { gate delay}), we \nget an estimate of the time needed to compute\nthe function.\n\nReferring again to our example equations, we find that\nEquation () requires 3 gate delays,\nEquation () requires 2 gate delays,\nEquation () requires 2 or 3 gate delays, depending\non whether we have variable complements available.  Now \nEquation () looks more attractive: better performance than\nEquation () in return for a small extra cost in area.\n\nHeuristics for estimating energy use are too \ncomplex to introduce at this point, \nbut you should be aware that every time electrons move, they\ngenerate heat, so we might favor an expression that minimizes the number\nof bit transitions inside the computation.  Such a measurement is\nnot easy to calculate by hand,\nsince you need to know the likelihood of input combinations.\n\n\n \n Question: What is the heuristic approach to problem solving? \n Answer: \n\nA. A heuristic is an approach to problem-solving that is designed to find a good, but not necessarily optimal, solution. In the context of logic expression optimization, a heuristic approach is one that is believed to yield fairly good solutions, but does not necessarily find an optimal solution."}, {"text": "Title: Terminology \n Text: \n\nWe use many technical terms when we talk about simplification of\nlogic expressions, so we now introduce those terms so as to make\nthe description of the tools and processes easier to understand.\n\nLet's assume that we have a logic function F(A,B,C,D) that we want\nto express concisely.  A { literal} in an expression of F refers \nto either one of the variables or its complement.  In other words,\nfor our function F, the following is a complete set of literals:\nA, , B, ,\nC, , D, and .\n\nWhen we introduced the AND and OR functions, we also introduced \nnotation borrowed from arithmetic, using multiplication to represent AND\nand addition to represent OR.  We also borrow the related terminology, \nso a { sum} \nin Boolean algebra refers to a number of terms OR'd together \n(for example, A+B, or AB+CD), and\na { product} in Boolean algebra refers to a number of terms AND'd\ntogether (for example, A, or AB(C+D).  Note that\nthe terms in a sum or product may themselves be sums, products, or\nother types of expressions (for example, A).\n\nThe construction method that we used to demonstrate logical completeness\nmade use of minterms for each input combination for which the \nfunction F produces a 1.\n\nWe can now use the idea of a literal to give a simpler definition of minterm:\na { minterm} for a function on N variables is a product (AND \nfunction) of N literals in which each variable or its complement \nappears exactly once.  For our function F, examples of minterms\ninclude ABC,\nACD, and\nBC.\nAs you know, a minterm produces a 1 for exactly\none combination of inputs.\n\nWhen we sum minterms for each output value of 1\nin a truth table to express a function, as we did to obtain\nEquation (), we\nproduce an example of the sum-of-products form.\nIn particular, a { sum-of-products} ({ SOP}) is a sum composed of \nproducts of literals.\nTerms in a sum-of-products need not be minterms, however.\nEquation () is also in sum-of-products form.\nEquation (), however, is not, since the last\nterm in the sum is not a product of literals.\n\nAnalogously to the idea of a minterm, \nwe define a { maxterm} for a function on N variables as a sum (OR \nfunction) of N literals in which each variable or its complement \nappears exactly once.  Examples for F include (A+B++D),\n(A+++D), and\n(++C+).\nA maxterm produces a 0 for exactly\none combination of inputs.  Just as we did with minterms, we can\nmultiply a maxterm corresponding to each input combination for which\na function produces 0 (each row in a truth table that produces a 0\noutput) to create\nan expression for the function.  The resulting expression is in\na { product-of-sums} ({ POS}) form: a product of sums of literals.  \n\nThe carry out function that we used to produce \nEquation () has 10 input combinations that produce 0,\nso the expression formed in this way is unpleasantly long:\n\n{eqnarray*}\nC&=&\n({A_1}+{A_0}+B_1+B_0)\n({A_1}+A_0+B_1+{B_0})\n({A_1}+A_0+B_1+B_0)\n(A_1+{A_0}+{B_1}+B_0)\n&&(A_1+{A_0}+B_1+{B_0})\n(A_1+{A_0}+B_1+B_0)\n(A_1+A_0+{B_1}+{B_0})\n(A_1+A_0+{B_1}+B_0)\n&&(A_1+A_0+B_1+{B_0})\n(A_1+A_0+B_1+B_0)\n{eqnarray*}\n\nHowever, the approach can be helpful with functions that produce mostly 1s.\nThe literals in maxterms are complemented with respect to the\nliterals used in minterms.  For example, the maxterm\n({A_1}+{A_0}+B_1+B_0) in the equation above\nproduces a zero for input combination A_1=1, A_0=1, B_1=0, B_0=0.\n\nAn { implicant} G of a function F is defined to be a second\nfunction operating on the same variables for which\nthe implication G is true.  In terms of logic functions\nthat produce 0s and 1s, { if G is an implicant of F, \nthe input combinations for which G produces 1s are a subset of \nthe input combinations for which F produces 1s}.\n\nAny minterm for which F produces a 1, for example, is an implicant of F.\n\nIn the context of logic design, { the term implicant is used\nto refer to a single product of literals.}  In other words, if we\nhave a function F(A,B,C,D), examples of possible implicants of F\ninclude AB, B, ABC, and .\nIn contrast, although they may technically imply F, we typically\ndo not call expressions such as (A+B), C(+D), nor\nA+C implicants.\n\nLet's say that we have expressed function F in sum-of-products \nform.\nAll of the individual product terms in the expression are implicants of F.  \n\nAs a first step in simplification, we can ask: for each implicant, is it\npossible to remove any of the literals that make up the product?\n\nIf we have an implicant G for which the answer is no, we \ncall G a { prime implicant} of F.\n\nIn other words, if one removes any of the literals from a prime \nimplicant G of F,\nthe resulting product is not an implicant of F.\n\nPrime implicants are the main idea that we use to simplify logic expressions,\nboth algebraically and with graphical tools (computer tools use algebra\ninternally---by graphical here we mean drawings on paper).\n\n\n \n Question: What is the definition of a minterm for a function on N variables? \n Answer: \n\nA. A minterm is a product (AND function) of N literals in which each variable or its complement appears exactly once."}, {"text": "Title: Veitch Charts and Karnaugh Maps \n Text: {Veitch Charts and Karnaugh Maps}\n\nVeitch's 1952 paper was the first to introduce the idea of\nusing a graphical representation to simplify logic expressions.\nEarlier approaches were algebraic.\nA year later, Maurice Karnaugh published\na paper showing a similar idea with a twist.  The twist makes the use of \n{ Karnaugh maps} to simplify expressions\nmuch easier than the use of Veitch charts.  As a result,\nfew engineers have heard of Veitch, but everyone who has ever\ntaken a class on digital logic knows how to make use of a { K-map}. \n\n\n\nBefore we introduce the Karnaugh map, let's think about the structure\nof the domain of a logic function.  Recall that a function's { domain}\nis the space on which the function is defined, that is, for which the\nfunction produces values.  For a Boolean logic function on N variables,\nyou can think of the domain as sequences of N bits, but you can also\nvisualize the domain as an {N-dimensional} hypercube.  \nAn\n\n\n\n\n\n{ {N-dimensional} hypercube} is the generalization\nof a cube to N dimensions.  Some people only use the term hypercube when\nN, since we have other names for the smaller values: a point\nfor N=0, a line segment for N=1, a square for N=2, and a cube\nfor N=3.  The diagrams above and to the right illustrate the cases that \nare easily drawn on paper.  The black dots represent specific input\ncombinations, and the blue edges connect input combinations that differ\nin exactly one input value (one bit).\n\n\n\nBy viewing a function's domain in this way, we can make a connection\nbetween a product of literals and the structure of the domain.  Let's\nuse the {3-dimensional} version as an example.  We call the \nvariables A, B, and C, and note that the cube has 2^3=8 corners\ncorresponding to the 2^3 possible combinations\nof A, B, and C.\nThe simplest product of literals in this\ncase is 1, which is the product of 0 literals.  Obviously, the product 1 \nevaluates to 1 for any variable values.  We can thus think of it as\ncovering the entire domain of the function.  In the case of our example,\nthe product 1 covers the whole cube.  In order for the product 1 to \nbe an implicant of a function, the function itself must be the function 1.\n\nWhat about a product consisting of a single literal, such as A \nor ?  The dividing lines in the diagram illustrate the\nanswer: any such product term evaluates to 1 on a face of the cube,\nwhich includes 2^2=4 of the corners.  If a function evaluates to 1\non any of the six faces of the cube, the corresponding product term\n(consisting of a single literal) is an implicant of the function.\n\nContinuing with products of two literals, we see that any product of \ntwo literals, such as A or C, corresponds\nto an edge of our {3-dimensional} cube.  The edge includes 2^1=2\ncorners.  And, if a function evaluates to 1 on any of the 12 edges\nof the cube, the corresponding product term (consisting of two literals)\nis an implicant of the function.\n\nFinally, any product of three literals, such as B,\ncorresponds to a corner of the cube.  But for a function on three variables,\nthese are just the minterms.  As you know, if a function evaluates to 1\non any of the 8 corners of the cube, that minterm is an implicant of the\nfunction (we used this idea to construct the function to prove\nlogical completeness).\n\nHow do these connections help us to simplify functions?  If we're\ncareful, we can map cubes onto paper in such a way that product terms\n(the possible implicants of the function) usually form contiguous \ngroups of 1s, allowing us to spot them easily.  Let's work upwards\nstarting from one variable to see how this idea works.  The end result\nis called a Karnaugh map.\n\n\nThe first drawing shown to the right replicates our view of \nthe {1-dimensional} hypercube, corresponding to the domain of a\nfunction on one variable, in this case the variable A.  \nTo the right of the hypercube (line segment) are two variants\nof a Karnaugh map on one variable.  The middle variant clearly \nindicates the column corresponding to the product A (the other \ncolumn corresponds to ).  The right variant simply labels\nthe column with values for A.\n\n\n\n\n\n\nThe three drawings shown to the right illustrate the three possible product\nterms on one variable.  { The functions shown in \nthese Karnaugh maps are arbitrary, except that we have chosen\nthem such that each implicant shown is a prime implicant for the\nillustrated function.}\n\n\n\n\n\n\n\nLet's now look at two-variable functions.  We have replicated\nour drawing of the {2-dimensional} hypercube (square) to the right \nalong with two variants of Karnaugh maps on two variables.\nWith only two variables (A and B), the extension is fairly \nstraightforward, since we can use the second dimension of the \npaper (vertical) to express the second variable (B).\n\n\n\n\n\n\nThe number of possible products of literals grows rapidly with the \nnumber of variables.\nFor two variables, nine are possible, as shown to the right.\nNotice that all implicants have two properties.  First, they\noccupy contiguous regions of the grid.  And, second, their height\nand width are always powers of two.  These properties seem somewhat\ntrivial at this stage, but they are the key to the utility of\n{K-maps} on more variables.\n\n\n\n\n\n\nThree-variable functions are next.  The cube diagram is again replicated\nto the right.  But now we have a problem: how can we map four points\n(say, from the top half of the cube) into a line in such a way that \nany points connected by a blue line are adjacent in the {K-map}?\nThe answer is that we cannot, but we can preserve most of the connections\nby choosing an order such as the one illustrated by the arrow.\nThe result\n\n\n\n\n\nis called a Gray code.  Two {K-map} variants again appear\nto the right of the cube.  Look closely at the order of the two-variable\ncombinations along the top, which allows us to have as many contiguous\nproducts of literals as possible.  Any product of literals that contains\n but not A nor  wraps around the edges\nof the {K-map}, so you should think of it as rolling up into a cylinder \nrather than a grid.  Or you can think that we're unfolding the cube to\nfit the corners onto a sheet of paper, but the place that we split\nthe cube should still be considered to be adjacent when looking for \nimplicants.  The use of a Gray code is the one difference between a\n{K-map} and a Veitch chart; Veitch used the base 2 order, which\nmakes some implicants hard to spot.\n\nWith three variables, we have 27 possible products of literals.  You\nmay have noticed that the count scales as 3^N for N variables;\ncan you explain why?  We illustrate several product terms\nbelow.  Note that we sometimes\nneed to wrap around the end of the {K-map}, but that if we account\nfor wrapping, the squares covered by all product terms are contiguous.\nAlso notice that both the width and the height of all product terms \nare powers of two.  { Any square or rectangle that meets these two \nconstraints corresponds to a product term!}  And any such square or \nrectangle that is filled with 1s is an implicant of the function\nin the {K-map}.\n\n\n\nLet's keep going.  With a function on four variables---A, B, C, \nand D---we can use a Gray code order on two of the variables in \neach dimension.  Which variables go with which dimension in the grid\nreally doesn't matter, so we'll assign AB to the horizontal dimension\nand CD to the vertical dimension.  A few of the 81 possible product\nterms are illustrated at the top of the next page.  Notice that while wrapping \ncan now occur\nin both dimensions, we have exactly the same rule for finding implicants\nof the function: any square or rectangle (allowing for wrapping) \nthat is filled with 1s and has both height and width equal to (possibly\ndifferent) powers of two is an implicant of the function.  { \nFurthermore, unless such a square or rectangle is part of a larger\nsquare or rectangle that meets these criteria, the corresponding\nimplicant is a prime implicant of the function.}\n\n\n\n\n\n\n\n\nFinding a simple expression for a function using a {K-map} then\nconsists of solving the following problem: pick a minimal set of prime \nimplicants such that every 1 produced by the function is covered\nby at least one prime implicant.  The metric that you choose to\nminimize the set may vary in practice, but for simplicity, let's say \nthat we minimize the number of prime implicants chosen.\n\n\nLet's try a few!  The table on the left below reproduces (from\nNotes Set 1.4) the truth\ntable for addition of two {2-bit} unsigned numbers, A_1A_0\nand B_1B_0, to produce a sum S_1S_0 and a carry out C.\n{K-maps} for each output bit appear to the right.\nThe colors are used only to make the different prime implicants\neasier to distinguish.\nThe equations produced by summing these prime implicants appear\nbelow the {K-maps}.\n\n\n{cccc|ccc}\n{c|}& \nA_1& A_0& B_1& B_0& C& S_1& S_0 \n0&0& 0&0& 0& 0&0\n0&0& 0&1& 0& 0&1\n0&0& 1&0& 0& 1&0\n0&0& 1&1& 0& 1&1\n0&1& 0&0& 0& 0&1\n0&1& 0&1& 0& 1&0\n0&1& 1&0& 0& 1&1\n0&1& 1&1& 1& 0&0\n1&0& 0&0& 0& 1&0\n1&0& 0&1& 0& 1&1\n1&0& 1&0& 1& 0&0\n1&0& 1&1& 1& 0&1\n1&1& 0&0& 0& 1&1\n1&1& 0&1& 1& 0&0\n1&1& 1&0& 1& 0&1\n1&1& 1&1& 1& 1&0\n\n\n\n\n\n\n{eqnarray*}\nC&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0\n\nS_1&=&\nA_1 {B_1} {B_0}+\nA_1 {A_0} {B_1}+\n{A_1} {A_0} B_1+\n{A_1} B_1 {B_0}+\n&&{A_1} A_0 {B_1} B_0+\nA_1 A_0 B_1 B_0\n\nS_0&=&A_0 {B_0}+{A_0} B_0\n{eqnarray*}\n\n\nIn theory, {K-maps} extend to an arbitrary number of variables.\nCertainly Gray codes can be extended.  An { {N-bit} Gray code} \nis a sequence of {N-bit} patterns that includes all possible\npatterns such that any two adjacent patterns differ in only one bit.\nThe code is actually a cycle: the first and last patterns also differ\nin only one bit.  You can construct a Gray code recursively as follows:\nfor an {(N+1)-bit} Gray code, write the sequence for an\n{N-bit} Gray code, then add a 0 in front of all patterns.\nAfter this sequence, append a second copy of the {N-bit} Gray\ncode in reverse order, then put a 1 in front of all patterns in the\nsecond copy.  The result is an {(N+1)-bit} Gray code.\nFor example, the following are Gray codes:\n\n\n1-bit& 0, 1\n2-bit& 00, 01, 11, 10\n3-bit& 000, 001, 011, 010, 110, 111, 101, 100\n4-bit& \n0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100,\n1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000\n\n\nUnfortunately, some of the beneficial properties of {K-maps} do\nnot extend beyond two variables in a dimension.  { Once you have three\nvariables in one dimension}, as is necessary if a function operates\non five or more variables, { not all product terms are contiguous\nin the grid}.  The terms still require a total number of rows and columns\nequal to a power of two, but they don't all need to be a contiguous\ngroup.  Furthermore, { some contiguous groups of appropriate size do\nnot correspond to product terms}.  So you can still make use of {K-maps}\nif you have more variables, but their use is a little trickier.\n\n \n Question: What is the difference between a Veitch chart and a Karnaugh map? \n Answer: \n\nA. The difference between a Veitch chart and a Karnaugh map is that a Karnaugh map uses a Gray code order, which makes some implicants easier to spot."}, {"text": "Title: Canonical Forms \n Text: {Canonical Forms}\n\nWhat if we want to compare two expressions to determine whether they\nrepresent the same logic function?  Such a comparison is a test of\n{ logical equivalence}, and is an important part of hardware design.\nTools today provide help with this problem, but you should understand\nthe problem.\n\nYou know that any given function can be expressed in many ways, and that\ntwo expressions that look quite different may in fact represent the\nsame function (look back at Equations () to ()\nfor an example).  But what if we rewrite the function using only \nprime implicants?  Is the result unique?  Unfortunately, no.\n\n\nIn general, { a sum of products is not unique (nor is a product of sums),\neven if the sum contains only prime implicants}.\n\nFor example, consensus terms may or may not be included in our\nexpressions.  (They are necessary for reliable design of \ncertain types of systems, as you will learn in a later ECE class.)\n\nThe green ellipse in the K-map to the right represents the consensus \nterm BC.\n{eqnarray*}\nZ &=& A C +  B + B C\nZ &=& A C +  B\n{eqnarray*}\n\n\n\n\n\n\nSome functions allow several equivalent formulations as\nsums of prime implicants, even without consensus terms.  \nThe K-maps shown to the right, for example, illustrate \nhow one function might be written in either of the following ways:\n\n{eqnarray*}\nZ &=&\n  D +\n C  +\nA B C +\nB  D\nZ &=&\n  C +\nB C  +\nA B D +\n  D\n{eqnarray*}\n\n\n\n\n\n\n\n\nWhen we need to compare two things (such as functions), we need to\ntransform them into what in mathematics is known as a { canonical form},\nwhich simply means a form that is defined so as to be unique \nfor each thing of the given type.\nWhat can we use for logic functions?  You already know two answers!  \n\nThe { canonical sum} of a function (sometimes called the\n{ canonical SOP form}) is the sum of minterms.  \n\nThe { canonical product} of a function (sometimes called the\n{ canonical POS form}) is the product of maxterms.  \n\nThese forms technically\nonly meet the mathematical definition of canonical if we agree on an order\nfor the min/maxterms, but that problem is solvable.\n\nHowever, as you already know, the forms are not particularly \nconvenient to use.  \n\nIn practice, people and tools in the industry use more compact \napproaches when comparing functions, but those solutions are a subject for\na later class (such as ECE 462).\n\n\n\n\n \n Question: What is the canonical sum of a function? \n Answer: \n\nThe canonical SOP form of a logic function is the sum of minterms."}, {"text": "Title: Two-Level Logic \n Text: {Two-Level Logic}\n\n\n{ Two-level logic} is a popular way of expressing logic functions.\nThe two levels refer simply to the number of functions through which an\ninput passes to reach an output, and both the SOP and POS forms are \nexamples of two-level logic.  In this section, we illustrate one of the \nreasons for this popularity and \nshow you how to graphically manipulate\nexpressions, which can sometimes help when trying to understand gate\ndiagrams.\n\nWe begin with one of DeMorgan's laws, which we can illustrate both \nalgebraically and graphically:\nC  =  B+A  =   \n\n\n{file=part2/figs/demorgan-nand.eps,width=0.95in}\n\n\n\n\nLet's say that we have a function expressed in SOP form, such as\nZ=ABC+DE+FGHJ.  The diagram on the left below shows the function\nconstructed from three AND gates and an OR gate.  Using DeMorgan's\nlaw, we can replace the OR gate with a NAND with inverted inputs.\nBut the bubbles that correspond to inversion do not need to sit at the\ninput to the gate.  We can invert at any point along the wire,\nso we slide each bubble down the wire to the output of the first column\nof AND gates.  { Be careful: if the wire splits, which does not\nhappen in our example, you have to replicate\nthe inverter onto the other output paths as you slide past the split\npoint!}  The end result is shown on the right: we have not changed the \nfunction, but now we use only NAND gates.  Since CMOS technology\nonly supports NAND and NOR directly, using two-level logic makes\nit simple to map our expression into CMOS gates.\n\n{file=part2/figs/SOP-equiv.eps,width=6.5in}\n\n\nYou may want to make use of DeMorgan's other law, illustrated graphically\nto the right, to perform the same transformation on a POS expression.  What\ndo you get?\n\n\n{file=part2/figs/demorgan-nor.eps,width=0.95in}\n\n\n\n \n Question: What is the name of two-level logic? \n Answer: \n\nA. Two-level logic is a popular way of expressing logic functions.\nThe two levels refer simply to the number of functions through which an\ninput passes to reach an output."}, {"text": "Title: Multi-Metric Optimization \n Text: {Multi-Metric Optimization}\n\nAs engineers, almost every real problem that you encounter will admit \nmultiple metrics for evaluating possible designs.  Becoming a good\nengineer thus requires not only that you be able to solve problems\ncreatively so as to improve the quality of your solutions, but also\nthat you are aware of how people might evaluate those solutions and\nare able both to identify the most important metrics and to balance \nyour design effectively according to them.  In this section, we\nintroduce some general ideas and methods that may be of use to you\nin this regard.\n\n{ We will not test you on the concepts in this section.}\n\nWhen you start thinking about a new problem, your first step\nshould be to think carefully about metrics of possible interest.\n\nSome important metrics may not be easy to quantify.  \n\nFor example, compatibility of a design with other products already \nowned by a customer has frequently defined the success or failure\nof computer hardware and software solutions.\n\nBut how can you compute the compability of your approach as\na number?\n\nHumans---including engineers---are not good at\ncomparing multiple metrics simultaneously.\n\nThus, once you have a set of metrics that you feel is complete, \nyour next step is to get rid of as many as you can.\n\nTowards this end, you may identify metrics that have no practical \nimpact in current technology, set threshold values for other metrics\nto simplify reasoning about them, eliminate redundant metrics,\ncalculate linear sums to reduce the count of metrics, and, finally,\nmake use of the notion of Pareto optimality.  All of these ideas are\ndescribed in the rest of this section.\n\nLet's start by considering metrics that we can quantify as real\nnumbers.\n\nFor a given metric, we can divide possible measurement values into\nthree ranges.\n\nIn the first range,\nall measurement values are equivalently useful.\nIn the second range, \npossible values are ordered and interesting with respect to\none another.\nValues in the third range are all impossible to use in practice.\nUsing power consumption as\nour example, the first range corresponds to systems in which\nwhen a processor's power consumption in a digital \nsystem is extremely low relative to the\npower consumption\nof the system.\nFor example, the processor in a computer might use less than 1 \nof the total used by \nthe system including the disk drive, the monitor, the power \nsupply, and so forth.  One power consumption value in this range \nis just as good as any\nanother, and no one cares about the power consumption of the processor \nin such cases.  In the second range, power consumption of the\nprocessor makes a difference.  Cell phones use most of their energy\nin radio operation, for example, but if you own a phone with a powerful\nprocessor, you may have noticed that you can turn off the phone and \ndrain the battery fairly quickly by playing a game.  Designing a\nprocessor that uses half as much power lengthens the battery life in\nsuch cases.  Finally,\nthe third region of power consumption measurements is impossible:\nif you use so much power, your chip will overheat or even burst\ninto flames.  Consumers get unhappy when such things happen.\n\nAs a first step, you can remove any metrics for which all solutions\nare effectively equivalent.\n\nUntil a little less than a decade ago, for example, the power \nconsumption of a desktop\nprocessor actually was in the first range that we discussed.  Power\nwas simply not a concern to engineers: all designs of \ninterest consumed so little power that no one cared.\n\nUnfortunately, at that point, power consumption jumped into the\nthird range rather quickly.  Processors hit a wall, and \nproducts had to be cancelled.  Given that the time spent designing\na processor has historically\nbeen about five years, a lot of engineering effort\nwas wasted because people had not thought carefully enough about\npower (since it had never mattered in the past).\n\nToday, power is an important metric that engineers must take into\naccount in their designs. \n\nHowever, in some areas, such as desktop and high-end server\nprocessors,\nother metrics (such as performance) may be so \nimportant that we always want to operate at the edge of the\ninteresting range.  In such cases, we might choose to treat \na metric such as power consumption as a { threshold}: stay\nbelow 150 Watts for a desktop processor, for example.  One still\nhas to make a coordinated effort to ensure that the system as\na whole does not exceed the threshold, but reasoning about \nthreshold values, a form of constraint, is easier than trying to\nthink about multiple metrics at once.\n\nSome metrics may only allow discrete quantification.  For example,\none could choose to define compatibility with previous processor\ngenerations as binary: either an existing piece of software\n(or operating system)\nruns out of the box on your new processor, or it does not.  If you \nwant people who own that software to make use of your new processor,\nyou must ensure that the value of this binary metric is 1, which\ncan also be viewed as a threshold.\n\nIn some cases, two metrics may be strongly { correlated}, meaning\nthat a design that is good for one of the metrics is frequently \ngood for the other metric as well.\n\nChip area and cost, for\nexample, are technically distinct ways to measure a digital design,\nbut we rarely consider them separately.\n\nA design that requires a larger chip is probably more complex,\nand thus takes more engineering time to get right (engineering time\ncosts money).  \n\nEach silicon wafer costs money to fabricate, and fewer copies of a \nlarge design fit on one wafer, so large chips mean more fabrication\ncost.\n\nPhysical defects in silicon can cause some chips not to work.  A large\nchip uses more silicon than a small one, and is thus more likely to suffer\nfrom defects (and not work).  Cost thus goes up again for large chips\nrelative to small ones.\n\nFinally, large chips usually require more careful testing to ensure\nthat they work properly (even ignoring the cost of getting the design\nright, we have to test for the presence of defects), which adds still\nmore cost for a larger chip.\n\nAll of these factors tend to correlate chip area and chip cost, to the\npoint that most engineers do not consider both metrics.\n\n\nAfter you have tried to reduce your set of metrics as much as possible,\nor simplified them by turning them into thresholds, you should consider\nturning the last few metrics into a weighted linear sum.  All remaining\nmetrics must be quantifiable in this case.\n\nFor example, if you are left with three metrics for which a given\ndesign has values A, B, and C, you might reduce these to one\nmetric by calculating D=w_AA+w_BB+w_CC.  What are the w values?\nThey are weights for the three metrics.  Their values represent the\nrelative importance of the three metrics to the overall evaluation.\nHere we've assumed that larger values of A, B, and C are either\nall good or all bad.  If you have metrics with different senses,\nuse the reciprocal values.  For example, if a large value of A is good,\na small value of 1/A is also good.  \n\nThe difficulty with linearizing metrics is that not everyone agrees\non the weights.  Is using less power more important than having a cheaper\nchip?  The answer may depend on many factors.\n\nWhen you are left with several metrics of interest, you can use the\nidea of Pareto optimality to identify interesting designs.\n\nLet's say that you have two metrics.  If a design D_1 is better than\na second design D_2 for both metrics, we say that D_1 \n{ dominates} D_2.\n\nA design D is then said to be { Pareto optimal} if no other design\ndominates D.  Consider the figure on the left below, which illustrates\nseven possible designs measured with two metrics.  The design corresponding\nto point B dominates the designs corresponding to points A and C, so \nneither of the latter designs is Pareto optimal.  No other point \nin the figure dominates B, however, so that design is Pareto optimal.\nIf we remove all points that do not represent Pareto optimal designs,\nand instead include only those designs that are Pareto optimal, we\nobtain the version shown on the right.  These are points in a two-dimensional\nspace, not a line, but we can imagine a line going through the points,\nas illustrated in the figure: the points that make up the line are\ncalled a { Pareto curve}, or, if you have more than two metrics,\na { Pareto surface}.\n\n{\n\n\n\n\nAs an example of the use of Pareto optimality, consider the figure\nto the right, which is copied with permission from Neal Crago's Ph.D. \ndissertation (UIUC ECE, 2012).  The figure compares hundreds of thousands \nof possible\ndesigns based on a handful of different core approaches for\nimplementing a processor.  The axes in the graph are two metrics \nof interest.  The horizontal axis measures the average performance of a \ndesign when\nexecuting a set of benchmark applications, normalized to\na baseline processor design.  The vertical axis measures the energy\nconsumed by a design when\n\n\n{file=part2/cited/bench_pareto.eps,width=3in}\n\n\nexecuting the same benchmarks, normalized\nagain to the energy consumed by a baseline design.  The six sets of\npoints in the graph represent alternative design techniques for the\nprocessor, most of which are in commercial use today.  The points\nshown for each set are the subset of many thousands of possible variants\nthat are Pareto optimal.  In this case, more performance and less energy\nconsumption are the good directions, so any point in a set for which\nanother point is both further to the right and further down is not\nshown in the graph.  The black line represents an absolute power\nconsumption of 150 Watts, which is a nominal threshold for a desktop\nenvironment.  Designs above and to the right of that line are not\nas interesting for desktop use.  The { design-space exploration} that\nNeal reported in this figure was of course done by many computers using\nmany hours of computation, but he had to design the process by which the\ncomputers calculated each of the points.\n\n\n\n\n\n \n Question: What is the name of the process in which computers calculate points in a given space? \n Answer: \n\nA. Design-space exploration is a process in which computers calculate points in a given space in order to find possible solutions to a problem."}, {"text": "Title: Sequential Logic \n Text: {Sequential Logic}\n\nThese notes introduce logic components for storing bits, building up\nfrom the idea of a pair of cross-coupled inverters through an\nimplementation of a flip-flop, the storage abstractions used in most \nmodern logic design processes.  We then introduce simple forms of\ntiming diagrams, which we use to illustrate the behavior of\na logic circuit.\n\nAfter commenting on the benefits of using a clocked synchronous \nabstraction when designing systems that store and manipulate bits,\nwe illustrate timing issues and explain how these are abstracted\naway in clocked synchronous designs.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later\nclasses.} \n\n\n\n \n Question: What is the main purpose of introducing logic components? \n Answer: \n\nA. The main purpose of introducing logic components is to store bits and build up from the idea of a pair of cross-coupled inverters."}, {"text": "Title: Storing One Bit \n Text: {Storing One Bit}\n\nSo far we have discussed only implementation of Boolean functions:\ngiven some bits as input, how can we design a logic circuit to calculate\nthe result of applying some function to those bits?  The answer to\nsuch questions is called { combinational logic} (sometimes \n{ combinatorial logic}), a name stemming from the fact that we\nare combining existing bits with logic, not storing new bits.\n\nYou probably already know, however, that combinational logic alone is\nnot sufficient to build a computer.  We need the ability to store bits,\nand to change those bits.  Logic designs that make use of stored \nbits---bits that can be changed, not just wires to high voltage and \nground---are called { sequential logic}.  The name comes from the idea \nthat such a system moves through a sequence of stored bit patterns (the \ncurrent stored bit pattern is called the { state} of the system).\n\n\nConsider the diagram to the right.  What is it?  A {1-input} \nNAND gate, or an inverter drawn badly?  If you think carefully about\nhow these two gates are built, you will realize that they are the\nsame thing.  Conceptually, we use two inverters to store a bit, but\nin most cases we make use of NAND gates to simplify the \nmechanism for changing the stored bit.\n\n\n{file=part2/figs/latch-step-1.eps,width=1.9in}\n\n\n\nTake a look at the design to the right.  Here we have taken two \ninverters (drawn as NAND gates) and coupled each gate's output to\nthe other's input.  What does the circuit do?  Let's make some\nguesses and see where they take us.  Imagine that the value at Q\nis 0.  In that case, the lower gate drives P to 1.  \nBut P drives the upper gate, which forces Q to 0.\nIn other words, this combination forms a\n\n\n{file=part2/figs/latch-step-2.eps,width=1.65in}\n\n\n{|cc}\nQ& P \n0& 1\n1& 0\n\n\n\nstable state of the system:\nonce the gates reach this state, they continue to hold these values.\nThe first row of the truth table to the right (outputs only) shows\nthis state.\n\nWhat if Q=1, though?  In this case, the lower gate forces P to 0,\nand the upper gate in turn forces Q to 1.  Another stable state!\nThe Q=1 state appears as the second row of the truth table.\n\nWe have identified all of the stable states.{Most logic\nfamilies also allow unstable states in which the values alternate rapidly \nbetween 0 and 1.  These metastable states are \nbeyond the scope of our class, but ensuring that they do not occur\nin practice is important for real designs.}  Notice that our cross-coupled\ninverters can store a bit.  Unfortunately, we have no way to \nspecify which value should be stored, nor to change the bit's value\nonce the gates have settled into a stable state.\nWhat can we do?  \n\n\n\n\n\n\nLet's add an input to the upper gate, as shown to the \nright.  We call the input .  The ``S'' stands for set---as\nyou will see, our new input allows us to { set} our stored bit Q to 1.\nThe use of a complemented name for the input indicates that the input\nis { active low}.  In other words, the input performs its intended \ntask (setting Q to 1) when its value is 0 (not 1).\n\n\n{file=part2/figs/latch-step-3.eps,width=2.1in}\n\n\n{c|cc}\n& Q& P \n1& 0& 1\n1& 1& 0\n0& 1& 0\n\n\n\nThink about what happens when the new input is not active, \n=1.  As you know, ANDing any value with 1 produces the \nsame value, \nso our new input has no effect when =1.  The first two\nrows of the truth table are simply a copy of our previous table: the \ncircuit can store either bit value when =1.\nWhat happens when =0?  In that case, the upper gate's\noutput is forced to 1, and thus the lower gate's is forced to 0.\nThis third possibility is reflected in the last row of the truth table.\n\n\nNow we have the ability to force bit Q to have value 1, but if we\nwant Q=0, we just have to hope that the circuit happens to settle into\nthat state when we turn on the power.  What can we do?\n\nAs you probably guessed, we add an input to the other gate, as \nshown to the right.  We call the new input : the input's\npurpose is to { reset} bit Q to 0, and the input\nis active low.  We extend the truth table to include a row \nwith =0 and =1, which forces Q=0 and P=1.\n\n\n{file=part2/figs/latch-step-4.eps,width=2.1in}\n\n\n{cc|cc}\n& & Q& P \n1& 1& 0& 1\n1& 1& 1& 0\n1& 0& 1& 0\n0& 1& 0& 1\n0& 0& 1& 1\n\n\n\n is active low.  We extend the truth table to include a row \n with =0 and =1, which forces Q=0 and P=1.\n\nThe circuit that we have drawn has a name: an \n{ {- latch}}. \nOne can also build {R-S latches} (with active high set and reset\ninputs).  The textbook also shows an\n{- latch} (labeled \nincorrectly).\nCan you figure out how to build an {R-S} latch yourself?\n\nLet's think a little more about\nthe {- latch}. \nWhat happens if we set =0 and =0 at\nthe same time?  Nothing bad happens immediately.  Looking at the design,\nboth gates produce 1, so Q=1 and P=1.  The bad part happens later:\nif we raise both  and  back to 1 at around\nthe same time, the stored bit may end up in either state.{Or,\nworse, in a metastable state, as mentioned earlier.}\n\nWe can avoid the problem by adding gates to prevent the\ntwo control inputs ( and )\nfrom ever being 1 at the same time.  A single inverter\nmight technically suffice, but let's build up the structure shown below,\nnoting that the two inverters in sequence connecting D to \nhave no practical effect at the moment.\n\nA truth table is shown to the right of the logic diagram.\n\nWhen D=0,  is forced to 0, and the bit is reset.\n\nSimilarly, when D=1,  is forced to 0, and the bit is set.\n\n{\n\n{file=part2/figs/latch-step-5.eps,width=3.25in}\n\n\n{c|cccc}\nD& & & Q& P \n0& 0& 1& 0& 1\n1& 1& 0& 1& 0\n\n\n}\n\nUnfortunately, except for some interesting timing characteristics, \nthe new design has the same functionality as a piece of wire.\nAnd, if you ask a circuit designer, thin wires also have some \ninteresting timing characteristics.  What can we do?  Rather than \nhaving Q always reflect the current value of D, let's add\nsome extra inputs to the new NAND gates that allow us to control\nwhen the value of D is copied to Q, as shown on the next page.\n\n{\n\n{file=part2/figs/latch-step-6.eps,width=3.35in}\n\n\n{cc|cccc}\nWE& D& & & Q& P \n1& 0& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0\n0& 0& 1& 1& 0& 1\n0& 1& 1& 1& 0& 1\n0& 0& 1& 1& 1& 0\n0& 1& 1& 1& 1& 0\n\n\n}\n\n\nThe WE (write enable) input controls whether or not Q mirrors\nthe value of D.  The first two rows in the truth table are replicated\nfrom our ``wire'' design: a value of WE=1 has no effect on the first\ntwo NAND gates, and Q=D.  A value of WE=0 forces the\nfirst two NAND gates to output 1, thus =1, =1,\nand the bit Q can\noccupy either of the two possible states, regardless of the value\nof D, as reflected in the lower four lines of the truth table.\n\nThe circuit just shown is called a { gated D latch}, and is an\nimportant mechanism\n\n\n{file=part2/figs/latch-step-7.eps,width=1.1in}\n\n\nfor storing state in sequential \nlogic.  (Random-access memory uses a slightly different \ntechnique to connect the cross-coupled inverters, but latches are\nused for nearly every other application of stored state.)\nThe ``D''\nstands for ``data,'' meaning that the bit stored is matches\nthe value of the input.  Other types of latches (including\n{S-R latches}) have been used historically, but\nD latches are used predominantly today, so we omit discussion of\nother types.\nThe ``gated'' qualifier refers to the presence of an enable input\n(we called it WE) to control\nwhen the latch copies its input into the stored bit.\nA symbol for a gated D latch appears to the right.  Note that we have\ndropped the name P in favor of , since P=\nin a gated D latch. \n\n \n Question: What is the purpose of the input? \n Answer: \n\nA. The input is used to set the bit to 0."}, {"text": "Title: The Clock Abstraction \n Text: {The Clock Abstraction}\n\nHigh-speed logic designs often use latches directly.  Engineers\nspecify the number of latches as well as combinational logic \nfunctions needed to connect one latch to the next,\nand the CAD tools optimize the combinational logic.\nThe enable inputs of successive groups of latches are then driven\nby what we call a clock signal, a single bit line distributed across\nmost of the chip that alternates between 0 and 1 with a regular\nperiod.  While the clock is 0, one set of latches holds its bit values fixed,\nand combinational logic uses those latches as inputs to produce \nbits that are copied into a\nsecond set of latches.  When the clock switches to 1, the second\nset of latches stops storing their data inputs and\nretains their bit values in order to drive other combinational logic,\nthe results of which are copied into a third set of latches.\nOf course, some of the latches in the first and third sets may be the\nsame.\n\nThe timing of signals in such designs plays a critical role in their\ncorrect operation.  Fortunately, we have developed powerful abstractions \nthat allow engineers to ignore much of the complexity while thinking\nabout the Boolean logic needed for a given design.\n\nTowards that end, we make a simplifying assumption for the\nrest of our class, and for most of your career as an undergraduate:\nthe clock signal is a { square wave} delivered uniformly across a\nchip.  For example, if the period of a clock is 0.5 nanoseconds (2 GHz),\nthe clock signal is a 1 for 0.25 nanoseconds, then a 0 for\n0.25 nanoseconds.  We assume that the clock signal changes instantaneously\nand at the same time across the chip.  Such a signal can never exist in\nthe real world: voltages do not change instantaneously, and the \nphrase ``at the same time'' may not even make sense at these scales.\nHowever, circuit designers can usually provide a clock signal that\nis close enough, allowing us to forget for now that no physical signal can\nmeet our abstract definition.\n\n\n SSL altered terminology on 3 Dec 21\n\n\n\n\nThe device shown to the right is a { master-slave} implementation of a \n\nThe device shown to the right is a { dual-latch} implementation of a \n{ positive edge-triggered} D flip-flop.  As you can see, we have \nconstructed it from two gated D latches with opposite senses of write\nenable.  The ``D'' part of the name has the same meaning as with a\ngated D latch: the bit stored is the same as the one delivered\n\n\n{file=part2/figs/latch-step-8.eps,width=2.75in}\n\n\n{file=part2/figs/latch-step-9.eps,width=0.95in}\n\n\nto the\ninput.  Other variants of flip-flops have also been built, but this \ntype dominates designs today.  Most are actually generated automatically\nfrom hardware ``design'' languages (that is, computer programming languages\nfor hardware design).\n\nWhen the clock is low (0), the first latch copies its value from\nthe flip-flop's D input to the midpoint (marked X in our figure, but\nnot usually given a name).  When the clock is high (1), the second\nlatch copies its value from X to the flip-flop's output Q.\nSince X can not change when the clock is high,\nthe result is that the output changes each time the clock changes\nfrom 0 to 1, which is called the { rising edge} or { positive edge}\n(the derivative) of the clock signal.  Hence the qualifier \n``positive edge-triggered,'' which describes the flip-flop's behavior.\n\nThe ``master-slave'' implementation refers to the use of two latches.\n\nThe ``dual-latch'' implementation refers to the use of two \nlatches.{Historically, this implementation was called ``master-slave,''\nbut ECE Illinois has decided to eliminate use of such terminology.}\nlatches.\nIn practice, flip-flops are almost never built this way.  To see a \ncommercial design, look up 74LS74, which uses six {3-input} NAND\ngates and allows set/reset of the flip-flop (using two\nextra inputs).\n\n\n\nThe { timing diagram} to the right illustrates the operation of\nour flip-flop.  In a timing diagram, the horizontal axis represents\n(continuous) increasing time, and the individual lines represent\nvoltages for logic signals.  The relatively simple version shown here\nuses only binary values for each signal.  One can also draw \ntransitions more realistically (as taking finite time).  The dashed\nvertical lines here represent the times at which the clock rises.\nTo make the\n\n\n\n\n\nexample interesting, we have varied D over two clock\ncycles.  Notice that even though D rises and falls during the second\nclock cycle, its value is not copied to the output of our flip-flop.\nOne can build flip-flops that ``catch'' this kind of behavior\n(and change to output 1), but we leave such designs for later in your\ncareer.\n\nCircuits such as latches and flip-flops are called { sequential\nfeedback} circuits, and the process by which they are designed \nis beyond the scope of our course.  The ``feedback'' part of the\nname refers to the fact that the outputs of some gates are fed back \ninto the inputs of others.  Each cycle in a sequential feedback \ncircuit can store one bit.\n\nCircuits that merely use\nlatches and flip-flops as building blocks are called\n{ clocked synchronous sequential circuits}.  Such designs are\nstill sequential: their behavior depends on the bits currently\nstored in the latches and flip-flops.  However, their behavior\nis substantially simplified by the use of a clock signal (the ``clocked''\npart of the name) in a way that all elements change at the same\ntime (``synchronously'').\n\nThe value of using flip-flops and assuming a square-wave\nclock signal with uniform timing may not be clear to you yet,\nbut it bears emphasis.\n\nWith such assumptions, { we can treat time as having \ndiscrete values.}  In other words, time ``ticks'' along discretely,\nlike integers instead of real numbers.\n\nWe can look at the state of the system, calculate the inputs to our\nflip-flops through the combinational logic that drives their D\ninputs, and be confident that, when time moves to the next discrete\nvalue, we will know the new bit values stored in our flip-flops,\nallowing us to repeat the process for the next clock cycle without\nworrying about exactly when things change.  Values change only\non the rising edge of the clock!\n\nReal systems, of course, are not so simple, and we do not have one\nclock to drive the universe, so engineers must also design systems\nthat interact even though each has its own private clock signal\n(usually with different periods).\n\n\n\n \n Question: What is a clocked synchronous sequential circuit? \n Answer: \n\nA.\n\nA sequential feedback circuit is a circuit where the outputs of some gates are fed back into the inputs of others. A clocked synchronous sequential circuit is a circuit that uses a clock signal to control when values change."}, {"text": "Title: Static Hazards: Causes and Cures* \n Text: {Static Hazards: Causes and Cures*}\n\nBefore we forget about the fact that real designs do not provide\nperfect clocks, let's explore some of the issues that engineers\nmust sometimes face.  \n\nWe discuss these primarily to ensure that you appreciate the power\nof the abstraction that we use in the rest of our course.\nIn later classes (probably our 298, which will absorb material \nfrom 385), you may be required to master this material.\n{ For now, we provide it simply for your interest.}\n\nConsider the circuit shown below, for which the output is given by \nthe equation S=AB+. \n\n{{file=part2/figs/lec15-1.eps,width=4in}}\n\nThe timing diagram on the right shows a { glitch} in the output\nwhen the input shifts from ABC=110 to 100, that is, when B falls.\nThe problem lies in the possibility that the upper AND gate, driven \nby B, might go low before the lower AND gate, driven by , goes\nhigh.  In such a case, the OR gate output S falls until the second\nAND gate rises, and the output exhibits a glitch.\n\nA circuit that might exhibit a glitch in an output that functionally\nremains stable at 1 is said to have a { static-1 hazard}.  The\nqualifier ``static'' here refers to the fact that we expect the output\nto remain static, while the ``1'' refers to the expected value of the\noutput.\n\n\n\nThe presence of hazards in circuits can be problematic in certain\ncases.  In domino logic, for example, an output is precharged and kept\nat 1 until the output of a driving circuit pulls it to 0, at which\npoint it stays low (like a domino that has been knocked over).  If the\ndriving circuit contains {static-1} hazards, the output may fall\nin response to a glitch.\n\nSimilarly, hazards can lead to unreliable behavior in sequential\nfeedback circuits.  Consider the addition of a feedback loop to the\ncircuit just discussed, as shown in the figure below.  The output of\nthe circuit is now given by the equation S^*=AB+S,\nwhere S^* denotes the state after S feeds back through the lower\nAND gate.  In the case discussed previously, the transition from\nABC=110 to 100, the glitch in S can break the feedback, leaving\nS low or unstable.  The resulting sequential feedback circuit is\nthus unreliable.  \n\n{{file=part2/figs/lec15-2.eps,width=4in}}\n\n\nEliminating static hazards from {two-level} circuits\nis fairly straightforward.  The Karnaugh map to the right corresponds\nto our original circuit; the solid lines indicate the \nimplicants selected by the AND gates.  A {static-1} hazard is\npresent when two adjacent 1s in the {K-map} are not covered by\na common implicant.  {Static-0} hazards do not occur in\n{two-level} SOP circuits.\n\n\n{file=part2/figs/lec15-3.eps,width=1in}\n\n\nEliminating static hazards requires merely extending the circuit with\nconsensus terms in order to ensure that some AND gate remains high\nthrough every transition between input states with\noutput 1.{Hazard elimination is not in general simple; we\nhave considered only {two-level} circuits.}  In the\n{K-map} shown, the dashed line indicates the necessary\nconsensus term, A.\n\n\n\n \n Question: What is the consensus term for the safety of a circuit? \n Answer: \n\nA. Static-0 hazards."}, {"text": "Title: Dynamic Hazards* \n Text: {Dynamic Hazards*}\n\nConsider an input transition for which we expect to see a change in an\noutput.  Under certain timing conditions, the output may not\ntransition smoothly, but instead bounce between its original value\nand its new value before coming to rest at the new value.  A circuit\nthat might exhibit such behavior is said to contain a { dynamic hazard}.\nThe qualifier ``dynamic'' refers to the expected change in the output.\n\nDynamic hazards appear only in more complex circuits, such as the one\nshown below.  The output of this circuit is defined by the equation Q=B+++BD. \n\n{{file=part2/figs/lec15-4.eps,width=3in}}\n\n\nConsider the transition from the input state ABCD=1111 to 1011,\nin which B falls from 1 to 0.  For simplicity, assume that each\ngate has a delay of 1 time unit.  If B goes low at time T=0, the\ntable shows the progression over time of logic levels at several\nintermediate points in the circuit and at the output Q.  Each gate\nmerely produces the appropriate output based on its inputs in the\nprevious time step.  After one delay, the three gates with B as a\ndirect input change their outputs (to stable, final values).  After\nanother delay, at T=2, the other three gates\nre-{-8pt}\n\n\n{|c|c|c|c|c|c|c|}\nT&f&g&h&i&j&Q \n0& 0& 0& 0& 1& 1& 1\n1& 1& 1& 1& 1& 1& 1\n2& 1& 1& 1& 0& 0& 0\n3& 1& 1& 1& 0& 1& 1\n4& 1& 1& 1& 0& 1& 0\n\n\nspond to the initial changes and flip their outputs.  The resulting\nchanges induce another set of changes at T=3, which in turn causes\nthe output Q to change a final time at T=4.\n\nThe output column in the table illustrates the possible impact of a\ndynamic hazard: rather than a smooth transition from 1 to 0, the\noutput drops to 0, rises back to 1, and finally falls to 0\nagain.  The dynamic hazard in this case can be attributed to the\npresence of a static hazard in the logic that produces intermediate\nvalue j.\n\n\n\n \n Question: What is the output of the circuit at time t? \n Answer: \nThe output Q of the circuit will have a value of 1 at time t because:\n\nA. The output of the gate with input B is 1.\n\nB. The output of the gate with input B is 0.\n\nC. The output of the gate with input B is 1, and the output of the gate with input D is 0.\n\nD. The output of the gate with input B is 0, and the output of the gate with input D is 1."}, {"text": "Title: Essential Hazards* \n Text: {Essential Hazards*}\n\n{ Essential hazards} are inherent to the function of a circuit and\nmay appear in any implementation.  In sequential feedback circuit\ndesign, they must be addressed at a low level to ensure that\nvariations in logic path lengths ({ timing skew}) through a circuit\ndo not expose them.  With clocked synchronous circuits, essential\nhazards are abstracted into a single form: { clock skew}, or\ndisparate clock edge arrival times at a circuit's flip-flops.\n\nAn example demonstrates the possible effects: consider the\nconstruction of a clocked synchronous circuit to recognize {0-1}\nsequences on an input IN.  Output Q should be held high for one\ncycle after recognition, that is, until the next rising clock edge.  A\ndescription of states and a state diagram for such a circuit appear\nbelow.\n\n\n{\n\nS_1S_0 &state& meaning\n00& A& nothing, 1, or 11 seen last\n01& B& 0 seen last\n10& C& 01 recognized (output high)\n11& unused&\n\n}\n\n\n{{file=part2/figs/lec15-5.eps,width=2in}}\n\n\nFor three states, we need two (=_2 3) flip-flops.\nDenote the internal state S_1S_0.  The specific internal\nstate values for each logical state (A, B, and C) simplify the\nimplementation and the example.  A { state table} \nand {K-maps} for the next-state logic appear\nbelow.  The state table uses one line per state with separate \ncolumns\nfor each input combination, making the table more compact than one\nwith one line per state/input combination.  Each column contains the\nfull next-state information, including output.  Using this form of the\nstate table, the {K-maps} can be read directly from the table.\n\n\n{\n\n& {IN}\nS_1S_0& 0& 1 \n00& 01/0& 00/0\n01& 01/0& 10/0\n11& x& x\n10& 01/1& 00/1\n\n}\n\n\n{{file=part2/figs/lec15-6.eps,width=3.5in}}\n\n\nExamining the {K-maps}, we see that the excitation and output\nequations are S_1^+=IN S_0, S_0^+=, and Q=S_1.\nAn implementation of the circuit using two D flip-flops appears below.\nImagine that mistakes in routing or process variations have made the\nclock signal's path to flip-flop 1 much longer than its path into\nflip-flop 0, as illustrated.\n\n{{file=part2/figs/lec15-7.eps,width=3in}}\n\nDue to the long delays, we cannot assume that rising clock edges\narrive at the flip-flops at the same time.  The result is called clock\nskew, and can make the circuit behave improperly by exposing essential\nhazards.  In the logical B to C transition, for example, we begin in\nstate S_1S_0=01 with IN=1 and the clock edge rising.  Assume that\nthe edge reaches flip-flop 0 at time T=0.  After a flip-flop delay\n(T=1), S_0 goes low.  After another AND gate delay (T=2), input\nD_1 goes low, but the second flip-flop has yet to change state!\nFinally, at some later time, the clock edge reaches flip-flop 1.\nHowever, the output S_1 remains at 0, leaving the system in state A\nrather than state C.\n\nFortunately, in clocked synchronous sequential circuits, all essential\nhazards are related to clock skew.  This fact implies that we can\neliminate a significant amount of complexity from circuit design by\ndoing a good job of distributing the clock signal.  It also implies\nthat, as a designer, you should avoid specious addition of logic in a\nclock path, as you may regret such a decision later, as you try to\ndebug the circuit timing.\n\n\n\n \n Question: What is the state of the system when it reaches flip-flop 1? \n Answer: \n\nA. True"}, {"text": "Title: Proof Outline for Clocked Synchronous Design* \n Text: {Proof Outline for Clocked Synchronous Design*}\n\nThis section outlines a proof of the claim made regarding clock\nskew being the only source of essential hazards for clocked\nsynchronous sequential circuits.  A { proof outline} suggests\nthe form that a proof might take and provides some of the logical\narguments, but is not rigorous enough to be considered a proof.\nHere we use a D flip-flop to illustrate a method for identifying\nessential hazards ({ the D flip-flop has no essential hazards, however}),\nthen argue that the method can be applied generally to collections\nof flip-flops in a clocked synchronous design to show that essential\nhazards occur only in the form of clock skew. \n\n\n{\n\n&{1-2}\nlow&L&clock low, last input low\nhigh&H&clock high, last input low\npulse low&PL&clock low, last input high (output high, too)\npulse high&PH&clock high, last input high (output high, too)\n\n}\n\n\n{{file=part2/figs/lec15-8.eps,width=2in}}\n\n\nConsider the sequential feedback state table for a positive\nedge-triggered D flip-flop, shown above.  In designing and analyzing\nsuch circuits, we assume that only one input bit changes at a time.\nThe state table consists of one row for each state and one column for\neach input combination.  Within a row, input combinations that have no\neffect on the internal state of the circuit (that is, those that do \nnot cause any change in\nthe state) are said to be stable; these states are circled.  Other\nstates are unstable, and the circuit changes state in response to\nchanges in the inputs.\n\nFor example, given an initial state L with low output, low clock,\nand high input D, the solid arcs trace the reaction of the circuit to a\nrising clock edge.  From the 01 input combination, we move along the\ncolumn to the 11 column, which indicates the new state, PH.  Moving\ndown the column to that state's row, we see that the new state is\nstable for the input combination 11, and we stop.  If PH were not\nstable, we would continue to move within the column until coming to\nrest on a stable state.\n\nAn essential hazard appears in such a table as a difference between\nthe final state when flipping a bit once and the final state when\nflipping a bit thrice in succession.  The dashed arcs in the figure\nillustrate the concept: after coming to rest in the PH state, we reset\nthe input to 01 and move along the PH row to find a new state of PL.\nMoving up the column, we see that the state is stable.  We then flip\nthe clock a third time and move back along the row to 11, which\nindicates that PH is again the next state.  Moving down the column, we\ncome again to rest in PH, the same state as was reached after one\nflip.  Flipping a bit three times rather than once evaluates the\nimpact of timing skew in the circuit; if a different state is reached\nafter two more flips, timing skew could cause unreliable behavior.  As\nyou can verify from the table, a D flip-flop has no essential hazards.\n\nA group of flip-flops, as might appear in a clocked synchronous\ncircuit, can and usually does have essential hazards, but only dealing\nwith the clock.  As you know, the inputs to a clocked synchronous\nsequential circuit consist of a clock signal and other inputs (either\nexternal of fed back from the flip-flops).  Changing an input other\nthan the clock can change the internal state of a flip-flop (of the\n\n master-slave variety), but flip-flop designs do not capture the number\n\ndual-latch variety), but flip-flop designs do not capture the number\nof input changes in a clock cycle beyond one, and changing an input\nthree times is the same as changing it once.  Changing the clock, of\ncourse, results in a synchronous state machine transition.\n\nThe detection of essential hazards in a clocked synchronous design\nbased on flip-flops thus reduces to examination of the state machine.\nIf the next state of the machine has any dependence on the current\nstate, an essential hazard exists, as a second rising clock edge moves\nthe system into a second new state.  For a single D flip-flop, the\nnext state is independent of the current state, and no essential\nhazards are present.\n\n\n\n\n\n \n Question: What is an essential hazard? \n Answer: \n\nAn essential hazard is a difference between the final state when flipping a bit once and the final state when flipping a bit thrice in succession."}, {"text": "Title: Using Abstraction to Simplify Problems \n Text: {Using Abstraction to Simplify Problems}\n\nIn this set of notes, we illustrate the use of abstraction to simplify\nproblems, then introduce a component called a multiplexer that allows\nselection among multiple inputs.\n\nWe begin by showing how two specific \nexamples---integer subtraction and identification of letters\nin ASCII---can be implemented using logic functions that we have already\ndeveloped.  We also introduce a conceptual technique for\nbreaking functions into smaller pieces, which allows us to solve\nseveral simpler problems and then to compose a full solution from \nthese partial solutions.\n\nTogether with the idea of bit-sliced designs that we introduced earlier,\nthese techniques help to simplify the process of designing logic that\noperates correctly.  The techniques can, of course, lead to \nless efficient designs,\nbut { correctness is always more important than performance}.\n\nThe potential loss of efficiency is often acceptable for three reasons.\n\nFirst, as we mentioned earlier, computer-aided design tools for \noptimizing logic functions are fairly effective, and in many cases\nproduce better results than human engineers (except in the rare cases \nin which the human effort required to beat the tools is worthwhile).\n\nSecond, as you know from the design of the {2's complement} \nrepresentation, we may be able to reuse specific pieces of hardware if\nwe think carefully about how we define our problems and representations.\n\nFinally, many tasks today are executed in software, which is designed\nto leverage the fairly general logic available via an instruction set\narchitecture.  A programmer cannot easily add new logic to a user's\nprocessor.  As a result, the hardware used to execute a\nfunction typically is not optimized for that function.\n\nThe approaches shown in this set of notes illustrate how abstraction\ncan be used to design logic.\n\n \n Question: What is the purpose of abstraction? \n Answer: \n\nA. Abstraction helps to simplify problems by breaking them down into smaller pieces that can be more easily solved."}, {"text": "Title: Subtraction \n Text: \n\nOur discussion of arithmetic implementation has focused so far on \naddition.  What about other operations, such as subtraction, multiplication,\nand division?  The latter two require more work, and we will not\ndiscuss them in detail until later in our class (if at all).\n\nSubtraction, however, can be performed almost trivially using logic that\nwe have already designed.\n\nLet's say that we want to calculate the difference D between \ntwo {N-bit} numbers A and B.  In particular, we want \nto find D=A-B.  For now, think of A, B, and D \nas 2's complement values.\n\nRecall how we defined the 2's complement representation: \nthe {N-bit} pattern that we use to represent -B is the \nsame as the base 2 bit pattern for (2^N-B), so we can use an adder if we\nfirst calculate the bit pattern for -B, then add the resulting\npattern to A.\nAs you know, our {N-bit} adder always produces a result that\nis correct modulo 2^N, so the result of such an operation,\nD=2^N+A-B, is correct so long as the subtraction does not overflow.\n\n\nHow can we calculate 2^N-B?  The same way that we do by hand!\nCalculate the 1's complement, (2^N-1)-B, then add 1.\n\nThe diagram to the right shows how we can use the {N-bit} adder\nthat we designed in Notes Set 2.3 to build an {N-bit} subtracter.\n\nNew elements appear in blue in the figure---the rest of the logic\nis just an adder.  The box labeled ``1's comp.'' calculates \nthe {1's complement} of the value B, which together with the\ncarry in value of 1 correspond to calculating -B.  What's in the\n``1's comp.'' box?  One inverter per bit in B.  That's all \nwe need to calculate the 1's complement.\n\nYou might now ask: does this approach also work for unsigned numbers?\nThe answer is yes, absolutely.  However, the overflow conditions for\nboth 2's complement and unsigned subtraction are different than the\noverflow condition for either type of addition.  What does the carry\nout of our adder signify, for example?  The answer may not be \nimmediately obvious.\n\n\n\n\n\nLet's start with the overflow condition for unsigned subtraction.\n\nOverflow means that we cannot represent the result.  With an {N-bit}\nunsigned number, we have A-B[0,2^N-1].  Obviously, the difference\ncannot be larger than the upper limit, since A is representable and\nwe are subtracting a non-negative (unsigned) value.  We can thus assume\nthat overflow occurs only when A-B<0.  In other words, when A<B.\n\n\n\nTo calculate the unsigned subtraction overflow condition in terms of the\nbits, recall that our adder is calculating 2^N+A-B.  The carry out\nrepresents the 2^N term.  When A, the result of the adder\nis at least 2^N, and we see a carry out, C_=1.  However, when\nA<B, the result of the adder is less than 2^N, and we see no carry\nout, C_=0.  { Overflow for unsigned subtraction is thus inverted \nfrom overflow for unsigned addition}: a carry out of 0 indicates an \noverflow for subtraction.\n\nWhat about overflow for 2's complement subtraction?  We can use arguments\nsimilar to those that we used to reason about overflow of 2's complement\naddition to prove that subtraction of one negative number from a second\nnegative number can never overflow.  Nor can subtraction of a non-negative\nnumber from a second non-negative number overflow.\n\nIf A and B<0, the subtraction overflows iff A-B{2^{N-1}}.\nAgain using similar arguments as before, we can prove that the difference D\nappears to be negative in the case of overflow, so the product\n{A_{N-1}} B_{N-1} D_{N-1} evaluates to 1 when this type of\noverflow occurs (these variables represent the most significant bits of the \ntwo operands and the difference; in the case of 2's complement, they are\nalso the sign bits).\n\nSimilarly, if A<0 and B, we have overflow when A-B<-2^{N-1}.\nHere we can prove that D on overflow, so \nA_{N-1} {B_{N-1}} {D_{N-1}} evaluates to 1.\n\nOur overflow condition for {N-bit} 2's complement subtraction\nis thus given by the following:\n\n{eqnarray*}\n{A_{N-1}} B_{N-1} D_{N-1}+A_{N-1} {B_{N-1}} {D_{N-1}}\n{eqnarray*}\n\nIf we calculate all four overflow conditions---unsigned and 2's complement,\naddition and subtraction---and provide some way to choose whether or not to \ncomplement B and to control the C_ input, we can use the same hardware\nfor addition and subtraction of either type.\n\n \n Question: What does an overflow for unsigned subtraction indicate? \n Answer: \n\nA. A carry out of 1 for unsigned subtraction indicates an overflow."}, {"text": "Title: Checking ASCII for Upper-case Letters \n Text: {Checking ASCII for Upper-case Letters}\n\nLet's now consider how we can check whether or not an ASCII character is\nan upper-case letter.  Let's call the {7-bit}\nletter C=C_6C_5C_4C_3C_2C_1C_0 and the function that we want to\ncalculate U(C).  The function U should equal 1 whenever C represents\nan upper-case letter, and should equal 0 whenever C does not.\n\nIn ASCII, the {7-bit} patterns from 0x41 through 0x5A correspond\nto the letters A through Z in alphabetic order.  Perhaps you want to draw \na {7-input} {K-map}?  Get a few large sheets of paper!\n\nInstead, imagine that we've written the full {128-row} truth\ntable.  Let's break the truth table into pieces.  Each piece will\ncorrespond to one specific pattern of the three high bits C_6C_5C_4,\nand each piece will have 16 entries for the four low bits C_3C_2C_1C_0.\nThe truth tables for high bits 000, 001, 010, 011, 110, and 111\nare easy: the function is exactly 0.  The other two truth tables appear \non the left below.  We've called the two functions T_4 and T_5, where\nthe subscripts correspond to the binary value of the three high bits of C.\n\n{\n\n{cccc|cc}\nC_3& C_2& C_1& C_0& T_4& T_5 \n0& 0& 0& 0& 0& 1\n0& 0& 0& 1& 1& 1\n0& 0& 1& 0& 1& 1\n0& 0& 1& 1& 1& 1 \n0& 1& 0& 0& 1& 1\n0& 1& 0& 1& 1& 1\n0& 1& 1& 0& 1& 1\n0& 1& 1& 1& 1& 1 \n1& 0& 0& 0& 1& 1\n1& 0& 0& 1& 1& 1\n1& 0& 1& 0& 1& 1\n1& 0& 1& 1& 1& 0 \n1& 1& 0& 0& 1& 0\n1& 1& 0& 1& 1& 0\n1& 1& 1& 0& 1& 0\n1& 1& 1& 1& 1& 0\n\n\n\n{file=part2/figs/ascii-t4.eps,width=1.15in}\n{file=part2/figs/ascii-t5.eps,width=1.15in}\n\n\n{eqnarray*}\nT_4&=&C_3+C_2+C_1+C_0      \nT_5&=&{C_3}+{C_2} {C_1}+{C_2} {C_0}\n{eqnarray*}\n\n\nAs shown to the right of the truth tables, we can then draw simpler\n{K-maps} for T_4 and T_5, and can solve the {K-maps}\nto find equations for each, as shown to the right (check that you get\nthe same answers).\n\nHow do we merge these results to form our final expression for U?\n\nWe AND each of the term functions (T_4 and T_5) with the \nappropriate minterm for the\nhigh bits of C, then OR the results together, as shown here:\n\n{eqnarray*}\nU&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5\n\n&=&C_6 {C_5} {C_4} (C_3+C_2+C_1+C_0)+\nC_6 {C_5} C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0})\n{eqnarray*}\n\nRather than trying to optimize by hand, we can at this point let the CAD\ntools take over, confident that we have the right function to identify\nan upper-case ASCII letter.\n\nBreaking the truth table into pieces and using simple logic to reconnect\nthe pieces is one way to make use of abstraction when solving complex\nlogic problems.  \n\nIn fact, recruiters for some companies often ask\nquestions that involve using specific logic elements as building blocks\nto implement other functions.  Knowing that you can implement\na truth table one piece at a time will help you to solve this type of\nproblem.\n\nLet's think about other ways to tackle the problem of calculating U.\nIn Notes Sets 2.3 and 2.4, we developed adders and comparators.  Can\nwe make use of these components as building blocks to check whether C \nrepresents an\nupper-case letter?  Yes, of course we can: by comparing C with the\nends of the range of upper-case letters, we can check whether or not C\nfalls in that range.\n\nThe idea is illustrated on the left below using two {7-bit} comparators\nconstructed as discussed in Notes Set 2.4.\nThe comparators are the black parts of the drawing, while the blue parts\nrepresent our extensions to calculate U.  Each comparator is given\nthe value C as one input.  The second value to the comparators is \neither the letter A (0x41) or the letter Z (0x5A).  The meaning of \nthe {2-bit} input to and result of each comparator is given in the \ntable on the right below.  The inputs on the right of each comparator\nare set to 0 to ensure that equality is produced if C matches\nthe second input (B).\n\nOne output from each comparator is then routed to\na NOR gate to calculate U.  Let's consider how this combination works.\nThe left comparator compares C with the letter A (0x41).  If C 0x41,\nthe comparator produces Z_0=0.  In this case, we may have a letter.\nOn the other hand, if C< 0x41, the comparator produces Z_0=1, and\nthe NOR gate outputs U=0, since we do not have a letter in this case.\n\nThe right comparator compares C with the letter Z (0x5A).  If C 0x5A,\nthe comparator produces Z_1=0.  In this case, we may have a letter.\nOn the other hand, if C> 0x5A, the comparator produces Z_1=1, and\nthe NOR gate outputs U=0, since we do not have a letter in this case.\n\nOnly when 0x41  0x5A does U=1, as desired. \n\n\n\n{\n\n{file=part2/figs/ascii-cmp-based.eps,width=3.6in}\n\n\n{cc|l}\nZ_1& Z_0& meaning \n0& 0& A=B\n0& 1& A<B\n1& 0& A>B\n1& 1& not used\n\n\n\n\n\n\n\n\n\nWhat if we have only {8-bit} adders available for our use, such as\nthose developed in Notes Set 2.3?  Can we still calculate U?  Yes.\nThe diagram shown to the right illustrates the approach, again with black\nfor the adders and blue for our extensions.  Here we are \nactually using the adders as subtracters, but calculating the 1's complements\nof the constant values by hand.  The ``zero extend'' box simply adds\na leading 0 to our {7-bit} ASCII letter.  The left adder \nsubtracts the letter A from C: if no carry is produced, we know that\nC< 0x41 and thus C does not represent an upper-case letter, and U=0.\n\nSimilarly, the right adder subtracts 0x5B (the letter Z plus one)\nfrom C.  If a carry is produced, we know that C 0x5B, and \nthus C does not represent an upper-case letter, and U=0.\n\nWith the right combination of carries (1 from the left and 0 from the\nright), we obtain U=1.\n\n\n{file=part2/figs/ascii-add-based.eps,width=2.75in}\n\n\nLooking carefully at this solution, however, you might be struck by the\nfact that we are calculating two sums and then discarding them.  Surely\nsuch an approach is inefficient?\n\nWe offer two answers.  First, given the design shown above, a good CAD tool\nrecognizes that the sum outputs of the adders are not being used,\nand does not generate logic to calculate them.  The logic for the two \ncarry bits used to calculate U can then be optimized.  Second, the design\nshown, including the calculation of the sums, is similar in efficiency\nto what happens at the rate of about 10^ times per second, \n24 hours a day, seven days a week,\ninside processors in data centers processing HTML, XML, and other types\nof human-readable Internet traffic.  Abstraction is a powerful tool.\n\nLater in our class, you will learn how to control logical connections\nbetween hardware blocks so that you can make use of the same hardware\nfor adding, subtracting, checking for upper-case letters, and so forth.\n\n \n Question: What is the value of an ASCII character compared to the letters A through Z? \n Answer: \n\nA. U represents the value of an ASCII character compared to the letters A through Z. If the character is equal to or greater than A and less than or equal to Z, then U will equal 1. If the character is not in that range, then U will equal 0."}, {"text": "Title: Checking ASCII for Lower-case Letters \n Text: {Checking ASCII for Lower-case Letters}\n\nHaving developed several approaches for checking for an upper-case letter,\nthe task of checking for a lower-case letter should be straightforward.\nIn ASCII, lower-case letters are represented by the {7-bit} patterns \nfrom 0x61 through 0x7A.\n\nOne can now easily see how more abstract designs make solving similar\ntasks easier.  If we have designed our upper-case checker \nwith {7-variable} K-maps,\nwe must start again with new K-maps for the lower-case checker.  If\ninstead we have taken the approach of designing logic for the upper and \nlower bits of the ASCII character, we can reuse most of that logic,\nsince the functions T_4 and T_5 are identical when checking for\na lower-case character.  Recalling the algebraic form of U(C), we can\nthen write a function L(C) (a lower-case checker) as shown on the\nleft below.\n\n\n{eqnarray*}\nU&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5\n\nL&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5\n\n&=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+\n&&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0})\n{eqnarray*}\n\n\n{file=part2/figs/ascii-cmp-lower.eps,width=3.6in}\n\n\nFinally, if we have used a design based on comparators or adders, the\ndesign of a lower-case checker becomes trivial: simply change the numbers\nthat we input to these components, as shown in the figure on the right above\nfor the comparator-based design.  The only changes from the upper-case checker\ndesign are the inputs to the comparators and the output produced, \nhighlighted with blue text in the figure.\n\n\n\n \n Question: What is the advantage of using a design based on comparators or adders? \n Answer: \n\nA. The advantage of using a design based on comparators or adders when checking for a lower-case letter is that the design becomes trivial: simply change the numbers that we input to these components."}, {"text": "Title: The Multiplexer \n Text: {The Multiplexer}\n\nUsing the more abstract designs for checking ranges of ASCII characters,\nwe can go a step further and create a checker for both upper- and lower-case\nletters.  To do so, we add another input S that allows us to select the \nfunction that we want---either the upper-case checker U(C) or the \nlower-case checker L(C).\n\nFor this purpose, we make use of a logic block called a { multiplexer},\nor { mux}.\nMultiplexers are an important abstraction for digital logic.  In \ngeneral, a multiplexer allows us to use one digital signal to \nselect which of several others is forwarded to an output.\n\n\nThe simplest form of the multiplexer is the 2-to-1 multiplexer shown to \nthe right.   The logic diagram illustrates how the mux works.  The block \nhas two inputs from the left and one from the top.  The top input allows \nus to choose which of the left inputs is forwarded to the output.  \nWhen the input S=0, the upper AND gate outputs 0, and the lower AND gate\noutputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0.\nSimilarly, when input S=1, the upper AND gate outputs D_1, and the\nlower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1.\n\n\n\n\n\nThe symbolic form of the mux is a trapezoid with data inputs on the \nlarger side, an output on the smaller side, and a select input on the\nangled part of the trapezoid.  The labels inside the trapezoid indicate \nthe value of the select input S for which the adjacent data signal, \nD_1 or D_0, is copied to the output Q.\n\nWe can generalize multiplexers in two ways.  First, we can extend the \nsingle select input to a group of select inputs.  An {N-bit}\nselect input allows selection from amongst 2^N inputs.  A {4-to-1} \nmultiplexer is shown below, for example.  The logic diagram on the left\nshows how the {4-to-1 mux} operates.  For any combination of S_1S_0,\nthree of the AND gates produce 0, and the fourth outputs the D input\ncorresponding to the interpretation of S as an unsigned number.\nGiven three zeroes and one D input, the OR gate thus reproduces one of \nthe D's.  When S_1S_0=10, for example, the third AND gate copies D_2,\nand Q=D_2.\n\n{{file=part2/figs/mux4-to-1.eps,width=5.60in}}\n\nAs shown in the middle figure, a {4-to-1} mux can also be built from\nthree {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux \nappears on the right in the figure.\n\n\n\nThe second way in which we can generalize multiplexers is by using\nseveral multiplexers of the same type and using the same signals for \nselection.  For example, we might use a single select bit T to choose \nbetween any number of paired inputs.  Denote input pair by i D_1^i \nand D_0^i.  For each pair, we have an output Q_i.  \n\nWhen T=0, Q_i=D_0^i for each value of i.\n\nAnd, when T=1, Q_i=D_1^i for each value of i.\n\nEach value of i requires a {2-to-1} mux with its select input\ndriven by the global select signal T.\n\nReturning to the example of the upper- and lower-case checker, we\ncan make use of two groups of seven {2-to-1} muxes, all controlled by\na single bit select signal S, to choose between the inputs needed\nfor an upper-case checker and those needed for a lower-case checker.\n\nSpecific configurations of multiplexers are often referred to\nas { N-to-M multiplexers}.  Here the value N refers to the\nnumber of inputs, and M refers to the number of outputs.  The\nnumber of select bits can then be calculated as _2(N/M)---N/M \nis generally a power of two---and one way to build such a \nmultiplexer is to use M copies of\nan  (N/M)-to-1 multiplexer.\n\nLet's extend our upper- and lower-case checker to check\nfor four different ranges of ASCII characters, as shown below.\nThis design uses two {28-to-7} muxes to create a single\nchecker for the four ranges.  Each of the muxes in the figure logically \nrepresents seven {4-to-1} muxes.\n\n{{file=part2/figs/ascii-four-range.eps,width=3.75in}}\n\n\nThe table to the right describes the behavior of the checker.\n\nWhen the select input S is set to 00, the left mux selects the value 0x00,\nand the right mux selects the value 0x1F, which checks whether the ASCII\ncharacter represented by C is a control character.\n\nWhen the select input S=01, the muxes produce the values needed to check\nwhether C is an upper-case letter.\n\nSimilarly, when the select input S=10,\n\n\n{c|c|c|c}\n& left& right& \n& comparator& comparator& \nS_1S_0& input& input& R(C) produced  \n00& 0x00& 0x1F& control character?\n01& 0x41& 0x5A& upper-case letter?\n10& 0x61& 0x7A& lower-case letter?\n11& 0x30& 0x39& numeric digit? \n\n\n\nthe muxes produce the values \nneeded to check whether C is a lower-case letter.\n\nFinally, when the select input S=11, the left mux selects the value 0x30,\nand the right mux selects the value 0x39, which checks whether the ASCII\ncharacter represented by C is a digit (0 to 9).\n\n\n\n\n \n Question: Is the multiplexer a basic logic gate? \n Answer: \n\nA. No, the multiplexer is not a basic logic gate."}, {"text": "Title: Example: Bit-Sliced Comparison \n Text: {Example: Bit-Sliced Comparison}\n\nThis set of notes develops comparators for unsigned and 2's complement \nnumbers using the bit-sliced approach that we introduced in Notes Set 2.3.  \nWe then use algebraic manipulation and variation of the internal \nrepresentation to illustrate design tradeoffs.\n\n\n \n Question: What is the difference between unsigned and 2's complement numbers? \n Answer: \n\nA.\n\nThe difference between unsigned and 2's complement numbers is that 2's complement numbers can represent both positive and negative numbers, while unsigned numbers can only represent positive numbers."}, {"text": "Title: Comparing Two Numbers \n Text: {Comparing Two Numbers}\n\n\nLet's begin by thinking about how we as humans compare two {N-bit}\nnumbers, A and B.\n\nAn illustration appears to the right, using N=8.\n\nFor now, let's assume that our numbers are stored in an unsigned \nrepresentation, so we can just think of them as binary numbers\nwith leading 0s.\n\nWe handle 2's complement values later in these notes.\n\nAs humans, we typically start comparing at the most significant bit.\nAfter all, if we find a difference in that bit, we are done, saving\nourselves some time.  In the example to the right, we know that A<B \nas soon as we reach bit 4 and observe that A_4<B_4.\n\nIf we instead start from the least significant bit,\nwe must always look at all of the bits.\n\nWhen building hardware to compare all of the bits at once, however,\nhardware for comparing each bit must exist, and the final result \nmust be able to consider\n\n\n\n\n\nall of the bits.  Our choice of direction should thus instead depend on \nhow effectively we can build the \ncorresponding functions.  For a single bit slice, the two directions \nare almost identical.  Let's develop a bit slice for\ncomparing from least to most significant.\n\n{ NOTE TO SELF: We should either do the bit-by-bit comparator \nstate machine in the notes or as an FSM homework problem.  Probably\nthe lattter.}\n\n \n Question: What is the most significant bit of a number? \n Answer: \n\nA. If a difference is found in the most significant bit, the comparison can be stopped, saving time."}, {"text": "Title: An Abstract Model \n Text: {An Abstract Model}\n\nComparison of two numbers, A and B, can produce three possible\nanswers: A<B, A=B, or A>B (one can also build an equality\ncomparator that combines the A<B and A>B cases into a single \nanswer).\n\nAs we move from bit to bit in our design, how much information needs \nto pass from one bit to the next?\n\nHere you may want to think about how you perform the task yourself.\nAnd perhaps to focus on the calculation for the most significant bit.\nYou need to know the values of the two bits that you are comparing.\nIf those two are not equal, you are done.\n\nBut if the two bits are equal, what do you do?\n\nThe answer is fairly simple: pass along the result\nfrom the less significant bits.\n\nThus our bit slice logic for bit M needs to be able to accept \nthree possible answers from the bit slice logic for bit M-1 and must\nbe able to pass one of three possible answers to the logic for bit M+1.\n\nSince _2(3)=2, we need two bits of input and two bits\nof output in addition to our input bits from numbers A and B.\n\n\nThe diagram to the right shows an abstract model of our \ncomparator bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming from the top or left \nand outputs going to the bottom or right.  Outside of the bit \nslice logic, we index\nthese comparison bits using the bit number.  The bit slice\nhas C_1^{M-1} and C_0^{M-1} provided as inputs and \nproduces C_1^M and C_0^M as outputs.\n\nInternally, we use C_1 and C_0 to denote these inputs,\nand Z_1 and Z_0 to denote the outputs.\n\nSimilarly, the\n\n\n\n\n\nbits A_M and B_M from the numbers A and B are\nrepresented internally simply as A and B.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.\n\n\n\n\n \n Question: What is the bit slice logic for bit M? \n Answer: \n\nA. The bit slice logic needs to be able to accept three possible answers from the bit slice logic for bit M-1 and must be able to pass one of three possible answers to the logic for bit M+1."}, {"text": "Title: A Representation and the First Bit \n Text: {A Representation and the First Bit}\n\n\nWe need to select a representation for our three possible answers before\nwe can design any logic.  The representation chosen affects the\nimplementation, as we discuss later in these notes.  For now, we simply\nchoose the representation to the right, which seems reasonable.\n\nNow we can design the logic for the first bit (bit 0).  In keeping with\nthe bit slice philosophy, in practice we simply use another\ncopy of the full bit slice design for bit 0 and attach the C_1C_0 \ninputs to ground (to denote A=B).  Here we tackle the simpler problem\nas a warm-up exercise.\n\n\n{cc|l}\nC_1& C_0& meaning \n0& 0& A=B\n0& 1& A<B\n1& 0& A>B\n1& 1& not used\n\n\n\n\nThe truth table for bit 0 appears to the right (recall that we use Z_1\nand Z_0 for the output names).  Note that the bit 0 function has\nonly two meaningful inputs---there is no bit to the right of bit 0.\n\nIf the two inputs A and B are the same, we output equality.\nOtherwise, we do a {1-bit} comparison and use our representation mapping\nto select the outputs.\n\nThese functions are fairly straightforward to derive by inspection.\nThey are:{-12pt}\n\n{eqnarray*}\nZ_1 &=& A \nZ_0 &=&  B\n{eqnarray*}\n\n\n{\n{cc|cc}\nA& B& Z_1& Z_0 \n0& 0& 0& 0\n0& 1& 0& 1\n1& 0& 1& 0\n1& 1& 0& 0\n\n}\n\n\nThese forms should also be intuitive, given the representation\nthat we chose: \nA>B if and only if A=1 and B=0;\nA<B if and only if A=0 and B=1. \n\n\nImplementation diagrams for our one-bit functions appear to the right.\nThe diagram to the immediate right shows the implementation as we might\ninitially draw it, and the diagram on the far right shows the \nimplementation\n\n\n\n\n\n\n\n\nconverted to NAND/NOR gates for a more accurate estimate of complexity\nwhen implemented in CMOS.\n\nThe exercise of designing the logic for bit 0 is also useful in the sense\nthat the logic structure illustrated forms the core of the full design\nin that it identifies the two cases that matter: A<B and A>B.\n\nNow we are ready to design the full function.  Let's start by writing\na full truth table, as shown on the left below.\n\n[t]\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0\n0& 0& 1& 1& x& x \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1\n0& 1& 1& 1& x& x \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0\n1& 0& 1& 1& x& x \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0\n1& 1& 1& 1& x& x\n\n\n\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0 \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1 \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0 \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0 \nx& x& 1& 1& x& x\n  \n\n\n\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0 \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1 \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0 \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0 \n{c|}& x& x\n  \n\n\n\nIn the truth table, we \nmarked the outputs as ``don't care'' (x's) whenever C_1C_0=11.\nYou might recall that we ran into problems with our ice cream dispenser\ncontrol in Notes Set 2.2.  However, in that case we could not safely\nassume that a user did not push multiple buttons.  Here, our bit\nslice logic only accepts inputs from other copies of itself (or a fixed\nvalue for bit 0), and---assuming that we design the logic\ncorrectly---our bit slice never generates the 11 combination.\nIn other words, that input combination is impossible (rather than\nundesirable or unlikely), so the result produced on the outputs is \nirrelevant.\n\nIt is tempting to shorten the full truth table by replacing groups of rows.\nFor example, if AB=01, we know that A<B, so the less significant\nbits (for which the result is represented by the C_1C_0 inputs)\ndon't matter.  We could write one row with input pattern ABC_1C_0=01xx and\noutput pattern Z_1Z_0=01.  We might also collapse our ``don't care'' output\npatterns: whenever the input matches ABC_1C_0=xx11, we don't care\nabout the output, so Z_1Z_0=xx.  But these two rows overlap in the\ninput space!  In other words, some input patterns, such as ABC_1C_0=0111,\nmatch both of our suggested new rows.  Which output should take precedence?\nThe answer is that a reader should not have to guess.  { Do not use \noverlapping rows to shorten a truth table.}  In fact, the first of the \nsuggested new rows is not valid: we don't need to produce output 01 \nif we see C_1C_0=11.  Two valid short forms of this truth table appear to \nthe right of the full table.  If you have an ``other'' entry, as\nshown in the rightmost table, this entry should always appear as the \nlast row.  Normal rows, including rows representing multiple input\npatterns, are not required\nto be in any particular order.  Use whatever order makes the table\neasiest to read for its purpose (usually by treating the input pattern\nas a binary number and ordering rows in increasing numeric order).\n\n\nIn order to translate our design into algebra, we transcribe the truth\ntable into a {K-map} for each output variable, as shown to the right.\nYou may want to perform this exercise yourself and check that you \nobtain the same solution.  Implicants for each output are marked\nin the {K-maps}, giving the following equations:\n\n{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\nZ_0 &=&  B +  C_0 + B C_0 \n{eqnarray*}\n\n\n\n\n\n\n\n\n\nAn implementation based on our equations appears to the right.  \nThe figure makes it easy to see the symmetry between the inputs, which\narises from the representation that we've chosen.  Since the design\nonly uses two-level logic (not counting the inverters on the A\nand B inputs, since inverters can be viewed as {1-input} \nNAND or NOR gates), converting to NAND/NOR simply requires replacing\nall of the AND and OR gates with NAND gates.\n\nLet's discuss the design's efficiency roughly in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\n\nOur initial design uses two inverters, six {2-input} gates, and \ntwo {3-input} gates.\n\nFor speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.  \n\nWe can thus estimate our design's\n\n\n{file=part2/figs/comparator-try-one.eps,width=2.8in}\n\n\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer,\nbut, as we have discussed, the diagram above is equivalent on a \ngate-for-gate basis.  Here we have three gate delays from the A \nand B inputs to the outputs (through the inverters).\n\nBut when we connect multiple copies of our bit slice logic together to \nform a comparator, as shown on the next page, the delay from \nthe A and B inputs\nto the outputs is not as important as the delay from the C_1 and C_0 \ninputs to the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis.  Looking again at the diagram, \nnotice that we have only two gate delays from the C_1 and C_0 inputs \nto the\noutputs.  The total delay for an {N-bit} comparator based on this\nimplementation is thus three gate delays for bit 0 and two more gate\ndelays per additional bit, for a total of 2N+1 gate delays.\n\n\n\n\n \n Question: What is the logic for bit 0? \n Answer: \nQ. }\nQ. \nQ. ]\n\n\n\n[{The One-Bit Comparator in More Detail}\n\n\nBefore moving to the full design, let's briefly discuss the design\nof the bit slice logic more carefully, starting with the one-bit\ncomparator.\n\nOur truth table for bit 0 included four empty rows (indicated by ``x'').\nThese rows are not needed since we know the values of the C_1C_0 inputs\nfor those rows.  Thus, we can remove those rows from the truth table,\ngiving the truth table shown to the right.\n\n{file=part2/figs/ice-cream-try-one.eps,width=3.3in}\n\nThe truth table and the {K-maps} (shown to the right below)\nindicate that our design should be fairly simple.  In fact, we can\nderive the following {implicants} for each output variable:\n\n{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\nZ_0 &=&  B +  C_0 + B C_0 "}, {"text": "Title: Optimizing Our Design \n Text: {Optimizing Our Design}\n\nWe have a fairly good design at this point---good enough for a homework\nor exam problem in this class, certainly---but let's consider how we\nmight further optimize it.  Today, optimization of logic at this level is \ndone mostly by computer-aided design (CAD) tools, but we want you to \nbe aware of the sources of optimization potential and the tradeoffs \ninvolved.  And, if the topic interests you, someone has to continue to \nimprove CAD software!\n\nThe first step is to manipulate our algebra to expose common terms that\noccur due to the design's symmetry.  Starting with our original equation\nfor Z_1, we have\n\n{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\n    &=& A  + ( A +  )  C_1\n    &=& A  +  B} C_1\n     Z_0 &=&  B + {A  C_0\n{eqnarray*}{-12pt}\n\nNotice that the second term in each equation now includes the complement of \nfirst term from the other equation.  For example, the Z_1 equation includes\nthe complement of the B product that we need to compute Z_0.  \nWe may be able to improve our design by combining these computations.\n\n\nAn implementation based on our new algebraic formulation appears to the \nright.  In this form, we seem to have kept the same number of gates,\nalthough we have replaced the {3-input} gates with inverters.\nHowever, the middle inverters disappear when we convert to NAND/NOR form,\nas shown below to the right.  Our new design requires only two inverters\nand six {2-input} gates, a substantial reduction relative to the\noriginal implementation.\n\nIs there a disadvantage?  Yes, but only a slight one.  Notice that the\npath from the A and B inputs to the outputs is now four gates (maximum)\ninstead of three.  Yet the path from C_1 and C_0 to the outputs is\nstill only two gates.  Thus, overall, we have merely increased our\n{N-bit} comparator's delay from 2N+1 gate delays to\n2N+2 gate delays.\n\n\n{file=part2/figs/comparator-opt.eps,width=4.1in}\n\n{file=part2/figs/comparator-opt-nn.eps,width=4.1in}\n\n\n\n\n \n Question: What is the advantage of a shorter path from the inputs to the outputs? \n Answer: \n\nA. A shorter path from the A and B inputs to the outputs results in a faster overall comparator."}, {"text": "Title: Extending to 2's Complement \n Text: {Extending to 2's Complement}\n\nWhat about comparing 2's complement numbers?  Can we make use of the\nunsigned comparator that we just designed?\n\nLet's start by thinking about the sign of the numbers A and B.\nRecall that 2's complement records a number's sign in the most \nsignificant bit.  For example, in the {8-bit} numbers shown in the \nfirst diagram in this set of notes, the sign \nbits are A_7 and B_7.\n\nLet's denote these sign bits in the general case by A_s and B_s.\n\nNegative numbers have a sign bit equal to 1, and non-negative numbers\nhave a sign bit equal to 0.\n\nThe table below outlines an initial evaluation of the four possible\ncombinations of sign bits.\n\n{cc|c|l}\nA_s& B_s& interpretation& solution \n0& 0& A AND B& use unsigned comparator on remaining bits\n0& 1& A AND B<0& A>B\n1& 0& A<0 AND B& A<B\n1& 1& A<0 AND B<0& unknown\n\n\n\nWhat should we do when both numbers are negative?  Need we design \na completely separate logic circuit?  Can we somehow convert a \nnegative value to a positive one?\n\nThe answer is in fact much simpler.  Recall that {2's complement}\nis defined based on modular arithmetic.  Given an {N-bit} negative\nnumber A, the representation for the bits A[N-2:0] is the same\nas the binary (unsigned) representation of A+2^{N-1}.  An example\nappears to the right.\n\n\n{file=part2/figs/comparing-2s.eps,width=2.55in}\n\n\nLet's define A_r=A+2^{N-1} as the value of the \nremaining bits for A and B_r similarly for B.\n\nWhat happens if we just go ahead and compare A_r and B_r using \nan {(N-1)-bit} unsigned comparator?\n\nIf we find that A_r<B_r we know that A_r-2^{N-1}<B_r-2^{N-1} as well, \nbut that means A<B!  We can do the same with either of the other\npossible results.  In other words, simply comparing A_r with B_r\ngives the correct answer for two negative numbers as well.\n\n\nAll we need to design is a logic block for the sign bits.  At this point,\nwe might write out a {K-map}, but instead let's rewrite our\nhigh-level table with the new information, as shown to the right.\n\nLooking at the table, notice the similarity \nto the high-level design for a single bit of an unsigned value.  The\n\n\n{cc|l}\nA_s& B_s& solution \n0& 0& pass result from less significant bits\n0& 1& A>B\n1& 0& A<B\n1& 1& pass result from less significant bits\n\n\n\nonly difference is that the two A=B cases are reversed.  If we\nswap A_s and B_s, the function is identical.  We can simply use\nanother bit slice but swap these two inputs.\n\nImplementation of an {N-bit} 2's complement comparator based\non our bit slice comparator is shown below.  The blue circle highlights\nthe only change from the {N-bit} unsigned comparator, which is\nto swap the two inputs on the sign bit.\n\n{{file=part2/figs/integrated-2s.eps,width=5.5in}}\n\n\n\n \n Question: What is the only difference between the two cases of A and B? \n Answer: \n\nA. The only difference is that the two A=B cases are reversed."}, {"text": "Title: Further Optimization \n Text: {Further Optimization}\n\n\nLet's return to the topic of optimization.  To what extent \ndid the representation of the three outcomes affect our ability\nto develop a good bit slice design?  Although selecting a good \nrepresentation can be quite important, for this particular problem\nmost representations lead to similar implementations.\n\nSome representations, however, have interesting properties.  Consider\n\n\n{cc|l|l}\nC_1& C_0& original& alternate \n0& 0& A=B& A=B\n0& 1& A<B& A>B\n1& 0& A>B& not used\n1& 1& not used& A<B\n\n\n\nthe alternate representation on the right, for example (a copy of the \noriginal representation is included for comparison).  Notice that \nin the alternate representation, C_0=1 whenever A=B.  \n\nOnce we have found the numbers to be different in some bit, the end\nresult can never be equality, so perhaps with the right \nrepresentation---the new one, for example---we might be able to\ncut delay in half?\n\n\nAn implementation based on the alternate representation appears in the\ndiagram to the right.  As you can see, in terms of gate count,\nthis design replaces one {2-input} gate with an inverter and\na second {2-input} gate with a {3-input} gate.  The path\nlengths are the same, requiring 2N+2 gate delays for \nan {N-bit} comparator.\nOverall, it is about the same as our original design.\n\n\n{file=part2/figs/comparator-opt-alt.eps,width=4.1in}\n\n\nWhy didn't it work?  Should we consider still other representations?\nIn fact, none of the possible representations that we might choose\nfor a bit slice can cut the delay down to one gate delay per bit.  \nThe problem is fundamental, and is related to the nature of CMOS.\nFor a single bit slice, we define the incoming and outgoing \nrepresentations to be the same.  We also need to have at least\none gate in the path to combine the C_1 and C_0 inputs with\ninformation from the bit slice's A and B inputs.  But all CMOS\ngates invert the sense of their inputs.  Our choices are limited\nto NAND and NOR.  Thus we need at least two gates in the path to\nmaintain the same representation.\n\nOne simple answer is to use different representations for odd and\neven bits.  Instead, we optimize a logic circuit\nfor comparing two bits.  We base our design on the alternate \nrepresentation.  The implementation is shown below.  The left\nshows an implementation based on the algebra, and the right shows\na NAND/NOR implementation.  Estimating by gate count and number of\ninputs, the {two-bit} design doesn't save much over two\nsingle bit slices in terms of area.  In terms of delay, however,\nwe have only two gate delays from C_1 and C_0 to either output.\nThe longest path from the A and B inputs to the outputs is\nfive gate delays.  Thus, for an {N-bit} comparator built\nwith this design, the total delay is only N+3 gate delays.\nBut N has to be even.\n\n{file=part2/figs/comparator-two.eps,width=3.125in}\n{file=part2/figs/comparator-two-nn.eps,width=3.125in}\n\nAs you can imagine, continuing to scale up the size of our logic\nblock gives us better performance at the expense of a more complex\ndesign.  Using the alternate representation may help you to see\nhow one can generalize the approach to larger groups of bits---for\nexample, you may have noticed the two bitwise \ncomparator blocks on the left of the implementations above.\n\n\n\n\n\n \n Question: What is the difference between the original and the alternate representation? \n Answer: \n\nA. The difference between the original representation and the alternate representation is that in the alternate representation, C_0=1 whenever A=B."}, {"text": "Title: Logic Operations \n Text: {Logic Operations}\n\n \n Boolean logic functions, truth tables, etc.\n \n overflow expression using logic functions (and difficulty with\n    binary overflow!)\n \n completeness\n \n implications of completeness: enable abstraction, and evaluate\n     future possible implementations of devices quickly: logically complete\n     or not?\n \n examples\n \n generalizing to operations on sets of bits\n \n\nThis set of notes briefly describes a generalization to truth tables,\nthen introduces Boolean logic operations as well as \nnotational conventions and tools that we use to express general\nfunctions on bits.  We illustrate how logic operations enable \nus to express functions such as overflow conditions concisely,\nthen show by construction that a small number of logic operations\nsuffices to describe any operation on any number of bits.  \nWe close by discussing a few implications and examples.\n\n\n\n \n Question: What is the significance of completeness? \n Answer: "}, {"text": "Title: Truth Tables \n Text: {Truth Tables}\n\nYou have seen the basic form of truth tables in the textbook and in class.\nOver the semester, we will introduce several\nextensions to the basic concept, mostly with the\ngoal of reducing the amount of writing necessary when using truth\ntables.  For example, the truth table to the right uses two generalizations\nto show the carry out C (also the unsigned overflow indicator) and the \nsum S produced by\nadding two {2-bit} unsigned numbers.\nFirst, rather than writing each input bit separately, we have grouped\npairs of input bits into the numbers A and B.  \nSecond, we have defined multiple \noutput columns so as to include both bits of S as well as C in \nthe same table.\nFinally, we have \ngrouped the two bits of S into one column.\n\nKeep in mind as you write truth tables that only rarely does an operation\ncorrespond to a simple and familiar process such as addition of base 2\nnumbers.  We had to choose the unsigned and 2's complement representations\ncarefully to allow ourselves to take advantage of a familiar process.\nIn general, for each line of a truth table for an operation, you may\nneed to make use of the input representation to identify the input values,\ncalculate the operation's result as a value, and then translate the value\nback into the correct bit pattern using the output representation.\nSigned magnitude addition, for example, does not always correspond to\nbase 2 addition: when the\n\n\n{cc|cc}\n{c|}& \nA& B& C& S \n00& 00& 0& 00\n00& 01& 0& 01\n00& 10& 0& 10\n00& 11& 0& 11\n01& 00& 0& 01\n01& 01& 0& 10\n01& 10& 0& 11\n01& 11& 1& 00\n10& 00& 0& 10\n10& 01& 0& 11\n10& 10& 1& 00\n10& 11& 1& 01\n11& 00& 0& 11\n11& 01& 1& 00\n11& 10& 1& 01\n11& 11& 1& 10\n\n\n\nsigns of the two input operands differ, \none should instead use base 2 subtraction.  For other operations or\nrepresentations, base 2 arithmetic may have no relevance at all.\n\n\n \n Question: What is the purpose of a truth table? \n Answer: \n\nA. The purpose of a truth table is to show the carry out C and the sum S produced by adding two unsigned 2-bit numbers."}, {"text": "Title: Boolean Logic Operations \n Text: {Boolean Logic Operations}\n\nIn the middle of the 19^ century, George Boole introduced a \nset of logic operations that are today known as { Boolean logic}\n(also as { Boolean algebra}).  These operations today form one of\nthe lowest abstraction levels in digital systems, and an understanding\nof their meaning and use is critical to the effective development of \nboth hardware and software.\n\nYou have probably seen these functions many times already in your \neducation---perhaps first in set-theoretic form as Venn diagrams.  \nHowever, given the use of common\nEnglish words { with different meanings} to name some of the functions,\nand the sometimes confusing associations made even by engineering\neducators, we want to provide you with a concise set of\ndefinitions that generalizes correctly to more than two operands.\nYou may have learned these functions based on truth values \n(true and false), but we define them based on bits, \nwith 1 representing true and 0 representing false.\n\nTable  on the next page lists logic operations.\n\nThe first column in the table lists the name of each function.  The second\nprovides a fairly complete set of the notations that you are likely\nto encounter for each function, including both the forms used in\nengineering and those used in mathematics.  The third column defines \nthe function's\nvalue for two or more input operands (except for NOT, which operates on\na single value).  The last column shows the form generally used in\nlogic schematics/diagrams and mentions the important features used\nin distinguishing each function (in pictorial form usually called\na { gate}, in reference to common physical implementations) from the \nothers.\n\n\n{\n{|c|c|c|c|}\n{ Function}& { Notation}& { Explanation}& { Schematic} \nAND& {A AND B}{AB}{A}{A}{A}& {the ``all'' function: result is 1 iff}{{ all} input operands are equal to 1}& {flat input, round output}  \nOR& {A OR B}{A+B}{A}& {the ``any'' function: result is 1 iff}{{ any} input operand is equal to 1}& {{-6pt}}{round input, pointed output} \nNOT& {NOT A}{A'}{}{}& {logical complement/negation:}{NOT 0 is 1, and NOT 1 is 0}& {triangle and circle} \n{exclusive OR}& {A XOR B}{A}& {the ``odd'' function: result is 1 iff an { odd}}{number of input operands are equal to 1}& {{-6pt}}{OR with two lines}{on input side} \n{``or''}& A, B, or C& {the ``one of'' function: result is 1 iff exactly}{{ one of} the input operands is equal to 1}& (not used) \n\n}\n{Boolean logic operations, notation, definitions, and symbols.}{-12pt}\n\n\n\n\n\nThe first function of importance is { AND}.  Think of { AND} as the\n``all'' function: given a set of input values as operands, AND evaluates \nto 1 if and only if { all} of the input values are 1.  The first\nnotation line simply uses the name of the function.  In Boolean algebra,\nAND is typically represented as multiplication, and the middle three\nforms reflect various ways in which we write multiplication.  The last\nnotational variant is from mathematics, where the AND function is formally\ncalled { conjunction}.\n\nThe next function of importance is { OR}.  Think of { OR} as the\n``any'' function: given a set of input values as operands, OR evaluates\nto 1 if and only if { any} of the input values is 1.  The actual\nnumber of input values equal to 1 only matters in the sense of whether\nit is at least one.  The notation for OR is organized in the same way\nas for AND, with the function name at the top, the algebraic variant that\nwe will use in class---in this case addition---in the middle, and\nthe mathematics variant, in this case called { disjunction}, at the\nbottom.\n\n{ The definition of Boolean OR is not the same as our use of \nthe word ``or'' \nin English.}  For example, if you are fortunate enough to enjoy\na meal on a plane, you might be offered several choices: ``Would you like\nthe chicken, the beef, or the vegetarian lasagna today?''  Unacceptable\nanswers to this English question include: ``Yes,'' ``Chicken and lasagna,''\nand any other combination that involves more than a single choice!\n\nYou may have noticed that we might have instead mentioned that\nAND evaluates to 0 if any input value is 0, and that OR evaluates to 0\nif all input values are 0.  These relationships reflect a mathematical\nduality underlying Boolean logic that has important practical\nvalue in terms of making it easier for humans to digest complex logic\nexpressions.\nWe will talk more about duality later in the course, but you\nshould learn some of the practical\nvalue now: if you are trying to evaluate an AND function, look for an input\nwith value 0; if you are trying to evaluate an OR function, look for an \ninput with value 1.  If you find such an input, you know the function's\nvalue without calculating any other input values.\n\nWe next consider the { logical complement} function, { NOT}.  The NOT\nfunction is also called { negation}.  Unlike our\nfirst two functions, NOT accepts only a single operand, and reverses\nits value, turning 0 into 1 and 1 into 0.  The notation follows the\nsame pattern: a version using the function name at the top, followed\nby two variants used in Boolean algebra, and finally the version\nfrequently used in mathematics.  For the NOT gate, or { inverter},\nthe circle is actually the important part: the triangle by itself\nmerely copies the input.  You will see the small circle added to other\ngates on both inputs and outputs; in both cases the circle implies a NOT\nfunction.\n\n\n\nLast among the Boolean logic functions, we have the\n{ XOR}, or { exclusive OR} function.\nThink of XOR as the ``odd'' function: given a set of input values as\noperands, XOR evaluates to 1 if and only if { an odd number} of\nthe input values are 1.  Only two variants of XOR notation are given:\nthe first using the function name, and the second used with Boolean\nalgebra.  Mathematics rarely uses this function.\n\nFinally, we have included the meaning of the word ``or'' in English as\na separate function entry to enable you to compare that meaning\nwith the Boolean logic functions easily.  Note that many people refer\nto English'\n\n\nuse of the word ``or'' as ``exclusive'' because one\ntrue value excludes all others from being true.  Do not let this \nhuman language ambiguity confuse you about XOR!  For all logic design\npurposes, { XOR is the odd function}.\n\nThe truth table to the right provides values illustrating these functions operating\non three inputs.  The AND, OR, and XOR functions are all \nassociative---(A  B)  C = A  (B  C)---and\ncommutative---A  B = B  A,\nas you may have already realized from their definitions.\n\n\n{ccc|cccc}\n{c|}& \nA& B& C& ABC& A+B+C& & A \n0& 0& 0& 0& 0& 1& 0\n0& 0& 1& 0& 1& 1& 1\n0& 1& 0& 0& 1& 1& 1\n0& 1& 1& 0& 1& 1& 0\n1& 0& 0& 0& 1& 0& 1\n1& 0& 1& 0& 1& 0& 0\n1& 1& 0& 0& 1& 0& 0\n1& 1& 1& 1& 1& 0& 1\n\n\n\n\n\n \n Question: What is the meaning of the word 'or' in English? \n Answer: \n\nThe OR function evaluates to 1 if any of the input values is 1."}, {"text": "Title: Overflow as Logic Expressions \n Text: {Overflow as Logic Expressions}\n\nIn the last set of notes, we discussed overflow conditions for unsigned\nand 2's complement representations.  Let's use Boolean logic to express\nthese conditions.  \n\nWe begin with addition of two {1-bit} unsigned numbers.  \nCall the two input bits A_0 and B_0.  If you write a truth table for\nthis operation, you'll notice that overflow occurs only when all (two)\nbits are 1.  If either bit is 0, the sum can't exceed 1, so overflow\ncannot occur.  In other words, overflow in this case can be written using\nan AND operation: \n\n{eqnarray*}\nA_0B_0\n{eqnarray*}\n\nThe truth table for adding two {2-bit} unsigned numbers is four\ntimes as large, and seeing the structure may be difficult.  One way of\nwriting the expression for overflow of {2-bit} unsigned addition is\nas follows:\n\n{eqnarray*}\nA_1B_1 + (A_1+B_1)A_0B_0\n{eqnarray*}\n\nThis expression is slightly trickier to understand.  Think about the\nplace value of the bits.  If both of the most significant bits---those\nwith place value 2---are 1,\nwe have an overflow, just as in the case of {1-bit} addition.\nThe A_1B_1 term represents this case.  We also have an overflow if\none or both (the OR) of the most significant bits are 1 and the\nsum of the two next significant bits---in this case those with place \nvalue 1---generates a carry.\n\nThe truth table for adding two {3-bit} unsigned numbers is \nprobably not something that you want to write out.  Fortunately,\na pattern should start to become clear with the following expression:\n\n{eqnarray*}\nA_2B_2 + (A_2+B_2)A_1B_1 + (A_2+B_2)(A_1+B_1)A_0B_0\n{eqnarray*}\n\nIn the {2-bit} case, we mentioned the ``most significant bit''\nand the ``next most significant bit'' to help you see the pattern.\nThe same reasoning describes the first two product terms in our\noverflow expression for {3-bit} unsigned addition (but the place\nvalues are 4 for the most significant bit and 2 for the next most\nsignificant bit).  The last term represents the overflow case in which\nthe two least significant bits generate a carry which then propagates\nup through all of the other bits because at least one of the two bits\nin every position is a 1.\n\n\n\n\nThe overflow condition for addition of two {N-bit} 2's complement\nnumbers can be written fairly concisely in terms of the first bits\nof the two numbers and the first bit of the sum.  Recall that overflow\nin this case depends only on whether the three numbers are negative\nor non-negative, which is given by\n\n\n{ 0pt\n 0pt\n\n&A_{N-1}&A_{N-2}&&A_2&A_1&A_0\n+   &B_{N-1}&B_{N-2}&&B_2&B_1&B_0 \n&S_{N-1}&S_{N-2}&&S_2&S_1&S_0\n\n\n}\n\n\nthe most significant bit.  Given\nthe bit names as shown to the right, we can write the overflow condition\nas follows:\n\n{eqnarray*}\nA_{N-1} B_{N-1} {S_{N-1}}+\n{A_{N-1}} {B_{N-1}} S_{N-1}\n{eqnarray*}\n\nThe overflow condition does of course depend on all of the bits in\nthe two numbers being added.  In the expression above, we have simplified\nthe form by using S_{N-1}.  But S_{N-1} depends on the bits A_{N-1}\nand B_{N-1} as well as the carry out of bit (N-2).  \n\nLater in this set of notes, we present a technique with which you can derive\nan expression for an arbitrary Boolean logic function.  As an exercise,\nafter you have finished reading these notes, try using that technique to\nderive an overflow expression for addition of \ntwo {N-bit} 2's complement numbers based on \nA_{N-1}, B_{N-1}, and the carry out of bit (N-2) (and into \nbit (N-1)), which we might\ncall C_{N-1}.  You might then calculate C_{N-1} in terms of the rest\nof the bits of A and B\nusing the expressions for\nunsigned overflow just discussed.\n\nIn the next month or so, you will learn how to derive\nmore compact expressions yourself from truth tables or other representations\nof Boolean logic functions.\n\n\n \n Question: What is the overflow condition for adding two <unk>1-bit<unk> unsigned numbers? \n Answer: \n\nA. A_2B_2 + (A_2+B_2)A_1B_1 + (A_2+B_2)(A_1+B_1)A_0B_0"}, {"text": "Title: Logical Completeness \n Text: {Logical Completeness}\n\nWhy do we feel that such a short list of functions is enough?  If you \nthink about the\nnumber of possible functions on N bits, you might think that we need\nmany more functions to be able to manipulate bits.  With 10 bits, for\nexample, there are 2^ such functions.  Obviously, some of them\nhave never been used in any computer system, but maybe we should define\nat least a few more logic operations?\nIn fact, we do not\neven need XOR.  The functions AND, OR, and NOT are sufficient, even if\nwe only allow two input operands for AND and OR!\n\nThe theorem below captures this idea, called { logical completeness}.\nIn this case, we claim that the set of functions {AND, OR, NOT} is\nsufficient to express any operation on any finite number of variables, \nwhere each variable is a bit.\n\n{ Theorem:} \n\nGiven enough {2-input} AND, {2-input} OR, and {1-input}\nNOT functions, one can express any Boolean logic function on any finite \nnumber of variables.\n\n { Proof:} \n\nThe proof of our theorem is { by construction}.  In other words, \nwe show a systematic\napproach for transforming an arbitrary Boolean logic function on an\narbitrary number of variables into a form that uses only AND, OR, and\nNOT functions on one or two operands.\n\nAs a first step, we remove the restriction on the number of inputs for\nthe AND and OR functions.  For this purpose, we state and prove two\n{ lemmas}, which are simpler theorems used to support the proof of\na main theorem.\n\n{ Lemma 1:}\n\nGiven enough {2-input} AND functions, one can express an AND function\non any finite number of variables.\n\n{ Proof:}\n\nWe prove the Lemma { by induction}.{We assume that you have\nseen proof by induction previously.}  Denote the number of inputs to a\nparticular AND function by N.\n\n\nThe base case is N=2.  Such an AND function is given.\n\nTo complete the proof, we need only show that, given \nany number of AND functions with up to N inputs, we\ncan express an AND function with N+1 inputs.  To do so, we need\nmerely use one {2-input} AND function to join together\nthe result of an {N-input} AND function with an additional\ninput, as illustrated to the right.\n\n\n\n\n\n{ Lemma 2:}\n\nGiven enough {2-input} OR functions, one can express an OR function\non any finite number of variables.\n\n{ Proof:}\n\nThe proof of Lemma 2 is identical in structure to that of Lemma 1, but\nuses OR functions instead of AND functions.\n\n\nLet's now consider a small subset of functions on N variables.  \nFor any such function, you can write out the truth table for the\nfunction.  The output of a logic function is just a bit, \neither a 0 or a 1.  Let's consider the set of functions on N\nvariables that produce a 1 for exactly one combination of the N\nvariables.  In other words, if you were to write out the truth\ntable for such a function, exactly one row in the truth table would\nhave output value 1, while all other rows had output value 0.\n\n{ Lemma 3:}\n\nGiven enough AND functions and {1-input}\nNOT functions, one can express any Boolean logic function \nthat produces a 1 for exactly one combination of\nany finite number of variables.\n\n{ Proof:}\n\nThe proof of Lemma 3 is by construction.\nLet N be the number of variables on which the function operates.\nWe construct a { minterm} on these N variables,\nwhich is an AND operation on each variable or its complement.\nThe minterm is specified by looking at the unique combination of \nvariable values that produces a 1 result for the function.\nEach variable that must be a 1 is included as itself, while\neach variable that must be a 0 is included as the variable's complement\n(using a NOT function).  The resulting minterm produces the\ndesired function exactly.  When the variables all match\nthe values for which the function should produce 1, the\ninputs to the AND function are all 1, and the function produces 1.\nWhen any variable does not match the value for which the function\nshould produce 1, that variable (or its complement) acts as a 0\ninput to the AND function, and the function produces a 0, as desired.\n\nThe table below shows all eight minterms for three variables.\n\n{\n{ccc|cccccccc}\n{c|}& \nA& B& C& \n  &\n  C&\n B &\n B C&\nA  &\nA  C&\nA B &\nA B C\n \n0& 0& 0& 1& 0& 0& 0& 0& 0& 0& 0\n0& 0& 1& 0& 1& 0& 0& 0& 0& 0& 0\n0& 1& 0& 0& 0& 1& 0& 0& 0& 0& 0\n0& 1& 1& 0& 0& 0& 1& 0& 0& 0& 0\n1& 0& 0& 0& 0& 0& 0& 1& 0& 0& 0\n1& 0& 1& 0& 0& 0& 0& 0& 1& 0& 0\n1& 1& 0& 0& 0& 0& 0& 0& 0& 1& 0\n1& 1& 1& 0& 0& 0& 0& 0& 0& 0& 1\n\n}\n\nWe are now ready to prove our theorem.\n\n{ Proof (of Theorem):}\n\nAny given function on N variables\nproduces the value 1 for some set of combinations\nof inputs.  Let's say that M such combinations produce 1.  \nNote that M{2^N}.\nFor each\ncombination that produces 1, we can use\nLemma 1 to construct an {N-input} AND function.\nThen, using Lemma 3, we can use as many as M NOT functions\nand the {N-input} AND function to construct a minterm\nfor that input combination.\nFinally, using Lemma 2, we can construct an {M-input}\nOR function and OR together all of the minterms.\nThe result of the OR is the desired function. If the function\nshould produce a 1 for some combination of inputs, that combination's \nminterm provides a 1 input to the OR, which in turn produces a 1.\nIf a combination should produce a 0, its minterm does not appear in\nthe OR; all other minterms produce 0 for that combination, and thus\nall inputs to the OR are 0 in such cases, and the OR produces 0,\nas desired.\n\nThe construction that we used to prove logical completeness does\nnot necessarily help with efficient design of logic functions.  Think\nabout some of the expressions that we discussed earlier in these\nnotes for overflow conditions.  How many minterms do you need \nfor {N-bit} unsigned overflow?  A single Boolean logic function\ncan be expressed in many different ways, and learning how to develop\nan efficient implementation of a function as well as how to determine\nwhether two logic expressions are identical without actually writing out\ntruth tables are important engineering skills that you will start to \nlearn in the coming months.\n\n\n\n \n Question: What is logical completeness? \n Answer: \n\nLogical completeness is the ability to express any Boolean logic function on any finite number of variables using only AND, OR, and NOT functions on one or two operands."}, {"text": "Title: Implications of Logical Completeness \n Text: {Implications of Logical Completeness}\n\nIf logical completeness doesn't really help us to engineer logic functions,\nwhy is the idea important?  Think back to the layers of abstraction\nand the implementation of bits from the first couple of lectures.  \nVoltages are real numbers, not bits.  { The device layer implementations\nof Boolean logic functions must abstract away the analog properties\nof the physical system.}  Without such abstraction, we must think\ncarefully about analog issues such as noise every time we make use\nof a bit!\n\nLogical completeness assures us that no matter what we want to do\nwith bits, implementating a handful of operations correctly is\nenough to guarantee that we never have to worry.\n\nA second important value of logical completeness is as a tool in\nscreening potential new technologies for computers.  If a new technology\ndoes not allow implementation of a logically complete set of functions,\nthe new technology is extremely unlikely\nto be successful in replacing the current one.\n\nThat said, {AND, OR, and NOT} is not the only logically complete set of\nfunctions.  In fact, our current complementary metal-oxide semiconductor\n(CMOS) technology, on which most of the computer industry is now built,\ndoes not directly implement these functions, as you will see later in\nour class.\n\n\n\nThe functions that are implemented directly in CMOS are NAND and NOR, which\nare abbreviations for AND followed by NOT and OR followed by NOT, \nrespectively.  Truth tables for the two are shown to the right.\n\nEither of these functions by itself forms a logically complete set.\nThat is, both the set  and the set  are logically\ncomplete.  For now, we leave the proof of this claim to you.  Re-\n\n\n{cc|cc}\n{c|}& \n& & & {A+B}\nA& B& A NAND B& A NOR B \n0& 0& 1& 1\n0& 1& 1& 0\n1& 0& 1& 0\n1& 1& 0& 0\n\n\n\nmember that all\nyou need to show is that you can implement any set known to be\nlogically complete, so in order to prove that  is \nlogically complete (for example),\nyou need only show that you can implement AND, OR, and NOT using only\nNAND.\n\n \n Question: Why is logical completeness important? \n Answer: \n\nA. The idea of logical completeness is important because it assures us that no matter what we want to do with bits, implementing a handful of operations correctly is enough to guarantee that we never have to worry."}, {"text": "Title: Examples and a Generalization \n Text: {Examples and a Generalization}\n\nLet's use our construction to solve a few examples.  We begin with\nthe functions that we illustrated with the first truth table from this\nset of notes,\nthe carry out C and sum S of two {2-bit}\nunsigned numbers.  Since each output bit requires a separate expression,\nwe now write S_1S_0 for the two bits of the sum.  We also need to\nbe able to make use of the individual bits of the input values, so we \nwrite these as A_1A_0 and B_1B_0, as shown on the left below.\nUsing our \nconstruction from the logical completeness theorem, we obtain the\nequations on the right.\nYou should verify these expressions yourself.\n\n{\n\n{cccc|ccc}\n{c|}& \nA_1& A_0& B_1& B_0& C& S_1& S_0 \n0&0& 0&0& 0& 0&0\n0&0& 0&1& 0& 0&1\n0&0& 1&0& 0& 1&0\n0&0& 1&1& 0& 1&1\n0&1& 0&0& 0& 0&1\n0&1& 0&1& 0& 1&0\n0&1& 1&0& 0& 1&1\n0&1& 1&1& 1& 0&0\n1&0& 0&0& 0& 1&0\n1&0& 0&1& 0& 1&1\n1&0& 1&0& 1& 0&0\n1&0& 1&1& 1& 0&1\n1&1& 0&0& 0& 1&1\n1&1& 0&1& 1& 0&0\n1&1& 1&0& 1& 0&1\n1&1& 1&1& 1& 1&0\n\n\n\n{eqnarray*}\n\nC &=& \n{A_1} A_0 B_1 B_0 +\nA_1 {A_0} B_1 {B_0} +\nA_1 {A_0} B_1 B_0 +\n&& A_1 A_0 {B_1} B_0 +\nA_1 A_0 B_1 {B_0} +\nA_1 A_0 B_1 B_0\n\n\nS_1 &=& \n{A_1} {A_0} B_1 {B_0} +\n{A_1} {A_0} B_1 B_0 +\n{A_1} A_0 {B_1} B_0 +\n&& {A_1} A_0 B_1 {B_0} +\nA_1 {A_0} {B_1} {B_0} +\nA_1 {A_0} {B_1} B_0 +\n&& A_1 A_0 {B_1} {B_0} +\nA_1 A_0 B_1 B_0\n\n\nS_0 &=& \n{A_1} {A_0} {B_1} B_0 +\n{A_1} {A_0} B_1 B_0 +\n{A_1} A_0 {B_1} {B_0} +\n&& {A_1} A_0 B_1 {B_0} +\nA_1 {A_0} {B_1} B_0 +\nA_1 {A_0} B_1 B_0 +\n&& A_1 A_0 {B_1} {B_0} +\nA_1 A_0 B_1 {B_0}\n{eqnarray*}\n\n}\n\n\nNow let's consider a new function.  Given\nan {8-bit} 2's complement number, A=A_7A_6A_5A_4A_3A_2A_1A_0,\nwe want to compare it with the value -1.  We know that\nwe can construct this function using AND, OR, and NOT, but how?\nWe start by writing the representation for -1, which is 11111111.\nIf the number A matches that representation, we want to produce a 1.\nIf the number A differs in any bit, we want to produce a 0.\nThe desired function has exactly one combination of inputs that\nproduces a 1, so in fact we need only one minterm!  In this case, we\ncan compare with -1 by calculating the expression:\n\n{eqnarray*}\nA_7  A_6  A_5  A_4  A_3  A_2  A_1  A_0\n{eqnarray*}\n\nHere we have explicitly included multiplication symbols to avoid\nconfusion with our notation for groups of bits, as we used when \nnaming the individual bits of A.\n\n\nIn closing, we briefly introduce a generalization of logic\noperations to groups of bits.  Our representations for integers,\nreal numbers, and characters from human languages all use more than\none bit to represent a given value.  When we use computers, we often\nmake use of multiple bits in groups in this way.  A { byte}, for example,\ntoday means an ordered group of eight bits.\n\nWe can extend our logic functions to operate on such groups by pairing\nbits from each of two groups and performing the logic operation on each\npair.  For example, given\nA=A_7A_6A_5A_4A_3A_2A_1A_0=01010101\nand\nB=B_7B_6B_5B_4B_3B_2B_1B_0=11110000, we calculate\nA AND B by computing the AND of each pair of bits, \nA_7 AND B_7,\nA_6 AND B_6,\nand so forth, to\nproduce the result 01010000, as shown to the right.\nIn the same way, we can extend other logic\noperations, such as OR, NOT, and XOR, to operate on bits of groups.\n\n\n{ 0pt\n 0pt\n\n&A  & 0&1& 0&1& 0&1& 0&1\n  AND   &B& 1&1&1&1&0&0&0&0 \n&& 0& 1&0&1&0&0&0&0\n\n}\n\n\n\n\n\n\n \n Question: What is the function for adding two 2-bit unsigned numbers? \n Answer: \n\nA. The function for adding two 2-bit unsigned numbers."}, {"text": "Title: The 2's Complement Representation \n Text: {The 2's Complement Representation}\n\nThis set of notes explains the rationale for using the 2's complement\nrepresentation for signed integers and derives the representation \nbased on equivalence of the addition function to that of addition\nusing the unsigned representation with the same number of bits.\n\n \n Question: What is the rationale for using the 2's complement representation? \n Answer: \n\nA. The rationale for using the 2's complement representation is that it allows for the addition function to be equivalent to addition using the unsigned representation."}, {"text": "Title: Review of Bits and the Unsigned Representation \n Text: {Review of Bits and the Unsigned Representation}\n\nIn modern digital systems, we represent all types of information\nusing binary digits, or { bits}.  Logically, a bit is either 0 or 1.\nPhysically, a bit may be a voltage, a magnetic field, or even the\nelectrical resistance of a tiny sliver of glass.\n\nAny type of information can be represented with an ordered set of\nbits, provided that \n{ any given pattern of bits corresponds to only\none value} and that { we agree in advance on which pattern of bits\nrepresents which value}.\n\nFor unsigned integers---that is, whole numbers greater or equal to\nzero---we chose to use the base 2 representation already familiar to\nus from mathematics.  We call this representation the { unsigned\nrepresentation}.  For example, in a {4-bit} unsigned representation,\nwe write the number 0 as 0000, the number 5 as 0101, and the number 12\nas 1100.  Note that we always write the same number of bits for any\npattern in the representation: { in a digital system, there is no \n``blank'' bit value}.\n\n \n Question: What is important about a given pattern of bits? \n Answer: \n\nA. It is important that any given pattern of bits corresponds to only one value because it allows us to unambiguously represent any value using a finite number of bits."}, {"text": "Title: Picking a Good Representation \n Text: {Picking a Good Representation}\n\nIn class, we discussed the question of what makes one representation\nbetter than another.  The value of the unsigned representation, for\nexample, is in part our existing familiarity with the base 2 analogues\nof arithmetic.  For base 2 arithmetic, we can use nearly identical\ntechniques to those that we learned in elementary school for adding, \nsubtracting, multiplying, and dividing base 10 numbers.\n\nReasoning about the relative merits of representations from a\npractical engineering perspective is (probably) currently beyond \nyour ability.\n\nSaving energy, making the implementation simple, and allowing the\nimplementation to execute quickly probably all sound attractive, but \na quantitative comparison between two representations on any of these\nbases requires knowledge that you will acquire in the\nnext few years.\n\nWe can sidestep such questions, however, by realizing that if a\ndigital system has hardware to perform operations such as addition\non unsigned values, using the same piece of hardware to operate\non other representations incurs little or no additional cost.\nIn this set of notes, we discuss the 2's complement representation,\nwhich allows reuse of the unsigned add unit (as well as a basis for\nperforming subtraction of either representation using an add unit!).\nIn discussion section and in your homework, you will \nuse the same idea to perform operations on other representations, such as\nchanging an upper case letter in ASCII to a lower case one, or converting\nfrom an ASCII digit to an unsigned representation of the same number.  \n\n \n Question: What is important to engineers? \n Answer: \n\nA. Reasoning about the relative merits of representations from a practical engineering perspective is important because it allows engineers to optimize digital systems for energy efficiency, simplicity, and speed."}, {"text": "Title: The Unsigned Add Unit \n Text: {The Unsigned Add Unit}\n\nIn order to define a representation for signed integers that allows\nus to reuse a piece of hardware designed for unsigned integers, we\nmust first understand what such a piece of hardware actually does (we\ndo not need to know how it works yet---we'll explore that question \nlater in our class).\n\nThe unsigned representation using {N} bits is not closed\nunder addition.  In other words, for any value of N, we can easily\nfind two {N-bit} unsigned numbers that, when added together,\ncannot be represented as an {N-bit} unsigned number.  With N=4, \nfor example, we can add 12 (1100) and 6 (0110) to obtain 18.\nSince 18 is outside of the range [0,2^4-1] representable using\nthe {4-bit} unsigned representation, our representation breaks\nif we try to represent the sum using this representation.  We call\nthis failure an { overflow} condition: the representation cannot\nrepresent the result of the operation, in this case addition.\n\n\nUsing more bits to represent the answer is not an attractive solution, \nsince we might then want to use more bits for the inputs, which in turn\nrequires more bits for the outputs, and so on.  We cannot build \nsomething supporting an infinite number of bits.  Instead, we \nchoose a value for N and build an add unit that adds two {N-bit}\nnumbers and produces an {N-bit} sum (and some overflow \nindicators, which we discuss in the next set of notes).  The diagram\nto the right shows how we might draw such a device, with two {N-bit}\nnumbers entering at from the top, and the {N-bit} sum coming out\nfrom the bottom.\n\n\n\n\n\n\nThe function used for {N-bit} unsigned addition is addition \nmodulo 2^N.  In a practical sense, you can think of this function\nas simply keeping the last N bits of the answer; other bits \nare simply discarded.  In the example to the right,\nwe add 12 and 6 to obtain 18, but then discard the extra bit on the\nleft, so the add unit produces 2 (an overflow).\n\n\n\n\n\n\n{ Modular arithmetic} defines a way of performing arithmetic for\na finite number of possible values, usually integers.  \nAs a concrete example, let's use modulo 16, which corresponds to\nthe addition unit for our {4-bit} examples.\n\nStarting with the full range of integers, we break the number\nline into contiguous groups of 16 integers, as shown to the right.\n\n\n\n\n\nThe numbers 0 to 15 form one group.  The numbers -16 to -1 form a\nsecond group, and the numbers from 16 to 31 form a third group. \nAn infinite number of groups are defined in this manner.\n\nWe then define 16 { equivalence classes} consisting of the first numbers\nfrom all groups, the second numbers from all groups, and so forth.\nFor example, the numbers , -32, -16, 0, 16, 32,  form\none such equivalence class.\n\nMathematically, we say that two numbers A and B are equivalent modulo 16,\nwhich we write as\n\n{eqnarray*}\n(A &=& B)  16, {or sometimes as}\nA && B {(mod 16)}\n{eqnarray*}\n\nif and only if A=B+16k for some integer k.\n\nEquivalence as defined by a particular modulus\ndistributes over addition and multiplication.  If, for example,\nwe want to find the equivalence class for (A + B)  16,\nwe can find the equivalence classes for A (call it C) and B \n(call it D) and then calculate the equivalence class \nof (C + D)  16.\nAs a concrete example of distribution over multiplication, \ngiven (A = 1,083,102,112  7,323,127)  10,\nfind A.\n\nFor this problem, we note that the first number is equivalent \nto 2  10, while the second number is equivalent \nto 7  10.  We then write (A = 2  7)  10,\nand, since 2  7 = 14, we have (A = 4)  10.\n\n\n \n Question: What is the name of the function that allows us to reuse a piece of hardware designed for unsigned integers? \n Answer: "}, {"text": "Title: Deriving 2's Complement \n Text: {Deriving 2's Complement}\n\n\nGiven these equivalence classes, we might instead choose to draw a circle\nto identify the equivalence classes and to associate each class with one\nof the sixteen possible {4-bit} patterns, as shown to the right.\nUsing this circle representation, we can add by counting clockwise around\nthe circle, and we can subtract by counting in a counterclockwise direction\naround the circle.  With an unsigned representation, we choose to use the\ngroup from [0,15] (the middle group in the diagram markings to the right)\nas the number represented by each of the patterns.  Overflow occurs\nwith unsigned addition (or subtraction) because we can only choose one\nvalue for each binary pattern.\n\n\n\n\n\nIn fact, we can choose any single value for each pattern to create a \nrepresentation, and our add unit will always produce results that\nare correct modulo 16.  Look back at our overflow example, where\nwe added 12 and 6 to obtain 2, and notice that (2=18)  16.\nNormally, only a contiguous sequence of integers makes a useful\nrepresentation, but we do not have to restrict ourselves to \nnon-negative numbers.\n\nThe 2's complement representation can then be defined by choosing a \nset of integers balanced around zero from the groups.  In the circle \ndiagram, for example, we might choose to represent numbers\nin the range [-7,7] when using 4 bits.  What about the last pattern, 1000?\nWe could choose to represent either -8 or 8.  The number of arithmetic\noperations that overflow is the same with both choices (the choices\nare symmetric around 0, as are the combinations of input operands that \noverflow), so we gain nothing in that sense from either choice.\nIf we choose to represent -8, however, notice that all patterns starting\nwith a 1 bit then represent negative numbers.  No such simple check\narises with the opposite choice, and thus an {N-bit} 2's complement \nrepresentation is defined to represent the range [-2^{N-1},2^{N-1}-1],\nwith patterns chosen as shown in the circle.\n\n \n Question: What is the effect of using -8 instead of 8? \n Answer: \n\nA. The effect of choosing to represent -8 instead of 8 in a 4-bit 2's complement system is that all patterns starting with a 1 bit then represent negative numbers."}, {"text": "Title: An Algebraic Approach \n Text: {An Algebraic Approach}\n\nSome people prefer an algebraic approach to understanding the\ndefinition of 2's complement, so we present such an approach next.\nLet's start by writing f(A,B) for the result of our add unit:\n\n{eqnarray*}\nf(A,B) = (A + B)  2^N\n{eqnarray*}\n\nWe assume that we want to represent a set of integers balanced around 0\nusing our signed representation, and that we will use the same binary\npatterns as we do with an unsigned representation to represent\nnon-negative numbers.  Thus, with an {N-bit} representation,\nthe patterns in the range [0,2^{N-1}-1] are the same as those\nused with an unsigned representation.  In this case, we are left with\nall patterns beginning with a 1 bit.\n\nThe question then is this: given an integer k, 2^{N-1}>k>0, for which we \nwant to find a pattern to represent -k, and any integer m\nthat we might want to add to -k, \ncan we find another integer p>0\nsuch that \n\n\n(-k + m = p + m)  2^N   ?\n\n\nIf we can, we can use p's representation to represent -k and our\nunsigned addition unit f(A,B) will work correctly.\n\nTo find the value p, start by subtracting m from both sides of\nEquation () to obtain:\n\n\n(-k = p)  2^N\n\n\nNote that (2^N=0)  2^N, and add this equation to \nEquation () to obtain\n\n{eqnarray*}\n(2^N-k = p)  2^N\n{eqnarray*}\n\nLet p=2^N-k.  \n\nFor example, if N=4, k=3 gives p=16-3=13, which is the pattern 1101.\nWith N=4 and k=5, we obtain p=16-5=11, which is the pattern 1011.\nIn general, since\n2^{N-1}>k>0, \nwe have 2^{N-1}<p<2^N.  But these patterns are all unused---they all\nstart with a 1 bit!---so the patterns that we have defined for negative\nnumbers are disjoint from those that we used for positive numbers, and\nthe meaning of each pattern is unambiguous.\n\nThe algebraic definition of bit patterns for negative numbers\nalso matches our circle diagram from the last\nsection exactly, of course.\n\n\n\n \n Question: What is the meaning of each pattern? \n Answer: \n\nA. The meaning of each pattern is unambiguous."}, {"text": "Title: Negating 2's Complement Numbers \n Text: {Negating 2's Complement Numbers}\n\nThe algebraic approach makes understanding negation of an integer\nrepresented using 2's complement fairly straightforward, and gives \nus an easy procedure for doing so.\nRecall that given an integer k in an {N-bit} 2's complement\nrepresentation, the {N-bit} pattern for -k is given by 2^N-k \n(also true for k=0 if we keep only the low N bits of the result).  \nBut 2^N=(2^N-1)+1.  Note that 2^N-1 is the pattern of\nall 1 bits.  Subtracting any value k from this value is equivalent\nto simply flipping the bits, changing 0s to 1s and 1s to 0s.\n(This operation is called a { 1's complement}, by the way.)\nWe then add 1 to the result to find the pattern for -k.\n\nNegation can overflow, of course.  Try finding the negative pattern for -8 \nin {4-bit} 2's complement.\n\nFinally, be aware that people often overload the term 2's complement\nand use it to refer to the operation of negation in a 2's complement\nrepresentation.  In our class, we try avoid this confusion: 2's complement\nis a representation for signed integers, and negation is an operation\nthat one can apply to a signed integer (whether the representation used\nfor the integer is 2's complement or some other representation for signed\nintegers).\n\n\n\n\n\n \n Question: What is the algebraic approach to negation of an integer? \n Answer: \n\nA. The algebraic approach makes understanding negation of an integer in a 2's complement representation straightforward because it is simply a matter of subtracting the value from 2^N-1 and then adding 1."}, {"text": "Title: The Halting Problem \n Text: {The Halting Problem}\n\nFor some of the topics in this course, we plan to cover the material\nmore deeply than does the textbook.  We will provide notes in this\nformat to supplement the textbook for this purpose.\n\nIn order to make these notes more useful as a reference, definitions are\nhighlighted with boldface, and italicization emphasizes pitfalls or other\nimportant points.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}\n\nThese notes are broken up into four parts, corresponding to the three\nmidterm exams and the final exam.  Each part is covered by\none examination in our class.  { The last section of each of the four\nparts gives you\na summary of material that you are expected to know for the corresponding\nexam.}  Feel\nfree to read it in advance.\n\nAs discussed in the textbook and in class, a { universal\ncomputational device} (or { computing machine}) is a device \nthat is capable of computing the\nsolution to any problem that can be computed, provided that the device\nis given enough storage and time for the computation to finish.  \n\nOne might ask whether we can describe problems that we cannot answer (other \nthan philosophical ones, such as the meaning of life).\n\nThe answer is yes: there are problems that are provably { undecidable},\nfor which no amount of computation can solve the problem in general.\nThis set of notes describes the first problem known to be\nundecidable, the { halting problem}.  For our class, you need only recognize\nthe name and realize that one can, in fact, give examples of problems\nthat cannot be solved by computation.  In the future, you should be able\nto recognize this type of problem so as to avoid spending your time\ntrying to solve it.\n\n \n Question: What is the first problem that is undecidable? \n Answer: \n\nA. The first problem known to be undecidable is the halting problem."}, {"text": "Title: Universal Computing Machines* \n Text: {Universal Computing Machines*}\n\nThe things that we call computers today, whether we are talking about\na programmable microcontroller in a microwave oven or the Blue Waters\nsupercomputer sitting on the south end of our campus (the United\nStates' main resource to support computational science research), are\nall equivalent in the sense of what problems they can solve.  \n\nThese machines do, of course, have access to different amounts of memory, \nand compute at different speeds.  \n\nThe idea that a single model of computation could be described and proven\nto be equivalent to all other models came out of a 1936 paper by Alan Turing, \nand today we generally refer to these devices as { Turing machines}.\n\nAll computers mentioned earlier, as well as all computers with which you \nare familiar in your daily life, are provably equivalent to Turing machines.\n\nTuring also conjectured that his definition of computable was\nidentical to the ``natural'' definition (today, this claim is known\nas the { Church-Turing conjecture}).  In other words, a problem that\ncannot be solved by a Turing machine cannot be solved in any\nsystematic manner, with any machine, or by any person.  This conjecture\nremains unproven!  However, neither has anyone been able to disprove\nthe conjecture, and it is widely believed to be true.  Disproving the\nconjecture requires that one demonstrate a systematic technique (or a\nmachine) capable of solving a problem that cannot be solved by a\nTuring machine.  No one has been able to do so to date.\n\n\n \n Question: What is the significance of the Church-Turing conjecture? \n Answer: \n\nA. The significance of the Church-Turing conjecture is that it is a statement that has yet to be proven or disproven. If it were to be proven, it would mean that there is a systematic technique or machine that can solve a problem that cannot be solved by a Turing machine. As of now, no one has been able to do so."}, {"text": "Title: The Halting Problem* \n Text: {The Halting Problem*}\n\nYou might reasonably ask whether any problems can be shown to\nbe incomputable.  More common terms for such problems---those known\nto be insolvable by any computer---are { intractable} or \nundecidable.\n\nIn the same 1936 paper in which he introduced the universal computing\nmachine, Alan Turing also provided an answer to this question\nby introducing (and proving) that there are in fact problems that cannot be\ncomputed by a universal computing machine.\nThe problem that\nhe proved undecidable, using proof techniques almost identical to those\ndeveloped for similar problems in the 1880s, is now known as { the\nhalting problem}.\n\n\n\nThe halting problem is easy to state and easy to prove undecidable.\nThe problem is this: given a Turing machine and an input to the Turing\nmachine, does the Turing machine finish computing in a finite number\nof steps (a finite amount of time)?  In order to solve the problem, an\nanswer, either yes or no, must be given in a finite amount of time\nregardless of the machine or input in question.  Clearly some machines\nnever finish.  For example, we can write a Turing machine that counts\nupwards starting from one.\n\nYou may find the proof structure for undecidability of the halting problem\neasier to understand if\nyou first think about a related problem with which you may\nalready be familiar, the Liar's paradox\n(which is at least 2,300 years old).  In its stengthened form, it is\nthe following sentence: ``This sentence is not true.''\n\n\nTo see that no Turing machine can solve the halting problem, we begin\nby assuming that such a machine exists, and then show that its\nexistence is self-contradictory.  We call the machine the ``Halting\nMachine,'' or HM for short.  HM is a machine that operates on \nanother\n\n\n\n\n\nTuring machine and its inputs to produce a yes or no answer in finite time:\neither the machine in question finishes in finite time (HM returns\n``yes''), or it does not (HM returns ``no'').  The figure illustrates\nHM's operation.\n\n\nFrom HM, we construct a second machine that we call the HM Inverter,\nor HMI.  This machine inverts the sense of the answer given by HM.  In\nparticular, the inputs are fed directly into a copy of HM, and if HM\nanswers ``yes,'' HMI enters an infinite loop.  If HM answers ``no,'' HMI\nhalts.  A diagram appears to the right.\n\nThe inconsistency can now be seen by asking HM whether HMI halts when\ngiven itself as an input (repeatedly), as\n\n\n\n\n\nshown below.  Two\ncopies of HM are thus\nbeing asked the same question.  One copy is the rightmost in the figure below\nand the second is embedded in the HMI machine that we are using as the\ninput to the rightmost HM.  As the two copies of HM operate on the same input\n(HMI operating on HMI), they should return the same answer: a Turing\nmachine either halts on an input, or it does not; they are\ndeterministic.\n\n\n\nLet's assume that the rightmost HM tells us that HMI operating on itself halts.\nThen the copy of HM in HMI (when HMI executes on itself, with itself\nas an input) must also say ``yes.''  But this answer implies that HMI\ndoesn't halt (see the figure above), so the answer should have been\nno!\n\nAlternatively, we can assume that the rightmost HM says that HMI operating on itself\ndoes not halt.  Again, the copy of HM in HMI must give the same\nanswer.  But in this case HMI halts, again contradicting our\nassumption.\n\nSince neither answer is consistent, no consistent answer can be given,\nand the original assumption that HM exists is incorrect.  Thus, no\nTuring machine can solve the halting problem.\n\n\n\n\n\n \n Question: What is the halting problem? \n Answer: \n\nA. That some problems are incomputable, or insolvable by any computer."}, {"text": "Title: Programming Concepts and the C Language \n Text: {Programming Concepts and the C Language}\n\nThis set of notes introduces the C programming language and explains\nsome basic concepts in computer programming.  Our purpose in showing\nyou a high-level language at this early stage of the course is to give\nyou time to become familiar with the syntax and meaning of the language,\nnot to teach you how to program.  Throughout this semester, we will\nuse software written in C to demonstrate and validate the digital system design\nmaterial in our course.  Towards the end of the semester, you will\nlearn to program computers using instructions and assembly language.\nIn ECE 220, you will make use of the C language to write\nprograms, at which point already being familiar with the language will\nmake the material easier to master.  \n\nThese notes are meant to complement the\nintroduction provided by Patt and Patel.\n\nAfter a brief introduction to the history of C and the structure of\na program written in C, we connect the idea of representations developed \nin class to the data types used in high-level languages.\n\nWe next discuss the use of variables in C, then describe some of the \noperators available to the programmer, including arithmetic and logic\noperators.  The notes next introduce C functions that support the ability to\nread user input from the keyboard and to print results to the monitor.\n\nA description of the structure of statements in C follows, explaining\nhow programs are executed and how a programmer can create statements\nfor conditional execution as well as loops to perform repetitive tasks.\n\nThe main portion of the notes concludes with \nan example program, which is used to illustrate both the execution of \nC statements as well as the difference between variables in programs\nand variables in algebra.\n\nThe remainder of the notes covers more advanced topics.  First, we \ndescribe how the compilation process works,\nillustrating how a program written in a high-level language is\ntransformed into instructions.  You will learn this process in much\nmore detail in ECE 220.  \n\nSecond, we briefly introduce the C preprocessor.\n\nFinally, we discuss implicit and explicit data type conversion in C.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}\n\n\n\n \n Question: What is the purpose of introducing the C programming language in this course? \n Answer: \n\nA. The purpose of introducing the C programming language in this course is to give students time to become familiar with the syntax and meaning of the language, not to teach them how to program."}, {"text": "Title: The C Programming Language \n Text: {The C Programming Language}\n\nProgramming languages attempt to bridge the semantic gap between human\ndescriptions of problems and the relatively simple instructions that\ncan be provided by an instruction set architecture (ISA).  \nSince 1954, when the Fortran language\nfirst enabled scientists to enter FORmulae symbolically and to have\nthem TRANslated automatically into instructions, people have invented\nthousands of computer languages.  \n\n\nThe C programming language was developed by Dennis Ritchie at Bell Labs\nin order to simplify the task of writing the Unix operating system.\nThe C language provides a fairly transparent mapping to typical ISAs,\nwhich makes it a good choice both for system software such as operating\nsystems and for our class.\n\nThe { syntax} used in C---that is, the rules that one must follow\nto write valid C programs---has also heavily influenced many other\nmore recent languages, such as C++, Java, and Perl.  \n\nFor our purposes, a C program consists of a set of { variable \ndeclarations} and a sequence of { statements}.  \n\n\n{\n\naaaa=/* =\nint\nmain ()\n{\n>  int answer = 42;        /* the Answer! */\n\n>  printf (\"The answer is d.n\", answer);\n\n>  /*> Our work here is done.\n>    > Let's get out of here! */\n>  return 0;\n}\n\n}\n\n\nBoth of these parts are written into a single C function called { main},\nwhich executes when the program starts.  \n\nA simple example appears to the right.  The program uses one variable\ncalled { answer}, which it initializes to the value 42.\nThe program prints a line of output to the monitor for the user,\nthen terminates using the { return} statement.  { Comments} for human\nreaders begin with the characters { /*} (a slash followed by an \nasterisk) and end with the characters { */} (an asterisk followed \nby a slash).\n\nThe C language ignores white space in programs, so we encourage\nyou to use blank lines and extra spacing to make your programs\neasier to read.\n\nThe variables defined in the { main} function allow a programmer\nto associate arbitrary { symbolic names} (sequences of English characters, \nsuch as ``sum'' or ``product'' or ``highScore'') with specific\ntypes of data, such as a {16-bit} unsigned integer or a\ndouble-precision floating-point number. \n\nIn the example program above, the variable { answer} is declared\nto be a {32-bit} {2's} complement number.\n\nThose with no programming experience may at first find the difference\nbetween variables in algebra and variables in programs slightly \nconfusing.  { As a program executes, the values of variables can \nchange from step to step of execution.}\n\nThe statements in the { main} function are executed one by one\nuntil the program terminates.  \n\nPrograms are not limited to simple sequences of statements, however.\nSome types of statements allow a programmer\nto specify conditional behavior.  For example, a program might only\nprint out secret information if the user's name is ``lUmeTTa.''\nOther types of statements allow a programmer to repeat the execution\nof a group of statements until a condition is met.  For example, a program\nmight print the numbers from 1 to 10, or ask for input until the user\ntypes a number between 1 and 10.\n\nThe order of statement execution is well-defined in C, but the\nstatements in { main} do not necessarily make up an algorithm:\n{ we can easily write a C program that never terminates}.\n\nIf a program terminates, the { main} function\nreturns an integer to the operating system, usually by executing\na { return} statement, as in the example program.\n\nBy convention, returning the value 0 indicates successful completion\nof the program, while any non-zero value indicates a program-specific\nerror.\n\nHowever, { main} is not necessarily a function in the mathematical \nsense because { the value returned from { main} is not \nnecessarily unique for a given set of input values to the program}.  \n\nFor example, we can write a program that selects a number from 1 to 10 \nat random and returns the number to the operating system.\n\n\n\n\n \n Question: What is the difference between variables in algebra and variables in programs? \n Answer: \n\nA. In algebra, variables are unchanging values that are used to represent other values in equations. In programs, variables can change from step to step of execution, and are used to store data that can be used by the program."}, {"text": "Title: Data Types \n Text: {Data Types}\n\nAs you know, modern digital computers represent all information with\nbinary digits (0s and 1s), or { bits}.  Whether you are representing \nsomething as simple as an integer or as complex as an undergraduate \nthesis, the data are simply a bunch of 0s and 1s inside a computer.  \n\nFor any given type of information, a human selects a data type for the\ninformation.  A { data type} (often called just a { type})\nconsists of both a size in bits and a representation, such as the\n2's complement representation for signed integers, or the ASCII\nrepresentation for English text.  A { representation} is a way of\nencoding the things being represented as a set of bits, with each bit\npattern corresponding to a unique object or thing.\n\nA typical ISA supports a handful of\ndata types in hardware in the sense that it provides hardware \nsupport for operations on those data types.\n\nThe arithmetic logic units (ALUs) in most modern processors,\nfor example, support addition\nand subtraction of both unsigned and 2's complement representations, with\nthe specific data type (such as 16- or 64-bit 2's complement)\ndepending on the ISA.\n\nData types and operations not supported by the ISA must be handled in\nsoftware using a small set of primitive operations, which form the\n{ instructions} available in the ISA.  Instructions usually\ninclude data movement instructions such as loads and stores\nand control instructions such as branches and subroutine calls in\naddition to arithmetic and logic operations.  \n\nThe last quarter of our class covers these concepts in more detail\nand explores their meaning using an example ISA from the textbook.\n\nIn class, we emphasized the idea that digital systems such as computers\ndo not interpret the meaning of bits.  Rather, they do exactly what\nthey have been designed to do, even if that design is meaningless.\n\nIf, for example, you store\na sequence of ASCII characters \nin a computer's memory as \nand\nthen write computer instructions to add consecutive groups of four characters\nas 2's complement integers and to print the result to the screen, the\ncomputer will not complain about the fact that your code produces\nmeaningless garbage.  \n\nIn contrast, high-level languages typically require that a programmer\nassociate a data type with each datum in order to reduce the chance \nthat the bits \nmaking up an individual datum are misused or misinterpreted accidentally.  \n\nAttempts to interpret a set of bits differently usually generate at least\na warning message, since\n\n\n\nsuch re-interpretations of the\nbits are rarely intentional and thus rarely correct.  A compiler---a\nprogram that transforms code written in a high-level language into\ninstructions---can also generate the proper type conversion instructions \nautomatically when the \ntransformations are intentional, as is often the case with arithmetic.\n\nSome high-level languages, such as Java, \nprevent programmers from changing the type of a given datum.\nIf you define a type that represents one of your\nfavorite twenty colors, for example, you are not allowed to turn a\ncolor into an integer, despite the fact that the color is represented\nas a handful of bits.  Such languages are said to be { strongly\ntyped}.  \n\nThe C language is not strongly typed, and programmers are free to\ninterpret any bits in any manner they see fit.  Taking advantage of\nthis ability in any but a few exceptional cases, however, \nresults in arcane and non-portable code, and is thus considered to be\nbad programming practice.  We discuss conversion between types in more\ndetail later in these notes.\n\nEach high-level language defines a number of { primitive data\ntypes}, which are always available.  Most languages, including C,\nalso provide ways of defining new types in terms of primitive types,\nbut we leave that part of C for ECE 220.\n\nThe primitive data types in C include signed and unsigned integers of various\nsizes as well as single- and double-precision IEEE floating-point numbers.\n\n\nThe primitive integer types in C include both unsigned and 2's\ncomplement representations.  These types were originally defined so as\nto give reasonable performance when code was ported.  In particular,\nthe { int} type is intended to be the native integer type for the\ntarget ISA.  Using data types supported directly in hardware is faster \nthan using larger or smaller integer types.  When C was standardized in 1989,\nthese types were defined so as to include a range of existing\nC compilers rather than requiring all compilers to produce uniform\nresults.  At the\n\n\n{-8pt}{|r|l|l|}\n& {|c|}{2's complement}& {|c|}\n8 bits& { char}& { unsigned char} \n16 bits& { short}& { unsigned short}\n& { short int}& { unsigned short int} \n32 bits& { int}& { unsigned}\n&& { unsigned int} \n32 or & { long}& { unsigned long}\n64 bits& { long int}& { unsigned long int} \n64 bits& { long long}& { unsigned long long}\n& { long long int}& { unsigned long long int}\n\n{-14pt}\n\ntime, most workstations and mainframes were 32-bit machines, while\nmost personal computers were 16-bit machines, thus flexibility was somewhat\ndesirable.  For the GCC compiler on Linux, the C integer data \ntypes are defined\nin the table above.  Although the { int} and { long}\ntypes are usually the same, there is a semantic difference in common\nusage.  In particular, on most architectures and most compilers, a\n{ long} has enough bits to identify a location in the computer's\nmemory, while an { int} may not.\n\nWhen in doubt, the { size in bytes} of any type or variable can be\nfound using the built-in C function { sizeof}.\n\n\nOver time, the flexibility of size in C types has become less\nimportant (except for the embedded markets, where one often wants even\nmore accurate bit-width control), and the fact that the size of an\n{ int} can vary from machine to machine and compiler to compiler\nhas become more a source of headaches than a helpful feature.  In the\nlate 1990s, a new set of fixed-size types were recommended for\ninclusion\n\n\n{-8pt}{|r|l|l|}\n& {|c|}{2's complement}& {|c|}\n 8 bits& {  int8_t}& {  uint8_t}\n16 bits& { int16_t}& { uint16_t}\n32 bits& { int32_t}& { uint32_t}\n64 bits& { int64_t}& { uint64_t}\n\n{-14pt}\n\nin the C library, reflecting the fact that many companies\nhad already developed and were using such definitions to make their\nprograms platform-independent.\n\nWe encourage you to make use of these types, which are shown in \nthe table above.  In Linux, they can be made available by including \nthe { stdint.h} header file.\n\nFloating-point types in C include { float} and { double},\nwhich correspond respectively to single- and double-precision IEEE\nfloating-point values.  Although the {32-bit} { float} type\ncan save memory compared with use of {64-bit} { double}\nvalues, C's math library works with double-precision values, and\nsingle-precision data are uncommon in scientific and engineering\ncodes.  In contrast, single-precision floating-point operations\ndominated the\ngraphics industry until recently, and are still well-supported even\non today's graphics processing units.\n\n\n \n Question: What is the standard data type for an integer in C? \n Answer: \n\nA. The standard data type for an integer in C is the int data type. This data type is a fixed-size type that is typically 32 bits in size."}, {"text": "Title: Variable Declarations \n Text: {Variable Declarations}\n\nThe function { main} executed by a program begins with a list\nof { variable declarations}.  Each declaration consists of two parts:\na data type specification and a comma-separated list of variable names.\nEach variable declared can also \nbe { initialized} by assigning an initial value.  A few examples \nappear below.  Notice that one can initialize a variable to have the same\nvalue as a second variable.\n\n{\n\naaaaaaa=aa=aaaaaaaaaaaaaaaaaaaaa=/=* a third 2's complement variable with unknown initial value =\nint > x > = 42; >/>* a 2's complement variable, initially equal to 42 > */\nint > y > = x;  >/>* a second 2's complement variable, initially equal to x > */\nint > z;>       >/>* a third 2's complement variable with unknown initial value > */\ndouble> a, b, c, pi = 3.1416; > >/>*\n>>>>* four double-precision IEEE floating-point variables\n>>>>* a, b, and c are initially of unknown value, while pi is\n>>>>* initially 3.1416\n>>>>*/\n\n}\n\nWhat happens if a programmer declares a variable but does not \ninitialize it?\n\nRemember that bits can only be 0 or 1.\n\nAn uninitialized variable does have a value, but its value is unpredictable.\n\nThe compiler tries to detect uses of uninitialized variables, but sometimes\nit fails to do so, so { until you are more familiar with programming,\nyou should always initialize every variable}.\n\nVariable names, also called { identifiers}, can include both letters\nand digits in C.\n\nGood programming style requires that programmers select variable names\nthat are meaningful and are easy to distinguish from one another.\n\nSingle letters are acceptable in some situations, but longer names with\nmeaning are likely to help people (including you!) understand your \nprogram.\n\nVariable names are also case-sensitive in C, which allows programmers\nto use capitalization to differentiate behavior and meaning, if desired.\nSome programs, for example, use identifiers with all capital letters\nto indicate variables with values that remain constant for the program's\nentire execution.\n\nHowever, the fact that identifiers are case-sensitive also means \nthat a programmer can declare distinct variables \nnamed { variable}, { Variable}, { vaRIable}, { vaRIabLe}, \nand { VARIABLE}.  We strongly discourage you from doing so.\n\n\n\n\n \n Question: What is a variable that has not been initialized? \n Answer: \n\nA. A variable that has not been assigned a value."}, {"text": "Title: Expressions and Operators \n Text: {Expressions and Operators}\n\n\nThe { main} function also contains a sequence of statements.\n\nA statement is a complete specification of a single step\nin the program's execution.\n\nWe explain the structure of\nstatements in the next section.  \n\nMany statements in C include one or more { expressions},\nwhich represent calculations such as arithmetic, comparisons,\nand logic operations.\n\nEach expression is in turn composed of { operators} and { operands}.\n\nHere we give only a brief introduction to some of the operators available\nin the C language.  We deliberately omit operators with more\ncomplicated meanings, as well as operators for which the original\npurpose was to make writing common operations a little shorter.\n\nFor the interested reader, both the textbook and ECE 220 give more \ndetailed introductions.\n\nThe table to the right gives examples for the operators described \nhere.  \n\n\n\n{\n\n{int i = 42, j = 1000;}\n{/* i = 0x0000002A, j = 0x000003E8 */}\n\ni + j &  1042\ni - 4 * j &  -3958\n-j &  -1000\nj / i &  23\nj  i &   42\ni & j &   40& /* 0x00000028 */\ni | j &   1002& /* 0x000003EA */\ni  j &   962& /* 0x000003C2 */\ni &   -43& /* 0xFFFFFFD5 */\n(i) >> 2 &   -11& /* 0xFFFFFFF5 */\n((i) >> 4) &   2& /* 0x00000002 */\nj >> 4 &  62& /* 0x0000003E */ \nj << 3 &  8000& /* 0x00001F40 */ \ni > j &   0\ni <= j &   1\ni == j &   0\nj = i &   42& /* ...and j is changed! */\n\n}\n\n\n\n\n{ Arithmetic operators} in C include addition ({ +}), \nsubtraction ({ -}), negation (a minus sign not \npreceded by another expression), multiplication ({ *}), \ndivision ({ /}), and modulus ({ }).  No exponentiation\noperator exists; instead, library routines are defined for this purpose\nas well as for a range of more complex mathematical functions.\n\nC also supports { bitwise operations} on integer types, including \nAND ({ &}), OR ({ |}), XOR ({ ^{ }}), NOT ({ }), \nand left ({ <<}) and right ({ >>}) bit shifts.\nRight shifting a signed integer results in an { arithmetic right shift}\n(the sign bit is copied), while right shifting an unsigned integer\nresults in a { logical right shift} (0 bits are inserted).\n\nA range of { relational} or { comparison operators} are \navailable, including equality ({ ==}),\ninequality ({ !=}), and relative order ({ <}, { <=},\n{ >=}, and { >}).\n\nAll such operations evaluate to 1 to indicate a true relation\nand 0 to indicate a false relation.  Any non-zero value is considered\nto be true for the purposes of tests (for example, in an { if} statement\nor a { while} loop) in C---these statements are explained later in \nthese notes.\n\n{ Assignment} of a new value to a variable \nuses a single equal sign ({ =}) in C.  \n\nFor example, the expression { A = B} copies\nthe value of variable { B} into variable { A}, overwriting the\nbits representing the previous value of { A}.\n\n{ The use of two equal signs for an equality check and a single\nequal sign for assignment is a common source of errors,} although\nmodern compilers generally detect and warn about this type of mistake.\n\nAssignment in C does not solve equations, even simple equations.  \nWriting ``{ A-4=B}'', for example, generates a compiler error.\nYou must solve such equations yourself to calculate the desired\nnew value of a single variable, such as ``{  A=B+4}.''\nFor the purposes of our class, you must always write a single variable \non the left side of an assignment, and can write an arbitrary expression \non the right side.\n\nMany operators can be combined into a single expression.  When an\nexpression has more than one operator, which operator is executed first?\nThe answer depends on the operators' { precedence}, a well-defined order on\noperators that specifies how to resolve the ambiguity.  In the case\nof arithmetic, the C language's precedence specification matches the\none that you learned in elementary school.  For example, { 1+2*3}\nevaluates to 7, not to 9, because multiplication has precedence over\naddition.  For non-arithmetic operators, or for any case in which\nyou do not know the precedence specification for a language, {\ndo not look it up---other programmers will not remember the\nprecedence ordering, either!}  Instead, add parentheses to make your \nexpressions clear and easy to understand.\n\n\n \n Question: What is the purpose of an expression? \n Answer: \n\nA. The purpose of an expression is to represent a calculation, such as an arithmetic operation, a comparison, or a logic operation."}, {"text": "Title: Basic I/O \n Text: {Basic I/O}\n\nThe { main} function returns an integer to the operating system.\nAlthough we do not discuss how additional functions can be written\nin our class, we may sometimes make use of functions that have been\nwritten in advance by making { calls} to those functions.\nA { function call} is type of expression in C, but we leave \nfurther description for ECE 220.  In our class, we make use of only\ntwo additional functions to enable our programs to receive input\nfrom a user via the keyboard and to write output to the monitor for \na user to read.\n\nLet's start with output.  The { printf} function allows a program\nto print output to the monitor using a programmer-specific format.\nThe ``f'' in { printf} stands for ``formatted.''{The \noriginal, unformatted variant of printing was never available\nin the C language.  Go learn Fortran.}\n\nWhen we want to use { printf}, we write a expression with\nthe word { printf} followed by a parenthesized, comma-separated\nlist of expressions.  The expressions in this list are called\nthe { arguments} to the { printf} function.\n\nThe first argument to the { printf} function is a format string---a \nsequence of ASCII characters between quotation marks---which tells \nthe function what kind of information we want printed to\nthe monitor as well as how to format that information.\n\nThe remaining arguments are C expressions that give { printf}\na copy of any values that we want printed.\n\nHow does the format string specify the format?\n\nMost of the characters in the format string are simply printed to \nthe monitor.  \n\nIn the first example shown to on the next page, we use { printf}\nto print a hello message followed by an ASCII newline character\nto move to the next line on the monitor.\n\n\nThe percent sign---``''---is used \nas an { escape character} in the\n{ printf} function.  When ``'' appears in the format\nstring, the function examines the next character in the format string\nto determine which format to use, then takes\nthe next expression from the sequence\nof arguments and prints the value of that expression to the \nmonitor.  Evaluating an expression generates a bunch of bits, so it is up to\nthe programmer to ensure that those bits are not misinterpreted.\nIn other words, the programmer must make sure that the number and\ntypes of formatted values match the number and types of arguments passed\nto { printf} (not counting the format string itself).\n\nThe { printf} function returns the number of characters printed\nto the monitor.\n\n\n\noutput: =\n> { printf (\"Hello, world!n\");}\noutput: > { Hello, world!} [and a newline]\n\n> { printf (\"To x or not to d...n\", 190, 380 / 2);}\noutput: > { To be or not to 190...} [and a newline]\n\n> { printf (\"My favorite number is cc.n\", 0x34, '0'+2);}\noutput: > { My favorite number is 42.} [and a newline]\n\n> { printf (\"What is pi?  f or e?n\", 3.1416, 3.1416);}\noutput: > { What is pi?  3.141600 or 3.141600e+00?} [and a newline]\n\n\n{|c|l|}\nescape  &                         \nsequence& { printf} function's interpretation of expression bits \n{ c}& 2's complement integer printed as an ASCII character\n{ d}& 2's complement integer printed as decimal\n{ e}& double printed in decimal scientific notation\n{ f}& double printed in decimal\n{ u}& unsigned integer printed as decimal\n{ x}& integer printed as hexadecimal (lower case)\n{ X}& integer printed as hexadecimal (upper case)\n{ }& a single percent sign \n\n\n\n\nA program can read input from the user with the { scanf} function.\nThe user enters characters in ASCII using the keyboard, and the\n{ scanf} function converts the user's input into C primitive types,\nstoring the results into variables.  As with { printf}, the\n{ scanf} function takes a format string followed by a comma-separated\nlist of arguments.  Each argument after the format string provides\n{ scanf} with the memory address of a variable into which the\nfunction can store a result.\n\nHow does { scanf} use the format string?\n\nFor { scanf}, the format string is usually just a sequence\nof conversions, one for each variable to be typed in by the user.\nAs with { printf}, the conversions start with ``'' and\nare followed by characters specifying the type of conversion\nto be performed.  The first example shown to the right reads\ntwo integers.\n\nThe conversions in the format string can be separated by spaces \nfor readability, as shown in the exam-\n\n\n\neffect: =unsigned =\n> { int     } > { a, b;  /* example variables */}\n> { char    } > { c;}\n> { unsigned} > { u;}\n> { double  } > { d;}\n> { float   } > { f;}\n\n> { scanf (\"dd\", &a, &b);   /* These have the */}\n> { scanf (\"d d\", &a, &b);  /* same effect.   */}\neffect: > try to convert two integers typed in decimal to\n> 2's complement and store the results in { a} and { b}\n\n> { scanf (\"cx lf\", &c, &u, &d);}\neffect: > try to read an ASCII character into { c}, a value\n> typed in hexadecimal into { u}, and a double-\n> precision > floating-point number into { d}\n\n> { scanf (\"lf f\", &d, &f);}\neffect: > try to read two real numbers typed as decimal,\n> convert the first to double-precision and store it \n> in { d}, and convert the second to single-precision \n> and store it in { f}\n\n\n\n\nple.  The spaces are ignored\nby { scanf}.  However, { any non-space characters in the\nformat string must be typed exactly by the user!}\n\nThe remaining arguments to { scanf} specify memory addresses\nwhere the function can store the converted values.  \n\nThe ampersand (``&'') in front of each variable name in the examples is an\noperator that returns the address of a variable in memory.\n\nFor each con-\n\n\n{|c|l|}\nescape  &                         \nsequence& { scanf} function's conversion to bits \n{ c}& store one ASCII character (as { char})\n{ d}& convert decimal integer to 2's complement\n{ f}& convert decimal real number to float\n{ lf}& convert decimal real number to double\n{ u}& convert decimal integer to unsigned int\n{ x}& convert hexadecimal integer to unsigned int\n{ X}& (as above) \n\n\n\nversion\nin the format string, the { scanf} function tries to convert\ninput from the user into the appropriate result, then stores the\nresult in memory at the address given by the next argument.\n\nThe programmer is responsible for ensuring that the number of \nconversions in the format string\nmatches the number of arguments provided (not counting\nthe format string itself).  The programmer must also ensure that\nthe type of information produced by each conversion can be\nstored at the address passed for that conversion---in other words,\nthe address of a\nvariable with the correct type must be\nprovided.  Modern compilers often detect missing { &} operators\nand incorrect variable types, but many only give warnings to the\nprogrammer.  The { scanf} function itself cannot tell whether\nthe arguments given to it are valid or not.\n\nIf a conversion fails---for example, if a user types ``hello'' when\n{ scanf} expects an integer---{ scanf} does not overwrite the\ncorresponding variable and immediately stops trying to convert input.\n\nThe { scanf} function returns the number of successful \nconversions, allowing a programmer to check for bad input from\nthe user.\n\n \n Question: What is the & operator used to get the address of a variable? \n Answer: \n\nThe & operator is used to get the address of a variable. This is necessary because scanf needs to know where to store the result of the conversion."}, {"text": "Title: Types of Statements in C \n Text: {Types of Statements in C}\n\nEach statement in a C program specifies a complete operation.\n\nThere are three types of statements, but two of these types can\nbe constructed from additional statements, which can in turn be\nconstructed from additional statements.  The C language specifies\nno bound on this type of recursive construction, but code \nreadability does impose a practical limit.\n\n\nThe three types are shown to the right.\nThey are the { null statement}, \n{ simple statements}, \nand { compound statements}.\n\nA null statement is just a semicolon, and a compound statement \nis just a sequence of statements surrounded by braces.\n\nSimple statements can take several forms.  All of the examples\nshown to the right, including the call to { printf}, are\nsimple state-\n\n\n{\n\naaaa=aaaaaaaaaaaaaa=/* =aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa=\n;   > > /* >a null statement (does nothing) >*/\n\nA = B; > > /*  >examples of simple statements >*/\nprintf (\"Hello, world!n\");\n\n{    > > /* > a compound statement >*/ \n>  C = D; > /* > (a sequence of statements >*/\n>  N = 4; > /* > between braces) >*/ \n>  L = D - N;\n}\n\n}\n{-2pt}\n\n\nments consisting of a C expression followed by a \nsemicolon.\n\nSimple statements can also consist of conditionals or iterations, which\nwe introduce next.\n\nRemember that after variable declarations, the { main} function\ncontains a sequence of statements.  These statements are executed one\nat a time in the order given in the program, as shown to the right\nfor two statements.  We say that the statements are executed in\nsequential order.\n\nA program must also be able to execute statements only when \nsome condition holds.  In the C language, such a condition can be\nan arbitrary expression.  The expression is first evaluated.\nIf the result is 0, the condition\nis considered to be false.  Any result other than 0 is considered\nto be true.  The C statement\nfor conditional execution is called an { if}\n\n\n{file=part1/figs/part1-sd-sequential.eps,width=0.8in}\n\n\n\nstatement.  Syntactically, we put the expression for the condition\nin parentheses after the keyword { if} and follow the parenthesized\nexpression with a compound statement containing the statements\nthat should be executed when the condition is true.  Optionally,\nwe can append the keyword { else} and a second compound\nstatement containing statements to be executed when the condition\nevaluates to false.  \nThe corresponding flow chart is shown to the right.\n\n{\n\naaaa=aaaaaaaaaaaa=\n/* Set the variable y to the absolute value of variable x. */\nif (0 <= x) {> >/* Is x greater or equal to 0? */\n> y = x;     >/* Then block: assign x to y. */\n} else {\n> y = -x;    >/* Else block: assign negative x to y. */\n}\n\n}\n\n\n{file=part1/figs/part1-sd-conditional.eps,width=2in}\n\n\nIf instead we chose to assign the absolute value of variable { x}\nto itself, we can do so without an { else} block:\n\n{\n\naaaa=aaaaaaaaaaaa=\n/* Set the variable x to its absolute value. */\nif (0 > x) {> >/* Is x less than 0? */\n> x = -x;    >/* Then block: assign negative x to x. */\n}            >> /* No else block is given--no work is needed. */\n\n}\n\n\nFinally, we sometimes need to repeatedly execute a set of statements,\neither a fixed number of times or so long as some condition holds.\nWe refer to such repetition as an { iteration} or a { loop}.\nIn our class, we make use of C's { for} loop when we need to\nperform such a task.  A { for} loop is structured as follows:\n\n{ \n\nfor ([initialization] ; [condition] ; [update]) {\n    [subtask to be repeated]\n}\n\n}\n\nA flow chart corresponding to execution of a { for} loop appears\nto the right.  First, any initialization is performed.  Then the\ncondition---again an arbitrary C expression---is checked.  If the\ncondition evaluates to false (exactly 0), the loop is done.  Otherwise,\nif the condition evaluates to true (any non-zero value),\nthe statements in the compound statement, the subtask or { loop body},\nare executed.  The loop body can contain anything: a sequence of simple \nstatements, a conditional, another loop, or even just an empty list.\nOnce the loop body has finished executing, the { for} loop\nupdate rule is executed.  Execution then checks the condition again,\nand this process repeats until the condition evaluates to 0.\nThe { for} loop below, for example, prints the numbers \nfrom 1 to 42.\n\n{ \n\n/* Print the numbers from 1 to 42. */\nfor (i = 1; 42 >= i; i = i + 1) {\n    printf (\"dn\", i);\n}\n\n}\n\n\n{file=part1/figs/part1-sd-iterative.eps,width=1.35in}\n\n\n\n \n Question: What is a simple statement? \n Answer: \nThe type of statement that consists of a C expression followed by a semicolon is a simple statement."}, {"text": "Title: Program Execution \n Text: {Program Execution}\n\n\nWe are now ready to consider the execution of a simple program,\nillustrating how variables change value from step to step and\ndetermine program behavior.\n\nLet's say that two numbers are ``friends'' if they have at least one\n1 bit in common when written in base 2.  So, for example, 100_2 and \n111_2 are friends because both numbers have a 1 in the bit with \nplace value 2^2=4.  Similarly, 101_2 and 010_2 are not friends,\nsince no bit position is 1 in both numbers.\n\nThe program to the right prints all friendships between numbers\nin the interval [0,7].\n\n\n{\n\naaaa=aaaa=aaaaaaaaaa=/* a second number to consider as check's friend =\nint\nmain ()\n{\n>  int > check;  > /* number to check for friends > */\n>  int > friend; > /* a second number to consider as check's friend > */\naaaa=aaaa=aaaa=aaaa=\n>  \n>  /* Consider values of check from 0 to 7. */\n>  for (check = 0; 8 > check; check = check + 1) {\n\n>  >  /* Consider values of friend from 0 to 7. */\n>  >  for (friend = 0; 8 > friend; friend = friend + 1) {\n\n>  >  >  /* Use bitwise AND to see if the two share a 1 bit. */\n>  >  >  if (0 != (check & friend)) {\n\n>  >  >  >  /* We have friendship! */\n>  >  >  >  printf (\"d and d are friends.n\", check, friend);\n>  >  >  }\n>  >  }\n>  }\n}\n\n}\n\n\nThe program uses two\ninteger variables, one for each of the numbers that we consider.\nWe use a { for} loop to iterate over all values of our first\nnumber, which we call { check}.  The loop initializes { check}\nto 0, continues until check reaches 8, and adds 1 to check after\neach loop iteration.  We use a similar { for} loop to iterate\nover all possible values of our second number, which we call { friend}.\nFor each pair of numbers, we determine whether they are friends\nusing a bitwise AND operation.  If the result is non-zero, they\nare friends, and we print a message.  If the two numbers are not\nfriends, we do nothing, and the program moves on to consider the\nnext pair of numbers.\n\n\nNow let's think about what happens when this program executes.\n\nWhen the program starts, both variables are filled with random bits,\nso their values are unpredictable.  \n\nThe first step is the initialization of the first { for} loop, which\nsets { check} to 0.\n\nThe condition for that loop is { 8 > check}, which is true, so\nexecution enters the loop body and starts to execute the first\nstatement, which is our second { for} loop.\n\nThe next step is then the initialization code for the second { for}\nloop, which sets { friend} to 0.\n\nThe condition for the second loop is { 8 > friend}, which is true, so\nexecution enters the loop body and starts to execute the first\nstatement, which\n\n\n\nafter executing...& { check} is...& and { friend} is... \n(variable declarations)& unpredictable bits& unpredictable bits\n{ check = 0}& 0& unpredictable bits\n{ 8 > check}& 0& unpredictable bits\n{ friend = 0}& 0& 0\n{ 8 > friend}& 0& 0\n{ if (0 != (check & friend))}& 0& 0\n{ friend = friend + 1}& 0& 1\n{ 8 > friend}& 0& 1\n{ if (0 != (check & friend))}& 0& 1\n{ friend = friend + 1}& 0& 2\n{(repeat last three lines six more times; number 0 has no friends!)}\n{ 8 > friend}& 0& 8\n{ check = check + 1}& 1& 8\n{ 8 > check}& 1& 8\n{ friend = 0}& 1& 0\n{ 8 > friend}& 1& 0\n{ if (0 != (check & friend))}& 1& 0\n{ friend = friend + 1}& 1& 1\n{ 8 > friend}& 1& 1\n{ if (0 != (check & friend))}& 1& 1\n{ printf ...}& 1& 1\n{(our first friend!?)}\n\n\n\nis the { if} statement.\n\nSince both variables are 0, the { if} condition is false, and\nnothing is printed.\n\nHaving finished the loop body for the inner loop (on { friend}),\nexecution continues with the update rule for that loop---{ friend = \nfriend + 1}---then returns to check the loop's condition again.\n\nThis process repeats, always finding that the number 0 (in { check})\nis not\nfriends (0 has no friends!) until { friend} reaches 8, at which\npoint the inner loop condition becomes false.\n\nExecution then moves to the update rule for the first { for} loop,\nwhich increments { check}.  Check is then compared with 8 to\nsee if the loop is done.  Since it is not, we once again enter the\nloop body and start the second { for} loop over.  The initialization\ncode again sets { friend} to 0, and we move forward as before.\nAs you see above, the first time that we find our { if} condition\nto be true is when both { check} and { friend} are equal to 1.\n\nIs that result what you expected?  To learn that the number 1 is\nfriends with itself?  If so, the program works.  If you assumed that\nnumbers could not be friends with themselves, perhaps we should fix the \nbug?  We could, for example, add another { if} statement to \navoid printing anything when { check == friend}.\n\nOur program, you might also realize, prints each pair of friends twice.\nThe numbers 1 and 3, for example, are printed in both possible orders.  To\neliminate this redundancy, we can change the initialization in the \nsecond { for} loop, either to { friend = check} or to\n{ friend = check + 1}, depending on how we want to define friendship\n(the same question as before: can a number be friends with itself?).\n\n\n\n \n Question: What is the question that we want to answer about friendship? \n Answer: \nCan a number be friends with itself?\n\nYes, a number can be friends with itself."}, {"text": "Title: Compilation and Interpretation* \n Text: {Compilation and Interpretation*}\n\n\nMany programming languages, including C, can be \n{ compiled}, which means that the program is converted into \ninstructions for a particular ISA before the program is run\non a processor that supports that ISA.\nThe figure to the right illustrates the compilation process for \nthe C language.  \n\nIn this type of figure, files and other data are represented as cylinders,\nwhile rectangles represent processes, which are usually implemented in \nsoftware.\n\nIn the figure to the right, the outer dotted box represents the full \ncompilation\nprocess that typically occurs when one compiles a C program.\nThe inner dotted box represents the work performed by the { compiler}\nsoftware itself.\n\nThe cylinders for data passed between the processes that compose\nthe full compilation process\nhave been left out of the figure; instead, we have written the type\nof data being passed next to the arrows that indicate the flow of information\nfrom one process to the next.\n\nThe C preprocessor (described later in these notes) forms the\nfirst step in the compilation process.  The preprocessor\noperates on the program's { source code} along\nwith { header files} that describe data types and\noperations.  The preprocessor merges these together\ninto a single file of preprocessed source code.  The preprocessed\nsource code is then analyzed by the front end of the compiler based on the\nspecific programming language being used (in our case, the C language),\nthen converted by the back end of the compiler\ninto instructions for the desired ISA.  The output of a compiler\nis not binary instructions, however, but is instead\na human-readable form of instructions called { assembly code},\nwhich we cover in the last quarter of our class.  A tool called\nan assembler then converts these human-readable instructions into\nbits that a processor can understand.  If a program consists of\nmulti-\n\n\n{file=part1/figs/part1-compiler.eps,width=3in}\n\n\nple source files, or needs to make use of additional \npre-programmed operations (such as math functions, graphics, or sound),\na tool called a linker merges the object code of the program with\nthose additional elements to form the final { executable image}\nfor the program.  The executable image is typically then stored on\na disk, from which it can later be read into memory in order to\nallow a processor to execute the program.\n\nSome languages are difficult or even impossible to compile.  Typically, the\nbehavior of these languages depends on input data that are only available \nwhen the program runs.  Such languages can be { interpreted}: each step \nof the algorithm described by a program is executed by a software interpreter\nfor the language.  Languages such as Java, Perl, and Python are usually\ninterpreted.  Similarly, when we use software to simulate one ISA using\nanother ISA, as we do at the end of our class with the {LC-3}\nISA described by the textbook, the simulator is a form of interpreter.\nIn the lab, you will use a simulator compiled into and executing as x86 \ninstructions in order to interpret {LC-3} instructions.  \n\nWhile a program is executing in an interpreter, enough information\nis sometimes available to compile part or all of the program to\nthe processor's ISA as the program runs, \na technique known as { ``just in time'' ( JIT) compilation}.\n\n\n\n\n \n Question: What is the output of a compiler? \n Answer: \n\nA. The compiler converts the human-readable instructions in the source code into machine code, which is a set of instructions that can be executed by a processor. The machine code is then stored in an executable image, which can be run on a computer."}, {"text": "Title: The C Preprocessor* \n Text: {The C Preprocessor*}\n\nThe C language uses a preprocessor to support inclusion of common\ninformation (stored in header files) into multiple source files.\n\nThe most frequent use of the preprocessor is to enable the unique\ndefinition of new data types and operations within\nheader files that can then be included by reference within source\nfiles that make use of them.  This capability is based on the \n{ include directive}, { #include}, as shown here:  \n\n{\n\n\n#include \"my_header.h\"    = /* search in current followed by standard directories =\n#include <stdio.h>      > /* search in standard directories > */\n#include \"my_header.h\" > /* search in current followed by standard directories > */\n\n\n}\n\nThe preprocessor also supports integration of compile-time constants\ninto source files before compilation.  For example, many\nsoftware systems allow the definition of a symbol such as { NDEBUG}\n(no debug) to compile without additional debugging code included in\nthe sources.  \n\nTwo directives are necessary for this purpose: the { define directive},\n{ #define}, which\nprovides a text-replacement facility, and { conditional inclusion} (or\nexclusion) of parts of a file within { #if}/{ #else}/{\n#endif} directives.\n\nThese directives are also useful in allowing\n\n\na single header file to\nbe included multiple times without causing problems, as C does not\nallow redefinition of types, variables, and so forth, even if the\nredundant \ndefinitions are identical.  Most header files are thus wrapped as shown\nto the right.\n\n\n{\n\n#if !defined(MY_HEADER_H)\n#define MY_HEADER_H\n/* actual header file material goes here */\n#endif /* MY_HEADER_H */\n\n}\n\n\nThe preprocessor performs a simple linear pass on the source and does\nnot parse or interpret any C syntax.\n\nDefinitions for text replacement are valid as soon as they are defined\nand are performed until they are undefined or until the end of the\noriginal source file.\n\nThe preprocessor does recognize spacing and will not replace part of a\nword, thus ``{ #define i 5}'' will not wreak havoc on your {\nif} statements, but will cause problems if you name any variable { i}.\n\nUsing the text replacement capabilities of the preprocessor does have\ndrawbacks, most importantly in that almost none of the information is\npassed on for debugging purposes.  \n\n \n Question: What is the name of the directive that allows for the inclusion of common information into multiple source files? \n Answer: \n\nA. The name of the directive that allows for the inclusion of common information into multiple source files is the include directive."}, {"text": "Title: Changing Types in C* \n Text: {Changing Types in C*}\n\nChanging the type of a datum is necessary from time to time, but\nsometimes a compiler can do the work for you.\n\nThe most common form of { implicit type conversion} occurs with binary\narithmetic operations.  Integer arithmetic in C always uses types of\nat least the size of { int}, and all floating-point arithmetic uses\n{ double}.\n\nIf either or both operands have smaller integer types, or differ from\none another, the compiler implicitly converts them before performing\nthe operation, and the type of the result may be different from those of\nboth operands.\n\nIn general, the compiler selects the final type according to some\npreferred ordering in which floating-point is preferred over integers,\nunsigned values are preferred over signed values, and more bits are\npreferred over fewer bits.\n\nThe type of the result must be at least as large as either argument,\nbut is also at least as large as an { int} for integer operations\nand a { double} for floating-point operations.\n\nModern C compilers always extend an integer type's bit width before\nconverting from signed to unsigned.  The original C specification\ninterleaved bit width extensions to { int} with sign changes, thus\n{ older compilers may not be consistent, and implicitly require\nboth types of conversion in a single operation may lead to portability\nbugs.}\n\nThe implicit extension to { int} can also be confusing in the sense\nthat arithmetic that seems to work on smaller integers fails with\nlarger ones.  For example, multiplying two 16-bit integers set to 1000\nand printing the result works with most compilers because the 32-bit \n{ int} result is wide enough to hold the right answer.  In contrast,\nmultiplying two 32-bit integers set to 100,000 produces the wrong\nresult because the high bits of the result are discarded before it can\nbe converted to a larger type.  For this operation to produce the\ncorrect result, one of the integers must be converted explicitly (as\ndiscussed later) before the multiplication.\n\n\n\nImplicit type conversions also occur due to assignments.  Unlike\narithmetic conversions, the final type must match the left-hand side\nof the assignment (for example, a variable to which a result is assigned), and\nthe compiler simply performs any necessary conversion.\n\n{ Since the desired type may be smaller than the type of the value\nassigned, information can be lost.}  Floating-point values are\ntruncated when assigned to integers, and high bits of wider integer\ntypes are discarded when assigned to narrower integer types.  { Note\nthat a positive number may become a negative number when bits are\ndiscarded in this manner.}\n\nPassing arguments to functions can be viewed as a special case of\nassignment.  Given a function prototype, the compiler knows the type\nof each argument and can perform conversions as part of the code\ngenerated to pass the arguments to the function.  Without such a\nprototype, or for functions with variable numbers of arguments, the\ncompiler lacks type information and thus cannot perform necessary\nconversions, leading to unpredictable behavior.  By default, however,\nthe compiler extends any integer smaller than an { int}\nto the width of an { int} and converts { float} to\n{ double}.\n\n\nOccasionally it is convenient to use an { explicit type cast} to force\nconversion from one type to another.  { Such casts must be used\nwith caution, as they silence many of the warnings that a compiler\nmight otherwise generate when it detects potential problems.}  One\ncommon use is to promote integers to floating-point before an\narithmetic operation, as shown to the right.\n\n\n{\n\naaaa=\nint\nmain ()\n{\n>  int numerator = 10;\n>  int denominator = 20;\n>\n>  printf (\"fn\", numerator / (double)denominator);\n>  return 0;\n}\n\n}\n{-14pt}\n\nThe type to which a value is to be converted\nis placed in parentheses in front of the value.  In most cases,\nadditional parentheses should be used to avoid confusion about the\nprecedence of type conversion over other operations.\n\n\n\n\n\n \n Question: What is the most common form of implicit type conversion? \n Answer: \n\nA. The most common form of implicit type conversion is with binary arithmetic operations."}, {"text": "Title: Summary of Part 1 of the Course \n Text: {Summary of Part 1 of the Course}\n\nThis short summary provides a list of both terms that we expect you to\nknow and and skills that we expect you to have after our first few weeks\ntogether.  The first part of the course is shorter than the other three\nparts, so the amount of material is necessarily less.\n\nThese notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.\n\nAccording to educational theory, the difficulty of learning depends on \nthe type of task involved.  Remembering new terminology is relatively \neasy, while applying the ideas underlying design decisions shown by \nexample to new problems posed as human tasks is relatively hard.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nThis time, we'll list the skills first and leave the easy stuff for the \nnext page.\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Represent decimal numbers with unsigned, 2's complement, and IEEE\nfloating-point representations, and be able to calculate the decimal value\nrepresented by a bit pattern in any of these representations.}\n\n{Be able to negate a number represented in the 2's complement\nrepresentation.}\n\n{Perform simple arithmetic by hand on unsigned and 2's complement\nrepresentations, and identify when overflow occurs.}\n\n{Be able to write a truth table for a Boolean expression.}\n\n{Be able to write a Boolean expression as a sum of minterms.}\n\n MOVED TO PART 4\n\n {Be able to calculate the Hamming distance of a code/representation.}\n \n {Know the relationships between Hamming distance and the abilities\n to detect and to correct bit errors.}\n\n{Know how to declare and initialize C variables with one of the \nprimitive data types.}\n\n\n\nAt a more abstract level, we expect you to be able to:\n\n{}{{}{}\n{}{}{}\n\n{Understand the value of using a common mathematical basis, such\nas modular arithmetic, in defining multiple representations (such as\nunsigned and 2's complement).}\n\n{Write Boolean expressions for the overflow conditions\non both unsigned and 2's complement addition.}\n\n MOVED TO PART 4\n\n {Be able to use parity for error detection, and Hamming codes for\n error correction.}\n\n{Be able to write single { if} statements and { for} loops\nin C in order to perform computation.}\n\n{Be able to use { scanf} and { printf} for basic input and \noutput in C.}\n\n\n\nAnd, at the highest level, we expect that you will be able to reason about\nand analyze problems in the following ways:\n\n{}{{}{}\n{}{}{}\n\n{Understand the tradeoffs between integer\n  FIXME?     not covered by book nor notes currently \n, fixed-point,    \nand floating-point representations for numbers.}\n\n{Understand logical completeness and be able to prove or disprove\nlogical completeness for sets of logic functions.}\n\n PARTIALLY MOVED TO PART 4\n\n {Understand the properties necessary in a representation, and understand\n basic tradeoffs in the sparsity of code words with error detection and\n correction capabilities.}\n{Understand the properties necessary in a representation: no ambiguity\nin meaning for any bit pattern, and agreement in advance on the meanings of \nall bit patterns.}\n\n{Analyze a simple, single-function C program and be able to explain its purpose.}\n\n\n\n\n\n\n\nYou should recognize all of these terms\nand be able to explain what they mean.\n(the parentheses give page numbers,\nor ``P&P'' for Patt & Patel).\n\nActually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  (You may skip the *'d terms in Fall 2012.)\n\nNote that we are not saying that you should, for example, be able to \nwrite down the ASCII representation from memory.  In that example, \nknowing that it is a {7-bit} representation used for English\ntext is sufficient.  You can always look up the detailed definition \nin practice.\n\n[t]\n{}{{}{}\n{}{}{}\n\n{universal computational devices /  computing machines ()\n{--}{{}{}\n{}{}{}\n undecidable ()\n the halting problem ()\n\n}\n\n pre-Fall 2015 version\n\n {Turing machines\n {--}{{}{}\n {}{}{}\n  universal computational device/ computing machine\n  intractable/undecidable\n  the halting problem\n \n }\n\n{information storage in computers\n{--}{{}{}\n{}{}{}\n bits ()\n representation (P&P)\n data type ()\n unsigned representation ()\n 2's complement representation\n\n FIXME?  not covered by book nor notes currently\n  fixed-point representation\n\n IEEE floating-point representation\n ASCII representation\n equivalence classes\n\n}\n\n{operations on bits\n{--}{{}{}\n{}{}{}\n 1's complement operation\n carry (from addition)\n overflow (on any operation) ()\n Boolean logic and algebra\n logic functions/gates\n truth table\n AND/conjunction\n OR/disjunction\n NOT/logical complement/ (logical) negation/inverter\n XOR\n logical completeness\n minterm\n\n}\n\n{mathematical terms\n{--}{{}{}\n{}{}{}\n modular arithmetic\n implication\n contrapositive\n proof approaches: by construction, by contradiction, by induction\n without loss of generality (w.l.o.g.)\n\n}\n\n MOVED TO PART 4\n\n {error detection and correction\n {--}{{}{}\n {}{}{}\n  code/sparse representation\n  code word\n  bit error\n  odd/even parity bit\n  Hamming distance between code words\n  Hamming distance of a code\n  Hamming code\n  SEC-DED\n \n }\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{high-level language concepts\n{--}{{}{}\n{}{}{}\n syntax\n{variables\n{--}{{}{}\n{}{}{}\n declaration\n primitive data types\n symbolic name/identifier\n initialization\n\n}\n FIXME?  really not necessary for them\n strongly typed languages\n expression\n statement\n\n}\n\n{C operators\n{--}{{}{}\n{}{}{}\n operands\n arithmetic\n bitwise\n comparison/relational\n assignment\n address\n arithmetic shift\n logical shift\n precedence\n\n}\n\n{functions in C\n{--}{{}{}\n{}{}{}\n { main}\n function call\n arguments\n {{ printf} and { scanf}\n{--}{{}{}\n{}{}{}\n format string\n escape character\n\n}\n { sizeof} (built-in)\n\n}\n\n{transforming tasks into programs\n{--}{{}{}\n{}{}{}\n flow chart\n sequential construct\n conditional construct\n iterative construct/iteration/loop\n loop body\n\n}\n\n{C statements\n{--}{{}{}\n{}{}{}\n statement: null, simple, compound\n { if} statement\n { for} loop\n { return} statement\n\n}\n\n THESE ARE NOT REQUIRED TOPICS\n\n {execution of C programs\n {--}{{}{}\n {}{}{}\n  compiler/interpreter\n  source code\n  header files\n  assembly code\n  instructions\n  executable image\n \n }\n \n {the C preprocessor\n {--}{{}{}\n {}{}{}\n  #include directive\n  #define directive\n \n }\n\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n\n \n Question: What is a universal computational device? \n Answer: \n\nA. A universal computational device is a device that can perform any computable task."}, {"text": "Title: Overflow Conditions \n Text: {Overflow Conditions}\n\nThis set of notes discusses the overflow conditions for unsigned and\n2's complement addition.  For both types, we formally prove that\nthe conditions that we state are correct.  Many of our faculty want our\nstudents to learn to construct formal proofs, so we plan to begin\nexposing you to this process in our classes.\n\nProf. Lumetta is a fan of Prof. George Polya's educational theories\nwith regard to proof techniques, and in particular the idea that one\nbuilds up a repertoire of approaches by seeing the approaches used \nin practice.\n\n , but teaching proof\n techniques effectively is challenging, particularly when exercises\n are of the form, ``Prove that <insert true theorem> is true'' rather\n than the more open-ended form that we typically encounter in research,\n ``Prove that <insert some conjecture> is true, or find a \n counterexample.''\n\n \n Question: What is the overflow condition for unsigned and 2's complement addition? \n Answer: \n\nA. An unsigned overflow occurs when the result of an unsigned addition operation is too large to be represented within the allotted space."}, {"text": "Title: Implication and Mathematical Notation \n Text: {Implication and Mathematical Notation}\n\nSome of you may not have been exposed to basics of mathematical logic, so\nlet's start with a brief introduction to implication.  We'll use \nvariables p and q to represent statements that can be either true\nor false.  For example, p might represent the statement, ``Jan is\nan ECE student,'' while q might represent the statement, ``Jan\nworks hard.''  The { logical complement} or { negation} of \na statement p,\nwritten for example as ``not p,'' has the opposite truth value:\nif p is true, not p is false, and if p is false, not p is\ntrue.\n\nAn { implication} is a logical relationship between two statements.\nThe implication itself is also a logical statement, and may be true or\nfalse.  In English, for example, we might say, ``If p, q.''\nIn mathematics, the same implication is usually written as either \n``q if p'' or ``p,'' and the latter is read \nas, ``p implies q.''  \n\nUsing our example values for p and q, we can see that\np is true: ``Jan is an ECE student'' does\nin fact imply that ``Jan works hard!''\n\nThe implication p \nis only considered false if p is true and q is false.\nIn all other cases, the implication is true.\nThis definition can be a little confusing at first, so let's use\nanother example to see why.\n\nLet  p represent the statement\n``Entity X is a flying pig,'' and let q represent \nthe statement, ``Entity X obeys air traffic control regulations.''\n\nHere the implication p is again true: \nflying pigs do not exist, so p is false, and thus \n``p'' is true---for any value of statement q!\n\nGiven an implication ``p,'' we say that the {\nconverse} of the implication is the statement \n``q,'' which is also an implication.\nIn mathematics, the converse of \np\nis sometimes written\nas ``q only if p.''  The converse of an implication may or may not have\nthe same truth value as the implication itself.  Finally,\nwe frequently use the shorthand notation, ``p if and only if q,''\n(or, even shorter, ``p iff q'') to mean \n``p { and}\nq.'' This last statement is true only when both\nimplications are true.\n\n \n Question: Why is the converse of an implication not always true? \n Answer: \n\nThe reason that the converse of an implication is not always true is that the implication may be false."}, {"text": "Title: Overflow for Unsigned Addition \n Text: {Overflow for Unsigned Addition}\n\nLet's say that we add two {N-bit} unsigned numbers, A\nand B.  The {N-bit} unsigned representation \ncan represent integers in the range [0,2^N-1].\n\nRecall that we say that the addition operation has \noverflowed if the number represented by the {N-bit} pattern\nproduced for the sum does not actually represent the number A+B.\n\nFor clarity, let's name the bits of A by writing the number\nas a_{N-1}a_{N-2}...a_1a_0.  Similarly, let's write B as\nb_{N-1}b_{N-2}...b_1b_0.  Name the sum C=A+B.  The sum that\ncomes out of the add unit has only N bits, but recall that\nwe claimed in class that the overflow condition for unsigned \naddition is given by the { carry} out of the most significant\nbit.  So let's write the sum as \nc_c_{N-1}c_{N-2}...c_1c_0, realizing that c_N is the\ncarry out and not actually part of the sum produced by the \nadd unit.\n\n{ Theorem:}\n\nAddition of two {N-bit} unsigned numbers\nA=a_{N-1}a_{N-2}...a_1a_0\nand\nB=b_{N-1}b_{N-2}...b_1b_0\nto produce sum\nC=A+B=c_c_{N-1}c_{N-2}...c_1c_0,\noverflows if and only if\nthe carry out c_N of the addition is a 1 bit.\n\n\n\n{ Proof:}\n\nLet's start with the ``if'' direction.  In other words, c_N=1 implies\noverflow.  Recall that unsigned addition is the same as base 2 addition,\nexcept that we discard bits beyond c_{N-1} from the sum C.\nThe bit c_N has place value 2^N, so, when c_N=1 we can write that \nthe correct sum C{2^N}.  But no value that large can be represented\nusing the {N-bit} unsigned representation, so we have an overflow.\n\nThe other direction (``only if'') is slightly more complex: we need to\nshow that overflow implies that c_N=1.  We use a range-based argument\nfor this purpose.  Overflow means that the sum C is outside the\nrepresentable range [0,2^N-1].  Adding two non-negative numbers cannot\nproduce a negative number, so the sum can't be smaller than 0.  Overflow \nthus implies that C{2^N}.\n\nDoes that argument complete the proof?  No, because some numbers, such \nas 2^{N+1}, are larger than 2^N, but do not have a 1 bit in the Nth\nposition when written in binary.  We need to make use of the constraints\non A and B implied by the possible range of the representation.\n\nIn particular, given that A and B are represented as {N-bit}\nunsigned values, we can write\n\n{eqnarray*}\n0  & A &  2^N - 1\n0  & B &  2^N - 1\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n0  & C &  2^{N + 1} - 2\n{eqnarray*}\n\nCombining the new inequality with the one implied by the overflow \ncondition, we obtain\n\n{eqnarray*}\n2^N  & C &  2^{N + 1} - 2\n{eqnarray*}\n\nAll of the numbers in the range allowed by this inequality have c_N=1,\ncompleting our proof.\n\n \n Question: What is the result of an overflow? \n Answer: \n\nA. Knowing when an addition operation has overflowed is important in order to avoid errors in computation. If an addition operation overflows, the result will not be the correct sum of the two numbers being added."}, {"text": "Title: Overflow for 2's Complement Addition \n Text: {Overflow for 2's Complement Addition}\n\nUnderstanding overflow for 2's complement addition is somewhat trickier,\nwhich is why the problem is a good one for you to think about on your\nown first.\n\nOur operands, A and B, are now two {N-bit} 2's complement numbers.\nThe {N-bit} 2's complement representation \ncan represent integers in the range [-2^{N-1},2^{N-1}-1].\n\nLet's start by ruling out a case that we can show never leads to overflow.\n\n{ Lemma:} \n\nAddition of two {N-bit} 2's complement numbers A and B\ndoes not overflow if one of the numbers is negative and the other is\nnot.\n\n{ Proof:}\n\nWe again make use of the constraints implied by the fact that A and B\nare represented as {N-bit} 2's complement values.  We can assume\n{ without loss of generality}{This common mathematical phrasing\nmeans that we are using a problem symmetry to cut down the length of the\nproof discussion.  In this case, the names A and B aren't particularly\nimportant, since addition is commutative (A+B=B+A).  Thus the proof\nfor the case in which A is negative (and B is not) is identical to the\ncase in which B is negative (and A is not), except that all of the \nnames are swapped.  The term ``without loss of generality'' means that\nwe consider the proof complete even with additional assumptions, in\nour case that A<0 and B.}, or { w.l.o.g.}, \nthat A<0 and B.\n\nCombining these constraints with the range representable \nby {N-bit} 2's complement, we obtain\n\n{eqnarray*}\n-2^{N-1}  & A & < 0\n0  & B & < 2^{N-1}\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n-2^{N-1}  & C & < 2^{N-1}\n{eqnarray*}\n\nBut anything in the range specified by this inequality can be represented\nwith {N-bit} 2's complement, and thus the addition does not overflow.\n\n\nWe are now ready to state our main theorem.  For convenience, \nlet's use different names for the actual sum C=A+B and the sum S\nreturned from the add unit.  We define S as the number represented by\nthe bit pattern produced by the add unit.  When overflow \noccurs, S=C, but we always have (S=C)  2^N.\n\n{ Theorem:} \n\nAddition of two {N-bit} 2's complement numbers A and B\noverflows if and only if one of the following conditions holds:\n\n{A<0 and B<0 and S}\n{A and B and S<0}\n\n\n{ Proof:}\n\nWe once again start with the ``if'' direction.  That is, if condition 1 \nor condition 2 holds, we have an overflow.  The proofs are straightforward.\nGiven condition 1, we can add the two inequalities A<0 and B<0 to \nobtain C=A+B<0.  But S, so clearly S=C, thus overflow \nhas occurred.\n\nSimilarly, if condition 2 holds, we can add the inequalities A\nand B to obtain C=A+B.  Here we have S<0, so again\nS=C, and we have an overflow.\n\nWe must now prove the ``only if'' direction, showing that any overflow\nimplies either condition 1 or condition 2.  By the \n{ contrapositive}{If we have a statement of the form\n(p implies q), its contrapositive is the \nstatement (not q implies not p).\nBoth statements have the same truth value.  In this case, we can turn\nour Lemma around as stated.} of our\nLemma, we know that if an overflow occurs, either both operands are \nnegative, or they are both positive.\n\n\n\n\n\nLet's start with the case in which both operands are negative, so A<0\nand B<0, and thus the real sum C<0 as well.  Given that A and B\nare represented as {N-bit} 2's complement, they must fall in\nthe representable range, so we can write\n\n{eqnarray*}\n-2^{N-1}  & A & < 0\n-2^{N-1}  & B & < 0\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n-2^N  & C & < 0\n{eqnarray*}\n\nGiven that an overflow has occurred, C must fall outside of the \nrepresentable range.  Given that C<0, it cannot be larger than the\nlargest possible number representable using {N-bit} 2's\ncomplement, so we can write\n\n{eqnarray*}\n-2^N  & C & < -2^{N-1}\n{eqnarray*}\n\nWe now add 2^N to each part to obtain\n\n{eqnarray*}\n0  & C + 2^N & < 2^{N-1}\n{eqnarray*}\n\nThis range of integers falls within the representable range \nfor {N-bit} 2's complement, so we can replace the middle\nexpression with S (equal to C modulo 2^N) to find that\n\n{eqnarray*}\n0  & S & < 2^{N-1}\n{eqnarray*}\n\nThus, if we have an overflow and both A<0 and B<0, the resulting\nsum S, and condition 1 holds.\n\nThe proof for the case in which we observe an overflow when \nboth operands are non-negative (A and B)\nis similar, and leads to condition 2.  We again begin with\ninequalities for A and B:\n\n{eqnarray*}\n0  & A & < 2^{N-1}\n0  & B & < 2^{N-1}\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n0  & C < & 2^N\n{eqnarray*}\n\nGiven that an overflow has occurred, C must fall outside of the \nrepresentable range.  Given that C{}0, it cannot be smaller than the\nsmallest possible number representable using {N-bit} 2's\ncomplement, so we can write\n\n{eqnarray*}\n2^{N-1}  & C & < 2^N\n{eqnarray*}\n\nWe now subtract 2^N to each part to obtain\n\n{eqnarray*}\n-2^{N-1}  & C - 2^N & < 0\n{eqnarray*}\n\nThis range of integers falls within the representable range \nfor {N-bit} 2's complement, so we can replace the middle\nexpression with S (equal to C modulo 2^N) to find that\n\n{eqnarray*}\n-2^{N-1}  & S & < 0\n{eqnarray*}\n\nThus, if we have an overflow and both A and B, the resulting\nsum S<0, and condition 2 holds.\n\nThus overflow implies either condition 1 or condition 2, completing our\nproof.\n\n\n\n\n\n \n Question: What does the overflow for 2's complement addition mean? \n Answer: \n\nA. Yes, it does imply an overflow has occurred."}], "fine_tuned_data": [{"text": "Title: Finite State Machine Design Examples, Part I \n Text: {Finite State Machine Design Examples, Part I}\n\nThis set of notes uses a series of examples to illustrate design principles \nfor the implementation of finite state machines (FSMs) using digital logic.\nWe begin with an overview of the design process for a digital FSM, from\nthe development of an abstract model through the implementation of\nfunctions for the next-state variables and output signals.\nOur first few examples cover only the concrete aspects:\nwe implement several counters, which illustrate the basic \nprocess of translating a concrete and complete state transition diagram\ninto an implementation based on flip-flops and logic gates.\nWe next consider a counter with a number of states that is not a power of\ntwo, with which we illustrate the need for FSM initialization.\n As part of solving the initialization problem, we also introduce \n a general form of selection logic called a multiplexer.\n\nWe then consider the design process as a whole through a more general\nexample of a counter with multiple inputs to control its behavior. \nWe work from\nan abstract model down to an implementation, illustrating how semantic\nknowledge from the abstract model can be used to simplify the \nimplementation.  Finally, we illustrate how the choice of representation\nfor the FSM's internal state affects the complexity of the implementation.\nFortunately, designs that are more intuitive and easier for humans to\nunderstand also typically make the best designs in terms of \nother metrics, such as logic complexity.\n\n\n \n Question: What is the implementation of the finite state machine? \n Answer: \n\nA. The design process for a digital FSM typically begins with the development of an abstract model. This model is then used to generate the functions for the next-state variables and output signals. Finally, the design is implemented using flip-flops and logic gates."}, {"text": "Title: Steps in the Design Process \n Text: {Steps in the Design Process}\n\nBefore we begin exploring designs, let's talk briefly about the general\napproach that we take when designing an FSM.  We follow a six-step\nprocess:{-8pt}\n\n{{}{}\n{}{}{}\n{develop an abstract model}{step-abs}\n{specify I/O behavior}{step-io}\n{complete the specification}{step-complete}\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n}\n{-8pt}\n\nIn Step {step-abs}, we translate our description in human language\ninto a model with states and desired behavior.  At this stage, we \nsimply try to capture the intent of the description and are not\nparticularly thorough nor exact.\n\nStep {step-io} begins to formalize the model, starting with its\ninput and output behavior.  If we eventually plan to develop an\nimplementation of our FSM as a digital system (which is not the \nonly choice, of course!), all input and output\nmust consist of bits.  Often, input and/or output specifications\nmay need to match other digital systems to which we plan to connect\nour FSM.  In fact, { most problems in developing large digital systems\ntoday arise because of incompatibilities when composing two or more\nseparately designed pieces} (or { modules}) into an integrated system.\n\nOnce we know the I/O behavior for our FSM, in Step {step-complete}\nwe start to make\nany implicit assumptions clear and to make any other decisions\nnecessary to the design.  Occasionally, we may choose to leave\nsomething undecided in the hope of simplifying the design with\n``don't care'' entries in the logic formulation.\n\nIn Step {step-repn}, we select an internal representation\nfor the bits necessary to encode the state of our FSM.  In practice,\nfor small designs, this representation can be selected by a computer \nin such a way as to optimize the implementation.  However, for large\ndesigns, such as the LC-3 instruction set architecture that we\nstudy later in this class, humans do most of the work by hand.\n\nIn the later examples in this set of notes, we show how even a \nsmall design can\nleverage meaningful information from the design when selecting\nthe representation, leading to an implementation that is simpler\nand is easier to build correctly.\n\nWe also show how one can\nuse abstraction to simplify an implementation.\n\nBy Step {step-logic}, our design is a complete specification in\nterms of bits, and we need merely derive logic expressions for the\nnext-state variables and the output signals.  This process is no\ndifferent than for combinational logic, and should already be fairly \nfamiliar to you.\n\n\n\nFinally, in Step {step-gates}, we translate our logic expressions\ninto gates and use flip-flops (or registers) to hold the internal\nstate bits of the FSM.  In later notes, we use more complex\nbuilding blocks when implementing an FSM, building up abstractions\nin order to simplify the design process in much the same way that\nwe have shown for combinational logic.\n\n\n \n Question: Why do most problems arise in developing large digital systems today? \n Answer: \n\nA. Most problems in developing large digital systems today arise because of incompatibilities when composing two or more\nseparately designed pieces into an integrated system."}, {"text": "Title: Example: A Two-Bit Gray Code Counter \n Text: {Example: A Two-Bit Gray Code Counter}\n\nLet's begin with a two-bit Gray code counter with no inputs.\nAs we mentioned in Notes Set 2.1, a Gray code is a cycle over all\nbit patterns of a certain length in which consecutive patterns differ\nin exactly one bit.\n\nFor simplicity, our first few examples are based on counters and\nuse the internal state\nof the FSM as the output values.  You should already know\nhow to design combinational logic for the outputs if it were necessary.\n\nThe inputs to a counter, if any, are typically limited to functions\nsuch as starting and stopping the counter, controlling the counting \ndirection, and resetting the counter to a particular state.\n\nA fully-specified transition diagram for \na two-bit Gray code counter appears below.\nWith no inputs, the states simply form a loop, with\nthe counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_1S_0 \n(before the ``/'') and the output Z_1Z_0 (after the ``/''), which are \nalways equal for this counter.\n\nBased on the transition diagram, we can fill in the K-maps for the \nnext-state values S_1^+ and S_0^+ as shown to the right of the\ntransition diagram, then \nderive algebraic expressions in the usual way to obtain\nS_1^+=S_0 and S_0^+={{S_1}}.\n\nWe then use the next-state logic to develop the implementation\nshown on the far right, completing our first counter design.\n\n\n\n\n\n\n\n\n\n\n\n\n \n Question: What is a Gray code? \n Answer: \n\nA Gray code is a cycle over all bit patterns of a certain length in which consecutive patterns differ in exactly one bit."}, {"text": "Title: Example: A Three-Bit Gray Code Counter \n Text: {Example: A Three-Bit Gray Code Counter}\n\n\nNow we'll add a third bit to our counter, but again use a Gray code\nas the basis for the state sequence.\n\nA fully-specified transition diagram for such a counter appears to \nthe right.  As before, with no inputs, the states simply form a loop, \nwith the counter moving from one state to the next each cycle.\n\nEach state in the diagram is marked with the internal state value S_2S_1S_0 \n(before ``/'') and the output Z_2Z_1Z_0 (after ``/''). \n\n\n\n\n\n\nBased on the transition diagram, we can fill in the K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+ as shown to the right, then \nderive algebraic expressions.  The results are more complex this \ntime.\n\n\n\n\n\n\n\n\n\nFor our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_0 + S_1 {{S_0}} \nS_1^+ &=& {{S_2}} S_0 + S_1 {{S_0}} \nS_0^+ &=& {{S_2}} {{S_1}} + S_2 S_1\n{eqnarray*}\n\n\nNotice that the equations for S_2^+ and S_1^+ share a common term,\nS_1{{S_0}}.\n\nThis design does not allow much choice in developing good equations for\nthe next-state logic, but some designs may enable you to reduce \nthe design complexity by explicitly identifying and making use of \ncommon algebraic terms and sub-expressions for different outputs.\nIn modern design processes, identifying such opportunities is generally\nperformed by a computer program, but it's important to understand\nhow they arise.  Note that the common term becomes a single AND gate\nin the implementation of our counter, as shown to the right.\n\nLooking at the counter's implementation diagram, notice that the vertical\nlines carrying the current state values and their inverses back to the\nnext state\nlogic inputs have been carefully ordered to simplify\nunderstanding the diagram.  In particular, they are ordered from\nleft to right (on the left side of the figure) as \n{{S_0}}S_0{{S_1}}S_1{{S_2}}S_2.\nWhen designing any logic diagram, be sure to make use of a reasonable\norder so as to make it easy for someone (including yourself!) to read \nand check the correctness of the logic.\n\n\n\n\n\n\n \n Question: What is the output code of a three-bit gray code counter? \n Answer: \n\nA.\n\nA three-bit gray code counter works by cycling through a sequence of states, with each state corresponding to a different output code. The output code is determined by the state of the three flip-flops in the counter. The output code is a gray code, which means that only one bit changes between successive states."}, {"text": "Title: Example: A Color Sequencer \n Text: {Example: A Color Sequencer}\n\n\nEarly graphics systems used a three-bit red-green-blue (RGB) \nencoding for colors.  The color mapping for such a system is shown to\nthe right.\n\nImagine that you are charged with creating a counter to drive a light\nthrough a sequence of colors.  The light takes an RGB input as just\ndescribed, and the desired pattern is\n\n{off (black)     yellow     violet     green     blue}\n\nYou immediately recognize that you merely need a counter with five\nstates.  How many flip-flops will we need?  At least three, since\n_2 (5)=3.  Given that we need three flip-flops, \nand that the colors we need to produce as\n\n\n{c|l}\nRGB& color \n000& black\n001& blue\n010& green\n011& cyan\n100& red\n101& violet\n110& yellow\n111& white\n\n\n\noutputs are all unique\nbit patterns, we can again choose to use the counter's internal \nstate directly as our output values.\n\n\nA fully-specified transition diagram for our color sequencer\nappears to the right.  The states again form a loop,\nand are marked with the internal state value S_2S_1S_0 \nand the output RGB.\n\n\n\n\n\n\nAs before, we can use the transition diagram to fill in K-maps for the \nnext-state values S_2^+, S_1^+, and S_0^+, as shown to the right.\nFor each of the three states not included in our transition diagram,\nwe have inserted x's\n\n\n\n\n\n\n\n\n\ninto the K-maps to indicate ``don't care.'' \nAs you know, we can treat each x as either a 0 or a 1, whichever\nproduces better results (where ``better'' usually means simpler \nequations).  The terms that we have chosen for our algebraic \nequations are illustrated in the K-maps.  The x's within the ellipses\nbecome 1s in the implementation, and the x's outside of the ellipses\nbecome 0s.\n\n\nFor our next-state logic, we obtain:\n{eqnarray*}\nS_2^+ &=& S_2 S_1 + {{S_1}} {{S_0}} \nS_1^+ &=& S_2 S_0 + {{S_1}} {{S_0}} \nS_0^+ &=& S_1\n{eqnarray*}\n\nAgain our equations for S_2^+ and S_1^+ share a common term,\nwhich becomes a single AND gate in the implementation shown to the\nright.\n\n\n\n\n\n \n Question: Is it possible to create a color sequencer? \n Answer: \n\nA. No, it is not possible."}, {"text": "Title: Identifying an Initial State \n Text: {Identifying an Initial State}\n\nLet's say that you go the lab and build the implementation above, \nhook it up\nto the light, and turn it on.  Does it work?  Sometimes.\nSometimes it works perfectly, but sometimes\nthe light glows cyan or red briefly first.\nAt other times, the light is an\nunchanging white.\n\n\nWhat could be going wrong?\n\nLet's try to understand.  We begin by deriving\nK-maps for the implementation, as shown to the right.  In these\nK-maps, each of the x's in our design has been replaced by either a 0\nor a 1.  These entries are highlighted with green italics.\n\n\n{file=part3/figs/colS2-bad.eps,width=1.00in}\n\n{file=part3/figs/colS1-bad.eps,width=1.00in}\n\n{file=part3/figs/colS0-bad.eps,width=1.00in}\n\n\nNow let's imagine what might happen if somehow our FSM got into the\nS_2S_1S_0=111 state.  In such a state, the light would appear white,\nsince RGB=S_2S_1S_0=111.\n\nWhat happens in the next cycle?\n\nPlugging into the equations or looking into the K-maps gives (of\ncourse) the same answer: the next state is the\nS_2^+S_1^+S_0^+=111 state.\nIn other words, the light stays white indefinitely!\n\nAs an exercise, you should check what happens \nif the light is red or cyan.\n\nWe can extend the transition diagram that we developed for our design\nwith the extra states possible in the implementation, as shown below.\nAs with the five states in the design, the extra states are named with\nthe color of light that they produce.\n\n{{file=part3/figs/colors-full.eps,width=5.8in}}\n\nNotice that the FSM does not move out of the WHITE state (ever).  \n\nYou may at this point wonder whether more careful decisions \nin selecting our next-state expressions might address this issue.\nTo some extent, yes.  For example, if we replace the \nS_2S_1 term in the equation for S_2^+ with S_2{{S_0}}, \na decision allowed\nby the ``don't care'' boxes in the K-map for our design,\nthe resulting transition diagram does not suffer from the problem\nthat we've found.\n\nHowever, even if we do change our implementation slightly, we need\nto address another aspect of the problem:\n\nhow can the FSM ever get into the unexpected states?\n\n\nWhat is the initial state of the three flip-flops in our implementation?\n\n{ The initial state may not even be 0s and 1s unless we have an \nexplicit mechanism for initialization.} \n\nInitialization can work in two ways.  \n\nThe first approach makes use of the flip-flop design.\nAs you know, a flip-flop is built from a pair of latches, and\nwe can \nmake use of the internal reset lines on these latches\nto force each flip-flop into the 0 state (or the 1 state) using an\nadditional input. \n\nAlternatively, we can add some extra logic to our design.\n\nConsider adding a few AND gates and a  input\n(active low), as shown in the dashed box in the figure to the right.\nIn this case, when we assert  by setting it to 0,\nthe FSM moves to state 000 in the next cycle, putting it into\nthe BLACK state.  The approach taken here is for clarity; one can\noptimize the design, if desired.  For example, we could simply connect\n as an extra input into the three AND gates on the\nleft rather than adding new ones, with the same effect.\n\nWe may sometimes want a more powerful initialization mechanism---one\nthat allows us to force the FSM into any specific state in the next\ncycle.  In such a case, we can add multiplexers to each of our \nflip-flop inputs, allowing us to use the INIT input to choose between\nnormal operation (INIT=0) of the FSM and forcing the FSM into the\nnext state given by I_2I_1I_0 (when INIT=1).\n\n\n\n\n\n\n\n\n \n Question: What is the INIT input? \n Answer: \n\nA. The INIT input allows us to force the FSM into a specific state in the next cycle."}, {"text": "Title: Developing an Abstract Model \n Text: {Developing an Abstract Model}\n\n\nWe are now ready to discuss the design process for an FSM from start\nto finish.\n\nFor this first abstract FSM example, we build upon something\nthat we have already seen: a two-bit Gray code counter.\nWe now want a counter that allows us to start and stop the\n\n\n{c|ccc}\nstate&    no input&  halt button& go button \ncounting& counting&      halted& \nhalted&   halted&              & counting\n\n\n\ncount.\n\nWhat is the mechanism for stopping and starting?  To\nbegin our design, we could sketch out an abstract next-state\ntable such as the one shown to the right above.  In this form of the table,\nthe first column lists the states, while each of the other columns lists\nstates to which the FSM transitions after a clock cycle for a particular\ninput combination. \n\nThe table contains two states, counting and halted, and specifies\nthat the design uses two distinct buttons to move between the\nstates.\nThe table further implies that if the counter is halted,\nthe ``halt'' button has no additional effect, and if the counter\nis counting, the ``go'' button has no additional effect.\n\n\nA counter with a single counting state, of course, does not provide\nmuch value.  We extend the table with four counting states and four\nhalted states, as shown to the right.  This version of the\ntable also introduces more formal state names, for which these notes \nuse all capital letters.\n\nThe upper four states represent uninterrupted counting, in which \nthe counter cycles through these states indefinitely.\n\nA user can stop the counter in any state by pressing the ``halt''\nbutton, causing the counter to retain its current value until the\nuser presses the ``go'' button.\n\nBelow the state table is an abstract transition diagram, which provides\nexactly the same information in graphical form.  Here circles represent\nstates (as labeled) and arcs represent transitions from one state\nto another based on an input combination (which is used to label the\narc).\n\nWe have already implicitly made a few choices about our counter design.\n\nFirst, the counter\n\n\n{c|ccc}\nstate&    no input&  halt button& go button \n{ COUNT A}& { COUNT B}& { HALT A}& \n{ COUNT B}& { COUNT C}& { HALT B}& \n{ COUNT C}& { COUNT D}& { HALT C}& \n{ COUNT D}& { COUNT A}& { HALT D}& \n{ HALT A}&  { HALT A}&              & { COUNT B}\n{ HALT B}&  { HALT B}&              & { COUNT C}\n{ HALT C}&  { HALT C}&              & { COUNT D}\n{ HALT D}&  { HALT D}&              & { COUNT A}\n\n\n\n\nshown retains the current state of the system when\n``halt'' is pressed.\nWe could instead reset the counter state whenever it\nis restarted, in which case we need only five states: four for\ncounting and one more for a halted counter.\n\nSecond, we've designed the counter to stop\nwhen the user presses ``halt'' and to resume counting \nwhen the user presses ``go.''  We could instead choose to delay these \neffects by a cycle.  For example, pressing ``halt'' in state { COUNT B}\ncould take the counter to state { HALT C}, and pressing ``go'' \nin state { HALT C} could take the system to state { COUNT C}.\n\nIn these notes, we implement only the diagrams shown.\n\n \n Question: What is the mechanism for stopping and starting? \n Answer: \n\nA. The mechanism for stopping and starting is that a user can stop the counter in any state by pressing the \"halt\" button, causing the counter to retain its current value until the user presses the \"go\" button."}, {"text": "Title: Specifying I/O Behavior \n Text: {Specifying I/O Behavior}\n\n\nWe next start to formalize our design by specifying its input and \noutput behavior digitally.  Each of the two control buttons provides\na single bit of input.  The ``halt'' button we call H, and the\n``go'' button we call G.\n\nFor the output, we use a two-bit \nGray code.  With these choices, we can redraw the transition diagram \nas show to the right.\n\nIn this figure, the states are marked with output values Z_1Z_0 and\ntransition arcs are labeled in terms of our two input buttons, G and H.  \nThe uninterrupted counting cycle is labeled with \nto indicate that it continues until we press H.\n\n\n\n \n\n \n Question: What is the purpose of the two-bit Gray code? \n Answer: \n\nA. The purpose of the two-bit Gray code is to allow for easy determination of the output value when the input value changes."}, {"text": "Title: Completing the Specification \n Text: {Completing the Specification}\n\nNow we need to think about how the system should behave if something \noutside of our initial expectations occurs.  Having drawn out a partial\ntransition diagram can help with this process, since we can use the\ndiagram to systematically consider all possible input conditions from\nall possible states.  The state table form can make the missing\nparts of the specification even more obvious.\n\n\n\n\nFor our counter, the symmetry between counting states makes the problem \nsubstantially simpler.  Let's write out part of a list of states and\npart of a state table with one \ncounting state and one halt state, as shown to the right.\nFour values of the inputs HG \nare possible (recall that N bits allow 2^N possible patterns).\nWe list the columns in Gray code order, since we may want to\ntranscribe this table into K-maps later.\n\n\n{\n\n& \nfirst counting state& { COUNT A}& counting, output Z_1Z_0=00\n  first halted state&  { HALT A}& halted, output Z_1Z_0=00\n\n\n{c|cccc}\n&{HG}\n        state&            00&            01&          11&           10 \n{ COUNT A}& { COUNT B}&   unspecified& unspecified& { HALT A}\n { HALT A}&  { HALT A}& { COUNT B}& unspecified&  unspecified\n\n\n\nLet's start with the { COUNT A} state.  \n\nWe know that if neither button is pressed (HG=00), we want \nthe counter to move to the { COUNT B} state.  And, if we press the\n``halt'' button (HG=10), we want the counter to move to the { HALT A}\nstate.  What should happen if a user presses the ``go'' button (HG=01)?\nOr if the user presses both buttons (HG=11)?\n\nAnswering these questions is part of fully specifying our design.  We\ncan choose to leave some parts unspecified, but { any implementation of\nour system will imply answers}, and thus we must be careful.\n\nWe choose to ignore the ``go'' button while counting, and to have the\n``halt'' button override the ``go'' button.  Thus, if HG=01 when the\ncounter is in state { COUNT A}, the counter moves to state { COUNT B}.\nAnd, if HG=11, the counter moves to state { HALT A}.\n\nUse of explicit bit patterns for the inputs HG may help you to check \nthat all four possible input values are covered from each state.  If \nyou choose to use a transition diagram instead of a state table,\nyou might even want to add four arcs from each state, each labeled \nwith a specific\nvalue of HG.  When two arcs connect the same two states, we can either \nuse multiple labels or can indicate bits that do not matter using a\n{ don't-care} symbol, x.  For example, the arc from state { COUNT A}\nto state { COUNT B} could be labeled HG=00,01 or HG=0x.  The\narc from state { COUNT A} to state { HALT A} could be labeled\nHG=10,11 or HG=1x.  We can also use logical expressions as labels,\nbut such notation can obscure unspecified transitions.\n\nNow consider the state { HALT A}.  The transitions specified so far\nare that when we press ``go'' (HG=01), the counter moves to \nthe { COUNT B} state, and that the counter remains halted in \nstate { HALT A} if no buttons are pressed (HG=00).\nWhat if the ``halt'' button is pressed (HG=10), or\nboth buttons are pressed (HG=11)?  For consistency, we decide that\n``halt'' overrides ``go,'' but does nothing special if it alone is pressed\nwhile the counter is halted.  Thus, input patterns HG=10 and HG=11 also \ntake state { HALT A} back to itself.\nHere the arc could be labeled HG=00,10,11 or, equivalently,\nHG=00,1x or HG=x0,11.\n\n\nTo complete our design, we apply the same decisions that we made for \nthe { COUNT A} state to all of the other counting states, and the \ndecisions that we made for the { HALT A} state to all of the other \nhalted states.  If we had chosen not to specify an answer, an implementation\ncould produce different behavior from the different counting\nand/or halted states, which might confuse a user.\n\nThe resulting design appears to the right.\n\n\n\n \n\n\n \n Question: What is important to specify all possible inputs for each state? \n Answer: \n\nAns. It is important to specify all possible inputs for each state because any implementation of the system will imply answers to these inputs, and thus we must be careful."}, {"text": "Title: Choosing a State Representation \n Text: {Choosing a State Representation}\n\nNow we need to select a representation for the states.  Since our counter\nhas eight states, we need at least three (_2 (8)=3)\nstate bits S_2S_1S_0 to keep track of the current state.\n\nAs we show later, { the choice of representation for an FSM's states\ncan dramatically affect the design complexity}.  For a design as simple as \nour counter, you could just let a computer implement all possible \nrepresentations (there aren't more than 840, if we consider simple \nsymmetries) and select one according to whatever metrics are interesting.\n\nFor bigger designs, however, the number of possibilities quickly becomes\nimpossible to explore completely.\n\nFortunately, { use of abstraction in selecting a representation \nalso tends to produce better designs} for a wide variety of metrics\n(such as design complexity, area, power consumption, and performance).\n\nThe right strategy is thus often to start by selecting a representation \nthat makes sense to a human, even if it requires more bits than are\nstrictly necessary.  The\nresulting implementation will be easier to\ndesign and to debug than an implementation in which only the global \nbehavior has any meaning.\n\n\nLet's return to our specific example, the counter.  We can use one bit, \nS_2, to record whether or not our counter is counting (S_2=0) or\nhalted (S_2=1).  The other two bits can then record the counter state\nin terms of the desired output.  Choosing this representation\nimplies that only wires will be necessary to compute outputs Z_1 \nand Z_0 from the internal state: Z_1=S_1 and Z_0=S_0.  The resulting\ndesign, in which states are now labeled with both internal state and\noutputs (S_2S_1S_0/Z_1Z_0) appears to the right.  In this version,\nwe have changed the arc labeling to use logical expressions, which\ncan sometimes help us to think about the implementation.\n\n\n\n\n\nThe equivalent state listing and state table appear below.  We have ordered\nthe rows of the state table in Gray code order to simplify transcription\nof K-maps.\n\n\n\n& S_2S_1S_0& \n{ COUNT A}& 000& counting, output Z_1Z_0=00\n{ COUNT B}& 001& counting, output Z_1Z_0=01\n{ COUNT C}& 011& counting, output Z_1Z_0=11\n{ COUNT D}& 010& counting, output Z_1Z_0=10\n { HALT A}& 100& halted, output Z_1Z_0=00\n { HALT B}& 101& halted, output Z_1Z_0=01\n { HALT C}& 111& halted, output Z_1Z_0=11\n { HALT D}& 110& halted, output Z_1Z_0=10\n\n\n{rc|cccc}\n&&{HG}\n&S_2S_1S_0& 00& 01& 11& 10 \n{ COUNT A}&000& 001& 001& 100& 100\n{ COUNT B}&001& 011& 011& 101& 101\n{ COUNT C}&011& 010& 010& 111& 111\n{ COUNT D}&010& 000& 000& 110& 110\n { HALT D}&110& 110& 000& 110& 110\n { HALT C}&111& 111& 010& 111& 111\n { HALT B}&101& 101& 011& 101& 101\n { HALT A}&100& 100& 001& 100& 100\n\n\n\nHaving chosen a representation, we can go ahead and implement our\ndesign in the usual way.  As shown to the right, K-maps for the \nnext-state logic are complicated, since we have five variables\nand must consider implicants that are not contiguous in the K-maps.\nThe S_2^+ logic is easy enough: we only need two terms, \nas shown.\n\nNotice that we have used color and\nline style to distinguish different\n\n\n\n\n\n\n\n\n\nimplicants in the K-maps.  Furthermore, the symmetry of the design\nproduces symmetry in the S_1^+ and S_0^+ formula, so we have\nused the same color and line style for analogous terms in these\ntwo K-maps.\n\nFor S_1^+, we need four terms.  The green \nellipses in the HG=01 column are part of the same term, as are\nthe two halves of the dashed blue circle.  In S_0^+, we still\nneed four terms, but three of them are split into two pieces \nin the K-map.  As you can see, the utility of the K-map is starting\nto break down with five variables.\n\n\n \n Question: What is the right strategy for a counter? \n Answer: \n\nA. The right strategy is thus often to start by selecting a representation that makes sense to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning."}, {"text": "Title: Abstracting Design Symmetries \n Text: {Abstracting Design Symmetries}\n\nRather than implementing the design as two-level logic, let's try to\ntake advantage of our design's symmetry to further simplify the\nlogic (we reduce gate count at the expense of longer, slower paths).\n\nLooking back to the last transition diagram, in which the arcs\nwere labeled with logical expressions, let's calculate an expression\nfor when the counter should retain its current value in the next\ncycle.  We call \nthis variable HOLD.  In the counting states, when S_2=0, \nthe counter stops (moves into a halted state without changing value) \nwhen H is true.\nIn the halted states, when S_2=1, the counter stops (stays in \na halted state) when H+ is true.  We can thus write\n\n{eqnarray*}\nHOLD &=& {S_2}  H + S_2  ( H +  )\nHOLD &=& {S_2} H + S_2 H + S_2 \nHOLD &=& H + S_2 \n{eqnarray*}\n\nIn other words, the counter should hold its current \nvalue (stop counting) if we press the ``halt'' button or if the counter\nwas already halted and we didn't press the ``go'' button.  As desired,\nthe current value of the counter (S_1S_0) has no impact on this \ndecision.  You may have noticed that the expression we derived for\nHOLD also matches S_2^+, the next-state value of S_2 in the \nK-map on the previous page.\n\nNow let's re-write our state transition table in terms of HOLD.  The\nleft version uses state names for clarity; the right uses state values\nto help us transcribe K-maps.\n\n{\n{rc|cc}\n&&{HOLD}\n&S_2S_1S_0& 0& 1 \n{ COUNT A}&000& { COUNT B}& { HALT A}\n{ COUNT B}&001& { COUNT C}& { HALT B}\n{ COUNT C}&011& { COUNT D}& { HALT C}\n{ COUNT D}&010& { COUNT A}& { HALT D}\n { HALT A}&100& { COUNT B}& { HALT A}\n { HALT B}&101& { COUNT C}& { HALT B}\n { HALT C}&111& { COUNT D}& { HALT C}\n { HALT D}&110& { COUNT A}& { HALT D}\n\n{rc|cc}\n&&{HOLD}\n&S_2S_1S_0& 0& 1 \n{ COUNT A}&000& 001& 100\n{ COUNT B}&001& 011& 101\n{ COUNT C}&011& 010& 111\n{ COUNT D}&010& 000& 110\n { HALT A}&100& 001& 100\n { HALT B}&101& 011& 101\n { HALT C}&111& 010& 111\n { HALT D}&110& 000& 110\n\n\n\nThe K-maps based on the HOLD abstraction are shown to the right.\nAs you can see, the necessary logic has been simplified substantially,\nrequiring only two terms each for both S_1^+ and S_0^+.  Writing\nthe next-state logic algebraically, we obtain\n\n{eqnarray*}\nS_2^+ &=& HOLD\nS_1^+ &=&   S_0 + HOLD  S_1\nS_0^+ &=&   {{S_1}} + HOLD  S_0\n{eqnarray*}\n\n\n\n\n\n\n\n\n\nNotice the similarity between the equations for S_1^+S_0^+ and the \nequations for a {2-to-1} mux: when HOLD=1, the counter retains \nits state, and when HOLD=0, it counts.\n\n\n\n\n\nAn implementation appears below.\n\nBy using semantic meaning in our choice of representation---in\nparticular the use of S_2 to record whether\nthe counter is currently halted (S_2=1) or counting (S_2=0)---we\nhave enabled ourselves to \nseparate out the logic for deciding whether to advance the counter\nfairly cleanly from the logic for advancing the counter itself.\nOnly the HOLD bit in the diagram is used to determine\nwhether or not the counter should advance in the current cycle.\n\nLet's check that the implementation matches our original design.\n\nStart by verifying that the HOLD variable is calculated correctly,\nHOLD=H+S_2,\nthen look back at the K-map for S_2^+ in the low-level design to\nverify that the expression we used does indeed match.\n\n\n\nNext, check the mux abstraction.\n\nWhen HOLD=1, the next-state logic for S_1^+ and S_0^+ \nreduces to S_1^+=S_1 and S_0^+=S_0;\nin other words, the counter stops counting and simply stays in its \ncurrent state.  When HOLD=0, these equations become\nS_1^+=S_0 and S_0^+={{S_1}}, which produces the repeating\nsequence for S_1S_0 of 00, 01, 11, 10, as desired.\nYou may want to look back at our two-bit Gray code counter design\nto compare the next-state equations.\n\nWe can now verify that the implementation produces the correct transition\nbehavior.  In the counting states, S_2=0, and the HOLD value simplifies\nto HOLD=H.  Until we push the ``halt'' button, S_2 remains 0, and\nand the counter continues to count in the correct sequence.\nWhen H=1, HOLD=1, and the counter stops at its current value\n(S_2^+S_1^+S_0^+=1S_1S_0, \nwhich is shorthand for S_2^+=1, S_1^+=S_1, and S_0^+=S_0).\n\nIn any of the halted states, S_2=1, and we can reduce HOLD to\nHOLD=H+.  Here, so long as we press the ``halt'' button\nor do not press the ``go'' button, the counter stays in its current\nstate, because HOLD=1.  If we release ``halt'' and press ``go,''\nwe have HOLD=0, and the counter resumes counting\n(S_2^+S_1^+S_0^+=0S_0{{S_1}},\nwhich is shorthand for S_2^+=0, S_1^+=S_0, and \nS_0^+={{S_1}}).\n\nWe have now verified the implementation.\n\nWhat if you wanted to build a three-bit Gray code counter with the same\ncontrols for starting and stopping?  You could go back to basics and struggle \nwith six-variable {K-maps}.  Or you could simply copy the HOLD \nmechanism from the two-bit design above, insert muxes between the next \nstate logic and the flip-flops of the three-bit Gray code counter that \nwe designed earlier, and control the muxes with the HOLD bit.  \nAbstraction is a powerful tool.\n\n\n\n \n Question: What is the value of HOLD? \n Answer: \n\nA. The value of HOLD is 1."}, {"text": "Title: Impact of the State Representation \n Text: {Impact of the State Representation}\n\nWhat happens if we choose a bad representation?  For the same FSM---the\ntwo-bit Gray code counter with start and stop inputs---the \ntable below shows a poorly chosen mapping from states to internal \nstate representation.\n\nBelow the table is a diagram of an implementation using that\nrepresentation.\n\nVerifying that the implementation's behavior\nis correct is left as an exercise for the determined reader.\n\n{\n{|c|c|c|c|c|}{1-2}{4-5}\nstate& S_2S_1S_0& & state& S_2S_1S_0  {1-2}{4-5}\n{ COUNT A}& 000& & { HALT A}& 111 \n{ COUNT B}& 101& & { HALT B}& 110 \n{ COUNT C}& 011& & { HALT C}& 100 \n{ COUNT D}& 010& & { HALT D}& 001  {1-2}{4-5}\n\n}\n\n\n\n\n\n\n\n\n \n Question: What is the impact of changing the state representation? \n Answer: \n\nA. Changing the state representation can impact the design of the FSM in several ways. For example, if the states are represented by numbers instead of by letters, the transition table and state diagram will be different. In addition, the implementation of the FSM may be more complex if the states are represented by numbers instead of by letters."}, {"text": "Title: Random Access Memories \n Text: {Random Access Memories}\n\nThis set of notes describes random access memories (RAMs), providing slightly\nmore detail than is available in the textbook.  We begin with a discussion\nof the memory abstraction and the types of memory most commonly used in\ndigital systems, then examine how one can build memories (static RAMs) \nusing logic.  We next introduce tri-state buffers as a way of simplifying\nouput connections, and illustrate how memory chips can be combined to\nprovide larger and wider memories.  A more detailed description of dynamic \nRAMs finishes this set.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}\n\n \n Question: What type of RAMs are dynamic? \n Answer: \n\nA. Static RAMs store data in a flip-flop, while dynamic RAMs store data in a capacitor."}, {"text": "Title: Memory \n Text: \n\nA computer { memory} is a group of storage elements and the logic\nnecessary to move data in and out of the elements.  The size of the\nelements in a memory---called the { addressability} of the \nmemory---varies from a single binary digit, or { bit},\nto a { byte} (8 bits) or more.  Typically, we refer to data\nelements larger than a byte as { words}, but the size of a word\ndepends on context. \n\nEach element in a memory is assigned a unique name, called an {\naddress}, that allows an external circuit to identify the particular\nelement of interest.  These addresses are not unlike the street\naddresses that you use when you send a letter.  Unlike street\naddresses, however, memory addresses usually have little or no\nredundancy; each possible combination of bits in an address identifies\na distinct set of bits in the memory.  The figure on the right below \nillustrates the concept.  Each house represents a storage element and \nis associated with a unique address.\n\n{{file=part3/figs/lec18-1.eps,width=4in}}\n\nThe memories that we consider in this class have several properties in\ncommon.  These memories support two operations: { write} places a\nword of data into an element, and { read} retrieves a copy of a\nword of data from an element.  The memories are also { volatile},\nwhich means that the data held by a memory are erased when electrical\npower is turned off or fails.  { Non-volatile} forms of memory\ninclude magnetic and optical storage media such as DVDs, CD-ROMs, disks, \nand tapes, capacitive storage media such as Flash drives,\nand some programmable logic devices.\nFinally, the memories considered in this class are { random access\nmemories (RAMs)}, which means that the time required to access an\nelement in the memory is independent of the element being accessed.\nIn contrast, { serial memories} such as magnetic tape require much\nless time to access data near the current location in the tape than\ndata far away from the current location.\n\nThe figure on the left above shows a generic RAM structure.  The\nmemory contains 2^k elements of N bits each.  A {k-bit}\naddress input, ADDR, identifies the memory element of interest for\nany particular operation.  The write enable\ninput, WE, selects the operation to be performed: if\nWE is high, the operation is a write; if it is low, the\noperation is a read.  Data to be written into an element are provided\nthrough N inputs at the top, and data read from an element appear on\nN outputs at the bottom.  Finally, a { chip select} input, CS,\nfunctions as an enable control for the memory; when CS is low, the\nmemory neither reads nor writes any location.\n\nRandom access memory further divides into two important types: {\nstatic RAM}, or { SRAM}, and { dynamic RAM}, or { DRAM}.\nSRAM employs active logic in the form of a two-inverter loop to\nmaintain stored values.  DRAM uses a charged capacitor to store a bit;\nthe charge drains over time and must be replaced, giving rise to the\nqualifier ``dynamic.''  ``Static'' thus serves only to differentiate\nmemories with active logic elements from those with capacitive\nelements.  Both types are volatile, that is, both lose all data when the\npower supply is removed.  We consider both SRAM and DRAM \nin this course, but the details of DRAM operation are beyond our scope. \n\n\n \n Question: What type of memory does SRAM use? \n Answer: \n\nSRAM uses active logic elements to store data, while DRAM uses capacitive elements."}, {"text": "Title: Static Random Access Memory \n Text: {Static Random Access Memory}\n\nStatic random access memory is used for high-speed applications such\nas processor caches and some embedded designs.  As SRAM bit\n{density---the} number of bits in a given chip {area---is}\nsignificantly lower than DRAM bit density, most applications with less\ndemanding speed requirements use DRAM.  The main memory in most\ncomputers, for example, is DRAM, whereas the memory on the same chip\nas a processor is SRAM.{Chips combining both DRAM and processor\nlogic are available, and are used by some processor manufacturers (such \nas IBM).  Research is underway to couple such logic types more efficiently\nby building 3D stacks of chips.}  DRAM is also unavailable\nwhen recharging its capacitors, which can be a problem for\napplications with stringent real-time needs.\n\n\nA diagram of an SRAM { cell} (a single bit) appears to\nthe right.  A dual-inverter loop stores the bit, and is connected\nto opposing BIT lines through transistors controlled by a SELECT\nline.  \n\nThe cell works as follows.  When SELECT is high, the\ntransistors connect the inverter loop to the bit lines.  When writing\na cell, the bit lines are held at opposite logic values, forcing the\ninverters to match the values on the lines and storing the value from\nthe BIT input.  When reading a cell, the bit lines are disconnected\nfrom other logic, allowing the inverters to drive the lines with\ntheir current outputs.  \n\n\n{file=part3/figs/lec18-2.eps,width=2.20in}\n\n\nThe value stored previously is thus copied onto\nthe BIT line as an output, and the opposite value is placed on the\n line.  When SELECT is low, the transistors\ndisconnect the inverters from the bit lines, and the cell\nholds its current value until SELECT goes high again.\n\nThe actual operation of an SRAM cell is more complicated than we\nhave described.  For example, when writing a bit, the BIT lines \ncan temporarily connect high voltage to ground (a short).  The \ncircuit must be designed carefully to minimize the power consumed\nduring this process.  When reading a bit, the BIT lines\nare pre-charged halfway between high-voltage and ground, and \nanalog devices called sense amplifiers are used to detect the\nvoltage changes on the BIT lines (driven by the inverter loop)\nas quickly as possible.  These analog design issues are outside of \nthe scope of our class.\n\n\nA number of cells are combined into a { bit slice}, as shown to\nthe right.\n\nThe labels along the bottom of the figure are external inputs to the \nbit slice, and match the labels for the abstract\n\n\n{file=part3/figs/lec18-3.eps,width=5in}\n\n\nmemory discussed earlier.  The \nbit slice in the figure can be thought of as a {16-address},\n{1-bit-addressable} memory (2^4b).\n\nThe cells in a bit slice\nshare bit lines and analog read and write logic, which appears to the\nright in the figure.  Based on the ADDR input, a decoder sets one\ncell's SELECT line high to enable a read or write operation to the\ncell.  \n\nThe chip select input CS drives the enable input of\nthe decoder, so none of the memory cells is active when chip select is\nlow (CS=0), and exactly one of the memory cells is active when\nchip select is high (CS=1).\n\nActual bit slices can contain many more cells than are shown in the \nfigure---more cells means less extra logic per cell, but slower memory,\nsince longer wires have higher capacitance.\n\nA read operation is performed as follows.  We set CS=1 and WE=0,\nand place the address of the cell to be read on the ADDR input.\nThe decoder outputs a 1 on the appropriate cell's SELECT line,\nand the read logic reads the bit from the cell and delivers it\nto its Q output, which is then available on the bit \nslice's {DATA-OUT} output.\n\nFor a write operation, we set CS=1 and WE=1.  We again place the\naddress of the cell to be written on the ADDR input and set the\nvalue of the bit slice's {DATA-IN} input to the value to be written\ninto the memory cell.  When the decoder activates the cell's SELECT line,\nthe write logic writes the new value from its D input into\nthe memory cell.  Later reads from that cell then produce the new value.\n\n{{file=part3/figs/lec18-4.eps,width=6.15in}}\n\nThe outputs of the cell selection decoder can be used to control\nmultiple bit slices, as shown in the figure above of a {2^6b}\nmemory.  Selection between bit slices is\nthen based on other bits from the address (ADDR).  In the figure\nabove, a {2-to-4} decoder is used to deliver write requests to\none of four bit slices, and a {4-to-1} mux is used to choose\nthe appropriate output bit for read requests.\n\nThe {4-to-16} decoder now activates one cell in each of the four \nbit slices.  For a read operation, WE=0, and the {2-to-4} decoder \nis not enabled, so it outputs all 0s.  All four bit slices thus perform\nreads, and the desired result bit is forwarded to {DATA-OUT} by the \n{4-to-1} mux.  The tri-state buffer between the mux \nand {DATA-OUT} is explained in a later section.\n\nFor a write operation, exactly one of the bit\nslices has its WE input set to 1 by the {2-to-4} decoder.\nThat bit slice writes the bit value delivered to all bit slices\nfrom {DATA-IN}.  The other three bit slices perform reads, but their \nresults are simply discarded.\n\nThe approach shown above, in which a cell is selected\nthrough a two-dimensional indexing scheme, is known as { coincident\nselection}.  The qualifier ``coincident'' arises from the notion that\nthe desired cell coincides with the intersection of the active row and\ncolumn outputs from the decoders.\n\nThe benefit of coincident selection is easily calculated in terms of\nthe number of gates required for the decoders.  Decoder complexity is\nroughly equal to the number of outputs, as each output is a minterm\nand requires a unique gate to calculate it.  \n\n Fanout trees for input terms and inverted terms add relatively few gates.  \n\nConsider a 1M8b RAM chip.  The number of addresses is 2^,\nand the total number of memory cells is 8,388,608 (2^).\nOne option is to use eight bit slices and a {20-to-1,048,576}\ndecoder, or about 2^ gates.  Alternatively, we can use 8,192 bit\nslices of 1,024 cells.  For the second implementation, we need \ntwo {10-to-1024} decoders, or about 2^ gates.  As chip \narea is roughly proportional to the number of gates, the savings are \nsubstantial.  Other schemes are possible as well: if we want a more \nsquare chip area, we might choose to use 4,096 bit slices of 2,048 \ncells along with one {11-to-2048} decoder and\none {9-to-512} decoder.  This approach requires roughly 25 more\ndecoder gates than our previous example, but is still far superior to\nthe eight-bit-slice implementation.\n\nMemories are typically unclocked devices.  However, as you have seen,\nthe circuits are highly structured, which enables engineers to cope\nwith the complexity of sequential feedback design.  Devices used to\ncontrol memories are typically clocked, and the interaction between\nthe two can be fairly complex.  \n\n\nTiming diagrams for reads and writes\nto SRAM are shown to the right.  A write operation\nappears on the left.  In the first cycle, the controller raises the\nchip select signal and places the memory address to be written on the\naddress inputs.  Once the memory has had time to set up the \nappropriate\n\n\n{file=part3/figs/lec18-6.eps,width=4in}\n\n\nselect lines\ninternally, the WE input is raised, and data are placed\non the data inputs.  The delay, which is specified by the memory\nmanufacturer, is necessary to avoid writing data to the incorrect\nelement within the memory.  The timing shown in the\nfigure rounds this delay up to a single clock cycle, but the\nactual delay needed depends on the clock speed and the memory's \nspecification.  At some point after new data have been\ndelivered to the memory, the write operation completes within the\nmemory.  The time from the application of the address until the\n(worst-case) completion of the write operation is called the {\nwrite cycle} of the memory, and is also specified by the memory \nmanufacturer.  Once the write cycle has passed, the controlling logic \nlowers WE, waits for the change to settle within the memory,\nthen removes the address and lowers the chip select signal.  The\nreason for the delay between these signal changes is the same: to \navoid mistakenly overwriting another memory location.\n\nA read operation is quite similar.  As shown on the right, the\ncontrolling logic places the address on the input lines and raises the\nchip select signal.  No races need be considered, as read operations\non SRAM do not affect the stored data.  After a delay called the {\nread cycle}, the data can be read from the data outputs.  The address\ncan then be removed and the chip select signal lowered.\n\nFor both reads and writes, the number of cycles required for an\noperation depends on a combination of the clock cycle of the\ncontroller and the cycle time of the memory.  For example, with a\n25 nanosecond write cycle and a 10 nanosecond clock cycle, a write\nrequires three cycles.  In general, the number of cycles required is\ngiven by the formula {memory cycle time}/{clock cycle\ntime}.\n\n\n\n \n Question: What is the name of the type of memory used for high speed applications? \n Answer: \n\nA. SRAM is a type of static random access memory."}, {"text": "Title: Tri-State Buffers and Combining Chips \n Text: {Tri-State Buffers and Combining Chips}\n\nRecall the buffer symbol---a triangle like an inverter, but with no\ninversion bubble---between the mux and the {DATA-OUT} \nsignal of the {2^6b} memory shown earlier.  This \n{ tri-state buffer} serves to disconnect the memory logic \nfrom the output line when the memory is not performing a read. \n\n\nAn implementation diagram for a tri-state buffer appears to the right \nalong with the symbolic\nform and a truth table.  The ``Z'' in the truth table output means \nhigh impedance (and is sometimes written ``hi-Z'').  In other words,\nthere is effectively no electrical connection between the tri-state \nbuffer and the output OUT.\n\nThis logical disconnection is achieved by using the outer\n\n\n{file=part3/figs/tri-state.eps,width=3in}\n\n\n{cc|c}\nEN& IN& OUT \n0& x& Z\n1& 0& 0\n1& 1& 1\n\n\n\n(upper and lower)\npair of transistors in the logic diagram.  When EN=0, both transistors\nturn off, meaning that regardless of the value of IN, OUT is connected\nneither to high voltage nor to ground.\n\nWhen EN=1, both transistors turn on, and the tri-state buffer acts as\na pair of back-to-back inverters, copying the signal from IN to OUT,\nas shown in the truth table.\n\nWhat benefit does this logical disconnection provide?\n\nSo long as only one memory's chip select input is high at any time,\nthe same output line can be shared by more than one memory\nwithout the need for additional multiplexers.\n\nMemory chips were often combined in this way to produce larger memories.\n\n\nThe figure to the right illustrates how larger memories can be constructed\nusing multiple chips.  In the case shown, two {2^-bit} \nmemories are used to implement a {2^{k+1}-bit} memory.\nOne of the address bits---in the case shown, the most significant bit---is\nused to drive a decoder that determines which of the two chips is \nactive (CS=1).  The decoder is enabled with the chip select signal for\nthe larger memory, so neither chip is enabled when the external CS is\nlow, as desired.  The\n\n\n{file=part3/figs/mem-larger.eps,width=4.75in}\n\n\nrest of the address bits, as well as the external\ndata inputs and write enable signal, are simply delivered to both memories.\nThe external data outputs are also connected to both memories.  \nEnsuring that at most one chip select signal is high at any time\nguarantees that at most one of the two memory chips drives logic values\non the data outputs.\n\n\nMultiple chips can also be used to construct wider memories, as shown to\nthe right.  In the case shown, two {2^-bit} \nmemories are used to implement a {2^-bit} memory.\nBoth chips are either active or inactive at the same time, so the external \naddress, write enable, and chip select inputs are routed to both chips.\nIn contrast, the data inputs and outputs are separate: the left chip\nhandles the high N bits of input on writes and produces the high N\nbits of output on reads, while the right chip handles the low N bits of \ninput and produces the low N bits of output.\n\n\n{file=part3/figs/mem-wider.eps,width=4.05in}\n\n\nHistorically, tri-state buffers were also used to reduce the number of\npins needed on chips.  Pins have long been a scarce resource, and the \namount of data that can cross a chip's pins in a second (the product of the\nnumber of pins and the data rate per pin) has not grown nearly as rapidly \nas the number of transistors packed into a fixed area.\n\nBy combining inputs and outputs, chip designers were able to halve the\nnumber of pins needed.  For example, data inputs and outputs of memory\nwere often combined into a single set of data wires, with bidirectional\nsignals.  When performing a read from a memory chip, the memory chip\ndrove the data pins with the bits being read (tri-state buffers on the\nmemory chip were enabled).  When performing a write, other logic such as \na processor wrote the value to be stored onto the data pins (tri-state \nbuffers were not enabled).\n\n\n \n Question: What is the effect of a tri-state buffer on a circuit? \n Answer: \n\nA. A tri-state buffer is a device that can be used to disconnect a circuit from an output line. The buffer has three states: high, low, and high impedance. When the buffer is in the high impedance state, the circuit is effectively disconnected from the output line."}, {"text": "Title: Dynamic Random Access Memory* \n Text: {Dynamic Random Access Memory*}\n\nDynamic random access memory, or DRAM, is used for main memory in\ncomputers and for other applications in which size is more important\nthan speed.  While slower than SRAM, DRAM is denser (has\nmore bits per chip area).  A substantial part of DRAM density is\ndue to transistor count: typical SRAM cells use six transistors\n(two for each inverter, and two more to connect the inverters to the \nbit lines), while DRAM cells use only a single transistor.\nHowever, memory designers have also made significant advances in\nfurther miniaturizing DRAM cells to improve density beyond the \nbenefit available from simple transistor count.\n\n\nA diagram of a DRAM cell appears to the right.  \nDRAM storage is capacitive: a bit is stored by charging or not charging \na capacitor.  The capacitor is attached to a BIT line \nthrough a transistor controlled by a SELECT line.  \n\nWhen SELECT is low, the capacitor is isolated and \nholds its charge.  However, the transistor's resistance is\nfinite, and some charge leaks out onto the bit line.  Charge also\nleaks into the substrate on which the transistor is constructed.  After\nsome amount of time, all of the charge dissipates, and the bit is\nlost.  To avoid such loss, the cell must be { refreshed}\nperiodically by reading the contents and writing them back with active\nlogic.\n\n\n{file=part3/figs/lec18-8.eps,width=1.1in}\n\n\nWhen the SELECT line is high during a write operation, logic driving\nthe bit line forces charge onto the capacitor or removes all charge\nfrom it.  For a read operation, the bit line is first brought to an\nintermediate voltage level (a voltage level between 0 and 1), then\nSELECT is raised, allowing the capacitor to either pull a small\namount of charge from the bit line or to push a small amount of charge\nonto the bit line.  The resulting change in voltage is then detected\nby a { sense amplifier} at the end of the bit line.  A sense amp \nis analogous to a marble on a mountaintop: a small push causes the\nmarble to roll rapidly downhill in the direction of the push.\nSimilarly, a small change in voltage causes a sense amp's output to\nmove rapidly to a logical 0 or 1, depending on the direction of the\nsmall change.  As mentioned earlier, sense amplifiers also appear in \nSRAM implementations.\nWhile not technically necessary, as they are with DRAM, the use of a\nsense amp to react to small changes in voltage makes reads faster.\n\nEach read operation on a DRAM cell brings the voltage on its capacitor\ncloser to the intermediate voltage level, in effect destroying the\ndata in the cell.  DRAM is thus said to have { destructive reads}.\nTo preserve data during a read, the bits must be written back\ninto the cells after a read.  For example, the output of the sense \namplifiers can\nbe used to drive the bit lines, rewriting the cells with the\nappropriate data.\n\nAt the chip level, typical DRAM inputs and outputs differ from those\nof SRAM.  \n\nDue to the large size and high density of DRAM,\naddresses are split into row and column components and provided\nthrough a common set of pins.  The DRAM stores the components in\nregisters to support this approach.  Additional inputs, known as the\n{ row} and { column address} {{ strobes}---RAS} and\nCAS, {respectively---are} used to indicate when address\ncomponents are available.  As\nyou might guess from the structure of coincident selection, DRAM\nrefresh occurs on a row-by-row basis (across bit slices---on columns\nrather than rows in the figures earlier in these notes, but the terminology\nof DRAM is a row).  Raising the SELECT line for a\nrow destructively reads the contents of all cells on that row, forcing\nthe cells to be rewritten and effecting a refresh.  The row is thus a\nnatural basis for the refresh cycle.  The DRAM data pins provide\nbidirectional signals for reading and writing elements of the DRAM.\nAn { output enable} input, OE, controls tri-state buffers with\nthe DRAM to determine whether or not the DRAM drives the data pins.\nThe WE input, which controls the type of operation, is\nalso present.\n\n\nTiming diagrams for writes and reads on a historical DRAM implementation\nappear to the right.  In both cases, the row component of the address is \nfirst applied to the address pins, then RAS is raised.  In the\nnext cycle of the controlling logic, the column component is applied\nto the address pins, and CAS is raised.  \n\nFor a write, as shown on the left, the WE signal and the\ndata can\n\n\n{file=part3/figs/lec18-9.eps,width=4in}\n\n\nalso be applied in the second cycle.  The DRAM has internal\ntiming and control logic that prevent races from overwriting an\nincorrect element (remember that the row and column addresses have to\nbe stored in registers).  The DRAM again specifies a write cycle,\nafter which the operation is guaranteed to be complete.  In order, the\nWE, CAS, and RAS signals are then lowered.  \n\nFor a read operation, the output enable signal, OE, is raised after\nCAS is raised.  The DATA pins, which should be floating (in other\nwords, not driven by any logic), are then driven by the DRAM.  After the \nread cycle, valid data appear on the DATA pins, and OE, CAS, and\nRAS are lowered in order after the data are read.\n\nModern DRAM chips are substantially more sophisticated than those\ndiscussed here, and many of the functions that used to be provided\nby external logic are now integrated onto the chips themselves.\n\nAs an example of modern DRAMs, one can obtain\nthe data sheet for Micron Semiconductor's 8Gb ({2^b},\nfor example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016.\n\nThe ability to synchronize to an external clock has become prevalent in \nthe industry, leading to the somewhat confusing term SDRAM, which stands \nfor { synchronous DRAM}.  The memory structures themselves\nare still unclocked, but logic is provided on the chip to synchronize \naccesses to the external clock without the need for additional \nlogic.\n\nThe clock provided to the Micron chip just mentioned\ncan be as fast as 1.6 GHz, and data can be\ntransferred on both the rising and falling edges of the clock\n(hence the name DDR, or { double data rate}).  \n\nIn addition to row and\ncolumn components of the address, these chips further separate cells into\n{ banks} and groups of banks.  These allow a user to exploit parallelism\nby starting reads or writes to separate banks at the same time, thus\nimproving the speed at which data can move in and out of the memory.\n\nFor the {2^b} version of the Micron chip,\nthe cells are structured into 4 groups of 4 banks (16 banks total),\neach with 131,072 rows and 1,024 columns.  \n\nDRAM implementations provide interfaces for specifying\nrefresh operations in addition to reads and writes.\nManaging refresh timing and execution is\ngenerally left to an external DRAM controller.\n\nFor the Micron chip, refresh commands must be issued every \n7.8 microseconds at normal temperatures.  Each\ncommand refreshes about 2^ cells, so 8,192 commands refresh\nthe whole chip in less than 64 milliseconds.\n\nAlternatively, the chip can handle refresh on-chip in\norder to maintain memory contents when the rest of the system is \npowered down.\n\n\n\n \n Question: What is the main difference between DRAM and SRAM? \n Answer: \n\nA. DRAM is denser (has more bits per chip area) than SRAM."}, {"text": "Title: Design of the Finite State Machine for the Lab \n Text: {Design of the Finite State Machine for the Lab}\n\nThis set of notes explains the process that Prof. Doug Jones used to develop\nthe FSM for the lab.\n\nThe lab simulates a vending machine mechanism for automatically \nidentifying coins (dimes and quarters only), tracking the amount \nof money entered by the user, accepting or rejecting \ncoins, and emitting a signal when a total of 35 cents has been \naccepted.  In the lab, we will only drive a light with \nthe ``paid in full'' signal.  \n\nSorry, neither candy nor Dew will be distributed!\n\nProf. Doug Jones designed the vending machine application and the FSM,\nwhile Prof. Chris Schmitz prototyped and constructed the physical elements \nwith some help from the ECE shop.\n\nProf. Volodymyr Kindratenko together with Prof. Geoffrey Herman created \nboth the wiki documentation and the Altera Quartus portions of the lab\n(the latter were based on earlier Mentor Graphics work by Prof. Herman).\n\nProf. Kindratenko also helped to scale the design \nin a way that made it possible to deliver to the over 400 students entering\nECE every semester.  \n\nProf. Juan Jos'e Jaramillo later identified\ncommon failure modes, including variability caused by sunshine through \nthe windows in ECEB,{No wonder people say that engineers hate \nsunlight!} and made some changes to improve robustness.  He also\ncreated the PowerPoint slides that are typically used to describe the lab in\nlecture.  Casey Smith, head guru of the ECE Instructional Labs,\ndeveloped a new debounce design and made some other hardware \nimprovements to reduce the rate of student headaches.\nFinally, Prof. Kirill Levchenko together with UA Saidivya Ashok\nstruck a blow against COVID-19 by developing an inexpensive and\nportable replacement for the physical ``vending machine'' systems\nused for testing in previous semesters.\n\n \n Question: What is the FSM? \n Answer: \n\nA. The vending machine uses a FSM to automatically identify coins, track the amount of money entered by the user, accept or reject coins, and emit a signal when a total of 35 cents has been accepted."}, {"text": "Title: Physical Design, Sensors, and Timing \n Text: {Physical Design, Sensors, and Timing}\n\nA user inserts a coin into a slot at one end of the device.  The coin\nthen rolls down a slope towards a gate controlled by a servo.  The gate\ncan be raised or lowered, and determines whether the coin exits from the\nother side or the bottom of the device.\n\nAs the coin rolls, it passes two optical sensors.{The full system\nactually allows four sensors to differentiate four types of coins, but\nour lab uses only two of these sensors.}  One of these sensors is \npositioned high enough above the slope that a dime passes beneath the\nsesnor, allowing the signal T produced by the sensor to tell us whether \nthe coin is a dime or a quarter.  The second sensor is positioned so\nthat all coins pass in front of it.  The sensor positions are chosen \ncarefully to ensure that, in the case of a quarter, the coin is still\nblocking the first sensor when it reaches the second sensor.  \nBlocked sensors give a signal of 1 in this design, so the rising edge \nthe signal from the second sensor can be used as a ``clock'' for our \nFSM.  When the rising edge occurs, the signal T from the first sensor \nindicates whether the coin is a quarter (T=1) or a dime (T=0).\n\n\nA sample timing diagram for the lab appears to the right.  The clock\nsignal generated by the lab is not only not a square wave---in other words,\nthe high and low portions are not equal---but is also unlikely to be periodic.\nInstead, the ``cycle'' is defined by the time between coin insertions.\nThe T signal serves as the single input to our FSM.  In the timing\n\n\n{file=part3/figs/lab-timing.eps,width=2.55in}\n\n\ndiagram, T is shown as rising and falling before the clock edge.\nWe use positive edge-triggered flip-flops to implement our FSM,\nthus the aspect of the relative timing that matters to our design\nis that, when the clock rises, the value of T is stable and indicates \nthe type of coin entered.  The signal T may fall before or after\nthe clock does---the two are equivalent for our FSM's needs.\n\nThe signal A in the timing diagram is an output from the FSM, and\nindicates whether or not the coin should be accepted.  This signal \ncontrols the servo that drives the gate, and thus determines whether\nthe coin is accepted (A=1) as payment or rejected (A=0) and returned\nto the user.  \n\nLooking at the timing diagram, you should note that our FSM makes \na decision based on its current state and the input T and enters a \nnew state at the rising clock edge.  The value of A in the next cycle\nthus determines the position of the gate when the coin eventually\nrolls to the end of the slope.\n\nAs we said earlier, our FSM is thus a Moore machine: the output A\ndoes not depend on the input T, but only on the current internal \nstate bits of the the FSM.\n\nHowever, you should also now realize that making A depend on T\nis not adequate for this lab.  If A were to rise with T and\nfall with the rising clock edge (on entry to the next state), or\neven fall with the falling edge of T, the gate would return to the\nreject position by the time the coin reached the gate, regardless\nof our FSM's decision!\n\n \n Question: What is the signal that determines whether or not a coin should be accepted? \n Answer: \n\nA. The signal A in the timing diagram serves as an output from the FSM, and\nindicates whether or not the coin should be accepted."}, {"text": "Title: An Abstract Model\\vspace4pt \n Text: {An Abstract Model}\n\n\nWe start by writing down states for a user's expected behavior.\nGiven the fairly tight constraints that we have placed on our lab,\nfew combinations are pos-\n\n\n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nSTART& DIME& QUARTER& & no\nDIME& & PAID& yes& no\nQUARTER& PAID& & yes& no\nPAID& & & yes& yes\n\n\n\nsible.  For a total of 35 cents, a user should either insert a dime \nfollowed by a quarter, or a quarter followed by a dime.\n\nWe begin in a START state, which transitions to states DIME or QUARTER\nwhen the user inserts the first coin.  With no previous coin, we need not\nspecify a value for A.  No money has been deposited, so we set \noutput P=0 in the START state.\n\nWe next create DIME and QUARTER states corresponding to the user having\nentered one coin.  The first coin should be accepted, but more money is\nneeded, so both of these states output A=1 and P=0.\nWhen a coin of the opposite type is entered, each state moves to a\nstate called PAID, which we use for the case in which a total of 35 cents has\nbeen received.  For now, we ignore the possibility that the same type\nof coin is deposited more than once.  Finally, the PAID state accepts\nthe second coin (A=1) and indicates that the user has paid the full\nprice of 35 cents (P=1).\n\n\nWe next extend our design to handle user mistakes.  If a user enters\na second dime in the DIME state, our FSM should reject the coin.  We\ncreate a REJECTD state and add it as the next state from\n\n\n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nSTART& DIME& QUARTER& & no\nDIME& REJECTD& PAID& yes& no\nREJECTD& REJECTD& PAID& no& no\nQUARTER& PAID& REJECTQ& yes& no\nREJECTQ& PAID& REJECTQ& no& no\nPAID& & & yes& yes\n\n\n\nDIME when a dime is entered.\nThe REJECTD state rejects the dime (A=0) and\ncontinues to wait for a quarter (P=0).  What should we use as next \nstates from REJECTD?  If the user enters a third dime (or a fourth, \nor a fifth, and so on), we want to reject the new dime as well.  \nIf the user enters a quarter, we want to accept the coin, at which point\nwe have received 35 cents (counting the first dime).  We use\nthis reasoning to complete the description of REJECTD.  We also create\nan analogous state, REJECTQ, to handle a user who inserts more than\none quarter.\n\nWhat should happen after a user has paid 35 cents and bought \none item?  The FSM at that point is in the PAID state, which delivers\nthe item by setting P=1.\n\nGiven that we want the FSM to allow the user to purchase another item, \nhow should we choose the next states from PAID?\n\nThe behavior that we want from PAID is identical to the behavior that\nwe defined from START.  The 35 cents already \ndeposited was used to pay for the item delivered, so the machine is\nno longer holding any of the user's money.\n\nWe can thus simply set the next states from PAID to be DIME when a \ndime is inserted and QUARTER when a quarter is inserted.\n\n\n\n\nAt this point, we make a decision intended primarily to simplify the\nlogic needed to build the lab.  Without a physical item delivery \nmechanism with a specification for how its in-\n\n\n{c|cc|cc}\nstate& dime (T=0)& quarter (T=1)& accept? (A)& paid? (P) \nPAID& DIME& QUARTER& yes& yes\nDIME& REJECTD& PAID& yes& no\nREJECTD& REJECTD& PAID& no& no\nQUARTER& PAID& REJECTQ& yes& no\nREJECTQ& PAID& REJECTQ& no& no\n\n\n\nput must be driven, \nthe behavior of the output signal P can be fairly flexible.  \nFor example, we could build a delivery mechanism that used the rising\nedge of P to open a chute.  In this case, the output P=0 in the\nstart state is not relevant, and we can merge the state START with\nthe state PAID.  The way that we handle P in the lab, we might\nfind it strange to have a ``paid'' light turn on before inserting any\nmoney, but keeping the design simple enough for a first lab exercise \nis more important.  Our final abstract state table appears above.\n\n \n Question: What is the purpose of the state that we created? \n Answer: "}, {"text": "Title: Picking the Representation \n Text: {Picking the Representation}\n\nWe are now ready to choose the state representation for the lab FSM.\n\nWith five states, we need three bits of internal state.\n\nProf. Jones decided to leverage human meaning in assigning the\nbit patterns, as follows:\n\n{\n\nS_2& type of last coin inserted (0 for dime, 1 for quarter)\nS_1& more than one quarter inserted? (1 for yes, 0 for no)\nS_0& more than one dime inserted? (1 for yes, 0 for no)\n\n}\n\n\nThese meanings are not easy to apply to all of our states.  For example,\nin the PAID state, the last coin inserted may have been of either type,\nor of no type at all, since we decided to start our FSM in that state as \nwell.  However, for the other four states, the meanings provide a clear\nand unique set of bit pattern assignments, as shown to the right.  We\ncan choose any of the remaining four bit patterns (010, 011, 101, or 111)\nfor the PAID state.  In fact, { we can choose all of the remaining\npatterns} for the PAID state.  We can always represent any state\n\n\n\nstate& S_2S_1S_0  \nPAID& ???\nDIME& 000\nREJECTD& 001\nQUARTER& 100\nREJECTQ& 110\n\n\n\nwith more\nthan one pattern if we have spare patterns available.  Prof. Jones\nused this freedom to simplify the logic design.\n\nThis particular example is slightly tricky.  The four free patterns do\nnot share any single bit in common, so we cannot simply insert x's\ninto all {K-map} entries for which the next state is PAID.\nFor example, if we insert an x into the {K-map} for  S_2^+,\nand then choose a function for S_2^+ that produces a value of 1\nin place of the don't care, we must also produce a 1 in\nthe corresponding entry of the {K-map} for S_0^+.  Our options\nfor PAID include 101 and 111, but not 100 nor 110.  These latter\ntwo states have other meanings.\n\n\nLet's begin by writing a next-state table consisting mostly of bits,\nas shown to the right.  We use this table to write out a {K-map}\nfor S_2^+ as follows: any of the patterns that may be used for the\nPAID state obey the next-state rules for PAID.  Any next-state marked\nas PAID is marked as don't care in the {K-map},\n\n\n{cc|cc}\n&&{|c}{S_2^+S_1^+S_0^+}\nstate& S_2S_1S_0& T=0& T=1 \nPAID& PAID& 000& 100\nDIME& 000& 001& PAID\nREJECTD& 001& 001& PAID\nQUARTER& 100& PAID& 110\nREJECTQ& 110& PAID& 110\n\n\n\n\n\n\nsince we can\nchoose patterns starting with either or both values to represent our\nPAID state.  The resulting {K-map} appears to the far right.\nAs shown, we simply set S_2^+=T, which matches our\noriginal ``meaning'' for S_2.  That is, S_2 is the type of the\nlast coin inserted.\n\n\nBased on our choice for S_2^+, we can rewrite the {K-map} as\nshown to the right, with green italics and shading marking the\nvalues produced for the x's in the specification.  Each of these\nboxes corresponds to one transition into the PAID state.  By \nspecifying the S_2 value, we cut the number of possible choices\nfrom four to two in each case.  For those combinations in which the\nimplementation produces S_2^+=0, we must choose S_1^+=1, but\nare still free to leave S_0^+ marked as a don't care.  Similarly,\nfor those combinations in which the implementation produces S_2^+=1, \nwe must choose S_0^+=1, but\nare still free to leave S_1^+ marked as a don't care.\n\n\n\n\n\n\nThe {K-maps} for S_1^+ and S_0^+ are shown to the right.\nWe have not given algebraic expressions for either, but have indicated\nour choices by highlighting the resulting replacements of don't care\nentries with the values produced by our expressions.\n\nAt this point, we can review the state patterns actually produced by\neach of the four next-state transitions into the PAID state.  From\nthe DIME state, we move into the 101 state when the user inserts a\n\n\n\n\n\n\n\n\nquarter.  The result is the same from the REJECTD state.  From the\nQUARTER state, however, we move into the 010 state when the user \ninserts a dime.  The result is the same from the REJECTQ state.  We\nmust thus classify both patterns, 101 and 010, as PAID states.  The\nremaining two patterns, 011 and 111, cannot\n\n\nbe reached from any of\nthe states in our design.  We might then try to leverage the fact\nthat the next-state patterns from these two states are not relevant \n(recall that we fixed the next-state patterns for all four of the \npossible PAID states) to further simplify our logic, but doing so \ndoes not provide any advantage (you may want to check our claim).\n\nThe final state table is shown to the right.  We have included the\nextra states at the bottom of the table.  We have specified the\nnext-state logic for these\n\n\n{cc|cc|cc}\n&&{|c|}{S_2^+S_1^+S_0^+}\nstate& S_2S_1S_0& T=0& T=1& A& P \nPAID1& 010& 000& 100& 1& 1\nPAID2& 101& 000& 100& 1& 1\nDIME& 000& 001& 101& 1& 0\nREJECTD& 001& 001& 101& 0& 0\nQUARTER& 100& 010& 110& 1& 0\nREJECTQ& 110& 010& 110& 0& 0 \nEXTRA1& 011& 000& 100& x& x\nEXTRA2& 111& 000& 100& x& x\n\n\n\nstates, but left the output bits as don't\ncares.  A state transition diagram appears at the bottom of this page.\n\n \n Question: What is the state of the FSM? \n Answer: \n\nThe bit pattern S_2S_1S_0 represents the internal state of the FSM."}, {"text": "Title: Testing the Design \n Text: {Testing the Design}\n\nHaving a complete design on paper is a good step forward, but humans\nmake mistakes at all stages.  How can we know that a circuit that\nwe build in the lab correctly implements the FSM that we have outlined \nin these notes?\n\nFor the lab design, we have two problems to solve.\n\nFirst, we have not specified an initialization scheme for the FSM.\nWe may want the FSM to start in one of the PAID states, but adding\ninitialization logic to the design may mean requiring you to wire together\nsignificantly more chips.  Second, we need a sequence of inputs that\nmanages to test that all of the next-state and output logic implementations\nare correct.\n\nTesting sequential logic, including FSMs, is in general extremely difficult.\nIn fact, large sequential systems today are generally converted into \ncombinational logic by using shift registers to fill the \nflip-flops with a particular pattern, \nexecuting the logic for one clock cycle, and checking that the resulting \npattern of bits in the flip-flops is correct.  This approach is called \n{ scan-based testing}, and is discussed in ECE 543.  You \nwill make use of a similar approach\nwhen you test your combinational logic in the second week of the lab,\nbefore wiring up the flip-flops.\n\nWe have designed our FSM to be easy to test (even small FSMs\nmay be challenging) with a brute force approach.  In particular, we \nidentify two input sequences that together serve both to initialize and \nto test a correctly implemented variant of our FSM.  Our initialization\nsequence forces the FSM into a specific state regardless of its initial\nstate.  And our test sequence crosses every transition arc leaving the\nsix valid states.\n\n\n\nIn terms of T, the coin type, we initialize the FSM with the\ninput sequence 001.  Notice that such a sequence takes any initial \nstate into PAID2.\n\nFor testing, we use the input sequence 111010010001.  You should trace \nthis sequence, starting from PAID2, on the diagram below to see how the\ntest sequence covers all of the possible arcs.  As we test, we need also\nto observe the A and P outputs in each state to check the output\nlogic.\n\n{{file=part3/figs/lab-diag-notes.eps,width=4.25in}}\n\n\n\n \n Question: What is the first step in testing the circuit? \n Answer: "}, {"text": "Title: Finite State Machine Design Examples, Part II \n Text: {Finite State Machine Design Examples, Part II}\n\nThis set of notes provides several additional examples of FSM design.\nWe first design an FSM to control a vending machine, introducing\nencoders and decoders as components that help us to implement our\ndesign.  We then design a game controller for a logic puzzle\nimplemented as a children's game.  Finally, we analyze a digital FSM\ndesigned to control the stoplights at the intersection of two roads.\n\n\n \n Question: What is an encoder? \n Answer: \n\nA. An encoder is a component that helps to implement an FSM design. It converts input signals into a code that can be read by a machine."}, {"text": "Title: Design of a Vending Machine \n Text: {Design of a Vending Machine}\n\nFor the next example, we design an FSM to control a simple vending machine.  \nThe machine accepts {U.S. coins}{Most countries have small \nbills or coins in demoninations suitable for vending machine prices, so think \nabout some other currency if you prefer.} as payment and offers a choice\nof three items for sale.\n\nWhat states does such an FSM need?  The FSM needs to keep track of how\nmuch money has been inserted in order to decide whether a user can \npurchase one of the items.  That information alone is enough for the\nsimplest machine, but let's create a machine with adjustable item\nprices.  We can use registers to hold the item prices, which \nwe denote P_1, P_2, and P_3.\n\nTechnically, the item prices are also part of the internal state of the \nFSM.  However,  we leave out discussion (and, indeed, methods) for setting\nthe item prices, so no state with a given combination of prices has any \ntransition to a state with a different set of item prices.\nIn other words, any given combination of item prices induces a subset \nof states that operate independently of the subset induced by a distinct \ncombination of item prices.  By abstracting away the prices in this way,\nwe can focus on a general design that allows the owner of the machine\nto set the prices dynamically.\n\n\nOur machine will not accept pennies, so let's have the FSM keep track of\nhow much money has been inserted as a multiple of 5 cents (one nickel).\nThe table to the right shows five types of coins, their value in \ndollars, and their value in terms of nickels.\n\nThe most expensive item in the machine might cost a dollar or two, so\nthe FSM must track at least 20 or 40 nickels of value.  Let's\n\n\n{l|c|c}\n{c|}{coin type}& value& # of nickels \nnickel&      0.05& 1\ndime&        0.10& 2\nquarter&     0.25& 5\nhalf dollar& 0.50& 10\ndollar&      1.00& 20\n\n\n\ndecide to\nuse six bits to record the number of nickels, which allows the machine\nto keep track of up to 3.15 (63 nickels).  We call the abstract\nstates { STATE00} through { STATE63}, and refer to a state with\nan inserted value of N nickels as { STATE{<}N{>}}.\n\nLet's now create a next-state table, as shown at the top of the next page.\nThe user can insert one of the five coin types, or can pick one of the \nthree items.  What should happen if the user inserts more money than the \nFSM can track?  Let's make the FSM reject such coins.  Similarly, if the \nuser tries to buy an item without inserting enough money first, the FSM \nmust reject the request.  For each of the possible input events, we add a \ncondition to separate the FSM states that allow the input event to \nbe processed as the user desires from those states that do not.  For example,\nif the user inserts a quarter, those states with N<59 transition to\nstates with value N+5 and accept the quarter.  Those states with\nN reject the coin and remain in { STATE{<}N{>}}.\n\n\n{c|l|c|l|c|c}\n&&& {|c}{final state}\n&&& {|c}{}& & release \ninitial state& {|c|}{input event}& condition& {|c}& & product \n{ STATE{<}N{>}}& no input& always& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& nickel inserted& N<63& { STATE{<}N+1{>}}& yes& none\n{ STATE{<}N{>}}& nickel inserted& N=63& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& dime inserted& N<62& { STATE{<}N+2{>}}& yes& none\n{ STATE{<}N{>}}& dime inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& quarter inserted& N<59& { STATE{<}N+5{>}}& yes& none\n{ STATE{<}N{>}}& quarter inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& half dollar inserted& N<54& { STATE{<}N+10{>}}& yes& none\n{ STATE{<}N{>}}& half dollar inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& dollar inserted& N<44& { STATE{<}N+20{>}}& yes& none\n{ STATE{<}N{>}}& dollar inserted& N& { STATE{<}N{>}}& no& none\n{ STATE{<}N{>}}& item 1 selected& N{P_1}& { STATE{<}N-P_1{>}}& ---& 1\n{ STATE{<}N{>}}& item 1 selected& N<P_1& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& item 2 selected& N{P_2}& { STATE{<}N-P_2{>}}& ---& 2\n{ STATE{<}N{>}}& item 2 selected& N<P_2& { STATE{<}N{>}}& ---& none\n{ STATE{<}N{>}}& item 3 selected& N{P_3}& { STATE{<}N-P_3{>}}& ---& 3\n{ STATE{<}N{>}}& item 3 selected& N<P_3& { STATE{<}N{>}}& ---& none\n\n\n\nWe can now begin to formalize the I/O for our machine.  Inputs include \ninsertion of coins and selection of items for purchase.  Outputs include\na signal to accept or reject an inserted coin as well as signals to release\neach of the three items.\n\n\nFor input to the FSM, we assume that a coin inserted in any given cycle \nis classified and delivered to our FSM using the three-bit representation \nshown to the right.\n\nFor item selection, we assume that the user has access to three buttons,\nB_1, B_2, and B_3, that indicate a desire to purchase the \ncorresponding item.\n\nFor output, the FSM must produce a signal A indicating whether a coin\nshould be accepted.  To control the release of items that have been purchased,\nthe FSM must produce the signals R_1, R_2, and R_3, corresponding\nto the re-\n\n\n{l|c}\n{c|}{coin type}& C_2C_1C_0 \nnone&        110\nnickel&      010\ndime&        000\nquarter&     011\nhalf dollar& 001\ndollar&      111\n\n\n\nlease of each item.  Since outputs in our class depend only on\nstate, we extend the internal state of the FSM to include bits for each of\nthese output signals.  The output signals go high in the cycle after\nthe inputs that generate them.  Thus, for example, the accept signal A\ncorresponds to a coin inserted in the previous cycle, even if a second\ncoin is inserted in the current cycle.  This meaning must be made clear to\nwhomever builds the mechanical system to return coins.\n\nNow we are ready to complete the specification.  How many states does the\nFSM have?  With six bits to record money inserted and four bits to \ndrive output signals, we have a total of 1,024 (2^) states!\nSix different coin inputs are possible, and the selection buttons allow\neight possible combinations, giving 48 transitions from each state.\nFortunately, we can use the meaning of the bits to greatly simplify\nour analysis.\n\nFirst, note that the current state of the coin accept bit and item\nrelease bits---the four bits of FSM state that control the outputs---have\nno effect on the next state of the FSM.  Thus, we can consider only the\ncurrent amount of money in a given state when thinking about the \ntransitions from the state.  As you have seen, we can further abstract\nthe states using the number N, the number of nickels currently held by \nthe vending machine.\n\nWe must still consider all 48 possible transitions from { STATE{<}N{>}}.\nLooking back at our abstract next-state table, notice that we had only\neight types of input events (not counting ``no input'').  If we\nstrictly prioritize these eight possible events, we can safely ignore\ncombinations.  Recall that we adopted a similar strategy for several \nearlier designs, including the ice cream dispenser in Notes Set 2.2 and\nthe keyless entry system developed in Notes Set 3.1.3.\n\nWe choose to prioritize purchases over new coin insertions, and to \nprioritize item 3 over item 2 over item 1.  These prioritizations\nare strict in the sense that if the user presses B_3, both other\nbuttons are ignored, and any coin inserted is rejected, regardless of\nwhether or not the user can actually purchase item 3 (the machine\nmay not contain enough money to cover the item price).\n\nWith the choice of strict prioritization, all transitions from all\nstates become well-defined.  We apply the transition rules in order of\ndecreasing priority,\nwith conditions, and with {don't-cares} for lower-priority inputs. \nFor example, for any of the 16 { STATE50}'s (remember that the four\ncurrent output bits do not affect transitions), the table below lists all\npossible transitions assuming that P_3=60, P_2=10, and P_1=35.\n\n{\n{ccccc|ccccc}\n&&&&& {|c}{next state}\ninitial state& B_3& B_2& B_1& C_2C_1C_0& state& A& R_3& R_2& R_1 \n{ STATE50}& 1&x&x& xxx& { STATE50}& 0& 0&0&0\n{ STATE50}& 0&1&x& xxx& { STATE40}& 0& 0&1&0\n{ STATE50}& 0&0&1& xxx& { STATE15}& 0& 0&0&1\n{ STATE50}& 0&0&0& 010& { STATE51}& 1& 0&0&0\n{ STATE50}& 0&0&0& 000& { STATE52}& 1& 0&0&0\n{ STATE50}& 0&0&0& 011& { STATE55}& 1& 0&0&0\n{ STATE50}& 0&0&0& 001& { STATE60}& 1& 0&0&0\n{ STATE50}& 0&0&0& 111& { STATE50}& 0& 0&0&0\n{ STATE50}& 0&0&0& 110& { STATE50}& 0& 0&0&0\n\n}\nNext, we need to choose a state representation.  But this task is \nessentially done: each output bit (A, R_1, R_2, and R_3) is\nrepresented with one bit in the internal representation, and the\nremaining six bits record the number of nickels held by the vending\nmachine using an unsigned representation.\n\nThe choice of a numeric representation for the money held is important,\nas it allows us to use an adder to compute the money held in\nthe next state.\n\n \n Question: What is the output of the vending machine? \n Answer: \n\nA. The output of the vending machine would be \"1\" for the release of item 2, and \"0\" for everything else."}, {"text": "Title: Encoders and Decoders \n Text: {Encoders and Decoders}\n\nSince we chose to prioritize purchases, let's begin by building logic\nto perform state transitions for purchases.  Our first task is to\nimplement prioritization among the three selection buttons.  For this\npurpose, we construct a {4-input} { priority encoder}, which \ngenerates a signal P whenever any of its four input lines is active\nand encodes the index of the highest active input as a two-bit unsigned\nnumber S.   A truth table for our priority encoder appears on the \nleft below, with {K-maps} for each of the output bits on the right.\n\n\n{cccc|cc}\nB_3& B_2& B_1& B_0& P& S \n1&x&x&x& 1& 11\n0&1&x&x& 1& 10\n0&0&1&x& 1& 01\n0&0&0&1& 1& 00\n0&0&0&0& 0& xx\n\n\n\n\n\n\n\n\n\n\nFrom the {K-maps}, we extract the following equations:\n{eqnarray*}\nP &=& B_3 + B_2 + B_1 + B_0\nS_1 &=& B_3 + B_2\nS_0 &=& B_3 + {B_2}B_1\n{eqnarray*}\nwhich allow us to implement our encoder as shown to the right.\n\nIf we connect our buttons B_1, B_2, and B_3 to the priority \nencoder (and feed 0 into the fourth input), it produces a signal P \nindicating that the user is trying to make a purchase and a two-bit\nsignal S indicating which item the user wants.\n\n\n\n\n\nWe also need to build logic to control the item release outputs R_1, R_2,\nand R_3.  An item should be released only when it has been selected \n(as indicated by the priority encoder signal S) and the vending machine\nhas enough money.  For now, let's leave aside calculation of the item \nrelease signal, which we call R, and focus on how we can produce the\ncorrect values of R_1, R_2, and R_3 from S and R.\n\nThe component to the right is a { decoder} with an enable input.  A \ndecoder takes an input signal---typically one coded as a binary number---and \nproduces one output for each possible value of the signal.  You may\nnotice the similarity with the structure of a mux: when the decoder\nis enabled (EN=1), each of the AND gates produces\n\n\n\n\n\none minterm of the input signal S.  In\nthe mux, each of the inputs is then included in\none minterm's AND gate, and the outputs of all AND gates are ORd together.\nIn the decoder, the AND gate outputs are the outputs of the decoder.\nThus, when enabled, the decoder produces exactly one 1 bit on its outputs.\nWhen not enabled (EN=0), the decoder produces all 0 bits.\n\nWe use a decoder to generate the release signals for the vending machine\nby connecting the signal S produced by the \npriority encoder to the decoder's S input and connecting the item\nrelease signal R to the decoder's EN input.  The outputs D_1,\nD_2, and D_3 then correspond to the individual item release \nsignals R_1, R_2, and R_3 for our vending machine.\n\n \n Question: What is the purpose of a priority encoder? \n Answer: \n\nA. The purpose of a priority encoder is to generate a signal P whenever any of its four input lines is active and encode the index of the highest active input as a two-bit unsigned number S."}, {"text": "Title: Vending Machine Implementation \n Text: {Vending Machine Implementation}\n\n\nWe are now ready to implement the FSM to handle purchases, as shown to the \nright.  The current number of nickels, N, is stored in a register in the\ncenter of the diagram.  Each cycle, N is fed into a {6-bit} adder,\nwhich subtracts the price of any purchase requested in that cycle. \n\nRecall that we chose to record item prices in registers.  We avoid the \nneed to negate prices before adding them by storing the negated prices in\nour registers.  Thus, the value of register PRICE1 is -P_1, the\nthe value of register PRICE2 is -P_2, and the\nthe value of register PRICE3 is -P_3.\n\nThe priority encoder's S signal is then used to select the value of \none of these three registers (using a {24-to-6} mux) as the second\ninput to the adder.\n\nWe use the adder to execute a subtraction, so the carry out C_ \nis 1 whenever the value of N is at least as great as the amount \nbeing subtracted.  In that case, the purchase is successful.  The AND\ngate on the left calculates the signal R indicating a successful purchase,\nwhich is then used to select the next value of N using the {12-to-6}\nmux below the adder.  \n\nWhen no item selection buttons are pushed, P and thus R are both 0, \nand the mux below the adder keeps N unchanged in the next cycle.  \nSimilarly, if P=1 but N is\ninsufficient, C_ and thus R are both 0, and again N does\nnot change.  Only when P=1 and C_=1 is the purchase successful,\nin which case the price is subtracted from N in the next cycle.\n\n\n\n\n\nThe signal R is also used to enable a decoder that generates the three\nindividual item release outputs.  The correct output is generated based on\nthe decoded S signal from the priority encoder, and all three output\nbits are latched into registers to release the purchased item in the next \ncycle.\n\nOne minor note on the design so far: by hardwiring C_ to 0, we created \na problem for items that cost nothing (0 nickels): in that case, C_ is\nalways 0.  We could instead store\n-P_1-1 in PRICE1 (and so forth) and feed P in to C_, but maybe\nit's better not to allow free items.\n\n\nHow can we support coin insertion?  Let's use the same adder to add\neach inserted coin's value to N.  The table at the right shows\nthe value of each coin as a {5-bit} unsigned number of nickels.\nUsing this table, we can fill in {K-maps} for each bit of V, as\nshown below.  Notice that we have marked the two undefined bit patterns\nfor the coin type C as don't cares in the {K-maps}.\n\n\n{l|c|c}\n{c|}{coin type}& C_2C_1C_0& V_4V_3V_2V_1V_0 \nnone&        110& 00000\nnickel&      010& 00001\ndime&        000& 00010\nquarter&     011& 00101\nhalf dollar& 001& 01010\ndollar&      111& 10100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the {K-maps} gives the following equations, which we\nimplement as shown to the right.\n\n{eqnarray*}\nV_4 &=& C_2C_0\nV_3 &=& {C_1}C_0\nV_2 &=& C_1C_0\nV_1 &=& {C_1}\nV_0 &=& {C_2}C_1\n{eqnarray*}\n\n\n\n\n\n\nNow we can extend the design to handle coin insertion, as shown to the right\nwith new elements highlighted in blue.  The output of the coin value \ncalculator is extended with a leading 0 and then fed into a {12-to-6} mux.\nWhen a purchase is requested, P=1 and the mux forwards the item price to \nthe adder---recall that we chose to give purchases priority over coin\ninsertion.  When no purchase is requested, the value of any coin inserted\n(or 0 when no coin is inserted) is passed to the adder.\n\nTwo new gates have been added on the lower left.  First, let's verify that\npurchases work as before.  When a purchase is requested, P=1, so the\nNOR gate outputs 0, and the OR gate simple forwards R to control the\nmux that decides whether the purchase was successful, just as in our\noriginal design.\n\nWhen no purchase is made (P=0, and R=0), the adder adds the value of \nany inserted coin to N.  If the addition overflows, C_=1, and\nthe output of the NOR gate is 0.  Note that the NOR gate output is stored\nas the output A in the next cycle, so a coin that causes overflow in the\namount of money stored is rejected.  The OR gate also outputs 0, and N\nremains unchanged.  If the addition does not overflow, the NOR gate\noutputs a 1, the coin is accepted (A=1 in the next cycle), and \nthe mux allows the sum N+V to be written back as the new value of N.\n\n\n\n\n\nThe tables at the top of the next page define all of the state variables,\ninputs, outputs, and internal signals used in the design, and list the\nnumber of bits for each variable.\n\n\n\n\n{|ccl|}\n{|r|}{{ FSM state}}\nPRICE1& 6& negated price of item 1 (-P_1)\nPRICE2& 6& negated price of item 2 (-P_2)\nPRICE3& 6& negated price of item 3 (-P_3)\nN& 6& value of money in machine (in nickels)\nA& 1& stored value of accept coin output\nR_1& 1& stored value of release item 1 output\nR_2& 1& stored value of release item 2 output\nR_3& 1& stored value of release item 3 output \n{|r|}{{ internal signals}}\nV& 5& inserted coin value in nickels\nP& 1& purchase requested (from priority encoder)\nS& 2& item # requested (from priority encoder)\nR& 1& release item (purchase approved) \n\n\n{|ccl|}\n{|r|}{{ inputs}}\nB_1& 1& item 1 selected for purchase\nB_2& 1& item 2 selected for purchase\nB_3& 1& item 3 selected for purchase\nC& 3& coin inserted (see earlier table \n& & for meaning) \n{|r|}{{ outputs}}\nA& 1& accept inserted coin (last cycle)\nR_1& 1& release item 1\nR_2& 1& release item 2\nR_3& 1& release item 3\n{|l|}{{ Note that outputs correspond one-to-one}}\n{|l|}{{ with four bits of FSM state.}} \n\n\n\n\n \n Question: What is the value of an inserted coin stored as? \n Answer: \n\nA. The vending machine uses the coin type input C and the coin value input V to determine the value of an inserted coin."}, {"text": "Title: Design of a Game Controller \n Text: {Design of a Game Controller}\n\nFor the next example, imagine that you are part of a team building a\ngame for children to play at Engineering Open House.\n\nThe game revolves around an old logic problem in which a farmer must\ncross a river in order to reach the market.  The farmer is traveling\nto the market to sell a fox, a goose, and some corn.  The farmer has\na boat, but the boat is only large enough to carry the fox, the goose,\nor the corn along with the farmer.  The farmer knows that if he leaves\nthe fox alone with the goose, the fox will eat the goose.  Similarly,\nif the farmer leaves the goose alone with the corn, the goose will \neat the corn.\n\nHow can the farmer cross the river?\n\nYour team decides to build a board illustrating the problem with\na river from top to bottom and lights illustrating the positions of \nthe farmer (always with the boat), the fox, the goose, and the\ncorn on either the left bank or the right bank of the river.\nEverything starts on the left bank, and the children can play \nthe game until they win by getting everything to the right bank or\nuntil they make a mistake.\n\nAs the ECE major on your team, you get to design the FSM!\n\n{choose a state representation}{step-repn}\n{calculate logic expressions}{step-logic}\n{implement with flip-flops and gates}{step-gates}\n\nSince the four entities (farmer, fox, goose, and corn) can be only\non one bank or the other, we can use one bit to represent the location\nof each entity.  Rather than giving the states names, let's just\ncall a state FXGC.  The value of F represents the location of the farmer,\neither on the left bank (F=0) or the right bank (F=1).  Using\nthe same representation (0 for the left bank, 1 for the right bank),\nthe value of X represents the location of the fox, G represents the\nlocation of the goose, and C represents the location of the corn.\n\n\nWe can now put together an abstract next-state table, as shown to\nthe right.  Once the player wins or loses, let's have the game indicate\ntheir final status and stop accepting requests to have the farmer cross\nthe river.  We can use a reset button to force the game back into the\noriginal state for the next player.\n\nNote that we have included conditions for some of the input events, as \nwe did previously\n\n\n{c|l|c|c}\ninitial state& {|c|}{input event}& condition& final state \nFXGC& no input& always& FXGC\nFXGC& reset & always& 0000\nFXGC& cross alone& always& XGC\nFXGC& cross with fox& F=X& GC\nFXGC& cross with goose& F=G& XC\nFXGC& cross with corn& F=C& XG\n\n\n\nwith the vending machine design.\n\nThe conditions here require that the farmer be on the same bank as any\nentity that the player wants the farmer to carry across the river.\n\nNext, we specify the I/O interface. \n\nFor input, the game has five buttons.  A reset button R forces the\nFSM back into the initial state.  The other four buttons cause the\nfarmer to cross the river: B_F crosses alone, B_X with the fox,\nB_G with the goose, and B_C with the corn.\n\nFor output, we need position indicators for the four entities, but let's\nassume that we can simply output the current state FXGC and have\nappropriate images or lights appear on the correct banks of the \nriver.  We also need two more indicators: W for reaching the winning\nstate, and L for reaching a losing state.\n\nNow we are ready to complete the specification.  We could use a strict\nprioritization of input events, as we did with earlier examples.  Instead,\nin order to vary the designs a bit, we use a strict prioritization among \nallowed inputs.  The reset button R has the highest priority, followed\nby B_F, B_C, B_G, and finally B_X.  However, only those buttons \nthat result in an allowed move are considered when selecting one button \namong several pressed in a single clock cycle.\n\n\nAs an example, consider the \nstate FXGC=0101.  The farmer is not on the same bank as the fox, nor\nas the corn, so the B_X and B_C buttons are ignored, leading to the\nnext-state table to the right.  Notice that B_G is accepted even if B_C\nis pressed because the farmer is not on the same\n\n\n{cccccc|c}\nFXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+ \n0101& 1& x& x& x& x& 0000\n0101& 0& 1& x& x& x& 1101\n0101& 0& 0& x& 1& x& 1111\n0101& 0& 0& x& 0& x& 0101\n\n\n\nbank as the corn.\nAs shown later, this approach to prioritization\nof inputs is equally simple in terms of implementation.  \n\n\nRecall that we want to stop the game when the player wins or loses.\nIn these states, only the reset button is accepted.  For example, the \nstate FXGC=0110 is a losing state because\n\n\n{cccccc|c}\nFXGC& R& B_F& B_X& B_G& B_C& F^+X^+G^+C^+ \n0110& 1& x& x& x& x& 0000\n0110& 0& x& x& x& x& 0110\n\n\n\nthe farmer\nhas left the fox with the goose on the opposite side of the river.\nIn this case, the player can reset the game, but other buttons\nare ignored.\n\nAs we have already chosen a representation, we now move on to implement\nthe FSM.  Let's begin by calculating the next state ignoring the\nreset button and the winning and losing states, as shown in the logic \ndiagram below.\n\n\n\nThe left column of XOR gates determines whether the farmer is on the same\nbank as the corn (top gate), goose (middle gate), and fox (bottom gate).\nThe output of each gate is then used to mask out the corresponding button:\nonly when the farmer is on the same bank are these buttons considered.\nThe adjusted button values are then fed into a priority encoder, which\nselects the highest priority input event according to the scheme that\nwe outlined earlier (from highest to lowest, B_F, B_C, B_G, and\nB_X, ignoring the reset button).  \n\nThe output of the priority encoder is then used to drive another column\nof XOR gates on the right in order to calculate the next state.\nIf any of the allowed buttons is pressed, the priority encoder \noutputs P=1, and the farmer's bank is changed.  If B_C is allowed\nand selected by the priority encoder (only when B_F is not pressed),\nboth the farmer and the corn's banks are flipped.  The goose and the fox\nare handled in the same way.\n\n\nNext, let's build a component to produce the win and lose signals.\nThe one winning state is FXGC=1111, so we simply need an AND gate.\nFor the lose signal L, we can fill in a {K-map} and derive an \nexpression, as shown to the right, then implement as shown in the logic\ndiagram to the far right.  For the {K-map}, remember that the player\nloses whenever the fox and the goose are on the same side of the river,\nbut opposite from the farmer, or whenever the goose and the corn are on\nthe same side of the river, but opposite from the farmer.\n\n\n\n{eqnarray*}\nL &=& F   +  X G +\n&& F   +  G C\n{eqnarray*}\n\n\n\n\n\n\nFinally, we complete our design by integrating the next-state\ncalculation and the win-lose calculation with a couple of muxes, as\nshown to the right.\n\nThe lower mux controls the final value of the next state: note that\nit selects between the constant state FXGC=0000 when the reset button R\nis pressed and the output of the upper mux when R=0.\n\nThe upper mux is controlled by W+L, and retains the current state\nwhenever either signal is 1.  In other words, once the player has won\nor lost, the upper mux prevents further state changes until the reset\nbutton is pressed.\n\nWhen R, W, and L are all 0, the next state is calculated\naccording to whatever buttons have been pressed.\n\n\n\n\n\n\n\n \n Question: What is the output of the priority encoder? \n Answer: \n\nA. The output of the priority encoder is 011."}, {"text": "Title: Analysis of a Stoplight Controller \n Text: {Analysis of a Stoplight Controller}\n\nIn this example, we begin with a digital FSM design and analyze it to\nunderstand how it works and to verify that its behavior is appropriate.\n\nThe FSM that we analyze has been designed to control the stoplights\nat the intersection of two roads.  For naming purposes, we assume that one\nof the roads runs East and West (EW), and the second runs North and \nSouth (NS).  \n\nThe stoplight controller has two inputs, each of which \nsenses vehicles approaching from either direction on one of the two roads.\nThe input V^=1 when a vehicle approaches from either the East or the\nWest, and the input V^=1 when a vehicle approaches from either the\nNorth or the South.  These inputs are also active when vehicles are stopped\nwaiting at the corresponding lights.\n\nAnother three inputs, A, B, and C, control the timing behavior of the\nsystem; we do not discuss them here except as variables.\n\n\nThe outputs of the controller consist of two {2-bit} values, \nL^ and L^, that specify the light colors for the two roads.\nIn particular, L^ controls the lights facing East and West, and\nL^ controls the lights facing North and South.  The meaning of these\noutputs is given in the table to the right.\n\n\n{c|c}\nL& light color \n0x& red\n10& yellow\n11& green\n\n\n\nLet's think about the basic operation of the controller.\n\nFor safety reasons, the controller must ensure that the lights on one\nor both roads are red at all times.  \n\nSimilarly, if a road has a green light, the controller should \nshow a yellow light before showing a red light to give drivers some\nwarning and allow them to slow down.\n\nFinally, for fairness, the controller should alternate green lights\nbetween the two roads.\n\nNow take a look at the logic diagram below.\n\nThe state of the FSM has been split into two pieces: a {3-bit} \nregister S and a {6-bit} timer.  The timer is simply a binary \ncounter that counts downward and produces an output of Z=1 when it \nreaches 0.  Notice that the register S only takes a new value\nwhen the timer reaches 0, and that the Z signal from the timer\nalso forces a new value to be loaded into the timer in the next \ncycle.  We can thus think of transitions in the FSM on a cycle by \ncycle basis as consisting of two types.  The first type simply\ncounts downward for a number of cycles while holding the register S\nconstant, while the second changes the value of S and sets the\ntimer in order to maintain the new value of S \nfor some number of cycles.\n\n\n\n3.45\n\n\nLet's look at the next-state logic for S, which feeds into the IN\ninputs on the {3-bit} register (S_2^+=IN_2 and so forth).  Notice \nthat none of the inputs to the FSM directly affect these values.  The\nstates of S thus act like a counter.  By examining the connections,\nwe can derive equations for the next state and draw a transition\ndiagram, as shown to the right.\n\nAs the figure shows, there are six states in the loop defined by the \nnext-state logic, with the two remaining states converging into the\nloop after a single cycle.\n\nLet's now examine the outputs for each\nstate in order to understand how the stoplight sequencing\nworks.\n\nWe derive equations for the outputs that control the lights, as shown\nto the right, then calculate values and colors for each\nstate, as shown to the far right.  For completeness, the table \nincludes the states outside of the desired loop.  The \nlights are all red in both of these states, which is necessary for safety.\n\n\n{eqnarray*}\n\nS_2^+ &=& {S_2} + S_0\nS_1^+ &=& {S_2}  S_1\nS_0^+ &=& {S_2}\n{eqnarray*}\n\n{eqnarray*}\nL_1^ &=& S_2 S_1\nL_0^ &=& S_0\nL_1^ &=& S_2 {S_1}\nL_0^ &=& S_0\n{eqnarray*}\n\n\n\n{c|cc|cc}\n&&& EW& NS\n&&& light& light\nS& L^& L^& color& color \n000& 00& 00&    red&    red\n111& 11& 01&  green&    red\n110& 10& 00& yellow&    red\n010& 00& 00&    red&    red\n101& 01& 11&    red&  green\n100& 00& 10&    red& yellow \n001& 01& 01&    red&    red\n011& 01& 01&    red&    red\n\n\n\n\nNow let's think about how the timer works.  As we already noted, the\ntimer value is set whenever S enters a new state, but it can also be\nset under other conditions---in particular, by the signal F calculated\nat the bottom of the FSM logic diagram.  \n\n\nFor now, assume that F=0.  In this case, the timer is set only when\nthe state S changes, and we can find the duration of each state by\nanalyzing the muxes.  The bottom mux selects A when S_2=0, and \nselects the output of the top mux when S_2=1.  The top mux selects B\nwhen S_0=1, and selects C when S_0=0.  Combining these results,\nwe can calculate the duration of the next states of S when F=0, \nas shown in the table to the right.  We can then combine the next\nstate duration with our previous calculation of the state sequencing \n(also the order in the table) to obtain the durations of each state, also\nshown in the rightmost column of the table.\n\n\n{c|cc|cc}\n& EW& NS& next& current\n& light& light& state& state\nS& color& color& duration& duration \n000&    red&    red& A& C\n111&  green&    red& B& A\n110& yellow&    red& C& B\n010&    red&    red& A& C\n101&    red&  green& B& A\n100&    red& yellow& C& B \n001&    red&    red& A& ---\n011&    red&    red& A& ---\n\n\n\nWhat does F do?  Analyzing the gates that produce it gives \nF=S_1S_0{V^+{S_1}S_0{V^.  If we \nignore the two states outside of the main loop for S, the first term \nis 1 only when the lights are green on the East and West roads and the \ndetector for the North and South roads indicates that no vehicles are \napproaching.  Similarly, the second term is 1 only when the lights are \ngreen on the North and South roads and the detector for the East and \nWest roads indicates that no vehicles are approaching.\n\nWhat happens when F=1?  First, the OR gate feeding into the timer's\nLD input produces a 1, meaning that the timer loads a new value\ninstead of counting down.  Second, the OR gate controlling the lower\nmux selects the A input.  In other words, the timer is reset to A\ncycles, corresponding to the initial value for the green light states.\nIn other words, the light stays green until vehicles approach on \nthe other road, plus A more cycles.\n\nUnfortunately, the signal F may also be 1 in the unused states of S,\nin which case the lights on both roads may remain red even though cars\nare waiting on one of the roads.  To avoid this behavior, we must be \nsure to initialize the state S to one of the six states in the\ndesired loop.\n\n\n\n \n Question: What is the main purpose of the FSM? \n Answer: \n\nA. The FSM always has at least one red light on at all times by design. This is to ensure safety at the intersection."}, {"text": "Title: From FSM to Computer \n Text: {From FSM to Computer}\n\nThe FSM designs we have explored so far have started with a human-based\ndesign process in which someone writes down the desired behavior in\nterms of states, inputs, outputs, and transitions.  Such an approach\nmakes it easier to build a digital FSM, since the abstraction used\ncorresponds almost directly to the implementation.\n\nAs an alternative, one can start by mapping the desired task into a\nhigh-level programming language, then using components such as registers,\ncounters, and memories to implement the variables needed.  In this approach,\nthe control structure of the code maps into a high-level FSM design.\nOf course, in order to implement our FSM with digital logic, we eventually\nstill need to map down to bits and gates.\n\nIn this set of notes, we show how one can transform a piece of code\nwritten in a high-level language into an FSM.  This process is meant to\nhelp you understand how we can design an FSM that executes simple\npieces of a flow chart such as assignments, { if} statements, and \nloops.  Later, we generalize this concept and build an FSM that allows\nthe pieces to be executed to be specified after the FSM is built---in \nother words, the FSM executes a program specified by bits stored in \nmemory.  This more general model, as you might have already guessed, \nis a computer.  \n\n \n Question: What is the difference between an FSM and a computer? \n Answer: \n\nA. The difference between an FSM and a computer is that an FSM is a specific type of finite state machine that is used to execute simple pieces of a flow chart, while a computer is a more general model that allows the pieces to be executed to be specified after the FSM is built."}, {"text": "Title: Specifying the Problem \n Text: {Specifying the Problem}\n\nLet's begin by specifying the problem that we want to solve.\nSay that we want to find the minimum value in a set of 10 integers.\nUsing the C programming language, we can write the following fragment of \ncode:\n\n\n{\n\naaaa=aaaa=\nint >values[10];    /* 10 integers--filled in by other code */\nint >idx;\nint >min\n\nmin = values[0];\nfor (idx = 1; 10 > idx; idx = idx + 1) {\n>  if (min > values[idx]) {\n>  >  min = values[idx];\n>  }\n}\n/* The minimum value from array is now in min. */\n\n}\n\n\nThe code uses array notation, which we have not used previously in our \nclass, so let's first discuss the meaning of the code.\n\nThe code uses three variables.\n\nThe variable { values} represents the 10 values in our set.\nThe suffix ``[10]'' after the variable name tells the compiler that\nwe want an array of 10 integers ({ int}) indexed from 0 to 9.\nThese integers can be treated as 10 separate variables, but can be\naccessed using the single name ``{ values}'' along with an index\n(again, from 0 to 9 in this case).\n\nThe variable { idx} holds a loop index that we use to examine each\nof the values one by one in order to find the minimum value in the set.\n\nFinally, the variable { min} holds the smallest known value as \nthe program examines each of the values in the set.\n\nThe program body consists of two statements.  \n\nWe assume that some other piece of code---one not shown here---has \ninitialized the 10 values in our set before the code above executes.\n\nThe first statement initializes the\nminimum known value ({ min}) to the value stored at index 0 in the \narray ({ values[0]}).\nThe second statement is a loop in which the variable { index} \ntakes on values from 1 to 9.  For each value, an { if} statement\ncompares the current\nknown minimum with the value stored in the array at index given by the\n{ idx} variable.  If the stored value is smaller, the current known \nvalue (again, { min}) is updated to reflect the program's\nhaving found a smaller value.  When the loop finishes all nine iterations,\nthe variable { min} holds the smallest value among the set of 10 \nintegers stored in the { values} array.\n\n\nAs a first step towards designing an FSM to implement the code, we transform\nthe code into a flow chart, as shown to the right.  The program again begins\nwith initialization, which appears in the second column of the flow chart.  \nThe loop in the program translates to the third column of the flow chart, \nand the { if} statement to the middle comparison and update \nof { min}.\n\nOur goal is now to design an FSM to implement the flow chart.  In order\nto do so, we want to leverage the same kind of abstraction that we used\nearlier, when extending our keyless entry system with a timer.  Although the\ntimer's value was technically also\n\n\n{{file=part3/figs/part3-min-flow-chart.eps,width=3.78in}}\n\n\npart of the FSM's state, we treated it\nas data and integrated it into our next-state decisions in only a couple\nof cases.\n\nFor our minimum value problem, we have two sources of data.  First, an\nexternal program supplies data in the form of a set of 10 integers.  If\nwe assume {32-bit} integers, these data technically form 320 input bits!\nSecond, as with the keyless entry system timer, we have data used internally\nby our FSM, such as the loop index and the current minimum value.  These\nare technically state bits.  For both types of data, we treat them\nabstractly as values rather than thinking of them individually as bits,\nallowing us to develop our FSM at a high-level and then to implement it \nusing the components that we have developed earlier in our course.\n\n \n Question: What is the code used for? \n Answer: \n\nA. The code uses three variables."}, {"text": "Title: Choosing Components and Identifying States \n Text: {Choosing Components and Identifying States}\n\nNow we are ready to design an FSM that implements the flow chart.\nWhat components do we need, other than our state logic?\nWe use registers and counters to implement the variables { idx}\nand { min} in the program.\nFor the array { values}, we use a {1632-bit} \nmemory.{We technically only need a {1032-bit} \nmemory, but we round up the size of the address space to reflect more\nrealistic memory designs; one can always optimize later.}\nWe need a comparator to implement the test for the { if} statement.\nWe choose to use a serial comparator, which allows us to illustrate again\nhow one logical high-level state can be subdivided into many actual states.\nTo operate the serial comparator, we make use of two shift registers that \npresent the comparator with one bit per cycle on each input, and a counter\nto keep track of the comparator's progress.\n\nHow do we identify high-level states from our flow chart?  Although\nthe flow chart attempts to break down the program into `simple' steps,\none step of a flow chart may sometimes require more than one state\nin an FSM.  Similarly, one FSM state may be able to implement several\nsteps in a flow chart, if those steps can be performed simultaneously.\nOur design illustrates both possibilities.\n\nHow we map flow chart elements into FSM states also depends to some \ndegree on what components we use, which is why we began with some discussion\nof components.  In practice, one can go back and forth between the two, \nadjusting components to better match the high-level states, and adjusting \nstates to better match the desired components.\n\nFinally, note that we are only concerned with high-level states, so we do \nnot need to provide details (yet) down to the level of individual clock \ncycles, but we do want to define high-level states that can be implemented\nin a fixed number of cycles, or at least a controllable number of cycles.\nIf we cannot specify clearly when transitions occur from an FSM state, we\nmay not be able to implement the state.\n\n\n\nNow let's go through the flow chart and identify states.  Initialization of\n{ min} and { idx} need not occur serially, and the result of the\nfirst comparison between { idx} and the constant 10 is known in advance,\nso we can merge all three operations into a single state, which we \ncall { INIT}.\n\nWe can also merge the updates of { min} and { idx} into a second\nFSM state, which we call { COPY}.  However, the update to { min} \noccurs only when the comparison ({ min > value[idx]}) is true.  \nWe can use logic to predicate execution of the update.  In other words, we \ncan use the output of the comparator, which is available after the comparator \nhas finished comparing the two values (in a high-level FSM state that we \nhave yet to define), to determine whether or not the register holding \n{ min} loads a new value in the { COPY} state.\n\nOur model of use for this FSM involves external logic filling the memory\n(the array of integer values), executing the FSM ``code,'' and then\nchecking the answer.  To support this use model, we create a FSM state \ncalled { WAIT} for cycles in which the FSM has no work to do.\nLater, we also make use of an external input signal { START} \nto start the FSM\nexecution.  The { WAIT} state logically corresponds to the ``START'' \nbubble in the flow chart.\n\n\nOnly the test for the { if} statement remains.  Using a serial\ncomparator to compare two {32-bit} values requires 32 cycles.\nHowever, we need an additional cycle to move values into our shift \nregisters so that the comparator can see the first bit.  Thus our\nsingle comparison operation breaks into two high-level states.  In the\nfirst state, which we call { PREP}, we copy { min} to one\nof the shift registers, copy { values[idx]} to the other shift\nregister, and reset the counter that measures the cycles needed for\nour serial comparator.  We then move to a second high-level state,\nwhich we call { COMPARE}, in which we feed one bit per cycle\nfrom each shift register to the serial comparator.  The { COMPARE} \nstate\n\n\n{{file=part3/figs/part3-min-flow-chart-states.eps,width=3.96in}}\n\n\nexecutes for 32 cycles, after which the comparator\nproduces the one-bit answer that we need, and we can move to the\n{ COPY} state.  The association between the flow chart and the\nhigh-level FSM states is illustrated in the figure shown to the right\nabove.\n\n\nWe can now also draw an abstract state diagram for our FSM, as shown\nto the right.  The FSM begins in the { WAIT} state.  After external\nlogic fills the { values} array, it signals the FSM to begin by\nraising the { START} signal.  The FSM transitions into the \n{ INIT} state, and in the next cycle into the { PREP} state.\nFrom { PREP}, the FSM always moves to { COMPARE}, where it\nremains for 32 cycles while the serial comparator executes a comparison.\nAfter { COMPARE}, the FSM moves to the { COPY}\n\n\n{{file=part3/figs/part3-min-state-diag.eps,width=3in}}\n\n\nstate, where\nit remains for one cycle.  The transition from { COPY} depends on\nhow many loop iterations have executed.  If more loop iterations remain,\nthe FSM moves to { PREP} to execute the next iteration.  If the\nloop is done, the FSM returns to { WAIT} to allow external logic\nto read the result of the computation.\n\n\n\n \n Question: What do we need to implement the test for the if statement? \n Answer: \n\nWe need registers and counters to implement the variables idx and min in the program. For the array values, we use a 1632-bit memory. We need a comparator to implement the test for the if statement."}, {"text": "Title: Laying Out Components \n Text: {Laying Out Components}\n\n\nOur high-level FSM design tells us what our components need to be able to\ndo in any given cycle.  For example, when we load new values into the shift\nregisters that provide bits to the serial comparator, we always copy \n{ min} into one shift register and { values[idx]} into the second.\nUsing this information, we can put together our components and simplify our\ndesign by fixing the way in which bits flow between them.\n\nThe figure at the right shows how we can organize our components.\nAgain, in practice, one goes back and forth thinking about states,\ncomponents, and flow from state to state.  In these notes, we \npresent only a completed design.\n\nLet's take a detailed look at each of the components.\n\nAt the upper left of the figure is a {4-bit} binary counter called\n{ IDX} to hold the { idx}\nvariable.  The counter can be reset to 0 using the { RST} input.\nOtherwise, the { CNT} input controls whether or not the counter\nincrements its value.  With this counter design, we can force { idx} \nto 0 in the { WAIT} state and then count upwards in the { INIT}\nand { COPY} states.\n\n\n{{file=part3/figs/part3-min-components.eps,width=3.84in}}\n\n\nA memory labeled { VALUES} to hold the array { values} appears \nin the upper right of\nthe figure.  The read/write control for the memory is hardwired to 1 (read)\nin the figure, and the data input lines are unattached.  To integrate \nwith other logic that can operate our FSM, we need to add more \ncontrol logic to allow writing into the memory and to attach the data\ninputs to something that provides the data bits.  The address input of\nthe memory comes always from the { IDX} counter value; in other words,\nwhenever we access this memory by making use of the data output lines,\nwe read { values[idx]}.\n\nIn the middle left of the figure is a {32-bit} register for the \n{ min} variable.  It has a control input { LD} that \ndetermines whether or not it loads a new value at the end of the clock\ncycle.  If a new value is loaded, the new value always corresponds to\nthe output of the { VALUES} memory, { values[idx]}.  Recall\nthat { min} always changes in the { INIT} state, and may change\nin the { COPY} state.  But the new value stored in { min}\nis always { values[idx]}. \nNote also that when the FSM completes its task, the result of the \ncomputation is left in the { MIN} register for external logic to\nread (connections for this purpose are not shown in the figure).\n\nContinuing downward in the figure, we see two right shift registers\nlabeled { A} and { B}.  Each has a control input { LD}\nthat enables a parallel load.  Register { A} loads from register\n{ MIN}, and register { B} loads from the memory data output\n({ values[idx]}).  These loads are needed in the { PREP}\nstate of our FSM.  When { LD} is low, the shift registers\nsimply shifts to the right.  The serial output { SO} makes the\nleast significant bit of each shift register available.  Shifting\nis necessary to feed the serial comparator in the { COMPARE} state.\n\nBelow register { A} is a {5-bit} binary counter called { CNT}.\nThe counter is used to control the serial comparator in the { COMPARE}\nstate.  A reset input { RST} allows it to be forced to 0 in the \n{ PREP} state.  When the counter value is exactly zero, the \noutput { Z} is high.\n\n\n\nThe last major component is the serial comparator, which is based on the\ndesign developed in Notes Set 3.1.  The two bits to be compared in \na cycle come from shift registers { A} and { B}.  The first\nbit indicator comes from the zero indicator of counter { CNT}.\nThe comparator actually produces two outputs ({ Z1} and { Z0}),\nbut the meaning of the\n{ Z1} output by itself is { A > B}.  In the diagram,\nthis signal has been labeled { THEN}.\n\nThere are two additional elements in the figure that we have yet to discuss.\nEach simply compares the value in a register with a fixed constant and \nproduces a {1-bit} signal.  When the FSM finishes an iteration of\nthe loop in the { COPY} state, it must check the loop condition\n({ 10 > idx}) and move either to the { PREP} state or, when\nthe loop finishes, to the { WAIT} state to let the external logic\nread the answer from the { MIN} register.  The loop is done when the\ncurrent iteration count is nine, so we compare { IDX} with nine to\nproduce the { DONE} signal.  The other constant comparison is between\nthe counter { CNT} and the value 31 to produce the { LAST} signal,\nwhich indicates that the serial comparator is on its last cycle of \ncomparison.  In the cycle after { LAST} is high, the { THEN} \noutput of the comparator indicates whether or not { A > B}.\n\n \n Question: What are the DONE and LAST signals used for? \n Answer: \n\n\nThe DONE and LAST signals are used to compare the value in a register with a fixed constant. The DONE signal compares the value in the IDX register with the constant 9, and the LAST signal compares the value in the CNT register with the constant 31. These comparisons are used to determine when the loop is finished and when the serial comparator is on its last cycle of comparison, respectively."}, {"text": "Title: Control and Data \n Text: {Control and Data}\n\nOne can think of the components and the interconnections between them\nas enabling the movement of data between registers, while the high-level \nFSM controls which data move from register to register in each cycle.  \n\nWith this model in mind, we call the components and interconnections\nfor our design the { datapath}---a term that we will see again when\nwe examine the parts of a computer in the coming weeks.\n\nThe datapath requires several inputs to control the operation of the\ncomponents---these we can treat as outputs of the FSM.\nThese signals allow the FSM to control the motion of data in the \ndatapath, so we call them { control signals}.\n\nSimilarly, the datapath produces several outputs that we can treat\nas inputs to the FSM.\n\nThe tables below summarize the control signals (left) and outputs (right)\nfrom the datapath for our FSM.\n\n[t]\n{|c|l|}\ndatapath& \ninput& {|c|} \n{ IDX.RST}& reset { IDX} counter to 0\n{ IDX.CNT}& increment { IDX} counter\n{ MIN.LD}& load new value into { MIN} register\n{ A.LD}& load new value into shift register { A}\n{ B.LD}& load new value into shift register { B}\n{ CNT.RST}& reset { CNT} counter \n\n\n[t]\n{|c|l|c|}\ndatapath& &\noutput& {|c|}& based on \n{ DONE}& last loop iteration finished& { IDX = 9}\n{ LAST}& serial comparator executing& { CNT = 31}\n&    last cycle&\n{ THEN}& { if} statement condition true& { A > B} \n{}\n{}\n\n\n\nUsing the datapath controls signals and outputs, we can now write a more\nformal state transition table for the FSM, as shown below.\n\nThe ``actions'' column of the table\nlists the changes to register and counter values\nthat are made in each of the FSM states.  The notation used to represent\nthe actions is called { register transfer language} ({ RTL}).\nThe meaning of an individual action is similar to the meaning of the \ncorresponding statement from our C code or from the flow chart.\nFor example, in the { WAIT} state, ``{ IDX  0}''\nmeans the same thing as ``{ idx = 0;}''.  In particular, both mean\nthat the value currently stored in the { IDX} counter is overwritten \nwith the number 0 (all 0 bits).\n\n\n\nThe meaning of RTL is slightly different from the usual interpretation of\nhigh-level programming languages, however, in terms of when the actions\nhappen.  A list of C statements is generally executed one at a time.\n\nIn contrast, the entire list of RTL actions\n\n\n{|c|l|c|c|}\nstate& {|c|}{actions (simultaneous)}& condition& next state \n{ WAIT}& { IDX}  0 (to read { VALUES[0]} in { INIT})& { START}& { INIT} \n&& {{ START}}& { WAIT} \n{ INIT}& { MIN}  { VALUES[IDX]} ({ IDX} is 0 in this state)& (always)& { PREP} \n& { IDX}  { IDX} + 1& & \n{ PREP}& { A}  { MIN}& (always)& { COMPARE}\n& { B}  { VALUES[IDX]}& &\n& { CNT}  0& & \n{ COMPARE}& run serial comparator& { LAST}& { COPY}\n&& {{ LAST}}& { COMPARE} \n{ COPY}& { THEN}: { MIN}  { VALUES[IDX]}& { DONE}& { WAIT} \n& { IDX}  { IDX} + 1& {{ DONE}}& { PREP} \n\n\n\nfor an FSM state is executed\nsimultaneously, at the end of the clock cycle.  As you know, an FSM moves \nfrom its current state into a new state at the end of every clock cycle,\nso actions during different cycles usually are associated with different \nstates.\nWe can, however, change the value in more than one register at the end \nof the same clock cycle, so we can execute more than one RTL action in\nthe same state, so long as the actions do not exceed the capabilities\nof our datapath (the components must be able to support the simultaneous \nexecution of the actions).  Some care must be taken with states that \nexecute for more than one cycle to ensure that repeating the \nRTL actions is appropriate.  In our design, only the { WAIT} and\n{ COMPARE} states execute for more than one cycle.  The { WAIT}\nstate resets the { IDX} counter repeatedly, which causes no problems.\nThe { COMPARE} statement has no RTL actions---all of the shifting,\ncomparison, and counting activity needed to do its work occurs within \nthe datapath itself.\n\nOne additional piece of RTL syntax needs explanation.  In the { COPY}\nstate, the first action begins with ``{ THEN}:,'' which means that the \nprefixed RTL action occurs only when the { THEN} signal is high.\nRecall that the { THEN} signal indicates that the comparator has\nfound { A > B}, so the equivalent C code is ``{ if (A > B) \n{min = values[idx]}}''.\n\n \n Question: What is the state of the FSM? \n Answer: \n\nA. The FSM moves from its current state into a new state at the end of every clock cycle."}, {"text": "Title: State Representation and Logic Expressions \n Text: {State Representation and Logic Expressions}\n\nLet's think about the representation for the FSM states.  The FSM has \nfive states, so we could use as few as three flip-flops.  Instead, we choose\nto use a { one-hot encoding}, in which any valid bit pattern has exactly\none 1 bit.  In other words, we use five flip-flops instead of three,\nand our states are represented with the bit patterns 10000, 01000, 00100,\n00010, and 00001.\n\nThe table below shows the mapping from each high-level state to \nboth the five-bit encoding for the state as well as the six control signals \nneeded for the datapath.  For each state, the values of the control signals\ncan be found by examining the actions necessary in that state.\n\n{\n{|c|c|cccccc|}\nstate& { S_4S_3S_2S_1S_0}& { IDX.RST}& { IDX.CNT}& { MIN.LD}& { A.LD}& { B.LD}& { CNT.RST} \n{ WAIT}& { 1 0 0 0 0}& 1& 0& 0& 0& 0& 0\n{ INIT}& { 0 1 0 0 0}& 0& 1& 1& 0& 0& 0\n{ PREP}& { 0 0 1 0 0}& 0& 0& 0& 1& 1& 1\n{ COMPARE}& { 0 0 0 1 0}& 0& 0& 0& 0& 0& 0\n{ COPY}& { 0 0 0 0 1}& 0& 1& { THEN}& 0& 0& 0 \n{}\n\n}\n\nThe { WAIT} state needs to set { IDX} to 0 but need not affect \nother register or counter values, so { WAIT} produces a 1 only for\n{ IDX.RST}.  The { INIT} state needs to load { values[0]} into \nthe { MIN} register while simultaneously incrementing the { IDX}\ncounter (from 0 to 1), so { INIT} produces 1s for { IDX.CNT}\nand { MIN.LD}.  The { PREP} state loads both shift registers\nand resets the counter { CNT} by producing 1s for { A.LD},\n{ B.LD}, and { CNT.RST}.  The { COMPARE} state does not\nchange any register values, so it produces all 0s.  Finally, the { COPY}\nstate increments the { IDX} counter while simultaneously loading a\nnew value into the { MIN} register.  The { COPY} state produces 1\nfor { IDX.CNT}, but must use the signal { THEN} coming from the\ndatapath to decide whether or not { MIN} is loaded.\n\n\n\nThe advantage of a one-hot encoding becomes obvious when we write\nequations for the six control signals and the next-state logic, as shown\nto the right.  \n\nImplementing the logic to complete our design now requires only a handful \nof small logic gates.\n\n\n{eqnarray*}\n{ IDX.RST} & = & { S}_4\n{ IDX.CNT} & = & { S}_3 + { S}_0\n{ MIN.LD} & = & { S}_3 + { S}_0  { THEN}\n{ A.LD} & = & { S}_2\n{ B.LD} & = & { S}_2\n{ CNT.RST} & = & { S}_2\n{eqnarray*}\n\n\n{eqnarray*}\n{ S}_4^+ & = & { S}_4  { START} + { S}_0  { DONE}\n{ S}_3^+ & = & { S}_4  { START}\n{ S}_2^+ & = & { S}_3 + { S}_0  { DONE}\n{ S}_1^+ & = & { S}_2 + { S}_1  { LAST}\n{ S}_0^+ & = & { S}_1  { LAST}\n{eqnarray*}\n\n\nNotice that the terms in each control signal can be read directly from \nthe rows of the state table and OR'd together.  The terms in each of the\nnext-state equations represent the incoming arcs for the corresponding\nstate.  For example, the { WAIT} state has one self-loop (the first\nterm) and a transition arc coming from the { COPY} state when the\nloop is done.\n\nThese expressions complete our design.\n\n\n\n\n\n \n Question: What is the advantage of using a one-hot encoding? \n Answer: \n\nA. The advantage of using a one-hot encoding is that it is easy to write equations for the six control signals and the next-state logic."}, {"text": "Title: Extending Keyless Entry with a Timeout \n Text: {Extending Keyless Entry with a Timeout}\n\nThis set of notes builds on the keyless entry control FSM that we\ndesigned earlier.  In particular, we use a counter to make the alarm\ntime out, turning itself off after a fixed amount of time.  The goal\nof this extension is to illustrate how we can make use of components\nsuch as registers and counters as building blocks for our FSMs\nwithout fully expanding the design to explicitly illustrate all\npossible states.\n\n \n Question: What is the purpose of the extension? \n Answer: "}, {"text": "Title: Physical Design, Sensors, and Timing \n Text: {Physical Design, Sensors, and Timing}\n\n\nTo begin, let's review the FSM that we designed earlier for keyless \nentry.  The state transition diagram for our design is replicated to \nthe right.\n\nThe four states are labeled with state bits and output bits,\nS_1S_0/DRA, where D indicates that the driver's door should\nbe unlocked, R indicates that the rest of the doors should be\nunlocked, and A indicates that the alarm should be on.\n\nTransition arcs in the diagram are labeled with concise versions of \nthe inputs ULP (using don't cares), where U represents an unlock\nbutton, L represents a lock button, and P represents a panic \nbutton.\n\nIn this design, once a user presses the panic button P, the alarm\nsounds until the user presses the\n\n\n{file=part3/figs/ke-trans-diag-brief.eps,width=4.2in}\n\n\nlock button L to turn it off.\nInstead of sounding the alarm indefinitely, we might want to turn\nthe alarm off after a fixed amount of time.  In other words, after \nthe system has been in the ALARM state for, say, thirty or sixty seconds,\nwe might want to move back to the LOCKED state even if the user has\nnot pushed the lock button.  The blue annotation in the diagram indicates\nthe arc that we must adjust.  But thirty or sixty seconds is a large \nnumber of clock cycles, and our FSM must keep track of the time.\nDo we need to draw all of the states?\n\nInstead of following the design process that we outlined earlier, let's\nthink about how we can modify our existing design to incorporate the new\nfunctionality.  In order to keep track of time, we use a binary counter.\n\nLet's say that we want our timeout to be T cycles.\n\nWhen we enter the alarm state, we want to set the counter's value \nto T-1, then let the counter count down until it reaches 0, at \nwhich point a timeout occurs.\n\nTo load the initial value, our counter should have a parallel load \ncapability that sets the counter value when input LD=1.  When LD=0,\nthe counter counts down. \n\nThe counter also has an output Z that indicates that the counter's \nvalue is currently zero, which we can use to indicate a timeout on\nthe alarm.\n\nYou should be able to build such a counter based on what you have learned\nearlier in the class.  Here, we will assume that we can just make use of\nit.\n\nHow many bits do we need in our counter?  The answer depends on T.\nIf we add the counter to our design, the state of the counter is \ntechnically part of the state of our FSM, but we can treat it\nsomewhat abstractly.  For example, we only plan to make use of the \ncounter value in the ALARM state, so we ignore the counter bits in the\nthree other states.  In other words, S_1S_0=10 means that the system \nis in the LOCKED state regardless of the counter's value.\n\n\nWe expand the ALARM state into T separate states based on the value\nof the counter.  As shown to the right, we name the states ALARM(1) \nthrough ALARM(T).  All of these alarm states use S_1S_0=01, but\nthey can be differentiated using a ``timer'' (the counter value).\n\nWe need to make design decisions about how the arcs entering and\nleaving the ALARM state in our original design should be used once\nwe have incorporated the timeout.  As a first step, we decide that\nall arcs entering ALARM from other states now enter ALARM(1).  \nSimilarly, if the user presses the panic button P in any of the\nALARM(t) states, the system returns to ALARM(1).  Effectively, \npressing the panic button resets the timer.\n\nThe only arc leaving the ALARM state goes to the LOCKED state on \nULP=x10.  We replicate this arc for all ALARM(t) states: the\nuser can push the lock button at any time to silence the alarm.\n\nFinally, the self-loop back to the ALARM state on ULP=x00 becomes\nthe countdown arcs in our expanded states, taking ALARM(t) to ALARM(t+1),\nand ALARM(T) to LOCKED.\n\nNow that we have a complete specification for the extended design, we\ncan implement it.  We want to reuse our original design as much as possible,\nbut we have three new features that must be considered.  First, when\nwe enter the ALARM(1) state, we need to set the counter value to T-1.\nSecond, we need the counter value to count downward while in the ALARM\nstate.  Finally, we need to move back to the LOCKED state when a timeout\noccurs---that is, when the counter reaches zero.\n\n\n{file=part3/figs/ke-alarm-expansion.eps,width=1.75in}\n\n\nThe first problem is fairly easy.  Our counter supports parallel load,\nand the only value that we need to load is T-1, so we apply the constant\nbit pattern for T-1 to the load inputs and raise the LD input whenever\nwe enter the ALARM(1) state.  In our original design, we chose to enter\nthe ALARM state whenever the user pressed P, regardless of the other\nbuttons.  Hence we can connect P directly to our counter's LD input.\n\nThe second problem is handled by the counter's countdown functionality.\nIn the ALARM(t) states, the counter will count down each cycle, moving\nthe system from ALARM(t) to ALARM(t+1).\n\nThe last problem is slightly trickier, since we need to change S_1S_0.\nNotice that S_1S_0=01 for the ALARM state and S_1S_0=00 for the LOCKED\nstate.  Thus, we need only force S_0 to 0 when a timeout occurs.\nWe can use a single {2-to-1} multiplexer for this purpose.  The\n``0'' input of the mux comes from the original S_0^+ logic, and the\n``1'' input is a constant 0.  All other state logic remains unchanged.  \nWhen does a timeout occur? \n\nFirst, we must be in the ALARM(T) state, so S_1S_0=01 and the \ncounter's Z output is raised.  Second, the input combination must\nbe ULP=xx0---notice that both ULP=x00 and ULP=x10 return to \nLOCKED from ALARM(T).  A single, four-input AND gate thus suffices to\nobtain the timeout signal, {S_1}S_0Z, which we connect \nto the select input of the mux between the S_0^+ logic and the\nS_0 flip-flop.\n\nThe extension thus requires only a counter, a mux, and a gate, as shown below.\n\n{{file=part3/figs/ke-alarm-exp-impl.eps,width=2.65in}}\n\n\n\n\n\n \n Question: What is the function of the counter in the extended design? \n Answer: \nWhat is the function of the counter in the extended design?\n\nThe counter is used to keep track of time so that the alarm can sound for a fixed amount of time."}, {"text": "Title: Serialization and Finite State Machines \n Text: {Serialization and Finite State Machines}\n\nThe third part of our class builds upon the basic combinational and\nsequential logic elements that we developed in the second part.\n\nAfter discussing a simple application of stored state\nto trade between area and performance,\n\nwe introduce a powerful abstraction for formalizing and reasoning about\ndigital systems, the Finite State Machine (FSM).\n\nGeneral FSM models are broadly applicable in a range of engineering\ncontexts, including not only hardware and software design but also\nthe design of control systems and distributed systems.  We limit our\nmodel so as to avoid circuit timing issues in your first exposure, but\nprovide some amount of discussion as to how, when, and why you should \neventually learn the more sophisticated models.\n\nThrough development a range of FSM examples, we illustrate important \ndesign issues for these systems and motivate a couple of more advanced \ncombinational logic devices that can be used as building blocks.\n\nTogether with the idea of memory, another form of stored state,\nthese elements form the basis for development of our first computer.\n\nAt this point we return to the textbook, in which Chapters 4 and 5\nprovide a solid introduction to the von Neumann model of computing systems\nand the {LC-3} (Little Computer, version 3) instruction set \narchitecture.  By the end of this part of the course, you will have\nseen an example of the boundary between hardware and software, and will\nbe ready to write some instructions yourself.\n\nIn this set of notes, we cover the first few parts of this material.\nWe begin by describing the conversion of bit-sliced designs into \nserial designs, which store a single bit slice's output in \nflip-flops and then feed the outputs back into the bit slice in the next\ncycle.  As a specific example, we use our bit-sliced comparator \nto discuss tradeoffs in area and performance.  We introduce\nFinite State Machines and some of the tools used to design them,\nthen develop a handful of simple counter designs.  Before delving\ntoo deeply into FSM design issues, we spend a little time discussing\nother strategies for counter design and placing the material covered\nin our course in the broader context of digital system design.\n\nRemember that\n{ sections marked with an asterisk are provided solely for your\ninterest,} but you may need to learn this material in later\nclasses.\n\n\n \n Question: What is the difference between a finite state machine and a counter? \n Answer: \n\nThe design of a finite state machine differs from that of a counter in a few ways. For one, a finite state machine has a finite number of states that it can be in, while a counter can have an infinite number of states. Additionally, a finite state machine's output is determined by its current state, while a counter's output is determined by its current state and the inputs it receives. Finally, a finite state machine can have multiple inputs and outputs, while a counter usually has a single input and a single output."}, {"text": "Title: Serialization: General Strategy \n Text: {Serialization: General Strategy}\n\nIn previous notes, we discussed and illustrated the development of bit-sliced\nlogic, in which one designs a logic block to handle one bit of a multi-bit\noperation, then replicates the bit slice logic to construct a design for \nthe entire operation.  We developed ripple carry adders in this way in \nNotes Set 2.3 and both unsigned and 2's complement comparators in \nNotes Set 2.4.\n\n\nAnother interesting design strategy is { serialization}: rather than \nreplicating the bit slice, we can use flip-flops to store the bits passed\nfrom one bit slice to the next, then present the stored bits { to the\nsame bit slice} in the next cycle.  Thus, in a serial design, we only\nneed one copy of the bit slice logic!  The area needed for a serial design\nis usually much less than for a bit-sliced design, but such a design is\nalso usually slower.  After illustrating the general design strategy,\nwe'll consider these tradeoffs more carefully\nin the context of a detailed example.\n\n\nRecall the general bit-sliced design approach, as illustrated to the right.\n\nSome number of copies of the logic for a single bit slice are connected\nin sequence.  Each bit slice accepts P bits of operand input and\nproduces Q bits of external output.  Adjacent bit slices receive\nan additional M bits of information from the previous bit slice\nand pass along M bits to the next bit slice, generally using some\nrepresentation chosen by the designer.\n\n\n{file=part3/figs/gen-slice-comp.eps,width=3.8in}\n\n\nThe first bit slice is initialized\nby passing in constant values, and some calculation may be performed\non the final bit slice's results to produce R bits more external output.\n\n\n\nWe can transform this bit-sliced design to a serial design with a single\ncopy of the bit slice logic, M+Q flip-flops, and M gates (and sometimes\nan inverter).  The strategy is illustrated on the right below.  A single\ncopy of the bit slice operates on one set of P external input bits \nand produces one set of Q external output bits each clock cycle.  In\nthe design shown, these output bits are available during the next \ncycle, after they have been stored in the flip-flops. \nThe M bits to be passed to the ``next'' bit slice are also \nstored in flip-flops,\nand in the next cycle are provided back to the same physical bit slice\nas inputs.  The first cycle of a multi-cycle operation\nmust be handled slightly differently, so we\nadd selection logic and an control signal, F.  For the first cycle, we\napply F=1, and the initial values are passed into the bit slice.  \nFor all other bits, we apply F=0, and the values stored in the flip-flops\nare returned to the bit slice's inputs.  After all bits have passed through\nthe bit slice---after N cycles for an {N-bit} design---the\nfinal M bits are stored in the flip-flops, and the results are calculated\nby the output logic. \n\n{\n\n{file=part3/figs/init-ser-slice.eps,width=1.4in}\n\n\n{file=part3/figs/ser-slice-comp.eps,width=3.25in}\n\n}\n\nThe selection logic merits explanation.  Given that the original design\ninitialized the bits to constant values (0s or 1s), we need only simple\nlogic for selection.  The two drawings on the left above illustrate \nhow {B_i}, the complemented flip-flop output for a bit i, can be\ncombined with the first-cycle signal F to produce an appropriate input\nfor the bit slice.  Selection thus requires one extra gate for each of \nthe M inputs, and we need an inverter for F if any of the initial \nvalues is 1.\n\n\n \n Question: What is the main tradeoff of a bit-sliced design? \n Answer: \n\nThe main tradeoff of a serial design is that it is usually slower than a bit-sliced design. However, it requires less area, which may be important in some applications."}, {"text": "Title: Serialization: Comparator Example \n Text: {Serialization: Comparator Example}\n\n\nWe now apply the general strategy to a specific example, our bit-sliced\nunsigned comparator from Notes Set 2.4.  The result is shown to the right.\nIn terms of the general model, the single comparator bit slice accepts P=2\nbits of inputs each cycle, in this case a single bit from each of the two\nnumbers being compared, presented to the bit slice in increasing order\nof significance.  The bit slice produces no external output other than\nthe final result (Q=0).  Two bits (M=2) are produced each cycle by \nthe bit slice and stored\n\n\n{file=part3/figs/ser-compare.eps,width=3.5in}\n\n\ninto flip flops B_1 and B_0.  These bits\nrepresent the relationship between the two numbers compared so far\n(including only the bit already seen by the comparator bit slice).\nOn the first cycle, when the least significant bits of A and B are\nbeing fed into the bit slice, we set F=1, which forces the C_1 \nand C_0 inputs of the bit slice to 0 independent of the values stored\nin the flip-flops.  In all other cycles, F=0, and the NOR gates\nset C_1=B_1 and C_0=B_0.  Finally, after N cycles for an {N-bit}\ncomparison, the output logic---in this case simply wires, as shown \nin the dashed box---places the result of the comparison on \nthe Z_1 and Z_0 outputs (R=2 in the general model).  \nThe result is encoded in the \nrepresentation defined for constructing the bit slice (see Notes Set 2.4,\nbut the encoding does not matter here).\n\n\nHow does the serial design compare with the bit-sliced design?\n\nAs an estimate of area, let's count gates.  Our optimized \ndesign is replicated to the right for convenience.  \n\nEach bit slice requires six {2-input} gates and two inverters.  \n\nAssume that each \nflip-flop requires eight {2-input} gates and two inverters,\nso the serial design overall requires 24 gates\n\n\n{file=part3/figs/comparator-opt-nn.eps,width=4.1in}\n\n\nand six inverters to handle any number of bits.\n\nThus, for any number of bits N, the serial design is smaller\nthan the bit-sliced design, and the benefit grows with N.\n\nWhat about performance?  In Notes Set 2.4, we counted gate delays for \nour bit-sliced design.  The path from A or B to the outputs is\nfour gate delays, but the C to Z paths are all two gate delays.\nOverall, then, the bit-sliced design requires 2N+2 gate delays \nfor N bits.  What about the serial design?\n\nThe performance of the serial design is likely to be much worse\nfor three reasons.\n\nFirst, all paths in the design matter, not just the paths from bit\nslice to bit slice.  None of the inputs can be assumed to be available\nbefore the start of the clock cycle, so we must consider all paths\nfrom input to output.  \n\nSecond, we must also count gate delays for the\nselection logic as well as the gates embedded in the flip-flops.\n\nFinally, the result of these calculations may not matter, since the\nclock speed may well be limited by other logic elsewhere in the\nsystem.  If we want a common clock for all of our logic, the clock\nmust not go faster than the slowest element in the entire system, or\nsome of our logic will not work properly.\n\nWhat is the longest path through our serial comparator?\n\nLet's assume that \nthe path through a flip-flop is eight gate delays, with four on each \nside of the clock's rising edge.  The inputs A and B are\nlikely to be driven by flip-flops elsewhere in our system, so\nwe conservatively count four gate delays to A and B and\nfive gate delays to C_1 and C_0 (the extra one comes from\nthe selection logic).  The A and B paths thus dominate inside\nthe bit slice, adding four more gate delays to the outputs Z_1\nand Z_0.  Finally, we add the last four gate delays to flip\nthe first latch in the flip-flops for a total of 12 gate delays.\n\nIf we assume that our serial comparator limits the clock frequency\n(that is, if everything else in the system can use a faster clock),\nwe take 12 gate delays per cycle, or 12N gate delays to compare\ntwo {N-bit} numbers.\n\nYou might also notice that adding support for 2's complement is no \nlonger free.  We need extra logic to swap the A and B inputs\nin the cycle corresponding to the sign bits of A and B.  In\nother cycles, they must remain in the usual order.  This extra\nlogic is not complex, but adds further delay to the paths.\n\nThe bit-sliced and serial designs represent two extreme points in \na broad space of design possibilities.  Optimization of the entire\nN-bit logic function (for any metric) represents a third extreme.\nAs an engineer, you should realize that you can design systems\nanywhere in between these points as well.  At the end of Notes Set 2.4,\nfor example, we showed a design for a logic slice that compares \ntwo bits at a time.  In general, we can optimize logic for any \nnumber of bits and then apply multiple copies of the resulting\nlogic in space (a generalization of the bit-sliced approach),\nor in time (a generalization of the serialization approach),\nor in a combination of the two.\n\nSometimes these tradeoffs may happen at a higher level.\nAs mentioned in Notes Set 2.3, computer software uses \nthe carry out of an adder to perform addition of larger \ngroups of bits (over multiple clock cycles)\nthan is supported by the processor's adder hardware.\n\nIn computer system design, engineers often design hardware elements\nthat are general enough to support this kind of extension in software.\n\nAs a concrete example of the possible tradeoffs,\nconsider a serial comparator design based on the {2-bit} slice variant.\n\nThis approach leads to a serial design with 24 gates and 10 inverters, \nwhich is not much\nlarger than our earlier serial design.  In terms of gate delays,\nhowever, the new design is identical, meaning that we finish a comparison\nin half the time.  More realistic area and timing metrics show\nslightly more difference between the two designs.  These differences can \ndominate the \nresults if we blindly scale the idea to handle more bits without \nthinking carefully about the design.  Neither many-input gates\nnor gates driving many outputs work well in practice.\n\n\n\n \n Question: What is the longest path through the serial comparator? \n Answer: \n\nA. The serial comparator design uses a flip-flop to store the result of each comparison bit slice. The selection logic is used to determine when the flip-flop should be updated based on the inputs A and B. The gate delays for the selection logic must be counted in order to accurately determine the longest path through the serial comparator."}, {"text": "Title: Finite State Machines \n Text: {Finite State Machines}\n\nA { finite state machine} (or { FSM}) is a model for understanding\nthe behavior of a system by describing the system as occupying one of a\nfinite set of states, moving between these states in response to\nexternal inputs, and producing external outputs.  In any given state,\na particular input may cause the FSM to move to another state; this \ncombination is called a { transition rule}.\n\nAn FSM comprises five parts: a finite set of states, a set of \npossible inputs, a set of possible outputs, a set of transition rules,\nand methods for calculating outputs.  \n\nWhen an FSM is implemented as a digital system, all states must be\nrepresented as patterns using a fixed number of bits, all inputs must\nbe translated into bits, and all outputs must be translated into bits.\n\nFor a digital FSM, transition rules must be { complete}; in other words,\ngiven any state of the FSM, and any pattern of input bits,\na transition must be defined from that state to another state \n(transitions from a state to itself, called { self-loops}, \nare acceptable).\n\nAnd, of course, calculation of outputs for a digital FSM reduces\nto Boolean logic expressions.\n\nIn this class, we focus on clocked synchronous FSM implementations,\nin which the FSM's internal state bits are stored in flip-flops.\n\nIn this section, we introduce the tools used to describe, develop, and\nanalyze implementations of FSMs with digital logic.  In the next\nfew weeks, we will show you how an FSM can serve as the central control\nlogic in a computer.  At the same time, we will illustrate connections\nbetween FSMs and software and will make some connections with other\nareas of interest in ECE, such as the design and analysis of digital \ncontrol systems.\n\nThe table below gives a { list of abstract states} for a typical \nkeyless entry system for a car.  In this case, we have merely named the\nstates rather than specifying the bit patterns to be used for each\nstate---for this reason, we refer to them as abstract states.\nThe description of the states in the first column is an optional element\noften included in the early design stages for an FSM, when identifying\nthe states needed for the design.\nA list may also include the outputs for each state.  Again, in\nthe list below, we have specified these outputs abstractly.\n\nBy including outputs for each state,\nwe implicitly assume that outputs depend only on\nthe state of the FSM.  We discuss this assumption in more detail\nlater in these notes (see ``Machine Models''), \nbut will make the assumption throughout our class.\n\n\n\n{\n\nmeaning& state& driver's door& other doors& alarm on \nvehicle locked& LOCKED& locked& locked& no\ndriver door unlocked& DRIVER& unlocked& locked& no\nall doors unlocked& UNLOCKED& unlocked& unlocked& no\nalarm sounding& ALARM& locked& locked& yes\n\n}\n\n\n\nAnother tool used with FSMs is the { next-state table} (sometimes\ncalled a { state transition table}, or just a { state table}), \nwhich maps the current state and input combination into the next state of \nthe FSM.  The abstract variant shown below outlines desired behavior at\na high level, and is often ambiguous, incomplete, and even inconsistent.\nFor example, what happens if a user pushes two buttons?  What happens\nif they push unlock while the alarm is sounding?  These questions \nshould eventually be considered.  However, we can already start to see \nthe intended use of the design: starting from a locked car, a user\ncan push ``unlock'' once to gain entry to the driver's seat, or push\n``unlock'' twice to open the car fully for passengers.  To lock the\ncar, a user can push the ``lock'' button at any time.  And, if a user\nneeds help, pressing the ``panic'' button sets off an alarm. \n\n\n\n{\n\nstate& action/input& next state \nLOCKED& push ``unlock''& DRIVER\nDRIVER& push ``unlock''& UNLOCKED\n(any)& push ``lock''& LOCKED\n(any)& push ``panic''& ALARM\n\n}\n\n\n\n\nA { state transition diagram} (or { transition diagram}, or\n{ state diagram}),\nas shown to the right, illustrates the\ncontents of the next-state table graphically, with each state drawn\nin a circle, and arcs between states labeled with the input combinations\nthat cause these transitions from one state to another.\n\nPutting the FSM design into this graphical form does not solve the\nproblems with the abstract model.  The questions that we asked\nin regard to the next-state table remain unanswered.\n\nImplementing an FSM using digital logic requires that we translate\nthe design into bits, eliminate any ambiguity, and complete the\nspecification.  How many internal bits should we use?  What are the\npossible input values, and how are their meanings represented in bits? \nWhat are the possible output values, and how are their meanings\nrepresented in bits?  We will consider these questions for several\nexamples in the coming weeks.\n\n\n{file=part3/figs/ke-abs-trans-diag.eps,width=3.45in}\n\n\nFor now, we simply define answers for our example design, the keyless\nentry system.  Given four states, we need at \nleast _2(4)=2 bits of internal state, which we\nstore in two flip-flops and call S_1S_0.  The table below lists input\nand output signals and defines their meaning. \n\n{\n\noutputs&D& driver door; 1 means unlocked\n&R& other doors (remaining doors); 1 means unlocked\n&A& alarm; 1 means alarm is sounding\ninputs& U& unlock button; 1 means it has been pressed\n& L& lock button; 1 means it has been pressed\n& P& panic button; 1 means it has been pressed\n\n}\n\nWe can now choose a representation for our states and rewrite the list\nof states, using bits both for the states and for the outputs.  We\nalso include the meaning of each state for clarity in our example.\nNote that we can choose the internal representation in any way.  Here\nwe have matched the D and R outputs when possible to simplify\nthe output logic needed for the implementation.  The order of states\nin the list is not particularly important, but should be chosen for\nconvenience and clarity (including transcribing bits into to {K-maps}, \nfor example).\n\n{\n\n& & & driver's door& other doors& alarm on\nmeaning& state& S_1S_0& D& R& A \nvehicle locked& LOCKED& 00& 0& 0& 0\ndriver door unlocked& DRIVER& 10& 1& 0& 0\nall doors unlocked& UNLOCKED& 11& 1& 1& 0\nalarm sounding& ALARM& 01& 0& 0& 1\n\n}\n\n\n\nWe can also rewrite the next-state table in terms of bits.  We use\nGray code order on both axes, as these orders make it more \nconvenient to use {K-maps}.  The values represented in this table \nare the next FSM state given the current state S_1S_0 and the\ninputs U, L, and P.  Our symbols for the next-state bits\nare S_1^+ and S_0^+.  The ``+'' superscript\nis a common way of expressing\nthe next value in a discrete series, here induced by the use of\nclocked synchronous logic in implementing the FSM.  In other words,\nS_1^+ is the value of S_1 in the next clock cycle, and S_1^+\nin an FSM implemented as a digital system is a Boolean expression\nbased on the current state and the inputs.  For our example problem, we\nwant to be able to write down expressions for S_1^+(S_1,S_0,U,L,P) \nand S_1^+(S_1,S_0,U,L,P), as well as expressions for the \noutput logic D(S_1,S_0), R(S_1,S_0), and A(S_1,S_0). \n\n\n{\n{c|cccccccc}\ncurrent state & {|c}{ULP}\nS_1S_0& 000& 001& 011& 010& 110& 111& 101& 100 \n00& 00& 01& 01& 00& 00& 01& 01& 10\n01& 01& 01& 01& 00& 00& 01& 01& 01\n11& 11& 01& 01& 00& 00& 01& 01& 11\n10& 10& 01& 01& 00& 00& 01& 01& 11 \n\n}\n\n\nIn the process of writing out the next-state table, \nwe have made decisions for all of the questions that we asked earlier\nregarding the abstract state table.  These decisions are also reflected\nin the complete state transition diagram shown to the right.\nThe states have been extended with state bits and output bits,\nas S_1S_0/DRA.\nYou should recognize that we can\nalso leave some questions unanswered by placing x's (don't cares)\ninto our table.  However, you should also understand at this point\nthat any implementation will produce bits, not x's, so we must be\ncareful not to allow arbitrary choices unless any of the choices\nallowed is indeed acceptable for our FSM's purpose.  We will discuss\nthis process and the considerations necessary as we cover more FSM\ndesign examples.\n\n\n{file=part3/figs/ke-trans-diag.eps,width=4.2in}\n\n\nWe have deliberately omitted calculation of expressions for the\nnext-state variables S_1^+ and S_0^+,\nand for the outputs D,\nR, and A.  We expect that you are able to do so from the \ndetailed state table above, and may assign such an exercise as\npart of your homework.\n\n\n \n Question: What is a finite state machine? \n Answer: \n\nA finite state machine is a model for understanding the behavior of a system by describing the system as occupying one of a finite set of states, moving between these states in response to external inputs, and producing external outputs."}, {"text": "Title: Synchronous Counters \n Text: {Synchronous Counters}\n\nA { counter} is a clocked sequential circuit with a state diagram\nconsisting of a single logical cycle.\n\nNot all counters are synchronous.\nIn other words, not all flip-flops in a counter are required to use \nthe same clock signal.  A counter in which all flip-flops do \nutilize the same clock\nsignal is called a { synchronous counter}.  \n\nExcept for a brief introduction to other types of counters in the\nnext section, our class focuses entirely on clocked synchronous designs,\nincluding counters.\n\n {The definition ignores transition\n arcs resulting from functionalit such as a reset signal that\n forces the counter back into an initial state, \n\n\nThe design of synchronous counter circuits is a fairly straightforward\nexercise given the desired cycle of output patterns. \n\nThe task can be more complex if the internal state bits\nare allowed to differ from the output bits, so for now\nwe assume that output Z_i is equal to internal bit S_i.\nNote that distinction between internal states and outputs is necessary \nif any output pattern appears more than once in the desired cycle. \n\nThe cycle of states shown to the right corresponds to the\nstates of a {3-bit} binary counter.  The numbers in the states\nrepresent both internal state bits S_2S_1S_0 and output bits Z_2Z_1Z_0.\nWe transcribe this diagram into the next-state table shown on the left below.\nWe then write out {K-maps} for\nthe next state bits S_2^+, S_1^+, and S_0^+, as shown to the right,\nand use the {K-maps} to find expressions for these variables in\nterms of the current state.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n{\n\n{ccc|ccc}\nS_2& S_1& S_0& S_2^+& S_1^+& S_0^+ \n0& 0& 0& 0& 0& 1\n0& 0& 1& 0& 1& 0\n0& 1& 0& 0& 1& 1\n0& 1& 1& 1& 0& 0\n1& 0& 0& 1& 0& 1\n1& 0& 1& 1& 1& 0\n1& 1& 0& 1& 1& 1\n1& 1& 1& 0& 0& 0\n\n\n\n{file=part3/figs/sbin3-s2.eps,width=1in}\n{file=part3/figs/sbin3-s1.eps,width=1in}\n{file=part3/figs/sbin3-s0.eps,width=1in} {-10pt}\n{eqnarray*}\nS_2^+ =& {S_2}S_1S_0+S_2{S_1}+S_2{S_0} &= S_2(S_1S_0) \nS_1^+ =& S_1{S_0}+{S_1}S_0 &= S_1{S_0} \nS_0^+ =& {S_0} &= S_0 \n{eqnarray*}\n\n}\n\nThe first form of the expression for each next-state variable is\ntaken directly from the corresponding {K-map}.  We have rewritten\neach expression to make the emerging pattern more obvious.\n\nWe can also derive the pattern intuitively by asking the following:\n\ngiven a binary counter in state S_{N-1}S_{N-2} S_{j+1}S_jS_{j-1}\nS_1S_0, when does S_j change in the subsequent state?  The answer,\nof course, is that S_j changes when all of the\nbits below S_j are 1.  Otherwise, S_j remains the\nsame in the next state.  We thus \nwrite S_j^+=S_j (S_{j-1} S_1S_0) and\nimplement the counter as shown below for a {4-bit} design.\nNote that the \nusual order of\noutput bits along the bottom is reversed in the figure, with the most\nsignificant bit at the right rather than the left.\n\n{{file=part3/figs/ser-gating.eps,width=4.5in}}\n\nThe calculation of the left inputs to the XOR gates in the counter \nshown above is performed with a series of two-input AND gates.  Each \nof these gates AND's\nanother flip-flop value into the product.  This\napproach, called { serial gating}, implies that an {N-bit}\ncounter requires more than {N-2} gate delays\nto settle into the next state.  An alternative approach, called\n{ parallel gating}, calculates each input independently with a\nsingle logic gate, as shown below.  The blue inputs to the AND gate\nfor S_3 highlight the difference from the previous figure (note that\nthe two approaches differ only for bits S_3 and above).\nWith parallel gating, the \n{ fan-in} of the gates (the number of inputs)\nand the { fan-out} of the flip-flop outputs (number of other gates\ninto which an output feeds) grow with the size of the counter.  \nIn practice, large counters use a combination of these two approaches.\n\n{{file=part3/figs/par-gating.eps,width=4.5in}}\n\n\n \n Question: What is the main difference between a synchronous and an asynchronous counter? \n Answer: \n\nA. The main difference between a synchronous and an asynchronous counter is that all flip-flops in a synchronous counter are required to use the same clock signal, while this is not the case for an asynchronous counter."}, {"text": "Title: Ripple Counters \n Text: {Ripple Counters}\n\nA second class of\ncounter drives some of its flip-flops with a clock signal and feeds\nflip-flop outputs into the clock inputs of its remaining flip-flops,\npossibly through additional logic.  Such a counter is called a {\nripple counter}, because the effect of a clock edge ripples through\nthe flip-flops.  The delay inherent to the ripple effect, along with\nthe complexity of ensuring that timing issues do not render the design\nunreliable, are the major drawbacks of ripple counters.  Compared with\nsynchronous counters, however, ripple counters consume less energy,\nand are sometimes used for devices with restricted energy supplies.\n\n\nGeneral ripple counters\ncan be tricky because of timing issues, but certain types are easy.\n\nConsider the design of binary ripple counter.  The state diagram for \na {3-bit} binary counter is replicated to the right.\nLooking\nat the states, notice that the least-significant bit alternates with\neach state, while higher bits flip whenever the next smaller bit (to\nthe right) transitions from one to zero.  To take advantage of these\nproperties, we use positive edge-triggered D flip-flops with\ntheir complemented () outputs wired back to their inputs.\nThe clock input is fed only into the first\nflip-flop, and the complemented output of each flip-flop is also\nconnected to the clock of the next.\n\n\n{file=part3/figs/lec16-9.eps,width=1.5in}\n\n\n\nAn implementation of a {4-bit} binary ripple counter appears to the\nright.\nThe order of bits in the figure matches the order used for our synchronous\nbinary counters: least significant on the left, most significant on the\nright.\nAs you can see from the figure, the technique generalizes to arbitrarily \nlarge binary ripple coun-\n\n\n\n\n\nters, but the\ntime required for the outputs to settle after a clock edge scales with\nthe number of flip-flops in the counter.  On the other hand, an\naverage of only two flip-flops see each clock edge (1 + 1/2 + 1/4 +\n), which reduces the power requirements.{Recall that\nflip-flops record the clock state internally.  The logical activity\nrequired to record such state consumes energy.}\n\n\n\nBeginning with the state 0000, at the rising clock edge, the left (S_0)\nflip-flop toggles to 1.  The second (S_1) flip-flop sees this change as a\nfalling clock edge and does nothing, leaving the counter in\nstate 0001.  When the next rising clock edge arrives, the left\nflip-flop toggles back to 0, which the second flip-flop sees as a\nrising clock edge, causing it to toggle to 1.  The third (S_2) flip-flop\nsees the second flip-flop's change as a falling edge and does nothing,\nand the state settles as 0010.  We leave verification of the remainder \nof the cycle as an exercise.\n\n \n Question: What is the ripple effect? \n Answer: \n\nA. A ripple counter is a type of digital counter which uses a clock signal to trigger the flip-flops and feed the outputs of those flip-flops back into the clock inputs of other flip-flops. This type of counter is called a ripple counter because the effect of a clock edge ripples through the flip-flops."}, {"text": "Title: Timing Issues* \n Text: {Timing Issues*}\n\nRipple counters are a form of a more general strategy known as clock\ngating.{Fall 2012 students: This part may seem a little\nredundant, but we're going to remove the earlier mention of clock \ngating in future semesters.}\n{ Clock gating} uses logic to control the visibility of a\nclock signal to flip-flops (or latches).  Historically, digital system\ndesigners rarely used clock gating techniques because of the \ncomplexity introduced for the circuit designers, who must ensure \nthat clock edges are\ndelivered with little skew along a dynamically changing set of\npaths to flip-flops.  Today, however, the power benefits of hiding\nthe clock signal from flip-flops have made clock gating an attractive \nstrategy.\nNevertheless, digital logic designers and computer architects still almost\nnever use clock gating strategies directly.  In most of the industry,\nCAD tools insert logic for clock gating automatically.  \nA handful of companies (such as Intel and Apple/Samsung) design \ncustom circuits rather than relying on CAD tools to synthesize \nhardware designs from standard libraries of elements.\nIn these companies, clock gating is used widely by the circuit\ndesign teams, and some input is occasionally necessary from the \nhigher-level designers.\n\nMore aggressive gating strategies are also used in modern designs, but\nthese usually require more time to transition between the on and off \nstates and can be more\ndifficult to get right automatically (with the tools), hence\nhardware designers may need to provide high-level information about\ntheir designs.  A flip-flop that does not see any change in its clock\ninput still has connections to high voltage and ground, and thus allows\na small amount of { leakage current}.  In contrast,\nwith { power gating}, the voltage difference is removed, and the \ncircuit uses no power at all.  Power gating can be tricky---as you know,\nfor example, when you turn the power on, you need to make sure that\neach latch settles into a stable state.  Latches may need to be \ninitialized to guarantee that they settle, which requires time after\nthe power is restored.\n\nIf you want a deeper understanding of gating issues, take ECE482, \nDigital Integrated Circuit Design, or ECE527, System-on-a-Chip Design.\n\n\n \n Question: What is the purpose of power gating? \n Answer: \n\nA. Clock gating uses logic to control the visibility of a clock signal to flip-flops (or latches). Power gating uses logic to control the voltage difference between the flip-flops (or latches)."}, {"text": "Title: Machine Models \n Text: {Machine Models}\n\nBefore we dive fully into FSM design, we must point out that we have\nplaced a somewhat artificial restriction on the types of FSMs that\nwe use in our course.\n\nHistorically, this restriction was given a name, and machines of the type\nthat we have discussed are called Moore machines.\n\nHowever, outside of introductory classes, almost no one cares about\nthis name, nor about the name for the more general model used almost\nuniversally in hardware design, Mealy machines.\n\nWhat is the difference?  In a { Moore machine}, outputs depend only on\nthe internal state bits of the FSM (the values stored in the flip-flops).\nIn a { Mealy machine}, outputs\nmay be expressed as functions both of internal state and FSM inputs.\n\nAs we illustrate shortly, the benefit of using input signals to calculate\noutputs (the Mealy machine model) is that input bits effectively serve \nas additional system state, which means that the number of internal \nstate bits can be reduced.\n\nThe disadvantage of including input signals in the expressions for \noutput signals is that timing characteristics of input signals may not\nbe known, whereas an FSM designer may want to guarantee certain\ntiming characteristics for output signals.\n\nIn practice, when such timing guarantees are needed, the designer simply\nadds state to the FSM to accommodate the need, and the problem is solved.\n\nThe coin-counting FSM that we designed for our class' lab assignments,\nfor example, \nrequired that we use a Moore machine model to avoid sending the\nservo controlling the coin's path an output pulse that was too short\nto enforce the FSM's decision about which way to send the coin.\n\nBy adding more states to the FSM, we were able to hold the servo in\nplace, as desired.\n\nWhy are we protecting you from the model used in practice?\n\nFirst, timing issues add complexity to a topic that is complex enough \nfor an introductory course.\n\nAnd, second, most software FSMs are Moore machines, so the abstraction\nis a useful one in that context, too.\n\nIn many design contexts, the timing issues implied by a Mealy model\ncan be relatively simple to manage.  When working in a single clock\ndomain, all of the input signals come from flip-flops in the same \ndomain, and are thus stable for most of the clock cycle.  Only rarely\ndoes one need to keep additional state to improve timing characteristics\nin these contexts.  In contrast, when interacting across clock domains,\nmore care is sometimes needed to ensure correct behavior.\n\nWe now illustrate the state reduction benefit of the Mealy machine\nmodel with a simple example, an FSM that recognizes the \npattern of a 0 followed by a 1 on a single input and outputs\na 1 when it observes the pattern.\n\nAs already mentioned,\nMealy machines often require fewer flip-flops.\nIntuitively, the number of combinations of states and\ninputs is greater than the number of combinations of states alone, and\nallowing a function to depend on inputs reduces the number of internal\nstates needed.  \n\nA Mealy implementation of the FSM appears on the left below, and\nan example timing diagram illustrating the FSM's behavior is shown on\nthe right.\n\nThe machine shown below occupies state A when the last bit seen was a 0, and\nstate B when the last bit seen was a 1.\n\nNotice that the transition arcs in the state diagram\nare labeled with two values instead\nof one.  Since outputs can depend on input values as well as state,\ntransitions in a Mealy machine are labeled with input/output\ncombinations, while states are labeled only with their internal bits\n(or just their names, as shown below).  Labeling states with outputs\ndoes not make sense for a Mealy machine, since outputs may vary\nwith inputs.\n\nNotice that the outputs indicated on any given transition\nhold only until that transition is taken (at the rising clock edge), as is\napparent in the timing diagram.  When inputs are asynchronous, \nthat is,\nnot driven by the same clock signal, output pulses from a Mealy\nmachine can be arbitrarily short, which can lead to problems.\n\n\n\n{{file=part3/figs/lec17-3.eps,width=5in}}\n\n\n\nFor a Moore machine, we must create a special state in which the\noutput is high.  Doing so requires that we split state B into two\nstates, a state C in which the last two bits seen were 01, and a\nstate D in which the last two bits seen were 11.  Only state C\ngenerates output 1.  State D also becomes the starting state for the\nnew state machine.  The state diagram on the left below illustrates \nthe changes, using the transition diagram style that we introduced \nearlier to represent Moore machines.\nNotice in the associated timing diagram that the output pulse lasts a\nfull clock cycle.\n\n\n\n{{file=part3/figs/lec17-4.eps,width=5in}}\n\n\n\n\n\n \n Question: Why is the Mealy machine model more reliable in practice? \n Answer: \n\nA. There are several reasons for this. The main reason is that the Mealy machine model can be implemented with fewer flip-flops than the Moore machine model, which reduces the cost of hardware implementation. Additionally, the Mealy machine model is more flexible than the Moore machine model, and can be more easily adapted to changes in the input or output requirements. Finally, the Mealy machine model is more resistant to timing issues than the Moore machine model, which makes it more reliable in practice."}, {"text": "Title: Summary of Part 3 of the Course \n Text: {Summary of Part 3 of the Course}\n\n\nIn this short summary, we \ngive you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nStudents often find this part of the course more challenging than the\nearlier parts of the course.\n\nIn addition to these notes, you should read Chapters 4 and 5 of the \nPatt and Patel textbook, which cover the von Neumann\nmodel, instruction processing, and ISAs.  \n\nStudents typically find that the homeworks in this part of the course\nrequire more time than did those in earlier parts of the course.\nProblems on the exam will be similar in nature but designed to require \nless actual time to solve (assuming that you have been doing the homeworks).  \n\nWe'll start with the easy stuff.  \n\nYou should recognize all of these terms and be able\nto explain what they mean.  For the specific circuits, you should be able \nto draw them and explain how they work.  Actually, we don't care whether \nyou can draw something from memory---a mux, for example---provided that \nyou know what a mux does and can derive a gate diagram correctly for one \nin a few minutes.  Higher-level skills are much more valuable.\n\n[t]\n{}{{}{}\n{}{}{}\n\n{digital systems terms\n{--}{{}{}\n{}{}{}\n module\n fan-in\n fan-out\n machine models: Moore and Mealy\n\n}\n\n{simple state machines\n{--}{{}{}\n{}{}{}\n synchronous counter\n ripple counter\n serialization (of bit-sliced design)\n\n}\n\n{finite state machines (FSMs)\n{--}{{}{}\n{}{}{}\n states and state representation\n transition rule\n self-loop\n next state (+) notation\n meaning of don't care in input  combination\n meaning of don't care in output\n unused states and initialization\n completeness (with regard to  FSM specification)\n list of (abstract) states\n next-state table/state transition table/state table\n state transition diagram/transition  diagram/state diagram\n\n}\n\n{memory\n{--}{{}{}\n{}{}{}\n number of addresses\n addressability\n read/write logic\n serial/random access memory (RAM)\n volatile/non-volatile (N-V)\n static/dynamic RAM (SRAM/DRAM)\n SRAM cell\n DRAM cell\n design as a collection of cells\n coincident selection\n bit lines and sense amplifiers\n\n}\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann model\n{--}{{}{}\n{}{}{}\n{processing unit\n{--}{{}{}\n{}{}{}\n register file\n arithmetic logic unit (ALU)\n word size\n\n}\n{control unit\n{--}{{}{}\n{}{}{}\n program counter (PC)\n instruction register (IR)\n implementation as FSM\n\n}\n input and output units\n{memory\n{--}{{}{}\n{}{}{}\n memory address register (MAR)\n memory data register (MDR)\n\n}\n{processor datapath}\n\n{control signal}\n\n}\n\n{tri-state buffer\n{--}{{}{}\n{}{}{}\n meaning of Z/hi-Z output\n use in distributed mux\n\n}\n\n{instruction processing}\n{-}{{}{}\n{}{}{}\n\n\n\n{register transfer language (RTL)}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (of an encoded instruction)}\n{operation code (opcode)}\n{types of instructions}\n{-}{{}{}\n{}{}{}\n\n{data movement}\n{control flow}\n\n{addressing modes}\n{-}{{}{}\n{}{}{}\n\n\n{PC-relative}\n\n{base + offset}\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Transform a bit-sliced design into a serial design, and explain the \ntradeoffs involved in terms of area and time required to compute a result.}\n{Based on a transition diagram, implement a synchronous counter from \nflip-flops and logic gates.}\n{Implement a binary ripple counter (but not necessarily a more general \ntype of ripple counter) from flip-flops and logic gates.}\n{Given an FSM implemented as digital logic, analyze the FSM to produce \na state transition diagram.}\n{Design an FSM to meet an abstract specification for a task, including \nproduction of specified output signals, and possibly including selection \nof appropriate inputs.}\n{Complete the specification of an FSM by ensuring that each state \nincludes a transition rule for every possible input combination.}\n{Compose memory chips into larger memory systems, using additional\ndecoders when necessary.}\n{Encode {LC-3} instructions into machine code.}\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n\nAt a higher level, we expect that you understand the concepts and ideas \nsufficiently well to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Abstract design symmetries from an FSM specification in order to \nsimplify the implementation.}\n{Make use of a high-level state design, possibly with many sub-states \nin each high-level state, to simplify the implementation.}\n{Use counters to insert time-based transitions between states (such \nas timeouts).}\n{Implement an FSM using logic components such as registers, \ncounters, comparators, and adders as building blocks.}\n{Explain the basic organization of a computer's microarchitecture\nas well as the role played by elements of a von Neumann design in the\nprocessing of instructions.}\n{Identify the stages of processing an instruction (such as fetch,\ndecode, getting operands, execution, and writing back results) in a \nprocessor control unit state machine diagram.}\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the difference between the Moore and Mealy machine models, \nas well as why you might find each of them useful when designing an FSM.}\n{Understand the need for initialization of an FSM, be able to analyze \nand identify potential problems arising from lack of initialization, and \nbe able to extend an implementation to include initialization to an \nappropriate state when necessary.}\n{Understand how the choice of internal state bits for an FSM can \naffect the complexity of the implementation of next-state and output \nlogic, and be able to select a reasonable state assignment.}\n{Identify and fix design flaws in simple FSMs by analyzing an existing \nimplementation, comparing it with the specification, and removing any \ndifferences by making any necessary changes to the implementation.}\n\n\n\n\n\n{   }  empty 3rd page\n\n\n\n\n\n \n Question: What are the differences between the two models? \n Answer: \n\nA. The Moore and Mealy machine models differ in how they generate output signals. Moore machines generate outputs only on state transitions, while Mealy machines generate outputs on every clock cycle, based on the current input and state."}, {"text": "Title: Instruction Set Architecture* \n Text: {Instruction Set Architecture*}\n\nThis set of notes discusses \ntradeoffs and design elements of instruction set architectures (ISAs).\n{ The material is beyond the scope of our class, and is provided purely\nfor your interest.}  Those who find these topics interesting\nmay also want to read the ECE391 notes, which describe similar material\nwith a focus on the x86 ISA.\n\nAs you know, the ISA\ndefines the interface between software and hardware, abstracting the\ncapabilities of a computer's datapath and standardizing the format of\ninstructions to utilize those capabilities.  Successful ISAs are\nrarely discarded, as success implies the existence of large amounts of\nsoftware built to use the ISA.  Rather, they are extended, and their\noriginal forms must be supported for decades (consider, for example,\nthe IBM 360 and the Intel x86).  Employing sound design principles\nis thus imperative in an ISA.\n\n \n Question: What are some design principles that are important to consider when designing an ISA? \n Answer: \n\nA. Some design principles that are important to consider when designing an ISA include:\n\n-Ease of use: the ISA should be easy for programmers to use and understand.\n\n-Efficiency: the ISA should be designed to be efficient in terms of both execution time and memory usage.\n\n-Compatibility: the ISA should be compatible with existing software and hardware."}, {"text": "Title: Formats and Fields* \n Text: {Formats and Fields*}\n\nThe LC-3 ISA \nemploys fixed-length instructions and a load-store architecture, two\naspects that help to reduce the design space to a manageable set of\nchoices.  In a general ISA design, many other options exist for\ninstruction formats.\n\nRecall the idea of separating the bits of an instruction into\n(possibly non-contiguous) fields.  One of the fields must contain an\nopcode, which specifies the type of operation to be performed by the\ninstruction.  In the {LC-3} ISA, most opcodes specify both\nthe type of operation and the types of arguments to the operation.  \nMore generally, many addressing modes are possible for\neach operand, and we can think of the bits\nthat specify the addressing mode as a separate field, known as the\n{ mode} field.  \n\nAs a simple example, the {LC-3's} ADD and AND instructions \ncontain a {1-bit} mode field that specifies whether the second\noperand of the ADD/AND comes from a register or is an immediate value.\n\nSeveral questions must be answered in order to define the possible\ninstruction formats for an ISA.  First, are instructions fixed-length\nor variable-length?  Second, how many addresses are needed for each\ninstruction, and how many of the addresses can be memory addresses?\nFinally, what forms of addresses are possible for each operand?\nFor example, can one use\nfull memory addresses or only limited offsets relative to a register?\n\nThe answer to the first question depends on many factors, but several\nclear advantages exist for both answers.  { Fixed-length\ninstructions} are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions.  As an example of this\nsimplification, note that the {LC-3} ISA does not support \naddressing for individual bytes, only for {16-bit} words.\n\n{ Variable-length instructions} also have benefits, however.\nVariable-length encodings allow more efficient encodings, saving both\nmemory and disk space.  A register transfer operation, for example,\nclearly requires fewer bits than addition of values at two direct\nmemory addresses for storage at a third.  Fixed-length instructions\nmust be fixed at the length of the longest possible instruction,\nwhereas variable-length instructions can use lengths appropriate to\neach mode.  The same tradeoff has another form in the sense that\nfixed-length ISAs typically eliminate many addressing modes in order\nto limit the size of the instructions.  Variable-length instructions\nthus allow more flexibility; indeed, extensions to a variable-length\nISA can incorporate new addressing modes that require longer\ninstructions without affecting the original ISA.\n\nFor example, the maximum length of x86 instructions has grown from six \nbytes in 1978 (the 8086 ISA) to fifteen bytes in today's version of the\nISA.\n\n\n\n\nMoving to the last of the three questions posed for instruction format\ndefinition, operand address specification,\nwe explore a range of answers developed over the last few\ndecades.  Answers are usually chosen based on the number of bits\nnecessary, and we use this metric to organize the possibilities.  The\nfigure below separates approaches into two dimensions: the vertical\ndimension divides addressing into registers and memory, and the\nhorizontal dimension into varieties within each type.\n\n{{file=part4/figs/lec23-1.eps,width=4in}}\n\nAs a register file contains fewer registers than a memory does words,\nthe use of register operands rather than memory addresses reduces the\nnumber of bits required to specify an operand.  The {LC-3} ISA\nuses a restricted set of addressing modes to stay within the limit\nimposed by the use of {16-bit} instructions.  Both\nregister and memory addresses, however, admit a wide range of\nimplementations.\n\n{ Implicit operands} of either type require no additional bits for\nthe implicit address.  The {LC-3} procedure call instruction, JSR,\nfor example, stores the return address in R7.  No bits in the JSR \nencoding name the R7 register; R7 is used implicitly for every JSR\nexecuted.\n\nSimilarly, the procedure call instructions in many ISAs push the \nreturn address onto a stack using an implicit register for the top \nof stack pointer.\n\nMemory addresses can also be implicitly equated to other memory\naddresses.  An increment instruction operating on a memory address, for\nexample, implicitly writes the result back to the same address.  \n\nAt the opposite extreme, an instruction may include a full address,\neither to any register in the register file or to any address in the \nmemory.  The term { general-purpose registers} indicates that \nregisters are used in any operation.\n\n{ Special-purpose registers}, in contrast, split the register file\nand allow only certain registers to be used in each operation.  For\nexample, the Motorola 680x0 series, used in early Apple\nMacintosh computers, provides distinct sets of address and data\nregisters.  Loads and stores use the address registers; arithmetic,\nlogic, and shift operations use the data registers.  As a result, \neach instruction\nselects from a smaller set of registers and thus requires fewer bits\nin the instruction to name the register for use.\n\nAs full memory addresses require many more bits than full register\naddresses, a wider range of techniques has been employed to reduce the\nlength.  ``Zero page'' addresses, as defined in the 6510 (6502) ISA\nused by Commodore PET's,{My computer in junior high school.}\nC64's,{My computer in high school.} and VIC 20's, prefixed a\none-byte address with a zero byte, allowing shorter instructions when\nmemory addresses fell within the first 256 memory locations.  Assembly\nand machine language programmers made heavy use of these locations to\nproduce shorter programs.  \n\nRelative addressing is quite common in the {LC-3} ISA, in which many\naddresses are PC-relative.  Typical commerical ISAs also make use of\nrelative addressing.  The Alpha ISA, for example, has a PC-relative form of\nprocedure call with a {21-bit} offset (plus or minus a megabyte),\nand the x86 ISA has a ``short'' form of branch instructions that\nuses an {8-bit} offset.\n\nSegmented memory is a form of relative addressing that uses a register\n(usually implicit) to provide the high bits of an address and an\nexplicit memory address (or another register) to provide the low bits.\nIn the early x86 ISAs, for example, {20-bit} addresses are\nfound by adding a {16-bit} segment register extended with four\nzero bits to a {16-bit} offset.\n\n\n\n\n\n \n Question: What is one advantage of using fixed-length instructions? \n Answer: \n\nA. Fixed-length instructions are easy to fetch and decode.  A processor knows in\nadvance how many bits must be fetched to fetch a full instruction;\nfetching the opcode and mode fields in order to decide how many more\nbits are necessary to complete the instruction may require more than\none cycle.  Fixing the time necessary for instruction fetch also\nsimplifies pipelining.  Finally, fixed-length instructions simplify\nthe datapath by restricting instructions to the size of the bus and\nalways fetching properly aligned instructions."}, {"text": "Title: Addressing Architectures* \n Text: {Addressing Architectures*}\n\nOne question remains for the definition of instruction formats: how\nmany addresses are needed for each instruction, and how many of the\naddresses can be memory addresses?  The first part of this question\nusually ranges from zero to three, and is rarely allowed to go\nbeyond three.  The answer to the second part determines the {\naddressing architecture} implemented by an ISA.  We now illustrate the\ntradeoffs between five distinct addressing architectures through the\nuse of a running example, the assignment X=AB+C/D.\n\nA binary operator requires two source operands and one destination\noperand, for a total of three addresses.  The ADD instruction, for\nexample, has a { {3-address}} format:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B,C>; M[A]  M[B] + M[C]\n>or>ADD>R1,R2,R3>; R1  R2 + R3\n{-6pt}\n\nIf all three addresses can be memory addresses, the ISA is dubbed a\n{ memory-to-memory architecture}.  Such architectures may have\nsmall register sets or even lack a register file completely.  To\nimplement the assignment, we assume the availability of two memory\nlocations, T1 and T2, for temporary storage:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>MUL>T1,A,B>; T1  M[A] * M[B]\n>DIV>T2,C,D>; T2  M[C] / M[D]\n>ADD>X,T1,T2>; X  M[T1] + M[T2]\n{-6pt}\n\nThe assignment requires only three instructions to implement, but each\ninstruction contains three full memory addresses, and is thus quite long.\n\nAt the other extreme is the { load-store architecture} used by the\n{LC-3} ISA.  In a load-store architecture, only\nloads and stores can use memory addresses; all other operations use\nonly registers.  As most instructions use only registers, this type of\naddressing architecture is also called a { register-to-register\narchitecture}.  The example assignment translates to the code shown below,\nwhich assumes that R1, R2, and R3 are free for use (the instructions\nare { NOT} {LC-3} instructions, but rather a generic assembly\nlanguage for a load-store architecture). \n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>LD>R2,B>; R2  M[B]\n>MUL>R1,R1,R2>; R1  R1 * R2\n>LD>R2,C>; R2  M[C]\n>LD>R3,D>; R3  M[D]\n>DIV>R2,R2,R3>; R2  R2 / R3\n>ADD>R1,R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nEight instructions are necessary, but no instruction requires more\nthan one full memory address, and several use only register addresses,\nallowing the use of shorter instructions.  The need to move data in\nand out of memory explicitly, however, also requires a reasonably\nlarge register set, as is available in the ARM, Sparc, Alpha, and IA-64\nISAs.  \n\nArchitectures that use other combinations of memory and register\naddresses with {3-address} formats are not named.  Unary\noperators and transfer operators require only one source operand, thus\ncan use a {2-address} format (for example, NOT A,B).  Binary operations\ncan also use { {2-address}} format if one operand is implicit,\nas in the following instructions:\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>A,B>; M[A]  M[A] + M[B]\n>or>ADD>R1,B>; R1  R1 + M[B]\n{-6pt}\n\nThe second instruction, in which one address is a register and the\nsecond is a memory address, defines a { register-memory\narchitecture}.  As shown by the code on the next page, \nsuch architectures strike a balance\nbetween the two architectures just discussed.\n\n\n\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>R1,A>; R1  M[A]\n>MUL>R1,B>; R1  R1 * M[B]\n>LD>R2,C>; R2  M[C]\n>DIV>R2,D>; R2  R2 / M[D]\n>ADD>R1,R2>; R1  R1 + R2\n>ST>R1,X>; M[X]  R1\n{-6pt}\n\nThe assignment now requires six instructions using at most one memory\naddress each; like memory-to-memory architectures, register-memory\narchitectures use relatively few registers.  Note that two-register\noperations are also allowed.  Intel's x86 ISA is a register-memory\narchitecture.\n\nSeveral ISAs of the past{The 6510/6502 as well, if memory\nserves, as the 8080, Z80, and Z8000, which used to drive parlor video\ngames.}  used a special-purpose register called the accumulator for\nALU operations, and are called { accumulator architectures}.  The\naccumulator in such architectures is implicitly both a source and the\ndestination for any such operation, allowing a { {1-address}}\nformat for instructions, as shown below.\n\n{-6pt}\n=WW=WWW=WW,WW,WW=\n>>ADD>B>; ACC  ACC + M[B]\n>or>ST>E>; M[E]  ACC\n{-6pt}\n\nAccumulator architectures strike the same balance as register-memory\narchitectures, but use fewer registers.  Note that memory location X\nis used as a temporary storage location as well as the final storage\nlocation in the following code:\n\n{-6pt}\n=WWW=WW,WW,WW=\n>LD>A>; ACC  M[A]\n>MUL>B>; ACC  ACC * M[B]\n>ST>X>; M[X]  ACC\n>LD>C>; ACC  M[C]\n>DIV>D>; ACC  ACC / M[D]\n>ADD>X>; ACC  ACC + M[X]\n>ST>X>; M[X]  ACC\n{-6pt}\n\nThe last addressing architecture that we discuss is rarely used for\nmodern general-purpose processors, but may be familiar to\nyou because of its historical use in scientific and engineering calculators.\n\nA { stack architecture}\nmaintains a stack of values and draws all ALU operands from this\nstack, allowing these instructions to use a { {0-address}}\nformat.  A special-purpose stack pointer (SP) register points to the\ntop of the stack in memory, and operations analogous to load ({\npush}) and store ({ pop}) are provided to move values on and off\nthe stack.  To implement our example assignment, we first transform it\ninto postfix notation (also called reverse Polish notation):\n\n{-6pt}\n=\n>A  B  *  C  D  /  +\n{-6pt}\n\nThe resulting sequence of symbols transforms on a one-to-one basis\ninto instructions for a stack architecture:\n\n{-6pt}\n=WWWW=WW=; M[SP+1]  M[SP] + M[SP + 1], SP  SP + 1WW=AB+C/DW=AB+C/DW=\n>PUSH>A>; SP  SP - 1, M[SP]  M[A]>A\n>PUSH>B>; SP  SP - 1, M[SP]  M[B]>B>A\n>MUL>>; M[SP+1]  M[SP+1] * M[SP], SP  SP + 1>AB\n>PUSH>C>; SP  SP - 1, M[SP]  M[C]>C>AB\n>PUSH>D>; SP  SP - 1, M[SP]  M[D]>D>C>AB\n>DIV>>; M[SP+1]  M[SP+1] / M[SP], SP  SP + 1>C/D>AB\n>ADD>>; M[SP+1]  M[SP+1] + M[SP], SP  SP + 1>AB+C/D\n>POP>X>; M[X]  M[SP], SP  SP + 1\n{-6pt}\n\nThe values to the right are the values on the stack, starting with the\ntop value on the left and progressing downwards, { after the\ncompletion of each instruction}.\n\n\n\n\n\n \n Question: What is the difference between a memory-to-memory and a load-store architecture? \n Answer: \n\n\nA. In a memory-to-memory architecture, all operations can use memory addresses, whereas in a load-store architecture, only loads and stores can use memory addresses."}, {"text": "Title: Common Special-Purpose Registers* \n Text: {Common Special-Purpose Registers*}\n\nThis section illustrates the uses of special-purpose registers through\na few examples. \n\nThe { stack pointer (SP)} points to the top of the stack in memory.\nMost older architectures support push and pop operations that\nimplicitly use the stack pointer.  Modern architectures assign a\ngeneral-purpose register to be the stack pointer and reference it\nexplicitly, although an assembler may support\ninstructions that appear to use implicit operands but in fact\ntranslate to machine instructions with explicit reference to the\nregister defined to be the SP.\n\nThe { program counter (PC)} points to the next instruction to be\nexecuted.  Some modern architectures expose it as a general-purpose\nregister, although its distinct role in the implementation keeps such\na model from becoming as common as the use of a general-purpose\nregister for the SP.\n\nThe { processor status register (PSR)}, also known as the {\nprocessor status word (PSW)}, contains all status bits as well as a\nmode bit indicating whether the processor is operating in user mode or\nprivileged (operating system) mode.  Having a register with this\ninformation allows more general access than is possible solely through\nthe use of control flow instructions.\n\nThe { zero register} appears in modern architectures of the RISC\nvariety (defined in the next section of these notes).  The register is\nread-only and serves both as a useful constant and as a destination\nfor operations performed only for their side-effects (for example, setting\nstatus bits).  The availability of a zero register also allows certain\nopcodes to serve double duty.  A register-to-register add instruction\nbecomes a register move instruction when one source operand is zero.\nSimilarly, an immediate add instruction becomes an immediate load\ninstruction when one source operand is zero.\n\n\n \n Question: What is the purpose of the zero register? \n Answer: \n\nA. The zero register is used as a constant and as a destination for operations performed only for their side-effects."}, {"text": "Title: Reduced Instruction Set Computers* \n Text: {Reduced Instruction Set Computers*}\n\nBy the mid-1980's, the VAX architecture dominated the workstation and\nminicomputer markets, which included most universities.  Digital\nEquipment Corporation, the creator of the VAX, was second only to IBM\nin terms of computer sales.  VAXen, as the machines were called, used\nmicroprogrammed control units and supported numerous addressing modes\nas well as complex instructions ranging from ``square root'' to\n``find roots of polynomial equation.''\n\nThe impact of increasingly dense integrated circuit technology had\nbegun to have its effect, however, and in view of increasing processor\nclock speeds, more and more programmers were using high-level\nlanguages rather than writing assembly code.  Although assembly\nprogrammers often made use of the complex VAX instructions, compilers\nwere usually unable to recognize the corresponding high-level language\nconstructs and thus were unable to make use of the instructions.\n\nIncreasing density also led to rapid growth in memory sizes, to the\npoint that researchers began to question the need for variable-length\ninstructions.  Recall that variable-length instructions allow shorter\ncodes by providing more efficient instruction encodings.  With the\ntrend toward larger memories, code length was less important.  The\nperformance advantage of fixed-length instructions, which simplifies\nthe datapath and enables pipelining, on the other hand, was \nattractive.\n\nResearchers leveraged these ideas, which had been floating around the\nresearch community (and had appeared in some commercial architectures)\nto create { reduced instruction set computers}, or { RISC}\nmachines.  The competing VAXen were labeled { CISC} machines, which\nstands for { complex instruction set computers}.\n\nRISC machines employ fixed-length instructions and a load-store\narchitecture, allowing only a few addressing modes and small offsets.\nThis combination of design decisions enables deep pipelines and\nmultiple instruction issues in a single cycle (termed superscalar\nimplementations), and for years, RISC machines were viewed by many\nresearchers as the proper design for future ISAs.  However, companies\nsuch as Intel soon learned to pipeline microoperations after decoding\ninstructions, and CISC architectures now offer competitive if not\nsuperior performance in comparison with RISC machines.  The VAXen are\ndead, of course,{Unless you talk with customer support\nemployees, for whom no machine ever dies.}  having been replaced by\nthe Alpha, which in turn fell to x86, which is now struggling with ARM\nto enter the mobile market.  \n\n\n \n Question: Why were RISC machines viewed as the proper design for future ISAs? \n Answer: \n\nA. RISC machines were viewed as the proper design for future ISAs because they employed fixed-length instructions and a load-store architecture, which allowed only a few addressing modes and small offsets. This combination of design decisions enabled deep pipelines and multiple instruction issues in a single cycle (termed superscalar implementations), and for years, RISC machines were viewed by many researchers as the proper design for future ISAs."}, {"text": "Title: Procedure and System Calls* \n Text: {Procedure and System Calls*}\n\nA { procedure} is a sequence of instructions that executes a\nparticular task.  Procedures are used as building blocks for multiple,\nlarger tasks.  The concept of a procedure is fundamental to\nprogramming, and appears in some form in every high-level language as\nwell as in most ISAs.\n\nFor our purposes, the terms procedure, subroutine,\nfunction, and method are synonymous, although they usually have\nslightly different meanings from the linguistic point of view.\nProcedure calls are supported through { call} and { return}\ncontrol flow instructions.  The first instruction in the code below,\nfor example, transfers control to the procedure ``DoSomeWork,'' which\npresumably does some work, then returns control to the instruction\nfollowing the call.\n\n{-6pt}\n\n=DoSomeWork:WW=WWWW=DoSomeWorkWW= \n>loop:>CALL>DoSomeWork\n>>CMP>R6,#1>; compare return value in R6 to 1\n>>BEQ>loop>; keep doing work until R6 is not 1\n\n>DoSomeWork:>>> ; set R6 to 0 when all work is done, 1 otherwise \n>>RETN\n\n{-6pt}\n\nThe procedure also places a return value in R6, which the instruction\nfollowing the call compares with immediate value 1.  Until the two are\nnot equal (when all work is done), the branch returns control to the\ncall and executes the procedure again.\n\nAs you may recall, the call and return use the stack pointer to keep\ntrack of nested calls.  Sample RTL for these operations appears below.\n\n\n{eqnarray*}\n{call RTL}&&SP  SP - 1\n&& M[SP]  PC\n&& PC  {procedure start}\n{eqnarray*}\n\n\n{eqnarray*}\n{return RTL}&&PC  M[SP]\n&&SP  SP + 1\n{eqnarray*}\n\n\nWhile an ISA provides the call and return instructions necessary to\nsupport procedures, it does not specify how information is passed to\nor returned from a procedure.  A standard for such decisions is\nusually developed and included in descriptions of the architecture,\nhowever.  This { calling convention} specifies how information is\npassed between a caller and a callee.  In particular, it specifies the\nfollowing: where arguments must be placed, either in registers or in\nspecific stack memory locations; which registers can be used or\nchanged by the procedure; and where any return value must be placed.\n\nThe term ``calling convention'' is also used in the programming\nlanguage community to describe the convention for deciding what\ninformation is passed for a given call operation.  For example, are\nvariables passed by value, by pointers to values, or in some other\nway?  However, once the things to be sent are decided, the\narchitectural calling convention that we discuss here is used\nto determine where to put the data in order for the callee to be able\nto find it.\n\nCalling conventions for architectures with large register sets\ntypically pass arguments in registers, and nearly all conventions\nplace the return value in a register.  A calling convention also\ndivides the register set into { caller-saved} and \n{ callee-saved} registers.  Caller-saved registers can be modified \narbitrarily\nby the called procedure, whereas any value in a callee-saved register\nmust be preserved.  Similarly, before calling a procedure, a caller\nmust preserve the values of any caller saved registers that are needed\nafter the call.  Registers of both types usually saved on the stack by\nthe appropriate code (caller or callee).\n\n\n\n\n\nA typical stack structure appears in the figure to the right.  In\npreparation for a call, a caller first stores any caller-saved\nregisters on the stack.  Arguments to the procedure to be called are\npushed next.  The procedure is called next, implicitly pushing the\nreturn address (the address of the instruction following the call\ninstruction).  Finally, the called procedure may allocate space on the\nstack for storage of callee-saved registers as well as local\nvariables.\n\nAs an example, the following calling convention can be applied to \nan {8-register} load-store architecture similar to the {LC-3}\nISA: the first three arguments must be placed in R0 through R2 (in order), \nwith any remaining arguments on the stack; the return value must be placed \nin R6; R0 through R2 are caller-saved, as\nis R6, while R3 through R5 are callee-saved; R7 is used as the stack\npointer.  The code fragments below use this calling convention to\nimplement a procedure and a call of that procedure.\n\n\n{file=part4/figs/lec23-2.eps,width=1.25in}\n\n\n[t]\n{\n[t]\n\nint =add3 (int n1, int n2, int n3) {\n>return (n1 + n2 + n3);\n}\n\nprintf (``d'', add3 (10, 20, 30));\n\n\n\nby convention:\n= n1 is in R0\n>n2 is in R1\n>n3 is in R2\n>return value is in R6\n\n\n\n[t]\n\nadd3:  = WWWW= WWWWW= \nadd3:>ADD>R0,R0,R1\n>ADD>R6,R0,R2\n>RETN\n>\n>PUSH>R1>; save the value in R1\n>LDI>R0,#10>; marshal arguments\n>LDI>R1,#20\n>LDI>R2,#30\n>CALL>add3\n>MOV>R1,R6>; return value becomes 2nd argument\n>LDI>R0,``d''>; load a pointer to the string\n>CALL>printf\n>POP>R1>; restore R1\n\n\n\nThe add3 procedure takes three integers as arguments, adds them\ntogether, and returns the sum.  The procedure is called with the\nconstants 10, 20, and 30, and the result is printed.  By the calling\nconvention, when the call is made, R0 must contain the value 10, R1\nthe value 20, and R2 the value 30.  We assume that the caller wants to\npreserve the value of R1, but does not care about R3 or R5.  In the\nassembly language version on the right, R1 is first saved to the\nstack, then the arguments are marshaled into position, and finally the\ncall is made.  The procedure itself needs no local storage and does\nnot change any callee-saved registers, thus must simply add the\nnumbers together and place the result in R6.  After add3 returns, its\nreturn value is moved from R6 to R1 in preparation for the call to\nprintf.  After loading a pointer to the format string into R0, the\nsecond call is made, and R1 is restored, completing the translation.\n\n{ System calls} are almost identical to procedure calls.  As with\nprocedure calls, a calling convention is used: before invoking a\nsystem call, arguments are marshaled into the appropriate registers or\nlocations in the stack; after a system call returns, any result\nappears in a pre-specified register.  The calling convention used for\nsystem calls need not be the same as that used for procedure calls.\nRather than a call instruction, system calls are usually initiated\nwith a { trap} instruction, and system calls are also known as\ntraps.  With many architectures, a system call places the processor in\nprivileged or kernel mode, and the instructions that implement the\ncall are considered to be part of the operating system.  The term\nsystem call arises from this fact.\n\n\n\n\n\n \n Question: What is the name of the procedure that is used to describe its operation? \n Answer: \nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. [t]\nQ. ["}, {"text": "Title: Interrupts and Exceptions* \n Text: {Interrupts and Exceptions*}\n\nUnexpected processor interruptions arise both from interactions\nbetween a processor and external devices and from errors or unexpected\nbehavior in the program being executed.  The term { interrupt} is\nreserved for asynchronous interruptions generated by other devices,\nincluding disk drives, printers, network cards, video cards,\nkeyboards, mice, and any number of other possibilities.  {\nExceptions} occur when a processor encounters an unexpected opcode or\noperand.  An undefined instruction, for example, gives rise to an\nexception, as does an attempt to divide by zero.  Exceptions usually\ncause the current program to terminate, although many operating\nsystems will allow the program to catch the exception and to handle it\nmore intelligently.  The table below summarizes the characteristics of\nthe two types and compares them to system calls.\n\n{\n{|l|l|l|c|c|}\n{|c|}& {c|}{generated by}& {c|}& asynchronous& unexpected \ninterrupt& external device& packet arrived at network card& yes& yes\nexception& invalid opcode or operand& divide by zero& no& yes\ntrap/system call& deliberate, via trap instruction& print character to console& no& no \n\n}\n\nInterrupts occur asynchronously with respect to the program.  Most\ndesigns only recognize interrupts between instructions.  In other words, \nthe presence of interrupts is checked only after completing an instruction\nrather than in every cycle.  In pipelined designs, however,\ninstructions execute simultaneously, and the decision as to which\ninstructions occur ``before'' an interrupt and which occur ``after''\nmust be made by the processor.  Exceptions are not asynchronous in the\nsense that they occur for a particular instruction, thus no decision\nneed be made as to instruction ordering.  After determining which\ninstructions were before an interrupt, a pipelined processor discards\nthe state of any partially executed instructions that occur ``after''\nthe interrupt and completes all instructions that occur ``before.''\nThe terminated instructions are simply restarted after the interrupt\ncompletes.  Handling the decision, the termination, and the\ncompletion, however, significantly increases the design complexity of\nthe system.\n\nThe code associated with an interrupt, an exception, or a system call\nis a form of procedure called a { handler}, and is found by looking\nup the interrupt number, exception number, or trap number in a table\nof functions called a { vector table}.  Vector tables\nfor each type (interrupts, exceptions, and system calls) may be separate,\nor may be combined into a single table.\nInterrupts and exceptions share a need to save all registers and\nstatus bits before execution of the corresponding handler code (and to\nrestore those values afterward).  Generally, the\n{values---including} the status word {register---are} placed\non the stack.  With system calls, saving and restoring any necessary\nstate is part of the calling convention.  A special return from\ninterrupt instruction is used to return control from the interrupt\nhandler to the interrupted code; a similar instruction forces the\nprocessor back into user mode when returning from a system call.\n\nInterrupts are also interesting in the sense that typical computers\noften have many interrupt-generating devices but only a few\ninterrupts.  Interrupts are prioritized by number, and only an\ninterrupt with higher priority can interrupt another interrupt.\nInterrupts with equal or lower priority are blocked while an interrupt\nexecutes.  Some interrupts can also be blocked in some architectures\nby setting bits in a special-purpose register called an interrupt\nmask.  While an interrupt number is masked, interrupts of that type\nare blocked, and can not occur.\n\nAs several devices may generate interrupts with the same interrupt\nnumber, interrupt handlers can be { chained} together.  Each\nhandler corresponds to a particular device.  When an interrupt occurs,\ncontrol is passed to the handler for the first device, which accesses\ndevice registers to determine whether or not that device generated an\ninterrupt.  If it did, the appropriate service is provided.  If not,\nor after the service is complete, control is passed to the next\nhandler in the chain, which handles interrupts from the second device,\nand so forth until the last handler in the chain completes.  At this\npoint, registers and processor state are restored and control is\nreturned to the point at which the interrupt occurred.\n\n\n\n\n\n \n Question: What is an asynchronous signal from an external device indicating that it needs attention? \n Answer: \n\nAn interrupt is an asynchronous signal from an external device indicating that it requires attention."}, {"text": "Title: Control Flow Conditions* \n Text: {Control Flow Conditions*}\n\nControl flow instructions may change the PC, loading it with an\naddress specified by the instruction.  Although any addressing mode\ncan be supported, the most common specify an address directly in the\ninstruction, use a register as an address, or use an address relative\nto a register.  \n\nUnconditional control flow instructions typically provided by an ISA\ninclude procedure calls and returns, traps, and jumps.  Conditional\ncontrol flow instructions are branches, and are logically based on\nstatus bits set by two types of instructions: { comparisons} and\n{ bit tests}.  Comparisons subtract one value from another to set\nthe status bits, whereas bit tests use an AND operation to\ncheck whether certain bits are set or not in a value.\n\nMany ISAs implement\nstatus bits as special-purpose registers and implicitly set them when\nexecuting\ncertain instructions.  A branch based on R2 being less or equal to R3\ncan then be written as shown below.  The status bits are set by\nsubtracting R3 from R2 with the ALU.\n\n\n=WWWWW=WW,WW,WW=\n>CMP>R2,R3>; R2 < R3: CNZ  110, R2 = R3: CNZ  001,\n>>>;     R2 > R3: CNZ  000\n>BLE>R1>; Z  C = 1: PC  R1\n{-6pt}\n\nThe status bits are not always implemented as special-purpose\nregisters; instead, they may be kept in general-purpose registers or\nnot kept at all.  For example, the Alpha ISA stores the results of\ncomparisons in general-purpose registers, and the same branch is\ninstead implemented as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>CMPLE>R4,R2,R3>; R2  R3: R4  1, R2 > R3: R4  0\n>BNE>R4,R1>; R4  0: PC  R1\n{-6pt}\n\nFinally, status bits can be calculated, used, and discarded within\na single instruction, in which case the branch is written as follows:\n\n{-6pt}\n=WWWWW=WW,WW,WW=\n>BLE>R1,R2,R3>; R2  R3: PC  R1\n{-6pt}\n\nThe three approaches have advantages and disadvantages similar to\nthose discussed in the section on addressing architectures: the first\nhas the shortest instructions, the second is the most general and\nsimplest to implement, and the third requires the fewest instructions.\n\n\n \n Question: What is the disadvantage of using special-purpose registers to store status bits? \n Answer: \n\nA. The advantage of using special-purpose registers to store status bits is that it is the shortest instruction. The disadvantage is that it is the most general and simplest to implement."}, {"text": "Title: Stack Operations* \n Text: {Stack Operations*}\n\nTwo types of stack operations are commonly supported.  Push and pop\nare the basic operations in many older architectures, and values can\nbe placed upon or removed from the stack using these instructions.  In\nmore modern architectures, in which the SP becomes a general-purpose\nregister, push and pop are replaced with indexed loads and stores,\nthat is, loads and stores using the stack pointer and an offset as the\naddress for the memory operation.  Stack updates are performed using\nthe ALU, subtracting and adding immediate values from the SP as\nnecessary to allocate and deallocate local storage.\n\nStack operations serve three purposes in a typical architecture.  The\nfirst is to support procedure calls, as illustrated in a previous\nsection.  The second is to provide temporary storage during\ninterrupts, which was also mentioned earlier.  \n\nThe third use of stack operations is to support { spill code}\ngenerated by compilers.  Compilers first translate high-level\nlanguages into an intermediate representation much like assembly code\nbut with an extremely large (theoretically infinite) register set.\nThe final translation step translates this intermediate representation\ninto assembly code for the target architecture, assigning\narchitectural registers as necessary.  However, as real ISAs support\nonly a finite number of registers, the compiler must occasionally\nspill values into memory.  For example, if ten values are in use at\nsome point in the code, but the architecture has only eight registers,\nspill code must be generated to store the remaining two values on the\nstack and to restore them when they are needed.\n\n\n \n Question: What is the third use of stack operations? \n Answer: \n\nA. When the compiler encounters a situation where there are more values in use than there are registers available, it generates code to store the excess values on the stack and to restore them when they are needed."}, {"text": "Title: I/O* \n Text: {I/O*}\n\nAs a final topic, we now consider how a processor\nconnects to other devices to allow input and output.  We have already\ndiscussed interrupts, which are a special form of I/O in which only\nthe signal requesting attention is conveyed to the processor.\nCommunication of data occurs through instructions similar to loads and\nstores.  A processor is designed with a number of {{ I/O\nports}---usually} read-only or write-only registers to which devices\ncan be attached with opposite semantics.  That is, a port is usually\nwritten by the processor and read by a device or written by a device\nand read by the processor.\n\nThe question of exactly how I/O ports are accessed is an interesting\none.  One option is to create special instructions, such as the {\nin} and { out} instructions of the x86 architecture.  Port\naddresses can then be specified in the same way that memory addresses\nare specified, but use a distinct address space.  Just as two sets of\nspecial-purpose registers can be separated by the ISA, such an {\nindependent I/O} system separates I/O ports from memory addresses by\nusing distinct instructions for each class of operation.\n\nAlternatively, device registers can be accessed using the same load and store\ninstructions as are used to access memory.  This approach, known as\n{ memory-mapped I/O}, requires no new instructions for I/O, but\ndemands that a region of the memory address space be set aside for\nI/O.  The memory words with those addresses, if they exist, can not be\naccessed during normal processor operations.\n\n\n\n\n\n \n Question: What is the difference between independent and memory-mapped I/O? \n Answer: \n\n\n\nA. Independent I/O uses separate instructions to access I/O ports, while memory-mapped I/O uses the same load and store instructions as are used to access memory."}, {"text": "Title: Control Unit Design \n Text: {Control Unit Design}\n\nAppendix C of the Patt and Patel textbook describes a microarchitecture for\nthe LC-3 ISA, including a control unit implementation.\n\nIn this set of notes, we introduce a few concepts and strategies for\ncontrol unit design, using the textbook's {LC-3} microarchitecture \nto help illustrate them.  Several figures from the textbook are\nreproduced with permission in these notes as an aid to understanding.\n\nThe control unit of a computer based on the von Neumann model can be viewed\nas an FSM that fetches instructions from memory and executes them.  Many\npossible implementations exist both for the control unit itself and for the\nresources that it controls, the other components in the von Neumann model,\nwhich we collectively call the { datapath}.\n\nIn this set of notes, we discuss two strategies for structured control\nunit design and introduce the idea of using memories to encode logic \nfunctions.\n\nLet's begin by recalling that the control unit consists of three parts: a \nhigh-level FSM that controls instruction processing, a program counter (PC)\nregister that holds the address of the next instruction to be executed,\nand an instruction register (IR) that holds the current instruction as\nit executes.\n\nOther von Neumann components provide inputs to the control unit.  The\nmemory unit, for example, contains the instructions and data on which \nthe program executes.  \n\nThe processing unit contains a register file and condition \ncodes (N, Z, and P for the {LC-3} ISA).  \n\nThe outputs of the control unit are signals that control operation of the\ndatapath: the processing unit, the memory, and the I/O interfaces.\n\nThe basic problem that we must solve, then, for control unit design, is to \nmap instruction processing and the state of the FSM (including the PC and\nthe IR) into appropriate sequences of { control signals} for the datapath.\n\n \n Question: What are the inputs to the control unit? \n Answer: \n\nA. The inputs to the control unit are the instructions and data from memory, and the condition codes from the processing unit."}, {"text": "Title: LC-3 Datapath Control Signals \n Text: {LC-3 Datapath Control Signals}\n\nAs we have skipped over the implementation details of interrupts and \nprivilege of the {LC-3} in our class, let's consider a \nmicroarchitecture and datapath without those capabilities.  \n\nThe figure on the next page (Patt and Patel Figure C.3) shows an {LC-3}\ndatapath and control signals without support for interrupts and privilege.\n\n\nSome of the datapath control signals mentioned\nin the textbook are no longer necessary in the simplified design.\n\nLet's discuss the signals that remain and give some examples of how they\nare used.  A list appears to the right.\n\nFirst, we have a set of seven {1-bit} control signals (starting \nwith ``LD.'') that specifies whether registers in the datapath load \nnew values.\n\nNext, there are four {1-bit} signals (starting with ``Gate'') for \ntri-state buffers that control access to the bus.  These four implement \na distributed mux for the bus.  Only one value can\n\n\n\nsignal& meaning \nLD.MAR& load new value into memory address register\nLD.MDR& load new value into memory data register\nLD.IR& load new value into instruction register\nLD.BEN& load new value into branch enable register\nLD.REG& load new value into register file\nLD.CC& load new values into condition code registers (N,Z,P)\nLD.PC& load new value into program counter \nGatePC& write program counter value onto bus\nGateMDR& write memory data register onto bus\nGateALU& write arithmetic logic unit result onto bus\nGateMARMUX& write memory address register mux output onto bus \nPCMUX& select value to write to program counter (2 bits)\nDRMUX& select value to write to destination register (2 bits)\nSR1MUX& select register to read from register file (2 bits)\nADDR1MUX& select register component of address (1 bit)\nADDR2MUX& select offset component of address (2 bits)\nMARMUX& select type of address generation (1 bit) \nALUK& select arithmetic logic unit operation (2 bits) \nMIO.EN& enable memory \nR.W& read or write from memory \n\n\n\nappear on the bus in \nany cycle, so at most one of these signals can be 1; others must all \nbe 0 to avoid creating a short.\n\n\n\n{file=part4/patt-patel-appendix-C/APPC03.eps,width=6.5in}\n\n\n\nThe third group (ending with ``MUX'') of signals\ncontrols multiplexers in the datapath.\nThe number of bits for each depends on the number of inputs to the mux;\nthe total number of signals is 10.\nThe last two groups of signals control the ALU and the memory, \nrequiring a total of 4 more signals.  The total of all groups is thus\n25 control signals for the datapath without support for privilege\nand interrupts.\n\n \n Question: What is the number of signals in the LC-3 datapath? \n Answer: \nQ. 23\nQ. 26"}, {"text": "Title: Example Control Word: ADD \n Text: {Example Control Word: ADD}\n\nBefore we begin to discuss control unit design in more detail, let's\nwork through a couple of examples of implementing specific RTL with\nthe control signals available.  The figure below (Patt and Patel \nFigure C.2) shows a state machine for the {LC-3} ISA (again \nwithout detail on interrupts and privilege).\n\n{{file=part4/patt-patel-appendix-C/APPC02.eps,width=6.25in}}\n\nConsider now the state that implements the ADD instruction---state\nnumber 1 in the figure on the previous page, just below and to the \nleft of the decode state.\n\nThe RTL for the state is: DR  SR + OP2, set CC.\n\nWe can think of the 25 control signals that implement the desired RTL\nas a { control word} for the datapath. \n\nLet's begin with the register load control signals.  The RTL writes\nto two types of registers: the register file and the condition codes.\nTo accomplish these simultaneous writes, LD.REG and LD.CC must be high.\nNo other registers in the datapath should change, so the other five\nLD signals---LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC---should be low.\n\nWhat about the bus?  In order to write the result of the add operation\ninto the register file, the control signals must allow the ALU to write\nits result on to the bus.  So we need GateALU=1.  And the other Gate\nsignals---GatePC, GateMDR, and GateMARMUX---must all be 0.  The \ncondition codes are also calculated from the value on the bus, but\nthey are calculated on the same value as is written to the register\nfile (by the definition of the {LC-3} ISA).\n\nIf the RTL for an FSM state implicitly requires more than one value \nto appear on the bus in the same cycle, that state is impossible to \nimplement using the given datapath.  Either the datapath or the state \nmachine must be changed in such a case.  The textbook's design has been \nfairly thoroughly tested and debugged.\n\n\nThe earlier figure of the datapath does not \nshow all of the muxes.  The remaining muxes appear in the figure \nto the right (Patt and Patel Figure C.6).\n\nSome of the muxes in the datapath must be used to enable the\naddition needed for ADD to occur.  The\nDRMUX must select its IR[11:9] \ninput in order to write to the destination register specified by the \nADD instruction.  Similarly, the SR1MUX must select its IR[8:6] input \nin order to\n\n\n{file=part4/patt-patel-appendix-C/APPC06.eps,width=4.45in}\n\n\npass the first source register specified by the ADD to the ALU as input A\n(see the datapath figure).  SR2MUX is always controlled by the \nmode bit IR[5], so the control unit does not need to generate anything\n(note that this signal was not in the list given earlier).\n\nThe rest of the muxes in the datapath---PCMUX, ADDR1MUX, ADDR2MUX, and\nMARMUX---do not matter, and the signals controlling them are don't cares.\nFor example, since the PC does not change, the output of the PCMUX\nis simply discarded, thus which input the PCMUX forwards to its output\ncannot matter.\n\nThe ALU must perform an addition, so we must set the operation type\nALUK appropriately.  And memory should not be enabled (MIO.EN=0),\nin which case the read/write control for memory, R.W, is a don't\ncare.  These 25 signal values together (including seven don't cares)\nimplement the RTL for the single state of execution for an \nADD instruction.\n\n\n \n Question: What is the value of the gateALU? \n Answer: \n\nA. LD.REG = 1, LD.CC = 1, GateALU = 1, ALUK = 0, and MIO.EN = 0"}, {"text": "Title: Example Control Word: LDR \n Text: {Example Control Word: LDR}\n\nAs a second example, consider the first state in the sequence that \nimplements the LDR instruction---state number 6 in the figure on the \nprevious page.\n\nThe RTL for the state is: MAR  BaseR + off6, but BaseR is\nabbreviated to ``B'' in the state diagram.\n\nWhat is the control word for this state?\n\nLet's again begin with the register load control signals.  Only the\nMAR is written by the RTL, so we need LD.MAR=1 and the other load\nsignals all equal to 0.\n\nThe address (BaseR + off6) is generated by the address adder, then \npasses through the MARMUX to the bus, from which it can be written into\nthe MAR.  To allow the MARMUX to write to the bus, we set GateMARMUX\nhigh and set the other three Gate control signals low.\n\nMore of the muxes are needed for this state's RTL than we needed for\nthe ADD execution state's RTL.\n\nThe SR1MUX must again select its IR[8:6] input, this time in order to \npass the BaseR specified by the instruction to ADDR1MUX.  ADDR1MUX\nmust then select the output of the register file in order to pass the \nBaseR to the address adder.  The other input of the address adder should\nbe off6, which corresponds to the sign-\nextended version of IR[5:0].\nADDR2MUX must select this input to pass to the address adder. \nFinally, the MARMUX must select the output of the address adder.\nThe PCMUX and DRMUX do not matter for this case, and can be left as\ndon't cares.  Neither the PC nor any register in the register file\nis written.\n\nThe output of the ALU is not used, so which operation it performs is \nirrelevant, and the ALUK controls are also don't cares.  Memory is\nalso not used, so again we set MIO.EN=0.  And, as before, the R.W\ncontrol for memory is a don't care.\n\nThese 25 signal values together (again including seven don't cares)\nimplement the RTL for the first state of LDR execution.\n\n\n \n Question: What is the control word for the first state of LDR? \n Answer: \n\nThe control word for this state is:\n\nLD.MAR=1, GateMARMUX=1, ADDR1MUX=IR[8:6], ADDR2MUX=off6, MARMUX=ADDR+, PCMUX=X, DRMUX=X, ALUK=X, MIO.EN=0, R.W=X."}, {"text": "Title: Hardwired Control \n Text: {Hardwired Control}\n\n\nNow we are ready to think about how control signals can be generated.\n\nAs illustrated to the right, instruction processing consists of two \nsteps repeated infinitely: fetch an instruction, then execute the \ninstruction.\n\nLet's say that we choose a fixed number of cycles for each of these two\nsteps.  We can then control our system with a\n\n\n{file=part4/figs/inst-loop.eps,width=2.5in}\n\n\ncounter, using the counter's value and the IR to generate the\ncontrol signals through combinational\nlogic.  The PC is used only as data and has little or no direct effect on \nhow the system fetches and processes instructions.{Some ISAs do \nsplit the address space into privileged and non-privileged regions, but \nwe ignore that possibility here.}\n\nThis approach in general is called { hardwired control}.\n\nHow many cycles do we need for instruction fetch?  How many cycles do\nwe need for instruction processing?  The answers depend on the factors:\nthe complexity of the ISA, and the complexity of the datapath.\n\nGiven a simple ISA, we can design a datapath that\nis powerful enough to process any instruction in a single cycle.  \nThe control unit for such a design is\nan example of { single-cycle, hardwired control}.  While this\napproach simplifies the control unit, the cycle time\nfor the clock is limited by the slowest instruction.\nThe clock must also be slow enough to let memory \noperations complete in single cycle, both for instruction fetch and\nfor instructions that require a memory access.\n\nSuch a low clock rate is usually not acceptable.\n\nMore generally, we can use a simpler datapath and break both instruction\nfetch and instruction processing into multiple steps.  Using the datapath\nin the figure from Patt and Patel, for example, instruction fetch requires \nthree steps, and the number of steps for instruction processing depends \non the type of instruction being processed.  The state diagram shown\nearlier illustrates the steps for each opcode.\n\nAlthough the datapath is not powerful enough to complete instructions\nin a single cycle, we can build a { multi-cycle, hardwired control} \nunit (one based on combinational logic).  \n\nIn fact, this type of control unit is not much more complex than the \nsingle-cycle version.  For the control unit's FSM, we can use a binary \ncounter to enumerate first the steps of fetch, then the steps\nof processing.  The counter value along with the IR register can \ndrive combinational logic to produce control signals.  And, to avoid\nprocessing at the speed of the slowest instruction (the opcode that\nrequires the largest number of steps; LDI and STI in the {LC-3}\nstate diagram), we can add a reset signal to the counter to force it \nback to instruction fetch.  The FSM counter reset signal is simply \nanother control signal.  Finally, we can add one more signal that \npauses the counter while waiting for a memory operation to complete.  \nThe system clock can then run at the speed of the logic rather than \nat the speed of memory.\n\nThe control unit implementation discussed in Appendix C of Patt and Patel \nis not hardwired, but it does make use of a memory ready signal to \nachieve this decoupling between the clock speed of the processor and the\naccess time of the memory.\n\n\nThe figure to the right illustrates a general \nmulti-cycle, hardwired control unit.  The three blocks on\nthe left are the control unit state.  The combinational logic in\nthe middle uses the control unit state along with some\ndatapath status bits to compute the control signals for the datapath\nand the extra controls needed for the FSM counter, IR, and PC.\nThe datapath appears to the right in the figure.\n\n\n\n\n\nHow complex is the combinational logic?  As mentioned earlier, we \nassume that the PC does not directly affect control.  But we still \nhave 16 bits of IR, the FSM counter state, and the datapath status\nsignals.  Perhaps we need 24-variable {K-maps}?  \n\nHere's where engineering and human design come to \nthe rescue: by careful design of the ISA and the encoding, the\nauthors have made many of the datapath control signals for the {LC-3}\nISA quite simple.\nFor example, the register that appears on the register file's SR2\noutput is always specified by IR[2:0].  The SR1 output \nrequires a mux, but the choices are limited to IR[11:9] and IR[8:6]\n(and R6 in the design with support for interrupts).\nSimilarly, the destination register in the register file\nis always R7 or IR[11:9] (or, again, R6 when supporting interrupts).\n\nThe control signals for an {LC-3}\ndatapath depend almost entirely on the state of the control unit \nFSM (counter bits in a hardwired design) and the opcode IR[15:12].\n\nThe control signals are thus reduced to fairly simple functions.\n\nLet's imagine building a hardwired control unit for the {LC-3}.\n\nLet's start by being more precise about the number of inputs to\nthe combinational logic.\n\nAlthough most decisions are based on the opcode, the datapath\nand state diagram shown earlier for the {LC-3} ISA do have\none instance of using another instruction bit to determine behavior.\nSpecifically, the JSR instruction has two modes, and the control\nunit uses IR[11] to choose between them.  So we need to have\nfive bits of IR instead of four as input to our logic.\n\nHow many datapath status signals are needed?\n\nWhen the control unit accesses memory, it must wait until the \nmemory finishes the access, as indicated by a memory ready signal R.\nAnd the control unit must implement the conditional part of conditional\nbranches, for which it uses the datapath's branch enable signal BEN.\n\nThese two datapath status signals suffice for our design.\n\nHow many bits do we need for the counter?\nInstruction fetch requires three cycles: one to move\nthe PC to the MAR and increment the PC, a second to read from\nmemory into MDR, and a third to move the instruction bits across\nthe bus from MDR into IR.  Instruction decoding in a hardwired design\nis implicit and \nrequires no cycles: since all of our control signals can depend on\nthe IR, we do not need a cycle to change the FSM state to reflect the opcode.\nLooking at the {LC-3} state diagram, we see\nthat processing an instruction requires at most five cycles.  In\ntotal, at\nmost eight steps are needed to fetch and process any {LC-3} \ninstruction, so we can use a {3-bit} binary counter.\n\nWe thus have a total of ten bits of input: IR[15:11], R, BEN, and\na {3-bit} counter.\n\nAdding the RESET and PAUSE controls for our FSM counter to the \n25 control signals listed earlier, we need to find 27 functions\non 10 variables.\n\nThat's still a lot of big {K-maps} to solve.  Is there an\neasier way?\n\n\n \n Question: How many datapath status signals are needed for a hardwired control unit? \n Answer: \n\nA. Two datapath status signals are needed for a hardwired control unit."}, {"text": "Title: Using a Memory for Logic Functions \n Text: {Using a Memory for Logic Functions}\n\nConsider a case in which you need to compute many functions on a small \nnumber of bits, such as we just described for the multi-cycle, hardwired\ncontrol unit.  One strategy is to use a memory (possibly a read-only memory).\nA {2^m} memory can be viewed as computing N arbitrary\nfunctions on m variables.  The functions to be computed are specified\nby filling in the bits of the memory.  So long as the value of m is fairly\nsmall, the memory (especially SRAM) can be fast.\n\nSynthesis tools (or hard work) can, of course, produce smaller designs\nthat use fewer gates.  Actually, tools may be able to optimize a fixed\ndesign expressed as read-only memory, too.  But designing the functions\nwith a memory makes them easier to modify later.  If we make a mistake,\nfor example, in computing one of the functions, we need only change a bit\nor two in the memory instead of solving equations and reoptimizing and\nreplacing logic gates.  We can also extend our design if we have space\nremaining (that is, if the functions are undefined for some combinations \nof the m inputs).  The\nCray T3D supercomputer, for example, used a similar approach to add\nnew instructions to the Alpha processors on which it was based.\n\nThis strategy is effective in many contexts, so let's briefly discuss two \nanalogous cases.  In software, a memory becomes a lookup table.  Before\nhandheld calculators, lookup tables were used by humans to compute \ntranscendental functions such as sines, cosines, logarithms.  Computer \ngraphics hardware and software used a similar approach for transcendental\nfunctions in order to reduce cost and improve \nspeed.  Functions such as counting 1 bits in a word are useful for \nprocessor scheduling and networking, but not all ISAs provide this type\nof instruction.  In such cases, lookup tables in software are often the\nbest solution.\n\nIn programmable hardware such as Field Programmable Gate Arrays (FPGAs),\nlookup tables (called LUTs in this context) have played an important role\nin implementing arbitrary logic functions.\n\nThe FPGA is the modern form of the programmable logic array (PLA)\nmentioned in the textbook, and will be your main tool \nfor developing digital hardware in ECE385.\n\nFor many years, FPGAs served as a hardware prototyping platform, but\nmany companies today ship their first round products using \ndesigns mapped to FPGAs.  Why?  Chips are more and more expensive\nto design, and mistakes are costly to fix.  In contrast, while\ncompanies pay more to buy an FPGA than to produce a chip (after the \nfirst chip!), errors in the design can usually be fixed by sending \ncustomers a new version through the Internet.\n\nLet's return to our {LC-3} example.\n\nInstead of solving the {K-maps}, we can use a small memory:\n2^ bits (27,648 bits total).\n\nWe just need calculate the bits, put\nthem into the memory, and use the memory to produce the control signals.\n\nThe ``address'' input to the memory are the same 10 bits that we\nneeded for our combinational logic: IR[15:11], R, BEN, and the FSM counter.\nThe data outputs of the memory are the control signals and the\nRESET and PAUSE inputs to the FSM counter.  And we're done.\n\nWe can do a little better, though.  The datapath in the textbook was\ndesigned to work with the textbook's control unit.  If we add a little\nlogic, we can significantly simplify our memory-based, hardwired implementation.\nFor example, we only need to pause the FSM counter when waiting for memory.\nIf we can produce a control signal that indicates a need to wait for\nmemory, say {WAIT-MEM}, we can use a couple of gates to compute the\nFSM counter's PAUSE signal as {WAIT-MEM} AND (NOT R).  Making this change\nshrinks our memory to 2^9 bits.  The extra two control\nsignals in this case are RESET and {WAIT-MEM}.\n\nNext, look at how BEN is used in the state diagram:\nthe only use is to terminate the\nprocessing of branch instructions when no branch should occur (when \nBEN=0).  We can fold that functionality into the FSM counter's RESET\nsignal by producing a branch reset signal, {BR-RESET}, to reset\nthe counter to end a branch and a second signal, {INST-DONE},\nwhen an instruction is done.  The RESET input for the FSM counter\nis then ({BR-RESET} AND (NOT BEN)) OR {INST-DONE}.\n\nAnd our memory further shrinks to 2^8 bits, where the extra\nthree control signals are {WAIT-MEM}, {BR-RESET}, and \n{INST-DONE}.\n\nFinally, recall that the only need for IR[11] is to implement the two\nforms of JSR.  But we can add wires to connect SR1 to PCMUX's fourth \ninput, then control the PCMUX output selection using IR[11] when appropriate\n(using another control signal).  With this extension, we can implement\nboth forms with a single state, writing to both R7 and PC in the same\ncycle.\n\nOur final memory can then be 2^7 bits (3,712 bits total),\nwhich is less than one-seventh the number of bits that we needed before \nmodifying the datapath.\n\n\n\n\n \n Question: What is the benefit of using a memory to compute logic functions? \n Answer: \n\nA. Using a memory to compute logic functions allows for easy modification of the design if a mistake is made. Additionally, it allows for easy extension of the design if more space is needed."}, {"text": "Title: Microprogrammed Control \n Text: {Microprogrammed Control}\n\nWe are now ready to discuss the second approach to control unit design.\nTake another look at the state diagram for the the {LC-3} ISA.  Does it \nremind you of anything?  Like a flowchart, it has relatively few arcs \nleaving each state---usually only one or two.\n\nWhat if we treat the state diagram as a program?  We can use a small\nmemory to hold { microinstructions} (another name for control words) \nand use the FSM state number as the memory address.  \n\nWithout support for interrupts or privilege, and with the datapath \nextension for JSR mentioned for hardwired control, the {LC-3} state \nmachine requires fewer than 32 states.\n\nThe datapath has 25 control signals, but we need one more for the\ndatapath extension for JSR.\n\nWe thus start with {5-bit} state number (in a register)\nand a {2^5 bit} memory, which we call\nour control ROM (read-only memory) to distinguish\nit from the big, slow, von Neumann memory.\n\nEach cycle, the { microprogrammed control} unit applies the FSM state \nnumber to the control ROM (no IR bits, \njust the state number), gets back a set of control signals, and\nuses them to drive the datapath.\n\n\nTo write our microprogram, we need to calculate the control signals\nfor each microinstruction and put them in the control ROM, but we also\nneed to have a way to decide which microinstruction should execute \nnext.  We call the latter problem { sequencing} or microsequencing.\n\nNotice that most of the time there's no choice: we have only { one}\nnext microinstruction.  One simple approach is then to add the address\n(the {5-bit} state ID) of the next microinstruction to the control ROM.\nInstead of 26 bits per FSM state, we now have 31 bits per FSM state.\n\nSometimes we do need to have two possible next states.  When waiting\nfor memory (the von Neumann memory, not the control ROM) to\nfinish an access, for example, we want our FSM to stay\nin the same state, then move to the next state when the access completes.\nLet's add a second address to each microinstruction, and add\na branch control signal for the microprogram to decide whether we\nshould use the first address or the second for the next\nmicroinstruction.  This design, using a 2^5 bit memory\n(1,152 bits total), appears to the right.\n\n\n{file=part4/figs/microprogrammed-no-decode.eps,width=2in}\n\n\n\nThe microprogram branch control signal is a Boolean logic expression\nbased on the memory ready signal R and \nIR[11].  We can implement it with a state ID comparison and\na mux, as shown to the right.  For the branch instruction execution state, \nthe mux selects input 1, BEN.  For all other states, the mux selects \ninput 0, R.  When an FSM state has only a single next state, we set both \nIDs in the control ROM to the ID for that state, so the value of R \nhas no effect.\n\n\n\n\n\n We can simplify the implementation of the microprogram branch control\n by setting both next-state addresses to the same value when a state\n does not branch.  Then the branch control becomes a don't care for that\n state.\n\n\nWhat's missing?  Decode!  We do have one FSM state in which we need to be\nable to branch to one of sixteen possible next states, one for each\nopcode.  Let's just add another mux and choose the state IDs for starting\nto process each opcode in an easy way, as shown to the right with\nextensions highlighted in blue.  The textbook assigns the first state \nfor processing each opcode the IDs IR[15:12] preceeded by two 0s (the\ntextbook's design requires {6-bit} state IDs).  We adopt the same\nstrategy.  For example, the first FSM state for processing an ADD is 00001, \nand the first state for a TRAP is 01111.  In each case, the opcode \nencoding specifies the last four bits.\n\nNow we can pick the remaining state IDs arbitrarily and fill in the \ncontrol ROM with control \nsignals that implement each state's RTL and the two possible next states.\nTransitions from the decode state are handled by the extra mux.\n\nThe microprogrammed control unit implementation in Appendix C of \nPatt and Patel is similar to the one that we have developed here,\nbut is slightly more complex so that it can handle interrupts\nand privilege.   \n\n\n\n\n\n\n\n\n \n Question: What is the second approach to control unit design? \n Answer: programmed control} unit applies the FSM state \nQ. number to the control ROM (no IR bits, \nQ. just the state number), gets back a set of control signals, and\nQ. uses them to drive the datapath.\nQ. \nQ. \nQ. To write our microprogram, we need to calculate the control signals\nQ. for each microinstruction and put them in the control ROM, but we also\nQ. need to have a way to decide which microinstruction should execute \nQ. next.  We call the latter problem { sequencing} or microsequencing.\nQ. \nQ. Notice that most of the time there's no choice: we have only { one}\nQ. next microinstruction.  One simple approach is then to add the address\nQ. (the {5-bit} state ID) of the next microinstruction to the control ROM.\nQ. Instead of 26 bits per FSM state, we now have 31 bits per FSM state.\nQ. \nQ. Sometimes we do need to have two possible next states.  When waiting\nQ. for memory (the von Neumann memory, not the"}, {"text": "Title: Redundancy and Coding \n Text: {Redundancy and Coding}\n\nThis set of notes introduces the idea of using sparsely populated\nrepresentations to protect against accidental changes to bits.\nToday, such representations are used in almost every type of storage\nsystem, from bits on a chip to main memory to disk to archival tapes.\n\nWe begin our discussion with examples of representations in which some \nbit patterns have no meaning, then consider what happens when a bit \nchanges accidentally.  We next outline a general scheme \nthat allows a digital system to detect a single bit error.\n\nBuilding on the mechanism underlying this scheme,\nwe describe a distance metric that enables us to think more broadly about \nboth detecting and correcting such errors, and then show a general\napproach that allows correction of a single bit error.\n\nWe leave discussion of more sophisticated schemes to classes on\ncoding and information theory.\n\n\n \n Question: What is the purpose of sparsely populated representations? \n Answer: \n\nA. The purpose of using a sparsely populated representation is to\nprotect against accidental changes to bits."}, {"text": "Title: Sparse Representations \n Text: {Sparse Representations}\n\nRepresentations used by computers must avoid ambiguity: a single\nbit pattern in a representation cannot be used to represent more than\none value.  However, the converse need not be true.  A representation can\nhave several bit patterns representing the same value, and\nnot all bit patterns in a representation need be\nused to represent values.\n\nLet's consider a few example of representations with unused patterns.\nHistorically, one common class of representations of this type was \nthose used to represent individual decimal digits.  We examine three\nexamples from this class.\n\nThe first is Binary-coded Decimal (BCD), in which\ndecimal digits are encoded individually \nusing their representations in the unsigned\n(binary) representation.  Since we have 10 decimal digits, we need \n10 patterns, and four bits for each digit.  But four bits allow \n2^4=16 bit patterns.  In BCD, the patterns 1010,1011,,1111\nare unused.  It is important to note that BCD is not the same as the\nunsigned representation.  The decimal number 732, for example, requires 12 bits\nwhen encoded as BCD: 0111 0011 0010.  When written using a {12-bit}\nunsigned representation, 732 is written 001011011100.  Operations\non BCD values were implemented in early processors, including the\n8086, and are thus still available in the x86 instruction set architecture\ntoday!\n\n\nThe second example is an {Excess-3} code, in which each decimal\ndigit d is represented by the pattern corresponding to the \n{4-bit} unsigned pattern for d+3.  For example, the digit 4\nis represented as 0111, and the digit 7 is represented as 1010.\nThe {Excess-3} code has some attractive aspects when using \nsimple hardware.  For example, we can use a {4-bit} binary \nadder to add two digits c and d represented in the {Excess-3} code, \nand the carry out signal produced by the adder is the same as the carry\nout for the decimal addition, since c+d is equivalent to \n(c+3)+(d+3).\n\nThe third example of decimal digit representations is a {2-out-of-5}\ncode.  In such a code, five bits are used to encode each digit.  \n\nOnly patterns with exactly two 1s are used.  There are exactly ten \nsuch patterns, and an example representation is shown to the right (more\nthan one assignment of values to patterns has been used in real systems).\n\n\n{c|c}\n& a 2-out-of-5\ndigit& representation \n1& 00011\n2& 00101\n3& 00110\n4& 01001\n5& 01010\n6& 01100\n7& 10001\n8& 10010\n9& 10100\n0& 11000\n\n\n\n\n\n \n Question: What is the difference between a 2-out-of-5 code and an unsigned representation? \n Answer: \n\nA. A 2-out-of-5 code is a code in which five bits are used to encode each digit, and only patterns with exactly two 1s are used."}, {"text": "Title: Error Detection \n Text: {Error Detection}\n\nErrors in digital systems\ncan occur for many reasons, ranging from cosmic ray strikes to\ndefects in chip fabrication to errors in the design of the digital system.\n\nAs a simple model, we assume that an error takes the form of changes to\nsome number of bits.  In other words, a bit that should have the value 0\ninstead has the value 1, or a bit that should have the value 1 instead\nhas the value 0.  Such an error is called a { bit error}.\n\n\n\nDigital systems can be designed with or without tolerance to errors.\nWhen an error occurs, no notification nor identification of the error\nis provided.  Rather, if error tolerance is needed,\nthe system must be designed to be able to \nrecognize and identify errors automatically.\n\nOften, we assume that each of the bits may be in error independently\nof all of the others, each with some low probability.  With such\nan assumption, multiple bit errors\nare much less likely than single bit errors, and we can focus on \ndesigns that tolerate a single bit error.\n\nWhen a bit error occurs, however, we must assume that \nit can happen to any of the bits.\n\nThe use of many patterns to represent a smaller number of values, as\nis the case in a {2-out-of-5} code, enables a system\nto perform { error detection}.\n\nLet's consider what happens when a value represented using a \n{2-out-of-5} code is subjected to a single bit error.\nImagine that we have the digit 7.  In the table on the previous page,\nnotice that the digit 7 is represented with the pattern 10001.\n\nAs we mentioned, we must assume that the bit error can occur in any \nof the five bits, thus we have five possible bit patterns after the error \noccurs.  If the error occurs in the first bit, we have the pattern 00001.\nIf the error occurs in the second bit, we have the pattern 11001.\nThe complete set of possible error patterns is 00001, 11001, 10101,\n10011, and 10000.  \n\nNotice that none of the possible error patterns has exactly two 1s, and \nthus none of them is a meaningful pattern in our {2-out-of-5} code.\nIn other words, whenever a digital system represents the digit 7 and \na single bit error occurs, the system will be able to detect that an \nerror has occurred.\n\nWhat if the system needs to represent a different digit?  \nRegardless of which digit is represented, the pattern with no errors \nhas exactly two 1s,\nby the definition of our representation.  If we then flip one of the \nfive bits by subjecting it to a bit error, the resulting error pattern\nhas either one 1 (if the bit error changes a 1 to a 0) or three 1s\n(if the bit error changes a 0 to a 1).  In other words, regardless of\nwhich digit is represented, and regardless of which bit has an error,\nthe resulting error pattern never has a meaning in \nthe {2-out-of-5} code.  So this representation enables a digital\nsystem to detect any single bit error!\n\n\n \n Question: What is the meaning of the resulting pattern? \n Answer: \n\nA. If the system needs to represent a different digit, the pattern with no errors will still have exactly two 1s. If a bit error occurs, the resulting error pattern will have either one 1 or three 1s, which will not have a meaning in the 2-out-of-5 code. So this representation will still enable the digital system to detect any single bit error."}, {"text": "Title: Parity \n Text: \n\nThe ability to detect any single bit error is certainly useful.\nHowever, so far we have only shown how to protect ourselves when we\nwant to represent decimal digits.  Do we need to develop a separate\nerror-tolerant representation for every type of information that\nwe might want to represent?  Or can we instead come up with a more\ngeneral approach?\n\nThe answer to the second question is yes: we can, in fact, systematically\ntransform any representation into a representation that allows detection of a\nsingle bit error.  The key to this transformation is the idea of\n{ parity}.\n\n\nConsider an arbitrary representation for some type of information.\nBy way of example, we use the {3-bit} unsigned representation.\nFor each pattern used in the representation, we can count the number\nof 1s.  The resulting count is either odd or even.  By adding an extra\nbit---called a { parity bit}---to the representation, and \nselecting the parity bit's value \nappropriately for each bit pattern, we can ensure that the count of 1s\nis odd (called { odd parity}) or even (called { even parity})\nfor all values represented.  The idea is\n\nillustrated in the table\nto the right for the {3-bit} unsigned representation.  The parity\nbits are shown in bold.\n\n\n{c|c|c|c|c}\nvalue      &   3-bit & number& with odd& with even \nrepresented& unsigned& of 1s & parity  & parity  \n0& 000& 0& 000{ 1}& 000{ 0}\n1& 001& 1& 001{ 0}& 001{ 1}\n2& 010& 1& 010{ 0}& 010{ 1}\n3& 011& 2& 011{ 1}& 011{ 0}\n4& 100& 1& 100{ 0}& 100{ 1}\n5& 101& 2& 101{ 1}& 101{ 0}\n6& 110& 2& 110{ 1}& 110{ 0}\n7& 111& 3& 111{ 0}& 111{ 1}\n\n\n\nEither approach to selecting the parity bits ensures that any single\nbit error can be detected.  For example, if we choose to use odd\nparity, a single\nbit error changes either a 0 into a 1 or a 1 into a 0.\nThe number of 1s in the resulting error pattern thus\ndiffers by exactly one from the original pattern, and the parity of\nthe error pattern is even.  But all valid patterns have odd parity,\nso any single bit error can be detected by simply counting the number\nof 1s.\n\n\n \n Question: What is the key to systematically transform any representation into a representation that allows detection of a single bit error? \n Answer: \n\nA. A parity bit is used to detect single bit errors in a representation."}, {"text": "Title: Hamming Distance \n Text: {Hamming Distance}\n\nNext, let's think about how we might use representations---we might\nalso think of them as { codes}---to protect a system against \nmultiple bit errors.  As we have seen with parity, one strategy that\nwe can use to provide such error tolerance is the use of representations\nin which only some of the patterns actually represent values.\nLet's call such patterns { code words}.  In other words, the\ncode words in a representation are those patterns that correspond to \nreal values of information.  Other patterns in the representation have no \nmeaning.\n\nAs a tool to help us understand error tolerance, let's define\na measure of the distance between code words in a representation.\nGiven two code words X and Y, we can calculate the number N_ of \nbits that must change to transform X into Y.  Such a calculation\nmerely requires that we compare the patterns bit by bit and count\nthe number of places in which they differ.  Notice that this\nrelationship is symmetric: the same number of changes are required\nto transform Y into X, so N_=N_.  \n\nWe refer to this number N_ as the { Hamming distance} between\ncode word X and code word Y.  \n\nThe metric is named after Richard Hamming, a computing pioneer and an \nalumnus of the UIUC Math department.\n\nThe Hamming distance between two code words tells us how many bit errors\nare necessary in order for a digital system to mistake one code word\nfor the other.  Given a representation, we can calculate the minimum\nHamming distance between any pair of code words used by the \nrepresentation.  The result is called the { Hamming distance of the \nrepresentation}, and represents the minimum of bit errors that must\noccur before a system might fail to detect errors in a stored value.\n\nThe Hamming distance for nearly all of the representations that we \nintroduced in earlier sections is 1.  Since more than half of the\npatterns (and often all of the patterns!) correspond to meaningful\nvalues, some pairs of code words must differ in only one bit, and\nthese representations cannot tolerate any errors.  For example,\nthe decimal value 42 is stored as 101010 using a {6-bit} unsigned\nrepresentation, but any bit error in that pattern produces another \nvalid pattern corresponding to one of the following \ndecimal numbers: 10, 58, 34, 46, 40, 43.  Note that the Hamming distance\nbetween any two patterns is not necessarily 1.  Rather, the Hamming \ndistance of the unsigned representation, which corresponds to the \nminimum between any pair of valid patterns, is 1.\n\nIn contrast, the Hamming distance of the {2-out-of-5} code that\nwe discussed earlier is 2.  Similarly, the Hamming distance of any\nrepresentation extended with a parity bit is at least 2.\n\nNow let's think about the problem slightly differently.\n\nGiven a particular representation, \n\nhow many bit errors can we detect in values using that representation?\n\n{ A representation with Hamming distance d can detect up to d-1 bit errors.}\n\nTo understand this claim, start by selecting a code word from the\nrepresentation and changing up to d-1 of the bits.  No matter\nhow one chooses to change the bits, these changes cannot result in\nanother code word, since we know that any other code word has to \nrequire at least d changes from our original code word, by the\ndefinition of the representation's Hamming distance.\n\nA digital system using the representation can thus detect up to d-1\nerrors.  However, if d or more errors occur, the system might sometimes\nfail to detect any error in the stored value.\n\n\n \n Question: What is the Hamming distance of the unsigned representation? \n Answer: \n\nA. A representation with Hamming distance 2 can detect 1 bit error."}, {"text": "Title: Error Correction \n Text: {Error Correction}\n\nDetection of errors is important, but may sometimes not be enough.\nWhat can a digital system do when it detects an error?  In some\ncases, the system may be able to find the original value elsewhere, or\nmay be able to re-compute the value from other values.  In other \ncases, the value is simply lost, and the digital system may need\nto reboot or even shut down until a human can attend to it.\n\nMany real systems cannot afford such a luxury.  Life-critical systems\nsuch as medical equipment and airplanes should not turn themselves off\nand wait for a human's attention.  Space vehicles face a similar dilemma,\nsince no human may be able to reach them.\n\nCan we use a strategy similar to the one that we have developed for error\ndetection in order to try to perform { error correction}, recovering\nthe original value?  Yes, but the overhead---the\nextra bits that we need to provide such functionality---is higher.\n\n\n\nLet's start by thinking about a code with Hamming distance 2, such\nas {4-bit} 2's complement with odd parity.  We know that such a \ncode can detect one bit error.  Can it correct such a bit error, too?\n\nImagine that a system has stored the decimal value 6 using the \npattern 0110{ 1}, where the last bit is the odd parity bit.\nA bit error occurs, changing the stored pattern to 0111{ 1}, which is\nnot a valid pattern, since it has an even number of 1s.  But can the\nsystem know that the original value stored was 6?  No, it cannot.\nThe original value may also have been 7, in which case the original\npattern was 0111{ 0}, and the bit error occurred in the final\nbit.  The original value may also have been -1, 3, or 5.  The system\nhas no way of resolving this ambiguity.\n\nThe same problem arises if a digital system uses a code with\nHamming distance d to detect up to d-1 errors.\n\n\nError correction is possible, however, if we assume that fewer bit\nerrors occur (or if we instead use a representation with a larger Hamming\ndistance).\n\nAs a simple example, let's create a representation for the numbers 0\nthrough 3 by making three copies of the {2-bit} unsigned \nrepresentation, as shown to the right.  The Hamming distance of the\nresulting code is 3, so any two bit errors can be detected.  However,\nthis code also enables us to correct a single bit error.  Intuitively, \nthink of the three copies as voting on the right answer.\n\n\n{c|c}\nvalue      & three-copy\nrepresented& code \n0& 000000\n1& 010101\n2& 101010\n3& 111111\n\n\n\nSince a\nsingle bit error can only corrupt one copy, a majority vote always\ngives the right answer!\nTripling the number of bits needed in a representation is not a good\ngeneral strategy, however. \nNotice also that ``correcting'' a pattern with two bit errors can produce\nthe wrong result.\n\nLet's think about the problem in terms\nof Hamming distance.  Assume that we use a code with Hamming distance d\nand imagine that up to k bit errors affect a stored value.\nThe resulting pattern then falls within a neighborhood of distance k\nfrom the original code word.  This neighborhood contains all bit \npatterns within Hamming distance k of the original pattern.\nWe can define such a neighborhood around each code word.  Now, \nsince d bit errors are needed to transform a code word into\nany other code word, these neighborhoods are disjoint so long\nas 2k{d-1}.  In other words, if the inequality holds,\nany bit pattern in the representation can be in at most one code word's \nneighborhood.  The digital system can then correct the errors by \nselecting the unique value identified by the associated neighborhood.\nNote that patterns encountered as a result of up to k bit errors\nalways fall within the original code word's neighborhood; the inequality\nensures that the neighborhood identified in this way is unique.\nWe can manipulate the inequality to express the number of errors k that\ncan be corrected in terms of the Hamming distance d of the code.\n{ A code with Hamming distance d allows up to {d-1}\nerrors to be corrected}, where  represents the\ninteger floor function on x, or rounding x down to the nearest \ninteger.\n\n\n \n Question: What is error correction? \n Answer: \n\nError correction is the process of detecting and correcting errors in digital data."}, {"text": "Title: Hamming Codes \n Text: {Hamming Codes}\n\nHamming also developed a general and efficient approach for \nextending an arbitrary representation to allow correction of\na single bit error.  The approach yields codes with Hamming distance 3.\n\nTo understand how a { Hamming code} works, think of the bits in the\nrepresentation as being numbered starting from 1.  For example, if\nwe have seven bits in the code, we might write a bit pattern X \nas x_7x_6x_5x_4x_3x_2x_1.\n\nThe bits with indices that are powers of two are parity check bits.\nThese include x_1, x_2, x_4, x_8, and so forth.  The remaining\nbits can be used to hold data.  For example, we could use a {7-bit}\nHamming code and map the bits from a {4-bit} unsigned representation \ninto bits x_7, x_6, x_5, and x_3.  Notice that Hamming codes are\nnot so useful for small numbers of bits, but require only logarithmic\noverhead for large numbers of bits.  That is, in an {N-bit}\nHamming code, only _2(N+1) bits are used for parity\nchecks.\n\nHow are the parity checks defined?  Each parity bit is used to\nprovide even parity for those bits with indices for which the index,\nwhen written in binary, includes a 1 in the single position in which\nthe parity bit's index contains a 1.  The x_1 bit, for example,\nprovides even parity on all bits with odd indices.  The x_2 bit\nprovides even parity on x_2, x_3, x_6, x_7, x_, and so\nforth.\n\nIn a {7-bit} Hamming code, for example, \nx_1 is chosen so that\nit has even parity together with x_3, x_5, and x_7.\n\nSimilarly, x_2 is chosen so that\nit has even parity together with x_3, x_6, and x_7.\n\nFinally, x_4 is chosen so that\nit has even parity together with x_5, x_6, and x_7.\n\n\nThe table to the right shows the result of embedding \na {4-bit} unsigned representation into a {7-bit}\nHamming code.\n\nA Hamming code provides a convenient way to identify\nwhich bit should be corrected when a single bit error occurs.  \nNotice that each bit is protected by a unique subset of the parity bits\ncorresponding to the binary form of the bit's index.  Bit x_6, for\nexample, is protected by bits x_4 and x_2, because the number 6\nis written 110 in binary.\nIf a bit is affected by an error, the parity bits\nthat register the error are those corresponding to 1s in the binary\nnumber of the index.  So if we calculate check bits as 1 to represent\nan error (odd parity) and 0 to represent no error (even parity),\nthen concatenate those bits into a binary number, we obtain the\nbinary value of the index of the single bit affected by an error (or \nthe number 0\nif no error has occurred).\n\n\n{c|c|c|c|c|c}\n           &  4-bit         &     &     &     &7-bit\nvalue      & unsigned       &     &     &     &Hamming\nrepresented&(x_7x_6x_5x_3)&x_4&x_2&x_1&code \n 0& 0000& 0& 0& 0& 0000000\n 1& 0001& 0& 1& 1& 0000111\n 2& 0010& 1& 0& 1& 0011001\n 3& 0011& 1& 1& 0& 0011110\n 4& 0100& 1& 1& 0& 0101010\n 5& 0101& 1& 0& 1& 0101101\n 6& 0110& 0& 1& 1& 0110011\n 7& 0111& 0& 0& 0& 0110100\n 8& 1000& 1& 1& 1& 1001011\n 9& 1001& 1& 0& 0& 1001100\n10& 1010& 0& 1& 0& 1010010\n11& 1011& 0& 0& 1& 1010101\n12& 1100& 0& 0& 1& 1100001\n13& 1101& 0& 1& 0& 1100110\n14& 1110& 1& 0& 0& 1111000\n15& 1111& 1& 1& 1& 1111111\n\n\n\nLet's do a couple of examples based on the pattern for the decimal\nnumber 9, 1001100.  First, assume that no error occurs.  We calculate\ncheck bit c_4 by checking whether x_4, x_5, x_6, and x_7 together\nhave even parity.  Since no error occurred, they do, so c_4=0.\nSimilarly, for c_2 we consider x_2, x_3, x_6, and x_7.\nThese also have even parity, so c_2=0.  Finally, for c_1, we \nconsider x_1, x_3, x_5, and x_7.  As with the others, these\ntogether have even parity, so c_1=0.  Writing c_4c_2c_1, we obtain 000,\nand conclude that no error has occurred.\n\nNext assume that bit 3 has an error, giving us the pattern 1001000.\nIn this case, we have again that c_4=0, but the bits corresponding to\nboth c_2 and c_1 have odd parity, so c2=1 and c_1=1.  Now when\nwe write the check bits c_4c_2c_1, we obtain 011,\nand we are able to recognize that bit 3 has been changed.\n\nA Hamming code can only correct one bit error, however.  If two bit\nerrors occur, correction will produce the wrong answer.  Let's imagine\nthat both bits 3 and 5 have been flipped in our example pattern for\nthe decimal number 9, producing the pattern 1011000.  Calculating the\ncheck bits as before and writing them as c_4c_2c_1, we obtain 110,\nwhich leads us to incorrectly conclude that bit 6 has been flipped.\nAs a result, we ``correct'' the pattern to 1111000, which represents\nthe decimal number 14.\n\n\n \n Question: What is the Hamming distance of a Hamming code? \n Answer: \n\nA. The Hamming distance of a Hamming code is 3."}, {"text": "Title: SEC-DED Codes \n Text: {SEC-DED Codes}\n\nWe now consider one final extension of Hamming codes to enable a system\nto perform single error correction while also detecting any two bit errors.\nSuch codes are known as { Single Error Correction, Double Error \nDetection (SEC-DED)} codes.  Creating such a code from a Hamming code is\ntrivial: add a parity bit covering the entire Hamming code.  The extra\nparity bit increases the Hamming distance to 4.  A Hamming distance of 4\nstill allows only single bit error correction, but avoids the problem\nof Hamming distance 3 codes when two bit errors occur, since patterns\nat Hamming distance 2 from a valid code word cannot be within distance 1\nof another code word, and thus cannot be ``corrected'' to the wrong\nresult.\n\nIn fact, one can add a parity bit to any representation with an odd\nHamming distance to create a new representation with Hamming distance\none greater than the original representation.  To proof this convenient\nfact, begin with a representation with Hamming distance d, where d\nis odd.  If we choose two code words from the representation, and their \nHamming distance is already greater than d, their distance in the \nnew representation will also be greater than d.  Adding a parity\nbit cannot decrease the distance.  On the other hand, if the two code\nwords are exactly distance d apart, they must have opposite parity,\nsince they differ by an odd number of bits.  Thus the new parity bit\nwill be a 0 for one of the code words and a 1 for the other, increasing\nthe Hamming distance to d+1 in the new representation.  Since all\npairs of code words have Hamming distance of at least d+1, the\nnew representation also has Hamming distance d+1.\n\n\n\n\n\n \n Question: What is the Hamming distance of a SEC-DED code? \n Answer: \n\nA. The Hamming distance of a SEC-DED code is 4."}, {"text": "Title: Summary of Part 4 of the Course \n Text: {Summary of Part 4 of the Course}\n\nWith the exception of control unit design strategies and redundancy \nand coding, most of the material in this part of the course is drawn from\nPatt and Patel Chapters 4 through 7.  You may also want to read Patt and \nPatel's Appendix C for details of their control unit design.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nWe'll start with the easy stuff.  You should recognize all of these terms\nand be able to explain what they mean.  \n For the specific circuits, you \n should be able to draw them and explain how they work.\n(You may skip the *'d terms in Fall 2012.)\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{von Neumann elements}\n{-}{{}{}\n{}{}{}\n{program counter (PC)}\n{instruction register (IR)}\n{memory address register (MAR)}\n{memory data register (MDR)}\n{processor datapath}\n\n{control signal}\n{instruction processing}\n\n\n{Instruction Set Architecture (ISA)}\n{-}{{}{}\n{}{}{}\n{instruction encoding}\n{field (in an encoded instruction)}\n{operation code (opcode)}\n\n\n{assemblers and assembly code}\n{-}{{}{}\n{}{}{}\n{opcode mnemonic (such as ADD, JMP)}\n{two-pass process}\n\n{symbol table}\n{pseudo-op / directive}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{systematic decomposition}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n no documentation, and advanced topics ... no testing\n\n {logic design optimization}\n {-}{{}{}\n {}{}{}\n {bit-sliced (including multiple bits per slice)}\n \n {pipelined logic}\n {tree-based}\n \n\n{control unit design strategies}\n{-}{{}{}\n{}{}{}\n{control word / microinstruction}\n{sequencing / microsequencing}\n{hardwired control}\n{-}{{}{}\n{}{}{}\n{single-cycle}\n{multi-cycle}\n\n{microprogrammed control}\n {pipelining (of instruction processing)}\n\n\n{error detection and correction\n{--}{{}{}\n{}{}{}\n code/sparse representation\n code word\n bit error\n odd/even parity bit\n Hamming distance between code words\n Hamming distance of a code\n Hamming code\n SEC-DED\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n FIXME ... should write something about critical path, but expecting\n them to do much with it isn't reasonable\n\n {Implement arbitrary Boolean logic, and be able to reason about\n alternative designs in terms of their critical path delays and number\n of gates required to implement.}\n\n{Map RTL (register transfer language) operations into control words\nfor a given processor datapath.}\n\n{Systematically decompose a (simple enough) problem to the level \nof {LC-3} instructions.}\n\n{Encode {LC-3} instructions into machine code.}\n\n{Read and understand programs written in {LC-3} assembly/machine code.}\n\n{Test and debug a small program in {LC-3} assembly/machine code.}\n\n{Be able to calculate the Hamming distance of a code/representation.}\n\n{Know the relationships between Hamming distance and the abilities\nto detect and to correct bit errors.}\n\n\n\nWe expect that you will understand the concepts and ideas to the extent\nthat you can do the following:\n\n{}{{}{}\n{}{}{}\n\n{Explain the role of different types of instructions in allowing\na programmer to express a computation.}\n\n FIXME: ISA design is beyond the scope of this course; just include\n for interest, if at all\n\n {Explain the tradeoffs in different addressing modes so as to motivate\n inclusion of multiple addressing modes in an ISA.}\n\n{Explain the importance of the three types of subdivisions in systematic\ndecomposition (sequential, conditional, and iterative).}\n\n{Explain the process of transforming assembly code into machine code\n(that is, explain how an assembler works, including describing the use of\nthe symbol table).}\n\n{Be able to use parity for error detection, and Hamming codes for\nerror correction.}\n\n\n\nAt the highest level, \nwe hope that, while you do not have direct substantial experience in \nthis regard from our class (and should not expect to be tested on these\nskills), that you will nonetheless be able to begin\nto do the following when designing combinational logic:\n\n{}{{}{}\n{}{}{}\n\n{Design and compare implementations using gates, decoders, muxes, and/or memories \nas appropriate, and including reasoning about the relevant design tradeoffs \nin terms of area and delay.}\n\n{Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based \ndesign, again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Design and compare implementations of processor control units\nusing both hardwired and microprogrammed strategies,\nand again including reasoning about the relevant design tradeoffs in\nterms of area and delay.}\n\n{Understand basic tradeoffs in the sparsity of code words with error \ndetection and correction capabilities.}\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n \n Question: What is the purpose of the assembler course? \n Answer: \n\nAn assembler is a program that takes assembly code, which is code written in a human-readable form, and translates it into machine code, which is code that can be run by a computer."}, {"text": "Title: Example: Bit-Sliced Addition \n Text: {Example: Bit-Sliced Addition}\n\nIn this set of notes, we illustrate basic logic design using integer\naddition as an example.  By recognizing and mimicking the structured \napproach used by humans to perform addition, we introduce an important \nabstraction for logic design.  We follow this approach to design an\nadder known as a ripple-carry adder, then discuss some of the \nimplications of the approach and highlight how the same approach can \nbe used in software.  In the next set of notes, we use the same\ntechnique to design a comparator for two integers.\n\n \n Question: What is the traditional adder's approach to logic design? \n Answer: \n\nA. A ripple-carry adder is a type of adder that uses a ripple-carry technique to propagate the carry bit through each stage of the addition process. A traditional adder, on the other hand, does not use this technique and instead propagates the carry bit through each stage in a different way."}, {"text": "Title: One Bit at a Time \n Text: {One Bit at a Time}\n\nMany of the operations that we want to perform on groups of bits can \nbe broken down into repeated operations on individual bits.\n\nWhen we add two binary numbers, for example, we first add the least\nsignificant bits, then move to the second least significant, and so on.\nAs we go, we may need to carry from lower bits into higher bits.\n\nWhen we compare two (unsigned) binary numbers with the same number of bits,\nwe usually start with the most significant bits and move downward in\nsignificance until we find a difference or reach the end of the two \nnumbers.  In the latter case, the two numbers are equal.\n\nWhen we build combinational logic to implement this kind of calculation,\nour approach as humans can be leveraged as an abstraction technique.\nRather than building and optimizing a different Boolean function for \nan 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size\nthat we might want, we can instead design a circuit that adds a single\nbit and passes any necessary information into another copy of itself.\nBy using copies of this { bit-sliced} adder circuit, we can\nmimic our approach as humans and build adders of any size, just as\nwe expect that a human could add two binary numbers of any size.\n\nThe resulting designs are, of course, slightly less efficient than\ndesigns that are optimized for their specific purpose (such as adding\ntwo 17-bit numbers), but the simplicity of the approach makes the\ntradeoff an interesting one.\n\n\n \n Question: How can we mimic our approach as humans? \n Answer: "}, {"text": "Title: Abstracting the Human Process \n Text: {Abstracting the Human Process}\n\n\nThink about how we as humans add two {N-bit}\nnumbers, A and B.\n\nAn illustration appears to the right, using N=8.\n\nFor now, let's assume that our numbers are stored in an unsigned \nrepresentation.\n\nAs you know, addition for 2's complement is identical except for the\ncalculation of overflow.\n\nWe start adding from the least significant bit and move to the left.\nSince adding two 1s can overflow a single bit, we carry a 1 when\nnecessary into the next column.  Thus, in general, we are actually\nadding three input bits.  The carry from the previous column is usually\nnot written explicitly by humans, but in a digital system\nwe need to write a 0 instead of leaving the value blank.\n\nFocus now on the addition of a single column.  Except for the\nfirst and last bits, which we might choose to handle slightly \ndifferently, the addition process is identical \nfor any column.  We add a carry in bit (possibly 0) with one\nbit from each of our numbers to produce a sum bit and a carry\nout bit for the next column.  Column addition is the task\nthat our bit slice logic must perform.\n\nThe diagram to the right shows an abstract model of our \nadder bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming\nfrom the top or left \nand outputs going to the bottom or right.  Outside\nof the bit slice logic, we index the carry bits using the\n\n\n\n{{file=part2/figs/add-abs.eps,width=1.90in}}\n\n\n\n\nbit number.  The bit slice has C^M provided as an input and \nproduces C^{M+1} as an output.\n\nInternally, we use C_ to denote the carry input,\nand C_ to denote the carry output.\n\nSimilarly, the\nbits A_M and B_M from the numbers A and B are\nrepresented internally as A and B, and the bit S_M produced for\nthe sum S is represented internally as S.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.\n\nThe abstract device for adding three inputs bits and producing\ntwo output bits is called a { full adder}.  You may \nalso encounter the term { half adder}, which adds only two\ninput bits.  To form an {N-bit} adder, we integrate N\ncopies of the full adder---the bit slice that we design next---as \nshown below.  The result is called a { ripple carry adder}\nbecause the carry information moves from the low bits to the high\nbits slowly, like a ripple on the surface of a pond.\n\n{{file=part2/figs/add-integrated.eps,width=5.5in}}\n\n\n \n Question: What is the name of the abstract device for adding three inputs and producing two output bits? \n Answer: \n\nA. A full adder works by adding three input bits and producing two output bits."}, {"text": "Title: Designing the Logic \n Text: {Designing the Logic}\n\nNow we are ready to design our adder bit slice.\nLet's start by writing a truth table for C_ and S,\nas shown on the left below.\n\nTo the right of the truth tables are {K-maps} for each output,\nand equations for each output are then shown to the right of the \n{K-maps}.\n\nWe suggest that you work through identification of the prime implicants \nin the {K-maps} and check your work with the equations.\n\n{\n[t]\n{ccc|cc}\nA& B& C_& C_& S \n0& 0& 0& 0& 0\n0& 1& 0& 0& 1\n0& 0& 1& 0& 1\n0& 1& 1& 1& 0\n1& 0& 0& 0& 1\n1& 0& 1& 1& 0\n1& 1& 0& 1& 0\n1& 1& 1& 1& 1\n\n\n\n{file=part2/figs/add-cin.eps,width=1in}\n{file=part2/figs/add-s.eps,width=1in}\n\n\n{eqnarray*}\nC_&=&A B+A C_+B C_   \nS&=&A B C_+A  {C_+\n&& B {C_+  C_\n&=&A{C_\n{eqnarray*}\n\n}\n\nThe equation for C_ implements a { majority function}\non three bits.  In particular, a carry is produced whenever at least two \nout of the three input bits (a majority) are 1s.  Why do we mention\nthis name?  Although we know that we can build any logic function from\nNAND gates, common functions such as those used to add numbers may\nbenefit from optimization.  Imagine that in some technology, creating\na majority function directly may produce a better result than implementing\nsuch a function from logic gates.  In such a case, we want the \nperson designing the circuit to know that can make use of such an \nimprovement.\n\nWe rewrote the equation for S to make use of the XOR operation for \na similar reason: the implementation of XOR gates from transistors\nmay be slightly better than the implementation of XOR based on NAND gates.\nIf a circuit designer provides an optimized variant of XOR, we want\nour design to make use of the optimized version.\n\n{{file=part2/figs/adder-bit.eps,width=2.55in}\n{file=part2/figs/adder-bit-nn.eps,width=2.55in}}\n\nThe gate diagrams above implement a single bit slice for an adder.\nThe version on\nthe left uses AND and OR gates (and an XOR for the sum), while the\nversion on the right uses NAND gates, leaving the XOR as an XOR.\n\nLet's discuss the design in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\nFor each bit, we need three {2-input} NAND gates, one {3-input}\nNAND gate, and a {3-input} XOR gate (a big gate; around 30 transistors).\n\nFor speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.  \n\nWe can thus estimate our design's\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer.\nHere we have two gate delays from any of the inputs\nto the C_ output.  The XOR gate may be a little slower, but\nnone of its inputs come from other gates anyway.\n\nWhen we connect multiple copies of our bit slice logic together to \nform an adder, the A and B inputs\nto the outputs is not as important as the delay from C_\nto the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis---this propagation delay\ngives rise to the name ``ripple carry.''\nLooking again at the diagram, \nnotice that we have two gate delays from C_\nto C_.\nThe total delay for an {N-bit} comparator based on this\nimplementation is thus two gate delays per bit, \nfor a total of 2N gate delays.\n\n\n \n Question: What are the inputs to a bit slice? \n Answer: \n\nA. The inputs to a bit slice are A and B, and the outputs are S and C_."}, {"text": "Title: Adders and Word Size\\vspace12pt \n Text: {Adders and Word Size}\n\n\nNow that we know how to build an {N-bit} adder, we can add\nsome detail to the diagram that we drew when we \nintroduced 2's complement back in Notes Set 1.2, as shown to the right.\n\nThe adder is important enough to computer systems to merit its own\nsymbol in logic diagrams, which is shown to the right with the inputs\nand outputs from our design added as labels.  The text in the middle\nmarking the symbol as an adder is only included for clarity: { any time \nyou see a symbol of the shape shown to the right, it is an adder} (or \nsometimes a device that can add and do other operations).  The width \nof the operand input and output lines then tells you the size of the \nadder.\n\n\n{file=part2/figs/adder-trad.eps,width=1.3in}\n\n\nYou may already know that most computers have a { word size}\nspecified as part of the Instruction Set Architecture.  The word\nsize specifies the number of bits in each operand when the computer\nadds two numbers, and is often used widely within the \nmicroarchitecture as well (for example, to decide the number of \nwires to use when moving bits around).  Most desktop and laptop machines\nnow have a word size of 64 bits, but many phone processors (and\ndesktops/laptops a few years ago) use a {32-bit} word size.\nEmbedded microcontrollers may use a {16-bit} or even \nan {8-bit} word size.\n\n\nHaving seen how we can build an {N-bit} adder from simple\nchunks of logic operating on each pair of bits, you should not have\nmuch difficulty in understanding the diagram to the right.\n\nIf we start with a design for an {N-bit} adder---even if that\ndesign is not built from bit slices, but is instead optimized for\nthat particular size---we can create a {2N-bit} adder by \nsimply connecting two copies of the {N-bit} adder.  We give\nthe adder for the less significant bits (the one on the right\nin the figure) an initial carry of 0,\nand pass the carry produced by the adder for the less significant\nbits into the carry input of the adder for the more significant\nbits.  We calculate overflow based on the results of the adder\nfor more significant bits (the one on the left in the figure), \nusing the method appropriate to the \ntype of operands we are adding (either unsigned or 2's complement).\n\n\n{file=part2/figs/adder-x2.eps,width=2.15in}\n\n\nYou should also realize that this connection need not be physical.\nIn other words, if a computer has an {N-bit} adder, it can\nhandle operands with 2N bits (or 3N, or 10N, or 42N) by\nusing the {N-bit} adder repeatedly, starting with the\nleast significant bits and working upward until all of the bits\nhave been added.  The computer must of course arrange to have the\noperands routed to the adder a few bits at a time, and must\nensure that the carry produced by each addition is then delivered to\nthe carry input (of the same adder!) for the next addition.\nIn the coming months, you will learn how to design hardware that\nallows you to manage bits in this way, so that by the end of our\nclass, you will be able to design a simple computer on your own.\n\n\n\n\n\n \n Question: What is the symbol of an adder? \n Answer: \n\nA.\n\nAn adder is a device that can add two numbers.  The adder is important enough to computer systems to merit its own symbol in logic diagrams, which is shown to the right with the inputs and outputs from our design added as labels.  The width of the operand input and output lines then tells you the size of the adder."}, {"text": "Title: Summary of Part 2 of the Course \n Text: {Summary of Part 2 of the Course}\n\nThese notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.\n\nThe difficulty of learning depends on the type of task involved.\nRemembering new terminology is relatively easy, while applying\nthe ideas underlying design decisions shown by example to new problems \nposed as human tasks is relatively hard.\n\nIn this short summary, we give you lists at several levels of difficulty \nof what we expect you to be able to do as a result of the last few weeks \nof studying (reading, listening, doing homework, discussing your \nunderstanding with your classmates, and so forth).\n\nWe'll start with the skills, and leave the easy stuff for the next page.\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Design a CMOS gate for a simple Boolean function from n-type \nand p-type transistors.}\n\n{Apply DeMorgan's laws repeatedly to simplify the form of\nthe complement of a Boolean expression.}\n\n{Use a K-map to find a reasonable expression for a Boolean function (for\nexample, in POS or SOP form with the minimal number of terms).}\n\n{More generally, translate Boolean logic functions among \nconcise algebraic, truth table, K-map, and canonical (minterm/maxterm) forms.}\n\n\n\nWhen designing combinational logic, we expect you to be able to apply\nthe following design strategies:\n\n{}{{}{}\n{}{}{}\n\n{Make use of human algorithms \n(for example, multiplication from addition).}\n\n{Determine whether a bit-sliced approach is applicable, and, if so,\nmake use of one.}\n\n{Break truth tables into parts so as to solve each part of a function \nseparately.}\n\n{Make use of known abstractions (adders, comparators, muxes, or other\nabstractions available to you) to simplify the problem.}\n\n\n\nAnd, at the highest level, we expect that you will be able to do the following:\n\n{}{{}{}\n{}{}{}\n\n{Understand and be able to reason at a high-level about circuit design\ntradeoffs between area/cost and performance (and to know that power is also \nimportant, but we haven't given you any quantification methods).}\n\n{Understand the tradeoffs typically made to develop bit-sliced \ndesigns---typically, bit-sliced designs are simpler but bigger and \nslower---and how one can develop variants between the extremes of\nthe bit-sliced approach and optimization of functions specific\nto an {N-bit} design.}\n\n{Understand the pitfalls of marking a function's value as ``don't care'' \nfor some input combinations, and recognize that implementations do not \nproduce ``don't care.''}\n\n{Understand the tradeoffs involved in selecting a representation for\ncommunicating information between elements in a design, such as the bit \nslices in a bit-sliced design.}\n\n{Explain the operation of a latch or a flip-flop, particularly in \nterms of the bistable states used to hold a bit.}\n\n{Understand and be able to articulate the value of the clocked \nsynchronous design abstraction.}\n\n\n\n\n\n\n\nYou should recognize all of these terms\nand be able to explain what they mean.  For the specific circuits, you \nshould be able to draw them and explain how they work.\n\nActually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  \n\n[t]\n{}{{}{}\n{}{}{}\n\n{Boolean functions and logic gates}\n{-}{{}{}\n{}{}{}\n\n\n\n\n\n\n\n{majority function}\n\n\n{specific logic circuits}\n{-}{{}{}\n{}{}{}\n{full adder}\n{half adder}\n{ripple carry adder}\n N-to-M multiplexer (mux)\n N-to-2N decoder\n{{- latch}}\n{{R-S latch}}\n{gated D latch}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation of a positive edge-triggered D flip-flop}\n{dual-latch implementation of a positive edge-triggered D flip-flop}\n{(bidirectional) shift register}\n{register supporting parallel load}\n\n\n{design metrics}\n{-}{{}{}\n{}{}{}\n\n\n\n\n{power, area/cost, performance}\n{computer-aided design (CAD) tools}\n{gate delay}\n\n\n{general math concepts}\n{-}{{}{}\n{}{}{}\n{canonical form}\n{domain of a function}\n{{N-dimensional} hypercube}\n\n\n{tools for solving logic problems}\n{-}{{}{}\n{}{}{}\n{truth table}\n{Karnaugh map (K-map)}\n\n{prime implicant}\n{bit-slicing}\n{timing diagram}\n\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{device technology}\n{-}{{}{}\n{}{}{}\n{complementary metal-oxide semiconductor (CMOS)}\n{field effect transistor (FET)}\n{transistor gate, source, drain}\n\n\n{Boolean logic terms}\n{-}{{}{}\n{}{}{}\n\n{algebraic properties}\n{dual form, principle of duality}\n{sum, product}\n{minterm, maxterm}\n{sum-of-products (SOP)}\n{product-of-sums (POS)}\n{canonical sum/SOP form}\n{canonical product/POS form}\n{logical equivalence}\n\n\n{digital systems terms}\n{-}{{}{}\n{}{}{}\n{word size}\n{{N-bit} Gray code}\n{combinational/combinatorial logic}\n{-}{{}{}\n{}{}{}\n{two-level logic}\n{``don't care'' outputs (x's)}\n\n{sequential logic}\n{-}{{}{}\n{}{}{}\n\n{active low input}\n{set a bit (to 1)}\n{reset a bit (to 0)}\n\n SSL altered term 3 Dec 21 \n\n{master-slave implementation}\n{dual-latch implementation}\n{positive edge-triggered}\n\n{clock signal}\n{-}{{}{}\n{}{}{}\n{square wave}\n{rising/positive clock edge}\n{falling/negative clock edge}\n{clock gating}\n\n{clocked synchronous sequential circuit}\n{parallel/serial load of register}\n FIXME?  too informal to ask them to remember it\n {glue logic}\n{logical/arithmetic/cyclic shift}\n\n\n\n\n\n\n\n\n\n{   }  blank 3rd page\n\n\n\n\n\n\n\n \n Question: What is the basic concept of a flip-flop? \n Answer: \n\nA. A latch is a circuit that is used to store a bit of information, while a flip-flop is a type of latch that is used to store a bit of information in a way that allows it to be changed."}, {"text": "Title: Registers \n Text: \n\nThis set of notes introduces registers, an abstraction used for \nstorage of groups of bits in digital systems.  We introduce some\nterminology used to describe aspects of register design and\nillustrate the idea of a shift register.  The registers shown here\nare important abstractions for digital system design.\n\n { In the Fall 2012 offering of our course, we will cover this\n material on the third midterm.}\n\n\n \n Question: What is the purpose of the registers? \n Answer: "}, {"text": "Title: Registers \n Text: \n\n\nA { register} is a storage element composed from one or more\nflip-flops operating on a common clock.\n\nIn addition to the flip-flops,\nmost registers include logic to control the bits stored by the register.\n\nFor example, D flip-flops \ncopy their inputs at the rising edge of each clock cycle,\ndiscarding whatever bits they have stored before the rising edge\n(in the previous clock cycle).\n\nTo enable a flip-flop to retain its value, we might try to hide the \nrising edge of the clock from the flip-flop, as shown to the right.\n\nThe LOAD input controls the clock signals through a method known as\n{ clock gating}.\n\n\n{{file=part2/figs/lec16-1a.eps,width=2.45in}}\n{file=part2/figs/lec16-1b.eps,width=3.4in}\n\n\nWhen LOAD is high, the circuit reduces to a\nregular D flip-flop.  When LOAD is low, the flip-flop clock \ninput, c, is held high, and the flip-flop stores its \ncurrent value.\n\nThe problems with clock gating are twofold.  First, adding logic to\nthe clock path introduces clock skew, which may cause timing problems\nlater in the development process (or, worse, in future projects that\n\nuse your circuits as components).  Second, in the design shown above,\nthe LOAD signal\ncan only be lowered while the clock is high to prevent spurious rising\nedges from causing incorrect behavior, as shown in the timing diagram.\n\n\nA better approach is to use a mux and a feedback loop from the \nflip-flop's output, as shown in the figure to the right.  \n\nWhen LOAD is low, the mux selects the feedback line, and the \nregister reloads its current value.  \n\nWhen LOAD is high, the mux selects the IN input, and the register \nloads a new value.  \n\nThe result is similar to a gated D latch with distinct write enable \nand clock lines.\n\n\n{file=part2/figs/lec16-2.eps,width=2in}\n\n\n\nWe can use this extended flip-flop as a bit slice for a multi-bit register.\n\nA four-bit register of this type\nis shown to the right.  Four data {lines---one} for each\n{bit---enter} the registers from the top of the figure.  \nWhen LOAD is low, the logic copies each flip-flop's value back to its\ninput,\n\n\n{file=part2/figs/lec16-3.eps,width=5in}\n\n\nand the IN input lines are ignored.  When LOAD is high,\nthe muxes forward each IN line to the corresponding flip-flop's D\ninput, allowing the register to load the new {4-bit} value.\nThe use of one input line per bit to load a multi-bit\nregister in a single cycle is termed a { parallel load}.\n\n \n Question: What is the difference between a gated D latch and a LOAD low register? \n Answer: \n\nA. The extended flip-flop uses a mux and a feedback loop from the flip-flop's output to reload its current value when LOAD is low. When LOAD is high, the mux selects the IN input, and the register loads a new value."}, {"text": "Title: Shift Registers \n Text: {Shift Registers}\n\n\nCertain types of registers include logic to manipulate data held\nwithin the register.  A { shift register} is an important example\nof this\n\n\n{file=part2/figs/lec16-4.eps,width=5in}\n\n\ntype.  The simplest shift register is a series of D flip-flops,\nwith the output of each attached to the input of the next, as shown to the\nright above.  In the circuit shown, a serial input SI accepts a single bit \nof data per cycle and delivers the bit four cycles later to a serial \noutput SO.  Shift registers serve many purposes in modern systems, from the\nobvious uses of providing a fixed delay and performing bit shifts for\nprocessor arithmetic to rate matching between components and reducing\nthe pin count on programmable logic devices such as field programmable\ngate arrays (FPGAs), the modern form of the programmable logic array\nmentioned in the textbook.\n\nAn example helps to illustrate the rate matching problem: \nhistorical I/O buses used fairly slow clocks, as they had to\ndrive signals and be arbitrated over relatively long distances.\nThe Peripheral Control\nInterconnect (PCI) standard, for example, provided for 33 and 66 MHz\nbus speeds.  To provide adequate data rates, such buses use many wires\nin parallel, either 32 or 64 in the case of PCI.  In contrast, a\nGigabit Ethernet (local area network) signal travelling over a fiber\nis clocked at 1.25 GHz, but sends only one bit per cycle.  Several\nlayers of shift registers sit between the fiber and the I/O bus to\nmediate between the slow, highly parallel signals that travel over the\nI/O bus and the fast, serial signals that travel over the \nfiber.  The latest variant of PCI, PCIe (e for ``express''),\nuses serial lines at much higher clock rates.\n\nReturning to the figure above, imagine that the outputs Q_i feed\ninto logic clocked at 1/4^ the rate of the shift register \n(and suitably synchronized).  Every four cycles, the flip-flops fill\nup with another four bits, at which point the outputs are read in\nparallel.  The shift register shown can thus serve to transform serial\ndata to {4-bit-parallel} data at one-quarter the clock speed.\nUnlike the registers discussed earlier, the shift register above does\nnot support parallel load, which prevents it from transforming a slow,\nparallel stream of data into a high-speed serial stream.  The use of\n{ serial load} requires N cycles for an {N-bit}\nregister, but can reduce the number of wires needed to support the\noperation of the shift register.  How would you add support for\nparallel load?  How many additional inputs would be necessary?\n\nThe shift register above also shifts continuously, and cannot store a \nvalue.  A set of muxes, analogous to those that we used to control \nregister loading, can be applied to control shifting, as shown \nbelow.\n\n{{file=part2/figs/lec16-5.eps,width=5.3in}}\n\nUsing a {4-to-1} mux, we can construct a shift\nregister with additional functionality.  The bit slice at the top\nof the next page allows us to build a { bidirectional shift register} with \nparallel load capability and the ability to retain its value indefinitely.\nThe two-bit control input C uses a representation that\nwe have chosen for the four operations supported by our shift register, \nas shown in the table below the bit slice design.\n\n\nThe bit slice allows us to build {N-bit} shift registers by\nreplicating the slice and adding a fixed amount of ``{ glue logic}.''\nFor example, the figure below represents a {4-bit} bidirectional \nshift register constructed in this way.  The mux\nused for the SO output logic is the glue logic needed in addition\nto the four bit slices.\n\nAt each rising clock edge, the action specified by C_1C_0 is taken.  \nWhen C_1C_0=00, the\nregister holds its current value, with the register\nvalue appearing on\nQ[3:0] and each flip-flop feeding its output back into its input.\nFor C_1C_0=01, the shift register shifts left: the serial input,\nSI, is fed into flip-flop 0, and Q_3 is passed to the serial\noutput, SO.  Similarly, when C_1C_0=11, the shift register shifts\nright: SI is fed into flip-flop 3, and Q_0 is passed to SO.\nFinally, the case C_1C_0=10 causes all flip-flops to accept new\nvalues from IN[3:0], effecting a parallel load.\n\n\n{file=part2/figs/lec16-6.eps,width=2.3in}\n{c|c}\nC_1C_0& meaning \n00& retain current value\n01& shift left (low to high)\n10& load new value (from IN)\n11& shift right (high to low)\n\n{-4pt}\n\n{{file=part2/figs/lec16-7.eps,width=5.2in}}\n\nSeveral specialized shift operations are used to support data\nmanipulation in modern processors (CPUs).  Essentially, these\nspecializations dictate the glue logic for a shift\nregister as well as the serial input value.  The simplest is a {\nlogical shift}, for which SI is hardwired to 0: incoming\nbits are always 0.  A { cyclic shift} takes SO and feeds it\nback into SI, forming a circle of register bits through which the\ndata bits cycle.\n\nFinally, an { arithmetic shift} treats the shift register contents\nas a number in 2's complement form.  For non-negative numbers and left\nshifts, an arithmetic shift is the same as a logical\nshift.  When a negative number is arithmetically shifted to\nthe right, however, the sign bit is retained, resulting in a function\nsimilar to division by two.  The difference lies in the rounding\ndirection.  Division by two rounds towards zero in most \nprocessors: -5/2 gives -2.\nArithmetic shift right rounds away from zero for negative numbers (and\ntowards zero for positive numbers): -5>>1 gives -3.  We transform our\nprevious shift register into one capable of arithmetic shifts by\neliminating the serial input and feeding the most significant bit,\nwhich represents the sign in 2's complement form, back into itself for\nright shifts, as shown below.  The bit shifted in for left shifts\nhas been hardwired to 0.\n\n{{file=part2/figs/lec16-8.eps,width=5.2in}}\n\n\n\n \n Question: What type of register is an important example of? \n Answer: \n\nA shift register is a register that includes logic to manipulate data held within the register. A shift register is an important example of this type."}, {"text": "Title: Boolean Properties and Don't Care Simplification \n Text: {Boolean Properties and Don't Care Simplification}\n\nThis set of notes begins with a brief illustration of a few properties\nof Boolean logic, which may be of use to you in manipulating algebraic\nexpressions and in identifying equivalent logic functions without resorting \nto truth tables.\n\nWe then discuss the value of underspecifying a logic function so as\nto allow for selection of the simplest possible implementation.\n\nThis technique must be used carefully to avoid incorrect behavior,\nso we illustrate the possibility of misuse with an example, then\ntalk about several ways of solving the example correctly.\n\nWe conclude by generalizing the ideas in the example to several\nimportant application areas and talking about related problems.\n\n \n Question: What is the value of underspecifying a logic function? \n Answer: \n\nA. The value of underspecifying a logic function is that it allows for selection of the simplest possible implementation."}, {"text": "Title: Logic Properties \n Text: {Logic Properties}\n\nTable  (on the next page)\nlists a number of properties of Boolean logic.\nMost of these are easy to derive from our earlier definitions, but\na few may be surprising to you.  In particular, in the algebra of\nreal numbers, multiplication\ndistributes over addition, but addition does not distribute over\nmultiplication.  For example, 3{(4+7)}=(3)+(3),\nbut 3+(4)=(3+4){(3+7)}.  In Boolean algebra,\nboth operators distribute over one another, as indicated in\nTable .  The consensus properties may also be \nnonintuitive.  Drawing a {K-map} may help you understand the\nconsensus property on the right side of the table.  For the \nconsensus variant on the left side of the table,\nconsider that since either A or  must be 0,\neither B or C or both must be 1 for the first two factors on the left\nto be 1 when ANDed together.  But in that case, the third factor \nis also 1, and is thus redundant.\n\nAs mentioned previously, Boolean\nalgebra has an elegant symmetry known as a duality, in which any\nlogic statement (an expression or an equation) is related to a\nsecond logic statement.\n\nTo calculate the { dual form} of a Boolean expression or equation, \nreplace 0 with 1, replace 1 with 0, \nreplace AND with OR, and\nreplace OR with AND.\n\n{ Variables are not changed when finding the dual form.}\n\nThe dual form of a dual form is the original logic statement.\n\nBe careful when calculating a dual form: our convention for ordering \narithmetic operations is broken by the exchange, so you may want\nto add explicit parentheses before calculating the dual.  For\nexample, the dual of AB+C is not A+BC.\nRather, the dual of AB+C is (A+B)C.\n\n{ Add parentheses as necessary when calculating a dual form to ensure\nthat the order of operations does not change.}\n\nDuality has several useful practical applications.  First, the \n{ principle of duality} states that any theorem or identity has \nthe same truth value in dual form (we do not prove the principle\nhere).\n\nThe rows of Table  are organized according to this\nprinciple: each row contains two equations that are the duals \nof one another.  \n However, we rename variables freely to\n simplify the dual form.  For example, taking the dual of 1+A=1,\n we obtain 0=0.  But we created variable A \n solely for the purpose of expressing the property, so we might\n instead have called the complementary value A.  We rename\n (using the same name!) to obtain 0=0, as shown in the table.\n\nSecond, the dual form is useful when designing certain types of logic,\nsuch as the networks of transistors connecting the output of a CMOS\ngate to high voltage and ground.  If you look at the gate designs in\nthe textbook (and particularly those in the exercises),\nyou will notice that these networks are duals.  \n\nA function/expression is not a theorem nor an identity,\nthus the principle of duality does not apply to the dual of an\nexpression.\n\nHowever, if you treat the value 0 as ``true,'' the dual form of an\nexpression has the same truth values as the original (operating with\nvalue 1 as ``true'').\n\nFinally, you can calculate the complement of a Boolean function\n(any expression) by calculating the dual form and then complementing\neach variable.\n\n \n Question: What is the principle of duality? \n Answer: \n\nA. The principle of duality states that any theorem or identity has the same truth value in dual form. This means that any theorem or identity has the same truth value in its dual form."}, {"text": "Title: Choosing the Best Function \n Text: {Choosing the Best Function}\n\nWhen we specify how something works using a human language, we\nleave out details.  Sometimes we do so deliberately, assuming that a\nreader or listener can provide the details themselves: ``Take me to the \nairport!''\nrather than ``Please bend your right arm at the elbow and shift your\nright upper arm forward so as to place your hand near the ignition key.\nNext, ...''  \n\n\n{\n{|lll|}\n1+A=1& 0=0&\n1=A& 0+A=A&\nA+A=A& A=A&\nA=0& A+=1&\n{A+B}= &\n=+& DeMorgan's laws\n(A+B)C=AC+BC&\nA B+C=(A+C)(B+C)&distribution\n(A+B)(+C)(B+C)=(A+B)(+C)&\nA B+ C+B C=A B+ C& consensus\n\n}\n{Boolean logic properties.  The two columns are dual forms of\none another.}\n\n\n\nYou know the basic technique for implementing a Boolean function\nusing { combinational logic}: use a {K-map} to identify a\nreasonable SOP or POS form, draw the resulting design, and perhaps\nconvert to NAND/NOR gates.\n\n\n\nWhen we develop combinational logic designs, we may also choose to\nleave some aspects unspecified.  In particular, the value of a\nBoolean logic function to be implemented may not matter for some\ninput combinations.  If we express the function as a truth table,\nwe may choose to mark the function's value for some input combinations \nas ``{ don't care},'' which is written as ``x'' (no quotes).\n\nWhat is the benefit of using ``don't care'' values?  \n\nUsing ``don't care'' values allows you to choose from among several\npossible logic functions, all of which produce the desired results\n(as well as some combination of 0s and 1s in place of the ``don't\ncare'' values).  \n\nEach input combination marked as ``don't care'' doubles the number\nof functions that can be chosen to implement the design, often enabling \nthe logic needed for implementation to be simpler.\n\n\nFor example, the {K-map} to the right specifies a function F(A,B,C)\nwith two ``don't care'' entries.  \n\nIf you are asked to design combinational logic for this function,\nyou can\nchoose any values for the two ``don't care'' entries.  When identifying\nprime implicants, each ``x'' can either be a 0 or a 1.\n\n\n\n\n\n\nDepending on the choices made for the x's, we obtain one of \nthe following four functions:\n\n{eqnarray*}\nF&=& B+B C\nF&=& B+B C+A  \nF&=&B\nF&=&B+A \n{eqnarray*}\n\n\n\n\n\nGiven this set of choices, a designer typically chooses the third: F=B,\nwhich corresponds to the {K-map} shown to the right of the \nequations.  The design then \nproduces F=1 when A=1, B=1, and C=0 (ABC=110), \nand produces F=0 when A=1, B=0, and C=0 (ABC=100).\nThese differences are marked with shading and green italics in the new\n{K-map}.  No implementation ever produces an ``x.''\n\n \n Question: What is the benefit of using 'don't care' values? \n Answer: \n\nA. Using \"don't care\" values allows you to choose from among several possible logic functions, all of which produce the desired results. This can often enable the logic needed for implementation to be simpler."}, {"text": "Title: Caring about Don't Cares \n Text: {Caring about Don't Cares}\n\nWhat can go wrong?\n\nIn the context of a digital system, unspecified details\nmay or may not be important.  However, { any implementation of \na specification \nimplies decisions} about these details, so decisions should only be left \nunspecified if any of the possible answers is indeed acceptable.\n\nAs a concrete example, let's design logic to control an ice cream \ndispenser.  The dispenser has two flavors, lychee and mango, but\nalso allows us to create a blend of the two flavors.\nFor each of the two flavors, our logic must output two bits\nto control the amount of ice cream that comes out of the dispenser.\nThe two-bit C_L[1:0] output of our logic must specify the number \nof half-servings of lychee ice cream as a binary number, and\nthe two-bit C_M[1:0] output must specify the number of \nhalf-servings of mango ice cream.  Thus, for either flavor,\n00 indicates none of that flavor,\n01 indicates one-half of a serving, and \n10 indicates a full serving.\n\nInputs to our logic will consist of three buttons: an L button to\nrequest a serving of lychee ice cream, a B button to request a\nblend---half a serving of each flavor, and an M button to request\na serving of mango ice cream.  Each button produces a 1 \nwhen pressed and a 0 when not pressed.\n\n\n\nLet's start with the assumption that the user only presses one button\nat a time.  In this case, we can treat input combinations in which\nmore than one button is pressed as ``don't care'' values in the truth\ntables for the outputs.  K-maps for all four output bits appear below.\nThe x's indicate ``don't care'' values.\n\n\n\nWhen we calculate the logic function for an output, each ``don't care''\nvalue can be treated as either 0 or 1, whichever is more convenient\nin terms of creating the logic.  In the case of C_M[1], for \nexample, we can treat the three x's in the ellipse as 1s, treat the x\noutside of the ellipse as a 0, and simply\nuse M (the implicant represented by the ellipse) for C_M[1].  The\nother three output bits are left as an exercise, although the result \nappears momentarily.\n\n\nThe implementation at right takes full advantage of the ``don't care''\nparts of our specification.  In this case, we require no logic at all;\nwe need merely connect the inputs to the correct outputs.  Let's verify\nthe operation.  We have four cases to consider.  First, if none of the \nbuttons are pushed (LBM=000), we get no ice cream, as desired (C_M=00 \nand C_L=00).  Second, if we request lychee ice cream (LBM=100), the\noutputs are C_L=10 and C_M=00, so we get a full serving of lychee\nand no mango.  Third, if we request a blend (LBM=010), the outputs\nare C_L=01 and C_M=01, giving us half a serving of each flavor.\nFinally, if we request mango ice cream (LBM=001), we get no lychee\nbut a full serving of mango.\n\n\n\n\n\nThe K-maps for this implementation appear below.  Each of\nthe ``don't care'' x's from the original design has been replaced with\neither a 0 or a 1 and highlighted with shading and green italics.\nAny implementation produces \neither 0 or 1 for every output bit for every possible input combination.\n\n{{file=part2/figs/CLhigh-basic.eps,width=1.00in}{file=part2/figs/CLlow-basic.eps,width=1.00in}{file=part2/figs/CMhigh-basic.eps,width=1.00in}{file=part2/figs/CMlow-basic.eps,width=1.00in}}\n\nAs you can see, leveraging ``don't care'' output bits can sometimes\nsignificantly simplify our logic.  In the case of this example, we were\nable to completely eliminate any need for gates!  Unfortunately, \nthe resulting implementation may sometimes produce unexpected results. \n\nBased on the implementation, what happens if a user presses more\nthan one button?  The ice cream cup overflows!\n\nLet's see why.\n\nConsider the case LBM=101, in which we've pressed both the lychee and\nmango buttons.  Here C_L=10 and C_M=10, so our dispenser releases a \nfull serving of each flavor, or two servings total.  Pressing other \ncombinations may have other repercussions as well.\nConsider pressing lychee and blend (LBM=110).  The outputs are then\nC_L=11 and C_M=01.  Hopefully the dispenser simply gives us one\nand a half servings of lychee and a half serving of mango.  However,\nif the person who designed the dispenser assumed that no one would ever\nask for more than one serving, something worse might happen.  In other\nwords, giving an input of C_L=11 to the ice cream dispenser may lead to\nother unexpected behavior if its designer decided that that input \npattern was a ``don't care.''\n\nThe root of the problem is that { while we don't care about the value of\nany particular output marked ``x'' for any particular input combination,\nwe do actually care about the relationship between the outputs}.  \n\nWhat can we do?  When in doubt, it is safest to make \nchoices and to add the new decisions to the specification rather than \nleaving output values specified as ``don't care.''\n\nFor our ice cream dispenser logic, rather than leaving the outputs \nunspecified whenever a user presses more than one button, we could \nchoose an acceptable outcome for each input combination and \nreplace the x's with 0s and 1s.  We might, for example, decide to\nproduce lychee ice cream whenever the lychee button is pressed, regardless\nof other buttons (LBM=1xx, which means that we don't care about the\ninputs B and M, so LBM=100, LBM=101, LBM=110, or LBM=111).  \nThat decision alone covers three of the\nfour unspecified input patterns.  We might also decide that when the \nblend and mango buttons are pushed together (but without the lychee\nbutton, LBM=011), our logic produces a blend.  \n\nThe resulting K-maps are shown below, again with shading and green italics \nidentifying\nthe combinations in which our original design specified ``don't care.''\n\n{{file=part2/figs/CLhigh-priority.eps,width=1.00in}{file=part2/figs/CLlow-priority.eps,width=1.00in}{file=part2/figs/CMhigh-priority.eps,width=1.00in}{file=part2/figs/CMlow-priority.eps,width=1.00in}}\n\n\nThe logic in the dashed box to the right implements the set of choices\njust discussed, and matches the K-maps above.\n\nBased on our additional choices, this implementation enforces\na strict priority scheme on the user's button presses.\n\nIf a user requests lychee, they can also press either or\nboth of the other buttons with no effect.  The lychee button has\npriority.  Similarly, if the user does not press lychee, but press-\n\n\n\n\n\nes the blend button, pressing the mango button at the same time has no\neffect.  Choosing mango requires that no other buttons be pressed.\nWe have thus chosen a prioritization order for the buttons and imposed \nthis order on the design.\n\nWe can view this same implementation in another way.\n\nNote the one-to-one correspondence between inputs (on the left) and \noutputs (on the right) for the dashed box.\n\nThis logic takes the user's button presses and chooses at most one of\nthe buttons to pass along to our original controller implementation \n(to the right of the dashed box).\n\nIn other words, rather than thinking of the logic in the dashed\nbox as implementing a specific set of decisions, we can think of the\nlogic as cleaning up the inputs to ensure that only valid\ncombinations are passed to our original implementation.\n\nOnce the inputs are cleaned up, the original implementation is \nacceptable, because input combinations containing more than a \nsingle 1 are in fact impossible.\n\nStrict prioritization is one useful way to clean up our inputs.\n\nIn general, we can design logic to map each\nof the four undesirable input patterns into one of the permissible \ncombinations (the\nfour that we specified explicitly in our original design, with LBM \nin the set ).  Selecting a prioritization scheme\nis just one approach for making these choices in a way\nthat is easy for a user to understand and \nis fairly easy to implement.\n\n\nA second simple approach is to ignore illegal\ncombinations by mapping them into the ``no buttons pressed'' \ninput pattern.\n\nSuch an implementation appears to the right, laid out to show that\none can again view the logic in the dashed box either as cleaning up \nthe inputs (by mentally grouping the logic with the inputs) or as a specific \nset of choices for our ``don't care'' output values (by grouping the \nlogic with the outputs).\n\nIn either case, the logic shown \nenforces our as-\n\n\n\n\n\nsumptions in a fairly conservative way:\nif a user presses more than one button, the logic squashes all button\npresses.  Only a single 1 value at a time can pass through to \nthe wires on the right of the figure.\n\n\n\nFor completeness, the K-maps corresponding to this implementation are given\nhere.\n\n{{file=part2/figs/CLhigh-conserve.eps,width=1.00in}{file=part2/figs/CLlow-conserve.eps,width=1.00in}{file=part2/figs/CMhigh-conserve.eps,width=1.00in}{file=part2/figs/CMlow-conserve.eps,width=1.00in}}\n\n\n \n Question: What is the difference between the two output bits? \n Answer: \n\nA. A \"don't care\" output bit can be either 0 or 1, while an output bit that is specified as 0 or 1 must be one or the other."}, {"text": "Title: Generalizations and Applications* \n Text: {Generalizations and Applications*}\n\nThe approaches that we illustrated to clean up the input signals to\nour design have application in many areas.  The ideas in this \nsection are drawn from the field and are sometimes the subjects of \nlater classes, but { are not exam material for our class.}\n\nPrioritization of distinct\ninputs is used to arbitrate between devices attached to a\nprocessor.  Processors typically execute much more quickly than do devices.\nWhen a device needs attention, the device signals the processor\nby changing the voltage on an interrupt line (the name comes from the\nidea that the device interrupts the processor's current activity, such\nas running a user program).  However, more than\none device may need the attention of the processor simultaneously, so\na priority encoder is used to impose a strict order on the devices and\nto tell the processor about their needs one at a time.\n\nIf you want to learn more about this application, take ECE391.\n\nWhen components are designed together, assuming that some input patterns\ndo not occur is common practice, since such assumptions can dramatically\nreduce the number of gates required, improve performance, reduce power\nconsumption, and so forth.  As a side effect, when we want to test a\nchip to make sure that no defects or other problems prevent the chip\nfrom operating correctly, we have to be careful so as not to ``test''\nbit patterns that should never occur in practice.  Making up random bit\npatterns is easy, but can produce bad results or even destroy the chip\nif some parts of the design have assumed that a combination produced \nrandomly can never occur.  To avoid these problems, designers add extra\nlogic that changes the disallowed patterns into allowed patterns, just\nas we did with our design.  The use of random bit patterns is common in\nBuilt-In Self Test (BIST), and so the process of inserting extra logic\nto avoid problems is called BIST hardening.  BIST hardening can add\n{10-20} additional logic to a design.\n\nOur graduate class on digital system testing, ECE543, covers this\nmaterial, but has not been offered recently.\n\n\n\n\n\n \n Question: Who is the target audience for this text? \n Answer: \n\nA. The audience for this text is primarily students in an introductory digital design class."}, {"text": "Title: Optimizing Logic Expressions \n Text: {Optimizing Logic Expressions}\n\nThe second part of the course covers digital design more\ndeeply than does the textbook.  The lecture notes will explain the\nadditional material, and we will provide further examples in lectures\nand in discussion sections.  Please let us know if you need further\nmaterial for study.\n\nIn the last notes, we introduced Boolean logic operations and showed\nthat with AND, OR, and NOT, we can express any Boolean function on any\nnumber of variables.\n\nBefore you begin these notes, please read the first two sections \nin Chapter 3 of the textbook,\nwhich discuss the operation of { complementary metal-oxide semiconductor}\n({ CMOS}) transistors, illustrate how gates implementing the\nAND, OR, and NOT operations can be built using transistors,\nand introduce DeMorgan's laws. \n\nThis set of notes exposes you to a mix of techniques,\nterminology, tools, and philosophy.  Some of the material is not\ncritical to our class (and will not be tested), but is useful for\nyour broader education, and may help you in later classes.  The value\nof this material has changed substantially in the last couple of decades,\nand particularly in the last few years, as algorithms for tools\nthat help with hardware design have undergone rapid advances.  We\ntalk about these issues as we introduce the ideas.\n\nThe notes begin with a discussion of the ``best'' way to\nexpress a Boolean function and some techniques used historically\nto evaluate such decisions.\n\nWe next introduce the terminology necessary to understand manipulation\nof expressions, and use these terms to explain the Karnaugh map, or\n{K-map}, a tool that we will use for many purposes this semester.\n\nWe illustrate the use of {K-maps} with a couple of\nexamples, then touch on a few important questions and useful\nways of thinking about Boolean logic.\n\nWe conclude with a discussion of the general problem of \nmulti-metric optimization, introducing some ideas and approaches\nof general use to engineers.\n\n\n \n Question: What is the best way to express a Boolean function? \n Answer: \n\nA. The best way to express a Boolean function is to use a truth table."}, {"text": "Title: Defining Optimality \n Text: {Defining Optimality}\n\nIn the notes on logic operations, you learned how to express an\narbitrary function on bits as an OR of minterms (ANDs with one\ninput per variable on which the function operates). \nAlthough this approach demonstrates logical completeness, the results\noften seem inefficient, as you can see by comparing the following \nexpressions for the carry out C from the\naddition of two {2-bit} unsigned numbers, A=A_1A_0 and B=B_1B_0.\n\n\nC &=& A_1B_1 + (A_1+B_1)A_0B_0  \n&=& A_1B_1 + A_1A_0B_0 + A_0B_1B_0  \n&=& \n {A_1} A_0 B_1 B_0 +\nA_1 {A_0} B_1 {B_0} +\nA_1 {A_0} B_1 B_0 +\n&&A_1 A_0 {B_1} B_0 +\nA_1 A_0 B_1 {B_0} +\nA_1 A_0 B_1 B_0 \n\n\nThese three expressions are identical in the sense that they have\nthe same truth tables---they are the same mathematical function.\n\nEquation () is the form that we gave when we introduced\nthe idea of using logic to calculate overflow.  In this form, we were\nable to explain the terms intuitively.\n\nEquation () results from distributing the parenthesized OR\nin Equation ().\n\nEquation () is the result of our logical completeness\nconstruction.  \n\nSince the functions are identical, does the form actually matter at all?\nCertainly either of the first two forms is easier for us to write \nthan is the third.  If we think\nof the form of an expression as a mapping from the function that we\nare trying to calculate into the AND, OR, and NOT functions that we\nuse as logical building blocks, we might also say that the first two\nversions use fewer building blocks.  That observation does have some\ntruth, but let's try to be more precise by framing a question.\n\nFor any given function, there are an infinite number of ways that we can\nexpress the function (for example, given one variable A on which the\nfunction depends, you can OR together any number of copies \nof A without changing the function).\n\n{ What exactly makes one expression better than another?}\n\n\n\nIn 1952, Edward Veitch wrote an article on simplifying truth functions.\nIn the introduction, he said, ``This general problem can be very complicated\nand difficult.  Not only does the complexity increase greatly with the\nnumber of inputs and outputs, but the criteria of the best circuit will\nvary with the equipment involved.''\n\nSixty years later, the answer is largely the same: the criteria depend\nstrongly on the underlying technology (the gates and the devices used\nto construct the gates), and no single { metric}, or way of measuring,\nis sufficient to capture the important differences between expressions \nin all cases.\n\nThree high-level metrics commonly used to evaluate chip\ndesigns are cost, power, and performance.  Cost usually represents the\nmanufacturing cost, which is closely related to the physical silicon\narea required for the design: the larger the chip, the more expensive\nthe chip is to produce.  Power measures energy consumption over\ntime.  A chip that consumes more power means that a user's energy bill\nis higher and, in a portable device, either that the device is heavier or has\na shorter battery life.  Performance measures the speed at which the\ndesign operates.  A faster design can offer more functionality, such as\nsupporting the latest games, or can just finish the same work in less\ntime than a slower design.  These metrics are sometimes related: if\na chip finishes its work, the chip can turn itself off, saving energy.\n\nHow do such high-level metrics relate to the problem at hand?  Only\nindirectly in practice.  There are too many factors involved to make\ndirect calculations of cost, power, or performance at the level of\nlogic expressions.  \n\nFinding an { optimal} solution---the best formulation of a specific\nlogic function for a given metric---is often impossible using the \ncomputational resources and algorithms available to us.\n\nInstead, tools typically use heuristic approaches\nto find solutions that strike a balance between these metrics.\nA { heuristic} approach is one that is\nbelieved to yield fairly good solutions to a problem, but does\nnot necessarily find an optimal solution.\n\nA human engineer can typically impose { constraints}, such as\nlimits on the chip area or limits on the minimum performance,\nin order to guide the process.\n\nHuman engineers may also restructure the implementation of a \nlarger design, such as a design to perform floating-point arithmetic,\nso as to change the logic functions used in the design.\n\n{ Today, manipulation of logic expressions for the purposes of \noptimization is performed almost entirely by computers.}  Humans must\nsupply the logic functions of interest, and must program the acceptable \ntransformations between equivalent forms, but computers do the grunt\nwork of comparing alternative formulations and deciding which one is\nbest to use in context.\n\nAlthough we believe that hand optimization of Boolean expressions is no\nlonger an important skill for our graduates, we do think that you\nshould be exposed to the ideas and metrics historically used for\nsuch optimization.  The rationale for retaining this exposure is \nthreefold. \n\nFirst, we believe that you still need to be able to perform basic\nlogic reformulations (slowly is acceptable)\nand logical equivalence checking (answering the question, ``Do two \nexpressions represent the same function?'').\n\nSecond, the complexity of the problem is a good way to introduce you\nto real engineering.\n\nFinally, the contextual information will help you to develop a better\nunderstanding of finite state machines and higher-level abstractions\nthat form the core of digital systems and are still defined directly\nby humans today.\n\nTowards that end, we conclude this introduction by discussing two\nmetrics that engineers traditionally used to optimize \nlogic expressions.  These metrics are now embedded in { computer-aided \ndesign} ({ CAD}) tools and tuned to specific underlying technologies,\nbut the reasons for their use are still interesting.\n\nThe first metric of interest is a heuristic for the area needed for\na design.\n\nThe measurement is simple: count the number of variable occurrences in\nan expression.  Simply go through and add up how many variables you see.\nUsing our example function C, \nEquation () gives a count of 6,\nEquation () gives a count of 8, and\nEquation () gives a count of 24.\nSmaller numbers represent better expressions, so \nEquation () is the best choice by this metric.\n\nWhy is this metric interesting?\nRecall how gates are built from transistors.\nAn {N-input} gate requires roughly 2N transistors, so if you \ncount up the number of variables in the\nexpression, you get an estimate of the number of transistors needed,\nwhich is in turn an estimate for the area required for the design.\n\nA variation on variable counting is to add the number of operations,\nsince each gate also takes space for wiring (within as well as between\ngates).  Note that we ignore the number of inputs to the operations,\nso a {2-input} AND counts as 1, but a {10-input} AND also counts\nas 1.  We do not usually count complementing variables as an operation\nfor this metric because the complements of variables are sometimes \navailable at no extra cost in gates or wires.\n\nIf we add the number of operations in our example, we get\na count of 10 for Equation ()---two ANDs, two ORs,\nand 6 variables,\n\na count of 12 for Equation ()---three ANDS, one OR,\nand 8 variables,\n\nand a count of 31 for Equation ()---six ANDs, one OR,\nand 24 variables.\n\nThe relative differences between these equations \nare reduced when one counts operations.\n\nA second metric of interest is a heuristic for the performance of a design.\n\nPerformance is inversely related to the delay necessary for a design\nto produce an output once its inputs are available.  For example, if\nyou know how many seconds it takes to produce a result, you can easily\ncalculate the number of results that can be produced per second, which \nmeasures performance.\n\nThe measurement needed is the longest chain of operations\nperformed on any instance of a variable.  The complement of\na variable is included if the variable's complement is not \navailable without using an inverter.\n\nThe rationale for this metric is that gate outputs do not change \ninstantaneously when their inputs\nchange. Once an input to a gate has reached an appropriate voltage to represent \na 0 or a 1, the transistors in the gate switch (on or off) and electrons \nstart to move.\nOnly when the output of the gate reaches the appropriate new voltage \ncan the gates driven by the output start to change. \nIf we count each\nfunction/gate as one delay (we call this time a { gate delay}), we \nget an estimate of the time needed to compute\nthe function.\n\nReferring again to our example equations, we find that\nEquation () requires 3 gate delays,\nEquation () requires 2 gate delays,\nEquation () requires 2 or 3 gate delays, depending\non whether we have variable complements available.  Now \nEquation () looks more attractive: better performance than\nEquation () in return for a small extra cost in area.\n\nHeuristics for estimating energy use are too \ncomplex to introduce at this point, \nbut you should be aware that every time electrons move, they\ngenerate heat, so we might favor an expression that minimizes the number\nof bit transitions inside the computation.  Such a measurement is\nnot easy to calculate by hand,\nsince you need to know the likelihood of input combinations.\n\n\n \n Question: What is the heuristic approach to problem solving? \n Answer: \n\nA. A heuristic is an approach to problem-solving that is designed to find a good, but not necessarily optimal, solution. In the context of logic expression optimization, a heuristic approach is one that is believed to yield fairly good solutions, but does not necessarily find an optimal solution."}, {"text": "Title: Terminology \n Text: \n\nWe use many technical terms when we talk about simplification of\nlogic expressions, so we now introduce those terms so as to make\nthe description of the tools and processes easier to understand.\n\nLet's assume that we have a logic function F(A,B,C,D) that we want\nto express concisely.  A { literal} in an expression of F refers \nto either one of the variables or its complement.  In other words,\nfor our function F, the following is a complete set of literals:\nA, , B, ,\nC, , D, and .\n\nWhen we introduced the AND and OR functions, we also introduced \nnotation borrowed from arithmetic, using multiplication to represent AND\nand addition to represent OR.  We also borrow the related terminology, \nso a { sum} \nin Boolean algebra refers to a number of terms OR'd together \n(for example, A+B, or AB+CD), and\na { product} in Boolean algebra refers to a number of terms AND'd\ntogether (for example, A, or AB(C+D).  Note that\nthe terms in a sum or product may themselves be sums, products, or\nother types of expressions (for example, A).\n\nThe construction method that we used to demonstrate logical completeness\nmade use of minterms for each input combination for which the \nfunction F produces a 1.\n\nWe can now use the idea of a literal to give a simpler definition of minterm:\na { minterm} for a function on N variables is a product (AND \nfunction) of N literals in which each variable or its complement \nappears exactly once.  For our function F, examples of minterms\ninclude ABC,\nACD, and\nBC.\nAs you know, a minterm produces a 1 for exactly\none combination of inputs.\n\nWhen we sum minterms for each output value of 1\nin a truth table to express a function, as we did to obtain\nEquation (), we\nproduce an example of the sum-of-products form.\nIn particular, a { sum-of-products} ({ SOP}) is a sum composed of \nproducts of literals.\nTerms in a sum-of-products need not be minterms, however.\nEquation () is also in sum-of-products form.\nEquation (), however, is not, since the last\nterm in the sum is not a product of literals.\n\nAnalogously to the idea of a minterm, \nwe define a { maxterm} for a function on N variables as a sum (OR \nfunction) of N literals in which each variable or its complement \nappears exactly once.  Examples for F include (A+B++D),\n(A+++D), and\n(++C+).\nA maxterm produces a 0 for exactly\none combination of inputs.  Just as we did with minterms, we can\nmultiply a maxterm corresponding to each input combination for which\na function produces 0 (each row in a truth table that produces a 0\noutput) to create\nan expression for the function.  The resulting expression is in\na { product-of-sums} ({ POS}) form: a product of sums of literals.  \n\nThe carry out function that we used to produce \nEquation () has 10 input combinations that produce 0,\nso the expression formed in this way is unpleasantly long:\n\n{eqnarray*}\nC&=&\n({A_1}+{A_0}+B_1+B_0)\n({A_1}+A_0+B_1+{B_0})\n({A_1}+A_0+B_1+B_0)\n(A_1+{A_0}+{B_1}+B_0)\n&&(A_1+{A_0}+B_1+{B_0})\n(A_1+{A_0}+B_1+B_0)\n(A_1+A_0+{B_1}+{B_0})\n(A_1+A_0+{B_1}+B_0)\n&&(A_1+A_0+B_1+{B_0})\n(A_1+A_0+B_1+B_0)\n{eqnarray*}\n\nHowever, the approach can be helpful with functions that produce mostly 1s.\nThe literals in maxterms are complemented with respect to the\nliterals used in minterms.  For example, the maxterm\n({A_1}+{A_0}+B_1+B_0) in the equation above\nproduces a zero for input combination A_1=1, A_0=1, B_1=0, B_0=0.\n\nAn { implicant} G of a function F is defined to be a second\nfunction operating on the same variables for which\nthe implication G is true.  In terms of logic functions\nthat produce 0s and 1s, { if G is an implicant of F, \nthe input combinations for which G produces 1s are a subset of \nthe input combinations for which F produces 1s}.\n\nAny minterm for which F produces a 1, for example, is an implicant of F.\n\nIn the context of logic design, { the term implicant is used\nto refer to a single product of literals.}  In other words, if we\nhave a function F(A,B,C,D), examples of possible implicants of F\ninclude AB, B, ABC, and .\nIn contrast, although they may technically imply F, we typically\ndo not call expressions such as (A+B), C(+D), nor\nA+C implicants.\n\nLet's say that we have expressed function F in sum-of-products \nform.\nAll of the individual product terms in the expression are implicants of F.  \n\nAs a first step in simplification, we can ask: for each implicant, is it\npossible to remove any of the literals that make up the product?\n\nIf we have an implicant G for which the answer is no, we \ncall G a { prime implicant} of F.\n\nIn other words, if one removes any of the literals from a prime \nimplicant G of F,\nthe resulting product is not an implicant of F.\n\nPrime implicants are the main idea that we use to simplify logic expressions,\nboth algebraically and with graphical tools (computer tools use algebra\ninternally---by graphical here we mean drawings on paper).\n\n\n \n Question: What is the definition of a minterm for a function on N variables? \n Answer: \n\nA. A minterm is a product (AND function) of N literals in which each variable or its complement appears exactly once."}, {"text": "Title: Veitch Charts and Karnaugh Maps \n Text: {Veitch Charts and Karnaugh Maps}\n\nVeitch's 1952 paper was the first to introduce the idea of\nusing a graphical representation to simplify logic expressions.\nEarlier approaches were algebraic.\nA year later, Maurice Karnaugh published\na paper showing a similar idea with a twist.  The twist makes the use of \n{ Karnaugh maps} to simplify expressions\nmuch easier than the use of Veitch charts.  As a result,\nfew engineers have heard of Veitch, but everyone who has ever\ntaken a class on digital logic knows how to make use of a { K-map}. \n\n\n\nBefore we introduce the Karnaugh map, let's think about the structure\nof the domain of a logic function.  Recall that a function's { domain}\nis the space on which the function is defined, that is, for which the\nfunction produces values.  For a Boolean logic function on N variables,\nyou can think of the domain as sequences of N bits, but you can also\nvisualize the domain as an {N-dimensional} hypercube.  \nAn\n\n\n\n\n\n{ {N-dimensional} hypercube} is the generalization\nof a cube to N dimensions.  Some people only use the term hypercube when\nN, since we have other names for the smaller values: a point\nfor N=0, a line segment for N=1, a square for N=2, and a cube\nfor N=3.  The diagrams above and to the right illustrate the cases that \nare easily drawn on paper.  The black dots represent specific input\ncombinations, and the blue edges connect input combinations that differ\nin exactly one input value (one bit).\n\n\n\nBy viewing a function's domain in this way, we can make a connection\nbetween a product of literals and the structure of the domain.  Let's\nuse the {3-dimensional} version as an example.  We call the \nvariables A, B, and C, and note that the cube has 2^3=8 corners\ncorresponding to the 2^3 possible combinations\nof A, B, and C.\nThe simplest product of literals in this\ncase is 1, which is the product of 0 literals.  Obviously, the product 1 \nevaluates to 1 for any variable values.  We can thus think of it as\ncovering the entire domain of the function.  In the case of our example,\nthe product 1 covers the whole cube.  In order for the product 1 to \nbe an implicant of a function, the function itself must be the function 1.\n\nWhat about a product consisting of a single literal, such as A \nor ?  The dividing lines in the diagram illustrate the\nanswer: any such product term evaluates to 1 on a face of the cube,\nwhich includes 2^2=4 of the corners.  If a function evaluates to 1\non any of the six faces of the cube, the corresponding product term\n(consisting of a single literal) is an implicant of the function.\n\nContinuing with products of two literals, we see that any product of \ntwo literals, such as A or C, corresponds\nto an edge of our {3-dimensional} cube.  The edge includes 2^1=2\ncorners.  And, if a function evaluates to 1 on any of the 12 edges\nof the cube, the corresponding product term (consisting of two literals)\nis an implicant of the function.\n\nFinally, any product of three literals, such as B,\ncorresponds to a corner of the cube.  But for a function on three variables,\nthese are just the minterms.  As you know, if a function evaluates to 1\non any of the 8 corners of the cube, that minterm is an implicant of the\nfunction (we used this idea to construct the function to prove\nlogical completeness).\n\nHow do these connections help us to simplify functions?  If we're\ncareful, we can map cubes onto paper in such a way that product terms\n(the possible implicants of the function) usually form contiguous \ngroups of 1s, allowing us to spot them easily.  Let's work upwards\nstarting from one variable to see how this idea works.  The end result\nis called a Karnaugh map.\n\n\nThe first drawing shown to the right replicates our view of \nthe {1-dimensional} hypercube, corresponding to the domain of a\nfunction on one variable, in this case the variable A.  \nTo the right of the hypercube (line segment) are two variants\nof a Karnaugh map on one variable.  The middle variant clearly \nindicates the column corresponding to the product A (the other \ncolumn corresponds to ).  The right variant simply labels\nthe column with values for A.\n\n\n\n\n\n\nThe three drawings shown to the right illustrate the three possible product\nterms on one variable.  { The functions shown in \nthese Karnaugh maps are arbitrary, except that we have chosen\nthem such that each implicant shown is a prime implicant for the\nillustrated function.}\n\n\n\n\n\n\n\nLet's now look at two-variable functions.  We have replicated\nour drawing of the {2-dimensional} hypercube (square) to the right \nalong with two variants of Karnaugh maps on two variables.\nWith only two variables (A and B), the extension is fairly \nstraightforward, since we can use the second dimension of the \npaper (vertical) to express the second variable (B).\n\n\n\n\n\n\nThe number of possible products of literals grows rapidly with the \nnumber of variables.\nFor two variables, nine are possible, as shown to the right.\nNotice that all implicants have two properties.  First, they\noccupy contiguous regions of the grid.  And, second, their height\nand width are always powers of two.  These properties seem somewhat\ntrivial at this stage, but they are the key to the utility of\n{K-maps} on more variables.\n\n\n\n\n\n\nThree-variable functions are next.  The cube diagram is again replicated\nto the right.  But now we have a problem: how can we map four points\n(say, from the top half of the cube) into a line in such a way that \nany points connected by a blue line are adjacent in the {K-map}?\nThe answer is that we cannot, but we can preserve most of the connections\nby choosing an order such as the one illustrated by the arrow.\nThe result\n\n\n\n\n\nis called a Gray code.  Two {K-map} variants again appear\nto the right of the cube.  Look closely at the order of the two-variable\ncombinations along the top, which allows us to have as many contiguous\nproducts of literals as possible.  Any product of literals that contains\n but not A nor  wraps around the edges\nof the {K-map}, so you should think of it as rolling up into a cylinder \nrather than a grid.  Or you can think that we're unfolding the cube to\nfit the corners onto a sheet of paper, but the place that we split\nthe cube should still be considered to be adjacent when looking for \nimplicants.  The use of a Gray code is the one difference between a\n{K-map} and a Veitch chart; Veitch used the base 2 order, which\nmakes some implicants hard to spot.\n\nWith three variables, we have 27 possible products of literals.  You\nmay have noticed that the count scales as 3^N for N variables;\ncan you explain why?  We illustrate several product terms\nbelow.  Note that we sometimes\nneed to wrap around the end of the {K-map}, but that if we account\nfor wrapping, the squares covered by all product terms are contiguous.\nAlso notice that both the width and the height of all product terms \nare powers of two.  { Any square or rectangle that meets these two \nconstraints corresponds to a product term!}  And any such square or \nrectangle that is filled with 1s is an implicant of the function\nin the {K-map}.\n\n\n\nLet's keep going.  With a function on four variables---A, B, C, \nand D---we can use a Gray code order on two of the variables in \neach dimension.  Which variables go with which dimension in the grid\nreally doesn't matter, so we'll assign AB to the horizontal dimension\nand CD to the vertical dimension.  A few of the 81 possible product\nterms are illustrated at the top of the next page.  Notice that while wrapping \ncan now occur\nin both dimensions, we have exactly the same rule for finding implicants\nof the function: any square or rectangle (allowing for wrapping) \nthat is filled with 1s and has both height and width equal to (possibly\ndifferent) powers of two is an implicant of the function.  { \nFurthermore, unless such a square or rectangle is part of a larger\nsquare or rectangle that meets these criteria, the corresponding\nimplicant is a prime implicant of the function.}\n\n\n\n\n\n\n\n\nFinding a simple expression for a function using a {K-map} then\nconsists of solving the following problem: pick a minimal set of prime \nimplicants such that every 1 produced by the function is covered\nby at least one prime implicant.  The metric that you choose to\nminimize the set may vary in practice, but for simplicity, let's say \nthat we minimize the number of prime implicants chosen.\n\n\nLet's try a few!  The table on the left below reproduces (from\nNotes Set 1.4) the truth\ntable for addition of two {2-bit} unsigned numbers, A_1A_0\nand B_1B_0, to produce a sum S_1S_0 and a carry out C.\n{K-maps} for each output bit appear to the right.\nThe colors are used only to make the different prime implicants\neasier to distinguish.\nThe equations produced by summing these prime implicants appear\nbelow the {K-maps}.\n\n\n{cccc|ccc}\n{c|}& \nA_1& A_0& B_1& B_0& C& S_1& S_0 \n0&0& 0&0& 0& 0&0\n0&0& 0&1& 0& 0&1\n0&0& 1&0& 0& 1&0\n0&0& 1&1& 0& 1&1\n0&1& 0&0& 0& 0&1\n0&1& 0&1& 0& 1&0\n0&1& 1&0& 0& 1&1\n0&1& 1&1& 1& 0&0\n1&0& 0&0& 0& 1&0\n1&0& 0&1& 0& 1&1\n1&0& 1&0& 1& 0&0\n1&0& 1&1& 1& 0&1\n1&1& 0&0& 0& 1&1\n1&1& 0&1& 1& 0&0\n1&1& 1&0& 1& 0&1\n1&1& 1&1& 1& 1&0\n\n\n\n\n\n\n{eqnarray*}\nC&=&A_1 B_1+A_1 A_0 B_0+A_0 B_1 B_0\n\nS_1&=&\nA_1 {B_1} {B_0}+\nA_1 {A_0} {B_1}+\n{A_1} {A_0} B_1+\n{A_1} B_1 {B_0}+\n&&{A_1} A_0 {B_1} B_0+\nA_1 A_0 B_1 B_0\n\nS_0&=&A_0 {B_0}+{A_0} B_0\n{eqnarray*}\n\n\nIn theory, {K-maps} extend to an arbitrary number of variables.\nCertainly Gray codes can be extended.  An { {N-bit} Gray code} \nis a sequence of {N-bit} patterns that includes all possible\npatterns such that any two adjacent patterns differ in only one bit.\nThe code is actually a cycle: the first and last patterns also differ\nin only one bit.  You can construct a Gray code recursively as follows:\nfor an {(N+1)-bit} Gray code, write the sequence for an\n{N-bit} Gray code, then add a 0 in front of all patterns.\nAfter this sequence, append a second copy of the {N-bit} Gray\ncode in reverse order, then put a 1 in front of all patterns in the\nsecond copy.  The result is an {(N+1)-bit} Gray code.\nFor example, the following are Gray codes:\n\n\n1-bit& 0, 1\n2-bit& 00, 01, 11, 10\n3-bit& 000, 001, 011, 010, 110, 111, 101, 100\n4-bit& \n0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100,\n1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000\n\n\nUnfortunately, some of the beneficial properties of {K-maps} do\nnot extend beyond two variables in a dimension.  { Once you have three\nvariables in one dimension}, as is necessary if a function operates\non five or more variables, { not all product terms are contiguous\nin the grid}.  The terms still require a total number of rows and columns\nequal to a power of two, but they don't all need to be a contiguous\ngroup.  Furthermore, { some contiguous groups of appropriate size do\nnot correspond to product terms}.  So you can still make use of {K-maps}\nif you have more variables, but their use is a little trickier.\n\n \n Question: What is the difference between a Veitch chart and a Karnaugh map? \n Answer: \n\nA. The difference between a Veitch chart and a Karnaugh map is that a Karnaugh map uses a Gray code order, which makes some implicants easier to spot."}, {"text": "Title: Canonical Forms \n Text: {Canonical Forms}\n\nWhat if we want to compare two expressions to determine whether they\nrepresent the same logic function?  Such a comparison is a test of\n{ logical equivalence}, and is an important part of hardware design.\nTools today provide help with this problem, but you should understand\nthe problem.\n\nYou know that any given function can be expressed in many ways, and that\ntwo expressions that look quite different may in fact represent the\nsame function (look back at Equations () to ()\nfor an example).  But what if we rewrite the function using only \nprime implicants?  Is the result unique?  Unfortunately, no.\n\n\nIn general, { a sum of products is not unique (nor is a product of sums),\neven if the sum contains only prime implicants}.\n\nFor example, consensus terms may or may not be included in our\nexpressions.  (They are necessary for reliable design of \ncertain types of systems, as you will learn in a later ECE class.)\n\nThe green ellipse in the K-map to the right represents the consensus \nterm BC.\n{eqnarray*}\nZ &=& A C +  B + B C\nZ &=& A C +  B\n{eqnarray*}\n\n\n\n\n\n\nSome functions allow several equivalent formulations as\nsums of prime implicants, even without consensus terms.  \nThe K-maps shown to the right, for example, illustrate \nhow one function might be written in either of the following ways:\n\n{eqnarray*}\nZ &=&\n  D +\n C  +\nA B C +\nB  D\nZ &=&\n  C +\nB C  +\nA B D +\n  D\n{eqnarray*}\n\n\n\n\n\n\n\n\nWhen we need to compare two things (such as functions), we need to\ntransform them into what in mathematics is known as a { canonical form},\nwhich simply means a form that is defined so as to be unique \nfor each thing of the given type.\nWhat can we use for logic functions?  You already know two answers!  \n\nThe { canonical sum} of a function (sometimes called the\n{ canonical SOP form}) is the sum of minterms.  \n\nThe { canonical product} of a function (sometimes called the\n{ canonical POS form}) is the product of maxterms.  \n\nThese forms technically\nonly meet the mathematical definition of canonical if we agree on an order\nfor the min/maxterms, but that problem is solvable.\n\nHowever, as you already know, the forms are not particularly \nconvenient to use.  \n\nIn practice, people and tools in the industry use more compact \napproaches when comparing functions, but those solutions are a subject for\na later class (such as ECE 462).\n\n\n\n\n \n Question: What is the canonical sum of a function? \n Answer: \n\nThe canonical SOP form of a logic function is the sum of minterms."}, {"text": "Title: Two-Level Logic \n Text: {Two-Level Logic}\n\n\n{ Two-level logic} is a popular way of expressing logic functions.\nThe two levels refer simply to the number of functions through which an\ninput passes to reach an output, and both the SOP and POS forms are \nexamples of two-level logic.  In this section, we illustrate one of the \nreasons for this popularity and \nshow you how to graphically manipulate\nexpressions, which can sometimes help when trying to understand gate\ndiagrams.\n\nWe begin with one of DeMorgan's laws, which we can illustrate both \nalgebraically and graphically:\nC  =  B+A  =   \n\n\n{file=part2/figs/demorgan-nand.eps,width=0.95in}\n\n\n\n\nLet's say that we have a function expressed in SOP form, such as\nZ=ABC+DE+FGHJ.  The diagram on the left below shows the function\nconstructed from three AND gates and an OR gate.  Using DeMorgan's\nlaw, we can replace the OR gate with a NAND with inverted inputs.\nBut the bubbles that correspond to inversion do not need to sit at the\ninput to the gate.  We can invert at any point along the wire,\nso we slide each bubble down the wire to the output of the first column\nof AND gates.  { Be careful: if the wire splits, which does not\nhappen in our example, you have to replicate\nthe inverter onto the other output paths as you slide past the split\npoint!}  The end result is shown on the right: we have not changed the \nfunction, but now we use only NAND gates.  Since CMOS technology\nonly supports NAND and NOR directly, using two-level logic makes\nit simple to map our expression into CMOS gates.\n\n{file=part2/figs/SOP-equiv.eps,width=6.5in}\n\n\nYou may want to make use of DeMorgan's other law, illustrated graphically\nto the right, to perform the same transformation on a POS expression.  What\ndo you get?\n\n\n{file=part2/figs/demorgan-nor.eps,width=0.95in}\n\n\n\n \n Question: What is the name of two-level logic? \n Answer: \n\nA. Two-level logic is a popular way of expressing logic functions.\nThe two levels refer simply to the number of functions through which an\ninput passes to reach an output."}, {"text": "Title: Multi-Metric Optimization \n Text: {Multi-Metric Optimization}\n\nAs engineers, almost every real problem that you encounter will admit \nmultiple metrics for evaluating possible designs.  Becoming a good\nengineer thus requires not only that you be able to solve problems\ncreatively so as to improve the quality of your solutions, but also\nthat you are aware of how people might evaluate those solutions and\nare able both to identify the most important metrics and to balance \nyour design effectively according to them.  In this section, we\nintroduce some general ideas and methods that may be of use to you\nin this regard.\n\n{ We will not test you on the concepts in this section.}\n\nWhen you start thinking about a new problem, your first step\nshould be to think carefully about metrics of possible interest.\n\nSome important metrics may not be easy to quantify.  \n\nFor example, compatibility of a design with other products already \nowned by a customer has frequently defined the success or failure\nof computer hardware and software solutions.\n\nBut how can you compute the compability of your approach as\na number?\n\nHumans---including engineers---are not good at\ncomparing multiple metrics simultaneously.\n\nThus, once you have a set of metrics that you feel is complete, \nyour next step is to get rid of as many as you can.\n\nTowards this end, you may identify metrics that have no practical \nimpact in current technology, set threshold values for other metrics\nto simplify reasoning about them, eliminate redundant metrics,\ncalculate linear sums to reduce the count of metrics, and, finally,\nmake use of the notion of Pareto optimality.  All of these ideas are\ndescribed in the rest of this section.\n\nLet's start by considering metrics that we can quantify as real\nnumbers.\n\nFor a given metric, we can divide possible measurement values into\nthree ranges.\n\nIn the first range,\nall measurement values are equivalently useful.\nIn the second range, \npossible values are ordered and interesting with respect to\none another.\nValues in the third range are all impossible to use in practice.\nUsing power consumption as\nour example, the first range corresponds to systems in which\nwhen a processor's power consumption in a digital \nsystem is extremely low relative to the\npower consumption\nof the system.\nFor example, the processor in a computer might use less than 1 \nof the total used by \nthe system including the disk drive, the monitor, the power \nsupply, and so forth.  One power consumption value in this range \nis just as good as any\nanother, and no one cares about the power consumption of the processor \nin such cases.  In the second range, power consumption of the\nprocessor makes a difference.  Cell phones use most of their energy\nin radio operation, for example, but if you own a phone with a powerful\nprocessor, you may have noticed that you can turn off the phone and \ndrain the battery fairly quickly by playing a game.  Designing a\nprocessor that uses half as much power lengthens the battery life in\nsuch cases.  Finally,\nthe third region of power consumption measurements is impossible:\nif you use so much power, your chip will overheat or even burst\ninto flames.  Consumers get unhappy when such things happen.\n\nAs a first step, you can remove any metrics for which all solutions\nare effectively equivalent.\n\nUntil a little less than a decade ago, for example, the power \nconsumption of a desktop\nprocessor actually was in the first range that we discussed.  Power\nwas simply not a concern to engineers: all designs of \ninterest consumed so little power that no one cared.\n\nUnfortunately, at that point, power consumption jumped into the\nthird range rather quickly.  Processors hit a wall, and \nproducts had to be cancelled.  Given that the time spent designing\na processor has historically\nbeen about five years, a lot of engineering effort\nwas wasted because people had not thought carefully enough about\npower (since it had never mattered in the past).\n\nToday, power is an important metric that engineers must take into\naccount in their designs. \n\nHowever, in some areas, such as desktop and high-end server\nprocessors,\nother metrics (such as performance) may be so \nimportant that we always want to operate at the edge of the\ninteresting range.  In such cases, we might choose to treat \na metric such as power consumption as a { threshold}: stay\nbelow 150 Watts for a desktop processor, for example.  One still\nhas to make a coordinated effort to ensure that the system as\na whole does not exceed the threshold, but reasoning about \nthreshold values, a form of constraint, is easier than trying to\nthink about multiple metrics at once.\n\nSome metrics may only allow discrete quantification.  For example,\none could choose to define compatibility with previous processor\ngenerations as binary: either an existing piece of software\n(or operating system)\nruns out of the box on your new processor, or it does not.  If you \nwant people who own that software to make use of your new processor,\nyou must ensure that the value of this binary metric is 1, which\ncan also be viewed as a threshold.\n\nIn some cases, two metrics may be strongly { correlated}, meaning\nthat a design that is good for one of the metrics is frequently \ngood for the other metric as well.\n\nChip area and cost, for\nexample, are technically distinct ways to measure a digital design,\nbut we rarely consider them separately.\n\nA design that requires a larger chip is probably more complex,\nand thus takes more engineering time to get right (engineering time\ncosts money).  \n\nEach silicon wafer costs money to fabricate, and fewer copies of a \nlarge design fit on one wafer, so large chips mean more fabrication\ncost.\n\nPhysical defects in silicon can cause some chips not to work.  A large\nchip uses more silicon than a small one, and is thus more likely to suffer\nfrom defects (and not work).  Cost thus goes up again for large chips\nrelative to small ones.\n\nFinally, large chips usually require more careful testing to ensure\nthat they work properly (even ignoring the cost of getting the design\nright, we have to test for the presence of defects), which adds still\nmore cost for a larger chip.\n\nAll of these factors tend to correlate chip area and chip cost, to the\npoint that most engineers do not consider both metrics.\n\n\nAfter you have tried to reduce your set of metrics as much as possible,\nor simplified them by turning them into thresholds, you should consider\nturning the last few metrics into a weighted linear sum.  All remaining\nmetrics must be quantifiable in this case.\n\nFor example, if you are left with three metrics for which a given\ndesign has values A, B, and C, you might reduce these to one\nmetric by calculating D=w_AA+w_BB+w_CC.  What are the w values?\nThey are weights for the three metrics.  Their values represent the\nrelative importance of the three metrics to the overall evaluation.\nHere we've assumed that larger values of A, B, and C are either\nall good or all bad.  If you have metrics with different senses,\nuse the reciprocal values.  For example, if a large value of A is good,\na small value of 1/A is also good.  \n\nThe difficulty with linearizing metrics is that not everyone agrees\non the weights.  Is using less power more important than having a cheaper\nchip?  The answer may depend on many factors.\n\nWhen you are left with several metrics of interest, you can use the\nidea of Pareto optimality to identify interesting designs.\n\nLet's say that you have two metrics.  If a design D_1 is better than\na second design D_2 for both metrics, we say that D_1 \n{ dominates} D_2.\n\nA design D is then said to be { Pareto optimal} if no other design\ndominates D.  Consider the figure on the left below, which illustrates\nseven possible designs measured with two metrics.  The design corresponding\nto point B dominates the designs corresponding to points A and C, so \nneither of the latter designs is Pareto optimal.  No other point \nin the figure dominates B, however, so that design is Pareto optimal.\nIf we remove all points that do not represent Pareto optimal designs,\nand instead include only those designs that are Pareto optimal, we\nobtain the version shown on the right.  These are points in a two-dimensional\nspace, not a line, but we can imagine a line going through the points,\nas illustrated in the figure: the points that make up the line are\ncalled a { Pareto curve}, or, if you have more than two metrics,\na { Pareto surface}.\n\n{\n\n\n\n\nAs an example of the use of Pareto optimality, consider the figure\nto the right, which is copied with permission from Neal Crago's Ph.D. \ndissertation (UIUC ECE, 2012).  The figure compares hundreds of thousands \nof possible\ndesigns based on a handful of different core approaches for\nimplementing a processor.  The axes in the graph are two metrics \nof interest.  The horizontal axis measures the average performance of a \ndesign when\nexecuting a set of benchmark applications, normalized to\na baseline processor design.  The vertical axis measures the energy\nconsumed by a design when\n\n\n{file=part2/cited/bench_pareto.eps,width=3in}\n\n\nexecuting the same benchmarks, normalized\nagain to the energy consumed by a baseline design.  The six sets of\npoints in the graph represent alternative design techniques for the\nprocessor, most of which are in commercial use today.  The points\nshown for each set are the subset of many thousands of possible variants\nthat are Pareto optimal.  In this case, more performance and less energy\nconsumption are the good directions, so any point in a set for which\nanother point is both further to the right and further down is not\nshown in the graph.  The black line represents an absolute power\nconsumption of 150 Watts, which is a nominal threshold for a desktop\nenvironment.  Designs above and to the right of that line are not\nas interesting for desktop use.  The { design-space exploration} that\nNeal reported in this figure was of course done by many computers using\nmany hours of computation, but he had to design the process by which the\ncomputers calculated each of the points.\n\n\n\n\n\n \n Question: What is the name of the process in which computers calculate points in a given space? \n Answer: \n\nA. Design-space exploration is a process in which computers calculate points in a given space in order to find possible solutions to a problem."}, {"text": "Title: Sequential Logic \n Text: {Sequential Logic}\n\nThese notes introduce logic components for storing bits, building up\nfrom the idea of a pair of cross-coupled inverters through an\nimplementation of a flip-flop, the storage abstractions used in most \nmodern logic design processes.  We then introduce simple forms of\ntiming diagrams, which we use to illustrate the behavior of\na logic circuit.\n\nAfter commenting on the benefits of using a clocked synchronous \nabstraction when designing systems that store and manipulate bits,\nwe illustrate timing issues and explain how these are abstracted\naway in clocked synchronous designs.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later\nclasses.} \n\n\n\n \n Question: What is the main purpose of introducing logic components? \n Answer: \n\nA. The main purpose of introducing logic components is to store bits and build up from the idea of a pair of cross-coupled inverters."}, {"text": "Title: Storing One Bit \n Text: {Storing One Bit}\n\nSo far we have discussed only implementation of Boolean functions:\ngiven some bits as input, how can we design a logic circuit to calculate\nthe result of applying some function to those bits?  The answer to\nsuch questions is called { combinational logic} (sometimes \n{ combinatorial logic}), a name stemming from the fact that we\nare combining existing bits with logic, not storing new bits.\n\nYou probably already know, however, that combinational logic alone is\nnot sufficient to build a computer.  We need the ability to store bits,\nand to change those bits.  Logic designs that make use of stored \nbits---bits that can be changed, not just wires to high voltage and \nground---are called { sequential logic}.  The name comes from the idea \nthat such a system moves through a sequence of stored bit patterns (the \ncurrent stored bit pattern is called the { state} of the system).\n\n\nConsider the diagram to the right.  What is it?  A {1-input} \nNAND gate, or an inverter drawn badly?  If you think carefully about\nhow these two gates are built, you will realize that they are the\nsame thing.  Conceptually, we use two inverters to store a bit, but\nin most cases we make use of NAND gates to simplify the \nmechanism for changing the stored bit.\n\n\n{file=part2/figs/latch-step-1.eps,width=1.9in}\n\n\n\nTake a look at the design to the right.  Here we have taken two \ninverters (drawn as NAND gates) and coupled each gate's output to\nthe other's input.  What does the circuit do?  Let's make some\nguesses and see where they take us.  Imagine that the value at Q\nis 0.  In that case, the lower gate drives P to 1.  \nBut P drives the upper gate, which forces Q to 0.\nIn other words, this combination forms a\n\n\n{file=part2/figs/latch-step-2.eps,width=1.65in}\n\n\n{|cc}\nQ& P \n0& 1\n1& 0\n\n\n\nstable state of the system:\nonce the gates reach this state, they continue to hold these values.\nThe first row of the truth table to the right (outputs only) shows\nthis state.\n\nWhat if Q=1, though?  In this case, the lower gate forces P to 0,\nand the upper gate in turn forces Q to 1.  Another stable state!\nThe Q=1 state appears as the second row of the truth table.\n\nWe have identified all of the stable states.{Most logic\nfamilies also allow unstable states in which the values alternate rapidly \nbetween 0 and 1.  These metastable states are \nbeyond the scope of our class, but ensuring that they do not occur\nin practice is important for real designs.}  Notice that our cross-coupled\ninverters can store a bit.  Unfortunately, we have no way to \nspecify which value should be stored, nor to change the bit's value\nonce the gates have settled into a stable state.\nWhat can we do?  \n\n\n\n\n\n\nLet's add an input to the upper gate, as shown to the \nright.  We call the input .  The ``S'' stands for set---as\nyou will see, our new input allows us to { set} our stored bit Q to 1.\nThe use of a complemented name for the input indicates that the input\nis { active low}.  In other words, the input performs its intended \ntask (setting Q to 1) when its value is 0 (not 1).\n\n\n{file=part2/figs/latch-step-3.eps,width=2.1in}\n\n\n{c|cc}\n& Q& P \n1& 0& 1\n1& 1& 0\n0& 1& 0\n\n\n\nThink about what happens when the new input is not active, \n=1.  As you know, ANDing any value with 1 produces the \nsame value, \nso our new input has no effect when =1.  The first two\nrows of the truth table are simply a copy of our previous table: the \ncircuit can store either bit value when =1.\nWhat happens when =0?  In that case, the upper gate's\noutput is forced to 1, and thus the lower gate's is forced to 0.\nThis third possibility is reflected in the last row of the truth table.\n\n\nNow we have the ability to force bit Q to have value 1, but if we\nwant Q=0, we just have to hope that the circuit happens to settle into\nthat state when we turn on the power.  What can we do?\n\nAs you probably guessed, we add an input to the other gate, as \nshown to the right.  We call the new input : the input's\npurpose is to { reset} bit Q to 0, and the input\nis active low.  We extend the truth table to include a row \nwith =0 and =1, which forces Q=0 and P=1.\n\n\n{file=part2/figs/latch-step-4.eps,width=2.1in}\n\n\n{cc|cc}\n& & Q& P \n1& 1& 0& 1\n1& 1& 1& 0\n1& 0& 1& 0\n0& 1& 0& 1\n0& 0& 1& 1\n\n\n\n is active low.  We extend the truth table to include a row \n with =0 and =1, which forces Q=0 and P=1.\n\nThe circuit that we have drawn has a name: an \n{ {- latch}}. \nOne can also build {R-S latches} (with active high set and reset\ninputs).  The textbook also shows an\n{- latch} (labeled \nincorrectly).\nCan you figure out how to build an {R-S} latch yourself?\n\nLet's think a little more about\nthe {- latch}. \nWhat happens if we set =0 and =0 at\nthe same time?  Nothing bad happens immediately.  Looking at the design,\nboth gates produce 1, so Q=1 and P=1.  The bad part happens later:\nif we raise both  and  back to 1 at around\nthe same time, the stored bit may end up in either state.{Or,\nworse, in a metastable state, as mentioned earlier.}\n\nWe can avoid the problem by adding gates to prevent the\ntwo control inputs ( and )\nfrom ever being 1 at the same time.  A single inverter\nmight technically suffice, but let's build up the structure shown below,\nnoting that the two inverters in sequence connecting D to \nhave no practical effect at the moment.\n\nA truth table is shown to the right of the logic diagram.\n\nWhen D=0,  is forced to 0, and the bit is reset.\n\nSimilarly, when D=1,  is forced to 0, and the bit is set.\n\n{\n\n{file=part2/figs/latch-step-5.eps,width=3.25in}\n\n\n{c|cccc}\nD& & & Q& P \n0& 0& 1& 0& 1\n1& 1& 0& 1& 0\n\n\n}\n\nUnfortunately, except for some interesting timing characteristics, \nthe new design has the same functionality as a piece of wire.\nAnd, if you ask a circuit designer, thin wires also have some \ninteresting timing characteristics.  What can we do?  Rather than \nhaving Q always reflect the current value of D, let's add\nsome extra inputs to the new NAND gates that allow us to control\nwhen the value of D is copied to Q, as shown on the next page.\n\n{\n\n{file=part2/figs/latch-step-6.eps,width=3.35in}\n\n\n{cc|cccc}\nWE& D& & & Q& P \n1& 0& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0\n0& 0& 1& 1& 0& 1\n0& 1& 1& 1& 0& 1\n0& 0& 1& 1& 1& 0\n0& 1& 1& 1& 1& 0\n\n\n}\n\n\nThe WE (write enable) input controls whether or not Q mirrors\nthe value of D.  The first two rows in the truth table are replicated\nfrom our ``wire'' design: a value of WE=1 has no effect on the first\ntwo NAND gates, and Q=D.  A value of WE=0 forces the\nfirst two NAND gates to output 1, thus =1, =1,\nand the bit Q can\noccupy either of the two possible states, regardless of the value\nof D, as reflected in the lower four lines of the truth table.\n\nThe circuit just shown is called a { gated D latch}, and is an\nimportant mechanism\n\n\n{file=part2/figs/latch-step-7.eps,width=1.1in}\n\n\nfor storing state in sequential \nlogic.  (Random-access memory uses a slightly different \ntechnique to connect the cross-coupled inverters, but latches are\nused for nearly every other application of stored state.)\nThe ``D''\nstands for ``data,'' meaning that the bit stored is matches\nthe value of the input.  Other types of latches (including\n{S-R latches}) have been used historically, but\nD latches are used predominantly today, so we omit discussion of\nother types.\nThe ``gated'' qualifier refers to the presence of an enable input\n(we called it WE) to control\nwhen the latch copies its input into the stored bit.\nA symbol for a gated D latch appears to the right.  Note that we have\ndropped the name P in favor of , since P=\nin a gated D latch. \n\n \n Question: What is the purpose of the input? \n Answer: \n\nA. The input is used to set the bit to 0."}, {"text": "Title: The Clock Abstraction \n Text: {The Clock Abstraction}\n\nHigh-speed logic designs often use latches directly.  Engineers\nspecify the number of latches as well as combinational logic \nfunctions needed to connect one latch to the next,\nand the CAD tools optimize the combinational logic.\nThe enable inputs of successive groups of latches are then driven\nby what we call a clock signal, a single bit line distributed across\nmost of the chip that alternates between 0 and 1 with a regular\nperiod.  While the clock is 0, one set of latches holds its bit values fixed,\nand combinational logic uses those latches as inputs to produce \nbits that are copied into a\nsecond set of latches.  When the clock switches to 1, the second\nset of latches stops storing their data inputs and\nretains their bit values in order to drive other combinational logic,\nthe results of which are copied into a third set of latches.\nOf course, some of the latches in the first and third sets may be the\nsame.\n\nThe timing of signals in such designs plays a critical role in their\ncorrect operation.  Fortunately, we have developed powerful abstractions \nthat allow engineers to ignore much of the complexity while thinking\nabout the Boolean logic needed for a given design.\n\nTowards that end, we make a simplifying assumption for the\nrest of our class, and for most of your career as an undergraduate:\nthe clock signal is a { square wave} delivered uniformly across a\nchip.  For example, if the period of a clock is 0.5 nanoseconds (2 GHz),\nthe clock signal is a 1 for 0.25 nanoseconds, then a 0 for\n0.25 nanoseconds.  We assume that the clock signal changes instantaneously\nand at the same time across the chip.  Such a signal can never exist in\nthe real world: voltages do not change instantaneously, and the \nphrase ``at the same time'' may not even make sense at these scales.\nHowever, circuit designers can usually provide a clock signal that\nis close enough, allowing us to forget for now that no physical signal can\nmeet our abstract definition.\n\n\n SSL altered terminology on 3 Dec 21\n\n\n\n\nThe device shown to the right is a { master-slave} implementation of a \n\nThe device shown to the right is a { dual-latch} implementation of a \n{ positive edge-triggered} D flip-flop.  As you can see, we have \nconstructed it from two gated D latches with opposite senses of write\nenable.  The ``D'' part of the name has the same meaning as with a\ngated D latch: the bit stored is the same as the one delivered\n\n\n{file=part2/figs/latch-step-8.eps,width=2.75in}\n\n\n{file=part2/figs/latch-step-9.eps,width=0.95in}\n\n\nto the\ninput.  Other variants of flip-flops have also been built, but this \ntype dominates designs today.  Most are actually generated automatically\nfrom hardware ``design'' languages (that is, computer programming languages\nfor hardware design).\n\nWhen the clock is low (0), the first latch copies its value from\nthe flip-flop's D input to the midpoint (marked X in our figure, but\nnot usually given a name).  When the clock is high (1), the second\nlatch copies its value from X to the flip-flop's output Q.\nSince X can not change when the clock is high,\nthe result is that the output changes each time the clock changes\nfrom 0 to 1, which is called the { rising edge} or { positive edge}\n(the derivative) of the clock signal.  Hence the qualifier \n``positive edge-triggered,'' which describes the flip-flop's behavior.\n\nThe ``master-slave'' implementation refers to the use of two latches.\n\nThe ``dual-latch'' implementation refers to the use of two \nlatches.{Historically, this implementation was called ``master-slave,''\nbut ECE Illinois has decided to eliminate use of such terminology.}\nlatches.\nIn practice, flip-flops are almost never built this way.  To see a \ncommercial design, look up 74LS74, which uses six {3-input} NAND\ngates and allows set/reset of the flip-flop (using two\nextra inputs).\n\n\n\nThe { timing diagram} to the right illustrates the operation of\nour flip-flop.  In a timing diagram, the horizontal axis represents\n(continuous) increasing time, and the individual lines represent\nvoltages for logic signals.  The relatively simple version shown here\nuses only binary values for each signal.  One can also draw \ntransitions more realistically (as taking finite time).  The dashed\nvertical lines here represent the times at which the clock rises.\nTo make the\n\n\n\n\n\nexample interesting, we have varied D over two clock\ncycles.  Notice that even though D rises and falls during the second\nclock cycle, its value is not copied to the output of our flip-flop.\nOne can build flip-flops that ``catch'' this kind of behavior\n(and change to output 1), but we leave such designs for later in your\ncareer.\n\nCircuits such as latches and flip-flops are called { sequential\nfeedback} circuits, and the process by which they are designed \nis beyond the scope of our course.  The ``feedback'' part of the\nname refers to the fact that the outputs of some gates are fed back \ninto the inputs of others.  Each cycle in a sequential feedback \ncircuit can store one bit.\n\nCircuits that merely use\nlatches and flip-flops as building blocks are called\n{ clocked synchronous sequential circuits}.  Such designs are\nstill sequential: their behavior depends on the bits currently\nstored in the latches and flip-flops.  However, their behavior\nis substantially simplified by the use of a clock signal (the ``clocked''\npart of the name) in a way that all elements change at the same\ntime (``synchronously'').\n\nThe value of using flip-flops and assuming a square-wave\nclock signal with uniform timing may not be clear to you yet,\nbut it bears emphasis.\n\nWith such assumptions, { we can treat time as having \ndiscrete values.}  In other words, time ``ticks'' along discretely,\nlike integers instead of real numbers.\n\nWe can look at the state of the system, calculate the inputs to our\nflip-flops through the combinational logic that drives their D\ninputs, and be confident that, when time moves to the next discrete\nvalue, we will know the new bit values stored in our flip-flops,\nallowing us to repeat the process for the next clock cycle without\nworrying about exactly when things change.  Values change only\non the rising edge of the clock!\n\nReal systems, of course, are not so simple, and we do not have one\nclock to drive the universe, so engineers must also design systems\nthat interact even though each has its own private clock signal\n(usually with different periods).\n\n\n\n \n Question: What is a clocked synchronous sequential circuit? \n Answer: \n\nA.\n\nA sequential feedback circuit is a circuit where the outputs of some gates are fed back into the inputs of others. A clocked synchronous sequential circuit is a circuit that uses a clock signal to control when values change."}, {"text": "Title: Static Hazards: Causes and Cures* \n Text: {Static Hazards: Causes and Cures*}\n\nBefore we forget about the fact that real designs do not provide\nperfect clocks, let's explore some of the issues that engineers\nmust sometimes face.  \n\nWe discuss these primarily to ensure that you appreciate the power\nof the abstraction that we use in the rest of our course.\nIn later classes (probably our 298, which will absorb material \nfrom 385), you may be required to master this material.\n{ For now, we provide it simply for your interest.}\n\nConsider the circuit shown below, for which the output is given by \nthe equation S=AB+. \n\n{{file=part2/figs/lec15-1.eps,width=4in}}\n\nThe timing diagram on the right shows a { glitch} in the output\nwhen the input shifts from ABC=110 to 100, that is, when B falls.\nThe problem lies in the possibility that the upper AND gate, driven \nby B, might go low before the lower AND gate, driven by , goes\nhigh.  In such a case, the OR gate output S falls until the second\nAND gate rises, and the output exhibits a glitch.\n\nA circuit that might exhibit a glitch in an output that functionally\nremains stable at 1 is said to have a { static-1 hazard}.  The\nqualifier ``static'' here refers to the fact that we expect the output\nto remain static, while the ``1'' refers to the expected value of the\noutput.\n\n\n\nThe presence of hazards in circuits can be problematic in certain\ncases.  In domino logic, for example, an output is precharged and kept\nat 1 until the output of a driving circuit pulls it to 0, at which\npoint it stays low (like a domino that has been knocked over).  If the\ndriving circuit contains {static-1} hazards, the output may fall\nin response to a glitch.\n\nSimilarly, hazards can lead to unreliable behavior in sequential\nfeedback circuits.  Consider the addition of a feedback loop to the\ncircuit just discussed, as shown in the figure below.  The output of\nthe circuit is now given by the equation S^*=AB+S,\nwhere S^* denotes the state after S feeds back through the lower\nAND gate.  In the case discussed previously, the transition from\nABC=110 to 100, the glitch in S can break the feedback, leaving\nS low or unstable.  The resulting sequential feedback circuit is\nthus unreliable.  \n\n{{file=part2/figs/lec15-2.eps,width=4in}}\n\n\nEliminating static hazards from {two-level} circuits\nis fairly straightforward.  The Karnaugh map to the right corresponds\nto our original circuit; the solid lines indicate the \nimplicants selected by the AND gates.  A {static-1} hazard is\npresent when two adjacent 1s in the {K-map} are not covered by\na common implicant.  {Static-0} hazards do not occur in\n{two-level} SOP circuits.\n\n\n{file=part2/figs/lec15-3.eps,width=1in}\n\n\nEliminating static hazards requires merely extending the circuit with\nconsensus terms in order to ensure that some AND gate remains high\nthrough every transition between input states with\noutput 1.{Hazard elimination is not in general simple; we\nhave considered only {two-level} circuits.}  In the\n{K-map} shown, the dashed line indicates the necessary\nconsensus term, A.\n\n\n\n \n Question: What is the consensus term for the safety of a circuit? \n Answer: \n\nA. Static-0 hazards."}, {"text": "Title: Dynamic Hazards* \n Text: {Dynamic Hazards*}\n\nConsider an input transition for which we expect to see a change in an\noutput.  Under certain timing conditions, the output may not\ntransition smoothly, but instead bounce between its original value\nand its new value before coming to rest at the new value.  A circuit\nthat might exhibit such behavior is said to contain a { dynamic hazard}.\nThe qualifier ``dynamic'' refers to the expected change in the output.\n\nDynamic hazards appear only in more complex circuits, such as the one\nshown below.  The output of this circuit is defined by the equation Q=B+++BD. \n\n{{file=part2/figs/lec15-4.eps,width=3in}}\n\n\nConsider the transition from the input state ABCD=1111 to 1011,\nin which B falls from 1 to 0.  For simplicity, assume that each\ngate has a delay of 1 time unit.  If B goes low at time T=0, the\ntable shows the progression over time of logic levels at several\nintermediate points in the circuit and at the output Q.  Each gate\nmerely produces the appropriate output based on its inputs in the\nprevious time step.  After one delay, the three gates with B as a\ndirect input change their outputs (to stable, final values).  After\nanother delay, at T=2, the other three gates\nre-{-8pt}\n\n\n{|c|c|c|c|c|c|c|}\nT&f&g&h&i&j&Q \n0& 0& 0& 0& 1& 1& 1\n1& 1& 1& 1& 1& 1& 1\n2& 1& 1& 1& 0& 0& 0\n3& 1& 1& 1& 0& 1& 1\n4& 1& 1& 1& 0& 1& 0\n\n\nspond to the initial changes and flip their outputs.  The resulting\nchanges induce another set of changes at T=3, which in turn causes\nthe output Q to change a final time at T=4.\n\nThe output column in the table illustrates the possible impact of a\ndynamic hazard: rather than a smooth transition from 1 to 0, the\noutput drops to 0, rises back to 1, and finally falls to 0\nagain.  The dynamic hazard in this case can be attributed to the\npresence of a static hazard in the logic that produces intermediate\nvalue j.\n\n\n\n \n Question: What is the output of the circuit at time t? \n Answer: \nThe output Q of the circuit will have a value of 1 at time t because:\n\nA. The output of the gate with input B is 1.\n\nB. The output of the gate with input B is 0.\n\nC. The output of the gate with input B is 1, and the output of the gate with input D is 0.\n\nD. The output of the gate with input B is 0, and the output of the gate with input D is 1."}, {"text": "Title: Essential Hazards* \n Text: {Essential Hazards*}\n\n{ Essential hazards} are inherent to the function of a circuit and\nmay appear in any implementation.  In sequential feedback circuit\ndesign, they must be addressed at a low level to ensure that\nvariations in logic path lengths ({ timing skew}) through a circuit\ndo not expose them.  With clocked synchronous circuits, essential\nhazards are abstracted into a single form: { clock skew}, or\ndisparate clock edge arrival times at a circuit's flip-flops.\n\nAn example demonstrates the possible effects: consider the\nconstruction of a clocked synchronous circuit to recognize {0-1}\nsequences on an input IN.  Output Q should be held high for one\ncycle after recognition, that is, until the next rising clock edge.  A\ndescription of states and a state diagram for such a circuit appear\nbelow.\n\n\n{\n\nS_1S_0 &state& meaning\n00& A& nothing, 1, or 11 seen last\n01& B& 0 seen last\n10& C& 01 recognized (output high)\n11& unused&\n\n}\n\n\n{{file=part2/figs/lec15-5.eps,width=2in}}\n\n\nFor three states, we need two (=_2 3) flip-flops.\nDenote the internal state S_1S_0.  The specific internal\nstate values for each logical state (A, B, and C) simplify the\nimplementation and the example.  A { state table} \nand {K-maps} for the next-state logic appear\nbelow.  The state table uses one line per state with separate \ncolumns\nfor each input combination, making the table more compact than one\nwith one line per state/input combination.  Each column contains the\nfull next-state information, including output.  Using this form of the\nstate table, the {K-maps} can be read directly from the table.\n\n\n{\n\n& {IN}\nS_1S_0& 0& 1 \n00& 01/0& 00/0\n01& 01/0& 10/0\n11& x& x\n10& 01/1& 00/1\n\n}\n\n\n{{file=part2/figs/lec15-6.eps,width=3.5in}}\n\n\nExamining the {K-maps}, we see that the excitation and output\nequations are S_1^+=IN S_0, S_0^+=, and Q=S_1.\nAn implementation of the circuit using two D flip-flops appears below.\nImagine that mistakes in routing or process variations have made the\nclock signal's path to flip-flop 1 much longer than its path into\nflip-flop 0, as illustrated.\n\n{{file=part2/figs/lec15-7.eps,width=3in}}\n\nDue to the long delays, we cannot assume that rising clock edges\narrive at the flip-flops at the same time.  The result is called clock\nskew, and can make the circuit behave improperly by exposing essential\nhazards.  In the logical B to C transition, for example, we begin in\nstate S_1S_0=01 with IN=1 and the clock edge rising.  Assume that\nthe edge reaches flip-flop 0 at time T=0.  After a flip-flop delay\n(T=1), S_0 goes low.  After another AND gate delay (T=2), input\nD_1 goes low, but the second flip-flop has yet to change state!\nFinally, at some later time, the clock edge reaches flip-flop 1.\nHowever, the output S_1 remains at 0, leaving the system in state A\nrather than state C.\n\nFortunately, in clocked synchronous sequential circuits, all essential\nhazards are related to clock skew.  This fact implies that we can\neliminate a significant amount of complexity from circuit design by\ndoing a good job of distributing the clock signal.  It also implies\nthat, as a designer, you should avoid specious addition of logic in a\nclock path, as you may regret such a decision later, as you try to\ndebug the circuit timing.\n\n\n\n \n Question: What is the state of the system when it reaches flip-flop 1? \n Answer: \n\nA. True"}, {"text": "Title: Proof Outline for Clocked Synchronous Design* \n Text: {Proof Outline for Clocked Synchronous Design*}\n\nThis section outlines a proof of the claim made regarding clock\nskew being the only source of essential hazards for clocked\nsynchronous sequential circuits.  A { proof outline} suggests\nthe form that a proof might take and provides some of the logical\narguments, but is not rigorous enough to be considered a proof.\nHere we use a D flip-flop to illustrate a method for identifying\nessential hazards ({ the D flip-flop has no essential hazards, however}),\nthen argue that the method can be applied generally to collections\nof flip-flops in a clocked synchronous design to show that essential\nhazards occur only in the form of clock skew. \n\n\n{\n\n&{1-2}\nlow&L&clock low, last input low\nhigh&H&clock high, last input low\npulse low&PL&clock low, last input high (output high, too)\npulse high&PH&clock high, last input high (output high, too)\n\n}\n\n\n{{file=part2/figs/lec15-8.eps,width=2in}}\n\n\nConsider the sequential feedback state table for a positive\nedge-triggered D flip-flop, shown above.  In designing and analyzing\nsuch circuits, we assume that only one input bit changes at a time.\nThe state table consists of one row for each state and one column for\neach input combination.  Within a row, input combinations that have no\neffect on the internal state of the circuit (that is, those that do \nnot cause any change in\nthe state) are said to be stable; these states are circled.  Other\nstates are unstable, and the circuit changes state in response to\nchanges in the inputs.\n\nFor example, given an initial state L with low output, low clock,\nand high input D, the solid arcs trace the reaction of the circuit to a\nrising clock edge.  From the 01 input combination, we move along the\ncolumn to the 11 column, which indicates the new state, PH.  Moving\ndown the column to that state's row, we see that the new state is\nstable for the input combination 11, and we stop.  If PH were not\nstable, we would continue to move within the column until coming to\nrest on a stable state.\n\nAn essential hazard appears in such a table as a difference between\nthe final state when flipping a bit once and the final state when\nflipping a bit thrice in succession.  The dashed arcs in the figure\nillustrate the concept: after coming to rest in the PH state, we reset\nthe input to 01 and move along the PH row to find a new state of PL.\nMoving up the column, we see that the state is stable.  We then flip\nthe clock a third time and move back along the row to 11, which\nindicates that PH is again the next state.  Moving down the column, we\ncome again to rest in PH, the same state as was reached after one\nflip.  Flipping a bit three times rather than once evaluates the\nimpact of timing skew in the circuit; if a different state is reached\nafter two more flips, timing skew could cause unreliable behavior.  As\nyou can verify from the table, a D flip-flop has no essential hazards.\n\nA group of flip-flops, as might appear in a clocked synchronous\ncircuit, can and usually does have essential hazards, but only dealing\nwith the clock.  As you know, the inputs to a clocked synchronous\nsequential circuit consist of a clock signal and other inputs (either\nexternal of fed back from the flip-flops).  Changing an input other\nthan the clock can change the internal state of a flip-flop (of the\n\n master-slave variety), but flip-flop designs do not capture the number\n\ndual-latch variety), but flip-flop designs do not capture the number\nof input changes in a clock cycle beyond one, and changing an input\nthree times is the same as changing it once.  Changing the clock, of\ncourse, results in a synchronous state machine transition.\n\nThe detection of essential hazards in a clocked synchronous design\nbased on flip-flops thus reduces to examination of the state machine.\nIf the next state of the machine has any dependence on the current\nstate, an essential hazard exists, as a second rising clock edge moves\nthe system into a second new state.  For a single D flip-flop, the\nnext state is independent of the current state, and no essential\nhazards are present.\n\n\n\n\n\n \n Question: What is an essential hazard? \n Answer: \n\nAn essential hazard is a difference between the final state when flipping a bit once and the final state when flipping a bit thrice in succession."}, {"text": "Title: Using Abstraction to Simplify Problems \n Text: {Using Abstraction to Simplify Problems}\n\nIn this set of notes, we illustrate the use of abstraction to simplify\nproblems, then introduce a component called a multiplexer that allows\nselection among multiple inputs.\n\nWe begin by showing how two specific \nexamples---integer subtraction and identification of letters\nin ASCII---can be implemented using logic functions that we have already\ndeveloped.  We also introduce a conceptual technique for\nbreaking functions into smaller pieces, which allows us to solve\nseveral simpler problems and then to compose a full solution from \nthese partial solutions.\n\nTogether with the idea of bit-sliced designs that we introduced earlier,\nthese techniques help to simplify the process of designing logic that\noperates correctly.  The techniques can, of course, lead to \nless efficient designs,\nbut { correctness is always more important than performance}.\n\nThe potential loss of efficiency is often acceptable for three reasons.\n\nFirst, as we mentioned earlier, computer-aided design tools for \noptimizing logic functions are fairly effective, and in many cases\nproduce better results than human engineers (except in the rare cases \nin which the human effort required to beat the tools is worthwhile).\n\nSecond, as you know from the design of the {2's complement} \nrepresentation, we may be able to reuse specific pieces of hardware if\nwe think carefully about how we define our problems and representations.\n\nFinally, many tasks today are executed in software, which is designed\nto leverage the fairly general logic available via an instruction set\narchitecture.  A programmer cannot easily add new logic to a user's\nprocessor.  As a result, the hardware used to execute a\nfunction typically is not optimized for that function.\n\nThe approaches shown in this set of notes illustrate how abstraction\ncan be used to design logic.\n\n \n Question: What is the purpose of abstraction? \n Answer: \n\nA. Abstraction helps to simplify problems by breaking them down into smaller pieces that can be more easily solved."}, {"text": "Title: Subtraction \n Text: \n\nOur discussion of arithmetic implementation has focused so far on \naddition.  What about other operations, such as subtraction, multiplication,\nand division?  The latter two require more work, and we will not\ndiscuss them in detail until later in our class (if at all).\n\nSubtraction, however, can be performed almost trivially using logic that\nwe have already designed.\n\nLet's say that we want to calculate the difference D between \ntwo {N-bit} numbers A and B.  In particular, we want \nto find D=A-B.  For now, think of A, B, and D \nas 2's complement values.\n\nRecall how we defined the 2's complement representation: \nthe {N-bit} pattern that we use to represent -B is the \nsame as the base 2 bit pattern for (2^N-B), so we can use an adder if we\nfirst calculate the bit pattern for -B, then add the resulting\npattern to A.\nAs you know, our {N-bit} adder always produces a result that\nis correct modulo 2^N, so the result of such an operation,\nD=2^N+A-B, is correct so long as the subtraction does not overflow.\n\n\nHow can we calculate 2^N-B?  The same way that we do by hand!\nCalculate the 1's complement, (2^N-1)-B, then add 1.\n\nThe diagram to the right shows how we can use the {N-bit} adder\nthat we designed in Notes Set 2.3 to build an {N-bit} subtracter.\n\nNew elements appear in blue in the figure---the rest of the logic\nis just an adder.  The box labeled ``1's comp.'' calculates \nthe {1's complement} of the value B, which together with the\ncarry in value of 1 correspond to calculating -B.  What's in the\n``1's comp.'' box?  One inverter per bit in B.  That's all \nwe need to calculate the 1's complement.\n\nYou might now ask: does this approach also work for unsigned numbers?\nThe answer is yes, absolutely.  However, the overflow conditions for\nboth 2's complement and unsigned subtraction are different than the\noverflow condition for either type of addition.  What does the carry\nout of our adder signify, for example?  The answer may not be \nimmediately obvious.\n\n\n\n\n\nLet's start with the overflow condition for unsigned subtraction.\n\nOverflow means that we cannot represent the result.  With an {N-bit}\nunsigned number, we have A-B[0,2^N-1].  Obviously, the difference\ncannot be larger than the upper limit, since A is representable and\nwe are subtracting a non-negative (unsigned) value.  We can thus assume\nthat overflow occurs only when A-B<0.  In other words, when A<B.\n\n\n\nTo calculate the unsigned subtraction overflow condition in terms of the\nbits, recall that our adder is calculating 2^N+A-B.  The carry out\nrepresents the 2^N term.  When A, the result of the adder\nis at least 2^N, and we see a carry out, C_=1.  However, when\nA<B, the result of the adder is less than 2^N, and we see no carry\nout, C_=0.  { Overflow for unsigned subtraction is thus inverted \nfrom overflow for unsigned addition}: a carry out of 0 indicates an \noverflow for subtraction.\n\nWhat about overflow for 2's complement subtraction?  We can use arguments\nsimilar to those that we used to reason about overflow of 2's complement\naddition to prove that subtraction of one negative number from a second\nnegative number can never overflow.  Nor can subtraction of a non-negative\nnumber from a second non-negative number overflow.\n\nIf A and B<0, the subtraction overflows iff A-B{2^{N-1}}.\nAgain using similar arguments as before, we can prove that the difference D\nappears to be negative in the case of overflow, so the product\n{A_{N-1}} B_{N-1} D_{N-1} evaluates to 1 when this type of\noverflow occurs (these variables represent the most significant bits of the \ntwo operands and the difference; in the case of 2's complement, they are\nalso the sign bits).\n\nSimilarly, if A<0 and B, we have overflow when A-B<-2^{N-1}.\nHere we can prove that D on overflow, so \nA_{N-1} {B_{N-1}} {D_{N-1}} evaluates to 1.\n\nOur overflow condition for {N-bit} 2's complement subtraction\nis thus given by the following:\n\n{eqnarray*}\n{A_{N-1}} B_{N-1} D_{N-1}+A_{N-1} {B_{N-1}} {D_{N-1}}\n{eqnarray*}\n\nIf we calculate all four overflow conditions---unsigned and 2's complement,\naddition and subtraction---and provide some way to choose whether or not to \ncomplement B and to control the C_ input, we can use the same hardware\nfor addition and subtraction of either type.\n\n \n Question: What does an overflow for unsigned subtraction indicate? \n Answer: \n\nA. A carry out of 1 for unsigned subtraction indicates an overflow."}, {"text": "Title: Checking ASCII for Upper-case Letters \n Text: {Checking ASCII for Upper-case Letters}\n\nLet's now consider how we can check whether or not an ASCII character is\nan upper-case letter.  Let's call the {7-bit}\nletter C=C_6C_5C_4C_3C_2C_1C_0 and the function that we want to\ncalculate U(C).  The function U should equal 1 whenever C represents\nan upper-case letter, and should equal 0 whenever C does not.\n\nIn ASCII, the {7-bit} patterns from 0x41 through 0x5A correspond\nto the letters A through Z in alphabetic order.  Perhaps you want to draw \na {7-input} {K-map}?  Get a few large sheets of paper!\n\nInstead, imagine that we've written the full {128-row} truth\ntable.  Let's break the truth table into pieces.  Each piece will\ncorrespond to one specific pattern of the three high bits C_6C_5C_4,\nand each piece will have 16 entries for the four low bits C_3C_2C_1C_0.\nThe truth tables for high bits 000, 001, 010, 011, 110, and 111\nare easy: the function is exactly 0.  The other two truth tables appear \non the left below.  We've called the two functions T_4 and T_5, where\nthe subscripts correspond to the binary value of the three high bits of C.\n\n{\n\n{cccc|cc}\nC_3& C_2& C_1& C_0& T_4& T_5 \n0& 0& 0& 0& 0& 1\n0& 0& 0& 1& 1& 1\n0& 0& 1& 0& 1& 1\n0& 0& 1& 1& 1& 1 \n0& 1& 0& 0& 1& 1\n0& 1& 0& 1& 1& 1\n0& 1& 1& 0& 1& 1\n0& 1& 1& 1& 1& 1 \n1& 0& 0& 0& 1& 1\n1& 0& 0& 1& 1& 1\n1& 0& 1& 0& 1& 1\n1& 0& 1& 1& 1& 0 \n1& 1& 0& 0& 1& 0\n1& 1& 0& 1& 1& 0\n1& 1& 1& 0& 1& 0\n1& 1& 1& 1& 1& 0\n\n\n\n{file=part2/figs/ascii-t4.eps,width=1.15in}\n{file=part2/figs/ascii-t5.eps,width=1.15in}\n\n\n{eqnarray*}\nT_4&=&C_3+C_2+C_1+C_0      \nT_5&=&{C_3}+{C_2} {C_1}+{C_2} {C_0}\n{eqnarray*}\n\n\nAs shown to the right of the truth tables, we can then draw simpler\n{K-maps} for T_4 and T_5, and can solve the {K-maps}\nto find equations for each, as shown to the right (check that you get\nthe same answers).\n\nHow do we merge these results to form our final expression for U?\n\nWe AND each of the term functions (T_4 and T_5) with the \nappropriate minterm for the\nhigh bits of C, then OR the results together, as shown here:\n\n{eqnarray*}\nU&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5\n\n&=&C_6 {C_5} {C_4} (C_3+C_2+C_1+C_0)+\nC_6 {C_5} C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0})\n{eqnarray*}\n\nRather than trying to optimize by hand, we can at this point let the CAD\ntools take over, confident that we have the right function to identify\nan upper-case ASCII letter.\n\nBreaking the truth table into pieces and using simple logic to reconnect\nthe pieces is one way to make use of abstraction when solving complex\nlogic problems.  \n\nIn fact, recruiters for some companies often ask\nquestions that involve using specific logic elements as building blocks\nto implement other functions.  Knowing that you can implement\na truth table one piece at a time will help you to solve this type of\nproblem.\n\nLet's think about other ways to tackle the problem of calculating U.\nIn Notes Sets 2.3 and 2.4, we developed adders and comparators.  Can\nwe make use of these components as building blocks to check whether C \nrepresents an\nupper-case letter?  Yes, of course we can: by comparing C with the\nends of the range of upper-case letters, we can check whether or not C\nfalls in that range.\n\nThe idea is illustrated on the left below using two {7-bit} comparators\nconstructed as discussed in Notes Set 2.4.\nThe comparators are the black parts of the drawing, while the blue parts\nrepresent our extensions to calculate U.  Each comparator is given\nthe value C as one input.  The second value to the comparators is \neither the letter A (0x41) or the letter Z (0x5A).  The meaning of \nthe {2-bit} input to and result of each comparator is given in the \ntable on the right below.  The inputs on the right of each comparator\nare set to 0 to ensure that equality is produced if C matches\nthe second input (B).\n\nOne output from each comparator is then routed to\na NOR gate to calculate U.  Let's consider how this combination works.\nThe left comparator compares C with the letter A (0x41).  If C 0x41,\nthe comparator produces Z_0=0.  In this case, we may have a letter.\nOn the other hand, if C< 0x41, the comparator produces Z_0=1, and\nthe NOR gate outputs U=0, since we do not have a letter in this case.\n\nThe right comparator compares C with the letter Z (0x5A).  If C 0x5A,\nthe comparator produces Z_1=0.  In this case, we may have a letter.\nOn the other hand, if C> 0x5A, the comparator produces Z_1=1, and\nthe NOR gate outputs U=0, since we do not have a letter in this case.\n\nOnly when 0x41  0x5A does U=1, as desired. \n\n\n\n{\n\n{file=part2/figs/ascii-cmp-based.eps,width=3.6in}\n\n\n{cc|l}\nZ_1& Z_0& meaning \n0& 0& A=B\n0& 1& A<B\n1& 0& A>B\n1& 1& not used\n\n\n\n\n\n\n\n\n\nWhat if we have only {8-bit} adders available for our use, such as\nthose developed in Notes Set 2.3?  Can we still calculate U?  Yes.\nThe diagram shown to the right illustrates the approach, again with black\nfor the adders and blue for our extensions.  Here we are \nactually using the adders as subtracters, but calculating the 1's complements\nof the constant values by hand.  The ``zero extend'' box simply adds\na leading 0 to our {7-bit} ASCII letter.  The left adder \nsubtracts the letter A from C: if no carry is produced, we know that\nC< 0x41 and thus C does not represent an upper-case letter, and U=0.\n\nSimilarly, the right adder subtracts 0x5B (the letter Z plus one)\nfrom C.  If a carry is produced, we know that C 0x5B, and \nthus C does not represent an upper-case letter, and U=0.\n\nWith the right combination of carries (1 from the left and 0 from the\nright), we obtain U=1.\n\n\n{file=part2/figs/ascii-add-based.eps,width=2.75in}\n\n\nLooking carefully at this solution, however, you might be struck by the\nfact that we are calculating two sums and then discarding them.  Surely\nsuch an approach is inefficient?\n\nWe offer two answers.  First, given the design shown above, a good CAD tool\nrecognizes that the sum outputs of the adders are not being used,\nand does not generate logic to calculate them.  The logic for the two \ncarry bits used to calculate U can then be optimized.  Second, the design\nshown, including the calculation of the sums, is similar in efficiency\nto what happens at the rate of about 10^ times per second, \n24 hours a day, seven days a week,\ninside processors in data centers processing HTML, XML, and other types\nof human-readable Internet traffic.  Abstraction is a powerful tool.\n\nLater in our class, you will learn how to control logical connections\nbetween hardware blocks so that you can make use of the same hardware\nfor adding, subtracting, checking for upper-case letters, and so forth.\n\n \n Question: What is the value of an ASCII character compared to the letters A through Z? \n Answer: \n\nA. U represents the value of an ASCII character compared to the letters A through Z. If the character is equal to or greater than A and less than or equal to Z, then U will equal 1. If the character is not in that range, then U will equal 0."}, {"text": "Title: Checking ASCII for Lower-case Letters \n Text: {Checking ASCII for Lower-case Letters}\n\nHaving developed several approaches for checking for an upper-case letter,\nthe task of checking for a lower-case letter should be straightforward.\nIn ASCII, lower-case letters are represented by the {7-bit} patterns \nfrom 0x61 through 0x7A.\n\nOne can now easily see how more abstract designs make solving similar\ntasks easier.  If we have designed our upper-case checker \nwith {7-variable} K-maps,\nwe must start again with new K-maps for the lower-case checker.  If\ninstead we have taken the approach of designing logic for the upper and \nlower bits of the ASCII character, we can reuse most of that logic,\nsince the functions T_4 and T_5 are identical when checking for\na lower-case character.  Recalling the algebraic form of U(C), we can\nthen write a function L(C) (a lower-case checker) as shown on the\nleft below.\n\n\n{eqnarray*}\nU&=&C_6 {C_5} {C_4} T_4+C_6 {C_5} C_4 T_5\n\nL&=&C_6 C_5 {C_4} T_4+C_6 C_5 C_4 T_5\n\n&=&C_6 C_5 {C_4} (C_3+C_2+C_1+C_0)+\n&&C_6 C_5 C_4 ({C_3}+{C_2} {C_1}+{C_2} {C_0})\n{eqnarray*}\n\n\n{file=part2/figs/ascii-cmp-lower.eps,width=3.6in}\n\n\nFinally, if we have used a design based on comparators or adders, the\ndesign of a lower-case checker becomes trivial: simply change the numbers\nthat we input to these components, as shown in the figure on the right above\nfor the comparator-based design.  The only changes from the upper-case checker\ndesign are the inputs to the comparators and the output produced, \nhighlighted with blue text in the figure.\n\n\n\n \n Question: What is the advantage of using a design based on comparators or adders? \n Answer: \n\nA. The advantage of using a design based on comparators or adders when checking for a lower-case letter is that the design becomes trivial: simply change the numbers that we input to these components."}, {"text": "Title: The Multiplexer \n Text: {The Multiplexer}\n\nUsing the more abstract designs for checking ranges of ASCII characters,\nwe can go a step further and create a checker for both upper- and lower-case\nletters.  To do so, we add another input S that allows us to select the \nfunction that we want---either the upper-case checker U(C) or the \nlower-case checker L(C).\n\nFor this purpose, we make use of a logic block called a { multiplexer},\nor { mux}.\nMultiplexers are an important abstraction for digital logic.  In \ngeneral, a multiplexer allows us to use one digital signal to \nselect which of several others is forwarded to an output.\n\n\nThe simplest form of the multiplexer is the 2-to-1 multiplexer shown to \nthe right.   The logic diagram illustrates how the mux works.  The block \nhas two inputs from the left and one from the top.  The top input allows \nus to choose which of the left inputs is forwarded to the output.  \nWhen the input S=0, the upper AND gate outputs 0, and the lower AND gate\noutputs the value of D_0.  The OR gate then produces Q=0+D_0=D_0.\nSimilarly, when input S=1, the upper AND gate outputs D_1, and the\nlower AND gate outputs 0.  In this case, the OR gate produces Q=D_1+0=D_1.\n\n\n\n\n\nThe symbolic form of the mux is a trapezoid with data inputs on the \nlarger side, an output on the smaller side, and a select input on the\nangled part of the trapezoid.  The labels inside the trapezoid indicate \nthe value of the select input S for which the adjacent data signal, \nD_1 or D_0, is copied to the output Q.\n\nWe can generalize multiplexers in two ways.  First, we can extend the \nsingle select input to a group of select inputs.  An {N-bit}\nselect input allows selection from amongst 2^N inputs.  A {4-to-1} \nmultiplexer is shown below, for example.  The logic diagram on the left\nshows how the {4-to-1 mux} operates.  For any combination of S_1S_0,\nthree of the AND gates produce 0, and the fourth outputs the D input\ncorresponding to the interpretation of S as an unsigned number.\nGiven three zeroes and one D input, the OR gate thus reproduces one of \nthe D's.  When S_1S_0=10, for example, the third AND gate copies D_2,\nand Q=D_2.\n\n{{file=part2/figs/mux4-to-1.eps,width=5.60in}}\n\nAs shown in the middle figure, a {4-to-1} mux can also be built from\nthree {2-to-1} muxes.  Finally, the symbolic form of a {4-to-1} mux \nappears on the right in the figure.\n\n\n\nThe second way in which we can generalize multiplexers is by using\nseveral multiplexers of the same type and using the same signals for \nselection.  For example, we might use a single select bit T to choose \nbetween any number of paired inputs.  Denote input pair by i D_1^i \nand D_0^i.  For each pair, we have an output Q_i.  \n\nWhen T=0, Q_i=D_0^i for each value of i.\n\nAnd, when T=1, Q_i=D_1^i for each value of i.\n\nEach value of i requires a {2-to-1} mux with its select input\ndriven by the global select signal T.\n\nReturning to the example of the upper- and lower-case checker, we\ncan make use of two groups of seven {2-to-1} muxes, all controlled by\na single bit select signal S, to choose between the inputs needed\nfor an upper-case checker and those needed for a lower-case checker.\n\nSpecific configurations of multiplexers are often referred to\nas { N-to-M multiplexers}.  Here the value N refers to the\nnumber of inputs, and M refers to the number of outputs.  The\nnumber of select bits can then be calculated as _2(N/M)---N/M \nis generally a power of two---and one way to build such a \nmultiplexer is to use M copies of\nan  (N/M)-to-1 multiplexer.\n\nLet's extend our upper- and lower-case checker to check\nfor four different ranges of ASCII characters, as shown below.\nThis design uses two {28-to-7} muxes to create a single\nchecker for the four ranges.  Each of the muxes in the figure logically \nrepresents seven {4-to-1} muxes.\n\n{{file=part2/figs/ascii-four-range.eps,width=3.75in}}\n\n\nThe table to the right describes the behavior of the checker.\n\nWhen the select input S is set to 00, the left mux selects the value 0x00,\nand the right mux selects the value 0x1F, which checks whether the ASCII\ncharacter represented by C is a control character.\n\nWhen the select input S=01, the muxes produce the values needed to check\nwhether C is an upper-case letter.\n\nSimilarly, when the select input S=10,\n\n\n{c|c|c|c}\n& left& right& \n& comparator& comparator& \nS_1S_0& input& input& R(C) produced  \n00& 0x00& 0x1F& control character?\n01& 0x41& 0x5A& upper-case letter?\n10& 0x61& 0x7A& lower-case letter?\n11& 0x30& 0x39& numeric digit? \n\n\n\nthe muxes produce the values \nneeded to check whether C is a lower-case letter.\n\nFinally, when the select input S=11, the left mux selects the value 0x30,\nand the right mux selects the value 0x39, which checks whether the ASCII\ncharacter represented by C is a digit (0 to 9).\n\n\n\n\n \n Question: Is the multiplexer a basic logic gate? \n Answer: \n\nA. No, the multiplexer is not a basic logic gate."}, {"text": "Title: Example: Bit-Sliced Comparison \n Text: {Example: Bit-Sliced Comparison}\n\nThis set of notes develops comparators for unsigned and 2's complement \nnumbers using the bit-sliced approach that we introduced in Notes Set 2.3.  \nWe then use algebraic manipulation and variation of the internal \nrepresentation to illustrate design tradeoffs.\n\n\n \n Question: What is the difference between unsigned and 2's complement numbers? \n Answer: \n\nA.\n\nThe difference between unsigned and 2's complement numbers is that 2's complement numbers can represent both positive and negative numbers, while unsigned numbers can only represent positive numbers."}, {"text": "Title: Comparing Two Numbers \n Text: {Comparing Two Numbers}\n\n\nLet's begin by thinking about how we as humans compare two {N-bit}\nnumbers, A and B.\n\nAn illustration appears to the right, using N=8.\n\nFor now, let's assume that our numbers are stored in an unsigned \nrepresentation, so we can just think of them as binary numbers\nwith leading 0s.\n\nWe handle 2's complement values later in these notes.\n\nAs humans, we typically start comparing at the most significant bit.\nAfter all, if we find a difference in that bit, we are done, saving\nourselves some time.  In the example to the right, we know that A<B \nas soon as we reach bit 4 and observe that A_4<B_4.\n\nIf we instead start from the least significant bit,\nwe must always look at all of the bits.\n\nWhen building hardware to compare all of the bits at once, however,\nhardware for comparing each bit must exist, and the final result \nmust be able to consider\n\n\n\n\n\nall of the bits.  Our choice of direction should thus instead depend on \nhow effectively we can build the \ncorresponding functions.  For a single bit slice, the two directions \nare almost identical.  Let's develop a bit slice for\ncomparing from least to most significant.\n\n{ NOTE TO SELF: We should either do the bit-by-bit comparator \nstate machine in the notes or as an FSM homework problem.  Probably\nthe lattter.}\n\n \n Question: What is the most significant bit of a number? \n Answer: \n\nA. If a difference is found in the most significant bit, the comparison can be stopped, saving time."}, {"text": "Title: An Abstract Model \n Text: {An Abstract Model}\n\nComparison of two numbers, A and B, can produce three possible\nanswers: A<B, A=B, or A>B (one can also build an equality\ncomparator that combines the A<B and A>B cases into a single \nanswer).\n\nAs we move from bit to bit in our design, how much information needs \nto pass from one bit to the next?\n\nHere you may want to think about how you perform the task yourself.\nAnd perhaps to focus on the calculation for the most significant bit.\nYou need to know the values of the two bits that you are comparing.\nIf those two are not equal, you are done.\n\nBut if the two bits are equal, what do you do?\n\nThe answer is fairly simple: pass along the result\nfrom the less significant bits.\n\nThus our bit slice logic for bit M needs to be able to accept \nthree possible answers from the bit slice logic for bit M-1 and must\nbe able to pass one of three possible answers to the logic for bit M+1.\n\nSince _2(3)=2, we need two bits of input and two bits\nof output in addition to our input bits from numbers A and B.\n\n\nThe diagram to the right shows an abstract model of our \ncomparator bit slice.  The inputs from the next least significant\nbit come in from the right.  We include arrowheads because \nfigures are usually drawn with inputs coming from the top or left \nand outputs going to the bottom or right.  Outside of the bit \nslice logic, we index\nthese comparison bits using the bit number.  The bit slice\nhas C_1^{M-1} and C_0^{M-1} provided as inputs and \nproduces C_1^M and C_0^M as outputs.\n\nInternally, we use C_1 and C_0 to denote these inputs,\nand Z_1 and Z_0 to denote the outputs.\n\nSimilarly, the\n\n\n\n\n\nbits A_M and B_M from the numbers A and B are\nrepresented internally simply as A and B.  The overloading of\nmeaning should not confuse you, since the context (designing the\nlogic block or thinking about the problem as a whole) should always\nbe clear.\n\n\n\n\n \n Question: What is the bit slice logic for bit M? \n Answer: \n\nA. The bit slice logic needs to be able to accept three possible answers from the bit slice logic for bit M-1 and must be able to pass one of three possible answers to the logic for bit M+1."}, {"text": "Title: A Representation and the First Bit \n Text: {A Representation and the First Bit}\n\n\nWe need to select a representation for our three possible answers before\nwe can design any logic.  The representation chosen affects the\nimplementation, as we discuss later in these notes.  For now, we simply\nchoose the representation to the right, which seems reasonable.\n\nNow we can design the logic for the first bit (bit 0).  In keeping with\nthe bit slice philosophy, in practice we simply use another\ncopy of the full bit slice design for bit 0 and attach the C_1C_0 \ninputs to ground (to denote A=B).  Here we tackle the simpler problem\nas a warm-up exercise.\n\n\n{cc|l}\nC_1& C_0& meaning \n0& 0& A=B\n0& 1& A<B\n1& 0& A>B\n1& 1& not used\n\n\n\n\nThe truth table for bit 0 appears to the right (recall that we use Z_1\nand Z_0 for the output names).  Note that the bit 0 function has\nonly two meaningful inputs---there is no bit to the right of bit 0.\n\nIf the two inputs A and B are the same, we output equality.\nOtherwise, we do a {1-bit} comparison and use our representation mapping\nto select the outputs.\n\nThese functions are fairly straightforward to derive by inspection.\nThey are:{-12pt}\n\n{eqnarray*}\nZ_1 &=& A \nZ_0 &=&  B\n{eqnarray*}\n\n\n{\n{cc|cc}\nA& B& Z_1& Z_0 \n0& 0& 0& 0\n0& 1& 0& 1\n1& 0& 1& 0\n1& 1& 0& 0\n\n}\n\n\nThese forms should also be intuitive, given the representation\nthat we chose: \nA>B if and only if A=1 and B=0;\nA<B if and only if A=0 and B=1. \n\n\nImplementation diagrams for our one-bit functions appear to the right.\nThe diagram to the immediate right shows the implementation as we might\ninitially draw it, and the diagram on the far right shows the \nimplementation\n\n\n\n\n\n\n\n\nconverted to NAND/NOR gates for a more accurate estimate of complexity\nwhen implemented in CMOS.\n\nThe exercise of designing the logic for bit 0 is also useful in the sense\nthat the logic structure illustrated forms the core of the full design\nin that it identifies the two cases that matter: A<B and A>B.\n\nNow we are ready to design the full function.  Let's start by writing\na full truth table, as shown on the left below.\n\n[t]\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0\n0& 0& 1& 1& x& x \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1\n0& 1& 1& 1& x& x \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0\n1& 0& 1& 1& x& x \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0\n1& 1& 1& 1& x& x\n\n\n\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0 \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1 \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0 \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0 \nx& x& 1& 1& x& x\n  \n\n\n\n{cccc|cc}\nA& B& C_1& C_0& Z_1& Z_0 \n0& 0& 0& 0& 0& 0\n0& 0& 0& 1& 0& 1\n0& 0& 1& 0& 1& 0 \n0& 1& 0& 0& 0& 1\n0& 1& 0& 1& 0& 1\n0& 1& 1& 0& 0& 1 \n1& 0& 0& 0& 1& 0\n1& 0& 0& 1& 1& 0\n1& 0& 1& 0& 1& 0 \n1& 1& 0& 0& 0& 0\n1& 1& 0& 1& 0& 1\n1& 1& 1& 0& 1& 0 \n{c|}& x& x\n  \n\n\n\nIn the truth table, we \nmarked the outputs as ``don't care'' (x's) whenever C_1C_0=11.\nYou might recall that we ran into problems with our ice cream dispenser\ncontrol in Notes Set 2.2.  However, in that case we could not safely\nassume that a user did not push multiple buttons.  Here, our bit\nslice logic only accepts inputs from other copies of itself (or a fixed\nvalue for bit 0), and---assuming that we design the logic\ncorrectly---our bit slice never generates the 11 combination.\nIn other words, that input combination is impossible (rather than\nundesirable or unlikely), so the result produced on the outputs is \nirrelevant.\n\nIt is tempting to shorten the full truth table by replacing groups of rows.\nFor example, if AB=01, we know that A<B, so the less significant\nbits (for which the result is represented by the C_1C_0 inputs)\ndon't matter.  We could write one row with input pattern ABC_1C_0=01xx and\noutput pattern Z_1Z_0=01.  We might also collapse our ``don't care'' output\npatterns: whenever the input matches ABC_1C_0=xx11, we don't care\nabout the output, so Z_1Z_0=xx.  But these two rows overlap in the\ninput space!  In other words, some input patterns, such as ABC_1C_0=0111,\nmatch both of our suggested new rows.  Which output should take precedence?\nThe answer is that a reader should not have to guess.  { Do not use \noverlapping rows to shorten a truth table.}  In fact, the first of the \nsuggested new rows is not valid: we don't need to produce output 01 \nif we see C_1C_0=11.  Two valid short forms of this truth table appear to \nthe right of the full table.  If you have an ``other'' entry, as\nshown in the rightmost table, this entry should always appear as the \nlast row.  Normal rows, including rows representing multiple input\npatterns, are not required\nto be in any particular order.  Use whatever order makes the table\neasiest to read for its purpose (usually by treating the input pattern\nas a binary number and ordering rows in increasing numeric order).\n\n\nIn order to translate our design into algebra, we transcribe the truth\ntable into a {K-map} for each output variable, as shown to the right.\nYou may want to perform this exercise yourself and check that you \nobtain the same solution.  Implicants for each output are marked\nin the {K-maps}, giving the following equations:\n\n{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\nZ_0 &=&  B +  C_0 + B C_0 \n{eqnarray*}\n\n\n\n\n\n\n\n\n\nAn implementation based on our equations appears to the right.  \nThe figure makes it easy to see the symmetry between the inputs, which\narises from the representation that we've chosen.  Since the design\nonly uses two-level logic (not counting the inverters on the A\nand B inputs, since inverters can be viewed as {1-input} \nNAND or NOR gates), converting to NAND/NOR simply requires replacing\nall of the AND and OR gates with NAND gates.\n\nLet's discuss the design's efficiency roughly in terms of area and speed.\nAs an estimate of area, we can count gates, remembering that we need\ntwo transistors per input on a gate.\n\nOur initial design uses two inverters, six {2-input} gates, and \ntwo {3-input} gates.\n\nFor speed, we make rough estimates in terms of the amount of time\nit takes for a CMOS gate to change its output once its input has changed.\nThis amount of time is called a { gate delay}.  \n\nWe can thus estimate our design's\n\n\n{file=part2/figs/comparator-try-one.eps,width=2.8in}\n\n\nspeed by simply counting the maximum number of gates on any path\nfrom input to output.  For this measurement, using a NAND/NOR \nrepresentation of the design is important to getting the right answer,\nbut, as we have discussed, the diagram above is equivalent on a \ngate-for-gate basis.  Here we have three gate delays from the A \nand B inputs to the outputs (through the inverters).\n\nBut when we connect multiple copies of our bit slice logic together to \nform a comparator, as shown on the next page, the delay from \nthe A and B inputs\nto the outputs is not as important as the delay from the C_1 and C_0 \ninputs to the outputs.  The latter delay adds to the total delay of\nour comparator on a per-bit-slice basis.  Looking again at the diagram, \nnotice that we have only two gate delays from the C_1 and C_0 inputs \nto the\noutputs.  The total delay for an {N-bit} comparator based on this\nimplementation is thus three gate delays for bit 0 and two more gate\ndelays per additional bit, for a total of 2N+1 gate delays.\n\n\n\n\n \n Question: What is the logic for bit 0? \n Answer: \nQ. }\nQ. \nQ. ]\n\n\n\n[{The One-Bit Comparator in More Detail}\n\n\nBefore moving to the full design, let's briefly discuss the design\nof the bit slice logic more carefully, starting with the one-bit\ncomparator.\n\nOur truth table for bit 0 included four empty rows (indicated by ``x'').\nThese rows are not needed since we know the values of the C_1C_0 inputs\nfor those rows.  Thus, we can remove those rows from the truth table,\ngiving the truth table shown to the right.\n\n{file=part2/figs/ice-cream-try-one.eps,width=3.3in}\n\nThe truth table and the {K-maps} (shown to the right below)\nindicate that our design should be fairly simple.  In fact, we can\nderive the following {implicants} for each output variable:\n\n{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\nZ_0 &=&  B +  C_0 + B C_0 "}, {"text": "Title: Optimizing Our Design \n Text: {Optimizing Our Design}\n\nWe have a fairly good design at this point---good enough for a homework\nor exam problem in this class, certainly---but let's consider how we\nmight further optimize it.  Today, optimization of logic at this level is \ndone mostly by computer-aided design (CAD) tools, but we want you to \nbe aware of the sources of optimization potential and the tradeoffs \ninvolved.  And, if the topic interests you, someone has to continue to \nimprove CAD software!\n\nThe first step is to manipulate our algebra to expose common terms that\noccur due to the design's symmetry.  Starting with our original equation\nfor Z_1, we have\n\n{eqnarray*}\nZ_1 &=& A  + A C_1 +  C_1\n    &=& A  + ( A +  )  C_1\n    &=& A  +  B} C_1\n     Z_0 &=&  B + {A  C_0\n{eqnarray*}{-12pt}\n\nNotice that the second term in each equation now includes the complement of \nfirst term from the other equation.  For example, the Z_1 equation includes\nthe complement of the B product that we need to compute Z_0.  \nWe may be able to improve our design by combining these computations.\n\n\nAn implementation based on our new algebraic formulation appears to the \nright.  In this form, we seem to have kept the same number of gates,\nalthough we have replaced the {3-input} gates with inverters.\nHowever, the middle inverters disappear when we convert to NAND/NOR form,\nas shown below to the right.  Our new design requires only two inverters\nand six {2-input} gates, a substantial reduction relative to the\noriginal implementation.\n\nIs there a disadvantage?  Yes, but only a slight one.  Notice that the\npath from the A and B inputs to the outputs is now four gates (maximum)\ninstead of three.  Yet the path from C_1 and C_0 to the outputs is\nstill only two gates.  Thus, overall, we have merely increased our\n{N-bit} comparator's delay from 2N+1 gate delays to\n2N+2 gate delays.\n\n\n{file=part2/figs/comparator-opt.eps,width=4.1in}\n\n{file=part2/figs/comparator-opt-nn.eps,width=4.1in}\n\n\n\n\n \n Question: What is the advantage of a shorter path from the inputs to the outputs? \n Answer: \n\nA. A shorter path from the A and B inputs to the outputs results in a faster overall comparator."}, {"text": "Title: Extending to 2's Complement \n Text: {Extending to 2's Complement}\n\nWhat about comparing 2's complement numbers?  Can we make use of the\nunsigned comparator that we just designed?\n\nLet's start by thinking about the sign of the numbers A and B.\nRecall that 2's complement records a number's sign in the most \nsignificant bit.  For example, in the {8-bit} numbers shown in the \nfirst diagram in this set of notes, the sign \nbits are A_7 and B_7.\n\nLet's denote these sign bits in the general case by A_s and B_s.\n\nNegative numbers have a sign bit equal to 1, and non-negative numbers\nhave a sign bit equal to 0.\n\nThe table below outlines an initial evaluation of the four possible\ncombinations of sign bits.\n\n{cc|c|l}\nA_s& B_s& interpretation& solution \n0& 0& A AND B& use unsigned comparator on remaining bits\n0& 1& A AND B<0& A>B\n1& 0& A<0 AND B& A<B\n1& 1& A<0 AND B<0& unknown\n\n\n\nWhat should we do when both numbers are negative?  Need we design \na completely separate logic circuit?  Can we somehow convert a \nnegative value to a positive one?\n\nThe answer is in fact much simpler.  Recall that {2's complement}\nis defined based on modular arithmetic.  Given an {N-bit} negative\nnumber A, the representation for the bits A[N-2:0] is the same\nas the binary (unsigned) representation of A+2^{N-1}.  An example\nappears to the right.\n\n\n{file=part2/figs/comparing-2s.eps,width=2.55in}\n\n\nLet's define A_r=A+2^{N-1} as the value of the \nremaining bits for A and B_r similarly for B.\n\nWhat happens if we just go ahead and compare A_r and B_r using \nan {(N-1)-bit} unsigned comparator?\n\nIf we find that A_r<B_r we know that A_r-2^{N-1}<B_r-2^{N-1} as well, \nbut that means A<B!  We can do the same with either of the other\npossible results.  In other words, simply comparing A_r with B_r\ngives the correct answer for two negative numbers as well.\n\n\nAll we need to design is a logic block for the sign bits.  At this point,\nwe might write out a {K-map}, but instead let's rewrite our\nhigh-level table with the new information, as shown to the right.\n\nLooking at the table, notice the similarity \nto the high-level design for a single bit of an unsigned value.  The\n\n\n{cc|l}\nA_s& B_s& solution \n0& 0& pass result from less significant bits\n0& 1& A>B\n1& 0& A<B\n1& 1& pass result from less significant bits\n\n\n\nonly difference is that the two A=B cases are reversed.  If we\nswap A_s and B_s, the function is identical.  We can simply use\nanother bit slice but swap these two inputs.\n\nImplementation of an {N-bit} 2's complement comparator based\non our bit slice comparator is shown below.  The blue circle highlights\nthe only change from the {N-bit} unsigned comparator, which is\nto swap the two inputs on the sign bit.\n\n{{file=part2/figs/integrated-2s.eps,width=5.5in}}\n\n\n\n \n Question: What is the only difference between the two cases of A and B? \n Answer: \n\nA. The only difference is that the two A=B cases are reversed."}, {"text": "Title: Further Optimization \n Text: {Further Optimization}\n\n\nLet's return to the topic of optimization.  To what extent \ndid the representation of the three outcomes affect our ability\nto develop a good bit slice design?  Although selecting a good \nrepresentation can be quite important, for this particular problem\nmost representations lead to similar implementations.\n\nSome representations, however, have interesting properties.  Consider\n\n\n{cc|l|l}\nC_1& C_0& original& alternate \n0& 0& A=B& A=B\n0& 1& A<B& A>B\n1& 0& A>B& not used\n1& 1& not used& A<B\n\n\n\nthe alternate representation on the right, for example (a copy of the \noriginal representation is included for comparison).  Notice that \nin the alternate representation, C_0=1 whenever A=B.  \n\nOnce we have found the numbers to be different in some bit, the end\nresult can never be equality, so perhaps with the right \nrepresentation---the new one, for example---we might be able to\ncut delay in half?\n\n\nAn implementation based on the alternate representation appears in the\ndiagram to the right.  As you can see, in terms of gate count,\nthis design replaces one {2-input} gate with an inverter and\na second {2-input} gate with a {3-input} gate.  The path\nlengths are the same, requiring 2N+2 gate delays for \nan {N-bit} comparator.\nOverall, it is about the same as our original design.\n\n\n{file=part2/figs/comparator-opt-alt.eps,width=4.1in}\n\n\nWhy didn't it work?  Should we consider still other representations?\nIn fact, none of the possible representations that we might choose\nfor a bit slice can cut the delay down to one gate delay per bit.  \nThe problem is fundamental, and is related to the nature of CMOS.\nFor a single bit slice, we define the incoming and outgoing \nrepresentations to be the same.  We also need to have at least\none gate in the path to combine the C_1 and C_0 inputs with\ninformation from the bit slice's A and B inputs.  But all CMOS\ngates invert the sense of their inputs.  Our choices are limited\nto NAND and NOR.  Thus we need at least two gates in the path to\nmaintain the same representation.\n\nOne simple answer is to use different representations for odd and\neven bits.  Instead, we optimize a logic circuit\nfor comparing two bits.  We base our design on the alternate \nrepresentation.  The implementation is shown below.  The left\nshows an implementation based on the algebra, and the right shows\na NAND/NOR implementation.  Estimating by gate count and number of\ninputs, the {two-bit} design doesn't save much over two\nsingle bit slices in terms of area.  In terms of delay, however,\nwe have only two gate delays from C_1 and C_0 to either output.\nThe longest path from the A and B inputs to the outputs is\nfive gate delays.  Thus, for an {N-bit} comparator built\nwith this design, the total delay is only N+3 gate delays.\nBut N has to be even.\n\n{file=part2/figs/comparator-two.eps,width=3.125in}\n{file=part2/figs/comparator-two-nn.eps,width=3.125in}\n\nAs you can imagine, continuing to scale up the size of our logic\nblock gives us better performance at the expense of a more complex\ndesign.  Using the alternate representation may help you to see\nhow one can generalize the approach to larger groups of bits---for\nexample, you may have noticed the two bitwise \ncomparator blocks on the left of the implementations above.\n\n\n\n\n\n \n Question: What is the difference between the original and the alternate representation? \n Answer: \n\nA. The difference between the original representation and the alternate representation is that in the alternate representation, C_0=1 whenever A=B."}, {"text": "Title: Logic Operations \n Text: {Logic Operations}\n\n \n Boolean logic functions, truth tables, etc.\n \n overflow expression using logic functions (and difficulty with\n    binary overflow!)\n \n completeness\n \n implications of completeness: enable abstraction, and evaluate\n     future possible implementations of devices quickly: logically complete\n     or not?\n \n examples\n \n generalizing to operations on sets of bits\n \n\nThis set of notes briefly describes a generalization to truth tables,\nthen introduces Boolean logic operations as well as \nnotational conventions and tools that we use to express general\nfunctions on bits.  We illustrate how logic operations enable \nus to express functions such as overflow conditions concisely,\nthen show by construction that a small number of logic operations\nsuffices to describe any operation on any number of bits.  \nWe close by discussing a few implications and examples.\n\n\n\n \n Question: What is the significance of completeness? \n Answer: "}, {"text": "Title: Truth Tables \n Text: {Truth Tables}\n\nYou have seen the basic form of truth tables in the textbook and in class.\nOver the semester, we will introduce several\nextensions to the basic concept, mostly with the\ngoal of reducing the amount of writing necessary when using truth\ntables.  For example, the truth table to the right uses two generalizations\nto show the carry out C (also the unsigned overflow indicator) and the \nsum S produced by\nadding two {2-bit} unsigned numbers.\nFirst, rather than writing each input bit separately, we have grouped\npairs of input bits into the numbers A and B.  \nSecond, we have defined multiple \noutput columns so as to include both bits of S as well as C in \nthe same table.\nFinally, we have \ngrouped the two bits of S into one column.\n\nKeep in mind as you write truth tables that only rarely does an operation\ncorrespond to a simple and familiar process such as addition of base 2\nnumbers.  We had to choose the unsigned and 2's complement representations\ncarefully to allow ourselves to take advantage of a familiar process.\nIn general, for each line of a truth table for an operation, you may\nneed to make use of the input representation to identify the input values,\ncalculate the operation's result as a value, and then translate the value\nback into the correct bit pattern using the output representation.\nSigned magnitude addition, for example, does not always correspond to\nbase 2 addition: when the\n\n\n{cc|cc}\n{c|}& \nA& B& C& S \n00& 00& 0& 00\n00& 01& 0& 01\n00& 10& 0& 10\n00& 11& 0& 11\n01& 00& 0& 01\n01& 01& 0& 10\n01& 10& 0& 11\n01& 11& 1& 00\n10& 00& 0& 10\n10& 01& 0& 11\n10& 10& 1& 00\n10& 11& 1& 01\n11& 00& 0& 11\n11& 01& 1& 00\n11& 10& 1& 01\n11& 11& 1& 10\n\n\n\nsigns of the two input operands differ, \none should instead use base 2 subtraction.  For other operations or\nrepresentations, base 2 arithmetic may have no relevance at all.\n\n\n \n Question: What is the purpose of a truth table? \n Answer: \n\nA. The purpose of a truth table is to show the carry out C and the sum S produced by adding two unsigned 2-bit numbers."}, {"text": "Title: Boolean Logic Operations \n Text: {Boolean Logic Operations}\n\nIn the middle of the 19^ century, George Boole introduced a \nset of logic operations that are today known as { Boolean logic}\n(also as { Boolean algebra}).  These operations today form one of\nthe lowest abstraction levels in digital systems, and an understanding\nof their meaning and use is critical to the effective development of \nboth hardware and software.\n\nYou have probably seen these functions many times already in your \neducation---perhaps first in set-theoretic form as Venn diagrams.  \nHowever, given the use of common\nEnglish words { with different meanings} to name some of the functions,\nand the sometimes confusing associations made even by engineering\neducators, we want to provide you with a concise set of\ndefinitions that generalizes correctly to more than two operands.\nYou may have learned these functions based on truth values \n(true and false), but we define them based on bits, \nwith 1 representing true and 0 representing false.\n\nTable  on the next page lists logic operations.\n\nThe first column in the table lists the name of each function.  The second\nprovides a fairly complete set of the notations that you are likely\nto encounter for each function, including both the forms used in\nengineering and those used in mathematics.  The third column defines \nthe function's\nvalue for two or more input operands (except for NOT, which operates on\na single value).  The last column shows the form generally used in\nlogic schematics/diagrams and mentions the important features used\nin distinguishing each function (in pictorial form usually called\na { gate}, in reference to common physical implementations) from the \nothers.\n\n\n{\n{|c|c|c|c|}\n{ Function}& { Notation}& { Explanation}& { Schematic} \nAND& {A AND B}{AB}{A}{A}{A}& {the ``all'' function: result is 1 iff}{{ all} input operands are equal to 1}& {flat input, round output}  \nOR& {A OR B}{A+B}{A}& {the ``any'' function: result is 1 iff}{{ any} input operand is equal to 1}& {{-6pt}}{round input, pointed output} \nNOT& {NOT A}{A'}{}{}& {logical complement/negation:}{NOT 0 is 1, and NOT 1 is 0}& {triangle and circle} \n{exclusive OR}& {A XOR B}{A}& {the ``odd'' function: result is 1 iff an { odd}}{number of input operands are equal to 1}& {{-6pt}}{OR with two lines}{on input side} \n{``or''}& A, B, or C& {the ``one of'' function: result is 1 iff exactly}{{ one of} the input operands is equal to 1}& (not used) \n\n}\n{Boolean logic operations, notation, definitions, and symbols.}{-12pt}\n\n\n\n\n\nThe first function of importance is { AND}.  Think of { AND} as the\n``all'' function: given a set of input values as operands, AND evaluates \nto 1 if and only if { all} of the input values are 1.  The first\nnotation line simply uses the name of the function.  In Boolean algebra,\nAND is typically represented as multiplication, and the middle three\nforms reflect various ways in which we write multiplication.  The last\nnotational variant is from mathematics, where the AND function is formally\ncalled { conjunction}.\n\nThe next function of importance is { OR}.  Think of { OR} as the\n``any'' function: given a set of input values as operands, OR evaluates\nto 1 if and only if { any} of the input values is 1.  The actual\nnumber of input values equal to 1 only matters in the sense of whether\nit is at least one.  The notation for OR is organized in the same way\nas for AND, with the function name at the top, the algebraic variant that\nwe will use in class---in this case addition---in the middle, and\nthe mathematics variant, in this case called { disjunction}, at the\nbottom.\n\n{ The definition of Boolean OR is not the same as our use of \nthe word ``or'' \nin English.}  For example, if you are fortunate enough to enjoy\na meal on a plane, you might be offered several choices: ``Would you like\nthe chicken, the beef, or the vegetarian lasagna today?''  Unacceptable\nanswers to this English question include: ``Yes,'' ``Chicken and lasagna,''\nand any other combination that involves more than a single choice!\n\nYou may have noticed that we might have instead mentioned that\nAND evaluates to 0 if any input value is 0, and that OR evaluates to 0\nif all input values are 0.  These relationships reflect a mathematical\nduality underlying Boolean logic that has important practical\nvalue in terms of making it easier for humans to digest complex logic\nexpressions.\nWe will talk more about duality later in the course, but you\nshould learn some of the practical\nvalue now: if you are trying to evaluate an AND function, look for an input\nwith value 0; if you are trying to evaluate an OR function, look for an \ninput with value 1.  If you find such an input, you know the function's\nvalue without calculating any other input values.\n\nWe next consider the { logical complement} function, { NOT}.  The NOT\nfunction is also called { negation}.  Unlike our\nfirst two functions, NOT accepts only a single operand, and reverses\nits value, turning 0 into 1 and 1 into 0.  The notation follows the\nsame pattern: a version using the function name at the top, followed\nby two variants used in Boolean algebra, and finally the version\nfrequently used in mathematics.  For the NOT gate, or { inverter},\nthe circle is actually the important part: the triangle by itself\nmerely copies the input.  You will see the small circle added to other\ngates on both inputs and outputs; in both cases the circle implies a NOT\nfunction.\n\n\n\nLast among the Boolean logic functions, we have the\n{ XOR}, or { exclusive OR} function.\nThink of XOR as the ``odd'' function: given a set of input values as\noperands, XOR evaluates to 1 if and only if { an odd number} of\nthe input values are 1.  Only two variants of XOR notation are given:\nthe first using the function name, and the second used with Boolean\nalgebra.  Mathematics rarely uses this function.\n\nFinally, we have included the meaning of the word ``or'' in English as\na separate function entry to enable you to compare that meaning\nwith the Boolean logic functions easily.  Note that many people refer\nto English'\n\n\nuse of the word ``or'' as ``exclusive'' because one\ntrue value excludes all others from being true.  Do not let this \nhuman language ambiguity confuse you about XOR!  For all logic design\npurposes, { XOR is the odd function}.\n\nThe truth table to the right provides values illustrating these functions operating\non three inputs.  The AND, OR, and XOR functions are all \nassociative---(A  B)  C = A  (B  C)---and\ncommutative---A  B = B  A,\nas you may have already realized from their definitions.\n\n\n{ccc|cccc}\n{c|}& \nA& B& C& ABC& A+B+C& & A \n0& 0& 0& 0& 0& 1& 0\n0& 0& 1& 0& 1& 1& 1\n0& 1& 0& 0& 1& 1& 1\n0& 1& 1& 0& 1& 1& 0\n1& 0& 0& 0& 1& 0& 1\n1& 0& 1& 0& 1& 0& 0\n1& 1& 0& 0& 1& 0& 0\n1& 1& 1& 1& 1& 0& 1\n\n\n\n\n\n \n Question: What is the meaning of the word 'or' in English? \n Answer: \n\nThe OR function evaluates to 1 if any of the input values is 1."}, {"text": "Title: Overflow as Logic Expressions \n Text: {Overflow as Logic Expressions}\n\nIn the last set of notes, we discussed overflow conditions for unsigned\nand 2's complement representations.  Let's use Boolean logic to express\nthese conditions.  \n\nWe begin with addition of two {1-bit} unsigned numbers.  \nCall the two input bits A_0 and B_0.  If you write a truth table for\nthis operation, you'll notice that overflow occurs only when all (two)\nbits are 1.  If either bit is 0, the sum can't exceed 1, so overflow\ncannot occur.  In other words, overflow in this case can be written using\nan AND operation: \n\n{eqnarray*}\nA_0B_0\n{eqnarray*}\n\nThe truth table for adding two {2-bit} unsigned numbers is four\ntimes as large, and seeing the structure may be difficult.  One way of\nwriting the expression for overflow of {2-bit} unsigned addition is\nas follows:\n\n{eqnarray*}\nA_1B_1 + (A_1+B_1)A_0B_0\n{eqnarray*}\n\nThis expression is slightly trickier to understand.  Think about the\nplace value of the bits.  If both of the most significant bits---those\nwith place value 2---are 1,\nwe have an overflow, just as in the case of {1-bit} addition.\nThe A_1B_1 term represents this case.  We also have an overflow if\none or both (the OR) of the most significant bits are 1 and the\nsum of the two next significant bits---in this case those with place \nvalue 1---generates a carry.\n\nThe truth table for adding two {3-bit} unsigned numbers is \nprobably not something that you want to write out.  Fortunately,\na pattern should start to become clear with the following expression:\n\n{eqnarray*}\nA_2B_2 + (A_2+B_2)A_1B_1 + (A_2+B_2)(A_1+B_1)A_0B_0\n{eqnarray*}\n\nIn the {2-bit} case, we mentioned the ``most significant bit''\nand the ``next most significant bit'' to help you see the pattern.\nThe same reasoning describes the first two product terms in our\noverflow expression for {3-bit} unsigned addition (but the place\nvalues are 4 for the most significant bit and 2 for the next most\nsignificant bit).  The last term represents the overflow case in which\nthe two least significant bits generate a carry which then propagates\nup through all of the other bits because at least one of the two bits\nin every position is a 1.\n\n\n\n\nThe overflow condition for addition of two {N-bit} 2's complement\nnumbers can be written fairly concisely in terms of the first bits\nof the two numbers and the first bit of the sum.  Recall that overflow\nin this case depends only on whether the three numbers are negative\nor non-negative, which is given by\n\n\n{ 0pt\n 0pt\n\n&A_{N-1}&A_{N-2}&&A_2&A_1&A_0\n+   &B_{N-1}&B_{N-2}&&B_2&B_1&B_0 \n&S_{N-1}&S_{N-2}&&S_2&S_1&S_0\n\n\n}\n\n\nthe most significant bit.  Given\nthe bit names as shown to the right, we can write the overflow condition\nas follows:\n\n{eqnarray*}\nA_{N-1} B_{N-1} {S_{N-1}}+\n{A_{N-1}} {B_{N-1}} S_{N-1}\n{eqnarray*}\n\nThe overflow condition does of course depend on all of the bits in\nthe two numbers being added.  In the expression above, we have simplified\nthe form by using S_{N-1}.  But S_{N-1} depends on the bits A_{N-1}\nand B_{N-1} as well as the carry out of bit (N-2).  \n\nLater in this set of notes, we present a technique with which you can derive\nan expression for an arbitrary Boolean logic function.  As an exercise,\nafter you have finished reading these notes, try using that technique to\nderive an overflow expression for addition of \ntwo {N-bit} 2's complement numbers based on \nA_{N-1}, B_{N-1}, and the carry out of bit (N-2) (and into \nbit (N-1)), which we might\ncall C_{N-1}.  You might then calculate C_{N-1} in terms of the rest\nof the bits of A and B\nusing the expressions for\nunsigned overflow just discussed.\n\nIn the next month or so, you will learn how to derive\nmore compact expressions yourself from truth tables or other representations\nof Boolean logic functions.\n\n\n \n Question: What is the overflow condition for adding two <unk>1-bit<unk> unsigned numbers? \n Answer: \n\nA. A_2B_2 + (A_2+B_2)A_1B_1 + (A_2+B_2)(A_1+B_1)A_0B_0"}, {"text": "Title: Logical Completeness \n Text: {Logical Completeness}\n\nWhy do we feel that such a short list of functions is enough?  If you \nthink about the\nnumber of possible functions on N bits, you might think that we need\nmany more functions to be able to manipulate bits.  With 10 bits, for\nexample, there are 2^ such functions.  Obviously, some of them\nhave never been used in any computer system, but maybe we should define\nat least a few more logic operations?\nIn fact, we do not\neven need XOR.  The functions AND, OR, and NOT are sufficient, even if\nwe only allow two input operands for AND and OR!\n\nThe theorem below captures this idea, called { logical completeness}.\nIn this case, we claim that the set of functions {AND, OR, NOT} is\nsufficient to express any operation on any finite number of variables, \nwhere each variable is a bit.\n\n{ Theorem:} \n\nGiven enough {2-input} AND, {2-input} OR, and {1-input}\nNOT functions, one can express any Boolean logic function on any finite \nnumber of variables.\n\n { Proof:} \n\nThe proof of our theorem is { by construction}.  In other words, \nwe show a systematic\napproach for transforming an arbitrary Boolean logic function on an\narbitrary number of variables into a form that uses only AND, OR, and\nNOT functions on one or two operands.\n\nAs a first step, we remove the restriction on the number of inputs for\nthe AND and OR functions.  For this purpose, we state and prove two\n{ lemmas}, which are simpler theorems used to support the proof of\na main theorem.\n\n{ Lemma 1:}\n\nGiven enough {2-input} AND functions, one can express an AND function\non any finite number of variables.\n\n{ Proof:}\n\nWe prove the Lemma { by induction}.{We assume that you have\nseen proof by induction previously.}  Denote the number of inputs to a\nparticular AND function by N.\n\n\nThe base case is N=2.  Such an AND function is given.\n\nTo complete the proof, we need only show that, given \nany number of AND functions with up to N inputs, we\ncan express an AND function with N+1 inputs.  To do so, we need\nmerely use one {2-input} AND function to join together\nthe result of an {N-input} AND function with an additional\ninput, as illustrated to the right.\n\n\n\n\n\n{ Lemma 2:}\n\nGiven enough {2-input} OR functions, one can express an OR function\non any finite number of variables.\n\n{ Proof:}\n\nThe proof of Lemma 2 is identical in structure to that of Lemma 1, but\nuses OR functions instead of AND functions.\n\n\nLet's now consider a small subset of functions on N variables.  \nFor any such function, you can write out the truth table for the\nfunction.  The output of a logic function is just a bit, \neither a 0 or a 1.  Let's consider the set of functions on N\nvariables that produce a 1 for exactly one combination of the N\nvariables.  In other words, if you were to write out the truth\ntable for such a function, exactly one row in the truth table would\nhave output value 1, while all other rows had output value 0.\n\n{ Lemma 3:}\n\nGiven enough AND functions and {1-input}\nNOT functions, one can express any Boolean logic function \nthat produces a 1 for exactly one combination of\nany finite number of variables.\n\n{ Proof:}\n\nThe proof of Lemma 3 is by construction.\nLet N be the number of variables on which the function operates.\nWe construct a { minterm} on these N variables,\nwhich is an AND operation on each variable or its complement.\nThe minterm is specified by looking at the unique combination of \nvariable values that produces a 1 result for the function.\nEach variable that must be a 1 is included as itself, while\neach variable that must be a 0 is included as the variable's complement\n(using a NOT function).  The resulting minterm produces the\ndesired function exactly.  When the variables all match\nthe values for which the function should produce 1, the\ninputs to the AND function are all 1, and the function produces 1.\nWhen any variable does not match the value for which the function\nshould produce 1, that variable (or its complement) acts as a 0\ninput to the AND function, and the function produces a 0, as desired.\n\nThe table below shows all eight minterms for three variables.\n\n{\n{ccc|cccccccc}\n{c|}& \nA& B& C& \n  &\n  C&\n B &\n B C&\nA  &\nA  C&\nA B &\nA B C\n \n0& 0& 0& 1& 0& 0& 0& 0& 0& 0& 0\n0& 0& 1& 0& 1& 0& 0& 0& 0& 0& 0\n0& 1& 0& 0& 0& 1& 0& 0& 0& 0& 0\n0& 1& 1& 0& 0& 0& 1& 0& 0& 0& 0\n1& 0& 0& 0& 0& 0& 0& 1& 0& 0& 0\n1& 0& 1& 0& 0& 0& 0& 0& 1& 0& 0\n1& 1& 0& 0& 0& 0& 0& 0& 0& 1& 0\n1& 1& 1& 0& 0& 0& 0& 0& 0& 0& 1\n\n}\n\nWe are now ready to prove our theorem.\n\n{ Proof (of Theorem):}\n\nAny given function on N variables\nproduces the value 1 for some set of combinations\nof inputs.  Let's say that M such combinations produce 1.  \nNote that M{2^N}.\nFor each\ncombination that produces 1, we can use\nLemma 1 to construct an {N-input} AND function.\nThen, using Lemma 3, we can use as many as M NOT functions\nand the {N-input} AND function to construct a minterm\nfor that input combination.\nFinally, using Lemma 2, we can construct an {M-input}\nOR function and OR together all of the minterms.\nThe result of the OR is the desired function. If the function\nshould produce a 1 for some combination of inputs, that combination's \nminterm provides a 1 input to the OR, which in turn produces a 1.\nIf a combination should produce a 0, its minterm does not appear in\nthe OR; all other minterms produce 0 for that combination, and thus\nall inputs to the OR are 0 in such cases, and the OR produces 0,\nas desired.\n\nThe construction that we used to prove logical completeness does\nnot necessarily help with efficient design of logic functions.  Think\nabout some of the expressions that we discussed earlier in these\nnotes for overflow conditions.  How many minterms do you need \nfor {N-bit} unsigned overflow?  A single Boolean logic function\ncan be expressed in many different ways, and learning how to develop\nan efficient implementation of a function as well as how to determine\nwhether two logic expressions are identical without actually writing out\ntruth tables are important engineering skills that you will start to \nlearn in the coming months.\n\n\n\n \n Question: What is logical completeness? \n Answer: \n\nLogical completeness is the ability to express any Boolean logic function on any finite number of variables using only AND, OR, and NOT functions on one or two operands."}, {"text": "Title: Implications of Logical Completeness \n Text: {Implications of Logical Completeness}\n\nIf logical completeness doesn't really help us to engineer logic functions,\nwhy is the idea important?  Think back to the layers of abstraction\nand the implementation of bits from the first couple of lectures.  \nVoltages are real numbers, not bits.  { The device layer implementations\nof Boolean logic functions must abstract away the analog properties\nof the physical system.}  Without such abstraction, we must think\ncarefully about analog issues such as noise every time we make use\nof a bit!\n\nLogical completeness assures us that no matter what we want to do\nwith bits, implementating a handful of operations correctly is\nenough to guarantee that we never have to worry.\n\nA second important value of logical completeness is as a tool in\nscreening potential new technologies for computers.  If a new technology\ndoes not allow implementation of a logically complete set of functions,\nthe new technology is extremely unlikely\nto be successful in replacing the current one.\n\nThat said, {AND, OR, and NOT} is not the only logically complete set of\nfunctions.  In fact, our current complementary metal-oxide semiconductor\n(CMOS) technology, on which most of the computer industry is now built,\ndoes not directly implement these functions, as you will see later in\nour class.\n\n\n\nThe functions that are implemented directly in CMOS are NAND and NOR, which\nare abbreviations for AND followed by NOT and OR followed by NOT, \nrespectively.  Truth tables for the two are shown to the right.\n\nEither of these functions by itself forms a logically complete set.\nThat is, both the set  and the set  are logically\ncomplete.  For now, we leave the proof of this claim to you.  Re-\n\n\n{cc|cc}\n{c|}& \n& & & {A+B}\nA& B& A NAND B& A NOR B \n0& 0& 1& 1\n0& 1& 1& 0\n1& 0& 1& 0\n1& 1& 0& 0\n\n\n\nmember that all\nyou need to show is that you can implement any set known to be\nlogically complete, so in order to prove that  is \nlogically complete (for example),\nyou need only show that you can implement AND, OR, and NOT using only\nNAND.\n\n \n Question: Why is logical completeness important? \n Answer: \n\nA. The idea of logical completeness is important because it assures us that no matter what we want to do with bits, implementing a handful of operations correctly is enough to guarantee that we never have to worry."}, {"text": "Title: Examples and a Generalization \n Text: {Examples and a Generalization}\n\nLet's use our construction to solve a few examples.  We begin with\nthe functions that we illustrated with the first truth table from this\nset of notes,\nthe carry out C and sum S of two {2-bit}\nunsigned numbers.  Since each output bit requires a separate expression,\nwe now write S_1S_0 for the two bits of the sum.  We also need to\nbe able to make use of the individual bits of the input values, so we \nwrite these as A_1A_0 and B_1B_0, as shown on the left below.\nUsing our \nconstruction from the logical completeness theorem, we obtain the\nequations on the right.\nYou should verify these expressions yourself.\n\n{\n\n{cccc|ccc}\n{c|}& \nA_1& A_0& B_1& B_0& C& S_1& S_0 \n0&0& 0&0& 0& 0&0\n0&0& 0&1& 0& 0&1\n0&0& 1&0& 0& 1&0\n0&0& 1&1& 0& 1&1\n0&1& 0&0& 0& 0&1\n0&1& 0&1& 0& 1&0\n0&1& 1&0& 0& 1&1\n0&1& 1&1& 1& 0&0\n1&0& 0&0& 0& 1&0\n1&0& 0&1& 0& 1&1\n1&0& 1&0& 1& 0&0\n1&0& 1&1& 1& 0&1\n1&1& 0&0& 0& 1&1\n1&1& 0&1& 1& 0&0\n1&1& 1&0& 1& 0&1\n1&1& 1&1& 1& 1&0\n\n\n\n{eqnarray*}\n\nC &=& \n{A_1} A_0 B_1 B_0 +\nA_1 {A_0} B_1 {B_0} +\nA_1 {A_0} B_1 B_0 +\n&& A_1 A_0 {B_1} B_0 +\nA_1 A_0 B_1 {B_0} +\nA_1 A_0 B_1 B_0\n\n\nS_1 &=& \n{A_1} {A_0} B_1 {B_0} +\n{A_1} {A_0} B_1 B_0 +\n{A_1} A_0 {B_1} B_0 +\n&& {A_1} A_0 B_1 {B_0} +\nA_1 {A_0} {B_1} {B_0} +\nA_1 {A_0} {B_1} B_0 +\n&& A_1 A_0 {B_1} {B_0} +\nA_1 A_0 B_1 B_0\n\n\nS_0 &=& \n{A_1} {A_0} {B_1} B_0 +\n{A_1} {A_0} B_1 B_0 +\n{A_1} A_0 {B_1} {B_0} +\n&& {A_1} A_0 B_1 {B_0} +\nA_1 {A_0} {B_1} B_0 +\nA_1 {A_0} B_1 B_0 +\n&& A_1 A_0 {B_1} {B_0} +\nA_1 A_0 B_1 {B_0}\n{eqnarray*}\n\n}\n\n\nNow let's consider a new function.  Given\nan {8-bit} 2's complement number, A=A_7A_6A_5A_4A_3A_2A_1A_0,\nwe want to compare it with the value -1.  We know that\nwe can construct this function using AND, OR, and NOT, but how?\nWe start by writing the representation for -1, which is 11111111.\nIf the number A matches that representation, we want to produce a 1.\nIf the number A differs in any bit, we want to produce a 0.\nThe desired function has exactly one combination of inputs that\nproduces a 1, so in fact we need only one minterm!  In this case, we\ncan compare with -1 by calculating the expression:\n\n{eqnarray*}\nA_7  A_6  A_5  A_4  A_3  A_2  A_1  A_0\n{eqnarray*}\n\nHere we have explicitly included multiplication symbols to avoid\nconfusion with our notation for groups of bits, as we used when \nnaming the individual bits of A.\n\n\nIn closing, we briefly introduce a generalization of logic\noperations to groups of bits.  Our representations for integers,\nreal numbers, and characters from human languages all use more than\none bit to represent a given value.  When we use computers, we often\nmake use of multiple bits in groups in this way.  A { byte}, for example,\ntoday means an ordered group of eight bits.\n\nWe can extend our logic functions to operate on such groups by pairing\nbits from each of two groups and performing the logic operation on each\npair.  For example, given\nA=A_7A_6A_5A_4A_3A_2A_1A_0=01010101\nand\nB=B_7B_6B_5B_4B_3B_2B_1B_0=11110000, we calculate\nA AND B by computing the AND of each pair of bits, \nA_7 AND B_7,\nA_6 AND B_6,\nand so forth, to\nproduce the result 01010000, as shown to the right.\nIn the same way, we can extend other logic\noperations, such as OR, NOT, and XOR, to operate on bits of groups.\n\n\n{ 0pt\n 0pt\n\n&A  & 0&1& 0&1& 0&1& 0&1\n  AND   &B& 1&1&1&1&0&0&0&0 \n&& 0& 1&0&1&0&0&0&0\n\n}\n\n\n\n\n\n\n \n Question: What is the function for adding two 2-bit unsigned numbers? \n Answer: \n\nA. The function for adding two 2-bit unsigned numbers."}, {"text": "Title: The 2's Complement Representation \n Text: {The 2's Complement Representation}\n\nThis set of notes explains the rationale for using the 2's complement\nrepresentation for signed integers and derives the representation \nbased on equivalence of the addition function to that of addition\nusing the unsigned representation with the same number of bits.\n\n \n Question: What is the rationale for using the 2's complement representation? \n Answer: \n\nA. The rationale for using the 2's complement representation is that it allows for the addition function to be equivalent to addition using the unsigned representation."}, {"text": "Title: Review of Bits and the Unsigned Representation \n Text: {Review of Bits and the Unsigned Representation}\n\nIn modern digital systems, we represent all types of information\nusing binary digits, or { bits}.  Logically, a bit is either 0 or 1.\nPhysically, a bit may be a voltage, a magnetic field, or even the\nelectrical resistance of a tiny sliver of glass.\n\nAny type of information can be represented with an ordered set of\nbits, provided that \n{ any given pattern of bits corresponds to only\none value} and that { we agree in advance on which pattern of bits\nrepresents which value}.\n\nFor unsigned integers---that is, whole numbers greater or equal to\nzero---we chose to use the base 2 representation already familiar to\nus from mathematics.  We call this representation the { unsigned\nrepresentation}.  For example, in a {4-bit} unsigned representation,\nwe write the number 0 as 0000, the number 5 as 0101, and the number 12\nas 1100.  Note that we always write the same number of bits for any\npattern in the representation: { in a digital system, there is no \n``blank'' bit value}.\n\n \n Question: What is important about a given pattern of bits? \n Answer: \n\nA. It is important that any given pattern of bits corresponds to only one value because it allows us to unambiguously represent any value using a finite number of bits."}, {"text": "Title: Picking a Good Representation \n Text: {Picking a Good Representation}\n\nIn class, we discussed the question of what makes one representation\nbetter than another.  The value of the unsigned representation, for\nexample, is in part our existing familiarity with the base 2 analogues\nof arithmetic.  For base 2 arithmetic, we can use nearly identical\ntechniques to those that we learned in elementary school for adding, \nsubtracting, multiplying, and dividing base 10 numbers.\n\nReasoning about the relative merits of representations from a\npractical engineering perspective is (probably) currently beyond \nyour ability.\n\nSaving energy, making the implementation simple, and allowing the\nimplementation to execute quickly probably all sound attractive, but \na quantitative comparison between two representations on any of these\nbases requires knowledge that you will acquire in the\nnext few years.\n\nWe can sidestep such questions, however, by realizing that if a\ndigital system has hardware to perform operations such as addition\non unsigned values, using the same piece of hardware to operate\non other representations incurs little or no additional cost.\nIn this set of notes, we discuss the 2's complement representation,\nwhich allows reuse of the unsigned add unit (as well as a basis for\nperforming subtraction of either representation using an add unit!).\nIn discussion section and in your homework, you will \nuse the same idea to perform operations on other representations, such as\nchanging an upper case letter in ASCII to a lower case one, or converting\nfrom an ASCII digit to an unsigned representation of the same number.  \n\n \n Question: What is important to engineers? \n Answer: \n\nA. Reasoning about the relative merits of representations from a practical engineering perspective is important because it allows engineers to optimize digital systems for energy efficiency, simplicity, and speed."}, {"text": "Title: The Unsigned Add Unit \n Text: {The Unsigned Add Unit}\n\nIn order to define a representation for signed integers that allows\nus to reuse a piece of hardware designed for unsigned integers, we\nmust first understand what such a piece of hardware actually does (we\ndo not need to know how it works yet---we'll explore that question \nlater in our class).\n\nThe unsigned representation using {N} bits is not closed\nunder addition.  In other words, for any value of N, we can easily\nfind two {N-bit} unsigned numbers that, when added together,\ncannot be represented as an {N-bit} unsigned number.  With N=4, \nfor example, we can add 12 (1100) and 6 (0110) to obtain 18.\nSince 18 is outside of the range [0,2^4-1] representable using\nthe {4-bit} unsigned representation, our representation breaks\nif we try to represent the sum using this representation.  We call\nthis failure an { overflow} condition: the representation cannot\nrepresent the result of the operation, in this case addition.\n\n\nUsing more bits to represent the answer is not an attractive solution, \nsince we might then want to use more bits for the inputs, which in turn\nrequires more bits for the outputs, and so on.  We cannot build \nsomething supporting an infinite number of bits.  Instead, we \nchoose a value for N and build an add unit that adds two {N-bit}\nnumbers and produces an {N-bit} sum (and some overflow \nindicators, which we discuss in the next set of notes).  The diagram\nto the right shows how we might draw such a device, with two {N-bit}\nnumbers entering at from the top, and the {N-bit} sum coming out\nfrom the bottom.\n\n\n\n\n\n\nThe function used for {N-bit} unsigned addition is addition \nmodulo 2^N.  In a practical sense, you can think of this function\nas simply keeping the last N bits of the answer; other bits \nare simply discarded.  In the example to the right,\nwe add 12 and 6 to obtain 18, but then discard the extra bit on the\nleft, so the add unit produces 2 (an overflow).\n\n\n\n\n\n\n{ Modular arithmetic} defines a way of performing arithmetic for\na finite number of possible values, usually integers.  \nAs a concrete example, let's use modulo 16, which corresponds to\nthe addition unit for our {4-bit} examples.\n\nStarting with the full range of integers, we break the number\nline into contiguous groups of 16 integers, as shown to the right.\n\n\n\n\n\nThe numbers 0 to 15 form one group.  The numbers -16 to -1 form a\nsecond group, and the numbers from 16 to 31 form a third group. \nAn infinite number of groups are defined in this manner.\n\nWe then define 16 { equivalence classes} consisting of the first numbers\nfrom all groups, the second numbers from all groups, and so forth.\nFor example, the numbers , -32, -16, 0, 16, 32,  form\none such equivalence class.\n\nMathematically, we say that two numbers A and B are equivalent modulo 16,\nwhich we write as\n\n{eqnarray*}\n(A &=& B)  16, {or sometimes as}\nA && B {(mod 16)}\n{eqnarray*}\n\nif and only if A=B+16k for some integer k.\n\nEquivalence as defined by a particular modulus\ndistributes over addition and multiplication.  If, for example,\nwe want to find the equivalence class for (A + B)  16,\nwe can find the equivalence classes for A (call it C) and B \n(call it D) and then calculate the equivalence class \nof (C + D)  16.\nAs a concrete example of distribution over multiplication, \ngiven (A = 1,083,102,112  7,323,127)  10,\nfind A.\n\nFor this problem, we note that the first number is equivalent \nto 2  10, while the second number is equivalent \nto 7  10.  We then write (A = 2  7)  10,\nand, since 2  7 = 14, we have (A = 4)  10.\n\n\n \n Question: What is the name of the function that allows us to reuse a piece of hardware designed for unsigned integers? \n Answer: "}, {"text": "Title: Deriving 2's Complement \n Text: {Deriving 2's Complement}\n\n\nGiven these equivalence classes, we might instead choose to draw a circle\nto identify the equivalence classes and to associate each class with one\nof the sixteen possible {4-bit} patterns, as shown to the right.\nUsing this circle representation, we can add by counting clockwise around\nthe circle, and we can subtract by counting in a counterclockwise direction\naround the circle.  With an unsigned representation, we choose to use the\ngroup from [0,15] (the middle group in the diagram markings to the right)\nas the number represented by each of the patterns.  Overflow occurs\nwith unsigned addition (or subtraction) because we can only choose one\nvalue for each binary pattern.\n\n\n\n\n\nIn fact, we can choose any single value for each pattern to create a \nrepresentation, and our add unit will always produce results that\nare correct modulo 16.  Look back at our overflow example, where\nwe added 12 and 6 to obtain 2, and notice that (2=18)  16.\nNormally, only a contiguous sequence of integers makes a useful\nrepresentation, but we do not have to restrict ourselves to \nnon-negative numbers.\n\nThe 2's complement representation can then be defined by choosing a \nset of integers balanced around zero from the groups.  In the circle \ndiagram, for example, we might choose to represent numbers\nin the range [-7,7] when using 4 bits.  What about the last pattern, 1000?\nWe could choose to represent either -8 or 8.  The number of arithmetic\noperations that overflow is the same with both choices (the choices\nare symmetric around 0, as are the combinations of input operands that \noverflow), so we gain nothing in that sense from either choice.\nIf we choose to represent -8, however, notice that all patterns starting\nwith a 1 bit then represent negative numbers.  No such simple check\narises with the opposite choice, and thus an {N-bit} 2's complement \nrepresentation is defined to represent the range [-2^{N-1},2^{N-1}-1],\nwith patterns chosen as shown in the circle.\n\n \n Question: What is the effect of using -8 instead of 8? \n Answer: \n\nA. The effect of choosing to represent -8 instead of 8 in a 4-bit 2's complement system is that all patterns starting with a 1 bit then represent negative numbers."}, {"text": "Title: An Algebraic Approach \n Text: {An Algebraic Approach}\n\nSome people prefer an algebraic approach to understanding the\ndefinition of 2's complement, so we present such an approach next.\nLet's start by writing f(A,B) for the result of our add unit:\n\n{eqnarray*}\nf(A,B) = (A + B)  2^N\n{eqnarray*}\n\nWe assume that we want to represent a set of integers balanced around 0\nusing our signed representation, and that we will use the same binary\npatterns as we do with an unsigned representation to represent\nnon-negative numbers.  Thus, with an {N-bit} representation,\nthe patterns in the range [0,2^{N-1}-1] are the same as those\nused with an unsigned representation.  In this case, we are left with\nall patterns beginning with a 1 bit.\n\nThe question then is this: given an integer k, 2^{N-1}>k>0, for which we \nwant to find a pattern to represent -k, and any integer m\nthat we might want to add to -k, \ncan we find another integer p>0\nsuch that \n\n\n(-k + m = p + m)  2^N   ?\n\n\nIf we can, we can use p's representation to represent -k and our\nunsigned addition unit f(A,B) will work correctly.\n\nTo find the value p, start by subtracting m from both sides of\nEquation () to obtain:\n\n\n(-k = p)  2^N\n\n\nNote that (2^N=0)  2^N, and add this equation to \nEquation () to obtain\n\n{eqnarray*}\n(2^N-k = p)  2^N\n{eqnarray*}\n\nLet p=2^N-k.  \n\nFor example, if N=4, k=3 gives p=16-3=13, which is the pattern 1101.\nWith N=4 and k=5, we obtain p=16-5=11, which is the pattern 1011.\nIn general, since\n2^{N-1}>k>0, \nwe have 2^{N-1}<p<2^N.  But these patterns are all unused---they all\nstart with a 1 bit!---so the patterns that we have defined for negative\nnumbers are disjoint from those that we used for positive numbers, and\nthe meaning of each pattern is unambiguous.\n\nThe algebraic definition of bit patterns for negative numbers\nalso matches our circle diagram from the last\nsection exactly, of course.\n\n\n\n \n Question: What is the meaning of each pattern? \n Answer: \n\nA. The meaning of each pattern is unambiguous."}, {"text": "Title: Negating 2's Complement Numbers \n Text: {Negating 2's Complement Numbers}\n\nThe algebraic approach makes understanding negation of an integer\nrepresented using 2's complement fairly straightforward, and gives \nus an easy procedure for doing so.\nRecall that given an integer k in an {N-bit} 2's complement\nrepresentation, the {N-bit} pattern for -k is given by 2^N-k \n(also true for k=0 if we keep only the low N bits of the result).  \nBut 2^N=(2^N-1)+1.  Note that 2^N-1 is the pattern of\nall 1 bits.  Subtracting any value k from this value is equivalent\nto simply flipping the bits, changing 0s to 1s and 1s to 0s.\n(This operation is called a { 1's complement}, by the way.)\nWe then add 1 to the result to find the pattern for -k.\n\nNegation can overflow, of course.  Try finding the negative pattern for -8 \nin {4-bit} 2's complement.\n\nFinally, be aware that people often overload the term 2's complement\nand use it to refer to the operation of negation in a 2's complement\nrepresentation.  In our class, we try avoid this confusion: 2's complement\nis a representation for signed integers, and negation is an operation\nthat one can apply to a signed integer (whether the representation used\nfor the integer is 2's complement or some other representation for signed\nintegers).\n\n\n\n\n\n \n Question: What is the algebraic approach to negation of an integer? \n Answer: \n\nA. The algebraic approach makes understanding negation of an integer in a 2's complement representation straightforward because it is simply a matter of subtracting the value from 2^N-1 and then adding 1."}, {"text": "Title: The Halting Problem \n Text: {The Halting Problem}\n\nFor some of the topics in this course, we plan to cover the material\nmore deeply than does the textbook.  We will provide notes in this\nformat to supplement the textbook for this purpose.\n\nIn order to make these notes more useful as a reference, definitions are\nhighlighted with boldface, and italicization emphasizes pitfalls or other\nimportant points.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}\n\nThese notes are broken up into four parts, corresponding to the three\nmidterm exams and the final exam.  Each part is covered by\none examination in our class.  { The last section of each of the four\nparts gives you\na summary of material that you are expected to know for the corresponding\nexam.}  Feel\nfree to read it in advance.\n\nAs discussed in the textbook and in class, a { universal\ncomputational device} (or { computing machine}) is a device \nthat is capable of computing the\nsolution to any problem that can be computed, provided that the device\nis given enough storage and time for the computation to finish.  \n\nOne might ask whether we can describe problems that we cannot answer (other \nthan philosophical ones, such as the meaning of life).\n\nThe answer is yes: there are problems that are provably { undecidable},\nfor which no amount of computation can solve the problem in general.\nThis set of notes describes the first problem known to be\nundecidable, the { halting problem}.  For our class, you need only recognize\nthe name and realize that one can, in fact, give examples of problems\nthat cannot be solved by computation.  In the future, you should be able\nto recognize this type of problem so as to avoid spending your time\ntrying to solve it.\n\n \n Question: What is the first problem that is undecidable? \n Answer: \n\nA. The first problem known to be undecidable is the halting problem."}, {"text": "Title: Universal Computing Machines* \n Text: {Universal Computing Machines*}\n\nThe things that we call computers today, whether we are talking about\na programmable microcontroller in a microwave oven or the Blue Waters\nsupercomputer sitting on the south end of our campus (the United\nStates' main resource to support computational science research), are\nall equivalent in the sense of what problems they can solve.  \n\nThese machines do, of course, have access to different amounts of memory, \nand compute at different speeds.  \n\nThe idea that a single model of computation could be described and proven\nto be equivalent to all other models came out of a 1936 paper by Alan Turing, \nand today we generally refer to these devices as { Turing machines}.\n\nAll computers mentioned earlier, as well as all computers with which you \nare familiar in your daily life, are provably equivalent to Turing machines.\n\nTuring also conjectured that his definition of computable was\nidentical to the ``natural'' definition (today, this claim is known\nas the { Church-Turing conjecture}).  In other words, a problem that\ncannot be solved by a Turing machine cannot be solved in any\nsystematic manner, with any machine, or by any person.  This conjecture\nremains unproven!  However, neither has anyone been able to disprove\nthe conjecture, and it is widely believed to be true.  Disproving the\nconjecture requires that one demonstrate a systematic technique (or a\nmachine) capable of solving a problem that cannot be solved by a\nTuring machine.  No one has been able to do so to date.\n\n\n \n Question: What is the significance of the Church-Turing conjecture? \n Answer: \n\nA. The significance of the Church-Turing conjecture is that it is a statement that has yet to be proven or disproven. If it were to be proven, it would mean that there is a systematic technique or machine that can solve a problem that cannot be solved by a Turing machine. As of now, no one has been able to do so."}, {"text": "Title: The Halting Problem* \n Text: {The Halting Problem*}\n\nYou might reasonably ask whether any problems can be shown to\nbe incomputable.  More common terms for such problems---those known\nto be insolvable by any computer---are { intractable} or \nundecidable.\n\nIn the same 1936 paper in which he introduced the universal computing\nmachine, Alan Turing also provided an answer to this question\nby introducing (and proving) that there are in fact problems that cannot be\ncomputed by a universal computing machine.\nThe problem that\nhe proved undecidable, using proof techniques almost identical to those\ndeveloped for similar problems in the 1880s, is now known as { the\nhalting problem}.\n\n\n\nThe halting problem is easy to state and easy to prove undecidable.\nThe problem is this: given a Turing machine and an input to the Turing\nmachine, does the Turing machine finish computing in a finite number\nof steps (a finite amount of time)?  In order to solve the problem, an\nanswer, either yes or no, must be given in a finite amount of time\nregardless of the machine or input in question.  Clearly some machines\nnever finish.  For example, we can write a Turing machine that counts\nupwards starting from one.\n\nYou may find the proof structure for undecidability of the halting problem\neasier to understand if\nyou first think about a related problem with which you may\nalready be familiar, the Liar's paradox\n(which is at least 2,300 years old).  In its stengthened form, it is\nthe following sentence: ``This sentence is not true.''\n\n\nTo see that no Turing machine can solve the halting problem, we begin\nby assuming that such a machine exists, and then show that its\nexistence is self-contradictory.  We call the machine the ``Halting\nMachine,'' or HM for short.  HM is a machine that operates on \nanother\n\n\n\n\n\nTuring machine and its inputs to produce a yes or no answer in finite time:\neither the machine in question finishes in finite time (HM returns\n``yes''), or it does not (HM returns ``no'').  The figure illustrates\nHM's operation.\n\n\nFrom HM, we construct a second machine that we call the HM Inverter,\nor HMI.  This machine inverts the sense of the answer given by HM.  In\nparticular, the inputs are fed directly into a copy of HM, and if HM\nanswers ``yes,'' HMI enters an infinite loop.  If HM answers ``no,'' HMI\nhalts.  A diagram appears to the right.\n\nThe inconsistency can now be seen by asking HM whether HMI halts when\ngiven itself as an input (repeatedly), as\n\n\n\n\n\nshown below.  Two\ncopies of HM are thus\nbeing asked the same question.  One copy is the rightmost in the figure below\nand the second is embedded in the HMI machine that we are using as the\ninput to the rightmost HM.  As the two copies of HM operate on the same input\n(HMI operating on HMI), they should return the same answer: a Turing\nmachine either halts on an input, or it does not; they are\ndeterministic.\n\n\n\nLet's assume that the rightmost HM tells us that HMI operating on itself halts.\nThen the copy of HM in HMI (when HMI executes on itself, with itself\nas an input) must also say ``yes.''  But this answer implies that HMI\ndoesn't halt (see the figure above), so the answer should have been\nno!\n\nAlternatively, we can assume that the rightmost HM says that HMI operating on itself\ndoes not halt.  Again, the copy of HM in HMI must give the same\nanswer.  But in this case HMI halts, again contradicting our\nassumption.\n\nSince neither answer is consistent, no consistent answer can be given,\nand the original assumption that HM exists is incorrect.  Thus, no\nTuring machine can solve the halting problem.\n\n\n\n\n\n \n Question: What is the halting problem? \n Answer: \n\nA. That some problems are incomputable, or insolvable by any computer."}, {"text": "Title: Programming Concepts and the C Language \n Text: {Programming Concepts and the C Language}\n\nThis set of notes introduces the C programming language and explains\nsome basic concepts in computer programming.  Our purpose in showing\nyou a high-level language at this early stage of the course is to give\nyou time to become familiar with the syntax and meaning of the language,\nnot to teach you how to program.  Throughout this semester, we will\nuse software written in C to demonstrate and validate the digital system design\nmaterial in our course.  Towards the end of the semester, you will\nlearn to program computers using instructions and assembly language.\nIn ECE 220, you will make use of the C language to write\nprograms, at which point already being familiar with the language will\nmake the material easier to master.  \n\nThese notes are meant to complement the\nintroduction provided by Patt and Patel.\n\nAfter a brief introduction to the history of C and the structure of\na program written in C, we connect the idea of representations developed \nin class to the data types used in high-level languages.\n\nWe next discuss the use of variables in C, then describe some of the \noperators available to the programmer, including arithmetic and logic\noperators.  The notes next introduce C functions that support the ability to\nread user input from the keyboard and to print results to the monitor.\n\nA description of the structure of statements in C follows, explaining\nhow programs are executed and how a programmer can create statements\nfor conditional execution as well as loops to perform repetitive tasks.\n\nThe main portion of the notes concludes with \nan example program, which is used to illustrate both the execution of \nC statements as well as the difference between variables in programs\nand variables in algebra.\n\nThe remainder of the notes covers more advanced topics.  First, we \ndescribe how the compilation process works,\nillustrating how a program written in a high-level language is\ntransformed into instructions.  You will learn this process in much\nmore detail in ECE 220.  \n\nSecond, we briefly introduce the C preprocessor.\n\nFinally, we discuss implicit and explicit data type conversion in C.\n\n{ Sections marked with an asterisk are provided solely for your\ninterest, but you probably need to learn this material in later classes.}\n\n\n\n \n Question: What is the purpose of introducing the C programming language in this course? \n Answer: \n\nA. The purpose of introducing the C programming language in this course is to give students time to become familiar with the syntax and meaning of the language, not to teach them how to program."}, {"text": "Title: The C Programming Language \n Text: {The C Programming Language}\n\nProgramming languages attempt to bridge the semantic gap between human\ndescriptions of problems and the relatively simple instructions that\ncan be provided by an instruction set architecture (ISA).  \nSince 1954, when the Fortran language\nfirst enabled scientists to enter FORmulae symbolically and to have\nthem TRANslated automatically into instructions, people have invented\nthousands of computer languages.  \n\n\nThe C programming language was developed by Dennis Ritchie at Bell Labs\nin order to simplify the task of writing the Unix operating system.\nThe C language provides a fairly transparent mapping to typical ISAs,\nwhich makes it a good choice both for system software such as operating\nsystems and for our class.\n\nThe { syntax} used in C---that is, the rules that one must follow\nto write valid C programs---has also heavily influenced many other\nmore recent languages, such as C++, Java, and Perl.  \n\nFor our purposes, a C program consists of a set of { variable \ndeclarations} and a sequence of { statements}.  \n\n\n{\n\naaaa=/* =\nint\nmain ()\n{\n>  int answer = 42;        /* the Answer! */\n\n>  printf (\"The answer is d.n\", answer);\n\n>  /*> Our work here is done.\n>    > Let's get out of here! */\n>  return 0;\n}\n\n}\n\n\nBoth of these parts are written into a single C function called { main},\nwhich executes when the program starts.  \n\nA simple example appears to the right.  The program uses one variable\ncalled { answer}, which it initializes to the value 42.\nThe program prints a line of output to the monitor for the user,\nthen terminates using the { return} statement.  { Comments} for human\nreaders begin with the characters { /*} (a slash followed by an \nasterisk) and end with the characters { */} (an asterisk followed \nby a slash).\n\nThe C language ignores white space in programs, so we encourage\nyou to use blank lines and extra spacing to make your programs\neasier to read.\n\nThe variables defined in the { main} function allow a programmer\nto associate arbitrary { symbolic names} (sequences of English characters, \nsuch as ``sum'' or ``product'' or ``highScore'') with specific\ntypes of data, such as a {16-bit} unsigned integer or a\ndouble-precision floating-point number. \n\nIn the example program above, the variable { answer} is declared\nto be a {32-bit} {2's} complement number.\n\nThose with no programming experience may at first find the difference\nbetween variables in algebra and variables in programs slightly \nconfusing.  { As a program executes, the values of variables can \nchange from step to step of execution.}\n\nThe statements in the { main} function are executed one by one\nuntil the program terminates.  \n\nPrograms are not limited to simple sequences of statements, however.\nSome types of statements allow a programmer\nto specify conditional behavior.  For example, a program might only\nprint out secret information if the user's name is ``lUmeTTa.''\nOther types of statements allow a programmer to repeat the execution\nof a group of statements until a condition is met.  For example, a program\nmight print the numbers from 1 to 10, or ask for input until the user\ntypes a number between 1 and 10.\n\nThe order of statement execution is well-defined in C, but the\nstatements in { main} do not necessarily make up an algorithm:\n{ we can easily write a C program that never terminates}.\n\nIf a program terminates, the { main} function\nreturns an integer to the operating system, usually by executing\na { return} statement, as in the example program.\n\nBy convention, returning the value 0 indicates successful completion\nof the program, while any non-zero value indicates a program-specific\nerror.\n\nHowever, { main} is not necessarily a function in the mathematical \nsense because { the value returned from { main} is not \nnecessarily unique for a given set of input values to the program}.  \n\nFor example, we can write a program that selects a number from 1 to 10 \nat random and returns the number to the operating system.\n\n\n\n\n \n Question: What is the difference between variables in algebra and variables in programs? \n Answer: \n\nA. In algebra, variables are unchanging values that are used to represent other values in equations. In programs, variables can change from step to step of execution, and are used to store data that can be used by the program."}, {"text": "Title: Data Types \n Text: {Data Types}\n\nAs you know, modern digital computers represent all information with\nbinary digits (0s and 1s), or { bits}.  Whether you are representing \nsomething as simple as an integer or as complex as an undergraduate \nthesis, the data are simply a bunch of 0s and 1s inside a computer.  \n\nFor any given type of information, a human selects a data type for the\ninformation.  A { data type} (often called just a { type})\nconsists of both a size in bits and a representation, such as the\n2's complement representation for signed integers, or the ASCII\nrepresentation for English text.  A { representation} is a way of\nencoding the things being represented as a set of bits, with each bit\npattern corresponding to a unique object or thing.\n\nA typical ISA supports a handful of\ndata types in hardware in the sense that it provides hardware \nsupport for operations on those data types.\n\nThe arithmetic logic units (ALUs) in most modern processors,\nfor example, support addition\nand subtraction of both unsigned and 2's complement representations, with\nthe specific data type (such as 16- or 64-bit 2's complement)\ndepending on the ISA.\n\nData types and operations not supported by the ISA must be handled in\nsoftware using a small set of primitive operations, which form the\n{ instructions} available in the ISA.  Instructions usually\ninclude data movement instructions such as loads and stores\nand control instructions such as branches and subroutine calls in\naddition to arithmetic and logic operations.  \n\nThe last quarter of our class covers these concepts in more detail\nand explores their meaning using an example ISA from the textbook.\n\nIn class, we emphasized the idea that digital systems such as computers\ndo not interpret the meaning of bits.  Rather, they do exactly what\nthey have been designed to do, even if that design is meaningless.\n\nIf, for example, you store\na sequence of ASCII characters \nin a computer's memory as \nand\nthen write computer instructions to add consecutive groups of four characters\nas 2's complement integers and to print the result to the screen, the\ncomputer will not complain about the fact that your code produces\nmeaningless garbage.  \n\nIn contrast, high-level languages typically require that a programmer\nassociate a data type with each datum in order to reduce the chance \nthat the bits \nmaking up an individual datum are misused or misinterpreted accidentally.  \n\nAttempts to interpret a set of bits differently usually generate at least\na warning message, since\n\n\n\nsuch re-interpretations of the\nbits are rarely intentional and thus rarely correct.  A compiler---a\nprogram that transforms code written in a high-level language into\ninstructions---can also generate the proper type conversion instructions \nautomatically when the \ntransformations are intentional, as is often the case with arithmetic.\n\nSome high-level languages, such as Java, \nprevent programmers from changing the type of a given datum.\nIf you define a type that represents one of your\nfavorite twenty colors, for example, you are not allowed to turn a\ncolor into an integer, despite the fact that the color is represented\nas a handful of bits.  Such languages are said to be { strongly\ntyped}.  \n\nThe C language is not strongly typed, and programmers are free to\ninterpret any bits in any manner they see fit.  Taking advantage of\nthis ability in any but a few exceptional cases, however, \nresults in arcane and non-portable code, and is thus considered to be\nbad programming practice.  We discuss conversion between types in more\ndetail later in these notes.\n\nEach high-level language defines a number of { primitive data\ntypes}, which are always available.  Most languages, including C,\nalso provide ways of defining new types in terms of primitive types,\nbut we leave that part of C for ECE 220.\n\nThe primitive data types in C include signed and unsigned integers of various\nsizes as well as single- and double-precision IEEE floating-point numbers.\n\n\nThe primitive integer types in C include both unsigned and 2's\ncomplement representations.  These types were originally defined so as\nto give reasonable performance when code was ported.  In particular,\nthe { int} type is intended to be the native integer type for the\ntarget ISA.  Using data types supported directly in hardware is faster \nthan using larger or smaller integer types.  When C was standardized in 1989,\nthese types were defined so as to include a range of existing\nC compilers rather than requiring all compilers to produce uniform\nresults.  At the\n\n\n{-8pt}{|r|l|l|}\n& {|c|}{2's complement}& {|c|}\n8 bits& { char}& { unsigned char} \n16 bits& { short}& { unsigned short}\n& { short int}& { unsigned short int} \n32 bits& { int}& { unsigned}\n&& { unsigned int} \n32 or & { long}& { unsigned long}\n64 bits& { long int}& { unsigned long int} \n64 bits& { long long}& { unsigned long long}\n& { long long int}& { unsigned long long int}\n\n{-14pt}\n\ntime, most workstations and mainframes were 32-bit machines, while\nmost personal computers were 16-bit machines, thus flexibility was somewhat\ndesirable.  For the GCC compiler on Linux, the C integer data \ntypes are defined\nin the table above.  Although the { int} and { long}\ntypes are usually the same, there is a semantic difference in common\nusage.  In particular, on most architectures and most compilers, a\n{ long} has enough bits to identify a location in the computer's\nmemory, while an { int} may not.\n\nWhen in doubt, the { size in bytes} of any type or variable can be\nfound using the built-in C function { sizeof}.\n\n\nOver time, the flexibility of size in C types has become less\nimportant (except for the embedded markets, where one often wants even\nmore accurate bit-width control), and the fact that the size of an\n{ int} can vary from machine to machine and compiler to compiler\nhas become more a source of headaches than a helpful feature.  In the\nlate 1990s, a new set of fixed-size types were recommended for\ninclusion\n\n\n{-8pt}{|r|l|l|}\n& {|c|}{2's complement}& {|c|}\n 8 bits& {  int8_t}& {  uint8_t}\n16 bits& { int16_t}& { uint16_t}\n32 bits& { int32_t}& { uint32_t}\n64 bits& { int64_t}& { uint64_t}\n\n{-14pt}\n\nin the C library, reflecting the fact that many companies\nhad already developed and were using such definitions to make their\nprograms platform-independent.\n\nWe encourage you to make use of these types, which are shown in \nthe table above.  In Linux, they can be made available by including \nthe { stdint.h} header file.\n\nFloating-point types in C include { float} and { double},\nwhich correspond respectively to single- and double-precision IEEE\nfloating-point values.  Although the {32-bit} { float} type\ncan save memory compared with use of {64-bit} { double}\nvalues, C's math library works with double-precision values, and\nsingle-precision data are uncommon in scientific and engineering\ncodes.  In contrast, single-precision floating-point operations\ndominated the\ngraphics industry until recently, and are still well-supported even\non today's graphics processing units.\n\n\n \n Question: What is the standard data type for an integer in C? \n Answer: \n\nA. The standard data type for an integer in C is the int data type. This data type is a fixed-size type that is typically 32 bits in size."}, {"text": "Title: Variable Declarations \n Text: {Variable Declarations}\n\nThe function { main} executed by a program begins with a list\nof { variable declarations}.  Each declaration consists of two parts:\na data type specification and a comma-separated list of variable names.\nEach variable declared can also \nbe { initialized} by assigning an initial value.  A few examples \nappear below.  Notice that one can initialize a variable to have the same\nvalue as a second variable.\n\n{\n\naaaaaaa=aa=aaaaaaaaaaaaaaaaaaaaa=/=* a third 2's complement variable with unknown initial value =\nint > x > = 42; >/>* a 2's complement variable, initially equal to 42 > */\nint > y > = x;  >/>* a second 2's complement variable, initially equal to x > */\nint > z;>       >/>* a third 2's complement variable with unknown initial value > */\ndouble> a, b, c, pi = 3.1416; > >/>*\n>>>>* four double-precision IEEE floating-point variables\n>>>>* a, b, and c are initially of unknown value, while pi is\n>>>>* initially 3.1416\n>>>>*/\n\n}\n\nWhat happens if a programmer declares a variable but does not \ninitialize it?\n\nRemember that bits can only be 0 or 1.\n\nAn uninitialized variable does have a value, but its value is unpredictable.\n\nThe compiler tries to detect uses of uninitialized variables, but sometimes\nit fails to do so, so { until you are more familiar with programming,\nyou should always initialize every variable}.\n\nVariable names, also called { identifiers}, can include both letters\nand digits in C.\n\nGood programming style requires that programmers select variable names\nthat are meaningful and are easy to distinguish from one another.\n\nSingle letters are acceptable in some situations, but longer names with\nmeaning are likely to help people (including you!) understand your \nprogram.\n\nVariable names are also case-sensitive in C, which allows programmers\nto use capitalization to differentiate behavior and meaning, if desired.\nSome programs, for example, use identifiers with all capital letters\nto indicate variables with values that remain constant for the program's\nentire execution.\n\nHowever, the fact that identifiers are case-sensitive also means \nthat a programmer can declare distinct variables \nnamed { variable}, { Variable}, { vaRIable}, { vaRIabLe}, \nand { VARIABLE}.  We strongly discourage you from doing so.\n\n\n\n\n \n Question: What is a variable that has not been initialized? \n Answer: \n\nA. A variable that has not been assigned a value."}, {"text": "Title: Expressions and Operators \n Text: {Expressions and Operators}\n\n\nThe { main} function also contains a sequence of statements.\n\nA statement is a complete specification of a single step\nin the program's execution.\n\nWe explain the structure of\nstatements in the next section.  \n\nMany statements in C include one or more { expressions},\nwhich represent calculations such as arithmetic, comparisons,\nand logic operations.\n\nEach expression is in turn composed of { operators} and { operands}.\n\nHere we give only a brief introduction to some of the operators available\nin the C language.  We deliberately omit operators with more\ncomplicated meanings, as well as operators for which the original\npurpose was to make writing common operations a little shorter.\n\nFor the interested reader, both the textbook and ECE 220 give more \ndetailed introductions.\n\nThe table to the right gives examples for the operators described \nhere.  \n\n\n\n{\n\n{int i = 42, j = 1000;}\n{/* i = 0x0000002A, j = 0x000003E8 */}\n\ni + j &  1042\ni - 4 * j &  -3958\n-j &  -1000\nj / i &  23\nj  i &   42\ni & j &   40& /* 0x00000028 */\ni | j &   1002& /* 0x000003EA */\ni  j &   962& /* 0x000003C2 */\ni &   -43& /* 0xFFFFFFD5 */\n(i) >> 2 &   -11& /* 0xFFFFFFF5 */\n((i) >> 4) &   2& /* 0x00000002 */\nj >> 4 &  62& /* 0x0000003E */ \nj << 3 &  8000& /* 0x00001F40 */ \ni > j &   0\ni <= j &   1\ni == j &   0\nj = i &   42& /* ...and j is changed! */\n\n}\n\n\n\n\n{ Arithmetic operators} in C include addition ({ +}), \nsubtraction ({ -}), negation (a minus sign not \npreceded by another expression), multiplication ({ *}), \ndivision ({ /}), and modulus ({ }).  No exponentiation\noperator exists; instead, library routines are defined for this purpose\nas well as for a range of more complex mathematical functions.\n\nC also supports { bitwise operations} on integer types, including \nAND ({ &}), OR ({ |}), XOR ({ ^{ }}), NOT ({ }), \nand left ({ <<}) and right ({ >>}) bit shifts.\nRight shifting a signed integer results in an { arithmetic right shift}\n(the sign bit is copied), while right shifting an unsigned integer\nresults in a { logical right shift} (0 bits are inserted).\n\nA range of { relational} or { comparison operators} are \navailable, including equality ({ ==}),\ninequality ({ !=}), and relative order ({ <}, { <=},\n{ >=}, and { >}).\n\nAll such operations evaluate to 1 to indicate a true relation\nand 0 to indicate a false relation.  Any non-zero value is considered\nto be true for the purposes of tests (for example, in an { if} statement\nor a { while} loop) in C---these statements are explained later in \nthese notes.\n\n{ Assignment} of a new value to a variable \nuses a single equal sign ({ =}) in C.  \n\nFor example, the expression { A = B} copies\nthe value of variable { B} into variable { A}, overwriting the\nbits representing the previous value of { A}.\n\n{ The use of two equal signs for an equality check and a single\nequal sign for assignment is a common source of errors,} although\nmodern compilers generally detect and warn about this type of mistake.\n\nAssignment in C does not solve equations, even simple equations.  \nWriting ``{ A-4=B}'', for example, generates a compiler error.\nYou must solve such equations yourself to calculate the desired\nnew value of a single variable, such as ``{  A=B+4}.''\nFor the purposes of our class, you must always write a single variable \non the left side of an assignment, and can write an arbitrary expression \non the right side.\n\nMany operators can be combined into a single expression.  When an\nexpression has more than one operator, which operator is executed first?\nThe answer depends on the operators' { precedence}, a well-defined order on\noperators that specifies how to resolve the ambiguity.  In the case\nof arithmetic, the C language's precedence specification matches the\none that you learned in elementary school.  For example, { 1+2*3}\nevaluates to 7, not to 9, because multiplication has precedence over\naddition.  For non-arithmetic operators, or for any case in which\nyou do not know the precedence specification for a language, {\ndo not look it up---other programmers will not remember the\nprecedence ordering, either!}  Instead, add parentheses to make your \nexpressions clear and easy to understand.\n\n\n \n Question: What is the purpose of an expression? \n Answer: \n\nA. The purpose of an expression is to represent a calculation, such as an arithmetic operation, a comparison, or a logic operation."}, {"text": "Title: Basic I/O \n Text: {Basic I/O}\n\nThe { main} function returns an integer to the operating system.\nAlthough we do not discuss how additional functions can be written\nin our class, we may sometimes make use of functions that have been\nwritten in advance by making { calls} to those functions.\nA { function call} is type of expression in C, but we leave \nfurther description for ECE 220.  In our class, we make use of only\ntwo additional functions to enable our programs to receive input\nfrom a user via the keyboard and to write output to the monitor for \na user to read.\n\nLet's start with output.  The { printf} function allows a program\nto print output to the monitor using a programmer-specific format.\nThe ``f'' in { printf} stands for ``formatted.''{The \noriginal, unformatted variant of printing was never available\nin the C language.  Go learn Fortran.}\n\nWhen we want to use { printf}, we write a expression with\nthe word { printf} followed by a parenthesized, comma-separated\nlist of expressions.  The expressions in this list are called\nthe { arguments} to the { printf} function.\n\nThe first argument to the { printf} function is a format string---a \nsequence of ASCII characters between quotation marks---which tells \nthe function what kind of information we want printed to\nthe monitor as well as how to format that information.\n\nThe remaining arguments are C expressions that give { printf}\na copy of any values that we want printed.\n\nHow does the format string specify the format?\n\nMost of the characters in the format string are simply printed to \nthe monitor.  \n\nIn the first example shown to on the next page, we use { printf}\nto print a hello message followed by an ASCII newline character\nto move to the next line on the monitor.\n\n\nThe percent sign---``''---is used \nas an { escape character} in the\n{ printf} function.  When ``'' appears in the format\nstring, the function examines the next character in the format string\nto determine which format to use, then takes\nthe next expression from the sequence\nof arguments and prints the value of that expression to the \nmonitor.  Evaluating an expression generates a bunch of bits, so it is up to\nthe programmer to ensure that those bits are not misinterpreted.\nIn other words, the programmer must make sure that the number and\ntypes of formatted values match the number and types of arguments passed\nto { printf} (not counting the format string itself).\n\nThe { printf} function returns the number of characters printed\nto the monitor.\n\n\n\noutput: =\n> { printf (\"Hello, world!n\");}\noutput: > { Hello, world!} [and a newline]\n\n> { printf (\"To x or not to d...n\", 190, 380 / 2);}\noutput: > { To be or not to 190...} [and a newline]\n\n> { printf (\"My favorite number is cc.n\", 0x34, '0'+2);}\noutput: > { My favorite number is 42.} [and a newline]\n\n> { printf (\"What is pi?  f or e?n\", 3.1416, 3.1416);}\noutput: > { What is pi?  3.141600 or 3.141600e+00?} [and a newline]\n\n\n{|c|l|}\nescape  &                         \nsequence& { printf} function's interpretation of expression bits \n{ c}& 2's complement integer printed as an ASCII character\n{ d}& 2's complement integer printed as decimal\n{ e}& double printed in decimal scientific notation\n{ f}& double printed in decimal\n{ u}& unsigned integer printed as decimal\n{ x}& integer printed as hexadecimal (lower case)\n{ X}& integer printed as hexadecimal (upper case)\n{ }& a single percent sign \n\n\n\n\nA program can read input from the user with the { scanf} function.\nThe user enters characters in ASCII using the keyboard, and the\n{ scanf} function converts the user's input into C primitive types,\nstoring the results into variables.  As with { printf}, the\n{ scanf} function takes a format string followed by a comma-separated\nlist of arguments.  Each argument after the format string provides\n{ scanf} with the memory address of a variable into which the\nfunction can store a result.\n\nHow does { scanf} use the format string?\n\nFor { scanf}, the format string is usually just a sequence\nof conversions, one for each variable to be typed in by the user.\nAs with { printf}, the conversions start with ``'' and\nare followed by characters specifying the type of conversion\nto be performed.  The first example shown to the right reads\ntwo integers.\n\nThe conversions in the format string can be separated by spaces \nfor readability, as shown in the exam-\n\n\n\neffect: =unsigned =\n> { int     } > { a, b;  /* example variables */}\n> { char    } > { c;}\n> { unsigned} > { u;}\n> { double  } > { d;}\n> { float   } > { f;}\n\n> { scanf (\"dd\", &a, &b);   /* These have the */}\n> { scanf (\"d d\", &a, &b);  /* same effect.   */}\neffect: > try to convert two integers typed in decimal to\n> 2's complement and store the results in { a} and { b}\n\n> { scanf (\"cx lf\", &c, &u, &d);}\neffect: > try to read an ASCII character into { c}, a value\n> typed in hexadecimal into { u}, and a double-\n> precision > floating-point number into { d}\n\n> { scanf (\"lf f\", &d, &f);}\neffect: > try to read two real numbers typed as decimal,\n> convert the first to double-precision and store it \n> in { d}, and convert the second to single-precision \n> and store it in { f}\n\n\n\n\nple.  The spaces are ignored\nby { scanf}.  However, { any non-space characters in the\nformat string must be typed exactly by the user!}\n\nThe remaining arguments to { scanf} specify memory addresses\nwhere the function can store the converted values.  \n\nThe ampersand (``&'') in front of each variable name in the examples is an\noperator that returns the address of a variable in memory.\n\nFor each con-\n\n\n{|c|l|}\nescape  &                         \nsequence& { scanf} function's conversion to bits \n{ c}& store one ASCII character (as { char})\n{ d}& convert decimal integer to 2's complement\n{ f}& convert decimal real number to float\n{ lf}& convert decimal real number to double\n{ u}& convert decimal integer to unsigned int\n{ x}& convert hexadecimal integer to unsigned int\n{ X}& (as above) \n\n\n\nversion\nin the format string, the { scanf} function tries to convert\ninput from the user into the appropriate result, then stores the\nresult in memory at the address given by the next argument.\n\nThe programmer is responsible for ensuring that the number of \nconversions in the format string\nmatches the number of arguments provided (not counting\nthe format string itself).  The programmer must also ensure that\nthe type of information produced by each conversion can be\nstored at the address passed for that conversion---in other words,\nthe address of a\nvariable with the correct type must be\nprovided.  Modern compilers often detect missing { &} operators\nand incorrect variable types, but many only give warnings to the\nprogrammer.  The { scanf} function itself cannot tell whether\nthe arguments given to it are valid or not.\n\nIf a conversion fails---for example, if a user types ``hello'' when\n{ scanf} expects an integer---{ scanf} does not overwrite the\ncorresponding variable and immediately stops trying to convert input.\n\nThe { scanf} function returns the number of successful \nconversions, allowing a programmer to check for bad input from\nthe user.\n\n \n Question: What is the & operator used to get the address of a variable? \n Answer: \n\nThe & operator is used to get the address of a variable. This is necessary because scanf needs to know where to store the result of the conversion."}, {"text": "Title: Types of Statements in C \n Text: {Types of Statements in C}\n\nEach statement in a C program specifies a complete operation.\n\nThere are three types of statements, but two of these types can\nbe constructed from additional statements, which can in turn be\nconstructed from additional statements.  The C language specifies\nno bound on this type of recursive construction, but code \nreadability does impose a practical limit.\n\n\nThe three types are shown to the right.\nThey are the { null statement}, \n{ simple statements}, \nand { compound statements}.\n\nA null statement is just a semicolon, and a compound statement \nis just a sequence of statements surrounded by braces.\n\nSimple statements can take several forms.  All of the examples\nshown to the right, including the call to { printf}, are\nsimple state-\n\n\n{\n\naaaa=aaaaaaaaaaaaaa=/* =aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa=\n;   > > /* >a null statement (does nothing) >*/\n\nA = B; > > /*  >examples of simple statements >*/\nprintf (\"Hello, world!n\");\n\n{    > > /* > a compound statement >*/ \n>  C = D; > /* > (a sequence of statements >*/\n>  N = 4; > /* > between braces) >*/ \n>  L = D - N;\n}\n\n}\n{-2pt}\n\n\nments consisting of a C expression followed by a \nsemicolon.\n\nSimple statements can also consist of conditionals or iterations, which\nwe introduce next.\n\nRemember that after variable declarations, the { main} function\ncontains a sequence of statements.  These statements are executed one\nat a time in the order given in the program, as shown to the right\nfor two statements.  We say that the statements are executed in\nsequential order.\n\nA program must also be able to execute statements only when \nsome condition holds.  In the C language, such a condition can be\nan arbitrary expression.  The expression is first evaluated.\nIf the result is 0, the condition\nis considered to be false.  Any result other than 0 is considered\nto be true.  The C statement\nfor conditional execution is called an { if}\n\n\n{file=part1/figs/part1-sd-sequential.eps,width=0.8in}\n\n\n\nstatement.  Syntactically, we put the expression for the condition\nin parentheses after the keyword { if} and follow the parenthesized\nexpression with a compound statement containing the statements\nthat should be executed when the condition is true.  Optionally,\nwe can append the keyword { else} and a second compound\nstatement containing statements to be executed when the condition\nevaluates to false.  \nThe corresponding flow chart is shown to the right.\n\n{\n\naaaa=aaaaaaaaaaaa=\n/* Set the variable y to the absolute value of variable x. */\nif (0 <= x) {> >/* Is x greater or equal to 0? */\n> y = x;     >/* Then block: assign x to y. */\n} else {\n> y = -x;    >/* Else block: assign negative x to y. */\n}\n\n}\n\n\n{file=part1/figs/part1-sd-conditional.eps,width=2in}\n\n\nIf instead we chose to assign the absolute value of variable { x}\nto itself, we can do so without an { else} block:\n\n{\n\naaaa=aaaaaaaaaaaa=\n/* Set the variable x to its absolute value. */\nif (0 > x) {> >/* Is x less than 0? */\n> x = -x;    >/* Then block: assign negative x to x. */\n}            >> /* No else block is given--no work is needed. */\n\n}\n\n\nFinally, we sometimes need to repeatedly execute a set of statements,\neither a fixed number of times or so long as some condition holds.\nWe refer to such repetition as an { iteration} or a { loop}.\nIn our class, we make use of C's { for} loop when we need to\nperform such a task.  A { for} loop is structured as follows:\n\n{ \n\nfor ([initialization] ; [condition] ; [update]) {\n    [subtask to be repeated]\n}\n\n}\n\nA flow chart corresponding to execution of a { for} loop appears\nto the right.  First, any initialization is performed.  Then the\ncondition---again an arbitrary C expression---is checked.  If the\ncondition evaluates to false (exactly 0), the loop is done.  Otherwise,\nif the condition evaluates to true (any non-zero value),\nthe statements in the compound statement, the subtask or { loop body},\nare executed.  The loop body can contain anything: a sequence of simple \nstatements, a conditional, another loop, or even just an empty list.\nOnce the loop body has finished executing, the { for} loop\nupdate rule is executed.  Execution then checks the condition again,\nand this process repeats until the condition evaluates to 0.\nThe { for} loop below, for example, prints the numbers \nfrom 1 to 42.\n\n{ \n\n/* Print the numbers from 1 to 42. */\nfor (i = 1; 42 >= i; i = i + 1) {\n    printf (\"dn\", i);\n}\n\n}\n\n\n{file=part1/figs/part1-sd-iterative.eps,width=1.35in}\n\n\n\n \n Question: What is a simple statement? \n Answer: \nThe type of statement that consists of a C expression followed by a semicolon is a simple statement."}, {"text": "Title: Program Execution \n Text: {Program Execution}\n\n\nWe are now ready to consider the execution of a simple program,\nillustrating how variables change value from step to step and\ndetermine program behavior.\n\nLet's say that two numbers are ``friends'' if they have at least one\n1 bit in common when written in base 2.  So, for example, 100_2 and \n111_2 are friends because both numbers have a 1 in the bit with \nplace value 2^2=4.  Similarly, 101_2 and 010_2 are not friends,\nsince no bit position is 1 in both numbers.\n\nThe program to the right prints all friendships between numbers\nin the interval [0,7].\n\n\n{\n\naaaa=aaaa=aaaaaaaaaa=/* a second number to consider as check's friend =\nint\nmain ()\n{\n>  int > check;  > /* number to check for friends > */\n>  int > friend; > /* a second number to consider as check's friend > */\naaaa=aaaa=aaaa=aaaa=\n>  \n>  /* Consider values of check from 0 to 7. */\n>  for (check = 0; 8 > check; check = check + 1) {\n\n>  >  /* Consider values of friend from 0 to 7. */\n>  >  for (friend = 0; 8 > friend; friend = friend + 1) {\n\n>  >  >  /* Use bitwise AND to see if the two share a 1 bit. */\n>  >  >  if (0 != (check & friend)) {\n\n>  >  >  >  /* We have friendship! */\n>  >  >  >  printf (\"d and d are friends.n\", check, friend);\n>  >  >  }\n>  >  }\n>  }\n}\n\n}\n\n\nThe program uses two\ninteger variables, one for each of the numbers that we consider.\nWe use a { for} loop to iterate over all values of our first\nnumber, which we call { check}.  The loop initializes { check}\nto 0, continues until check reaches 8, and adds 1 to check after\neach loop iteration.  We use a similar { for} loop to iterate\nover all possible values of our second number, which we call { friend}.\nFor each pair of numbers, we determine whether they are friends\nusing a bitwise AND operation.  If the result is non-zero, they\nare friends, and we print a message.  If the two numbers are not\nfriends, we do nothing, and the program moves on to consider the\nnext pair of numbers.\n\n\nNow let's think about what happens when this program executes.\n\nWhen the program starts, both variables are filled with random bits,\nso their values are unpredictable.  \n\nThe first step is the initialization of the first { for} loop, which\nsets { check} to 0.\n\nThe condition for that loop is { 8 > check}, which is true, so\nexecution enters the loop body and starts to execute the first\nstatement, which is our second { for} loop.\n\nThe next step is then the initialization code for the second { for}\nloop, which sets { friend} to 0.\n\nThe condition for the second loop is { 8 > friend}, which is true, so\nexecution enters the loop body and starts to execute the first\nstatement, which\n\n\n\nafter executing...& { check} is...& and { friend} is... \n(variable declarations)& unpredictable bits& unpredictable bits\n{ check = 0}& 0& unpredictable bits\n{ 8 > check}& 0& unpredictable bits\n{ friend = 0}& 0& 0\n{ 8 > friend}& 0& 0\n{ if (0 != (check & friend))}& 0& 0\n{ friend = friend + 1}& 0& 1\n{ 8 > friend}& 0& 1\n{ if (0 != (check & friend))}& 0& 1\n{ friend = friend + 1}& 0& 2\n{(repeat last three lines six more times; number 0 has no friends!)}\n{ 8 > friend}& 0& 8\n{ check = check + 1}& 1& 8\n{ 8 > check}& 1& 8\n{ friend = 0}& 1& 0\n{ 8 > friend}& 1& 0\n{ if (0 != (check & friend))}& 1& 0\n{ friend = friend + 1}& 1& 1\n{ 8 > friend}& 1& 1\n{ if (0 != (check & friend))}& 1& 1\n{ printf ...}& 1& 1\n{(our first friend!?)}\n\n\n\nis the { if} statement.\n\nSince both variables are 0, the { if} condition is false, and\nnothing is printed.\n\nHaving finished the loop body for the inner loop (on { friend}),\nexecution continues with the update rule for that loop---{ friend = \nfriend + 1}---then returns to check the loop's condition again.\n\nThis process repeats, always finding that the number 0 (in { check})\nis not\nfriends (0 has no friends!) until { friend} reaches 8, at which\npoint the inner loop condition becomes false.\n\nExecution then moves to the update rule for the first { for} loop,\nwhich increments { check}.  Check is then compared with 8 to\nsee if the loop is done.  Since it is not, we once again enter the\nloop body and start the second { for} loop over.  The initialization\ncode again sets { friend} to 0, and we move forward as before.\nAs you see above, the first time that we find our { if} condition\nto be true is when both { check} and { friend} are equal to 1.\n\nIs that result what you expected?  To learn that the number 1 is\nfriends with itself?  If so, the program works.  If you assumed that\nnumbers could not be friends with themselves, perhaps we should fix the \nbug?  We could, for example, add another { if} statement to \navoid printing anything when { check == friend}.\n\nOur program, you might also realize, prints each pair of friends twice.\nThe numbers 1 and 3, for example, are printed in both possible orders.  To\neliminate this redundancy, we can change the initialization in the \nsecond { for} loop, either to { friend = check} or to\n{ friend = check + 1}, depending on how we want to define friendship\n(the same question as before: can a number be friends with itself?).\n\n\n\n \n Question: What is the question that we want to answer about friendship? \n Answer: \nCan a number be friends with itself?\n\nYes, a number can be friends with itself."}, {"text": "Title: Compilation and Interpretation* \n Text: {Compilation and Interpretation*}\n\n\nMany programming languages, including C, can be \n{ compiled}, which means that the program is converted into \ninstructions for a particular ISA before the program is run\non a processor that supports that ISA.\nThe figure to the right illustrates the compilation process for \nthe C language.  \n\nIn this type of figure, files and other data are represented as cylinders,\nwhile rectangles represent processes, which are usually implemented in \nsoftware.\n\nIn the figure to the right, the outer dotted box represents the full \ncompilation\nprocess that typically occurs when one compiles a C program.\nThe inner dotted box represents the work performed by the { compiler}\nsoftware itself.\n\nThe cylinders for data passed between the processes that compose\nthe full compilation process\nhave been left out of the figure; instead, we have written the type\nof data being passed next to the arrows that indicate the flow of information\nfrom one process to the next.\n\nThe C preprocessor (described later in these notes) forms the\nfirst step in the compilation process.  The preprocessor\noperates on the program's { source code} along\nwith { header files} that describe data types and\noperations.  The preprocessor merges these together\ninto a single file of preprocessed source code.  The preprocessed\nsource code is then analyzed by the front end of the compiler based on the\nspecific programming language being used (in our case, the C language),\nthen converted by the back end of the compiler\ninto instructions for the desired ISA.  The output of a compiler\nis not binary instructions, however, but is instead\na human-readable form of instructions called { assembly code},\nwhich we cover in the last quarter of our class.  A tool called\nan assembler then converts these human-readable instructions into\nbits that a processor can understand.  If a program consists of\nmulti-\n\n\n{file=part1/figs/part1-compiler.eps,width=3in}\n\n\nple source files, or needs to make use of additional \npre-programmed operations (such as math functions, graphics, or sound),\na tool called a linker merges the object code of the program with\nthose additional elements to form the final { executable image}\nfor the program.  The executable image is typically then stored on\na disk, from which it can later be read into memory in order to\nallow a processor to execute the program.\n\nSome languages are difficult or even impossible to compile.  Typically, the\nbehavior of these languages depends on input data that are only available \nwhen the program runs.  Such languages can be { interpreted}: each step \nof the algorithm described by a program is executed by a software interpreter\nfor the language.  Languages such as Java, Perl, and Python are usually\ninterpreted.  Similarly, when we use software to simulate one ISA using\nanother ISA, as we do at the end of our class with the {LC-3}\nISA described by the textbook, the simulator is a form of interpreter.\nIn the lab, you will use a simulator compiled into and executing as x86 \ninstructions in order to interpret {LC-3} instructions.  \n\nWhile a program is executing in an interpreter, enough information\nis sometimes available to compile part or all of the program to\nthe processor's ISA as the program runs, \na technique known as { ``just in time'' ( JIT) compilation}.\n\n\n\n\n \n Question: What is the output of a compiler? \n Answer: \n\nA. The compiler converts the human-readable instructions in the source code into machine code, which is a set of instructions that can be executed by a processor. The machine code is then stored in an executable image, which can be run on a computer."}, {"text": "Title: The C Preprocessor* \n Text: {The C Preprocessor*}\n\nThe C language uses a preprocessor to support inclusion of common\ninformation (stored in header files) into multiple source files.\n\nThe most frequent use of the preprocessor is to enable the unique\ndefinition of new data types and operations within\nheader files that can then be included by reference within source\nfiles that make use of them.  This capability is based on the \n{ include directive}, { #include}, as shown here:  \n\n{\n\n\n#include \"my_header.h\"    = /* search in current followed by standard directories =\n#include <stdio.h>      > /* search in standard directories > */\n#include \"my_header.h\" > /* search in current followed by standard directories > */\n\n\n}\n\nThe preprocessor also supports integration of compile-time constants\ninto source files before compilation.  For example, many\nsoftware systems allow the definition of a symbol such as { NDEBUG}\n(no debug) to compile without additional debugging code included in\nthe sources.  \n\nTwo directives are necessary for this purpose: the { define directive},\n{ #define}, which\nprovides a text-replacement facility, and { conditional inclusion} (or\nexclusion) of parts of a file within { #if}/{ #else}/{\n#endif} directives.\n\nThese directives are also useful in allowing\n\n\na single header file to\nbe included multiple times without causing problems, as C does not\nallow redefinition of types, variables, and so forth, even if the\nredundant \ndefinitions are identical.  Most header files are thus wrapped as shown\nto the right.\n\n\n{\n\n#if !defined(MY_HEADER_H)\n#define MY_HEADER_H\n/* actual header file material goes here */\n#endif /* MY_HEADER_H */\n\n}\n\n\nThe preprocessor performs a simple linear pass on the source and does\nnot parse or interpret any C syntax.\n\nDefinitions for text replacement are valid as soon as they are defined\nand are performed until they are undefined or until the end of the\noriginal source file.\n\nThe preprocessor does recognize spacing and will not replace part of a\nword, thus ``{ #define i 5}'' will not wreak havoc on your {\nif} statements, but will cause problems if you name any variable { i}.\n\nUsing the text replacement capabilities of the preprocessor does have\ndrawbacks, most importantly in that almost none of the information is\npassed on for debugging purposes.  \n\n \n Question: What is the name of the directive that allows for the inclusion of common information into multiple source files? \n Answer: \n\nA. The name of the directive that allows for the inclusion of common information into multiple source files is the include directive."}, {"text": "Title: Changing Types in C* \n Text: {Changing Types in C*}\n\nChanging the type of a datum is necessary from time to time, but\nsometimes a compiler can do the work for you.\n\nThe most common form of { implicit type conversion} occurs with binary\narithmetic operations.  Integer arithmetic in C always uses types of\nat least the size of { int}, and all floating-point arithmetic uses\n{ double}.\n\nIf either or both operands have smaller integer types, or differ from\none another, the compiler implicitly converts them before performing\nthe operation, and the type of the result may be different from those of\nboth operands.\n\nIn general, the compiler selects the final type according to some\npreferred ordering in which floating-point is preferred over integers,\nunsigned values are preferred over signed values, and more bits are\npreferred over fewer bits.\n\nThe type of the result must be at least as large as either argument,\nbut is also at least as large as an { int} for integer operations\nand a { double} for floating-point operations.\n\nModern C compilers always extend an integer type's bit width before\nconverting from signed to unsigned.  The original C specification\ninterleaved bit width extensions to { int} with sign changes, thus\n{ older compilers may not be consistent, and implicitly require\nboth types of conversion in a single operation may lead to portability\nbugs.}\n\nThe implicit extension to { int} can also be confusing in the sense\nthat arithmetic that seems to work on smaller integers fails with\nlarger ones.  For example, multiplying two 16-bit integers set to 1000\nand printing the result works with most compilers because the 32-bit \n{ int} result is wide enough to hold the right answer.  In contrast,\nmultiplying two 32-bit integers set to 100,000 produces the wrong\nresult because the high bits of the result are discarded before it can\nbe converted to a larger type.  For this operation to produce the\ncorrect result, one of the integers must be converted explicitly (as\ndiscussed later) before the multiplication.\n\n\n\nImplicit type conversions also occur due to assignments.  Unlike\narithmetic conversions, the final type must match the left-hand side\nof the assignment (for example, a variable to which a result is assigned), and\nthe compiler simply performs any necessary conversion.\n\n{ Since the desired type may be smaller than the type of the value\nassigned, information can be lost.}  Floating-point values are\ntruncated when assigned to integers, and high bits of wider integer\ntypes are discarded when assigned to narrower integer types.  { Note\nthat a positive number may become a negative number when bits are\ndiscarded in this manner.}\n\nPassing arguments to functions can be viewed as a special case of\nassignment.  Given a function prototype, the compiler knows the type\nof each argument and can perform conversions as part of the code\ngenerated to pass the arguments to the function.  Without such a\nprototype, or for functions with variable numbers of arguments, the\ncompiler lacks type information and thus cannot perform necessary\nconversions, leading to unpredictable behavior.  By default, however,\nthe compiler extends any integer smaller than an { int}\nto the width of an { int} and converts { float} to\n{ double}.\n\n\nOccasionally it is convenient to use an { explicit type cast} to force\nconversion from one type to another.  { Such casts must be used\nwith caution, as they silence many of the warnings that a compiler\nmight otherwise generate when it detects potential problems.}  One\ncommon use is to promote integers to floating-point before an\narithmetic operation, as shown to the right.\n\n\n{\n\naaaa=\nint\nmain ()\n{\n>  int numerator = 10;\n>  int denominator = 20;\n>\n>  printf (\"fn\", numerator / (double)denominator);\n>  return 0;\n}\n\n}\n{-14pt}\n\nThe type to which a value is to be converted\nis placed in parentheses in front of the value.  In most cases,\nadditional parentheses should be used to avoid confusion about the\nprecedence of type conversion over other operations.\n\n\n\n\n\n \n Question: What is the most common form of implicit type conversion? \n Answer: \n\nA. The most common form of implicit type conversion is with binary arithmetic operations."}, {"text": "Title: Summary of Part 1 of the Course \n Text: {Summary of Part 1 of the Course}\n\nThis short summary provides a list of both terms that we expect you to\nknow and and skills that we expect you to have after our first few weeks\ntogether.  The first part of the course is shorter than the other three\nparts, so the amount of material is necessarily less.\n\nThese notes supplement the Patt and Patel textbook, so you will also \nneed to read and understand the relevant chapters (see the syllabus)\nin order to master this material completely.\n\nAccording to educational theory, the difficulty of learning depends on \nthe type of task involved.  Remembering new terminology is relatively \neasy, while applying the ideas underlying design decisions shown by \nexample to new problems posed as human tasks is relatively hard.\n\nIn this short summary, we give you lists at several levels of difficulty\nof what we expect you to be able to do as a result of the last few weeks\nof studying (reading, listening, doing homework, discussing your\nunderstanding with your classmates, and so forth).\n\nThis time, we'll list the skills first and leave the easy stuff for the \nnext page.\n\nWe expect you to be able to exercise the following skills:\n\n{}{{}{}\n{}{}{}\n\n{Represent decimal numbers with unsigned, 2's complement, and IEEE\nfloating-point representations, and be able to calculate the decimal value\nrepresented by a bit pattern in any of these representations.}\n\n{Be able to negate a number represented in the 2's complement\nrepresentation.}\n\n{Perform simple arithmetic by hand on unsigned and 2's complement\nrepresentations, and identify when overflow occurs.}\n\n{Be able to write a truth table for a Boolean expression.}\n\n{Be able to write a Boolean expression as a sum of minterms.}\n\n MOVED TO PART 4\n\n {Be able to calculate the Hamming distance of a code/representation.}\n \n {Know the relationships between Hamming distance and the abilities\n to detect and to correct bit errors.}\n\n{Know how to declare and initialize C variables with one of the \nprimitive data types.}\n\n\n\nAt a more abstract level, we expect you to be able to:\n\n{}{{}{}\n{}{}{}\n\n{Understand the value of using a common mathematical basis, such\nas modular arithmetic, in defining multiple representations (such as\nunsigned and 2's complement).}\n\n{Write Boolean expressions for the overflow conditions\non both unsigned and 2's complement addition.}\n\n MOVED TO PART 4\n\n {Be able to use parity for error detection, and Hamming codes for\n error correction.}\n\n{Be able to write single { if} statements and { for} loops\nin C in order to perform computation.}\n\n{Be able to use { scanf} and { printf} for basic input and \noutput in C.}\n\n\n\nAnd, at the highest level, we expect that you will be able to reason about\nand analyze problems in the following ways:\n\n{}{{}{}\n{}{}{}\n\n{Understand the tradeoffs between integer\n  FIXME?     not covered by book nor notes currently \n, fixed-point,    \nand floating-point representations for numbers.}\n\n{Understand logical completeness and be able to prove or disprove\nlogical completeness for sets of logic functions.}\n\n PARTIALLY MOVED TO PART 4\n\n {Understand the properties necessary in a representation, and understand\n basic tradeoffs in the sparsity of code words with error detection and\n correction capabilities.}\n{Understand the properties necessary in a representation: no ambiguity\nin meaning for any bit pattern, and agreement in advance on the meanings of \nall bit patterns.}\n\n{Analyze a simple, single-function C program and be able to explain its purpose.}\n\n\n\n\n\n\n\nYou should recognize all of these terms\nand be able to explain what they mean.\n(the parentheses give page numbers,\nor ``P&P'' for Patt & Patel).\n\nActually, we don't care whether you can draw something from memory---a full\nadder, for example---provided that you know what a full adder does and can\nderive a gate diagram correctly for one in a few minutes.  Higher-level\nskills are much more valuable.  (You may skip the *'d terms in Fall 2012.)\n\nNote that we are not saying that you should, for example, be able to \nwrite down the ASCII representation from memory.  In that example, \nknowing that it is a {7-bit} representation used for English\ntext is sufficient.  You can always look up the detailed definition \nin practice.\n\n[t]\n{}{{}{}\n{}{}{}\n\n{universal computational devices /  computing machines ()\n{--}{{}{}\n{}{}{}\n undecidable ()\n the halting problem ()\n\n}\n\n pre-Fall 2015 version\n\n {Turing machines\n {--}{{}{}\n {}{}{}\n  universal computational device/ computing machine\n  intractable/undecidable\n  the halting problem\n \n }\n\n{information storage in computers\n{--}{{}{}\n{}{}{}\n bits ()\n representation (P&P)\n data type ()\n unsigned representation ()\n 2's complement representation\n\n FIXME?  not covered by book nor notes currently\n  fixed-point representation\n\n IEEE floating-point representation\n ASCII representation\n equivalence classes\n\n}\n\n{operations on bits\n{--}{{}{}\n{}{}{}\n 1's complement operation\n carry (from addition)\n overflow (on any operation) ()\n Boolean logic and algebra\n logic functions/gates\n truth table\n AND/conjunction\n OR/disjunction\n NOT/logical complement/ (logical) negation/inverter\n XOR\n logical completeness\n minterm\n\n}\n\n{mathematical terms\n{--}{{}{}\n{}{}{}\n modular arithmetic\n implication\n contrapositive\n proof approaches: by construction, by contradiction, by induction\n without loss of generality (w.l.o.g.)\n\n}\n\n MOVED TO PART 4\n\n {error detection and correction\n {--}{{}{}\n {}{}{}\n  code/sparse representation\n  code word\n  bit error\n  odd/even parity bit\n  Hamming distance between code words\n  Hamming distance of a code\n  Hamming code\n  SEC-DED\n \n }\n\n\n\n[t]\n{}{{}{}\n{}{}{}\n\n{high-level language concepts\n{--}{{}{}\n{}{}{}\n syntax\n{variables\n{--}{{}{}\n{}{}{}\n declaration\n primitive data types\n symbolic name/identifier\n initialization\n\n}\n FIXME?  really not necessary for them\n strongly typed languages\n expression\n statement\n\n}\n\n{C operators\n{--}{{}{}\n{}{}{}\n operands\n arithmetic\n bitwise\n comparison/relational\n assignment\n address\n arithmetic shift\n logical shift\n precedence\n\n}\n\n{functions in C\n{--}{{}{}\n{}{}{}\n { main}\n function call\n arguments\n {{ printf} and { scanf}\n{--}{{}{}\n{}{}{}\n format string\n escape character\n\n}\n { sizeof} (built-in)\n\n}\n\n{transforming tasks into programs\n{--}{{}{}\n{}{}{}\n flow chart\n sequential construct\n conditional construct\n iterative construct/iteration/loop\n loop body\n\n}\n\n{C statements\n{--}{{}{}\n{}{}{}\n statement: null, simple, compound\n { if} statement\n { for} loop\n { return} statement\n\n}\n\n THESE ARE NOT REQUIRED TOPICS\n\n {execution of C programs\n {--}{{}{}\n {}{}{}\n  compiler/interpreter\n  source code\n  header files\n  assembly code\n  instructions\n  executable image\n \n }\n \n {the C preprocessor\n {--}{{}{}\n {}{}{}\n  #include directive\n  #define directive\n \n }\n\n\n\n\n\n\n{   }   blank 3rd page\n\n\n\n\n\n \n Question: What is a universal computational device? \n Answer: \n\nA. A universal computational device is a device that can perform any computable task."}, {"text": "Title: Overflow Conditions \n Text: {Overflow Conditions}\n\nThis set of notes discusses the overflow conditions for unsigned and\n2's complement addition.  For both types, we formally prove that\nthe conditions that we state are correct.  Many of our faculty want our\nstudents to learn to construct formal proofs, so we plan to begin\nexposing you to this process in our classes.\n\nProf. Lumetta is a fan of Prof. George Polya's educational theories\nwith regard to proof techniques, and in particular the idea that one\nbuilds up a repertoire of approaches by seeing the approaches used \nin practice.\n\n , but teaching proof\n techniques effectively is challenging, particularly when exercises\n are of the form, ``Prove that <insert true theorem> is true'' rather\n than the more open-ended form that we typically encounter in research,\n ``Prove that <insert some conjecture> is true, or find a \n counterexample.''\n\n \n Question: What is the overflow condition for unsigned and 2's complement addition? \n Answer: \n\nA. An unsigned overflow occurs when the result of an unsigned addition operation is too large to be represented within the allotted space."}, {"text": "Title: Implication and Mathematical Notation \n Text: {Implication and Mathematical Notation}\n\nSome of you may not have been exposed to basics of mathematical logic, so\nlet's start with a brief introduction to implication.  We'll use \nvariables p and q to represent statements that can be either true\nor false.  For example, p might represent the statement, ``Jan is\nan ECE student,'' while q might represent the statement, ``Jan\nworks hard.''  The { logical complement} or { negation} of \na statement p,\nwritten for example as ``not p,'' has the opposite truth value:\nif p is true, not p is false, and if p is false, not p is\ntrue.\n\nAn { implication} is a logical relationship between two statements.\nThe implication itself is also a logical statement, and may be true or\nfalse.  In English, for example, we might say, ``If p, q.''\nIn mathematics, the same implication is usually written as either \n``q if p'' or ``p,'' and the latter is read \nas, ``p implies q.''  \n\nUsing our example values for p and q, we can see that\np is true: ``Jan is an ECE student'' does\nin fact imply that ``Jan works hard!''\n\nThe implication p \nis only considered false if p is true and q is false.\nIn all other cases, the implication is true.\nThis definition can be a little confusing at first, so let's use\nanother example to see why.\n\nLet  p represent the statement\n``Entity X is a flying pig,'' and let q represent \nthe statement, ``Entity X obeys air traffic control regulations.''\n\nHere the implication p is again true: \nflying pigs do not exist, so p is false, and thus \n``p'' is true---for any value of statement q!\n\nGiven an implication ``p,'' we say that the {\nconverse} of the implication is the statement \n``q,'' which is also an implication.\nIn mathematics, the converse of \np\nis sometimes written\nas ``q only if p.''  The converse of an implication may or may not have\nthe same truth value as the implication itself.  Finally,\nwe frequently use the shorthand notation, ``p if and only if q,''\n(or, even shorter, ``p iff q'') to mean \n``p { and}\nq.'' This last statement is true only when both\nimplications are true.\n\n \n Question: Why is the converse of an implication not always true? \n Answer: \n\nThe reason that the converse of an implication is not always true is that the implication may be false."}, {"text": "Title: Overflow for Unsigned Addition \n Text: {Overflow for Unsigned Addition}\n\nLet's say that we add two {N-bit} unsigned numbers, A\nand B.  The {N-bit} unsigned representation \ncan represent integers in the range [0,2^N-1].\n\nRecall that we say that the addition operation has \noverflowed if the number represented by the {N-bit} pattern\nproduced for the sum does not actually represent the number A+B.\n\nFor clarity, let's name the bits of A by writing the number\nas a_{N-1}a_{N-2}...a_1a_0.  Similarly, let's write B as\nb_{N-1}b_{N-2}...b_1b_0.  Name the sum C=A+B.  The sum that\ncomes out of the add unit has only N bits, but recall that\nwe claimed in class that the overflow condition for unsigned \naddition is given by the { carry} out of the most significant\nbit.  So let's write the sum as \nc_c_{N-1}c_{N-2}...c_1c_0, realizing that c_N is the\ncarry out and not actually part of the sum produced by the \nadd unit.\n\n{ Theorem:}\n\nAddition of two {N-bit} unsigned numbers\nA=a_{N-1}a_{N-2}...a_1a_0\nand\nB=b_{N-1}b_{N-2}...b_1b_0\nto produce sum\nC=A+B=c_c_{N-1}c_{N-2}...c_1c_0,\noverflows if and only if\nthe carry out c_N of the addition is a 1 bit.\n\n\n\n{ Proof:}\n\nLet's start with the ``if'' direction.  In other words, c_N=1 implies\noverflow.  Recall that unsigned addition is the same as base 2 addition,\nexcept that we discard bits beyond c_{N-1} from the sum C.\nThe bit c_N has place value 2^N, so, when c_N=1 we can write that \nthe correct sum C{2^N}.  But no value that large can be represented\nusing the {N-bit} unsigned representation, so we have an overflow.\n\nThe other direction (``only if'') is slightly more complex: we need to\nshow that overflow implies that c_N=1.  We use a range-based argument\nfor this purpose.  Overflow means that the sum C is outside the\nrepresentable range [0,2^N-1].  Adding two non-negative numbers cannot\nproduce a negative number, so the sum can't be smaller than 0.  Overflow \nthus implies that C{2^N}.\n\nDoes that argument complete the proof?  No, because some numbers, such \nas 2^{N+1}, are larger than 2^N, but do not have a 1 bit in the Nth\nposition when written in binary.  We need to make use of the constraints\non A and B implied by the possible range of the representation.\n\nIn particular, given that A and B are represented as {N-bit}\nunsigned values, we can write\n\n{eqnarray*}\n0  & A &  2^N - 1\n0  & B &  2^N - 1\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n0  & C &  2^{N + 1} - 2\n{eqnarray*}\n\nCombining the new inequality with the one implied by the overflow \ncondition, we obtain\n\n{eqnarray*}\n2^N  & C &  2^{N + 1} - 2\n{eqnarray*}\n\nAll of the numbers in the range allowed by this inequality have c_N=1,\ncompleting our proof.\n\n \n Question: What is the result of an overflow? \n Answer: \n\nA. Knowing when an addition operation has overflowed is important in order to avoid errors in computation. If an addition operation overflows, the result will not be the correct sum of the two numbers being added."}, {"text": "Title: Overflow for 2's Complement Addition \n Text: {Overflow for 2's Complement Addition}\n\nUnderstanding overflow for 2's complement addition is somewhat trickier,\nwhich is why the problem is a good one for you to think about on your\nown first.\n\nOur operands, A and B, are now two {N-bit} 2's complement numbers.\nThe {N-bit} 2's complement representation \ncan represent integers in the range [-2^{N-1},2^{N-1}-1].\n\nLet's start by ruling out a case that we can show never leads to overflow.\n\n{ Lemma:} \n\nAddition of two {N-bit} 2's complement numbers A and B\ndoes not overflow if one of the numbers is negative and the other is\nnot.\n\n{ Proof:}\n\nWe again make use of the constraints implied by the fact that A and B\nare represented as {N-bit} 2's complement values.  We can assume\n{ without loss of generality}{This common mathematical phrasing\nmeans that we are using a problem symmetry to cut down the length of the\nproof discussion.  In this case, the names A and B aren't particularly\nimportant, since addition is commutative (A+B=B+A).  Thus the proof\nfor the case in which A is negative (and B is not) is identical to the\ncase in which B is negative (and A is not), except that all of the \nnames are swapped.  The term ``without loss of generality'' means that\nwe consider the proof complete even with additional assumptions, in\nour case that A<0 and B.}, or { w.l.o.g.}, \nthat A<0 and B.\n\nCombining these constraints with the range representable \nby {N-bit} 2's complement, we obtain\n\n{eqnarray*}\n-2^{N-1}  & A & < 0\n0  & B & < 2^{N-1}\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n-2^{N-1}  & C & < 2^{N-1}\n{eqnarray*}\n\nBut anything in the range specified by this inequality can be represented\nwith {N-bit} 2's complement, and thus the addition does not overflow.\n\n\nWe are now ready to state our main theorem.  For convenience, \nlet's use different names for the actual sum C=A+B and the sum S\nreturned from the add unit.  We define S as the number represented by\nthe bit pattern produced by the add unit.  When overflow \noccurs, S=C, but we always have (S=C)  2^N.\n\n{ Theorem:} \n\nAddition of two {N-bit} 2's complement numbers A and B\noverflows if and only if one of the following conditions holds:\n\n{A<0 and B<0 and S}\n{A and B and S<0}\n\n\n{ Proof:}\n\nWe once again start with the ``if'' direction.  That is, if condition 1 \nor condition 2 holds, we have an overflow.  The proofs are straightforward.\nGiven condition 1, we can add the two inequalities A<0 and B<0 to \nobtain C=A+B<0.  But S, so clearly S=C, thus overflow \nhas occurred.\n\nSimilarly, if condition 2 holds, we can add the inequalities A\nand B to obtain C=A+B.  Here we have S<0, so again\nS=C, and we have an overflow.\n\nWe must now prove the ``only if'' direction, showing that any overflow\nimplies either condition 1 or condition 2.  By the \n{ contrapositive}{If we have a statement of the form\n(p implies q), its contrapositive is the \nstatement (not q implies not p).\nBoth statements have the same truth value.  In this case, we can turn\nour Lemma around as stated.} of our\nLemma, we know that if an overflow occurs, either both operands are \nnegative, or they are both positive.\n\n\n\n\n\nLet's start with the case in which both operands are negative, so A<0\nand B<0, and thus the real sum C<0 as well.  Given that A and B\nare represented as {N-bit} 2's complement, they must fall in\nthe representable range, so we can write\n\n{eqnarray*}\n-2^{N-1}  & A & < 0\n-2^{N-1}  & B & < 0\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n-2^N  & C & < 0\n{eqnarray*}\n\nGiven that an overflow has occurred, C must fall outside of the \nrepresentable range.  Given that C<0, it cannot be larger than the\nlargest possible number representable using {N-bit} 2's\ncomplement, so we can write\n\n{eqnarray*}\n-2^N  & C & < -2^{N-1}\n{eqnarray*}\n\nWe now add 2^N to each part to obtain\n\n{eqnarray*}\n0  & C + 2^N & < 2^{N-1}\n{eqnarray*}\n\nThis range of integers falls within the representable range \nfor {N-bit} 2's complement, so we can replace the middle\nexpression with S (equal to C modulo 2^N) to find that\n\n{eqnarray*}\n0  & S & < 2^{N-1}\n{eqnarray*}\n\nThus, if we have an overflow and both A<0 and B<0, the resulting\nsum S, and condition 1 holds.\n\nThe proof for the case in which we observe an overflow when \nboth operands are non-negative (A and B)\nis similar, and leads to condition 2.  We again begin with\ninequalities for A and B:\n\n{eqnarray*}\n0  & A & < 2^{N-1}\n0  & B & < 2^{N-1}\n{eqnarray*}\n\nWe add these two inequalities and replace A+B with C to obtain\n\n{eqnarray*}\n0  & C < & 2^N\n{eqnarray*}\n\nGiven that an overflow has occurred, C must fall outside of the \nrepresentable range.  Given that C{}0, it cannot be smaller than the\nsmallest possible number representable using {N-bit} 2's\ncomplement, so we can write\n\n{eqnarray*}\n2^{N-1}  & C & < 2^N\n{eqnarray*}\n\nWe now subtract 2^N to each part to obtain\n\n{eqnarray*}\n-2^{N-1}  & C - 2^N & < 0\n{eqnarray*}\n\nThis range of integers falls within the representable range \nfor {N-bit} 2's complement, so we can replace the middle\nexpression with S (equal to C modulo 2^N) to find that\n\n{eqnarray*}\n-2^{N-1}  & S & < 0\n{eqnarray*}\n\nThus, if we have an overflow and both A and B, the resulting\nsum S<0, and condition 2 holds.\n\nThus overflow implies either condition 1 or condition 2, completing our\nproof.\n\n\n\n\n\n \n Question: What does the overflow for 2's complement addition mean? \n Answer: \n\nA. Yes, it does imply an overflow has occurred."}]}
