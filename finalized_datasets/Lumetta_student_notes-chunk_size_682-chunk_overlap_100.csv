text,metadata
"1 ECE120: Introduction to Computer Engineering Notes Set 1.1 The Halting Problem For some of the topics in this course, we plan to cover the material m ore deeply than does the textbook. We will provide notes in this format to supplement the textbook for this purpose. In order to make these notes more useful as a reference, deﬁnitions are highlighted with boldfac e, and italicization emphasizes pitfalls or other important points. Sections marked with an asterisk are provided solely for you r interest, but you probably need to learn this material in later classes. These notes are broken up into four parts, corresponding to the three midterm exams and the ﬁnal exam. Each part is covered by one examination in our class. The last section of each of the four parts gives you a summary of material that you are expected to know for the cor responding exam. Feel free to read it in advance. As discussed in the textbook and in class, a universal computational device (orcomputing machine ) is a device that is capable of computing the solution to any problem tha t can be computed, provided that the device is given enough storage and time for the computation to ﬁ nish. One might ask whether we can describe problems that we cannot ans wer (other than philosophical ones, such as the meaning of life). The answer is yes: there are problems t hat are provably undecidable , for which no amount of computation can solve the problem in general. This set of notes describes the ﬁrst problem known to be undecidable, the halting problem . For our class, you need only recognize the name and realize that one can, in fact, give examples of problems that can not be solved by computation. In the fu- ture, you should be able to recognizethis type of problem so as to av oidspending your time trying to solve it. 1.1.1 Universal Computing Machines* The things that we call computers today, whether we are talking ab out a programmable microcontroller in a microwave oven or the Blue Waters supercomputer sitting on the s outh end of our campus (the United States’ main resource to support computational science resear ch), are all equivalent in the sense of what problems they can solve. These machines do, of course, have acce ss to diﬀerent amounts of memory, and compute at diﬀerent speeds. The idea that a single model of computation could be described and pr oven to be equivalent to all other models came out of a 1936 paper by Alan Turing, and today we genera lly refer to these devices as Turing machines . All computers mentioned earlier, as well as all computers with which you are familiar in your daily life, are provably equivalent to Turing machines. Turing also conjectured that his deﬁnition of computable was identic al to the “natural” deﬁnition (today, this claim is known as the Church-Turing conjecture ). In other words, a problem that cannot be solved by a Turing machine cannot be solved in any systematic manner, with a ny machine, or by any person. This conjecture remains unproven! However, neither has anyone bee n able to disprove the conjecture, and it is widely believed to be true","{'page_number': 6, 'textbook_name': 'ECE-120-student-notes', 'text': 'In other words, a problem that cannot be solved by a Turing machine cannot be solved in any systematic manner, with a ny machine, or by any person. This conjecture remains unproven! However, neither has anyone bee n able to disprove the conjecture, and it is widely believed to be true. Disproving the conjecture requires that one demonstrate a systematic technique (or a machine) capable of solving a problem that cannot be solved by a Turing machine. No one has been able to do so to date. 1.1.2 The Halting Problem* You might reasonably ask whether any problems can be shown to be in computable. More common terms for such problems—those known to be insolvable by any computer—a reintractable or undecidable. In the same 1936 paper in which he introduced the universal computing mac hine, Alan Turing also provided an answertothis questionby introducing(andproving)that therear ein factproblemsthat cannotbe computed by a universal computing machine. The problem that he proved unde cidable, using proof techniques almost identical to those developed for similar problems in the 1880s, is now k nown as the halting problem .'}"
"In other words, a problem that cannot be solved by a Turing machine cannot be solved in any systematic manner, with a ny machine, or by any person. This conjecture remains unproven! However, neither has anyone bee n able to disprove the conjecture, and it is widely believed to be true. Disproving the conjecture requires that one demonstrate a systematic technique (or a machine) capable of solving a problem that cannot be solved by a Turing machine. No one has been able to do so to date. 1.1.2 The Halting Problem* You might reasonably ask whether any problems can be shown to be in computable. More common terms for such problems—those known to be insolvable by any computer—a reintractable or undecidable. In the same 1936 paper in which he introduced the universal computing mac hine, Alan Turing also provided an answertothis questionby introducing(andproving)that therear ein factproblemsthat cannotbe computed by a universal computing machine. The problem that he proved unde cidable, using proof techniques almost identical to those developed for similar problems in the 1880s, is now k nown as the halting problem .","{'page_number': 6, 'textbook_name': 'ECE-120-student-notes', 'text': 'In other words, a problem that cannot be solved by a Turing machine cannot be solved in any systematic manner, with a ny machine, or by any person. This conjecture remains unproven! However, neither has anyone bee n able to disprove the conjecture, and it is widely believed to be true. Disproving the conjecture requires that one demonstrate a systematic technique (or a machine) capable of solving a problem that cannot be solved by a Turing machine. No one has been able to do so to date. 1.1.2 The Halting Problem* You might reasonably ask whether any problems can be shown to be in computable. More common terms for such problems—those known to be insolvable by any computer—a reintractable or undecidable. In the same 1936 paper in which he introduced the universal computing mac hine, Alan Turing also provided an answertothis questionby introducing(andproving)that therear ein factproblemsthat cannotbe computed by a universal computing machine. The problem that he proved unde cidable, using proof techniques almost identical to those developed for similar problems in the 1880s, is now k nown as the halting problem .'}"
"2  The halting problem is easy to state and easy to prove undecidable. T he problem is this: given a Turing machine and an input to the Turing machine, does the Turing machine ﬁ nish computing in a ﬁnite number of steps (a ﬁnite amount of time)? In order to solve the problem, an answer, either yes or no, must be given in a ﬁnite amount of time regardlessof the machine or input in question . Clearly some machines never ﬁnish. For example, we can write a Turing machine that counts upwards sta rting from one. You may ﬁnd the proof structure for undecidability of the halting pr oblem easier to understand if you ﬁrst think about a related problem with which you may already be familiar, th e Liar’s paradox (which is at least 2,300 years old). In its stengthened form, it is the following sentenc e: “This sentence is not true.” To see that no Turing machine can solve the halting problem, we begin by assuming that such a machine exists, and then show that its existence is self-contradictory. We call the machine the “Halting Machine,” or HM for short. HM is a machine that operates on anotherHalting Machine (HM)Turing machine + inputsyes or no Turing machine and its inputs to produce a yes or no answer in ﬁnite tim e: either the machine in question ﬁnishes in ﬁnite time (HM returns “yes”), or it does not (HM returns “no”). The ﬁgure illustrates HM’s operation. From HM, we construct a second machine that we call the HM Inverter, or HMI. This machine inverts the sense of the answer given by HM. In particular, the inputs are fed directly into a copy of HM, and if HM answers “yes,” HMI enters an inﬁnite loop. If HM answers “no,” HMI halts. A diagram appears to the right. The inconsistency can now be seen by askingHM whether HMI halts when given itself as an input (repeatedly), asHalting Machine (HM)Turing machine + inputsHM said yes? count foreveryesnodone Halting Machine Inverter (HMI) shown below. Two copies of HM are thus being asked the same questio n. One copy is the rightmost in the ﬁgure below and the second is embedded in the HMI machine that we ar e using as the input to the rightmost HM. As the two copies of HM operate on the same input (HMI operatin g on HMI), they should return the same answer: a Turing machine either halts on an input, or it does not ; they are deterministic. Halting Machine (HM)HM said yes? count foreverHMI HMI Halting Machine (HM)yes or noyesnodone Halting Machine Inverter (HMI). . .a Turing machine + inputs Let’s assume that the rightmost HM tells us that HMI operating on its elf halts","{'page_number': 7, 'textbook_name': 'ECE-120-student-notes', 'text': 'Halting Machine (HM)HM said yes? count foreverHMI HMI Halting Machine (HM)yes or noyesnodone Halting Machine Inverter (HMI). . .a Turing machine + inputs Let’s assume that the rightmost HM tells us that HMI operating on its elf halts. Then the copy of HM in HMI (when HMI executes on itself, with itself as an input) must also sa y “yes.” But this answer implies that HMI doesn’t halt (see the ﬁgure above), so the answer should have been no! Alternatively, we can assume that the rightmost HM says that HMI o perating on itself does not halt. Again, the copy of HM in HMI must give the same answer. But in this case HMI h alts, again contradicting our assumption. Since neither answer is consistent, no consistent answer can be giv en, and the original assumption that HM exists is incorrect. Thus, no Turing machine can solve the halting pro blem.'}"
"Halting Machine (HM)HM said yes? count foreverHMI HMI Halting Machine (HM)yes or noyesnodone Halting Machine Inverter (HMI). . .a Turing machine + inputs Let’s assume that the rightmost HM tells us that HMI operating on its elf halts. Then the copy of HM in HMI (when HMI executes on itself, with itself as an input) must also sa y “yes.” But this answer implies that HMI doesn’t halt (see the ﬁgure above), so the answer should have been no! Alternatively, we can assume that the rightmost HM says that HMI o perating on itself does not halt. Again, the copy of HM in HMI must give the same answer. But in this case HMI h alts, again contradicting our assumption. Since neither answer is consistent, no consistent answer can be giv en, and the original assumption that HM exists is incorrect. Thus, no Turing machine can solve the halting pro blem.","{'page_number': 7, 'textbook_name': 'ECE-120-student-notes', 'text': 'Halting Machine (HM)HM said yes? count foreverHMI HMI Halting Machine (HM)yes or noyesnodone Halting Machine Inverter (HMI). . .a Turing machine + inputs Let’s assume that the rightmost HM tells us that HMI operating on its elf halts. Then the copy of HM in HMI (when HMI executes on itself, with itself as an input) must also sa y “yes.” But this answer implies that HMI doesn’t halt (see the ﬁgure above), so the answer should have been no! Alternatively, we can assume that the rightmost HM says that HMI o perating on itself does not halt. Again, the copy of HM in HMI must give the same answer. But in this case HMI h alts, again contradicting our assumption. Since neither answer is consistent, no consistent answer can be giv en, and the original assumption that HM exists is incorrect. Thus, no Turing machine can solve the halting pro blem.'}"
"1.2 The 2’s Complement Representation 3 ECE120: Introduction to Computer Engineering Notes Set 1.2 The 2’s Complement Representation This set of notes explains the rationale for using the 2’s complement r epresentation for signed integers and derives the representation based on equivalence of the addition fu nction to that of addition using the un- signed representation with the same number of bits. 1.2.1 Review of Bits and the Unsigned Representation In modern digital systems, we represent all types of information u sing binary digits, or bits. Logically, a bit is either 0 or 1. Physically, a bit may be a voltage, a magnetic ﬁeld, or even the electrical resistance of a tiny sliver of glass. Any type of information can be represented wit h an ordered set of bits, provided that any given pattern of bits corresponds to only one value and that we agree in advance on which pattern of bits represents which value . For unsigned integers—that is, whole numbers greater or equal to zero—we chose to use the base 2 represen- tation already familiar to us from mathematics. We call this represen tation the unsigned representation . For example, in a 4-bit unsigned representation, we write the numbe r 0 as 0000, the number 5 as 0101, and the number 12 as 1100. Note that we always write the same numb er of bits for any pattern in the representation: in a digital system, there is no “blank” bit value . 1.2.2 Picking a Good Representation In class, we discussed the question of what makes one representa tion better than another. The value of the unsigned representation, for example, is in part our existing fa miliarity with the base 2 analogues of arithmetic. For base 2 arithmetic, we can use nearly identical techn iques to those that we learned in elementary school for adding, subtracting, multiplying, and dividing base 10 numbers. Reasoning about the relative merits of representations from a pra ctical engineering perspective is (prob- ably) currently beyond your ability. Saving energy, making the implem entation simple, and allowing the implementation to execute quickly probably all sound attractive, bu t a quantitative comparison between two representations on any of these bases requires knowledge that y ou will acquire in the next few years. We can sidestep such questions, however, by realizing that if a digita l system has hardware to perform operations such as addition on unsigned values, using the same piece of hardware to operate on other representations incurs little or no additional cost. In this set of no tes, we discuss the 2’s complement repre- sentation, which allows reuse of the unsigned add unit (as well as a ba sis for performing subtraction of either representation using an add unit!). In discussion section and in your homework, you will use the same idea to perform operations on other representations, such as chang ing an upper case letter in ASCII to a lower case one, or converting from an ASCII digit to an unsigned represe ntation of the same number. 1.2","{'page_number': 8, 'textbook_name': 'ECE-120-student-notes', 'text': 'In discussion section and in your homework, you will use the same idea to perform operations on other representations, such as chang ing an upper case letter in ASCII to a lower case one, or converting from an ASCII digit to an unsigned represe ntation of the same number. 1.2.3 The Unsigned Add Unit In order to deﬁne a representation for signed integers that allows us to reuse a piece of hardware designed for unsigned integers, we must ﬁrst understand what such a piece of hardware actually does (we do not need to know how it works yet—we’ll explore that question later in our class ). The unsigned representationusing Nbits is not closedunder addition. In other words, for any value of N, we can easily ﬁnd two N-bit unsigned numbers that, when added together, cannot be rep resented as an N-bit unsigned number. With N= 4, for example, we can add 12 (1100) and 6 (0110) to obtain 18. Sin ce 18 is outside of the range [0 ,24−1] representable using the 4-bit unsigned representation, our re presentation breaks if we try to represent the sum using this representation. W e call this failure an overﬂow condition: the representation cannot represent the result of the operatio n, in this case addition.'}"
"In discussion section and in your homework, you will use the same idea to perform operations on other representations, such as chang ing an upper case letter in ASCII to a lower case one, or converting from an ASCII digit to an unsigned represe ntation of the same number. 1.2.3 The Unsigned Add Unit In order to deﬁne a representation for signed integers that allows us to reuse a piece of hardware designed for unsigned integers, we must ﬁrst understand what such a piece of hardware actually does (we do not need to know how it works yet—we’ll explore that question later in our class ). The unsigned representationusing Nbits is not closedunder addition. In other words, for any value of N, we can easily ﬁnd two N-bit unsigned numbers that, when added together, cannot be rep resented as an N-bit unsigned number. With N= 4, for example, we can add 12 (1100) and 6 (0110) to obtain 18. Sin ce 18 is outside of the range [0 ,24−1] representable using the 4-bit unsigned representation, our re presentation breaks if we try to represent the sum using this representation. W e call this failure an overﬂow condition: the representation cannot represent the result of the operatio n, in this case addition.","{'page_number': 8, 'textbook_name': 'ECE-120-student-notes', 'text': 'In discussion section and in your homework, you will use the same idea to perform operations on other representations, such as chang ing an upper case letter in ASCII to a lower case one, or converting from an ASCII digit to an unsigned represe ntation of the same number. 1.2.3 The Unsigned Add Unit In order to deﬁne a representation for signed integers that allows us to reuse a piece of hardware designed for unsigned integers, we must ﬁrst understand what such a piece of hardware actually does (we do not need to know how it works yet—we’ll explore that question later in our class ). The unsigned representationusing Nbits is not closedunder addition. In other words, for any value of N, we can easily ﬁnd two N-bit unsigned numbers that, when added together, cannot be rep resented as an N-bit unsigned number. With N= 4, for example, we can add 12 (1100) and 6 (0110) to obtain 18. Sin ce 18 is outside of the range [0 ,24−1] representable using the 4-bit unsigned representation, our re presentation breaks if we try to represent the sum using this representation. W e call this failure an overﬂow condition: the representation cannot represent the result of the operatio n, in this case addition.'}"
"4  Using more bits to represent the answer is not an attractive solutio n, since we might then want to use more bits for the inputs, which in turn requires more bits for the outputs, and so on. We cannot build something supporting an inﬁnite number of bits. Instead, we choose a value for Nand build an add unit that adds two N-bit numbers and produces anN-bit sum (and some overﬂow indicators, which we discuss in the next s et of notes). The diagramto the right showshow we might drawsuch a device, with t woN-bit numbers entering at from the top, and the N-bit sum coming out from the bottom.N−bit add unitN N N The function used for N-bit unsigned addition is addition modulo 2N. In a practical sense, you can think of this function as simply keeping the last Nbits of the answer; other bits are simply discarded. In the example to the right, we add 12 and 6 to o btain 18, but then discard the extra bit on the left, so the add unit produces 2 (an ove rﬂow).1100 10010+ 0110(12) (6) (2) Modular arithmetic deﬁnes a way of performing arithmetic for a ﬁnite number of possible values, usually integers. As a concrete example, let’s use modulo 16, which corresponds to the addition unit for our 4-bit examples. Starting with the full range of integers, we can deﬁne equiva- lence classes for groups of 16 integers by simply breaking upof numbers of numbersa third group a second group0 15 one group of numbers−16 −1 16 31 . . . . . . the number line into contiguous groups, starting with the numbers 0 to 15, as shown to the right. The numbers -16 to -1 form a group, as do the numbers from 16 to 31. A n inﬁnite number of groups are deﬁned in this manner. You can think of these groups as deﬁning equivalence classes modulo 16. All of the ﬁrst numbers in the groups are equivalent modulo 16. All of the second numbers in the gr oups are equivalent modulo 16. And so forth. Mathematically, we say that two numbers AandBare equivalent modulo 16, which we write as (A=B) mod 16 if and only if A=B+16kfor some integer k. It is worth noting that equivalence as deﬁned by a particular modulus distributes over addition and multipli- cation. If, for example, we want to ﬁnd the equivalence class for ( A+B) mod 16, we can ﬁnd the equivalence classes for A(call itC) andB(call itD) and then calculate the equivalence class of ( C+D) mod 16. As a concrete example of distribution over multiplication, given ( A= 1,083,102,112×7,323,127) mod 10, ﬁndA. For this problem, we note that the ﬁrst number is equivalent to 2 mo d 10, while the second number is equivalent to 7 mod 10","{'page_number': 9, 'textbook_name': 'ECE-120-student-notes', 'text': 'As a concrete example of distribution over multiplication, given ( A= 1,083,102,112×7,323,127) mod 10, ﬁndA. For this problem, we note that the ﬁrst number is equivalent to 2 mo d 10, while the second number is equivalent to 7 mod 10. We then write ( A= 2×7) mod 10, and, since 2 ×7 = 14, we have ( A= 4) mod 10. 1.2.4 Deriving 2’s Complement Given these equivalence classes, we might instead choose to draw a circle to iden- tify the equivalence classes and to associate each class with one of the sixteen possible 4-bit patterns, as shown to the right. Us- ing this circlerepresentation,wecan addby counting clockwise around the circle, and we can subtract by counting in a counter- clockwise direction around the circle. With an unsigned representation, we choose to use the groupfrom[0 ,15](the middle group in the diagram markings to the right) as the number represented by each of the pat- terns. Overﬂow occurs with unsigned addi- tion (or subtraction) because we can only choose one value for each binary pattern.01000001 0010 0011 0101 0110 01110000 10001111 100110101011110011011110..., −15, 1, 17, ... ..., −14, 2, 18, ... ..., −13, 3, 19, ... ..., −12, 4, 20, ... ..., −11, 5, 21 ..., −10, 6, 22 ..., −9, 7, 23 ..., −8, 8, 24, ......, −7, 9, 25, ......, −6, 10, 26, ......, −5, 11, 27, ......, −4, 12, 28, ......, −3, 13, 29, ......, −2, 14, 30, ......, −1, 15, 31, ... modulo 16 (binary patterns inside circle)equivalence classes..., −16, 0, 16, ...'}"
"As a concrete example of distribution over multiplication, given ( A= 1,083,102,112×7,323,127) mod 10, ﬁndA. For this problem, we note that the ﬁrst number is equivalent to 2 mo d 10, while the second number is equivalent to 7 mod 10. We then write ( A= 2×7) mod 10, and, since 2 ×7 = 14, we have ( A= 4) mod 10. 1.2.4 Deriving 2’s Complement Given these equivalence classes, we might instead choose to draw a circle to iden- tify the equivalence classes and to associate each class with one of the sixteen possible 4-bit patterns, as shown to the right. Us- ing this circlerepresentation,wecan addby counting clockwise around the circle, and we can subtract by counting in a counter- clockwise direction around the circle. With an unsigned representation, we choose to use the groupfrom[0 ,15](the middle group in the diagram markings to the right) as the number represented by each of the pat- terns. Overﬂow occurs with unsigned addi- tion (or subtraction) because we can only choose one value for each binary pattern.01000001 0010 0011 0101 0110 01110000 10001111 100110101011110011011110..., −15, 1, 17, ... ..., −14, 2, 18, ... ..., −13, 3, 19, ... ..., −12, 4, 20, ... ..., −11, 5, 21 ..., −10, 6, 22 ..., −9, 7, 23 ..., −8, 8, 24, ......, −7, 9, 25, ......, −6, 10, 26, ......, −5, 11, 27, ......, −4, 12, 28, ......, −3, 13, 29, ......, −2, 14, 30, ......, −1, 15, 31, ... modulo 16 (binary patterns inside circle)equivalence classes..., −16, 0, 16, ...","{'page_number': 9, 'textbook_name': 'ECE-120-student-notes', 'text': 'As a concrete example of distribution over multiplication, given ( A= 1,083,102,112×7,323,127) mod 10, ﬁndA. For this problem, we note that the ﬁrst number is equivalent to 2 mo d 10, while the second number is equivalent to 7 mod 10. We then write ( A= 2×7) mod 10, and, since 2 ×7 = 14, we have ( A= 4) mod 10. 1.2.4 Deriving 2’s Complement Given these equivalence classes, we might instead choose to draw a circle to iden- tify the equivalence classes and to associate each class with one of the sixteen possible 4-bit patterns, as shown to the right. Us- ing this circlerepresentation,wecan addby counting clockwise around the circle, and we can subtract by counting in a counter- clockwise direction around the circle. With an unsigned representation, we choose to use the groupfrom[0 ,15](the middle group in the diagram markings to the right) as the number represented by each of the pat- terns. Overﬂow occurs with unsigned addi- tion (or subtraction) because we can only choose one value for each binary pattern.01000001 0010 0011 0101 0110 01110000 10001111 100110101011110011011110..., −15, 1, 17, ... ..., −14, 2, 18, ... ..., −13, 3, 19, ... ..., −12, 4, 20, ... ..., −11, 5, 21 ..., −10, 6, 22 ..., −9, 7, 23 ..., −8, 8, 24, ......, −7, 9, 25, ......, −6, 10, 26, ......, −5, 11, 27, ......, −4, 12, 28, ......, −3, 13, 29, ......, −2, 14, 30, ......, −1, 15, 31, ... modulo 16 (binary patterns inside circle)equivalence classes..., −16, 0, 16, ...'}"
"1.2 The 2’s Complement Representation 5 In fact, we can choose any single value for each pattern to create a representation, and our add unit will always produce results that are correct modulo 16. Look back at o ur overﬂow example, where we added 12 and 6 to obtain 2, and notice that (2 = 18) mod 16. Normally, only a con tiguous sequence of integers makes a useful representation, but we do not have to restrict ourselve s to non-negative numbers. The 2’s complement representation can then be deﬁned by choosing a set of integers balanced around zero from the groups. In the circle diagram, for example, we might choos e to represent numbers in the range [−7,7] when using 4 bits. What about the last pattern, 1000? We could ch oose to represent either -8 or 8. The number of arithmetic operations that overﬂow is the same with b oth choices (the choices are symmetric around 0, as are the combinations of input operands that overﬂow ), so we gain nothing in that sense from ei- ther choice. If we choose to represent -8, however, notice that all patterns starting with a 1 bit then represent negative numbers. No such simple check arises with the opposite cho ice, and thus an N-bit 2’s complement representationisdeﬁnedtorepresenttherange[ −2N−1,2N−1−1], withpatternschosenasshowninthecircle. 1.2.5 An Algebraic Approach Some people prefer an algebraic approach to understanding the de ﬁnition of 2’s complement, so we present such an approach next. Let’s start by writing f(A,B) for the result of our add unit: f(A,B) = (A+B) mod 2N We assumethat we want to representa set of integersbalanced ar ound0 using oursigned representation, and that we will use the same binary patterns as we do with an unsigned re presentationto represent non-negative numbers. Thus, with an N-bit representation, the patterns in the range [0 ,2N−1−1] are the same as those used with an unsigned representation. In this case, we are left with all patterns beginning with a 1 bit. The question then is this: given an integer k, 2N−1> k >0, for which we want to ﬁnd a pattern to represent−k, and any integer m≥0 that we might want to add to −k, can we ﬁnd another integer p >0 such that (−k+m=p+m) mod 2N? (1) If we can, we can use p’s representation to represent −kand our unsigned addition unit f(A,B) will work correctly. To ﬁnd the value p, start by subtracting mfrom both sides of Equation (1) to obtain: (−k=p) mod 2N(2) Note that (2N= 0) mod 2N, and add this equation to Equation (2) to obtain (2N−k=p) mod 2N Letp= 2N−k","{'page_number': 10, 'textbook_name': 'ECE-120-student-notes', 'text': 'To ﬁnd the value p, start by subtracting mfrom both sides of Equation (1) to obtain: (−k=p) mod 2N(2) Note that (2N= 0) mod 2N, and add this equation to Equation (2) to obtain (2N−k=p) mod 2N Letp= 2N−k. For example, if N= 4,k= 3 gives p= 16−3 = 13, which is the pattern 1101. With N= 4 andk= 5, we obtain p= 16−5 = 11, which is the pattern 1011. In general, since 2N−1> k >0, we have 2N−1< p <2N. But these patterns are all unused—they all start with a 1 bit!—so t he patterns that we have deﬁned for negative numbers are disjoint from those that we used for positive numbers, and the meaning of each pattern is unambiguous. The algebraic deﬁnition of b it patterns for negative numbers also matches our circle diagram from the last section exactly, of course .'}"
"To ﬁnd the value p, start by subtracting mfrom both sides of Equation (1) to obtain: (−k=p) mod 2N(2) Note that (2N= 0) mod 2N, and add this equation to Equation (2) to obtain (2N−k=p) mod 2N Letp= 2N−k. For example, if N= 4,k= 3 gives p= 16−3 = 13, which is the pattern 1101. With N= 4 andk= 5, we obtain p= 16−5 = 11, which is the pattern 1011. In general, since 2N−1> k >0, we have 2N−1< p <2N. But these patterns are all unused—they all start with a 1 bit!—so t he patterns that we have deﬁned for negative numbers are disjoint from those that we used for positive numbers, and the meaning of each pattern is unambiguous. The algebraic deﬁnition of b it patterns for negative numbers also matches our circle diagram from the last section exactly, of course .","{'page_number': 10, 'textbook_name': 'ECE-120-student-notes', 'text': 'To ﬁnd the value p, start by subtracting mfrom both sides of Equation (1) to obtain: (−k=p) mod 2N(2) Note that (2N= 0) mod 2N, and add this equation to Equation (2) to obtain (2N−k=p) mod 2N Letp= 2N−k. For example, if N= 4,k= 3 gives p= 16−3 = 13, which is the pattern 1101. With N= 4 andk= 5, we obtain p= 16−5 = 11, which is the pattern 1011. In general, since 2N−1> k >0, we have 2N−1< p <2N. But these patterns are all unused—they all start with a 1 bit!—so t he patterns that we have deﬁned for negative numbers are disjoint from those that we used for positive numbers, and the meaning of each pattern is unambiguous. The algebraic deﬁnition of b it patterns for negative numbers also matches our circle diagram from the last section exactly, of course .'}"
"6  1.2.6 Negating 2’s Complement Numbers The algebraic approach makes understanding negation of an intege r represented using 2’s complement fairly straightforward, and gives us an easy procedure for doing so. Re call that given an integer kin anN-bit 2’s complement representation, the N-bit pattern for−kis given by 2N−k(also true for k= 0 if we keep only the low Nbits of the result). But 2N= (2N−1) +1. Note that 2N−1 is the pattern of all 1 bits. Subtracting any value kfrom this value is equivalent to simply ﬂipping the bits, changing 0s to 1s and 1s to 0s. (This operation is called a 1’s complement , by the way.) We then add 1 to the result to ﬁnd the pattern for−k. Negation can overﬂow, of course. Try ﬁnding the negative patter n for -8 in 4-bit 2’s complement. Finally, be aware that people often overload the term 2’s complement and use it to refer to the operation of negation in a 2’s complement representation. In our class, we try avoid this confusion: 2’s complement is a representation for signed integers, and negation is an operatio n that one can apply to a signed integer (whether the representation used for the integer is 2’s complemen t or some other representation for signed integers).","{'page_number': 11, 'textbook_name': 'ECE-120-student-notes', 'text': '6  1.2.6 Negating 2’s Complement Numbers The algebraic approach makes understanding negation of an intege r represented using 2’s complement fairly straightforward, and gives us an easy procedure for doing so. Re call that given an integer kin anN-bit 2’s complement representation, the N-bit pattern for−kis given by 2N−k(also true for k= 0 if we keep only the low Nbits of the result). But 2N= (2N−1) +1. Note that 2N−1 is the pattern of all 1 bits. Subtracting any value kfrom this value is equivalent to simply ﬂipping the bits, changing 0s to 1s and 1s to 0s. (This operation is called a 1’s complement , by the way.) We then add 1 to the result to ﬁnd the pattern for−k. Negation can overﬂow, of course. Try ﬁnding the negative patter n for -8 in 4-bit 2’s complement. Finally, be aware that people often overload the term 2’s complement and use it to refer to the operation of negation in a 2’s complement representation. In our class, we try avoid this confusion: 2’s complement is a representation for signed integers, and negation is an operatio n that one can apply to a signed integer (whether the representation used for the integer is 2’s complemen t or some other representation for signed integers).'}"
"1.3 Overﬂow Conditions 7 ECE120: Introduction to Computer Engineering Notes Set 1.3 Overﬂow Conditions This set of notes discusses the overﬂow conditions for unsigned an d 2’s complement addition. For both types, we formally prove that the conditions that we state are cor rect. Many of our faculty want our stu- dents to learn to construct formal proofs, so we plan to begin exp osing you to this process in our classes. Prof. Lumetta is a fan of Prof. George Polya’s educational theorie s with regard to proof techniques, and in particular the idea that one builds up a repertoire of approachesb y seeing the approachesused in practice. 1.3.1 Implication and Mathematical Notation Someofyoumaynothavebeenexposedtobasicsofmathematicallo gic,solet’sstartwithabriefintroduction to implication. We’ll use variables pandqto represent statements that can be either true or false. For example, pmight represent the statement, “Jan is an ECE student,” while qmight represent the statement, “Jan works hard.” The logical complement ornegation of a statement p, written for example as “not p,” has the opposite truth value: if pis true, not pis false, and if pis false, not pis true. Animplication is a logical relationship between two statements. The implication itself is also a logical statement, and may be true or false. In English, for example, we mig ht say, “If p,q.” In mathematics, the same implication is usually written as either “ qifp” or “p→q,” and the latter is read as, “ pimpliesq.” Using our example values for pandq, we can see that p→qis true: “Jan is an ECE student” does in fact imply that “Jan works hard!” The implication p→qis only considered false if pis true and qis false. In all other cases, the implication is true. This deﬁnition can be a little confusing at ﬁrst, so let’s use ano ther example to see why. Let p represent the statement “Entity X is a ﬂying pig,” and let qrepresent the statement, “Entity X obeys air traﬃc control regulations.” Here the implication p→qis again true: ﬂying pigs do not exist, so pis false, and thus “ p→q” is true—for any value of statement q! Given an implication “ p→q,” we say that the converse of the implication is the statement “ q→p,” which is also an implication. In mathematics, the converse of p→qis sometimes written as “ qonly ifp.” The converse of an implication may or may not have the same truth value a s the implication itself. Finally, we frequently use the shorthand notation, “ pif and only if q,” (or, even shorter, “ piﬀq”) to mean “ p→qand q→p.” This last statement is true only when both implications are true. 1.3.2 Overﬂow for Unsigned Addition Let’s say that we add two N-bit unsigned numbers, AandB","{'page_number': 12, 'textbook_name': 'ECE-120-student-notes', 'text': '” The converse of an implication may or may not have the same truth value a s the implication itself. Finally, we frequently use the shorthand notation, “ pif and only if q,” (or, even shorter, “ piﬀq”) to mean “ p→qand q→p.” This last statement is true only when both implications are true. 1.3.2 Overﬂow for Unsigned Addition Let’s say that we add two N-bit unsigned numbers, AandB. TheN-bit unsigned representation can represent integers in the range [0 ,2N−1]. Recall that we say that the addition operation has overﬂowed if the number represented by the N-bit pattern produced for the sum does not actually represent th e numberA+B. For clarity, let’s name the bits of Aby writing the number as aN−1aN−2...a1a0. Similarly, let’s write Bas bN−1bN−2...b1b0. Name the sum C=A+B. The sum that comes out of the add unit has only Nbits, but recall that we claimed in class that the overﬂow condition for unsigne d addition is given by the carryout of the most signiﬁcant bit. So let’s write the sum as cNcN−1cN−2...c1c0, realizing that cNis the carry out and not actually part of the sum produced by the add unit. Theorem: Addition of two N-bit unsigned numbers A=aN−1aN−2...a1a0andB=bN−1bN−2...b1b0to produce sum C=A+B=cNcN−1cN−2...c1c0, overﬂows if and only if the carry out cNof the addition is a 1 bit.'}"
"” The converse of an implication may or may not have the same truth value a s the implication itself. Finally, we frequently use the shorthand notation, “ pif and only if q,” (or, even shorter, “ piﬀq”) to mean “ p→qand q→p.” This last statement is true only when both implications are true. 1.3.2 Overﬂow for Unsigned Addition Let’s say that we add two N-bit unsigned numbers, AandB. TheN-bit unsigned representation can represent integers in the range [0 ,2N−1]. Recall that we say that the addition operation has overﬂowed if the number represented by the N-bit pattern produced for the sum does not actually represent th e numberA+B. For clarity, let’s name the bits of Aby writing the number as aN−1aN−2...a1a0. Similarly, let’s write Bas bN−1bN−2...b1b0. Name the sum C=A+B. The sum that comes out of the add unit has only Nbits, but recall that we claimed in class that the overﬂow condition for unsigne d addition is given by the carryout of the most signiﬁcant bit. So let’s write the sum as cNcN−1cN−2...c1c0, realizing that cNis the carry out and not actually part of the sum produced by the add unit. Theorem: Addition of two N-bit unsigned numbers A=aN−1aN−2...a1a0andB=bN−1bN−2...b1b0to produce sum C=A+B=cNcN−1cN−2...c1c0, overﬂows if and only if the carry out cNof the addition is a 1 bit.","{'page_number': 12, 'textbook_name': 'ECE-120-student-notes', 'text': '” The converse of an implication may or may not have the same truth value a s the implication itself. Finally, we frequently use the shorthand notation, “ pif and only if q,” (or, even shorter, “ piﬀq”) to mean “ p→qand q→p.” This last statement is true only when both implications are true. 1.3.2 Overﬂow for Unsigned Addition Let’s say that we add two N-bit unsigned numbers, AandB. TheN-bit unsigned representation can represent integers in the range [0 ,2N−1]. Recall that we say that the addition operation has overﬂowed if the number represented by the N-bit pattern produced for the sum does not actually represent th e numberA+B. For clarity, let’s name the bits of Aby writing the number as aN−1aN−2...a1a0. Similarly, let’s write Bas bN−1bN−2...b1b0. Name the sum C=A+B. The sum that comes out of the add unit has only Nbits, but recall that we claimed in class that the overﬂow condition for unsigne d addition is given by the carryout of the most signiﬁcant bit. So let’s write the sum as cNcN−1cN−2...c1c0, realizing that cNis the carry out and not actually part of the sum produced by the add unit. Theorem: Addition of two N-bit unsigned numbers A=aN−1aN−2...a1a0andB=bN−1bN−2...b1b0to produce sum C=A+B=cNcN−1cN−2...c1c0, overﬂows if and only if the carry out cNof the addition is a 1 bit.'}"
"8  Proof:Let’s start with the “if” direction. In other words, cN= 1 implies overﬂow. Recall that unsigned addition is the same as base 2 addition, except that we discard bits be yondcN−1from the sum C. The bitcNhas place value 2N, so, when cN= 1 we can write that the correct sum C≥2N. But no value that large can be represented using the N-bit unsigned representation, so we have an overﬂow. The other direction (“only if”) is slightly more complex: we need to sho w that overﬂow implies that cN= 1. We use a range-basedargumentfor this purpose. Overﬂowmeans that the sum Cis outside the representable range [0,2N−1]. Adding two non-negative numbers cannot produce a negative nu mber, so the sum can’t be smaller than 0. Overﬂow thus implies that C≥2N. Does that argument complete the proof? No, because some numbe rs, such as 2N+1, are larger than 2N, but do not have a 1 bit in the Nth position when written in binary. We need to make use of the constr aints onAandBimplied by the possible range of the representation. In particular, given that AandBare represented as N-bit unsigned values, we can write 0≤A≤2N−1 0≤B≤2N−1 We add these two inequalities and replace A+BwithCto obtain 0≤C≤2N+1−2 Combining the new inequality with the one implied by the overﬂow conditio n, we obtain 2N≤C≤2N+1−2 All of the numbers in the range allowed by this inequality have cN= 1, completing our proof. 1.3.3 Overﬂow for 2’s Complement Addition Understanding overﬂowfor 2’scomplement addition is somewhat tric kier, which is why the problem is a good one for you to think about on your own ﬁrst. Our operands, AandB, are now two N-bit 2’s complement numbers. The N-bit 2’s complement representation can represent integers in the r ange [−2N−1,2N−1−1]. Let’s start by ruling out a case that we can show never leads to over ﬂow. Lemma: Addition of two N-bit 2’s complement numbers AandBdoes not overﬂow if one of the numbers is negative and the other is not. Proof:We again make use of the constraints implied by the fact that AandBare represented as N-bit 2’s complement values. We can assume without loss of generality1, orw.l.o.g., thatA <0 andB≥0","{'page_number': 13, 'textbook_name': 'ECE-120-student-notes', 'text': 'Lemma: Addition of two N-bit 2’s complement numbers AandBdoes not overﬂow if one of the numbers is negative and the other is not. Proof:We again make use of the constraints implied by the fact that AandBare represented as N-bit 2’s complement values. We can assume without loss of generality1, orw.l.o.g., thatA <0 andB≥0. Combining these constraints with the range representable by N-bit 2’s complement, we obtain −2N−1≤A <0 0≤B <2N−1 We add these two inequalities and replace A+BwithCto obtain −2N−1≤C <2N−1 But anything in the range speciﬁed by this inequality can be represen ted with N-bit 2’s complement, and thus the addition does not overﬂow. 1This common mathematical phrasing means that we are using a p roblem symmetry to cut down the length of the proof discussion. In this case, the names AandBaren’t particularly important, since addition is commutat ive (A+B=B+A). Thus the proof for the case in which Ais negative (and Bis not) is identical to the case in which Bis negative (and Ais not), except that all of the names are swapped. The term “without lo ss of generality” means that we consider the proof complete even with additional assumptions, in our case that A <0 andB≥0.'}"
"Lemma: Addition of two N-bit 2’s complement numbers AandBdoes not overﬂow if one of the numbers is negative and the other is not. Proof:We again make use of the constraints implied by the fact that AandBare represented as N-bit 2’s complement values. We can assume without loss of generality1, orw.l.o.g., thatA <0 andB≥0. Combining these constraints with the range representable by N-bit 2’s complement, we obtain −2N−1≤A <0 0≤B <2N−1 We add these two inequalities and replace A+BwithCto obtain −2N−1≤C <2N−1 But anything in the range speciﬁed by this inequality can be represen ted with N-bit 2’s complement, and thus the addition does not overﬂow. 1This common mathematical phrasing means that we are using a p roblem symmetry to cut down the length of the proof discussion. In this case, the names AandBaren’t particularly important, since addition is commutat ive (A+B=B+A). Thus the proof for the case in which Ais negative (and Bis not) is identical to the case in which Bis negative (and Ais not), except that all of the names are swapped. The term “without lo ss of generality” means that we consider the proof complete even with additional assumptions, in our case that A <0 andB≥0.","{'page_number': 13, 'textbook_name': 'ECE-120-student-notes', 'text': 'Lemma: Addition of two N-bit 2’s complement numbers AandBdoes not overﬂow if one of the numbers is negative and the other is not. Proof:We again make use of the constraints implied by the fact that AandBare represented as N-bit 2’s complement values. We can assume without loss of generality1, orw.l.o.g., thatA <0 andB≥0. Combining these constraints with the range representable by N-bit 2’s complement, we obtain −2N−1≤A <0 0≤B <2N−1 We add these two inequalities and replace A+BwithCto obtain −2N−1≤C <2N−1 But anything in the range speciﬁed by this inequality can be represen ted with N-bit 2’s complement, and thus the addition does not overﬂow. 1This common mathematical phrasing means that we are using a p roblem symmetry to cut down the length of the proof discussion. In this case, the names AandBaren’t particularly important, since addition is commutat ive (A+B=B+A). Thus the proof for the case in which Ais negative (and Bis not) is identical to the case in which Bis negative (and Ais not), except that all of the names are swapped. The term “without lo ss of generality” means that we consider the proof complete even with additional assumptions, in our case that A <0 andB≥0.'}"
"1.3 Overﬂow Conditions 9 We are now ready to state our main theorem. For convenience, let’s use diﬀerent names for the actual sumC=A+Band the sum Sreturned from the add unit. We deﬁne Sas the number represented by the bit pattern produced by the add unit. When overﬂow occurs, S/n⌉}ationslash=C, but we always have ( S=C) mod 2N. Theorem: Addition of two N-bit 2’s complement numbers AandBoverﬂows if and only if one of the following conditions holds: 1.A <0 andB <0 andS≥0 2.A≥0 andB≥0 andS <0 Proof:We once again start with the “if” direction. That is, if condition 1 or co ndition 2 holds, we have an overﬂow. The proofs are straightforward. Given condition 1, w e can add the two inequalities A <0 and B <0 to obtain C=A+B <0. ButS≥0, so clearly S/n⌉}ationslash=C, thus overﬂow has occurred. Similarly, if condition 2 holds, we can add the inequalities A≥0 andB≥0 to obtain C=A+B≥0. Here we have S <0, so again S/n⌉}ationslash=C, and we have an overﬂow. Wemustnowprovethe“onlyif”direction,showingthatanyoverﬂow implieseithercondition1orcondition2. Bythecontrapositive2ofourLemma,weknowthatifanoverﬂowoccurs,eitherbothoper andsarenegative, or they are both positive. Let’s start with the case in which both operands are negative, so A <0 andB <0, and thus the real sumC <0 as well. Given that AandBare represented as N-bit 2’s complement, they must fall in the representable range, so we can write −2N−1≤A <0 −2N−1≤B <0 We add these two inequalities and replace A+BwithCto obtain −2N≤C <0 Given that an overﬂow has occurred, Cmust fall outside of the representable range. Given that C <0, it cannot be larger than the largest possible number representable u singN-bit 2’s complement, so we can write −2N≤C <−2N−1 We now add 2Nto each part to obtain 0≤C+2N<2N−1 This range of integers falls within the representable range for N-bit 2’s complement, so we can replace the middle expression with S(equal to Cmodulo 2N) to ﬁnd that 0≤S <2N−1 Thus, if we have an overﬂow and both A <0 andB <0, the resulting sum S≥0, and condition 1 holds. The proof for the case in which we observe an overﬂow when both op erands are non-negative ( A≥0 and B≥0) is similar, and leads to condition 2","{'page_number': 14, 'textbook_name': 'ECE-120-student-notes', 'text': 'The proof for the case in which we observe an overﬂow when both op erands are non-negative ( A≥0 and B≥0) is similar, and leads to condition 2. We again begin with inequalities for AandB: 0≤A <2N−1 0≤B <2N−1 We add these two inequalities and replace A+BwithCto obtain 0≤C <2N 2If we have a statement of the form ( pimpliesq), its contrapositive is the statement (not qimplies not p). Both statements have the same truth value. In this case, we can turn our Lemma a round as stated.'}"
"The proof for the case in which we observe an overﬂow when both op erands are non-negative ( A≥0 and B≥0) is similar, and leads to condition 2. We again begin with inequalities for AandB: 0≤A <2N−1 0≤B <2N−1 We add these two inequalities and replace A+BwithCto obtain 0≤C <2N 2If we have a statement of the form ( pimpliesq), its contrapositive is the statement (not qimplies not p). Both statements have the same truth value. In this case, we can turn our Lemma a round as stated.","{'page_number': 14, 'textbook_name': 'ECE-120-student-notes', 'text': 'The proof for the case in which we observe an overﬂow when both op erands are non-negative ( A≥0 and B≥0) is similar, and leads to condition 2. We again begin with inequalities for AandB: 0≤A <2N−1 0≤B <2N−1 We add these two inequalities and replace A+BwithCto obtain 0≤C <2N 2If we have a statement of the form ( pimpliesq), its contrapositive is the statement (not qimplies not p). Both statements have the same truth value. In this case, we can turn our Lemma a round as stated.'}"
"10  Given that an overﬂow has occurred, Cmust fall outside of the representable range. Given that C≥0, it cannot be smaller than the smallest possible number representable u singN-bit 2’s complement, so we can write 2N−1≤C <2N We now subtract 2Nto each part to obtain −2N−1≤C−2N<0 This range of integers falls within the representable range for N-bit 2’s complement, so we can replace the middle expression with S(equal to Cmodulo 2N) to ﬁnd that −2N−1≤S <0 Thus, if we have an overﬂow and both A≥0 andB≥0, the resulting sum S <0, and condition 2 holds. Thus overﬂow implies either condition 1 or condition 2, completing our p roof.","{'page_number': 15, 'textbook_name': 'ECE-120-student-notes', 'text': '10  Given that an overﬂow has occurred, Cmust fall outside of the representable range. Given that C≥0, it cannot be smaller than the smallest possible number representable u singN-bit 2’s complement, so we can write 2N−1≤C <2N We now subtract 2Nto each part to obtain −2N−1≤C−2N<0 This range of integers falls within the representable range for N-bit 2’s complement, so we can replace the middle expression with S(equal to Cmodulo 2N) to ﬁnd that −2N−1≤S <0 Thus, if we have an overﬂow and both A≥0 andB≥0, the resulting sum S <0, and condition 2 holds. Thus overﬂow implies either condition 1 or condition 2, completing our p roof.'}"
"1.4 Logic Operations 11 ECE120: Introduction to Computer Engineering Notes Set 1.4 Logic Operations This set of notes brieﬂy describes a generalization to truth tables, then introduces Boolean logic operations as well as notational conventions and tools that we use to express general functions on bits. We illustrate how logic operations enable us to express functions such as overﬂo w conditions concisely, then show by con- struction that a small number of logic operations suﬃces to describ e any operation on any number of bits. We close by discussing a few implications and examples. 1.4.1 Truth Tables You have seen the basic form of truth tables in the textbook and in c lass. Over the semester, we will introduce several extensions to the basic co ncept, mostly with the goal of reducing the amount of writing necessary when using tr uth tables. For example, the truth table to the right uses two generalizations to sh ow the carry outC(also the unsigned overﬂow indicator) and the sum Sproduced by adding two 2-bit unsigned numbers. First, rather than writing each input b it separately, we have grouped pairs of input bits into the numbers AandB. Second, we have deﬁned multiple output columns so as to include both bits of Sas well as Cin the same table. Finally, we have grouped the two bits of Sinto one column. Keepinmindasyouwritetruthtablesthatonlyrarelydoesanoperat ioncorrespond to a simple and familiar process such as addition of base 2 numbers. We had to choose the unsigned and 2’s complement representationscarefully to allow ourselves to take advantage of a familiar process. In general, for each line of a truth table for an operation, you may need to make use of the input representatio n to identify the input values, calculate the operation’sresult as a value, and then tr anslate the value back into the correct bit pattern using the output representatio n. Signed magni- tude addition, forexample, does notalwayscorrespondto base2a ddition: when theinputs outputs A B C S 00 00 0 00 00 01 0 01 00 10 0 10 00 11 0 11 01 00 0 01 01 01 0 10 01 10 0 11 01 11 1 00 10 00 0 10 10 01 0 11 10 10 1 00 10 11 1 01 11 00 0 11 11 01 1 00 11 10 1 01 11 11 1 10 signs of the two input operands diﬀer, one should instead use base 2 subtraction. For other operations or representations, base 2 arithmetic may have no relevance at all. 1.4.2 Boolean Logic Operations In the middle of the 19thcentury, George Boole introduced a set of logic operations that ar e today known as Boolean logic (alsoasBoolean algebra ). These operationstoday form one ofthe lowestabstractionleve ls in digital systems, and an understanding of their meaning and use is c ritical to the eﬀective development of both hardware and software. You have probably seen these functions many times already in your e ducation—perhaps ﬁrst in set-theoretic form as Venn diagrams","{'page_number': 16, 'textbook_name': 'ECE-120-student-notes', 'text': 'These operationstoday form one ofthe lowestabstractionleve ls in digital systems, and an understanding of their meaning and use is c ritical to the eﬀective development of both hardware and software. You have probably seen these functions many times already in your e ducation—perhaps ﬁrst in set-theoretic form as Venn diagrams. However, given the use of common English wo rdswith diﬀerent meanings to name some of the functions, and the sometimes confusing associations m ade even by engineering educators, we want to provide you with a concise set of deﬁnitions that generalizes correctly to more than two operands. You may have learned these functions based on truth values (true and false), but we deﬁne them based on bits, with 1 representing true and 0 representing false. Table 1 on the next page lists logic operations. The ﬁrst column in the t able lists the name of each function. The second provides a fairly complete set of the notations that you are likely to encounter for each function, including both the forms used in engineering and those used in mathem atics. The third column deﬁnes the function’s value for two or more input operands (except for NOT, w hich operates on a single value). The last column shows the form generally used in logic schematics/diagrams an d mentions the important features used in distinguishing each function (in pictorial form usually called a gate, in reference to common physical implementations) from the others.'}"
"These operationstoday form one ofthe lowestabstractionleve ls in digital systems, and an understanding of their meaning and use is c ritical to the eﬀective development of both hardware and software. You have probably seen these functions many times already in your e ducation—perhaps ﬁrst in set-theoretic form as Venn diagrams. However, given the use of common English wo rdswith diﬀerent meanings to name some of the functions, and the sometimes confusing associations m ade even by engineering educators, we want to provide you with a concise set of deﬁnitions that generalizes correctly to more than two operands. You may have learned these functions based on truth values (true and false), but we deﬁne them based on bits, with 1 representing true and 0 representing false. Table 1 on the next page lists logic operations. The ﬁrst column in the t able lists the name of each function. The second provides a fairly complete set of the notations that you are likely to encounter for each function, including both the forms used in engineering and those used in mathem atics. The third column deﬁnes the function’s value for two or more input operands (except for NOT, w hich operates on a single value). The last column shows the form generally used in logic schematics/diagrams an d mentions the important features used in distinguishing each function (in pictorial form usually called a gate, in reference to common physical implementations) from the others.","{'page_number': 16, 'textbook_name': 'ECE-120-student-notes', 'text': 'These operationstoday form one ofthe lowestabstractionleve ls in digital systems, and an understanding of their meaning and use is c ritical to the eﬀective development of both hardware and software. You have probably seen these functions many times already in your e ducation—perhaps ﬁrst in set-theoretic form as Venn diagrams. However, given the use of common English wo rdswith diﬀerent meanings to name some of the functions, and the sometimes confusing associations m ade even by engineering educators, we want to provide you with a concise set of deﬁnitions that generalizes correctly to more than two operands. You may have learned these functions based on truth values (true and false), but we deﬁne them based on bits, with 1 representing true and 0 representing false. Table 1 on the next page lists logic operations. The ﬁrst column in the t able lists the name of each function. The second provides a fairly complete set of the notations that you are likely to encounter for each function, including both the forms used in engineering and those used in mathem atics. The third column deﬁnes the function’s value for two or more input operands (except for NOT, w hich operates on a single value). The last column shows the form generally used in logic schematics/diagrams an d mentions the important features used in distinguishing each function (in pictorial form usually called a gate, in reference to common physical implementations) from the others.'}"
"12  Function Notation Explanation Schematic ANDAANDB AB A·B A×B A∧Bthe “all” function: result is 1 iﬀ allinput operands are equal to 1A BAB ﬂat input, round output ORAORB A+B A∨Bthe “any” function: result is 1 iﬀ anyinput operand is equal to 1BAA+B round input, pointed output NOTNOTA A′ A ¬Alogical complement/negation: NOT 0 is 1, and NOT 1 is 0A A triangle and circle XOR exclusive ORAXORB A⊕Bthe “odd” function: result is 1 iﬀ an odd number of input operands are equal to 1BAA XOR B OR with two lines on input side English “or”A,B, orCthe “one of” function: result is 1 iﬀ exactly one ofthe input operands is equal to 1(not used) Table 1: Boolean logic operations, notation, deﬁnitions, and symbols","{'page_number': 17, 'textbook_name': 'ECE-120-student-notes', 'text': 'The ﬁrst function of importance is AND. Think of ANDas the “all” function: given a set of input values as operands, AND evaluates to 1 if and only if allof the input values are 1. The ﬁrst notation line simply uses the name of the function. In Boolean algebra, AND is typically re presented as multiplication, and the middle three forms reﬂect various ways in which we write multiplication. The last notational variant is from mathematics, where the AND function is formally called conjunction . The next function of importance is OR. Think of ORas the “any” function: given a set of input values as operands, OR evaluates to 1 if and only if anyof the input values is 1. The actual number of input values equal to 1 only matters in the sense of whether it is at least on e. The notation for OR is organized in the same way as for AND, with the function name at the top, the alg ebraic variant that we will use in class—in this case addition—in the middle, and the mathematics variant , in this case called disjunction , at the bottom. The deﬁnition of Boolean OR is not the same as our use of the wor d “or” in English. For example, if you are fortunate enough to enjoy a meal on a plane, you might be oﬀer ed several choices: “Would you like the chicken, the beef, or the vegetarian lasagna today?” Unaccepta ble answers to this English question include: “Yes,” “Chicken and lasagna,” and any other combination that involve s more than a single choice! You may have noticed that we might have instead mentioned that AND evaluates to 0 if any input value is 0, and that OR evaluates to 0 if all input values are 0. These relation ships reﬂect a mathematical duality underlying Boolean logic that has important practical value in terms o f making it easier for humans to digest complex logic expressions. We will talk more about duality later in the co urse, but you should learn some of the practical value now: if you are trying to evaluate an AND func tion, look for an input with value 0; if you are trying to evaluate an OR function, look for an input with valu e 1. If you ﬁnd such an input, you know the function’s value without calculating any other input values. We next consider the logical complement function, NOT. The NOT function is also called negation . Unlike our ﬁrst two functions, NOT accepts only a single operand, an d reverses its value, turning 0 into 1 and 1 into 0. The notation follows the same pattern: a version using t he function name at the top, followed by two variants used in Boolean algebra, and ﬁnally the version frequ ently used in mathematics. For the NOT gate, or inverter , the circle is actually the important part: the triangle by itself merely copies the input. You will see the small circle added to other gates on both input s and outputs; in both cases the circle implies a NOT function.'}"
"The ﬁrst function of importance is AND. Think of ANDas the “all” function: given a set of input values as operands, AND evaluates to 1 if and only if allof the input values are 1. The ﬁrst notation line simply uses the name of the function. In Boolean algebra, AND is typically re presented as multiplication, and the middle three forms reﬂect various ways in which we write multiplication. The last notational variant is from mathematics, where the AND function is formally called conjunction . The next function of importance is OR. Think of ORas the “any” function: given a set of input values as operands, OR evaluates to 1 if and only if anyof the input values is 1. The actual number of input values equal to 1 only matters in the sense of whether it is at least on e. The notation for OR is organized in the same way as for AND, with the function name at the top, the alg ebraic variant that we will use in class—in this case addition—in the middle, and the mathematics variant , in this case called disjunction , at the bottom. The deﬁnition of Boolean OR is not the same as our use of the wor d “or” in English. For example, if you are fortunate enough to enjoy a meal on a plane, you might be oﬀer ed several choices: “Would you like the chicken, the beef, or the vegetarian lasagna today?” Unaccepta ble answers to this English question include: “Yes,” “Chicken and lasagna,” and any other combination that involve s more than a single choice! You may have noticed that we might have instead mentioned that AND evaluates to 0 if any input value is 0, and that OR evaluates to 0 if all input values are 0. These relation ships reﬂect a mathematical duality underlying Boolean logic that has important practical value in terms o f making it easier for humans to digest complex logic expressions. We will talk more about duality later in the co urse, but you should learn some of the practical value now: if you are trying to evaluate an AND func tion, look for an input with value 0; if you are trying to evaluate an OR function, look for an input with valu e 1. If you ﬁnd such an input, you know the function’s value without calculating any other input values. We next consider the logical complement function, NOT. The NOT function is also called negation . Unlike our ﬁrst two functions, NOT accepts only a single operand, an d reverses its value, turning 0 into 1 and 1 into 0. The notation follows the same pattern: a version using t he function name at the top, followed by two variants used in Boolean algebra, and ﬁnally the version frequ ently used in mathematics. For the NOT gate, or inverter , the circle is actually the important part: the triangle by itself merely copies the input. You will see the small circle added to other gates on both input s and outputs; in both cases the circle implies a NOT function.","{'page_number': 17, 'textbook_name': 'ECE-120-student-notes', 'text': 'The ﬁrst function of importance is AND. Think of ANDas the “all” function: given a set of input values as operands, AND evaluates to 1 if and only if allof the input values are 1. The ﬁrst notation line simply uses the name of the function. In Boolean algebra, AND is typically re presented as multiplication, and the middle three forms reﬂect various ways in which we write multiplication. The last notational variant is from mathematics, where the AND function is formally called conjunction . The next function of importance is OR. Think of ORas the “any” function: given a set of input values as operands, OR evaluates to 1 if and only if anyof the input values is 1. The actual number of input values equal to 1 only matters in the sense of whether it is at least on e. The notation for OR is organized in the same way as for AND, with the function name at the top, the alg ebraic variant that we will use in class—in this case addition—in the middle, and the mathematics variant , in this case called disjunction , at the bottom. The deﬁnition of Boolean OR is not the same as our use of the wor d “or” in English. For example, if you are fortunate enough to enjoy a meal on a plane, you might be oﬀer ed several choices: “Would you like the chicken, the beef, or the vegetarian lasagna today?” Unaccepta ble answers to this English question include: “Yes,” “Chicken and lasagna,” and any other combination that involve s more than a single choice! You may have noticed that we might have instead mentioned that AND evaluates to 0 if any input value is 0, and that OR evaluates to 0 if all input values are 0. These relation ships reﬂect a mathematical duality underlying Boolean logic that has important practical value in terms o f making it easier for humans to digest complex logic expressions. We will talk more about duality later in the co urse, but you should learn some of the practical value now: if you are trying to evaluate an AND func tion, look for an input with value 0; if you are trying to evaluate an OR function, look for an input with valu e 1. If you ﬁnd such an input, you know the function’s value without calculating any other input values. We next consider the logical complement function, NOT. The NOT function is also called negation . Unlike our ﬁrst two functions, NOT accepts only a single operand, an d reverses its value, turning 0 into 1 and 1 into 0. The notation follows the same pattern: a version using t he function name at the top, followed by two variants used in Boolean algebra, and ﬁnally the version frequ ently used in mathematics. For the NOT gate, or inverter , the circle is actually the important part: the triangle by itself merely copies the input. You will see the small circle added to other gates on both input s and outputs; in both cases the circle implies a NOT function.'}"
"1.4 Logic Operations 13 Last among the Boolean logic functions, we have the XOR, orexclusive OR function. Think of XOR as the “odd” function: given a set of input values as operands, XOR ev aluates to 1 if and only if an odd number of the input values are 1. Only two variants of XOR notation are given : the ﬁrst using the function name, and the second used with Boolean algebra. Mathematics rarely uses this function. Finally, we have included the meaning of the word “or” in English as a sep arate function entry to enable you to compare that meaning with the Boolean logic functions easily. Note that many people refer to English’ use of the word “or” as “exclusive” because one true value excludes all others from being true. Do notletthishumanlanguageambiguityconfuseyou about XOR! For all logic design purposes, XOR is the odd function . The truth table to the right provides values il- lustrating these functions operating on three in- puts. The AND, OR, and XOR functions are all associative—( AopB) opC=Aop (BopC)— and commutative— AopB=BopA, as you may have already realized from their deﬁnitions.inputs outputs A B C ABC A +B+CA A⊕B⊕C 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 1 1.4.3 Overﬂow as Logic Expressions In the last set of notes, we discussed overﬂow conditions for unsig ned and 2’s complement representations. Let’s use Boolean logic to express these conditions. We begin with addition of two 1-bit unsigned numbers. Call the two inpu t bitsA0andB0. If you write a truth table for this operation, you’ll notice that overﬂow occurs o nly when all (two) bits are 1. If either bit is 0, the sum can’t exceed 1, so overﬂow cannot occur. In other wo rds, overﬂow in this case can be written using an AND operation: A0B0 The truth table for adding two 2-bit unsigned numbers is four times a s large, and seeing the structure may be diﬃcult. One way of writing the expression for overﬂow of 2-bit un signed addition is as follows: A1B1+(A1+B1)A0B0 This expression is slightly trickier to understand. Think about the pla ce value of the bits. If both of the most signiﬁcant bits—those with place value 2—are 1, we have an overﬂow, just as in the case of 1-bit addition. TheA1B1term represents this case. We also have an overﬂow if one or both ( the OR) of the most signiﬁcant bits are 1 and the sum of the two next signiﬁcant bits—in this case tho se with place value 1—generates a carry","{'page_number': 18, 'textbook_name': 'ECE-120-student-notes', 'text': 'TheA1B1term represents this case. We also have an overﬂow if one or both ( the OR) of the most signiﬁcant bits are 1 and the sum of the two next signiﬁcant bits—in this case tho se with place value 1—generates a carry. The truth table for adding two 3-bit unsigned numbers is probably no t something that you want to write out. Fortunately, a pattern should start to become clear with the following expression: A2B2+(A2+B2)A1B1+(A2+B2)(A1+B1)A0B0 In the 2-bit case, we mentioned the “most signiﬁcant bit” and the “n ext most signiﬁcant bit” to help you see the pattern. The same reasoning describes the ﬁrst two product terms in our overﬂow expression for 3-bit unsigned addition (but the place values are 4 for the most signiﬁcant bit and 2 for the next most signiﬁcant bit). The last term represents the overﬂow case in which the two lea st signiﬁcant bits generate a carry which then propagates up through all of the other bits because at least one of the two bits in every position is a 1.'}"
"TheA1B1term represents this case. We also have an overﬂow if one or both ( the OR) of the most signiﬁcant bits are 1 and the sum of the two next signiﬁcant bits—in this case tho se with place value 1—generates a carry. The truth table for adding two 3-bit unsigned numbers is probably no t something that you want to write out. Fortunately, a pattern should start to become clear with the following expression: A2B2+(A2+B2)A1B1+(A2+B2)(A1+B1)A0B0 In the 2-bit case, we mentioned the “most signiﬁcant bit” and the “n ext most signiﬁcant bit” to help you see the pattern. The same reasoning describes the ﬁrst two product terms in our overﬂow expression for 3-bit unsigned addition (but the place values are 4 for the most signiﬁcant bit and 2 for the next most signiﬁcant bit). The last term represents the overﬂow case in which the two lea st signiﬁcant bits generate a carry which then propagates up through all of the other bits because at least one of the two bits in every position is a 1.","{'page_number': 18, 'textbook_name': 'ECE-120-student-notes', 'text': 'TheA1B1term represents this case. We also have an overﬂow if one or both ( the OR) of the most signiﬁcant bits are 1 and the sum of the two next signiﬁcant bits—in this case tho se with place value 1—generates a carry. The truth table for adding two 3-bit unsigned numbers is probably no t something that you want to write out. Fortunately, a pattern should start to become clear with the following expression: A2B2+(A2+B2)A1B1+(A2+B2)(A1+B1)A0B0 In the 2-bit case, we mentioned the “most signiﬁcant bit” and the “n ext most signiﬁcant bit” to help you see the pattern. The same reasoning describes the ﬁrst two product terms in our overﬂow expression for 3-bit unsigned addition (but the place values are 4 for the most signiﬁcant bit and 2 for the next most signiﬁcant bit). The last term represents the overﬂow case in which the two lea st signiﬁcant bits generate a carry which then propagates up through all of the other bits because at least one of the two bits in every position is a 1.'}"
"14  The overﬂow condition for addition of two N-bit 2’s complement numbers can be written fairly concisely in terms of the ﬁrst bits of the two num bers and the ﬁrst bit of the sum. Recall that overﬂow in this case depend s only on whether the three numbers are negative or non-negative, which is given byAN−1AN−2...A2A1A0 +BN−1BN−2...B2B1B0 SN−1SN−2...S2S1S0 the most signiﬁcant bit. Given the bit names as shown to the right, we can write the overﬂow condition as follows: AN−1BN−1SN−1+AN−1BN−1SN−1 The overﬂow condition does of course depend on all of the bits in the two numbers being added. In the expressionabove, wehavesimpliﬁed the formbyusing SN−1. ButSN−1depends on the bits AN−1andBN−1 as well as the carry out of bit ( N−2). Later in this set of notes, we present a technique with which you can derive an expression for an arbitrary Boolean logic function. As an exercise, after you have ﬁnished read ing these notes, try using that technique to derive an overﬂow expression for addition of two N-bit 2’s complement numbers based on AN−1,BN−1, and the carry out of bit ( N−2) (and into bit ( N−1)), which we might call CN−1. You might then cal- culateCN−1in terms of the rest of the bits of AandBusing the expressions for unsigned overﬂow just discussed. In the next month or so, you will learn how to derive more compact expressions yourself from truth tables or other representations of Boolean logic functions. 1.4.4 Logical Completeness Why do we feel that such a short list of functions is enough? If you t hink about the number of possible functions on Nbits, you might think that we need many more functions to be able to m anipulate bits. With 10 bits, for example, there are 21024such functions. Obviously, some of them have never been used in any computer system, but maybe we should deﬁne at least a few mor e logic operations? In fact, we do not even need XOR. The functions AND, OR, and NOT are suﬃcient, even if we only allow two input operands for AND and OR! The theorem below captures this idea, called logical completeness . In this case, we claim that the set of functions{AND, OR, NOT}is suﬃcient to express any operation on any ﬁnite number of variable s, where each variable is a bit. Theorem: Given enough 2-input AND, 2-input OR, and 1-input NOT functions, o ne can express any Boolean logic function on any ﬁnite number of variables. The proof of our theorem is by construction","{'page_number': 19, 'textbook_name': 'ECE-120-student-notes', 'text': 'In this case, we claim that the set of functions{AND, OR, NOT}is suﬃcient to express any operation on any ﬁnite number of variable s, where each variable is a bit. Theorem: Given enough 2-input AND, 2-input OR, and 1-input NOT functions, o ne can express any Boolean logic function on any ﬁnite number of variables. The proof of our theorem is by construction . In other words, we show a systematic approach for trans- forming an arbitrary Boolean logic function on an arbitrary number o f variables into a form that uses only AND, OR, and NOT functions on one or two operands. As a ﬁrst step , we remove the restriction on the number of inputs for the AND and OR functions. For this purpose, w e state and prove two lemmas , which are simpler theorems used to support the proof of a main theorem. Lemma 1: Given enough 2-input AND functions, one can express an AND funct ion on any ﬁnite number of variables. Proof:We prove the Lemma by induction .3Denote the number of inputs to a particular AND function byN. The base case is N= 2. Such an AND function is given. To complete the proof, we need only show that, given any number of AND functions with up to Ninputs, we can ex- press an AND function with N+1 inputs. To do so, we need merely use one 2-input AND function to join together the result of an N-input AND function with an additional input, as illustrated to the right.input N... AND of N+1 inputsinput N+1input 1 3We assume that you have seen proof by induction previously.'}"
"In this case, we claim that the set of functions{AND, OR, NOT}is suﬃcient to express any operation on any ﬁnite number of variable s, where each variable is a bit. Theorem: Given enough 2-input AND, 2-input OR, and 1-input NOT functions, o ne can express any Boolean logic function on any ﬁnite number of variables. The proof of our theorem is by construction . In other words, we show a systematic approach for trans- forming an arbitrary Boolean logic function on an arbitrary number o f variables into a form that uses only AND, OR, and NOT functions on one or two operands. As a ﬁrst step , we remove the restriction on the number of inputs for the AND and OR functions. For this purpose, w e state and prove two lemmas , which are simpler theorems used to support the proof of a main theorem. Lemma 1: Given enough 2-input AND functions, one can express an AND funct ion on any ﬁnite number of variables. Proof:We prove the Lemma by induction .3Denote the number of inputs to a particular AND function byN. The base case is N= 2. Such an AND function is given. To complete the proof, we need only show that, given any number of AND functions with up to Ninputs, we can ex- press an AND function with N+1 inputs. To do so, we need merely use one 2-input AND function to join together the result of an N-input AND function with an additional input, as illustrated to the right.input N... AND of N+1 inputsinput N+1input 1 3We assume that you have seen proof by induction previously.","{'page_number': 19, 'textbook_name': 'ECE-120-student-notes', 'text': 'In this case, we claim that the set of functions{AND, OR, NOT}is suﬃcient to express any operation on any ﬁnite number of variable s, where each variable is a bit. Theorem: Given enough 2-input AND, 2-input OR, and 1-input NOT functions, o ne can express any Boolean logic function on any ﬁnite number of variables. The proof of our theorem is by construction . In other words, we show a systematic approach for trans- forming an arbitrary Boolean logic function on an arbitrary number o f variables into a form that uses only AND, OR, and NOT functions on one or two operands. As a ﬁrst step , we remove the restriction on the number of inputs for the AND and OR functions. For this purpose, w e state and prove two lemmas , which are simpler theorems used to support the proof of a main theorem. Lemma 1: Given enough 2-input AND functions, one can express an AND funct ion on any ﬁnite number of variables. Proof:We prove the Lemma by induction .3Denote the number of inputs to a particular AND function byN. The base case is N= 2. Such an AND function is given. To complete the proof, we need only show that, given any number of AND functions with up to Ninputs, we can ex- press an AND function with N+1 inputs. To do so, we need merely use one 2-input AND function to join together the result of an N-input AND function with an additional input, as illustrated to the right.input N... AND of N+1 inputsinput N+1input 1 3We assume that you have seen proof by induction previously.'}"
"1.4 Logic Operations 15 Lemma 2: Given enough 2-input OR functions, one can express an OR function on any ﬁnite number of variables. Proof:The proof of Lemma 2 is identical in structure to that of Lemma 1, bu t uses OR functions instead of AND functions. Let’s now consider a small subset of functions on Nvariables. For any such function, you can write out the truth table for the function. The output of a logic function is just a bit, either a 0 or a 1. Let’s consider the set of functions on Nvariables that produce a 1 for exactly one combination of the Nvariables. In other words, if you were to write out the truth table for such a function, exactly one row in the truth table would have output value 1, while all other rows had output value 0. Lemma 3: Given enough AND functions and 1-input NOT functions, one can exp ress any Boolean logic function that produces a 1 for exactly one combination of any ﬁnite number of variables. Proof:The proof of Lemma 3 is by construction. Let Nbe the number of variables on which the function operates. We construct a minterm on these Nvariables, which is an AND operation on each variable or its complement. The minterm is speciﬁed by looking at the unique combinat ion of variable values that produces a 1 result for the function. Each variable that must be a 1 is included a s itself, while each variable that must be a 0 is included as the variable’s complement (using a NOT function). T he resulting minterm produces the desired function exactly. When the variables all match the values fo r which the function should produce 1, the inputs to the AND function are all 1, and the function produces 1. When any variable does not match the value for which the function should produce 1, that variable (or its complement) acts as a 0 input to the AND function, and the function produces a 0, as desired. The table below shows all eight minterms for three variables. inputs outputs A B C ABCAB CA BCA B C A BC AB C A B C A B C 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 We are now ready to prove our theorem. Proof (of Theorem): Any given function on Nvariables produces the value 1 for some set of combinations of inputs. Let’s say that Msuch combinations produce 1. Note that M≤2N. For each combination that produces 1, we can use Lemma 1 to construct an N-input AND function. Then, using Lemma 3, we can use as many as MNOT functions and the N-input AND function to construct a minterm for that input combination. Finally, using Lemma 2, we can construct an M-input OR function and OR together all of the minterms. The result of the OR is the desired function","{'page_number': 20, 'textbook_name': 'ECE-120-student-notes', 'text': 'Let’s say that Msuch combinations produce 1. Note that M≤2N. For each combination that produces 1, we can use Lemma 1 to construct an N-input AND function. Then, using Lemma 3, we can use as many as MNOT functions and the N-input AND function to construct a minterm for that input combination. Finally, using Lemma 2, we can construct an M-input OR function and OR together all of the minterms. The result of the OR is the desired function. If the fu nction should produce a 1 for some combination of inputs, that combination’s minterm providesa 1 input t o the OR, which in turn produces a 1. If a combination should produce a 0, its minterm does not appear in th e OR; all other minterms produce 0 for that combination, and thus all inputs to the OR are 0 in such case s, and the OR produces 0, as desired. The construction that we used to prove logical completeness does not necessarily help with eﬃcient design of logic functions. Think about some of the expressions that we disc ussed earlier in these notes for overﬂow conditions. How many minterms do you need for N-bit unsigned overﬂow? A single Boolean logic function can be expressed in many diﬀerent ways, and learning how to develop an eﬃcient implementation of a function as well as how to determine whether two logic expressions a re identical without actually writing out truth tables are important engineering skills that you will start t o learn in the coming months.'}"
"Let’s say that Msuch combinations produce 1. Note that M≤2N. For each combination that produces 1, we can use Lemma 1 to construct an N-input AND function. Then, using Lemma 3, we can use as many as MNOT functions and the N-input AND function to construct a minterm for that input combination. Finally, using Lemma 2, we can construct an M-input OR function and OR together all of the minterms. The result of the OR is the desired function. If the fu nction should produce a 1 for some combination of inputs, that combination’s minterm providesa 1 input t o the OR, which in turn produces a 1. If a combination should produce a 0, its minterm does not appear in th e OR; all other minterms produce 0 for that combination, and thus all inputs to the OR are 0 in such case s, and the OR produces 0, as desired. The construction that we used to prove logical completeness does not necessarily help with eﬃcient design of logic functions. Think about some of the expressions that we disc ussed earlier in these notes for overﬂow conditions. How many minterms do you need for N-bit unsigned overﬂow? A single Boolean logic function can be expressed in many diﬀerent ways, and learning how to develop an eﬃcient implementation of a function as well as how to determine whether two logic expressions a re identical without actually writing out truth tables are important engineering skills that you will start t o learn in the coming months.","{'page_number': 20, 'textbook_name': 'ECE-120-student-notes', 'text': 'Let’s say that Msuch combinations produce 1. Note that M≤2N. For each combination that produces 1, we can use Lemma 1 to construct an N-input AND function. Then, using Lemma 3, we can use as many as MNOT functions and the N-input AND function to construct a minterm for that input combination. Finally, using Lemma 2, we can construct an M-input OR function and OR together all of the minterms. The result of the OR is the desired function. If the fu nction should produce a 1 for some combination of inputs, that combination’s minterm providesa 1 input t o the OR, which in turn produces a 1. If a combination should produce a 0, its minterm does not appear in th e OR; all other minterms produce 0 for that combination, and thus all inputs to the OR are 0 in such case s, and the OR produces 0, as desired. The construction that we used to prove logical completeness does not necessarily help with eﬃcient design of logic functions. Think about some of the expressions that we disc ussed earlier in these notes for overﬂow conditions. How many minterms do you need for N-bit unsigned overﬂow? A single Boolean logic function can be expressed in many diﬀerent ways, and learning how to develop an eﬃcient implementation of a function as well as how to determine whether two logic expressions a re identical without actually writing out truth tables are important engineering skills that you will start t o learn in the coming months.'}"
"16  1.4.5 Implications of Logical Completeness If logical completeness doesn’t really help us to engineer logic functio ns, why is the idea important? Think back to the layers of abstraction and the implementation of bits fro m the ﬁrst couple of lectures. Voltages are real numbers, not bits. The device layer implementations of Boolean logic function s must abstract away the analog properties of the physical system. Without such abstraction, we must think carefully about analog issues such as noise every time we make use of a bit! Logical complete ness assures us that no matter what we want to do with bits, implementating a handful of operations corr ectly is enough to guarantee that we never have to worry. A second important value of logical completeness is as a tool in scree ning potential new technologies for computers. If a new technology does not allow implementation of a log ically complete set of functions, the new technology is extremely unlikely to be successful in replacing the current one. That said,{AND, OR, and NOT }is not the only logically complete set of functions. In fact, our curre nt complementary metal-oxide semiconductor (CMOS) technology, on which most of the computer industry is now built, does not directly implement these functions, as you will see later in our class. The functions that are implemented directly in CMOS are NAND and NOR, which are abbreviations for AND followed by NOT and OR followed by NOT, respectively. Truth tables for the two are shown to the right. Either of these functions by itself forms a logically complete set. That is, both the set {NAND}and the set{NOR}are logically complete. For now, we leave the proof of this claim to you. Re-inputs outputs AB A+B A B ANANDB ANORB 0 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 member that all you need to show is that you can implement any set kn own to be logically complete, so in order to prove that {NAND}is logicallycomplete (for example), you need only show that you can imp lement AND, OR, and NOT using only NAND. 1.4.6 Examples and a Generalization Let’s use our construction to solve a few examples. We begin with the functions that we illustrated with the ﬁrst truth table from this set of notes, the carry out Cand sum Sof two 2-bit unsigned numbers. Since each output bit requires a separate expression, we now write S1S0for the two bits of the sum. We also need to be able to make use of the individual bits of the input values, so we write t hese asA1A0andB1B0, as shown on the left below. Using our construction from the logical completen ess theorem, we obtain the equations on the right. You should verify these expressions yourself.","{'page_number': 21, 'textbook_name': 'ECE-120-student-notes', 'text': '16  1.4.5 Implications of Logical Completeness If logical completeness doesn’t really help us to engineer logic functio ns, why is the idea important? Think back to the layers of abstraction and the implementation of bits fro m the ﬁrst couple of lectures. Voltages are real numbers, not bits. The device layer implementations of Boolean logic function s must abstract away the analog properties of the physical system. Without such abstraction, we must think carefully about analog issues such as noise every time we make use of a bit! Logical complete ness assures us that no matter what we want to do with bits, implementating a handful of operations corr ectly is enough to guarantee that we never have to worry. A second important value of logical completeness is as a tool in scree ning potential new technologies for computers. If a new technology does not allow implementation of a log ically complete set of functions, the new technology is extremely unlikely to be successful in replacing the current one. That said,{AND, OR, and NOT }is not the only logically complete set of functions. In fact, our curre nt complementary metal-oxide semiconductor (CMOS) technology, on which most of the computer industry is now built, does not directly implement these functions, as you will see later in our class. The functions that are implemented directly in CMOS are NAND and NOR, which are abbreviations for AND followed by NOT and OR followed by NOT, respectively. Truth tables for the two are shown to the right. Either of these functions by itself forms a logically complete set. That is, both the set {NAND}and the set{NOR}are logically complete. For now, we leave the proof of this claim to you. Re-inputs outputs AB A+B A B ANANDB ANORB 0 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 member that all you need to show is that you can implement any set kn own to be logically complete, so in order to prove that {NAND}is logicallycomplete (for example), you need only show that you can imp lement AND, OR, and NOT using only NAND. 1.4.6 Examples and a Generalization Let’s use our construction to solve a few examples. We begin with the functions that we illustrated with the ﬁrst truth table from this set of notes, the carry out Cand sum Sof two 2-bit unsigned numbers. Since each output bit requires a separate expression, we now write S1S0for the two bits of the sum. We also need to be able to make use of the individual bits of the input values, so we write t hese asA1A0andB1B0, as shown on the left below. Using our construction from the logical completen ess theorem, we obtain the equations on the right. You should verify these expressions yourself.'}"
"1.4 Logic Operations 17 inputs outputs A1A0B1B0C S1S0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0C=A1A0B1B0+A1A0B1B0+A1A0B1B0+ A1A0B1B0+A1A0B1B0+A1A0B1B0 S1=A1A0B1B0+A1A0B1B0+A1A0B1B0+ A1A0B1B0+A1A0B1B0+A1A0B1B0+ A1A0B1B0+A1A0B1B0 S0=A1A0B1B0+A1A0B1B0+A1A0B1B0+ A1A0B1B0+A1A0B1B0+A1A0B1B0+ A1A0B1B0+A1A0B1B0 Now let’s consider a new function. Given an 8-bit 2’s complement numbe r,A=A7A6A5A4A3A2A1A0, we want to compare it with the value -1. We know that we can construct this function using AND, OR, and NOT, but how? We start by writing the representation for -1, which is 11111111. If the number Amatches that representation, we want to produce a 1. If the number Adiﬀers in any bit, we want to produce a 0. The desired function has exactly one combination of inputs that pro duces a 1, so in fact we need only one minterm! In this case, we can compare with -1 by calculating the expr ession: A7·A6·A5·A4·A3·A2·A1·A0 Here we have explicitly included multiplication symbols to avoid confusion with our notation for groups of bits, as we used when naming the individual bits of A. In closing, we brieﬂy introduce a generalization of logic operations to groups of bits. Ourrepresentationsforintegers,realnumbers, andcharacte rsfromhumanlanguages all use more than one bit to represent a given value. When we use com puters, we often make use of multiple bits in groups in this way. A byte, for example, today means an ordered group of eight bits. We can extend our logic funct ions to operate on such groups by pairing bits from each of two groups and perform ing the logic operation on each pair","{'page_number': 22, 'textbook_name': 'ECE-120-student-notes', 'text': 'Ourrepresentationsforintegers,realnumbers, andcharacte rsfromhumanlanguages all use more than one bit to represent a given value. When we use com puters, we often make use of multiple bits in groups in this way. A byte, for example, today means an ordered group of eight bits. We can extend our logic funct ions to operate on such groups by pairing bits from each of two groups and perform ing the logic operation on each pair. For example, given A=A7A6A5A4A3A2A1A0= 01010101 andB=B7B6B5B4B3B2B1B0= 11110000, we calculate AANDBby computing the AND of each pair of bits, A7ANDB7,A6ANDB6, and so forth, to produce the result 01010000, as shown to the right. In the same way, we ca n extend other logic operations, such as OR, NOT, and XOR, to operate on bits of gr oups.A01010101 ANDB11110000 01010000'}"
"Ourrepresentationsforintegers,realnumbers, andcharacte rsfromhumanlanguages all use more than one bit to represent a given value. When we use com puters, we often make use of multiple bits in groups in this way. A byte, for example, today means an ordered group of eight bits. We can extend our logic funct ions to operate on such groups by pairing bits from each of two groups and perform ing the logic operation on each pair. For example, given A=A7A6A5A4A3A2A1A0= 01010101 andB=B7B6B5B4B3B2B1B0= 11110000, we calculate AANDBby computing the AND of each pair of bits, A7ANDB7,A6ANDB6, and so forth, to produce the result 01010000, as shown to the right. In the same way, we ca n extend other logic operations, such as OR, NOT, and XOR, to operate on bits of gr oups.A01010101 ANDB11110000 01010000","{'page_number': 22, 'textbook_name': 'ECE-120-student-notes', 'text': 'Ourrepresentationsforintegers,realnumbers, andcharacte rsfromhumanlanguages all use more than one bit to represent a given value. When we use com puters, we often make use of multiple bits in groups in this way. A byte, for example, today means an ordered group of eight bits. We can extend our logic funct ions to operate on such groups by pairing bits from each of two groups and perform ing the logic operation on each pair. For example, given A=A7A6A5A4A3A2A1A0= 01010101 andB=B7B6B5B4B3B2B1B0= 11110000, we calculate AANDBby computing the AND of each pair of bits, A7ANDB7,A6ANDB6, and so forth, to produce the result 01010000, as shown to the right. In the same way, we ca n extend other logic operations, such as OR, NOT, and XOR, to operate on bits of gr oups.A01010101 ANDB11110000 01010000'}"
"18  ECE120: Introduction to Computer Engineering Notes Set 1.5 Programming Concepts and the C Language This set of notes introduces the C programming language and explain s some basic concepts in computer programming. Our purpose in showing you a high-level language at th is early stage of the course is to give you time to become familiar with the syntax and meaning of the languag e, not to teach you how to program. Throughout this semester, we will use software written in C to demo nstrate and validate the digital system design material in our course. Towards the end of the semester, y ou will learn to program computers using instructions and assembly language. In ECE 220, you will make use of the C language to write programs, at which point already being familiar with the language will make the materia l easier to master. These notes are meant to complement the introduction provided by Patt and Pat el. After a brief introduction to the history of C and the structure of a program written in C, we connect the idea of representations developed in class to the data types used in high-level languages. We next discuss the use of variables in C, then describe some of the operators available t o the programmer, including arithmetic and logic operators. The notes next introduce C functions that su pport the ability to read user input from the keyboard and to print results to the monitor. A description of t he structure of statements in C follows, explaininghowprogramsareexecutedandhowaprogrammercancr eatestatementsforconditionalexecution as well as loops to perform repetitive tasks. The main portion of the notes concludes with an example program, which is used to illustrate both the execution of C stateme nts as well as the diﬀerence between variables in programs and variables in algebra. The remainder of the notes covers more advanced topics. First, w e describe how the compilation process works, illustrating how a program written in a high-level language is tr ansformed into instructions. You will learn this process in much more detail in ECE 220. Second, we brieﬂy in troduce the C preprocessor. Finally, we discuss implicit and explicit data type conversion in C. Sections marked with an asterisk are provided solely for your interest, but you probably need to learn this material in later classes. 1.5.1 The C Programming Language Programminglanguagesattempt tobridgethe semanticgapbetwee n humandescriptionsofproblemsand the relatively simple instructions that can be provided by an instruction s et architecture (ISA). Since 1954, when the Fortran language ﬁrst enabled scientists to enter FORmulae sy mbolically and to have them TRANslated automatically into instructions, people have invented thousands of computer languages. The C programming language was developed by Den- nis Ritchie at Bell Labs in order to simplify the task of writing the Unix operating system. The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class. The syn- taxused in C—that is, the rules that one must follow to write valid C programs—hasalso heavily inﬂuenced manyothermorerecentlanguages,suchasC++,Java, and Perl. Forourpurposes,aCprogramconsistsofasetof vari- able declarations and a sequence of statements","{'page_number': 23, 'textbook_name': 'ECE-120-student-notes', 'text': 'The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class. The syn- taxused in C—that is, the rules that one must follow to write valid C programs—hasalso heavily inﬂuenced manyothermorerecentlanguages,suchasC++,Java, and Perl. Forourpurposes,aCprogramconsistsofasetof vari- able declarations and a sequence of statements .int main () { int answer = 42; /* the Answer! */ printf (""The answer is %d. \\n"", answer); /* Our work here is done. Let’s get out of here! */ return 0; } Both of these parts are written into a single C function called main, which executes when the program starts. A simple example appears to the right. The program uses one variable calledanswer, which it initializes to the value 42. The program prints a line of output to the monitor fo r the user, then terminates using thereturnstatement. Comments for human readers begin with the characters /*(a slash followed by an asterisk) and end with the characters */(an asterisk followed by a slash). The C language ignores white space in programs, so we encourage you to use blank lines and extra spacing to make your programs easier to read.'}"
"The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class. The syn- taxused in C—that is, the rules that one must follow to write valid C programs—hasalso heavily inﬂuenced manyothermorerecentlanguages,suchasC++,Java, and Perl. Forourpurposes,aCprogramconsistsofasetof vari- able declarations and a sequence of statements .int main () { int answer = 42; /* the Answer! */ printf (""The answer is %d. \n"", answer); /* Our work here is done. Let’s get out of here! */ return 0; } Both of these parts are written into a single C function called main, which executes when the program starts. A simple example appears to the right. The program uses one variable calledanswer, which it initializes to the value 42. The program prints a line of output to the monitor fo r the user, then terminates using thereturnstatement. Comments for human readers begin with the characters /*(a slash followed by an asterisk) and end with the characters */(an asterisk followed by a slash). The C language ignores white space in programs, so we encourage you to use blank lines and extra spacing to make your programs easier to read.","{'page_number': 23, 'textbook_name': 'ECE-120-student-notes', 'text': 'The C language provides a fairly transparent mapping to typical ISAs, which makes it a good choice both for system software such as operating systems and for our class. The syn- taxused in C—that is, the rules that one must follow to write valid C programs—hasalso heavily inﬂuenced manyothermorerecentlanguages,suchasC++,Java, and Perl. Forourpurposes,aCprogramconsistsofasetof vari- able declarations and a sequence of statements .int main () { int answer = 42; /* the Answer! */ printf (""The answer is %d. \\n"", answer); /* Our work here is done. Let’s get out of here! */ return 0; } Both of these parts are written into a single C function called main, which executes when the program starts. A simple example appears to the right. The program uses one variable calledanswer, which it initializes to the value 42. The program prints a line of output to the monitor fo r the user, then terminates using thereturnstatement. Comments for human readers begin with the characters /*(a slash followed by an asterisk) and end with the characters */(an asterisk followed by a slash). The C language ignores white space in programs, so we encourage you to use blank lines and extra spacing to make your programs easier to read.'}"
"1.5 Programming Concepts and the C Language 19 The variables deﬁned in the mainfunction allow a programmer to associate arbitrary symbolic names (sequences of English characters, such as “sum” or “product” o r “highScore”) with speciﬁc types of data, such as a16-bit unsigned integeror a double-precisionﬂoating-poin t number. In the example programabove, the variable answeris declared to be a 32-bit 2’s complement number. Those with no programming experience may at ﬁrst ﬁnd the diﬀerenc e between variables in algebra and variables in programs slightly confusing. As a program executes, the values of variables can change fro m step to step of execution. The statements in the mainfunction are executed one by one until the program terminates. P rograms are not limited to simple sequences of statements, however. Some type s of statements allow a programmer to specify conditional behavior. For example, a program might only prin t out secret information if the user’s name is “lUmeTTa.” Other types of statements allow a programmer to repeat the execution of a group of statements until a condition is met. For example, a program might pr int the numbers from 1 to 10, or ask for input until the user types a number between 1 and 10. The orde r of statement execution is well-deﬁned in C, but the statements in maindo not necessarily make up an algorithm: we can easily write a C program that never terminates . If a program terminates, the mainfunction returns an integer to the operating system, usually by ex ecuting areturnstatement, as in the example program. By convention, returning t he value 0 indicates successful completion of the program, while any non-zero value indicates a prog ram-speciﬁc error. However, mainis not necessarily a function in the mathematical sense because the value returned from mainis not necessarily unique for a given set of input values to the program . For example, we can write a program that selects a number from 1 to 10 at random and returns the number to the oper ating system. 1.5.2 Data Types As you know, modern digital computers represent all information w ith binary digits (0s and 1s), or bits. Whether you are representing something as simple as an integer or a s complex as an undergraduate thesis, the data are simply a bunch of 0s and 1s inside a computer. For any giv en type of information, a human selects a data type for the information. A data type (often called just a type) consists of both a size in bits and a representation, such as the 2’s complement represent ation for signed integers, or the ASCII representation for English text. A representation is a way of encoding the things being represented as a set of bits, with each bit pattern corresponding to a unique object or thing. A typical ISA supports a handful of data types in hardwarein the s ense that it provideshardwaresupport for operations on those data types. The arithmetic logic units (ALUs) in most modern processors, for example, support addition and subtraction of both unsigned and 2’s compleme nt representations, with the speciﬁc data type (such as 16- or 64-bit 2’s complement) depending on the I SA","{'page_number': 24, 'textbook_name': 'ECE-120-student-notes', 'text': 'A typical ISA supports a handful of data types in hardwarein the s ense that it provideshardwaresupport for operations on those data types. The arithmetic logic units (ALUs) in most modern processors, for example, support addition and subtraction of both unsigned and 2’s compleme nt representations, with the speciﬁc data type (such as 16- or 64-bit 2’s complement) depending on the I SA. Data types and operations not supported by the ISA must be handled in software using a small set o f primitive operations, which form the instructions available in the ISA. Instructions usually include data movement instr uctions such as loads and stores and control instructions such as branches and subro utine calls in addition to arithmetic and logic operations. The last quarter of our class covers these concepts in more detail and explores their meaning using an example ISA from the textbook. In class, we emphasized the idea that digital systems such as compu ters do not interpret the meaning of bits. Rather, they do exactly what they have been designed to do, even if that design is meaningless. If, for example, you store a sequence of ASCII characters in a compu ter’s memory as and then write computer instructions to add consecutive groups of four characters as 2’s complement integers and to print the result to the screen, the computer will not complain about the fact that y our code produces meaningless garbage. In contrast, high-level languages typically require that a program mer associate a data type with each datum in order to reduce the chance that the bits making up an individual da tum are misused or misinterpreted accidentally. Attempts to interpret a set of bits diﬀerently usually g enerate at least a warning message, since'}"
"A typical ISA supports a handful of data types in hardwarein the s ense that it provideshardwaresupport for operations on those data types. The arithmetic logic units (ALUs) in most modern processors, for example, support addition and subtraction of both unsigned and 2’s compleme nt representations, with the speciﬁc data type (such as 16- or 64-bit 2’s complement) depending on the I SA. Data types and operations not supported by the ISA must be handled in software using a small set o f primitive operations, which form the instructions available in the ISA. Instructions usually include data movement instr uctions such as loads and stores and control instructions such as branches and subro utine calls in addition to arithmetic and logic operations. The last quarter of our class covers these concepts in more detail and explores their meaning using an example ISA from the textbook. In class, we emphasized the idea that digital systems such as compu ters do not interpret the meaning of bits. Rather, they do exactly what they have been designed to do, even if that design is meaningless. If, for example, you store a sequence of ASCII characters in a compu ter’s memory as and then write computer instructions to add consecutive groups of four characters as 2’s complement integers and to print the result to the screen, the computer will not complain about the fact that y our code produces meaningless garbage. In contrast, high-level languages typically require that a program mer associate a data type with each datum in order to reduce the chance that the bits making up an individual da tum are misused or misinterpreted accidentally. Attempts to interpret a set of bits diﬀerently usually g enerate at least a warning message, since","{'page_number': 24, 'textbook_name': 'ECE-120-student-notes', 'text': 'A typical ISA supports a handful of data types in hardwarein the s ense that it provideshardwaresupport for operations on those data types. The arithmetic logic units (ALUs) in most modern processors, for example, support addition and subtraction of both unsigned and 2’s compleme nt representations, with the speciﬁc data type (such as 16- or 64-bit 2’s complement) depending on the I SA. Data types and operations not supported by the ISA must be handled in software using a small set o f primitive operations, which form the instructions available in the ISA. Instructions usually include data movement instr uctions such as loads and stores and control instructions such as branches and subro utine calls in addition to arithmetic and logic operations. The last quarter of our class covers these concepts in more detail and explores their meaning using an example ISA from the textbook. In class, we emphasized the idea that digital systems such as compu ters do not interpret the meaning of bits. Rather, they do exactly what they have been designed to do, even if that design is meaningless. If, for example, you store a sequence of ASCII characters in a compu ter’s memory as and then write computer instructions to add consecutive groups of four characters as 2’s complement integers and to print the result to the screen, the computer will not complain about the fact that y our code produces meaningless garbage. In contrast, high-level languages typically require that a program mer associate a data type with each datum in order to reduce the chance that the bits making up an individual da tum are misused or misinterpreted accidentally. Attempts to interpret a set of bits diﬀerently usually g enerate at least a warning message, since'}"
"20  such re-interpretations of the bits are rarely intentional and thu s rarely correct. A compiler—a program that transforms code written in a high-level language into instruct ions—can also generate the proper type conversion instructions automatically when the transformations a re intentional, as is often the case with arithmetic. Some high-level languages, such as Java, prevent programmers f rom changing the type of a given datum. If you deﬁne a type that represents one of your favorite twenty co lors, for example, you are not allowed to turn a color into an integer, despite the fact that the color is represent ed as a handful of bits. Such languages are said to be strongly typed . The C language is not strongly typed, and programmers are free to interpret any bits in any manner they see ﬁt. Taking advantage of this ability in any but a few exceptional c ases, however, results in arcane and non-portable code, and is thus considered to be bad programming p ractice. We discuss conversion between types in more detail later in these notes. Each high-level language deﬁnes a number of primitive data types , which are always available. Most languages, including C, also provide ways of deﬁning new types in term s of primitive types, but we leave that part of C for ECE 220. The primitive data types in C include signed and unsigned integers of various sizes as well as single- and double-precision IEEE ﬂoating-point numb ers. The primitive integer types in C include both unsigned and 2’s complement representations. These types were originally deﬁned so as to give reasonable performance when code was ported. In particular, the inttype is intended to be the native integer type for the target ISA. Using data types supported directly in hardware is faster than using larger or smaller integer types. When C was standardized in 1989, these types were deﬁned so as to include a range of existing C compilers rather than requiring all com- pilers to produce uniform results. At the2’s complement unsigned 8 bits char unsigned char 16 bits short unsigned short short int unsigned short int 32 bits int unsigned unsigned int 32 orlong unsigned long 64 bits long int unsigned long int 64 bits long long unsigned long long long long int unsigned long long int time, most workstations and mainframes were 32-bit machines, while most personal computers were 16-bit machines, thus ﬂexibility was somewhat desirable. For the GCC compile r on Linux, the C integer data types are deﬁned in the table above. Although the intandlongtypes are usually the same, there is a semantic diﬀerence in common usage. In particular, on most architectures a nd most compilers, a longhas enough bits to identify a location in the computer’s memory, while an intmay not. When in doubt, the size in bytesof any type or variable can be found using the built-in C function sizeof. Over time, the ﬂexibility of size in C types has become less important (except for the embedded markets, where one often wants even more accurate bit-width control), and the fact that the size of an intcan vary from machine to machine and compiler to compiler has become more a source of headaches than a helpful feature","{'page_number': 25, 'textbook_name': 'ECE-120-student-notes', 'text': 'When in doubt, the size in bytesof any type or variable can be found using the built-in C function sizeof. Over time, the ﬂexibility of size in C types has become less important (except for the embedded markets, where one often wants even more accurate bit-width control), and the fact that the size of an intcan vary from machine to machine and compiler to compiler has become more a source of headaches than a helpful feature. In the late 1990s, a new set of ﬁxed-size types were recommended for inclusion2’s complement unsigned 8 bits int8t uint8t 16 bits int16t uint16t 32 bits int32t uint32t 64 bits int64t uint64t in the C library, reﬂecting the fact that many companies had already developed and were using such deﬁ- nitions to make their programs platform-independent. We encoura ge you to make use of these types, which are shown in the table above. In Linux, they can be made available by in cluding the stdint.h header ﬁle. Floating-point types in C include floatanddouble, which correspond respectively to single- and double- precision IEEE ﬂoating-point values. Although the 32-bit floattype can save memory compared with use of 64-bit doublevalues, C’s math library works with double-precision values, and single -precision data are uncommon in scientiﬁc and engineering codes. In contrast, single-p recision ﬂoating-point operations domi- nated the graphics industry until recently, and are still well-suppo rted even on today’s graphics processing units.'}"
"When in doubt, the size in bytesof any type or variable can be found using the built-in C function sizeof. Over time, the ﬂexibility of size in C types has become less important (except for the embedded markets, where one often wants even more accurate bit-width control), and the fact that the size of an intcan vary from machine to machine and compiler to compiler has become more a source of headaches than a helpful feature. In the late 1990s, a new set of ﬁxed-size types were recommended for inclusion2’s complement unsigned 8 bits int8t uint8t 16 bits int16t uint16t 32 bits int32t uint32t 64 bits int64t uint64t in the C library, reﬂecting the fact that many companies had already developed and were using such deﬁ- nitions to make their programs platform-independent. We encoura ge you to make use of these types, which are shown in the table above. In Linux, they can be made available by in cluding the stdint.h header ﬁle. Floating-point types in C include floatanddouble, which correspond respectively to single- and double- precision IEEE ﬂoating-point values. Although the 32-bit floattype can save memory compared with use of 64-bit doublevalues, C’s math library works with double-precision values, and single -precision data are uncommon in scientiﬁc and engineering codes. In contrast, single-p recision ﬂoating-point operations domi- nated the graphics industry until recently, and are still well-suppo rted even on today’s graphics processing units.","{'page_number': 25, 'textbook_name': 'ECE-120-student-notes', 'text': 'When in doubt, the size in bytesof any type or variable can be found using the built-in C function sizeof. Over time, the ﬂexibility of size in C types has become less important (except for the embedded markets, where one often wants even more accurate bit-width control), and the fact that the size of an intcan vary from machine to machine and compiler to compiler has become more a source of headaches than a helpful feature. In the late 1990s, a new set of ﬁxed-size types were recommended for inclusion2’s complement unsigned 8 bits int8t uint8t 16 bits int16t uint16t 32 bits int32t uint32t 64 bits int64t uint64t in the C library, reﬂecting the fact that many companies had already developed and were using such deﬁ- nitions to make their programs platform-independent. We encoura ge you to make use of these types, which are shown in the table above. In Linux, they can be made available by in cluding the stdint.h header ﬁle. Floating-point types in C include floatanddouble, which correspond respectively to single- and double- precision IEEE ﬂoating-point values. Although the 32-bit floattype can save memory compared with use of 64-bit doublevalues, C’s math library works with double-precision values, and single -precision data are uncommon in scientiﬁc and engineering codes. In contrast, single-p recision ﬂoating-point operations domi- nated the graphics industry until recently, and are still well-suppo rted even on today’s graphics processing units.'}"
"1.5 Programming Concepts and the C Language 21 1.5.3 Variable Declarations The function mainexecuted by a program begins with a list of variable declarations . Each declaration consists of two parts: a data type speciﬁcation and a comma-sepa rated list of variable names. Each variable declared can also be initialized by assigning an initial value. A few examples appear below. Notice that one can initialize a variable to have the same value as a second variable. int x = 42; /* a 2’s complement variable, initially equal to 42 */ int y = x; /* a second 2’s complement variable, initially equa l to x */ int z; /* a third 2’s complement variable with unknown initia l value */ double a, b, c, pi = 3.1416; /* * four double-precision IEEE floating-point variables * a, b, and c are initially of unknown value, while pi is * initially 3.1416 */ What happens if a programmer declares a variable but does not initializ e it? Remember that bits can only be 0 or 1. An uninitialized variable does have a value, but its value is unpr edictable. The compiler tries to detect uses of uninitialized variables, but sometimes it fails to do so , sountil you are more familiar with programming, you should always initialize every variable . Variable names, also called identiﬁers , can include both letters and digits in C. Good programming style requires that programmers select variable names that are meaning ful and are easy to distinguish from one another. Single letters are acceptable in some situations, but longe r names with meaning are likely to help people (including you!) understand your program. Variable names ar e also case-sensitive in C, which al- lows programmers to use capitalization to diﬀerentiate behavior and meaning, if desired. Some programs, for example, use identiﬁers with all capital letters to indicate variab les with values that remain constant for the program’s entire execution. However, the fact that ident iﬁers are case-sensitive also means that a programmercan declare distinct variables named variable ,Variable ,vaRIable ,vaRIabLe , andVARIABLE . We strongly discourage you from doing so. 1.5.4 Expressions and Operators Themainfunction also contains a sequence of statements. A statement is a complete speciﬁcation of a single step in the pro- gram’s execution. We explain the struc- ture of statements in the next section. Many statements in C include one or more expressions , which represent calculations such as arithmetic, comparisons, and logic operations. Each expression is in turn com- posed of operators andoperands . Here we give only a brief introduction to some of the operators available in the C language. We deliberately omit operators with more complicated meanings, as well as operators for which the original purpose was to make writing common operations a little shorter. Fortheinterestedreader,boththetextbook and ECE 220 give more detailed introduc- tions. The table to the right gives examples for the operators described here","{'page_number': 26, 'textbook_name': 'ECE-120-student-notes', 'text': 'Each expression is in turn com- posed of operators andoperands . Here we give only a brief introduction to some of the operators available in the C language. We deliberately omit operators with more complicated meanings, as well as operators for which the original purpose was to make writing common operations a little shorter. Fortheinterestedreader,boththetextbook and ECE 220 give more detailed introduc- tions. The table to the right gives examples for the operators described here.int i = 42, j = 1000; /* i = 0x0000002A, j = 0x000003E8 */ i + j →1042 i - 4 * j →-3958 -j→-1000 j / i →23 j % i →42 i & j →40 /* 0x00000028 */ i | j →1002 /* 0x000003EA */ i∧j→962 /* 0x000003C2 */ ∼i→-43 /* 0xFFFFFFD5 */ (∼i) >> 2 →-11 /* 0xFFFFFFF5 */ ∼((∼i) >> 4) →2 /* 0x00000002 */ j >> 4 →62 /* 0x0000003E */ j << 3 →8000 /* 0x00001F40 */ i > j →0 i <= j →1 i == j →0 j = i →42 /* ...and j is changed! */'}"
"Each expression is in turn com- posed of operators andoperands . Here we give only a brief introduction to some of the operators available in the C language. We deliberately omit operators with more complicated meanings, as well as operators for which the original purpose was to make writing common operations a little shorter. Fortheinterestedreader,boththetextbook and ECE 220 give more detailed introduc- tions. The table to the right gives examples for the operators described here.int i = 42, j = 1000; /* i = 0x0000002A, j = 0x000003E8 */ i + j →1042 i - 4 * j →-3958 -j→-1000 j / i →23 j % i →42 i & j →40 /* 0x00000028 */ i | j →1002 /* 0x000003EA */ i∧j→962 /* 0x000003C2 */ ∼i→-43 /* 0xFFFFFFD5 */ (∼i) >> 2 →-11 /* 0xFFFFFFF5 */ ∼((∼i) >> 4) →2 /* 0x00000002 */ j >> 4 →62 /* 0x0000003E */ j << 3 →8000 /* 0x00001F40 */ i > j →0 i <= j →1 i == j →0 j = i →42 /* ...and j is changed! */","{'page_number': 26, 'textbook_name': 'ECE-120-student-notes', 'text': 'Each expression is in turn com- posed of operators andoperands . Here we give only a brief introduction to some of the operators available in the C language. We deliberately omit operators with more complicated meanings, as well as operators for which the original purpose was to make writing common operations a little shorter. Fortheinterestedreader,boththetextbook and ECE 220 give more detailed introduc- tions. The table to the right gives examples for the operators described here.int i = 42, j = 1000; /* i = 0x0000002A, j = 0x000003E8 */ i + j →1042 i - 4 * j →-3958 -j→-1000 j / i →23 j % i →42 i & j →40 /* 0x00000028 */ i | j →1002 /* 0x000003EA */ i∧j→962 /* 0x000003C2 */ ∼i→-43 /* 0xFFFFFFD5 */ (∼i) >> 2 →-11 /* 0xFFFFFFF5 */ ∼((∼i) >> 4) →2 /* 0x00000002 */ j >> 4 →62 /* 0x0000003E */ j << 3 →8000 /* 0x00001F40 */ i > j →0 i <= j →1 i == j →0 j = i →42 /* ...and j is changed! */'}"
"22  Arithmetic operators in C include addition ( +), subtraction ( -), negation (a minus sign not preceded by another expression), multiplication ( *), division ( /), and modulus ( %). No exponentiation operator exists; instead, library routines are deﬁned for this purpose as well as for a range of more complex mathematical functions. C also supports bitwise operations on integer types, including AND ( &), OR (|), XOR (^), NOT (∼), and left (<<) and right ( >>) bit shifts. Right shifting a signed integer results in an arithmetic right shift (the sign bit is copied), while right shifting an unsigned integer results in alogical right shift (0 bits are inserted). A range of relational orcomparison operators are available, including equality ( ==), inequality ( !=), and relative order ( <,<=,>=, and>). All such operations evaluate to 1 to indicate a true relation and 0 t o indicate a false relation. Any non-zero value is considered to be true for the purposes of tests (for example, in anifstatement or a whileloop) in C—these statements are explained later in these notes. Assignment of a new value to a variable uses a single equal sign ( =) in C. For example, the expression A = Bcopies the value of variable Binto variable A, overwriting the bits representing the previous value of A. The use of two equal signs for an equality check and a single eq ual sign for assignment is a common source of errors, although modern compilers generally detect and warn about this typ e of mistake. Assignment in C does not solve equations, even simple equations. Writing “ A-4=B”, for example, generates a compiler error. You must solve such equations yourself to calculate the des ired new value of a single variable, such as “A=B+4.” For the purposes of our class, you must always write a single variab le on the left side of an assignment, and can write an arbitrary expression on the right side . Many operators can be combined into a single expression. When an ex pression has more than one operator, which operator is executed ﬁrst? The answer depends on the oper ators’precedence , a well-deﬁned order on operators that speciﬁes how to resolve the ambiguity. In the case of arithmetic, the C language’s precedence speciﬁcation matches the one that you learned in elementary schoo l. For example, 1+2*3evaluates to 7, not to 9, because multiplication has precedence over addition. For non- arithmetic operators, or for any case in which you do not know the precedence speciﬁcation for a language, do not look it up—other programmers will not remember the precedence ordering, either! Instead, add parentheses to make your expressions clear and easy to understand. 1.5.5 Basic I/O Themainfunction returns an integer to the operating system. Although we do not discuss how additional functions can be written in our class, we may sometimes make use of f unctions that have been written in advance by making callsto those functions. A function call is type of expression in C, but we leave further description for ECE 220","{'page_number': 27, 'textbook_name': 'ECE-120-student-notes', 'text': '1.5.5 Basic I/O Themainfunction returns an integer to the operating system. Although we do not discuss how additional functions can be written in our class, we may sometimes make use of f unctions that have been written in advance by making callsto those functions. A function call is type of expression in C, but we leave further description for ECE 220. In our class, we make use of only two additio nal functions to enable our programs to receive input from a user via the keyboard and to write output to the monitor for a user to read. Let’s start with output. The printffunction allows a program to print output to the monitor using a programmer-speciﬁc format. The “f” in printfstands for “formatted.”4When we want to use printf, we write a expression with the word printffollowed by a parenthesized, comma-separated list of expressions . The expressions in this list are called the arguments to theprintffunction. The ﬁrst argument to the printffunction is a format string—a sequence of ASCII characters betw een quotation marks—which tells the function what kind of information we want printed to the monitor as well as how to format that information. The remaining arguments are C e xpressions that give printfa copy of any values that we want printed. How does the format string specify the format? Most of the chara cters in the format string are simply printed to the monitor. In the ﬁrst example shown to on the next pa ge, we use printfto print a hello message followed by an ASCII newline character to move to the next line on the monitor. 4The original, unformatted variant of printing was never ava ilable in the C language. Go learn Fortran.'}"
"1.5.5 Basic I/O Themainfunction returns an integer to the operating system. Although we do not discuss how additional functions can be written in our class, we may sometimes make use of f unctions that have been written in advance by making callsto those functions. A function call is type of expression in C, but we leave further description for ECE 220. In our class, we make use of only two additio nal functions to enable our programs to receive input from a user via the keyboard and to write output to the monitor for a user to read. Let’s start with output. The printffunction allows a program to print output to the monitor using a programmer-speciﬁc format. The “f” in printfstands for “formatted.”4When we want to use printf, we write a expression with the word printffollowed by a parenthesized, comma-separated list of expressions . The expressions in this list are called the arguments to theprintffunction. The ﬁrst argument to the printffunction is a format string—a sequence of ASCII characters betw een quotation marks—which tells the function what kind of information we want printed to the monitor as well as how to format that information. The remaining arguments are C e xpressions that give printfa copy of any values that we want printed. How does the format string specify the format? Most of the chara cters in the format string are simply printed to the monitor. In the ﬁrst example shown to on the next pa ge, we use printfto print a hello message followed by an ASCII newline character to move to the next line on the monitor. 4The original, unformatted variant of printing was never ava ilable in the C language. Go learn Fortran.","{'page_number': 27, 'textbook_name': 'ECE-120-student-notes', 'text': '1.5.5 Basic I/O Themainfunction returns an integer to the operating system. Although we do not discuss how additional functions can be written in our class, we may sometimes make use of f unctions that have been written in advance by making callsto those functions. A function call is type of expression in C, but we leave further description for ECE 220. In our class, we make use of only two additio nal functions to enable our programs to receive input from a user via the keyboard and to write output to the monitor for a user to read. Let’s start with output. The printffunction allows a program to print output to the monitor using a programmer-speciﬁc format. The “f” in printfstands for “formatted.”4When we want to use printf, we write a expression with the word printffollowed by a parenthesized, comma-separated list of expressions . The expressions in this list are called the arguments to theprintffunction. The ﬁrst argument to the printffunction is a format string—a sequence of ASCII characters betw een quotation marks—which tells the function what kind of information we want printed to the monitor as well as how to format that information. The remaining arguments are C e xpressions that give printfa copy of any values that we want printed. How does the format string specify the format? Most of the chara cters in the format string are simply printed to the monitor. In the ﬁrst example shown to on the next pa ge, we use printfto print a hello message followed by an ASCII newline character to move to the next line on the monitor. 4The original, unformatted variant of printing was never ava ilable in the C language. Go learn Fortran.'}"
"1.5 Programming Concepts and the C Language 23 The percent sign—“%”—is used as anescape character in the printffunction. When “%” ap- pears in the format string, the function examines the next char- acter in the format string to de- termine which format to use, then takes the next expression from the sequence of arguments and prints the value of that expression to the monitor. Evaluating an expression generates a bunch of bits, so it is up to the programmer to ensure that those bits are not misinter- preted. In other words, the pro- grammer must make sure that the numberand types offormatted val- ues match the number and types of arguments passed to printf(not counting the format string itself). Theprintffunction returns the numberofcharactersprintedtothe monitor.printf (""Hello, world! \n""); output: Hello, world! [and a newline] printf (""To %x or not to %d... \n"", 190, 380 / 2); output: To be or not to 190... [and a newline] printf (""My favorite number is %c%c. \n"", 0x34, ’0’+2); output: My favorite number is 42. [and a newline] printf (""What is pi? %f or %e? \n"", 3.1416, 3.1416); output: What is pi? 3.141600 or 3.141600e+00? [and a newline] escape sequence printffunction’s interpretation of expression bits %c2’s complement integer printed as an ASCII character %d2’s complement integer printed as decimal %edouble printed in decimal scientiﬁc notation %fdouble printed in decimal %uunsigned integer printed as decimal %xinteger printed as hexadecimal (lower case) %Xinteger printed as hexadecimal (upper case) %%a single percent sign A program can read input from the user with thescanffunction. The user enters charac- ters in ASCII using the keyboard, and the scanffunction converts the user’s input into Cprimitivetypes, storingtheresultsintovari- ables. As with printf, thescanffunction takes a format string followed by a comma- separated list of arguments. Each argument after the format string provides scanfwith the memory address of a variable into which the function can store a result. How does scanfuse the format string? For scanf, the format string is usually just a se- quence of conversions, one for each variable to be typed in by the user. As with printf, the conversions start with “%” and are followed by characters specifying the type of conver- siontobeperformed. Theﬁrstexampleshown to the right reads two integers. The conver- sions in the format string can be separated by spaces for readability, as shown in the exam-int a, b; /* example variables */ char c; unsigned u; double d; float f; scanf (""%d%d"", &a, &b); /* These have the */ scanf (""%d %d"", &a, &b); /* same effect","{'page_number': 28, 'textbook_name': 'ECE-120-student-notes', 'text': 'Theﬁrstexampleshown to the right reads two integers. The conver- sions in the format string can be separated by spaces for readability, as shown in the exam-int a, b; /* example variables */ char c; unsigned u; double d; float f; scanf (""%d%d"", &a, &b); /* These have the */ scanf (""%d %d"", &a, &b); /* same effect. */ eﬀect: try to convert two integers typed in decimal to 2’s complement and store the results in aandb scanf (""%c%x %lf"", &c, &u, &d); eﬀect: try to read an ASCII character into c, a value typed in hexadecimal into u, and a double- precision ﬂoating-point number into d scanf (""%lf %f"", &d, &f); eﬀect: try to read two real numbers typed as decimal, convert the ﬁrst to double-precision and store it ind, and convert the second to single-precision and store it in f ple. The spaces are ignored by scanf. How- ever,any non-space characters in the format string must be typed exactly by the user! The remaining arguments to scanfspecify memory addresses where the function can store the converted values. The ampersand (“&”) in front of each variable name in the examples is an operator that returns the ad- dress of a variable in memory. For each con-escape sequence scanffunction’s conversion to bits %cstore one ASCII character (as char) %dconvert decimal integer to 2’s complement %fconvert decimal real number to ﬂoat %lf convert decimal real number to double %uconvert decimal integer to unsigned int %xconvert hexadecimal integer to unsigned int %X(as above)'}"
"Theﬁrstexampleshown to the right reads two integers. The conver- sions in the format string can be separated by spaces for readability, as shown in the exam-int a, b; /* example variables */ char c; unsigned u; double d; float f; scanf (""%d%d"", &a, &b); /* These have the */ scanf (""%d %d"", &a, &b); /* same effect. */ eﬀect: try to convert two integers typed in decimal to 2’s complement and store the results in aandb scanf (""%c%x %lf"", &c, &u, &d); eﬀect: try to read an ASCII character into c, a value typed in hexadecimal into u, and a double- precision ﬂoating-point number into d scanf (""%lf %f"", &d, &f); eﬀect: try to read two real numbers typed as decimal, convert the ﬁrst to double-precision and store it ind, and convert the second to single-precision and store it in f ple. The spaces are ignored by scanf. How- ever,any non-space characters in the format string must be typed exactly by the user! The remaining arguments to scanfspecify memory addresses where the function can store the converted values. The ampersand (“&”) in front of each variable name in the examples is an operator that returns the ad- dress of a variable in memory. For each con-escape sequence scanffunction’s conversion to bits %cstore one ASCII character (as char) %dconvert decimal integer to 2’s complement %fconvert decimal real number to ﬂoat %lf convert decimal real number to double %uconvert decimal integer to unsigned int %xconvert hexadecimal integer to unsigned int %X(as above)","{'page_number': 28, 'textbook_name': 'ECE-120-student-notes', 'text': 'Theﬁrstexampleshown to the right reads two integers. The conver- sions in the format string can be separated by spaces for readability, as shown in the exam-int a, b; /* example variables */ char c; unsigned u; double d; float f; scanf (""%d%d"", &a, &b); /* These have the */ scanf (""%d %d"", &a, &b); /* same effect. */ eﬀect: try to convert two integers typed in decimal to 2’s complement and store the results in aandb scanf (""%c%x %lf"", &c, &u, &d); eﬀect: try to read an ASCII character into c, a value typed in hexadecimal into u, and a double- precision ﬂoating-point number into d scanf (""%lf %f"", &d, &f); eﬀect: try to read two real numbers typed as decimal, convert the ﬁrst to double-precision and store it ind, and convert the second to single-precision and store it in f ple. The spaces are ignored by scanf. How- ever,any non-space characters in the format string must be typed exactly by the user! The remaining arguments to scanfspecify memory addresses where the function can store the converted values. The ampersand (“&”) in front of each variable name in the examples is an operator that returns the ad- dress of a variable in memory. For each con-escape sequence scanffunction’s conversion to bits %cstore one ASCII character (as char) %dconvert decimal integer to 2’s complement %fconvert decimal real number to ﬂoat %lf convert decimal real number to double %uconvert decimal integer to unsigned int %xconvert hexadecimal integer to unsigned int %X(as above)'}"
"24  version in the format string, the scanffunction tries to convert input from the user into the appropriate result, then stores the result in memory at the address given by th e next argument. The programmer is responsible for ensuring that the number of conversions in the fo rmat string matches the number of arguments provided (not counting the format string itself). The p rogrammer must also ensure that the type of information produced by each conversioncan be stored at the a ddress passed for that conversion—inother words, the address of a variable with the correct type must be pro vided. Modern compilers often detect missing&operators and incorrect variable types, but many only give warning s to the programmer. The scanffunction itself cannot tell whether the arguments given to it are va lid or not. If a conversion fails—for example, if a user types “hello” when scanfexpects an integer— scanfdoes not overwrite the corresponding variable and immediately stops trying t o convert input. The scanffunction returns the number of successful conversions, allowing a progra mmer to check for bad input from the user. 1.5.6 Types of Statements in C Each statement in a C program speciﬁes a complete operation. Ther e are three types of statements, but two of these types can be constructed from additional statemen ts, which can in turn be constructed from additional statements. The C language speciﬁes no bound on this ty pe of recursive construction, but code readability does impose a practical limit. Thethreetypesareshowntotheright. They are the null statement ,simple state- ments, andcompound statements . A null statement is just a semicolon, and a compound statement is just a sequence of statements surrounded by braces. Simple statements can take several forms. All of the examples shown to the right, in- cluding the call to printf, are simple state-; /* a null statement (does nothing) */ A = B; /* examples of simple statements */ printf (""Hello, world! \n""); { /* a compound statement */ C = D; /* (a sequence of statements */ N = 4; /* between braces) */ L = D - N; } ments consisting of a C expression followed by a semicolon. Simple stat ements can also consist of conditionals or iterations, which we introduce next. Remember that after variable declarations, the mainfunction contains a sequence of state- ments. These statements are executed one at a time in the order g iven in the program, as shown to the right for two statements. We say that the statem ents are executed in sequential order. A program must also be able to execute statements only when some c ondition holds. In the C language, such a condition can be an arbitrary expression. Th e expression is ﬁrst evaluated. If the result is 0, the condition is considered to be false. Any result other than 0 is considered to be true. The C statement for conditional ex ecution is called an iffirst subtask subtasksecondsequential statement. Syntactically, we put the expression for the condition in parentheses after the keyword ifand follow the parenthesized expres- sion with a compound statement containing the statements that sh ould be executed when the condition is true","{'page_number': 29, 'textbook_name': 'ECE-120-student-notes', 'text': 'Th e expression is ﬁrst evaluated. If the result is 0, the condition is considered to be false. Any result other than 0 is considered to be true. The C statement for conditional ex ecution is called an iffirst subtask subtasksecondsequential statement. Syntactically, we put the expression for the condition in parentheses after the keyword ifand follow the parenthesized expres- sion with a compound statement containing the statements that sh ould be executed when the condition is true. Optionally, we can append th e keywordelseandasecondcompoundstatementcontainingstatements to be executed when the condition evaluates to false. The corresp ond- ing ﬂow chart is shown to the right. /* Set the variable y to the absolute value of variable x. */ if (0 <= x) {/* Is x greater or equal to 0? */ y = x; /* Then block: assign x to y. */ }else{ y = -x; /* Else block: assign negative x to y. */ }does some condition hold? subtask when condition holdsnot holdsubtask when condition doesconditional N Y then elseif'}"
"Th e expression is ﬁrst evaluated. If the result is 0, the condition is considered to be false. Any result other than 0 is considered to be true. The C statement for conditional ex ecution is called an iffirst subtask subtasksecondsequential statement. Syntactically, we put the expression for the condition in parentheses after the keyword ifand follow the parenthesized expres- sion with a compound statement containing the statements that sh ould be executed when the condition is true. Optionally, we can append th e keywordelseandasecondcompoundstatementcontainingstatements to be executed when the condition evaluates to false. The corresp ond- ing ﬂow chart is shown to the right. /* Set the variable y to the absolute value of variable x. */ if (0 <= x) {/* Is x greater or equal to 0? */ y = x; /* Then block: assign x to y. */ }else{ y = -x; /* Else block: assign negative x to y. */ }does some condition hold? subtask when condition holdsnot holdsubtask when condition doesconditional N Y then elseif","{'page_number': 29, 'textbook_name': 'ECE-120-student-notes', 'text': 'Th e expression is ﬁrst evaluated. If the result is 0, the condition is considered to be false. Any result other than 0 is considered to be true. The C statement for conditional ex ecution is called an iffirst subtask subtasksecondsequential statement. Syntactically, we put the expression for the condition in parentheses after the keyword ifand follow the parenthesized expres- sion with a compound statement containing the statements that sh ould be executed when the condition is true. Optionally, we can append th e keywordelseandasecondcompoundstatementcontainingstatements to be executed when the condition evaluates to false. The corresp ond- ing ﬂow chart is shown to the right. /* Set the variable y to the absolute value of variable x. */ if (0 <= x) {/* Is x greater or equal to 0? */ y = x; /* Then block: assign x to y. */ }else{ y = -x; /* Else block: assign negative x to y. */ }does some condition hold? subtask when condition holdsnot holdsubtask when condition doesconditional N Y then elseif'}"
"1.5 Programming Concepts and the C Language 25 If instead we chose to assign the absolute value of variable xto itself, we can do so without an elseblock: /* Set the variable x to its absolute value. */ if (0 > x) {/* Is x less than 0? */ x = -x; /* Then block: assign negative x to x. */ } /* No else block is given--no work is needed. */ Finally, wesometimesneedtorepeatedlyexecuteasetofstatemen ts, eitheraﬁxed number of times or so long as some condition holds. We refer to such r epetition as aniteration or aloop. In our class, we make use of C’s forloop when we need to perform such a task. A forloop is structured as follows: for ([initialization] ; [condition] ; [update]) { [subtask to be repeated] } A ﬂow chart corresponding to execution of a forloop appears to the right. First, any initialization is performed. Then the condition—again an arb itrary C expression—is checked. If the condition evaluates to false (exact ly 0), the loop is done. Otherwise, if the condition evaluates to true (any non-zer o value), the statements in the compound statement, the subtask or loop body , are executed. The loop body can contain anything: a sequence of simple statement s, a condi- tional, another loop, or even just an empty list. Once the loop body h as ﬁnished executing, the forloop update rule is executed. Execution then checks the con- dition again, and this process repeats until the condition evaluates to 0. The for loop below, for example, prints the numbers from 1 to 42. /* Print the numbers from 1 to 42. */ for (i = 1; 42 >= i; i = i + 1) { printf (""%d \n"", i); }one iterationsubtask forcondition holds for iterating? update for next iterationinitialize for first iterationiterative for N Y 1.5.7 Program Execution We arenow ready to consider the executionofasimple pro- gram, illustrating how vari- ables change value from step to step and determine pro- gram behavior. Let’s say that two numbers are “friends” if they have at least one 1 bit in com- mon when written in base 2. So, for example, 100 2and 1112are friends because both numbers have a 1 in the bit with place value22= 4. Sim- ilarly, 101 2and 010 2are not friends, since no bit position is 1 in both numbers. The program to the right prints all friendships between numbers in the interval [0 ,7].int main () { int check; /* number to check for friends */ int friend; /* a second number to consider as check’s friend * / /* Consider values of check from 0 to 7. */ for (check = 0; 8 > check; check = check + 1) { /* Consider values of friend from 0 to 7. */ for (friend = 0; 8 > friend; friend = friend + 1) { /* Use bitwise AND to see if the two share a 1 bit. */ if (0 != (check & friend)) { /* We have friendship! */ printf (""%d and %d are friends","{'page_number': 30, 'textbook_name': 'ECE-120-student-notes', 'text': '*/ for (check = 0; 8 > check; check = check + 1) { /* Consider values of friend from 0 to 7. */ for (friend = 0; 8 > friend; friend = friend + 1) { /* Use bitwise AND to see if the two share a 1 bit. */ if (0 != (check & friend)) { /* We have friendship! */ printf (""%d and %d are friends. \\n"", check, friend); } } } }'}"
"*/ for (check = 0; 8 > check; check = check + 1) { /* Consider values of friend from 0 to 7. */ for (friend = 0; 8 > friend; friend = friend + 1) { /* Use bitwise AND to see if the two share a 1 bit. */ if (0 != (check & friend)) { /* We have friendship! */ printf (""%d and %d are friends. \n"", check, friend); } } } }","{'page_number': 30, 'textbook_name': 'ECE-120-student-notes', 'text': '*/ for (check = 0; 8 > check; check = check + 1) { /* Consider values of friend from 0 to 7. */ for (friend = 0; 8 > friend; friend = friend + 1) { /* Use bitwise AND to see if the two share a 1 bit. */ if (0 != (check & friend)) { /* We have friendship! */ printf (""%d and %d are friends. \\n"", check, friend); } } } }'}"
"26  The program uses two integer variables, one for each of the numbe rs that we consider. We use a forloop to iterate over all values of our ﬁrst number, which we call check. The loop initializes checkto 0, continues until check reaches 8, and adds 1 to check after each loop iteratio n. We use a similar forloop to iterate over all possible values of our second number, which we call friend. For each pair of numbers, we determine whether they are friends using a bitwise AND operation. If the resu lt is non-zero, they are friends, and we print a message. If the two numbers are not friends, we do nothing , and the program moves on to consider the next pair of numbers. Now let’s think about what happens when this program executes. When the pro- gram starts, both variables are ﬁlled with random bits, so their values are unpre- dictable. The ﬁrst step is the initialization of the ﬁrst forloop, which sets check to 0. The condition for that loop is8 > check , which is true, so execution enters the loop body and starts to execute the ﬁrst statement, which is our second for loop. The next step is then the initialization code for the second forloop, which setsfriendto 0. The con- dition for the second loop is 8 > friend , which is true, so execution enters the loop body and starts to execute the ﬁrst statement, whichafter executing... checkis... and friendis... (variable declarations) unpredictable bits unpredictable bits check = 0 0 unpredictable bits 8 > check 0 unpredictable bits friend = 0 0 0 8 > friend 0 0 if (0 != (check & friend)) 0 0 friend = friend + 1 0 1 8 > friend 0 1 if (0 != (check & friend)) 0 1 friend = friend + 1 0 2 (repeat last three lines six more times; number 0 has no friends!) 8 > friend 0 8 check = check + 1 1 8 8 > check 1 8 friend = 0 1 0 8 > friend 1 0 if (0 != (check & friend)) 1 0 friend = friend + 1 1 1 8 > friend 1 1 if (0 != (check & friend)) 1 1 printf ... 1 1 (our ﬁrst friend!?) is theifstatement. Since both variables are 0, the ifcondition is false, and nothing is printed. Having ﬁnished the loop body for the inner loop (on friend), execution continues with the update rule for that loop—friend = friend + 1 —thenreturnstochecktheloop’sconditionagain. Thisprocessrep eats, always ﬁnding that the number 0 (in check) is not friends (0 has no friends!) until friendreaches 8, at which point the inner loop condition becomes false. Execution then moves t o the update rule for the ﬁrst forloop, which increments check. Check is then compared with 8 to see if the loop is done. Since it is not, we once again enter the loop body and start the second forloop over. The initialization code again sets friendto 0, and we move forward as before. As you see above, the ﬁrst time th at we ﬁnd our ifcondition to be true is when both checkandfriendare equal to 1","{'page_number': 31, 'textbook_name': 'ECE-120-student-notes', 'text': 'Check is then compared with 8 to see if the loop is done. Since it is not, we once again enter the loop body and start the second forloop over. The initialization code again sets friendto 0, and we move forward as before. As you see above, the ﬁrst time th at we ﬁnd our ifcondition to be true is when both checkandfriendare equal to 1. Is that result what you expected? To learn that the number 1 is frie nds with itself? If so, the program works. If you assumed that numbers could not be friends with them selves, perhaps we should ﬁx the bug? We could, for example, add another ifstatement to avoid printing anything when check == friend . Our program, you might also realize, prints each pair of friends twice . The numbers 1 and 3, for example, are printed in both possible orders. To eliminate this redundancy, we can change the initialization in the secondforloop, either to friend = check or tofriend = check + 1 , depending on how we want to deﬁne friendship (the same question as before: can a number be friends w ith itself?).'}"
"Check is then compared with 8 to see if the loop is done. Since it is not, we once again enter the loop body and start the second forloop over. The initialization code again sets friendto 0, and we move forward as before. As you see above, the ﬁrst time th at we ﬁnd our ifcondition to be true is when both checkandfriendare equal to 1. Is that result what you expected? To learn that the number 1 is frie nds with itself? If so, the program works. If you assumed that numbers could not be friends with them selves, perhaps we should ﬁx the bug? We could, for example, add another ifstatement to avoid printing anything when check == friend . Our program, you might also realize, prints each pair of friends twice . The numbers 1 and 3, for example, are printed in both possible orders. To eliminate this redundancy, we can change the initialization in the secondforloop, either to friend = check or tofriend = check + 1 , depending on how we want to deﬁne friendship (the same question as before: can a number be friends w ith itself?).","{'page_number': 31, 'textbook_name': 'ECE-120-student-notes', 'text': 'Check is then compared with 8 to see if the loop is done. Since it is not, we once again enter the loop body and start the second forloop over. The initialization code again sets friendto 0, and we move forward as before. As you see above, the ﬁrst time th at we ﬁnd our ifcondition to be true is when both checkandfriendare equal to 1. Is that result what you expected? To learn that the number 1 is frie nds with itself? If so, the program works. If you assumed that numbers could not be friends with them selves, perhaps we should ﬁx the bug? We could, for example, add another ifstatement to avoid printing anything when check == friend . Our program, you might also realize, prints each pair of friends twice . The numbers 1 and 3, for example, are printed in both possible orders. To eliminate this redundancy, we can change the initialization in the secondforloop, either to friend = check or tofriend = check + 1 , depending on how we want to deﬁne friendship (the same question as before: can a number be friends w ith itself?).'}"
"1.5 Programming Concepts and the C Language 27 1.5.8 Compilation and Interpretation* Many programming languages, including C, can be compiled ,whichmeansthat theprogramisconverted into instructions for a particular ISA before the pro- gramisrunonaprocessorthatsupportsthatISA.The ﬁgure to the right illustrates the compilation process for the C language. In this type of ﬁgure, ﬁles and other data are represented as cylinders, while rect- angles represent processes, which are usually imple- mented in software. In the ﬁgure to the right, the outer dotted box represents the full compilation pro- cess that typically occurs when one compiles a C pro- gram. The inner dotted box represents the work per- formed by the compiler software itself. The cylinders for data passed between the processes that compose the full compilation process have been left out of the ﬁgure; instead, we have written the type of data be- ing passed next to the arrows that indicate the ﬂow of information from one process to the next. The C preprocessor (described later in these notes) forms the ﬁrst step in the compilation process. The preprocessor operates on the program’s source code along with header ﬁles that describe data types and operations. The preprocessor merges these together into a single ﬁle of preprocessed source code. The pre- processed source code is then analyzed by the front end of the compiler based on the speciﬁc programming language being used (in our case, the C language), then converted by the back end of the compiler into instructions for the desired ISA. The output of a com- piler is not binary instructions, however, but is instead a human-readable form of instructions called assem- bly code , which we cover in the last quarter of our class. A tool called an assembler then converts these human-readable instructions into bits that a proces- sor can understand. If a program consists of multi-language−dependent front end ISA−dependent back endsource code analysisC preprocessor target code synthesis assembler linkerintermediate representation (IR) assembly code object code (instructions)compiler (strict sense)   compilation processpreprocessed source code executable imagefilesheaderC C source code other object files libraries ple source ﬁles, or needs to make use of additional pre-programme d operations (such as math functions, graphics, or sound), a tool called a linker merges the object code o f the program with those additional elements to form the ﬁnal executable image for the program. The executable image is typically then stored on a disk, from which it can later be read into memory in order t o allow a processor to execute the program. Some languages are diﬃcult or even impossible to compile. Typically, the behavior of these languages de- pends on input data that are only available when the program runs. S uch languages can be interpreted : each step of the algorithm described by a program is executed by a s oftware interpreter for the language. Languages such as Java, Perl, and Python are usually interpreted","{'page_number': 32, 'textbook_name': 'ECE-120-student-notes', 'text': 'Some languages are diﬃcult or even impossible to compile. Typically, the behavior of these languages de- pends on input data that are only available when the program runs. S uch languages can be interpreted : each step of the algorithm described by a program is executed by a s oftware interpreter for the language. Languages such as Java, Perl, and Python are usually interpreted . Similarly, when we use software to simu- late oneISA usinganotherISA, aswedo at the end ofourclasswith t he LC-3ISAdescribed by the textbook, the simulator is a form of interpreter. In the lab, you will use a simulat or compiled into and executing as x86 instructions in order to interpret LC-3 instructions. While a pro gram is executing in an interpreter, enough information is sometimes available to compile part or all of the p rogram to the processor’s ISA as the program runs, a technique known as “just in time” (JIT) compilation .'}"
"Some languages are diﬃcult or even impossible to compile. Typically, the behavior of these languages de- pends on input data that are only available when the program runs. S uch languages can be interpreted : each step of the algorithm described by a program is executed by a s oftware interpreter for the language. Languages such as Java, Perl, and Python are usually interpreted . Similarly, when we use software to simu- late oneISA usinganotherISA, aswedo at the end ofourclasswith t he LC-3ISAdescribed by the textbook, the simulator is a form of interpreter. In the lab, you will use a simulat or compiled into and executing as x86 instructions in order to interpret LC-3 instructions. While a pro gram is executing in an interpreter, enough information is sometimes available to compile part or all of the p rogram to the processor’s ISA as the program runs, a technique known as “just in time” (JIT) compilation .","{'page_number': 32, 'textbook_name': 'ECE-120-student-notes', 'text': 'Some languages are diﬃcult or even impossible to compile. Typically, the behavior of these languages de- pends on input data that are only available when the program runs. S uch languages can be interpreted : each step of the algorithm described by a program is executed by a s oftware interpreter for the language. Languages such as Java, Perl, and Python are usually interpreted . Similarly, when we use software to simu- late oneISA usinganotherISA, aswedo at the end ofourclasswith t he LC-3ISAdescribed by the textbook, the simulator is a form of interpreter. In the lab, you will use a simulat or compiled into and executing as x86 instructions in order to interpret LC-3 instructions. While a pro gram is executing in an interpreter, enough information is sometimes available to compile part or all of the p rogram to the processor’s ISA as the program runs, a technique known as “just in time” (JIT) compilation .'}"
"28  1.5.9 The C Preprocessor* The C language uses a preprocessor to support inclusion of common information (stored in header ﬁles) into multiple source ﬁles. The most frequent use of the preprocessor is to enable the unique deﬁnition of new data types and operations within header ﬁles that can then be includ ed by reference within source ﬁles that make use of them. This capability is based on the include directive ,#include , as shown here: #include <stdio.h> /* search in standard directories */ #include ""my header.h"" /* search in current followed by standard directo ries */ The preprocessor also supports integration of compile-time const ants into source ﬁles before compilation. For example, many software systems allow the deﬁnition of a symbol such asNDEBUG(no debug) to compile without additional debugging code included in the sources. Two direc tives are necessary for this purpose: thedeﬁne directive ,#define, which provides a text-replacement facility, and conditional inclusion (or exclusion) of parts of a ﬁle within #if/#else/#endifdirectives. These directives are also useful in allowing a single header ﬁle to be included multiple times without causing problems, as C does not allow re- deﬁnition of types, variables, and so forth, even if the redundant deﬁnitions are identical. Most header ﬁles are thus wrapped as shown to the right.#if !defined(MY HEADERH) #define MY HEADERH /* actual header file material goes here */ #endif /* MY HEADERH */ The preprocessor performs a simple linear pass on the source and d oes not parse or interpret any C syntax. Deﬁnitions for text replacement are valid as soon as they are deﬁne d and are performed until they are undeﬁned or until the end of the original source ﬁle. The preproce ssor does recognize spacing and will not replace part of a word, thus “ #define i 5 ” will not wreak havoc on your ifstatements, but will cause problems if you name any variable i. Using the text replacement capabilities of the preprocessor does h ave drawbacks, most importantly in that almost none of the information is passed on for debugging purposes . 1.5.10 Changing Types in C* Changing the type of a datum is necessary from time to time, but som etimes a compiler can do the work for you. The most common form of implicit type conversion occurs with binary arithmetic operations. Integer arithmetic in C always uses types of at least the size of int, and all ﬂoating-point arithmetic uses double. If either or both operands have smaller integer types, or diﬀer fr om one another, the compiler implicitly converts them before performing the operation, and the t ype of the result may be diﬀerent from those of both operands. In general, the compiler selects the ﬁnal type according to some preferred ordering in which ﬂoating-point is preferred over integers, unsigned values a re preferred over signed values, and more bits are preferred over fewer bits","{'page_number': 33, 'textbook_name': 'ECE-120-student-notes', 'text': 'If either or both operands have smaller integer types, or diﬀer fr om one another, the compiler implicitly converts them before performing the operation, and the t ype of the result may be diﬀerent from those of both operands. In general, the compiler selects the ﬁnal type according to some preferred ordering in which ﬂoating-point is preferred over integers, unsigned values a re preferred over signed values, and more bits are preferred over fewer bits. The type of the result must be at least as large as either argument, but is also at least as large as an intfor integer operations and a doublefor ﬂoating-point operations. Modern C compilers always extend an integer type’s bit width before c onverting from signed to unsigned. The original C speciﬁcation interleaved bit width extensions to intwith sign changes, thus older compilers may not be consistent, and implicitly require both types of c onversion in a single operation may lead to portability bugs. The implicit extension to intcan also be confusing in the sense that arithmetic that seems to wor k on smaller integers fails with larger ones. For example, multiplying two 16- bit integers set to 1000 and printing the result works with most compilers because the 32-bit intresult is wide enough to hold the right answer. In contrast, multiplying two 32-bit integers set to 100,000 produce s the wrong result because the high bits of the result are discarded before it can be converted to a larger t ype. For this operation to produce the correct result, one of the integers must be converted explicitly (a s discussed later) before the multiplication.'}"
"If either or both operands have smaller integer types, or diﬀer fr om one another, the compiler implicitly converts them before performing the operation, and the t ype of the result may be diﬀerent from those of both operands. In general, the compiler selects the ﬁnal type according to some preferred ordering in which ﬂoating-point is preferred over integers, unsigned values a re preferred over signed values, and more bits are preferred over fewer bits. The type of the result must be at least as large as either argument, but is also at least as large as an intfor integer operations and a doublefor ﬂoating-point operations. Modern C compilers always extend an integer type’s bit width before c onverting from signed to unsigned. The original C speciﬁcation interleaved bit width extensions to intwith sign changes, thus older compilers may not be consistent, and implicitly require both types of c onversion in a single operation may lead to portability bugs. The implicit extension to intcan also be confusing in the sense that arithmetic that seems to wor k on smaller integers fails with larger ones. For example, multiplying two 16- bit integers set to 1000 and printing the result works with most compilers because the 32-bit intresult is wide enough to hold the right answer. In contrast, multiplying two 32-bit integers set to 100,000 produce s the wrong result because the high bits of the result are discarded before it can be converted to a larger t ype. For this operation to produce the correct result, one of the integers must be converted explicitly (a s discussed later) before the multiplication.","{'page_number': 33, 'textbook_name': 'ECE-120-student-notes', 'text': 'If either or both operands have smaller integer types, or diﬀer fr om one another, the compiler implicitly converts them before performing the operation, and the t ype of the result may be diﬀerent from those of both operands. In general, the compiler selects the ﬁnal type according to some preferred ordering in which ﬂoating-point is preferred over integers, unsigned values a re preferred over signed values, and more bits are preferred over fewer bits. The type of the result must be at least as large as either argument, but is also at least as large as an intfor integer operations and a doublefor ﬂoating-point operations. Modern C compilers always extend an integer type’s bit width before c onverting from signed to unsigned. The original C speciﬁcation interleaved bit width extensions to intwith sign changes, thus older compilers may not be consistent, and implicitly require both types of c onversion in a single operation may lead to portability bugs. The implicit extension to intcan also be confusing in the sense that arithmetic that seems to wor k on smaller integers fails with larger ones. For example, multiplying two 16- bit integers set to 1000 and printing the result works with most compilers because the 32-bit intresult is wide enough to hold the right answer. In contrast, multiplying two 32-bit integers set to 100,000 produce s the wrong result because the high bits of the result are discarded before it can be converted to a larger t ype. For this operation to produce the correct result, one of the integers must be converted explicitly (a s discussed later) before the multiplication.'}"
"1.5 Programming Concepts and the C Language 29 Implicit type conversions also occur due to assignments. Unlike arith metic conversions, the ﬁnal type must match the left-hand side of the assignment (for example, a variable to which a result is assigned), and the compiler simply performs any necessary conversion. Since the desired type may be smaller than the type of the value assigned, information can be lost. Floating-point values are truncated when assigned to integers, and high bits of wider integer types are discarded when assigned to n arrower integer types. Note that a positive number may become a negative number when bits are di scarded in this manner. Passing arguments to functions can be viewed as a special case of a ssignment. Given a function prototype, the compiler knows the type of each argument and can perform con versions as part of the code generated to pass the arguments to the function. Without such a prototype , or for functions with variable numbers of arguments, the compiler lacks type information and thus cannot pe rform necessary conversions, leading to unpredictable behavior. By default, however, the compiler extend s any integer smaller than an intto the width of an intand converts floattodouble. Occasionally it is convenient to use an explicit type cast to force conver- sion from one type to another. Such casts must be used with caution, as they silence many of the warnings that a compiler might otherwise generate when it detects potential problems. One common use is to promote inte- gers to ﬂoating-point before an arith- metic operation, as shownto the right.int main () { int numerator = 10; int denominator = 20; printf (""%f \n"", numerator / (double)denominator); return 0; } The type to which a value is to be converted is placed in parentheses in front of the value. In most cases, additional parentheses should be used to avoid confusion about th e precedence of type conversion over other operations.","{'page_number': 34, 'textbook_name': 'ECE-120-student-notes', 'text': '1.5 Programming Concepts and the C Language 29 Implicit type conversions also occur due to assignments. Unlike arith metic conversions, the ﬁnal type must match the left-hand side of the assignment (for example, a variable to which a result is assigned), and the compiler simply performs any necessary conversion. Since the desired type may be smaller than the type of the value assigned, information can be lost. Floating-point values are truncated when assigned to integers, and high bits of wider integer types are discarded when assigned to n arrower integer types. Note that a positive number may become a negative number when bits are di scarded in this manner. Passing arguments to functions can be viewed as a special case of a ssignment. Given a function prototype, the compiler knows the type of each argument and can perform con versions as part of the code generated to pass the arguments to the function. Without such a prototype , or for functions with variable numbers of arguments, the compiler lacks type information and thus cannot pe rform necessary conversions, leading to unpredictable behavior. By default, however, the compiler extend s any integer smaller than an intto the width of an intand converts floattodouble. Occasionally it is convenient to use an explicit type cast to force conver- sion from one type to another. Such casts must be used with caution, as they silence many of the warnings that a compiler might otherwise generate when it detects potential problems. One common use is to promote inte- gers to ﬂoating-point before an arith- metic operation, as shownto the right.int main () { int numerator = 10; int denominator = 20; printf (""%f \\n"", numerator / (double)denominator); return 0; } The type to which a value is to be converted is placed in parentheses in front of the value. In most cases, additional parentheses should be used to avoid confusion about th e precedence of type conversion over other operations.'}"
"30  ECE120: Introduction to Computer Engineering Notes Set 1.6 Summary of Part 1 of the Course This short summary provides a list of both terms that we expect you to know and and skills that we expect you to have after our ﬁrst few weeks together. The ﬁrst part of the course is shorter than the other three parts, so the amount of material is necessarily less. These notes s upplement the Patt and Patel textbook, so you will also need to read and understand the relevant chapters (see the syllabus) in order to master this material completely. Accordingtoeducationaltheory,thediﬃcultyoflearningdepends onthetypeoftaskinvolved. Remembering new terminology is relatively easy, while applying the ideas underlying de sign decisions shown by example to new problems posed as human tasks is relatively hard. In this shor t summary, we give you lists at several levels of diﬃculty of what we expect you to be able to do as a result of t he last few weeks of studying (reading, listening, doing homework, discussing your understandin g with your classmates, and so forth). This time, we’ll list the skills ﬁrst and leave the easy stuﬀ for the next p age. We expect you to be able to exercise the following skills: •Represent decimal numbers with unsigned, 2’s complement, and IEE E ﬂoating-point representations, and be able to calculate the decimal value represented by a bit patte rn in any of these representations. •Be able to negate a number represented in the 2’s complement repre sentation. •Performsimple arithmetic by hand on unsigned and 2’s complement rep resentations, and identify when overﬂow occurs. •Be able to write a truth table for a Boolean expression. •Be able to write a Boolean expression as a sum of minterms. •Know how to declare and initialize C variables with one of the primitive dat a types. At a more abstract level, we expect you to be able to: •Understand the value of using a common mathematical basis, such a s modular arithmetic, in deﬁning multiple representations (such as unsigned and 2’s complement). •Write Boolean expressions for the overﬂow conditions on both unsig ned and 2’s complement addition. •Be able to write single ifstatements and forloops in C in order to perform computation. •Be able to use scanfandprintffor basic input and output in C. And, at the highest level, we expect that you will be able to reason ab out and analyze problems in the following ways: •Understand the tradeoﬀs between integer and ﬂoating-point rep resentations for numbers. •Understand logical completeness and be able to prove or disprove lo gical completeness for sets of logic functions. •Understand the properties necessary in a representation: no am biguity in meaning for any bit pattern, and agreement in advance on the meanings of all bit patterns. •Analyze a simple, single-function C program and be able to explain its pu rpose.","{'page_number': 35, 'textbook_name': 'ECE-120-student-notes', 'text': '30  ECE120: Introduction to Computer Engineering Notes Set 1.6 Summary of Part 1 of the Course This short summary provides a list of both terms that we expect you to know and and skills that we expect you to have after our ﬁrst few weeks together. The ﬁrst part of the course is shorter than the other three parts, so the amount of material is necessarily less. These notes s upplement the Patt and Patel textbook, so you will also need to read and understand the relevant chapters (see the syllabus) in order to master this material completely. Accordingtoeducationaltheory,thediﬃcultyoflearningdepends onthetypeoftaskinvolved. Remembering new terminology is relatively easy, while applying the ideas underlying de sign decisions shown by example to new problems posed as human tasks is relatively hard. In this shor t summary, we give you lists at several levels of diﬃculty of what we expect you to be able to do as a result of t he last few weeks of studying (reading, listening, doing homework, discussing your understandin g with your classmates, and so forth). This time, we’ll list the skills ﬁrst and leave the easy stuﬀ for the next p age. We expect you to be able to exercise the following skills: •Represent decimal numbers with unsigned, 2’s complement, and IEE E ﬂoating-point representations, and be able to calculate the decimal value represented by a bit patte rn in any of these representations. •Be able to negate a number represented in the 2’s complement repre sentation. •Performsimple arithmetic by hand on unsigned and 2’s complement rep resentations, and identify when overﬂow occurs. •Be able to write a truth table for a Boolean expression. •Be able to write a Boolean expression as a sum of minterms. •Know how to declare and initialize C variables with one of the primitive dat a types. At a more abstract level, we expect you to be able to: •Understand the value of using a common mathematical basis, such a s modular arithmetic, in deﬁning multiple representations (such as unsigned and 2’s complement). •Write Boolean expressions for the overﬂow conditions on both unsig ned and 2’s complement addition. •Be able to write single ifstatements and forloops in C in order to perform computation. •Be able to use scanfandprintffor basic input and output in C. And, at the highest level, we expect that you will be able to reason ab out and analyze problems in the following ways: •Understand the tradeoﬀs between integer and ﬂoating-point rep resentations for numbers. •Understand logical completeness and be able to prove or disprove lo gical completeness for sets of logic functions. •Understand the properties necessary in a representation: no am biguity in meaning for any bit pattern, and agreement in advance on the meanings of all bit patterns. •Analyze a simple, single-function C program and be able to explain its pu rpose.'}"
"1.6 Summary of Part 1 of the Course 31 You should recognize all of these terms and be able to explain what th ey mean. Note that we are not saying that you should, for example, be able to write down the ASCII representation from memory. In that example, knowing that it is a 7-bit representation used for English te xt is suﬃcient. You can always look up the detailed deﬁnition in practice","{'page_number': 36, 'textbook_name': 'ECE-120-student-notes', 'text': 'l.o.g.)•high-level language concepts – syntax – variables – declaration – primitive data types – symbolic name/identiﬁer – initialization – expression – statement •C operators – operands – arithmetic – bitwise – comparison/relational – assignment – address – arithmetic shift – logical shift – precedence •functions in C –main – function call – arguments –printfandscanf – format string – escape character –sizeof •transforming tasks into programs – ﬂow chart – sequential construct – conditional construct – iterative construct/iteration/loop – loop body •C statements – statement: null, simple, compound –ifstatement –forloop –returnstatement'}"
"•universal computational devices / computing machines – undecidable – the halting problem •information storage in computers – bits – representation – data type – unsigned representation – 2’s complement representation – IEEE ﬂoating-point representation – ASCII representation •operations on bits – 1’s complement operation – carry (from addition) – overﬂow (on any operation) – Boolean logic and algebra – logic functions/gates – truth table – AND/conjunction – OR/disjunction – NOT/logical complement/ (logical) negation/inverter – XOR – logical completeness – minterm •mathematical terms – modular arithmetic – implication – contrapositive – proof approaches: by construction, by contradiction, by induction – without loss of generality (w","{'page_number': 36, 'textbook_name': 'ECE-120-student-notes', 'text': 'l.o.g.)•high-level language concepts – syntax – variables – declaration – primitive data types – symbolic name/identiﬁer – initialization – expression – statement •C operators – operands – arithmetic – bitwise – comparison/relational – assignment – address – arithmetic shift – logical shift – precedence •functions in C –main – function call – arguments –printfandscanf – format string – escape character –sizeof •transforming tasks into programs – ﬂow chart – sequential construct – conditional construct – iterative construct/iteration/loop – loop body •C statements – statement: null, simple, compound –ifstatement –forloop –returnstatement'}"
"l.o.g.)•high-level language concepts – syntax – variables – declaration – primitive data types – symbolic name/identiﬁer – initialization – expression – statement •C operators – operands – arithmetic – bitwise – comparison/relational – assignment – address – arithmetic shift – logical shift – precedence •functions in C –main – function call – arguments –printfandscanf – format string – escape character –sizeof •transforming tasks into programs – ﬂow chart – sequential construct – conditional construct – iterative construct/iteration/loop – loop body •C statements – statement: null, simple, compound –ifstatement –forloop –returnstatement","{'page_number': 36, 'textbook_name': 'ECE-120-student-notes', 'text': 'l.o.g.)•high-level language concepts – syntax – variables – declaration – primitive data types – symbolic name/identiﬁer – initialization – expression – statement •C operators – operands – arithmetic – bitwise – comparison/relational – assignment – address – arithmetic shift – logical shift – precedence •functions in C –main – function call – arguments –printfandscanf – format string – escape character –sizeof •transforming tasks into programs – ﬂow chart – sequential construct – conditional construct – iterative construct/iteration/loop – loop body •C statements – statement: null, simple, compound –ifstatement –forloop –returnstatement'}"
"2.1 Optimizing Logic Expressions 33 ECE120: Introduction to Computer Engineering Notes Set 2.1 Optimizing Logic Expressions The second part of the course covers digital design more deeply th an does the textbook. The lecture notes will explain the additional material, and we will provide further example s in lectures and in discussion sections. Please let us know if you need further material for study . In the last notes, we introduced Boolean logic operations and showe d that with AND, OR, and NOT, we can express any Boolean function on any number of variables. Befo re you begin these notes, please read the ﬁrst two sections in Chapter 3 of the textbook, which discuss the o peration of complementary metal- oxide semiconductor (CMOS) transistors, illustrate how gates implementing the AND, OR, and NO T operations can be built using transistors, and introduce DeMorgan ’s laws. This set ofnotes exposes youto a mix oftechniques, terminology, t ools, and philosophy. Some ofthe material is not critical to our class (and will not be tested), but is useful for your broader education, and may help you in later classes. The value of this material has changed substan tially in the last couple of decades, and particularly in the last few years, as algorithms for tools that help wit h hardware design have undergone rapid advances. We talk about these issues as we introduce the idea s. The notes begin with a discussion of the “best” way to express a Boo lean function and some techniques used historically to evaluate such decisions. We next introduce the t erminology necessary to understand manipulation of expressions, and use these terms to explain the Kar naugh map, or K-map, a tool that we will use for many purposes this semester. We illustrate the use of K- maps with a couple of examples, then touch on a few important questions and useful ways of thinking abo ut Boolean logic. We conclude with a discussion of the general problem of multi-metric optimization, intro ducing some ideas and approaches of general use to engineers. 2.1.1 Deﬁning Optimality In the notes on logic operations, you learned how to express an arb itrary function on bits as an OR of minterms (ANDs with one input per variable on which the function oper ates). Although this approach demonstrates logical completeness, the results often seem ineﬃc ient, as you can see by comparing the following expressions for the carry out Cfrom the addition of two 2-bit unsigned numbers, A=A1A0 andB=B1B0. C=A1B1+(A1+B1)A0B0 (3) =A1B1+A1A0B0+A0B1B0 (4) =A1A0B1B0+A1A0B1B0+A1A0B1B0+ A1A0B1B0+A1A0B1B0+A1A0B1B0 (5) These three expressions are identical in the sense that they have the same truth tables—they are the same mathematical function. Equation (3) is the form that we gave when we introduced the idea of using logic to calculate overﬂow. In this form, we were able to explain the terms intuitively","{'page_number': 38, 'textbook_name': 'ECE-120-student-notes', 'text': 'Equation (3) is the form that we gave when we introduced the idea of using logic to calculate overﬂow. In this form, we were able to explain the terms intuitively. Equation (4) results from distributing the parenthesized OR in Equation (3). Equation (5) is th e result of our logical completeness construction. Since the functions are identical, does the form actually matter at a ll? Certainly either of the ﬁrst two forms is easier for us to write than is the third. If we think of the for m of an expression as a mapping from the function that we are trying to calculate into the AND, OR, and NO T functions that we use as logical building blocks, we might also say that the ﬁrst two versions use fewe r building blocks. That observation does have some truth, but let’s try to be more precise by framing a q uestion. For any given function, there are an inﬁnite number of ways that we can express the function (fo r example, given one variable Aon which the function depends, you can OR together any number of copies o fAAwithout changing the function). What exactly makes one expression better than another?'}"
"Equation (3) is the form that we gave when we introduced the idea of using logic to calculate overﬂow. In this form, we were able to explain the terms intuitively. Equation (4) results from distributing the parenthesized OR in Equation (3). Equation (5) is th e result of our logical completeness construction. Since the functions are identical, does the form actually matter at a ll? Certainly either of the ﬁrst two forms is easier for us to write than is the third. If we think of the for m of an expression as a mapping from the function that we are trying to calculate into the AND, OR, and NO T functions that we use as logical building blocks, we might also say that the ﬁrst two versions use fewe r building blocks. That observation does have some truth, but let’s try to be more precise by framing a q uestion. For any given function, there are an inﬁnite number of ways that we can express the function (fo r example, given one variable Aon which the function depends, you can OR together any number of copies o fAAwithout changing the function). What exactly makes one expression better than another?","{'page_number': 38, 'textbook_name': 'ECE-120-student-notes', 'text': 'Equation (3) is the form that we gave when we introduced the idea of using logic to calculate overﬂow. In this form, we were able to explain the terms intuitively. Equation (4) results from distributing the parenthesized OR in Equation (3). Equation (5) is th e result of our logical completeness construction. Since the functions are identical, does the form actually matter at a ll? Certainly either of the ﬁrst two forms is easier for us to write than is the third. If we think of the for m of an expression as a mapping from the function that we are trying to calculate into the AND, OR, and NO T functions that we use as logical building blocks, we might also say that the ﬁrst two versions use fewe r building blocks. That observation does have some truth, but let’s try to be more precise by framing a q uestion. For any given function, there are an inﬁnite number of ways that we can express the function (fo r example, given one variable Aon which the function depends, you can OR together any number of copies o fAAwithout changing the function). What exactly makes one expression better than another?'}"
"34  In 1952, Edward Veitch wrote an article on simplifying truth function s. In the introduction, he said, “This general problem can be very complicated and diﬃcult. Not only does t he complexity increase greatly with the number of inputs and outputs, but the criteria of the best circ uit will vary with the equipment involved.” Sixty years later, the answer is largely the same: the criteria depen d strongly on the underlying technology (the gates and the devices used to construct the gates), and no singlemetric, or way of measuring, is suﬃcient to capture the important diﬀerences between expressio ns in all cases. Three high-level metrics commonly used to evaluate chip designs are cost, power, and performance. Cost usually representsthe manufacturing cost, which is closely related to the physical silicon arearequired for the design: the larger the chip, the more expensive the chip is to produc e. Power measures energy consumption over time. A chip that consumes more power means that a user’s ene rgy bill is higher and, in a portable device, either that the device is heavier or has a shorter battery lif e. Performance measures the speed at which the design operates. A faster design can oﬀer more function ality, such as supporting the latest games, or can just ﬁnish the same work in less time than a slower design. Thes e metrics are sometimes related: if a chip ﬁnishes its work, the chip can turn itself oﬀ, saving energy. How do such high-level metrics relate to the problem at hand? Only ind irectly in practice. There are too many factors involved to make direct calculations of cost, powe r, or performance at the level of logic expressions. Finding an optimal solution—the best formulation of a speciﬁc logic function for a given metric—is often impossible using the computational resources and a lgorithms available to us. Instead, tools typically use heuristic approaches to ﬁnd solutions that strike a bala nce between these metrics. A heuristic approach is one that is believed to yield fairly good solutions to a proble m, but does not necessarily ﬁnd an optimal solution. A human engineer can typically impose constraints , such as limits on the chip area or limits on the minimum performance, in order to guide the process. Hum an engineers may also restructure the implementation of a larger design, such as a design to perform ﬂo ating-point arithmetic, so as to change the logic functions used in the design. Today, manipulation of logic expressions for the purposes o f optimization is performed almost entirely by computers. Humans must supply the logic functions of interest, and must progr am the acceptable transfor- mations between equivalent forms, but computers do the grunt wo rk of comparing alternative formulations and deciding which one is best to use in context. Although we believe that hand optimization of Boolean expressions is n o longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization. The rationale for retaining this exposure is threefold. First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and lo gical equivalence checking (answering the question, “Do two expressions represent the same function? ”)","{'page_number': 39, 'textbook_name': 'ECE-120-student-notes', 'text': 'Although we believe that hand optimization of Boolean expressions is n o longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization. The rationale for retaining this exposure is threefold. First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and lo gical equivalence checking (answering the question, “Do two expressions represent the same function? ”). Second, the complexity of the problem is a good way to introduce you to real engineering. Finally, the contex tual information will help you to develop a better understanding of ﬁnite state machines and higher-level a bstractions that form the core of digital systems and are still deﬁned directly by humans today. Towards that end, we conclude this introduction by discussing two m etrics that engineers traditionally used to optimize logic expressions. These metrics are now embedded in computer-aided design (CAD) tools and tuned to speciﬁc underlying technologies, but the reasons for their use are still interesting. The ﬁrst metric of interest is a heuristic for the area needed for a d esign. The measurement is simple: count the number of variable occurrences in an expression. Simply go thro ugh and add up how many variables you see. Using our example function C, Equation (3) gives a count of 6, Equation (4) gives a count of 8, and Equation (5) gives a count of 24. Smaller numbers represent be tter expressions, so Equation (3) is the best choice by this metric. Why is this metric interesting? Recall h ow gates are built from transistors. AnN-inputgaterequiresroughly2 Ntransistors,soifyoucountup thenumberofvariablesintheexpre ssion, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design. A variation on variable counting is to add the number of operations, s ince each gate also takes space for wiring (within as well as between gates). Note that we ignore the num ber of inputs to the operations, so a 2-input AND counts as 1, but a 10-input AND also counts as 1. We do not usually count complementing'}"
"Although we believe that hand optimization of Boolean expressions is n o longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization. The rationale for retaining this exposure is threefold. First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and lo gical equivalence checking (answering the question, “Do two expressions represent the same function? ”). Second, the complexity of the problem is a good way to introduce you to real engineering. Finally, the contex tual information will help you to develop a better understanding of ﬁnite state machines and higher-level a bstractions that form the core of digital systems and are still deﬁned directly by humans today. Towards that end, we conclude this introduction by discussing two m etrics that engineers traditionally used to optimize logic expressions. These metrics are now embedded in computer-aided design (CAD) tools and tuned to speciﬁc underlying technologies, but the reasons for their use are still interesting. The ﬁrst metric of interest is a heuristic for the area needed for a d esign. The measurement is simple: count the number of variable occurrences in an expression. Simply go thro ugh and add up how many variables you see. Using our example function C, Equation (3) gives a count of 6, Equation (4) gives a count of 8, and Equation (5) gives a count of 24. Smaller numbers represent be tter expressions, so Equation (3) is the best choice by this metric. Why is this metric interesting? Recall h ow gates are built from transistors. AnN-inputgaterequiresroughly2 Ntransistors,soifyoucountup thenumberofvariablesintheexpre ssion, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design. A variation on variable counting is to add the number of operations, s ince each gate also takes space for wiring (within as well as between gates). Note that we ignore the num ber of inputs to the operations, so a 2-input AND counts as 1, but a 10-input AND also counts as 1. We do not usually count complementing","{'page_number': 39, 'textbook_name': 'ECE-120-student-notes', 'text': 'Although we believe that hand optimization of Boolean expressions is n o longer an important skill for our graduates, we do think that you should be exposed to the ideas and metrics historically used for such optimization. The rationale for retaining this exposure is threefold. First, we believe that you still need to be able to perform basic logic reformulations (slowly is acceptable) and lo gical equivalence checking (answering the question, “Do two expressions represent the same function? ”). Second, the complexity of the problem is a good way to introduce you to real engineering. Finally, the contex tual information will help you to develop a better understanding of ﬁnite state machines and higher-level a bstractions that form the core of digital systems and are still deﬁned directly by humans today. Towards that end, we conclude this introduction by discussing two m etrics that engineers traditionally used to optimize logic expressions. These metrics are now embedded in computer-aided design (CAD) tools and tuned to speciﬁc underlying technologies, but the reasons for their use are still interesting. The ﬁrst metric of interest is a heuristic for the area needed for a d esign. The measurement is simple: count the number of variable occurrences in an expression. Simply go thro ugh and add up how many variables you see. Using our example function C, Equation (3) gives a count of 6, Equation (4) gives a count of 8, and Equation (5) gives a count of 24. Smaller numbers represent be tter expressions, so Equation (3) is the best choice by this metric. Why is this metric interesting? Recall h ow gates are built from transistors. AnN-inputgaterequiresroughly2 Ntransistors,soifyoucountup thenumberofvariablesintheexpre ssion, you get an estimate of the number of transistors needed, which is in turn an estimate for the area required for the design. A variation on variable counting is to add the number of operations, s ince each gate also takes space for wiring (within as well as between gates). Note that we ignore the num ber of inputs to the operations, so a 2-input AND counts as 1, but a 10-input AND also counts as 1. We do not usually count complementing'}"
"2.1 Optimizing Logic Expressions 35 variables as an operation for this metric because the complements o f variables are sometimes available at no extra cost in gates or wires. If we add the number of operations in our example, we get a count of 10 for Equation (3)—two ANDs, two ORs, and 6 variables, a count of 12 for Equation (4)—three ANDS, one OR, and 8 variables, and a count of 31 for Equation (5)—six ANDs, on e OR, and 24 variables. The relative diﬀerences between these equations are reduced when one count s operations. A second metric of interest is a heuristic for the performance of a d esign. Performance is inversely related to the delay necessary for a design to produce an output once its in puts are available. For example, if you know how many seconds it takes to produce a result, you can easily c alculate the number of results that can be produced per second, which measures performance. The m easurement needed is the longest chain of operations performed on any instance of a variable. The complemen t of a variable is included if the variable’s complement is not available without using an inverter. The rationale fo r this metric is that gate outputs do not change instantaneously when their inputs change. Once an inpu t to a gate has reached an appropriate voltage to represent a 0 or a 1, the transistors in the gate switch ( on or oﬀ) and electrons start to move. Only when the output of the gate reaches the appropriate new volt age can the gates driven by the output start to change. If we count each function/gate as one delay (we call this time a gate delay ), we get an estimate of the time needed to compute the function. Referring ag ain to our example equations, we ﬁnd that Equation (3) requires 3 gate delays, Equation (4) requires 2 g ate delays, Equation (5) requires 2 or 3 gate delays, depending on whether we have variable complements av ailable. Now Equation (4) looks more attractive: better performance than Equation (3) in return for a small extra cost in area. Heuristics for estimating energy use are too complex to introduce a t this point, but you should be aware that every time electrons move, they generate heat, so we might favor an expression that minimizes the number of bit transitions inside the computation. Such a measurement is not easy to calculate by hand, since you need to know the likelihood of input combinations. 2.1.2 Terminology We use many technical terms when we talk about simpliﬁcation of logic e xpressions, so we now introduce those terms so as to make the description of the tools and process es easier to understand. Let’s assume that we have a logic function F(A,B,C,D ) that we want to express concisely. A literalin an expression of Frefers to either one of the variables or its complement. In other wo rds, for our function F, the following is a complete set of literals: A,A,B,B,C,C,D, andD. When we introduced the AND and OR functions, we also introduced no tation borrowed from arithmetic, usingmultiplicationto representANDandadditiontorepresentOR","{'page_number': 40, 'textbook_name': 'ECE-120-student-notes', 'text': 'A literalin an expression of Frefers to either one of the variables or its complement. In other wo rds, for our function F, the following is a complete set of literals: A,A,B,B,C,C,D, andD. When we introduced the AND and OR functions, we also introduced no tation borrowed from arithmetic, usingmultiplicationto representANDandadditiontorepresentOR. W ealsoborrowthe relatedterminology, so asumin Boolean algebra refers to a number of terms OR’d together (for e xample,A+B, orAB+CD), and aproduct in Boolean algebra refers to a number of terms AND’d together (for example, AD, or AB(C+D). Note that the terms in a sum or product may themselves be sums, products, or other types of expressions (for example, A⊕B). The construction method that we used to demonstrate logical com pleteness made use of minterms for each input combination for which the function Fproduces a 1. We can now use the idea of a literal to give a simpler deﬁnition of minterm: a minterm for a function on Nvariables is a product (AND function) ofNliterals in which each variable or its complement appears exactly once. For our function F, examples of minterms include ABCD,ABCD, andABCD. As you know, a minterm produces a 1 for exactly one combination of inputs. When we sum minterms for each output value of 1 in a truth table to ex press a function, as we did to obtain Equation(5), weproduceanexampleofthesum-of-productsfor m. Inparticular,a sum-of-products (SOP) is a sum composed of products of literals. Terms in a sum-of-produc ts need not be minterms, however. Equation (4) is also in sum-of-products form. Equation (3), howev er, is not, since the last term in the sum is not a product of literals. Analogously to the idea of a minterm, we deﬁne a maxterm for a function on Nvariables as a sum (OR function) of Nliterals in which each variable or its complement appears exactly once. Examples for F'}"
"A literalin an expression of Frefers to either one of the variables or its complement. In other wo rds, for our function F, the following is a complete set of literals: A,A,B,B,C,C,D, andD. When we introduced the AND and OR functions, we also introduced no tation borrowed from arithmetic, usingmultiplicationto representANDandadditiontorepresentOR. W ealsoborrowthe relatedterminology, so asumin Boolean algebra refers to a number of terms OR’d together (for e xample,A+B, orAB+CD), and aproduct in Boolean algebra refers to a number of terms AND’d together (for example, AD, or AB(C+D). Note that the terms in a sum or product may themselves be sums, products, or other types of expressions (for example, A⊕B). The construction method that we used to demonstrate logical com pleteness made use of minterms for each input combination for which the function Fproduces a 1. We can now use the idea of a literal to give a simpler deﬁnition of minterm: a minterm for a function on Nvariables is a product (AND function) ofNliterals in which each variable or its complement appears exactly once. For our function F, examples of minterms include ABCD,ABCD, andABCD. As you know, a minterm produces a 1 for exactly one combination of inputs. When we sum minterms for each output value of 1 in a truth table to ex press a function, as we did to obtain Equation(5), weproduceanexampleofthesum-of-productsfor m. Inparticular,a sum-of-products (SOP) is a sum composed of products of literals. Terms in a sum-of-produc ts need not be minterms, however. Equation (4) is also in sum-of-products form. Equation (3), howev er, is not, since the last term in the sum is not a product of literals. Analogously to the idea of a minterm, we deﬁne a maxterm for a function on Nvariables as a sum (OR function) of Nliterals in which each variable or its complement appears exactly once. Examples for F","{'page_number': 40, 'textbook_name': 'ECE-120-student-notes', 'text': 'A literalin an expression of Frefers to either one of the variables or its complement. In other wo rds, for our function F, the following is a complete set of literals: A,A,B,B,C,C,D, andD. When we introduced the AND and OR functions, we also introduced no tation borrowed from arithmetic, usingmultiplicationto representANDandadditiontorepresentOR. W ealsoborrowthe relatedterminology, so asumin Boolean algebra refers to a number of terms OR’d together (for e xample,A+B, orAB+CD), and aproduct in Boolean algebra refers to a number of terms AND’d together (for example, AD, or AB(C+D). Note that the terms in a sum or product may themselves be sums, products, or other types of expressions (for example, A⊕B). The construction method that we used to demonstrate logical com pleteness made use of minterms for each input combination for which the function Fproduces a 1. We can now use the idea of a literal to give a simpler deﬁnition of minterm: a minterm for a function on Nvariables is a product (AND function) ofNliterals in which each variable or its complement appears exactly once. For our function F, examples of minterms include ABCD,ABCD, andABCD. As you know, a minterm produces a 1 for exactly one combination of inputs. When we sum minterms for each output value of 1 in a truth table to ex press a function, as we did to obtain Equation(5), weproduceanexampleofthesum-of-productsfor m. Inparticular,a sum-of-products (SOP) is a sum composed of products of literals. Terms in a sum-of-produc ts need not be minterms, however. Equation (4) is also in sum-of-products form. Equation (3), howev er, is not, since the last term in the sum is not a product of literals. Analogously to the idea of a minterm, we deﬁne a maxterm for a function on Nvariables as a sum (OR function) of Nliterals in which each variable or its complement appears exactly once. Examples for F'}"
"36  include ( A+B+C+D), (A+B+C+D), and (A+B+C+D). A maxterm produces a 0 for exactly one combination of inputs. Just as we did with minterms, we can multiply a ma xterm corresponding to each input combination for which a function produces 0 (each row in a trut h table that produces a 0 output) to create an expression for the function. The resulting expressio n is in a product-of-sums (POS) form: a product of sums of literals. The carry out function that we used t o produce Equation (5) has 10 input combinations that produce 0, so the expression formed in this way is unpleasantly long: C= (A1+A0+B1+B0)(A1+A0+B1+B0)(A1+A0+B1+B0)(A1+A0+B1+B0) (A1+A0+B1+B0)(A1+A0+B1+B0)(A1+A0+B1+B0)(A1+A0+B1+B0) (A1+A0+B1+B0)(A1+A0+B1+B0) However, the approach can be helpful with functions that produc e mostly 1s. The literals in maxterms are complemented with respect to the literals used in minterms. For exam ple, the maxterm ( A1+A0+B1+B0) in the equation above produces a zero for input combination A1= 1,A0= 1,B1= 0,B0= 0. Animplicant Gof a function Fis deﬁned to be a second function operating on the same variables fo r which the implication G→Fis true. In terms of logic functions that produce 0s and 1s, ifGis an implicant of F, the input combinations for which Gproduces 1s are a subset of the input combinations for which Fproduces 1s. Any minterm for which Fproduces a 1, for example, is an implicant of F. In the context of logic design, the term implicant is used to refer to a single product of lite rals.In other words, if we have a function F(A,B,C,D ), examples of possible implicants of FincludeAB,BC,ABCD, andA. In contrast, although they may technically imply F, we typically do not call expressions such as (A+B),C(A+D), norAB+Cimplicants. Let’s say that we have expressed function Fin sum-of-products form. All of the individual product terms in the expression are implicants of F. As a ﬁrst step in simpliﬁcation, we can ask: for each implicant, is it possible to remove any of the literals that make up the product? If w e have an implicant Gfor which the answer is no, we call Gaprime implicant ofF. In other words, if one removes any of the literals from a prime implicant GofF, the resulting product is not an implicant of F","{'page_number': 41, 'textbook_name': 'ECE-120-student-notes', 'text': 'As a ﬁrst step in simpliﬁcation, we can ask: for each implicant, is it possible to remove any of the literals that make up the product? If w e have an implicant Gfor which the answer is no, we call Gaprime implicant ofF. In other words, if one removes any of the literals from a prime implicant GofF, the resulting product is not an implicant of F. Prime implicants are the main idea that we use to simplify logic expression s, both algebraically and with graphical tools (computer tools use algebra internally—by graphica l here we mean drawings on paper). 2.1.3 Veitch Charts and Karnaugh Maps Veitch’s 1952 paper was the ﬁrst to introduce the idea of using a gra phical representation to simplify logic expressions. Earlier approaches were algebraic. A year later, Mau rice Karnaugh published a paper showing a similar idea with a twist. The twist makes the use of Karnaugh maps to simplify expressions much easier than the use of Veitch charts. As a result, few engineers have hea rd of Veitch, but everyone who has ever taken a class on digital logic knows how to make use of a K-map. Before we introduce the Karnaugh map, let’s think about the structure of the domain of a logic function. Recall that a function’s domain is the space on which the function is deﬁned, that is, for which the function produces values. For a Boolean logic function on Nvariables, you can think of the domain as sequences ofNbits, but you can also visualize the domain as an N-dimensional hypercube. AnA A A=1 A=0A BBA AB=01AB=00 AB=10 AB=11A A BBC C ABC=001 ABC=101 ABC=111 ABC=011ABC=000 ABC=100 ABC=110 ABC=010 N-dimensional hypercube is the generalization of a cube to Ndimensions. Some people only use the term hypercube when N≥4, since we have other names for the smaller values: a point for N= 0, a line segment for N= 1, a square for N= 2, and a cube for N= 3. The diagramsabove and to the right illustrate the cases that are easily drawn on paper. The black dots represen t speciﬁc input combinations, and the blue edges connect input combinations that diﬀer in exactly one input valu e (one bit).'}"
"As a ﬁrst step in simpliﬁcation, we can ask: for each implicant, is it possible to remove any of the literals that make up the product? If w e have an implicant Gfor which the answer is no, we call Gaprime implicant ofF. In other words, if one removes any of the literals from a prime implicant GofF, the resulting product is not an implicant of F. Prime implicants are the main idea that we use to simplify logic expression s, both algebraically and with graphical tools (computer tools use algebra internally—by graphica l here we mean drawings on paper). 2.1.3 Veitch Charts and Karnaugh Maps Veitch’s 1952 paper was the ﬁrst to introduce the idea of using a gra phical representation to simplify logic expressions. Earlier approaches were algebraic. A year later, Mau rice Karnaugh published a paper showing a similar idea with a twist. The twist makes the use of Karnaugh maps to simplify expressions much easier than the use of Veitch charts. As a result, few engineers have hea rd of Veitch, but everyone who has ever taken a class on digital logic knows how to make use of a K-map. Before we introduce the Karnaugh map, let’s think about the structure of the domain of a logic function. Recall that a function’s domain is the space on which the function is deﬁned, that is, for which the function produces values. For a Boolean logic function on Nvariables, you can think of the domain as sequences ofNbits, but you can also visualize the domain as an N-dimensional hypercube. AnA A A=1 A=0A BBA AB=01AB=00 AB=10 AB=11A A BBC C ABC=001 ABC=101 ABC=111 ABC=011ABC=000 ABC=100 ABC=110 ABC=010 N-dimensional hypercube is the generalization of a cube to Ndimensions. Some people only use the term hypercube when N≥4, since we have other names for the smaller values: a point for N= 0, a line segment for N= 1, a square for N= 2, and a cube for N= 3. The diagramsabove and to the right illustrate the cases that are easily drawn on paper. The black dots represen t speciﬁc input combinations, and the blue edges connect input combinations that diﬀer in exactly one input valu e (one bit).","{'page_number': 41, 'textbook_name': 'ECE-120-student-notes', 'text': 'As a ﬁrst step in simpliﬁcation, we can ask: for each implicant, is it possible to remove any of the literals that make up the product? If w e have an implicant Gfor which the answer is no, we call Gaprime implicant ofF. In other words, if one removes any of the literals from a prime implicant GofF, the resulting product is not an implicant of F. Prime implicants are the main idea that we use to simplify logic expression s, both algebraically and with graphical tools (computer tools use algebra internally—by graphica l here we mean drawings on paper). 2.1.3 Veitch Charts and Karnaugh Maps Veitch’s 1952 paper was the ﬁrst to introduce the idea of using a gra phical representation to simplify logic expressions. Earlier approaches were algebraic. A year later, Mau rice Karnaugh published a paper showing a similar idea with a twist. The twist makes the use of Karnaugh maps to simplify expressions much easier than the use of Veitch charts. As a result, few engineers have hea rd of Veitch, but everyone who has ever taken a class on digital logic knows how to make use of a K-map. Before we introduce the Karnaugh map, let’s think about the structure of the domain of a logic function. Recall that a function’s domain is the space on which the function is deﬁned, that is, for which the function produces values. For a Boolean logic function on Nvariables, you can think of the domain as sequences ofNbits, but you can also visualize the domain as an N-dimensional hypercube. AnA A A=1 A=0A BBA AB=01AB=00 AB=10 AB=11A A BBC C ABC=001 ABC=101 ABC=111 ABC=011ABC=000 ABC=100 ABC=110 ABC=010 N-dimensional hypercube is the generalization of a cube to Ndimensions. Some people only use the term hypercube when N≥4, since we have other names for the smaller values: a point for N= 0, a line segment for N= 1, a square for N= 2, and a cube for N= 3. The diagramsabove and to the right illustrate the cases that are easily drawn on paper. The black dots represen t speciﬁc input combinations, and the blue edges connect input combinations that diﬀer in exactly one input valu e (one bit).'}"
"2.1 Optimizing Logic Expressions 37 By viewing a function’s domain in this way, we can make a connection bet ween a product of literals and the structure of the domain. Let’s use the 3-dimensional version a s an example. We call the variables A,B, andC, and note that the cube has 23= 8 corners corresponding to the 23possible combinations of A,B, andC. The simplest product of literals in this case is 1, which is the product o f 0 literals. Obviously, the product 1 evaluates to 1 for any variable values. We can thus think o f it as covering the entire domain of the function. In the case of our example, the product 1 covers th e whole cube. In order for the product 1 to be an implicant of a function, the function itself must be the function 1. What about a product consisting of a single literal, such as AorC? The dividing lines in the diagram illustrate the answer: any such product term evaluates to 1 on a fa ce of the cube, which includes 22= 4 of the corners. If a function evaluates to 1 on any of the six faces of the cube, the corresponding product term (consisting of a single literal) is an implicant of the function. Continuing with products of two literals, we see that any product of two literals, such as ABorBC, corresponds to an edge of our 3-dimensional cube. The edge includ es 21= 2 corners. And, if a function evaluates to 1 on any of the 12 edges of the cube, the correspond ing product term (consisting of two literals) is an implicant of the function. Finally, any product of three literals, such as ABC, corresponds to a corner of the cube. But for a function on three variables, these are just the minterms. As you know, if a f unction evaluates to 1 on any of the 8 corners of the cube, that minterm is an implicant of the function (we used this idea to construct the function to prove logical completeness). How do these connections help us to simplify functions? If we’re care ful, we can map cubes onto paper in such a way that product terms (the possible implicants of the funct ion) usually form contiguous groups of 1s, allowing us to spot them easily. Let’s work upwards starting from one variable to see how this idea works. The end result is called a Karnaugh map. The ﬁrst drawing shown to the right replicates our view of the 1-dimensional hypercube, corresponding to the domain of a fu nc- tion on one variable, in this case the variable A. To the right of the hypercube (line segment) are two variants of a Karnaugh map on on e variable. The middle variant clearly indicates the column correspond- ing to the product A(the other column corresponds to A). The right variant simply labels the column with values for A.010 1A 010 1A A A A=1 A=0 The three drawings shown to the right illustrate the three possible product terms on one variable. The functions shown in these Karnaugh maps are arbitrary, except that we have chosen them such that each implicant shown is a prime implicant for the illustrated fun ction","{'page_number': 42, 'textbook_name': 'ECE-120-student-notes', 'text': 'The middle variant clearly indicates the column correspond- ing to the product A(the other column corresponds to A). The right variant simply labels the column with values for A.010 1A 010 1A A A A=1 A=0 The three drawings shown to the right illustrate the three possible product terms on one variable. The functions shown in these Karnaugh maps are arbitrary, except that we have chosen them such that each implicant shown is a prime implicant for the illustrated fun ction.10 1A 010 1A 10 1A 10 implicant: 1 implicant: A implicant: A Let’s now look at two-variable functions. We have repli- catedourdrawingofthe 2-dimensionalhypercube (square) to the right along with two variants of Karnaugh maps on two variables. With only two variables ( AandB), the extension is fairly straightforward, since we can use the second dimension of the paper (vertical) to express the second variable ( B).A BB0 11 00 10 1 BA A AB=01AB=00 AB=10 AB=11A 0 11 0 10 1 B0'}"
"The middle variant clearly indicates the column correspond- ing to the product A(the other column corresponds to A). The right variant simply labels the column with values for A.010 1A 010 1A A A A=1 A=0 The three drawings shown to the right illustrate the three possible product terms on one variable. The functions shown in these Karnaugh maps are arbitrary, except that we have chosen them such that each implicant shown is a prime implicant for the illustrated fun ction.10 1A 010 1A 10 1A 10 implicant: 1 implicant: A implicant: A Let’s now look at two-variable functions. We have repli- catedourdrawingofthe 2-dimensionalhypercube (square) to the right along with two variants of Karnaugh maps on two variables. With only two variables ( AandB), the extension is fairly straightforward, since we can use the second dimension of the paper (vertical) to express the second variable ( B).A BB0 11 00 10 1 BA A AB=01AB=00 AB=10 AB=11A 0 11 0 10 1 B0","{'page_number': 42, 'textbook_name': 'ECE-120-student-notes', 'text': 'The middle variant clearly indicates the column correspond- ing to the product A(the other column corresponds to A). The right variant simply labels the column with values for A.010 1A 010 1A A A A=1 A=0 The three drawings shown to the right illustrate the three possible product terms on one variable. The functions shown in these Karnaugh maps are arbitrary, except that we have chosen them such that each implicant shown is a prime implicant for the illustrated fun ction.10 1A 010 1A 10 1A 10 implicant: 1 implicant: A implicant: A Let’s now look at two-variable functions. We have repli- catedourdrawingofthe 2-dimensionalhypercube (square) to the right along with two variants of Karnaugh maps on two variables. With only two variables ( AandB), the extension is fairly straightforward, since we can use the second dimension of the paper (vertical) to express the second variable ( B).A BB0 11 00 10 1 BA A AB=01AB=00 AB=10 AB=11A 0 11 0 10 1 B0'}"
"38  The number of possible products of literals grows rapidly with the num- ber of variables. For two variables, nine are possible, as shown to the right. Notice that all implicants have two properties. First, they oc- cupy contiguous regions of the grid. And, second, their height and width are always powers of two. These properties seem somewhat trivial at this stage, but they are the key to the utility of K-maps on more vari- ables.A 11 0 10 1 B1 1 A 0 10 1 BA 00 10 1 B10A 0 10 1 BA 0 10 10 1 B0A 10 10 1 BA 0 11 0 10 1 B1A 10 10 1 BA 0 11 0 10 1 B0 110 11 0 0 1 0 0 0 11 0 0 0implicant: 1 implicant: AB implicant: AB implicant: AB implicant: ABimplicant: B implicant: B implicant: A implicant: A Three-variable functions are next. The cube diagram is again replicated to the right. But now we have a problem: how can we map four points (say, from the top half of the cube) into a line in such a way that any points connected by a blue line are adjacent in the K-map? The answer is that we cannot, but we can preserve most of the connections by choosing an order such as the one illustrated by the arrow. The result00 01 11 10 0 11 01 1 1 1AC 0 1B00 01 11 10 0 11 01 1 1 10 1AA A BBC C B CABC=001 ABC=101 ABC=111 ABC=011ABC=000 ABC=100 ABC=110 ABC=010 is called a Gray code. Two K-map variants again appear to the right of the cube. Look closely at the order of the two-variable combinations along the top, which allows us to hav e as many contiguous products of literals as possible. Any product of literals that contains Cbut notAnorAwraps around the edges of the K-map, so you should think of it as rolling up into a cylinder rather than a grid. Or you can think that we’re unfolding the cube to ﬁt the corners onto a sheet of paper, b ut the place that we split the cube should still be considered to be adjacent when looking for implicants. The us e of a Gray code is the one diﬀerence between a K-map and a Veitch chart; Veitch used the base 2 order, which makes some implicants hard to spot. With three variables, we have 27 possible products of literals. You ma y have noticed that the count scales as 3NforNvariables; can you explain why? We illustrate several product terms below. Note that we sometimes need to wrap around the end of the K-map, but that if we account for wrapping, the squares covered by all product terms are contiguous. Also notice that bot h the width and the height of all product terms are powers of two. Any square or rectangle that meets these two constraints cor responds to a prod- uct term! And any such squareor rectangle that is ﬁlled with 1s is an implicant of t he function in the K-map","{'page_number': 43, 'textbook_name': 'ECE-120-student-notes', 'text': 'Note that we sometimes need to wrap around the end of the K-map, but that if we account for wrapping, the squares covered by all product terms are contiguous. Also notice that bot h the width and the height of all product terms are powers of two. Any square or rectangle that meets these two constraints cor responds to a prod- uct term! And any such squareor rectangle that is ﬁlled with 1s is an implicant of t he function in the K-map. A A B C00 01 11 10 11 1 10 11 1 0 0A B C00 01 11 10 11 10 11 1 0 0 0A B C00 01 11 10 11 10 11 1 0 0 0A B C00 01 11 10 11 1 1 1 10 11 1 B C00 01 11 10 11 10 11 1 0 0 0 implicant: C implicant: AB implicant: AC implicant: 1 implicant: ABC Let’s keep going. With a function on four variables— A,B,C, andD—we can use a Gray code order on two ofthevariablesin eachdimension. Whichvariablesgowith whichdimensio ninthe gridreallydoesn’tmatter, so we’ll assign ABto the horizontal dimension and CDto the vertical dimension. A few of the 81 possible product terms are illustrated at the top of the next page. Notice t hat while wrapping can now occur in both dimensions, we have exactly the same rule for ﬁnding implicants of the function: any square or rectangle (al- lowing for wrapping) that is ﬁlled with 1s and has both height and width e qual to (possibly diﬀerent) powers of two is an implicant of the function. Furthermore, unless such a square or rectangle is part of a la rger square or rectangle that meets these criteria, the correspo nding implicant is a prime implicant of the function.'}"
"Note that we sometimes need to wrap around the end of the K-map, but that if we account for wrapping, the squares covered by all product terms are contiguous. Also notice that bot h the width and the height of all product terms are powers of two. Any square or rectangle that meets these two constraints cor responds to a prod- uct term! And any such squareor rectangle that is ﬁlled with 1s is an implicant of t he function in the K-map. A A B C00 01 11 10 11 1 10 11 1 0 0A B C00 01 11 10 11 10 11 1 0 0 0A B C00 01 11 10 11 10 11 1 0 0 0A B C00 01 11 10 11 1 1 1 10 11 1 B C00 01 11 10 11 10 11 1 0 0 0 implicant: C implicant: AB implicant: AC implicant: 1 implicant: ABC Let’s keep going. With a function on four variables— A,B,C, andD—we can use a Gray code order on two ofthevariablesin eachdimension. Whichvariablesgowith whichdimensio ninthe gridreallydoesn’tmatter, so we’ll assign ABto the horizontal dimension and CDto the vertical dimension. A few of the 81 possible product terms are illustrated at the top of the next page. Notice t hat while wrapping can now occur in both dimensions, we have exactly the same rule for ﬁnding implicants of the function: any square or rectangle (al- lowing for wrapping) that is ﬁlled with 1s and has both height and width e qual to (possibly diﬀerent) powers of two is an implicant of the function. Furthermore, unless such a square or rectangle is part of a la rger square or rectangle that meets these criteria, the correspo nding implicant is a prime implicant of the function.","{'page_number': 43, 'textbook_name': 'ECE-120-student-notes', 'text': 'Note that we sometimes need to wrap around the end of the K-map, but that if we account for wrapping, the squares covered by all product terms are contiguous. Also notice that bot h the width and the height of all product terms are powers of two. Any square or rectangle that meets these two constraints cor responds to a prod- uct term! And any such squareor rectangle that is ﬁlled with 1s is an implicant of t he function in the K-map. A A B C00 01 11 10 11 1 10 11 1 0 0A B C00 01 11 10 11 10 11 1 0 0 0A B C00 01 11 10 11 10 11 1 0 0 0A B C00 01 11 10 11 1 1 1 10 11 1 B C00 01 11 10 11 10 11 1 0 0 0 implicant: C implicant: AB implicant: AC implicant: 1 implicant: ABC Let’s keep going. With a function on four variables— A,B,C, andD—we can use a Gray code order on two ofthevariablesin eachdimension. Whichvariablesgowith whichdimensio ninthe gridreallydoesn’tmatter, so we’ll assign ABto the horizontal dimension and CDto the vertical dimension. A few of the 81 possible product terms are illustrated at the top of the next page. Notice t hat while wrapping can now occur in both dimensions, we have exactly the same rule for ﬁnding implicants of the function: any square or rectangle (al- lowing for wrapping) that is ﬁlled with 1s and has both height and width e qual to (possibly diﬀerent) powers of two is an implicant of the function. Furthermore, unless such a square or rectangle is part of a la rger square or rectangle that meets these criteria, the correspo nding implicant is a prime implicant of the function.'}"
"2.1 Optimizing Logic Expressions 39 00 01 11 10 00 01 11 10CD11AB 1 1 11 11 1 1 11 1 1 11 implicant: 100 01 11 10 0 00 01 11 10CD 0111 0AB 1 1 011 11 1 1 1 implicant: D00 01 11 10 0 00 01 11 10CD 0111 00AB 1 1 01 10 1 00 implicant: BD00 01 11 10 0 00 01 11 10CD 0111 00AB 1 1 01 10 1 00 implicant: AB00 01 11 10 0 00 01 11 10CD 0111 00AB 1 1 01 10 1 00 implicant: ACD00 01 11 10 0 00 01 11 10CD 0111 00AB 1 1 01 10 1 00 implicant: ABCD Finding a simple expression for a function using a K-map then consists of solving the following problem: pick a minimal set of prime implicants such that every 1 produced by th e function is covered by at least one prime implicant. The metric that you choose to minimize the set may var y in practice, but for simplicity, let’s say that we minimize the number of prime implicants chosen. Let’s try a few! The table on the left below reproduces (from Notes Set 1.4) the truth table for addition of two 2-bit unsigned numbers, A1A0andB1B0, to produce a sum S1S0and a carry out C. K-maps for each output bit appear to the right. The colors are used only to make the diﬀerent prime implicants easier to distinguish. The equations produced by summing these prime implicant s appear below the K-maps. inputs outputs A1A0B1B0C S1S0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 000 01 11 10 00 01 11 101 0 1 0A A B B00 0C 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0 01 1 0 000 01 11 10 00 01 11 101 11S11 0 1 0A A B B00 10 000 01 11 10 00 01 11 101 11 0 1 0A A B B0 0S0 1 0 1 0 1 01 01 0 0 1 C=A1B1+A1A0B0+A0B1B0 S1=A1B1B0+A1A0B1+A1A0B1+A1B1B0+ A1A0B1B0+A1A0B1B0 S0=A0B0+A0B0 In theory, K-maps extend to an arbitrary number of variables. Ce rtainly Gray codes can be extended","{'page_number': 44, 'textbook_name': 'ECE-120-student-notes', 'text': 'Ce rtainly Gray codes can be extended. An N-bit Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns diﬀer in only one bit. The code is actually a cycle: t he ﬁrst and last patterns also diﬀer in only one bit. You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this seq uence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all pattern s in the second copy. The result is an ( N+1)-bit Gray code. For example, the following are Gray codes: 1-bit 0, 1 2-bit 00, 01, 11, 10 3-bit 000, 001, 011, 010, 110, 111, 101, 100 4-bit 0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 11 01, 1111, 1110, 1010, 1011, 1001, 1000 Unfortunately, some of the beneﬁcial properties of K-maps do no t extend beyond two variables in a di- mension. Once you have three variables in one dimension , as is necessary if a function operates on ﬁve or more variables, not all product terms are contiguous in the grid . The terms still require a total number of rows and columns equal to a power of two, but they don’t all need to be a contiguous group. Furthermore, some contiguous groups of appropriate size do not correspon d to product terms . So you can still make use of K-maps if you have more variables, but their use is a little trickier.'}"
"Ce rtainly Gray codes can be extended. An N-bit Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns diﬀer in only one bit. The code is actually a cycle: t he ﬁrst and last patterns also diﬀer in only one bit. You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this seq uence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all pattern s in the second copy. The result is an ( N+1)-bit Gray code. For example, the following are Gray codes: 1-bit 0, 1 2-bit 00, 01, 11, 10 3-bit 000, 001, 011, 010, 110, 111, 101, 100 4-bit 0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 11 01, 1111, 1110, 1010, 1011, 1001, 1000 Unfortunately, some of the beneﬁcial properties of K-maps do no t extend beyond two variables in a di- mension. Once you have three variables in one dimension , as is necessary if a function operates on ﬁve or more variables, not all product terms are contiguous in the grid . The terms still require a total number of rows and columns equal to a power of two, but they don’t all need to be a contiguous group. Furthermore, some contiguous groups of appropriate size do not correspon d to product terms . So you can still make use of K-maps if you have more variables, but their use is a little trickier.","{'page_number': 44, 'textbook_name': 'ECE-120-student-notes', 'text': 'Ce rtainly Gray codes can be extended. An N-bit Gray code is a sequence of N-bit patterns that includes all possible patterns such that any two adjacent patterns diﬀer in only one bit. The code is actually a cycle: t he ﬁrst and last patterns also diﬀer in only one bit. You can construct a Gray code recursively as follows: for an (N+1)-bit Gray code, write the sequence for an N-bit Gray code, then add a 0 in front of all patterns. After this seq uence, append a second copy of the N-bit Gray code in reverse order, then put a 1 in front of all pattern s in the second copy. The result is an ( N+1)-bit Gray code. For example, the following are Gray codes: 1-bit 0, 1 2-bit 00, 01, 11, 10 3-bit 000, 001, 011, 010, 110, 111, 101, 100 4-bit 0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100, 1100, 11 01, 1111, 1110, 1010, 1011, 1001, 1000 Unfortunately, some of the beneﬁcial properties of K-maps do no t extend beyond two variables in a di- mension. Once you have three variables in one dimension , as is necessary if a function operates on ﬁve or more variables, not all product terms are contiguous in the grid . The terms still require a total number of rows and columns equal to a power of two, but they don’t all need to be a contiguous group. Furthermore, some contiguous groups of appropriate size do not correspon d to product terms . So you can still make use of K-maps if you have more variables, but their use is a little trickier.'}"
"40  2.1.4 Canonical Forms What if we want to compare two expressions to determine whether t hey represent the same logic function? Such a comparison is a test of logical equivalence , and is an important part of hardware design. Tools today provide help with this problem, but you should understand the problem. You know that any given function can be expressed in many ways, an d that two expressions that look quite diﬀerent may in fact represent the same function (look back at Equ ations (3) to (5) for an example). But what if we rewrite the function using only prime implicants? Is the resu lt unique? Unfortunately, no. In general, a sum of products is not unique (nor is a product of sums), even if the sum contains only prime implicants . For example, consensus terms may or may not be included in our expr essions. (They are necessary for reliable design of certain types of systems, as y ou will learn in a later ECE class.) The green ellipse in the K-map to the right represent s the consensus termBC. Z=A C+A B+B C Z=A C+A B00 01 11 10 0 1Z BC A 1 1 0 001 0 1 Some functions allow several equivalent formulations as sums of prime implicants, even without consensus terms. The K-maps shown to the right, for example, illustrate how one function might be written in either of the following ways: Z=AB D+A CD+A B C+BC D Z=AB C+B CD+A B D+AC D000 01 11 10 00 1 01 1100 01 11 10CD AB1 1 1 0 000 1Z 000 01 11 10 00 1 01 1100 01 11 10CD AB1 1 1 0 000 1Z When we need to compare two things (such as functions), we need t o transform them into what in math- ematics is known as a canonical form , which simply means a form that is deﬁned so as to be unique for each thing of the given type. What can we use for logic functions? Yo u already know two answers! The canonical sum of a function (sometimes called the canonical SOP form ) is the sum of minterms. The canonical product of a function (sometimes called the canonical POS form ) is the product of maxterms. These forms technically only meet the mathematical deﬁnition of can onical if we agree on an order for the min/maxterms, but that problem is solvable. However, as you alread y know, the forms are not particularly convenient to use. In practice, people and tools in the industry use more compact approaches when compar- ing functions, but those solutions are a subject for a later class (s uch as ECE 462). 2.1.5 Two-Level Logic Two-level logic is a popular way of expressing logic functions. The two levels refer simply to the number offunctions through which an input passes to r each an output, and boththeSOPandPOSformsareexamplesoftwo-levellogic. Inthiss ection, weillustrate one of the reasons for this popularity and show you how to graphica lly manipulate expressions, which can sometimes help when trying to understand g ate diagrams","{'page_number': 45, 'textbook_name': 'ECE-120-student-notes', 'text': '2.1.5 Two-Level Logic Two-level logic is a popular way of expressing logic functions. The two levels refer simply to the number offunctions through which an input passes to r each an output, and boththeSOPandPOSformsareexamplesoftwo-levellogic. Inthiss ection, weillustrate one of the reasons for this popularity and show you how to graphica lly manipulate expressions, which can sometimes help when trying to understand g ate diagrams. We begin with one of DeMorgan’s laws, which we can illustrate both algeb raically and graphically: C=B+A=BAA BCA BC'}"
"2.1.5 Two-Level Logic Two-level logic is a popular way of expressing logic functions. The two levels refer simply to the number offunctions through which an input passes to r each an output, and boththeSOPandPOSformsareexamplesoftwo-levellogic. Inthiss ection, weillustrate one of the reasons for this popularity and show you how to graphica lly manipulate expressions, which can sometimes help when trying to understand g ate diagrams. We begin with one of DeMorgan’s laws, which we can illustrate both algeb raically and graphically: C=B+A=BAA BCA BC","{'page_number': 45, 'textbook_name': 'ECE-120-student-notes', 'text': '2.1.5 Two-Level Logic Two-level logic is a popular way of expressing logic functions. The two levels refer simply to the number offunctions through which an input passes to r each an output, and boththeSOPandPOSformsareexamplesoftwo-levellogic. Inthiss ection, weillustrate one of the reasons for this popularity and show you how to graphica lly manipulate expressions, which can sometimes help when trying to understand g ate diagrams. We begin with one of DeMorgan’s laws, which we can illustrate both algeb raically and graphically: C=B+A=BAA BCA BC'}"
"2.1 Optimizing Logic Expressions 41 Let’s say that we have a function expressed in SOP form, such as Z=ABC+DE+FGHJ. The diagram on the left below shows the function constructed from three AND g ates and an OR gate. Using DeMorgan’s law, we can replace the OR gate with a NAND with inverted inputs. But t he bubbles that correspond to inversion do not need to sit at the input to the gate. We can invert at any point along the wire, so we slide each bubble down the wire to the output of the ﬁrst column of AND ga tes.Be careful: if the wire splits, which does not happen in our example, you have to replicate th e inverter onto the other output paths as you slide past the split point! The end result is shown on the right: we have not changed the funct ion, but now we use only NAND gates. Since CMOS technology only supports NAND a nd NOR directly, using two-level logic makes it simple to map our expression into CMOS gates. Z Z Z next, we slide these inversion bubbles down the wires to the leftwe now have the same function (SOP form) implemented with NAND gatesfirst, we replace this OR gate using DeMorgan’s lawA B C D E F G H JA B C D E F G H JA B C D E F G H J You may want to make use of DeMorgan’s other law, illustrated graph ically to the right, to perform the same transformation on a POS expression. What do you get? A BCA BC 2.1.6 Multi-Metric Optimization Asengineers,almosteveryrealproblemthatyouencounterwillad mitmultiplemetricsforevaluatingpossible designs. Becoming a good engineer thus requires not only that you b e able to solve problems creatively so as to improve the quality of your solutions, but also that you are awa re of how people might evaluate those solutions and are able both to identify the most important metrics an d to balance your design eﬀectively according to them. In this section, we introduce some general idea s and methods that may be of use to you in this regard. We will not test you on the concepts in this section. When you start thinking about a new problem, your ﬁrst step should be to think carefully about metrics of possible interest. Some important metrics may not be easy to qua ntify. For example, compatibility of a design with other products already owned by a customer has freq uently deﬁned the success or failure of computer hardware and software solutions. But how can you comp ute the compability of your approach as a number? Humans—including engineers—are not good at comparing multiple metr ics simultaneously. Thus, once you have a set of metrics that you feel is complete, your next step is to get rid of as many as you can. Towards this end, you may identify metrics that have no practical impact in cu rrent technology, set threshold values for other metrics to simplify reasoning about them, eliminate redund ant metrics, calculate linear sums to reduce the count of metrics, and, ﬁnally, make use of the notion of Pareto optimality. All of these ideas are described in the rest of this section","{'page_number': 46, 'textbook_name': 'ECE-120-student-notes', 'text': 'Towards this end, you may identify metrics that have no practical impact in cu rrent technology, set threshold values for other metrics to simplify reasoning about them, eliminate redund ant metrics, calculate linear sums to reduce the count of metrics, and, ﬁnally, make use of the notion of Pareto optimality. All of these ideas are described in the rest of this section. Let’s start by considering metrics that we can quantify as real num bers. For a given metric, we can divide possible measurement values into three ranges. In the ﬁrst range , all measurement values are equivalently useful. In the second range, possible values are ordered and inter esting with respect to one another. Values in the third range are all impossible to use in practice. Using power con sumption as our example, the ﬁrst rangecorrespondsto systems in which when a processor’spowerc onsumption in a digitalsystem is extremely low relative to the power consumption of the system. For example, t he processor in a computer might use less than 1% of the total used by the system including the disk drive, the monitor, the power supply, and so forth. One power consumption value in this range is just as good as a ny another, and no one cares about the power consumption of the processor in such cases. In the second range, power consumption of the processor'}"
"Towards this end, you may identify metrics that have no practical impact in cu rrent technology, set threshold values for other metrics to simplify reasoning about them, eliminate redund ant metrics, calculate linear sums to reduce the count of metrics, and, ﬁnally, make use of the notion of Pareto optimality. All of these ideas are described in the rest of this section. Let’s start by considering metrics that we can quantify as real num bers. For a given metric, we can divide possible measurement values into three ranges. In the ﬁrst range , all measurement values are equivalently useful. In the second range, possible values are ordered and inter esting with respect to one another. Values in the third range are all impossible to use in practice. Using power con sumption as our example, the ﬁrst rangecorrespondsto systems in which when a processor’spowerc onsumption in a digitalsystem is extremely low relative to the power consumption of the system. For example, t he processor in a computer might use less than 1% of the total used by the system including the disk drive, the monitor, the power supply, and so forth. One power consumption value in this range is just as good as a ny another, and no one cares about the power consumption of the processor in such cases. In the second range, power consumption of the processor","{'page_number': 46, 'textbook_name': 'ECE-120-student-notes', 'text': 'Towards this end, you may identify metrics that have no practical impact in cu rrent technology, set threshold values for other metrics to simplify reasoning about them, eliminate redund ant metrics, calculate linear sums to reduce the count of metrics, and, ﬁnally, make use of the notion of Pareto optimality. All of these ideas are described in the rest of this section. Let’s start by considering metrics that we can quantify as real num bers. For a given metric, we can divide possible measurement values into three ranges. In the ﬁrst range , all measurement values are equivalently useful. In the second range, possible values are ordered and inter esting with respect to one another. Values in the third range are all impossible to use in practice. Using power con sumption as our example, the ﬁrst rangecorrespondsto systems in which when a processor’spowerc onsumption in a digitalsystem is extremely low relative to the power consumption of the system. For example, t he processor in a computer might use less than 1% of the total used by the system including the disk drive, the monitor, the power supply, and so forth. One power consumption value in this range is just as good as a ny another, and no one cares about the power consumption of the processor in such cases. In the second range, power consumption of the processor'}"
"42  makes a diﬀerence. Cell phones use most of their energy in radio ope ration, for example, but if you own a phone with a powerful processor, you may have noticed that you can turn oﬀ the phone and drain the battery fairly quickly by playing a game. Designing a processor that u ses half as much power lengthens the battery life in such cases. Finally, the third region of power consump tion measurements is impossible: if you use so much power, your chip will overheat or even burst into ﬂames . Consumers get unhappy when such things happen. As a ﬁrst step, you can remove any metrics for which all solutions ar e eﬀectively equivalent. Until a little less than a decade ago, for example, the power consumption of a de sktop processor actually was in the ﬁrst range that we discussed. Power was simply not a concern to enginee rs: all designs of interest consumed so little power that no one cared. Unfortunately, at that point, powe r consumption jumped into the third range rather quickly. Processors hit a wall, and products had to be cance lled. Given that the time spent designing a processor has historically been about ﬁve years, a lot of engineer ing eﬀort was wasted because people had not thought carefully enough about power (since it had never matt ered in the past). Today, power is an important metric that engineers must take into account in their des igns. However, in some areas, such as desktop and high-end server pro cessors, other metrics (such as performance) may be so important that we always want to operate at the edge of t he interesting range. In such cases, we might choose to treat a metric such as power consumption as a threshold : stay below 150 Watts for a desktop processor, for example. One still has to make a coordinat ed eﬀort to ensure that the system as a whole does not exceed the threshold, but reasoning about thresh old values, a form of constraint, is easier than trying to think about multiple metrics at once. Some metrics may only allow discrete quantiﬁcation. For example, one could choose to deﬁne compatibility with previous processor generations as binary: either an existing p iece of software (or operating system) runs out of the box on your new processor, or it does not. If you want p eople who own that software to make use of your new processor, you must ensure that the value of this bina ry metric is 1, which can also be viewed as a threshold. In some cases, two metrics may be strongly correlated , meaning that a design that is good for one of the metrics is frequently good for the other metric as well. Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider the m separately. A design that requires a larger chip is probably more complex, and thus takes more engineerin g time to get right (engineering time costs money). Each silicon wafer costs money to fabricate, and fe wer copies of a large design ﬁt on one wafer, so large chips mean more fabrication cost. Physical defect s in silicon can cause some chips not to work","{'page_number': 47, 'textbook_name': 'ECE-120-student-notes', 'text': 'Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider the m separately. A design that requires a larger chip is probably more complex, and thus takes more engineerin g time to get right (engineering time costs money). Each silicon wafer costs money to fabricate, and fe wer copies of a large design ﬁt on one wafer, so large chips mean more fabrication cost. Physical defect s in silicon can cause some chips not to work. A large chip uses more silicon than a small one, and is thus more lik ely to suﬀer from defects (and not work). Cost thus goes up again for large chips relative to small o nes. Finally, large chips usually require more careful testing to ensure that they work properly (even ign oring the cost of getting the design right, we have to test for the presence of defects), which adds still mor e cost for a larger chip. All of these factors tend to correlate chip area and chip cost, to the point that most en gineers do not consider both metrics. After youhavetriedto reduceyourset ofmetricsasmuchasposs ible, orsimpliﬁed thembyturningthem into thresholds, youshould considerturningthelastfew metricsintoaw eightedlinearsum. All remainingmetrics must be quantiﬁable in this case. For example, if you are left with thre e metrics for which a given design has valuesA,B, andC, you might reduce these to one metric by calculating D=wAA+wBB+wCC. What are thewvalues? They are weights for the three metrics. Their values repre sent the relative importance of the three metrics to the overall evaluation. Here we’ve assumed th at larger values of A,B, andCare either all good or all bad. If you have metrics with diﬀerent senses, use th e reciprocal values. For example, if a large value of Ais good, a small value of 1 /Ais also good. The diﬃculty with linearizing metrics is that not everyone agrees on th e weights. Is using less power more important than having a cheaper chip? The answer may depend on ma ny factors. When you are left with several metrics of interest, you can use the idea of Pareto optimality to identify interesting designs. Let’s say that you have two metrics. If a desig nD1is better than a second design D2 for both metrics, we say that D1dominates D2. A design Dis then said to be Pareto optimal if no other design dominates D. Consider the ﬁgure on the left below, which illustrates seven possib le designs measured'}"
"Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider the m separately. A design that requires a larger chip is probably more complex, and thus takes more engineerin g time to get right (engineering time costs money). Each silicon wafer costs money to fabricate, and fe wer copies of a large design ﬁt on one wafer, so large chips mean more fabrication cost. Physical defect s in silicon can cause some chips not to work. A large chip uses more silicon than a small one, and is thus more lik ely to suﬀer from defects (and not work). Cost thus goes up again for large chips relative to small o nes. Finally, large chips usually require more careful testing to ensure that they work properly (even ign oring the cost of getting the design right, we have to test for the presence of defects), which adds still mor e cost for a larger chip. All of these factors tend to correlate chip area and chip cost, to the point that most en gineers do not consider both metrics. After youhavetriedto reduceyourset ofmetricsasmuchasposs ible, orsimpliﬁed thembyturningthem into thresholds, youshould considerturningthelastfew metricsintoaw eightedlinearsum. All remainingmetrics must be quantiﬁable in this case. For example, if you are left with thre e metrics for which a given design has valuesA,B, andC, you might reduce these to one metric by calculating D=wAA+wBB+wCC. What are thewvalues? They are weights for the three metrics. Their values repre sent the relative importance of the three metrics to the overall evaluation. Here we’ve assumed th at larger values of A,B, andCare either all good or all bad. If you have metrics with diﬀerent senses, use th e reciprocal values. For example, if a large value of Ais good, a small value of 1 /Ais also good. The diﬃculty with linearizing metrics is that not everyone agrees on th e weights. Is using less power more important than having a cheaper chip? The answer may depend on ma ny factors. When you are left with several metrics of interest, you can use the idea of Pareto optimality to identify interesting designs. Let’s say that you have two metrics. If a desig nD1is better than a second design D2 for both metrics, we say that D1dominates D2. A design Dis then said to be Pareto optimal if no other design dominates D. Consider the ﬁgure on the left below, which illustrates seven possib le designs measured","{'page_number': 47, 'textbook_name': 'ECE-120-student-notes', 'text': 'Chip area and cost, for example, are technically distinct ways to measure a digital design, but we rarely consider the m separately. A design that requires a larger chip is probably more complex, and thus takes more engineerin g time to get right (engineering time costs money). Each silicon wafer costs money to fabricate, and fe wer copies of a large design ﬁt on one wafer, so large chips mean more fabrication cost. Physical defect s in silicon can cause some chips not to work. A large chip uses more silicon than a small one, and is thus more lik ely to suﬀer from defects (and not work). Cost thus goes up again for large chips relative to small o nes. Finally, large chips usually require more careful testing to ensure that they work properly (even ign oring the cost of getting the design right, we have to test for the presence of defects), which adds still mor e cost for a larger chip. All of these factors tend to correlate chip area and chip cost, to the point that most en gineers do not consider both metrics. After youhavetriedto reduceyourset ofmetricsasmuchasposs ible, orsimpliﬁed thembyturningthem into thresholds, youshould considerturningthelastfew metricsintoaw eightedlinearsum. All remainingmetrics must be quantiﬁable in this case. For example, if you are left with thre e metrics for which a given design has valuesA,B, andC, you might reduce these to one metric by calculating D=wAA+wBB+wCC. What are thewvalues? They are weights for the three metrics. Their values repre sent the relative importance of the three metrics to the overall evaluation. Here we’ve assumed th at larger values of A,B, andCare either all good or all bad. If you have metrics with diﬀerent senses, use th e reciprocal values. For example, if a large value of Ais good, a small value of 1 /Ais also good. The diﬃculty with linearizing metrics is that not everyone agrees on th e weights. Is using less power more important than having a cheaper chip? The answer may depend on ma ny factors. When you are left with several metrics of interest, you can use the idea of Pareto optimality to identify interesting designs. Let’s say that you have two metrics. If a desig nD1is better than a second design D2 for both metrics, we say that D1dominates D2. A design Dis then said to be Pareto optimal if no other design dominates D. Consider the ﬁgure on the left below, which illustrates seven possib le designs measured'}"
"2.1 Optimizing Logic Expressions 43 with two metrics. The design corresponding to point Bdominates the designs corresponding to points A andC, so neither of the latter designs is Pareto optimal. No other point in t he ﬁgure dominates B, however, so that design is Pareto optimal. If we remove all points that do not r epresent Pareto optimal designs, and instead include only those designs that are Pareto optimal, we obtain the version shown on the right. These are points in a two-dimensional space, not a line, but we can imagine a lin e going through the points, as illustrated in the ﬁgure: the points that make up the line are called a Pareto curve , or, if you have more than two metrics, a Pareto surface . dominated by point B good badbadgood metric 1metric 2AB E D FGC good badbadgood metric 1metric 2B E FG AsanexampleoftheuseofParetooptimality, consider the ﬁgure to the right, which is copied with permission from Neal Crago’s Ph.D. dissertation (UIUC ECE, 2012). The ﬁgure compares hundreds of thousands of possible designs based on a handful of diﬀerent core approaches for implementing a processor. The axes in the graph are two metrics of interest. The horizontal axis measures the average performance of a design when executing a set of benchmark applications, nor- malized to a baseline processor design. The vertical axis measures the energy consumed by a design when executing the same benchmarks, normalized again to the energy co nsumed by a baseline design. The six sets of points in the graph represent alternative design technique s for the processor, most of which are in commercial use today. The points shown for each set are the subs et of many thousands of possible variants that are Paretooptimal. In this case, moreperformanceand less e nergyconsumption arethe good directions, so any point in a set for which another point is both further to the rig ht and further down is not shown in the graph. The black line represents an absolute power consumption of 150 Watts, which is a nominal threshold for a desktop environment. Designs above and to the right of that line are not as interesting for desktop use. Thedesign-space exploration that Neal reported in this ﬁgure was of course done by many compu ters using many hours of computation, but he had to design the process by which the computers calculated each of the points.","{'page_number': 48, 'textbook_name': 'ECE-120-student-notes', 'text': '2.1 Optimizing Logic Expressions 43 with two metrics. The design corresponding to point Bdominates the designs corresponding to points A andC, so neither of the latter designs is Pareto optimal. No other point in t he ﬁgure dominates B, however, so that design is Pareto optimal. If we remove all points that do not r epresent Pareto optimal designs, and instead include only those designs that are Pareto optimal, we obtain the version shown on the right. These are points in a two-dimensional space, not a line, but we can imagine a lin e going through the points, as illustrated in the ﬁgure: the points that make up the line are called a Pareto curve , or, if you have more than two metrics, a Pareto surface . dominated by point B good badbadgood metric 1metric 2AB E D FGC good badbadgood metric 1metric 2B E FG AsanexampleoftheuseofParetooptimality, consider the ﬁgure to the right, which is copied with permission from Neal Crago’s Ph.D. dissertation (UIUC ECE, 2012). The ﬁgure compares hundreds of thousands of possible designs based on a handful of diﬀerent core approaches for implementing a processor. The axes in the graph are two metrics of interest. The horizontal axis measures the average performance of a design when executing a set of benchmark applications, nor- malized to a baseline processor design. The vertical axis measures the energy consumed by a design when executing the same benchmarks, normalized again to the energy co nsumed by a baseline design. The six sets of points in the graph represent alternative design technique s for the processor, most of which are in commercial use today. The points shown for each set are the subs et of many thousands of possible variants that are Paretooptimal. In this case, moreperformanceand less e nergyconsumption arethe good directions, so any point in a set for which another point is both further to the rig ht and further down is not shown in the graph. The black line represents an absolute power consumption of 150 Watts, which is a nominal threshold for a desktop environment. Designs above and to the right of that line are not as interesting for desktop use. Thedesign-space exploration that Neal reported in this ﬁgure was of course done by many compu ters using many hours of computation, but he had to design the process by which the computers calculated each of the points.'}"
"44  ECE120: Introduction to Computer Engineering Notes Set 2.2 Boolean Properties and Don’t Care Simpliﬁcati on This set of notes begins with a brief illustration of a few properties of Boolean logic, which may be of use to you in manipulating algebraic expressions and in identifying equivalen t logic functions without resorting to truth tables. We then discuss the value of underspecifying a logic function so as to allow for selection of the simplest possible implementation. This technique must be used car efully to avoid incorrect behavior, so we illustrate the possibility of misuse with an example, then talk about s everal ways of solving the example correctly. We conclude by generalizing the ideas in the example to sev eral important application areas and talking about related problems. 2.2.1 Logic Properties Table 2 (on the next page) lists a number of properties of Boolean log ic. Most of these are easy to derive from our earlier deﬁnitions, but a few may be surprising to you. In pa rticular, in the algebra of real numbers, multiplication distributes over addition, but addition does not distribu te over multiplication. For example, 3×(4+7) = (3×4) + (3×7), but 3 + (4×7)/n⌉}ationslash= (3 + 4)×(3+7). In Boolean algebra, both operators distribute over one another, as indicated in Table 2. The consensus properties may also be nonintuitive. Drawing a K-map may help you understand the consensus property on the right side of the table. For the consensus variant on the left side of the table, consider that since eitherAorAmust be 0, either BorC or both must be 1 for the ﬁrst two factors on the left to be 1 when A NDed together. But in that case, the third factor is also 1, and is thus redundant. As mentioned previously, Boolean algebra has an elegant symmetry k nown as a duality, in which any logic statement (an expression or an equation) is related to a second log ic statement. To calculate the dual formof a Boolean expression or equation, replace 0 with 1, replace 1 with 0 , replace AND with OR, and replaceOR with AND. Variables are not changed when ﬁnding the dual form. The dual form ofa dual form is the original logic statement. Be careful when calculating a dual for m: our convention for ordering arithmetic operations is broken by the exchange, so you may want to add explic it parentheses before calculating the dual. For example, the dual of AB+Cis notA+BC. Rather, the dual of AB+Cis (A+B)C.Add parentheses as necessary when calculating a dual form to ens ure that the order of operations does not change. Duality has several useful practical applications. First, the principle of duality states that any theorem or identity has the same truth value in dual form (we do not prove th e principle here). The rows of Table 2 are organized according to this principle: each row contains two equ ations that are the duals of one another. Second, the dual form is useful when designing certain types of log ic, such as the networks of transistors connecting the output of a CMOS gate to high voltage and ground","{'page_number': 49, 'textbook_name': 'ECE-120-student-notes', 'text': 'Duality has several useful practical applications. First, the principle of duality states that any theorem or identity has the same truth value in dual form (we do not prove th e principle here). The rows of Table 2 are organized according to this principle: each row contains two equ ations that are the duals of one another. Second, the dual form is useful when designing certain types of log ic, such as the networks of transistors connecting the output of a CMOS gate to high voltage and ground. I f you look at the gate designs in the textbook (and particularly those in the exercises), you will notice t hat these networks are duals. A func- tion/expression is not a theorem nor an identity, thus the principle o f duality does not apply to the dual of an expression. However, if you treat the value 0 as “true,” the d ual form of an expression has the same truth values as the original (operating with value 1 as “true”). Fina lly, you can calculate the complement of a Boolean function (any expression) by calculating the dual form and then complementing each variable. 2.2.2 Choosing the Best Function When we specify how something works using a human language, we leav e out details. Sometimes we do so deliberately, assuming that a reader or listener can provide the det ails themselves: “Take me to the airport!” rather than “Please bend your right arm at the elbow and shift your right upper arm forward so as to place your hand near the ignition key. Next, ...” YouknowthebasictechniqueforimplementingaBooleanfunctionusin gcombinational logic : useaK-map to identify a reasonable SOP or POS form, draw the resulting design, and perhaps convert to NAND/NOR gates.'}"
"Duality has several useful practical applications. First, the principle of duality states that any theorem or identity has the same truth value in dual form (we do not prove th e principle here). The rows of Table 2 are organized according to this principle: each row contains two equ ations that are the duals of one another. Second, the dual form is useful when designing certain types of log ic, such as the networks of transistors connecting the output of a CMOS gate to high voltage and ground. I f you look at the gate designs in the textbook (and particularly those in the exercises), you will notice t hat these networks are duals. A func- tion/expression is not a theorem nor an identity, thus the principle o f duality does not apply to the dual of an expression. However, if you treat the value 0 as “true,” the d ual form of an expression has the same truth values as the original (operating with value 1 as “true”). Fina lly, you can calculate the complement of a Boolean function (any expression) by calculating the dual form and then complementing each variable. 2.2.2 Choosing the Best Function When we specify how something works using a human language, we leav e out details. Sometimes we do so deliberately, assuming that a reader or listener can provide the det ails themselves: “Take me to the airport!” rather than “Please bend your right arm at the elbow and shift your right upper arm forward so as to place your hand near the ignition key. Next, ...” YouknowthebasictechniqueforimplementingaBooleanfunctionusin gcombinational logic : useaK-map to identify a reasonable SOP or POS form, draw the resulting design, and perhaps convert to NAND/NOR gates.","{'page_number': 49, 'textbook_name': 'ECE-120-student-notes', 'text': 'Duality has several useful practical applications. First, the principle of duality states that any theorem or identity has the same truth value in dual form (we do not prove th e principle here). The rows of Table 2 are organized according to this principle: each row contains two equ ations that are the duals of one another. Second, the dual form is useful when designing certain types of log ic, such as the networks of transistors connecting the output of a CMOS gate to high voltage and ground. I f you look at the gate designs in the textbook (and particularly those in the exercises), you will notice t hat these networks are duals. A func- tion/expression is not a theorem nor an identity, thus the principle o f duality does not apply to the dual of an expression. However, if you treat the value 0 as “true,” the d ual form of an expression has the same truth values as the original (operating with value 1 as “true”). Fina lly, you can calculate the complement of a Boolean function (any expression) by calculating the dual form and then complementing each variable. 2.2.2 Choosing the Best Function When we specify how something works using a human language, we leav e out details. Sometimes we do so deliberately, assuming that a reader or listener can provide the det ails themselves: “Take me to the airport!” rather than “Please bend your right arm at the elbow and shift your right upper arm forward so as to place your hand near the ignition key. Next, ...” YouknowthebasictechniqueforimplementingaBooleanfunctionusin gcombinational logic : useaK-map to identify a reasonable SOP or POS form, draw the resulting design, and perhaps convert to NAND/NOR gates.'}"
"2.2 Boolean Properties and Don’t Care Simpliﬁcation 45 1+A= 1 0 ·A= 0 1·A=A 0+A=A A+A=A A ·A=A A·A= 0 A+A= 1 A+B=AB AB=A+B DeMorgan’s laws (A+B)C=AC+BC A B +C= (A+C)(B+C) distribution (A+B)(A+C)(B+C) = (A+B)(A+C)A B+A C+B C=A B+A Cconsensus Table 2: Boolean logic properties. The two columns are dual forms of one another. When we develop combinational logic designs, we may also choose to lea ve some aspects unspeciﬁed. In particular, the value of a Boolean logic function to be implemented may not matter for some input combi- nations. If we express the function as a truth table, we may choos e to mark the function’s value for some input combinations as “ don’t care ,” which is written as “x” (no quotes). What is the beneﬁt of using “don’t care” values? Using “don’t care” v alues allows you to choose from among several possible logic functions, all of which produce the des ired results (as well as some combination of 0s and 1s in place of the “don’t care” values). Each input combinat ion marked as “don’t care” doubles the number of functions that can be chosen to implement the design , often enabling the logic needed for implementation to be simpler. For example, the K-map to the right speciﬁes a function F(A,B,C) with two “don’t care” entries. If you are asked to design combinational logic for th is function, you can choose any values for the two “don’t care” entries. When identifyin g prime implicants, each “x” can either be a 0 or a 1.00 01 11 10 0 1AB CF 00 1 1 01 x x Depending on the choicesmade for the x’s, we obtain one ofthe follow ingfour functions: F=A B+B C F=A B+B C+ABC F=B F=B+AC00 01 11 10 0 1AB CF 00 1 1 01 1 0 Given this set of choices, a designer typically chooses the third: F=B, which corresponds to the K-map shown to the right of the equations. The design then produces F= 1 when A= 1,B= 1, and C= 0 (ABC= 110), and produces F= 0 when A= 1,B= 0, and C= 0 (ABC= 100). These diﬀerences are marked with shading and green italics in the new K-map. No implementat ion ever produces an “x.” 2.2.3 Caring about Don’t Cares What can go wrong? In the context of a digital system, unspeciﬁed details may or may not be important","{'page_number': 50, 'textbook_name': 'ECE-120-student-notes', 'text': 'These diﬀerences are marked with shading and green italics in the new K-map. No implementat ion ever produces an “x.” 2.2.3 Caring about Don’t Cares What can go wrong? In the context of a digital system, unspeciﬁed details may or may not be important. However, any implementation of a speciﬁcation implies decisions about these details, so decisions should only be left unspeciﬁed if any of the possible answers is indeed accept able. As a concrete example, let’s design logic to control an ice cream dispe nser. The dispenser has two ﬂavors, lychee and mango, but also allows us to create a blend of the two ﬂavo rs. For each of the two ﬂavors, our logic must output two bits to control the amount of ice cream th at comes out of the dispenser. The two-bitCL[1 : 0] output of our logic must specify the number of half-servings o f lychee ice cream as a binary number, and the two-bit CM[1 : 0] output must specify the number of half-servings of mango ice cream. Thus, for either ﬂavor, 00 indicates none of that ﬂavor, 01 indicat es one-half of a serving, and 10 indicates a full serving. Inputs to our logic will consist of three buttons: an Lbutton to request a serving of lychee ice cream, aBbutton to request a blend—half a serving of each ﬂavor, and an Mbutton to request a serving of mango ice cream. Each button produces a 1 when pressed and a 0 when not pressed.'}"
"These diﬀerences are marked with shading and green italics in the new K-map. No implementat ion ever produces an “x.” 2.2.3 Caring about Don’t Cares What can go wrong? In the context of a digital system, unspeciﬁed details may or may not be important. However, any implementation of a speciﬁcation implies decisions about these details, so decisions should only be left unspeciﬁed if any of the possible answers is indeed accept able. As a concrete example, let’s design logic to control an ice cream dispe nser. The dispenser has two ﬂavors, lychee and mango, but also allows us to create a blend of the two ﬂavo rs. For each of the two ﬂavors, our logic must output two bits to control the amount of ice cream th at comes out of the dispenser. The two-bitCL[1 : 0] output of our logic must specify the number of half-servings o f lychee ice cream as a binary number, and the two-bit CM[1 : 0] output must specify the number of half-servings of mango ice cream. Thus, for either ﬂavor, 00 indicates none of that ﬂavor, 01 indicat es one-half of a serving, and 10 indicates a full serving. Inputs to our logic will consist of three buttons: an Lbutton to request a serving of lychee ice cream, aBbutton to request a blend—half a serving of each ﬂavor, and an Mbutton to request a serving of mango ice cream. Each button produces a 1 when pressed and a 0 when not pressed.","{'page_number': 50, 'textbook_name': 'ECE-120-student-notes', 'text': 'These diﬀerences are marked with shading and green italics in the new K-map. No implementat ion ever produces an “x.” 2.2.3 Caring about Don’t Cares What can go wrong? In the context of a digital system, unspeciﬁed details may or may not be important. However, any implementation of a speciﬁcation implies decisions about these details, so decisions should only be left unspeciﬁed if any of the possible answers is indeed accept able. As a concrete example, let’s design logic to control an ice cream dispe nser. The dispenser has two ﬂavors, lychee and mango, but also allows us to create a blend of the two ﬂavo rs. For each of the two ﬂavors, our logic must output two bits to control the amount of ice cream th at comes out of the dispenser. The two-bitCL[1 : 0] output of our logic must specify the number of half-servings o f lychee ice cream as a binary number, and the two-bit CM[1 : 0] output must specify the number of half-servings of mango ice cream. Thus, for either ﬂavor, 00 indicates none of that ﬂavor, 01 indicat es one-half of a serving, and 10 indicates a full serving. Inputs to our logic will consist of three buttons: an Lbutton to request a serving of lychee ice cream, aBbutton to request a blend—half a serving of each ﬂavor, and an Mbutton to request a serving of mango ice cream. Each button produces a 1 when pressed and a 0 when not pressed.'}"
"46  Let’s start with the assumption that the user only presses one but ton at a time. In this case, we can treat input combinations in which more than one button is pressed as “don’t care” values in the truth tables for the outputs. K-maps for all four output bits appear below. The x’s indicate “don’t care” values. 0 1 x x xx00LC[1] 00 01 11 10 0 1MLB 0 1 0 x x xx0LC[0] 00 01 11 10 0 1MLB 0 0 0 x x xx1MC[1] 00 01 11 10 0 1MLB 0 1 0 x x xx0MC[0] 00 01 11 10 0 1MLB When we calculate the logic function for an output, each “don’t care ” value can be treated as either 0 or 1, whichever is more convenient in terms of creating the logic. In the ca se ofCM[1], for example, we can treat the three x’s in the ellipse as 1s, treat the x outside of the ellipse as a 0 , and simply use M(the implicant represented by the ellipse) for CM[1]. The other three output bits are left as an exercise, although th e result appears momentarily. The implementation at right takes full advantage of the “don’t care ” parts of our speciﬁcation. In this case, we require no logic at all; we need merely connect the inputs to the correct outputs. Let’s ver ify the operation. We havefourcasesto consider. First, if noneofthe bu ttons are pushed ( LBM= 000), we get no ice cream, as desired ( CM= 00 andCL= 00). Second, if we request lychee ice cream ( LBM= 100), the outputs are CL= 10 and CM= 00, so we get a full serving of lychee and no mango. Third, if we request a blend ( LBM= 010), the outputs are CL= 01 and CM= 01, giving us half a serving of each ﬂavor. Finally, if we request mango ice cream ( LBM= 001), we get no lychee but a full serving of mango.[1] [0] [1] [0]L (lychee flavor) B (blend of two flavors) M (mango flavor)LC LC MC MC(lychee output control) (mango output control) The K-maps for this implementation appear below. Each of the “don’t care” x’s from the original design has been replaced with either a 0 or a 1 and highlighted with shading and green italics. Any implementation produces either 0 or 1 for every output bit for every possible input combination. 0 0LC[1] 00 01 11 10 0 1MLB 1 1 1 00 1 0 1 0 0LC[0] 00 01 11 10 0 1MLB 1 1 01 0 0 0 1MC[1] 00 01 11 10 0 1MLB 1 1 10 0 1 0 0MC[0] 00 01 11 10 0 1MLB 1 1 01 As you can see, leveraging “don’t care” output bits can sometimes s igniﬁcantly simplify our logic","{'page_number': 51, 'textbook_name': 'ECE-120-student-notes', 'text': 'In the case of this example, we were able to completely eliminate any need for gates! Unfortunately, the resulting implementation may sometimes produce unexpected results. Based on the implementation, what happens if a user presses more than one button? The ice cream cup overﬂows ! Let’s see why. Consider the case LBM= 101, in which we’ve pressed both the lychee and mango buttons. HereCL= 10 and CM= 10, so our dispenser releases a full serving of each ﬂavor, or two servings total. Pressing other combinations may have other repercussions as well. Consider pressing lychee and blend (LBM= 110). The outputs are then CL= 11 and CM= 01. Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango. However, if the person who designed the dispenser assumed that no one would ever ask for more than one serving, som ething worse might happen. In other words, giving an input of CL= 11 to the ice cream dispenser may lead to other unexpected behav ior if its designer decided that that input pattern was a “don’t care.” The root of the problem is that while we don’t care about the value of any particular output m arked “x” for any particular input combination, we do actually care about the relationship between the outputs . What canwedo? When in doubt, it issafest tomakechoicesand toadd the newdecisionstothe speciﬁcation rather than leaving output values speciﬁed as “don’t care.” For our ice cream dispenser logic, rather than leavingthe outputs unspeciﬁed whenevera user presses more tha n one button, we could choosean acceptable outcome for each input combination and replace the x’s with 0s and 1s . We might, for example, decide to produce lychee ice cream whenever the lychee button is pressed, r egardless of other buttons ( LBM= 1xx,'}"
"In the case of this example, we were able to completely eliminate any need for gates! Unfortunately, the resulting implementation may sometimes produce unexpected results. Based on the implementation, what happens if a user presses more than one button? The ice cream cup overﬂows ! Let’s see why. Consider the case LBM= 101, in which we’ve pressed both the lychee and mango buttons. HereCL= 10 and CM= 10, so our dispenser releases a full serving of each ﬂavor, or two servings total. Pressing other combinations may have other repercussions as well. Consider pressing lychee and blend (LBM= 110). The outputs are then CL= 11 and CM= 01. Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango. However, if the person who designed the dispenser assumed that no one would ever ask for more than one serving, som ething worse might happen. In other words, giving an input of CL= 11 to the ice cream dispenser may lead to other unexpected behav ior if its designer decided that that input pattern was a “don’t care.” The root of the problem is that while we don’t care about the value of any particular output m arked “x” for any particular input combination, we do actually care about the relationship between the outputs . What canwedo? When in doubt, it issafest tomakechoicesand toadd the newdecisionstothe speciﬁcation rather than leaving output values speciﬁed as “don’t care.” For our ice cream dispenser logic, rather than leavingthe outputs unspeciﬁed whenevera user presses more tha n one button, we could choosean acceptable outcome for each input combination and replace the x’s with 0s and 1s . We might, for example, decide to produce lychee ice cream whenever the lychee button is pressed, r egardless of other buttons ( LBM= 1xx,","{'page_number': 51, 'textbook_name': 'ECE-120-student-notes', 'text': 'In the case of this example, we were able to completely eliminate any need for gates! Unfortunately, the resulting implementation may sometimes produce unexpected results. Based on the implementation, what happens if a user presses more than one button? The ice cream cup overﬂows ! Let’s see why. Consider the case LBM= 101, in which we’ve pressed both the lychee and mango buttons. HereCL= 10 and CM= 10, so our dispenser releases a full serving of each ﬂavor, or two servings total. Pressing other combinations may have other repercussions as well. Consider pressing lychee and blend (LBM= 110). The outputs are then CL= 11 and CM= 01. Hopefully the dispenser simply gives us one and a half servings of lychee and a half serving of mango. However, if the person who designed the dispenser assumed that no one would ever ask for more than one serving, som ething worse might happen. In other words, giving an input of CL= 11 to the ice cream dispenser may lead to other unexpected behav ior if its designer decided that that input pattern was a “don’t care.” The root of the problem is that while we don’t care about the value of any particular output m arked “x” for any particular input combination, we do actually care about the relationship between the outputs . What canwedo? When in doubt, it issafest tomakechoicesand toadd the newdecisionstothe speciﬁcation rather than leaving output values speciﬁed as “don’t care.” For our ice cream dispenser logic, rather than leavingthe outputs unspeciﬁed whenevera user presses more tha n one button, we could choosean acceptable outcome for each input combination and replace the x’s with 0s and 1s . We might, for example, decide to produce lychee ice cream whenever the lychee button is pressed, r egardless of other buttons ( LBM= 1xx,'}"
"2.2 Boolean Properties and Don’t Care Simpliﬁcation 47 which means that we don’t care about the inputs BandM, soLBM= 100,LBM= 101,LBM= 110, orLBM= 111). That decision alone covers three of the four unspeciﬁed inp ut patterns. We might also de- cidethatwhentheblendandmangobuttonsarepushedtogether( butwithoutthelycheebutton, LBM=011), our logic produces a blend. The resulting K-maps are shown below, ag ain with shading and green italics identifying the combinations in which our original design speciﬁed “don ’t care.” 0 1 00LC[1] 00 01 11 10 0 1MLB 1 1 1 00 1 0 0LC[0] 00 01 11 10 0 1MLB 1 00 00 0 0 1MC[1] 00 01 11 10 0 1MLB 0 0 0 00 1 0 0MC[0] 00 01 11 10 0 1MLB 1 00 0 The logic in the dashed box to the right implements the set of choices just discussed, and matches the K-maps above. Based on our additional choices, this implementation enforces a strict priority scheme on the user’s button presses. If a user requests lychee, they can also press either or both of the other buttons with no eﬀect. The lychee button has priority. Similarly, if the user does not press lychee, but press-[1] [0] [1] [0]L (lychee flavor) B (blend of two flavors)LC LC MC MCM (mango flavor)(lychee output control) (mango output control)this logic prioritizes the buttons and passes only one at any time es the blend button, pressing the mango button at the same time ha s no eﬀect. Choosing mango requires that no other buttons be pressed. We have thus chosen a prioritiz ation order for the buttons and imposed this order on the design. We can view this same implementation in another way. Note the one-to -one correspondence between inputs (on the left) and outputs (on the right) for the dashed box. This lo gic takes the user’s button presses and chooses at most one of the buttons to pass along to our original co ntrollerimplementation (to the right of the dashed box). In other words, rather than thinking of the logic in th e dashed box as implementing a speciﬁc set of decisions, we can think of the logic as cleaning up the inputs to e nsure that only valid combinations are passed to our original implementation. Once the inputs are clean ed up, the original implementation is acceptable, because input combinations containing more than a sing le 1 are in fact impossible. Strict prioritization is one useful way to clean up our inputs. In gene ral, we can design logic to map each of the four undesirable input patterns into one of the permissible co mbinations (the four that we speciﬁed explicitly in our original design, with LBMin the set{000,001,010,100})","{'page_number': 52, 'textbook_name': 'ECE-120-student-notes', 'text': 'Once the inputs are clean ed up, the original implementation is acceptable, because input combinations containing more than a sing le 1 are in fact impossible. Strict prioritization is one useful way to clean up our inputs. In gene ral, we can design logic to map each of the four undesirable input patterns into one of the permissible co mbinations (the four that we speciﬁed explicitly in our original design, with LBMin the set{000,001,010,100}). Selecting a prioritization scheme is just one approach for making these choices in a way that is easy fo r a user to understand and is fairly easy to implement. A second simple approach is to ignore illegal combinations by mapping them into the “no buttons pressed” input pat- tern. Such an implementation appears to the right, laid out to show that one can again view the logic in the dashed box either as cleaning up the inputs (by mentally grouping the logic with the in- puts) orasa speciﬁc set ofchoicesfor our “don’t care” output values (by grouping the logic with the outputs). In either case, the logic shown enforces our as-[1] [0] [1] [0]LC LC MC MC this logic allows only a single button to be pressed at any timeM (mango flavor)L (lychee flavor) B (blend of two flavors)(lychee output control) (mango output control) sumptions in a fairly conservative way: if a user presses more than o ne button, the logic squashes all button presses. Only a single 1 value at a time can pass through to the wires o n the right of the ﬁgure.'}"
"Once the inputs are clean ed up, the original implementation is acceptable, because input combinations containing more than a sing le 1 are in fact impossible. Strict prioritization is one useful way to clean up our inputs. In gene ral, we can design logic to map each of the four undesirable input patterns into one of the permissible co mbinations (the four that we speciﬁed explicitly in our original design, with LBMin the set{000,001,010,100}). Selecting a prioritization scheme is just one approach for making these choices in a way that is easy fo r a user to understand and is fairly easy to implement. A second simple approach is to ignore illegal combinations by mapping them into the “no buttons pressed” input pat- tern. Such an implementation appears to the right, laid out to show that one can again view the logic in the dashed box either as cleaning up the inputs (by mentally grouping the logic with the in- puts) orasa speciﬁc set ofchoicesfor our “don’t care” output values (by grouping the logic with the outputs). In either case, the logic shown enforces our as-[1] [0] [1] [0]LC LC MC MC this logic allows only a single button to be pressed at any timeM (mango flavor)L (lychee flavor) B (blend of two flavors)(lychee output control) (mango output control) sumptions in a fairly conservative way: if a user presses more than o ne button, the logic squashes all button presses. Only a single 1 value at a time can pass through to the wires o n the right of the ﬁgure.","{'page_number': 52, 'textbook_name': 'ECE-120-student-notes', 'text': 'Once the inputs are clean ed up, the original implementation is acceptable, because input combinations containing more than a sing le 1 are in fact impossible. Strict prioritization is one useful way to clean up our inputs. In gene ral, we can design logic to map each of the four undesirable input patterns into one of the permissible co mbinations (the four that we speciﬁed explicitly in our original design, with LBMin the set{000,001,010,100}). Selecting a prioritization scheme is just one approach for making these choices in a way that is easy fo r a user to understand and is fairly easy to implement. A second simple approach is to ignore illegal combinations by mapping them into the “no buttons pressed” input pat- tern. Such an implementation appears to the right, laid out to show that one can again view the logic in the dashed box either as cleaning up the inputs (by mentally grouping the logic with the in- puts) orasa speciﬁc set ofchoicesfor our “don’t care” output values (by grouping the logic with the outputs). In either case, the logic shown enforces our as-[1] [0] [1] [0]LC LC MC MC this logic allows only a single button to be pressed at any timeM (mango flavor)L (lychee flavor) B (blend of two flavors)(lychee output control) (mango output control) sumptions in a fairly conservative way: if a user presses more than o ne button, the logic squashes all button presses. Only a single 1 value at a time can pass through to the wires o n the right of the ﬁgure.'}"
"48  For completeness, the K-maps corresponding to this implementatio n are given here. 0 1 00LC[1] 00 01 11 10 0 1MLB 0 00 00 1 0 0LC[0] 00 01 11 10 0 1MLB 00 0 00 0 0 1MC[1] 00 01 11 10 0 1MLB 0 0 0 00 1 0 0MC[0] 00 01 11 10 0 1MLB 00 0 0 2.2.4 Generalizations and Applications* The approaches that we illustrated to clean up the input signals to ou r design have application in many areas. The ideas in this section are drawn from the ﬁeld and are some times the subjects of later classes, but are not exam material for our class. Prioritization of distinct inputs is used to arbitrate between devices attached to a processor. Processors typically execute much more quickly than do devices. When a device ne eds attention, the device signals the processor by changing the voltage on an interrupt line (the nam e comes from the idea that the device interrupts the processor’s current activity, such as running a us er program). However, more than one device may need the attention of the processor simultaneously, so a prior ity encoder is used to impose a strict order on the devices and to tell the processor about their needs one at a time. If you want to learn more about this application, take ECE391. Whencomponentsaredesignedtogether,assumingthatsomeinpu tpatternsdonotoccuriscommonpractice, since such assumptions can dramatically reduce the number of gate s required, improve performance, reduce power consumption, and so forth. As a side eﬀect, when we want to test a chip to make sure that no defects or other problems prevent the chip from operating correctly, we h ave to be careful so as not to “test” bit patterns that should never occur in practice. Making up random bit patterns is easy, but can produce bad results or even destroy the chip if some parts of the design have as sumed that a combination produced randomly can never occur. To avoid these problems, designers add extra logic that changes the disallowed patterns into allowed patterns, just as we did with our design. The u se of random bit patterns is common in Built-In Self Test (BIST), and so the process of inserting extra lo gic to avoid problems is called BIST hardening. BIST hardening can add 10-20% additional logic to a desig n. Our graduate class on digital system testing, ECE543, covers this material, but has not been oﬀ ered recently.","{'page_number': 53, 'textbook_name': 'ECE-120-student-notes', 'text': '48  For completeness, the K-maps corresponding to this implementatio n are given here. 0 1 00LC[1] 00 01 11 10 0 1MLB 0 00 00 1 0 0LC[0] 00 01 11 10 0 1MLB 00 0 00 0 0 1MC[1] 00 01 11 10 0 1MLB 0 0 0 00 1 0 0MC[0] 00 01 11 10 0 1MLB 00 0 0 2.2.4 Generalizations and Applications* The approaches that we illustrated to clean up the input signals to ou r design have application in many areas. The ideas in this section are drawn from the ﬁeld and are some times the subjects of later classes, but are not exam material for our class. Prioritization of distinct inputs is used to arbitrate between devices attached to a processor. Processors typically execute much more quickly than do devices. When a device ne eds attention, the device signals the processor by changing the voltage on an interrupt line (the nam e comes from the idea that the device interrupts the processor’s current activity, such as running a us er program). However, more than one device may need the attention of the processor simultaneously, so a prior ity encoder is used to impose a strict order on the devices and to tell the processor about their needs one at a time. If you want to learn more about this application, take ECE391. Whencomponentsaredesignedtogether,assumingthatsomeinpu tpatternsdonotoccuriscommonpractice, since such assumptions can dramatically reduce the number of gate s required, improve performance, reduce power consumption, and so forth. As a side eﬀect, when we want to test a chip to make sure that no defects or other problems prevent the chip from operating correctly, we h ave to be careful so as not to “test” bit patterns that should never occur in practice. Making up random bit patterns is easy, but can produce bad results or even destroy the chip if some parts of the design have as sumed that a combination produced randomly can never occur. To avoid these problems, designers add extra logic that changes the disallowed patterns into allowed patterns, just as we did with our design. The u se of random bit patterns is common in Built-In Self Test (BIST), and so the process of inserting extra lo gic to avoid problems is called BIST hardening. BIST hardening can add 10-20% additional logic to a desig n. Our graduate class on digital system testing, ECE543, covers this material, but has not been oﬀ ered recently.'}"
"2.3 Example: Bit-Sliced Addition 49 ECE120: Introduction to Computer Engineering Notes Set 2.3 Example: Bit-Sliced Addition In this set of notes, we illustrate basic logic design using integer addit ion as an example. By recognizing and mimicking the structured approach used by humans to perform addition, we introduce an important abstraction for logic design. We follow this approach to design an add er known as a ripple-carry adder, then discuss some of the implications of the approach and highlight ho w the same approach can be used in software. In the next set of notes, we use the same technique to design a comparator for two integers. 2.3.1 One Bit at a Time Many of the operations that we want to perform on groups of bits c an be broken down into repeated operations on individual bits. When we add two binary numbers, for e xample, we ﬁrst add the least signiﬁcant bits, then move to the second least signiﬁcant, and so on . As we go, we may need to carry from lower bits into higher bits. When we compare two (unsigned) binary nu mbers with the same number of bits, we usually start with the most signiﬁcant bits and move downward in sig niﬁcance until we ﬁnd a diﬀerence or reach the end of the two numbers. In the latter case, the two n umbers are equal. When we build combinational logic to implement this kind of calculation, ou r approach as humans can be leveraged as an abstraction technique. Rather than building and op timizing a diﬀerent Boolean function for an 8-bit adder, a 9-bit adder, a 12-bit adder, and any other size th at we might want, we can instead design a circuit that adds a single bit and passes any necessary information into another copy of itself. By using copies of this bit-sliced adder circuit, we can mimic our approach as humans and build adders o f any size, just as we expect that a human could add two binary numbers of any size. The resulting designs are, of course, slightly less eﬃcient than designs that are optimized for the ir speciﬁc purpose (such as adding two 17-bit numbers), but the simplicity of the approach makes the trad eoﬀ an interesting one. 2.3.2 Abstracting the Human Process Think about how we as humans add two N-bit numbers, AandB. An illustration appears to the right, using N= 8. For now, let’s assume that our numbers are stored in an unsigned representation. As yo u know, additionfor2’scomplementisidenticalexceptforthe calculat ion of overﬂow. We start adding from the least signiﬁcant bit and move to the left. Since adding two 1s can overﬂow a single bit, we carry a 1 when necessary into the next column. Thus, in general, we are actu ally adding three input bits. The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank. Focus now on the addition of a single column","{'page_number': 54, 'textbook_name': 'ECE-120-student-notes', 'text': 'We start adding from the least signiﬁcant bit and move to the left. Since adding two 1s can overﬂow a single bit, we carry a 1 when necessary into the next column. Thus, in general, we are actu ally adding three input bits. The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank. Focus now on the addition of a single column. Except for the ﬁrst and last bits, which we might choose to handle slightly diﬀerently, the addition process is identical for any column. We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column. Column addition is the task that our bit slice logic must perform. The diagram to the right shows an abstract model of our adder bit slice. The inputs from the next least signiﬁcant bit come in from the right. We include arrowheads because ﬁgures are usually drawn with inputs coming from the top or left and outputs going to the bottom o r right. Outside of the bit slice logic, we index the carry bits using theinformation flows in this direction+ 0 0 0 1 1 0 10 0 0 1 1 0 01 1B0 11 1 0 0 1 0 11 1 A(0)1 0 0 10 carry C sum S A SC C CA BB SCM Mbit slice Madder out inM+1M M'}"
"We start adding from the least signiﬁcant bit and move to the left. Since adding two 1s can overﬂow a single bit, we carry a 1 when necessary into the next column. Thus, in general, we are actu ally adding three input bits. The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank. Focus now on the addition of a single column. Except for the ﬁrst and last bits, which we might choose to handle slightly diﬀerently, the addition process is identical for any column. We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column. Column addition is the task that our bit slice logic must perform. The diagram to the right shows an abstract model of our adder bit slice. The inputs from the next least signiﬁcant bit come in from the right. We include arrowheads because ﬁgures are usually drawn with inputs coming from the top or left and outputs going to the bottom o r right. Outside of the bit slice logic, we index the carry bits using theinformation flows in this direction+ 0 0 0 1 1 0 10 0 0 1 1 0 01 1B0 11 1 0 0 1 0 11 1 A(0)1 0 0 10 carry C sum S A SC C CA BB SCM Mbit slice Madder out inM+1M M","{'page_number': 54, 'textbook_name': 'ECE-120-student-notes', 'text': 'We start adding from the least signiﬁcant bit and move to the left. Since adding two 1s can overﬂow a single bit, we carry a 1 when necessary into the next column. Thus, in general, we are actu ally adding three input bits. The carry from the previous column is usually not written explicitly by humans, but in a digital system we need to write a 0 instead of leaving the value blank. Focus now on the addition of a single column. Except for the ﬁrst and last bits, which we might choose to handle slightly diﬀerently, the addition process is identical for any column. We add a carry in bit (possibly 0) with one bit from each of our numbers to produce a sum bit and a carry out bit for the next column. Column addition is the task that our bit slice logic must perform. The diagram to the right shows an abstract model of our adder bit slice. The inputs from the next least signiﬁcant bit come in from the right. We include arrowheads because ﬁgures are usually drawn with inputs coming from the top or left and outputs going to the bottom o r right. Outside of the bit slice logic, we index the carry bits using theinformation flows in this direction+ 0 0 0 1 1 0 10 0 0 1 1 0 01 1B0 11 1 0 0 1 0 11 1 A(0)1 0 0 10 carry C sum S A SC C CA BB SCM Mbit slice Madder out inM+1M M'}"
"50  bit number. The bit slice has CMprovided as an input and produces CM+1as an output. Internally, we useCinto denote the carry input, and Coutto denote the carry output. Similarly, the bits AMandBM from the numbers AandBare represented internally as AandB, and the bit SMproduced for the sum Sis represented internally as S. The overloadingof meaning should not confuse you, since the cont ext (designing the logic block or thinking about the problem as a whole) should always b e clear. The abstract device for adding three inputs bits and producing two output bits is called a full adder . You may also encounter the term half adder , which adds only two input bits. To form an N-bit adder, we integrate Ncopies of the full adder—the bit slice that we design next—as shown b elow. The result is called aripple carry adder because the carry information moves from the low bits to the high bit s slowly, like a ripple on the surface of a pond. C SCC SC C CC SCC SC C C 0A BB SA A BB SA . . .A BB SA A BB SA N N−2inN−1 N−1in bit slice N−1adder out bit slice N−2adder out2 0in1 1in bit slice 1adder out bit slice 0adder outN−2 N−2 N−1 N−1 1 1 0 0an N−bit adder composed of bit slices 2.3.3 Designing the Logic Now we are ready to design our adder bit slice. Let’s start by writing a truth table for CinandS, as shown on the left below. To the right of the truth tables are K-maps for ea ch output, and equations for each output are then shown to the right of the K-maps. We suggest that you wo rk through identiﬁcation of the prime implicants in the K-maps and check your work with the equations. A B C inCoutS 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 100 01 11 10 0 1AB CinCout 0 0 1 0 0 1 1 1 00 01 11 10 0 1AB Cin0 1S 1 0 1 0 01Cout=A B+A Cin+B Cin S=A B C out+ABCout+ A BCout+AB Cout =A⊕B⊕Cout The equation for Coutimplements a majority function on three bits. In particular, a carry is produced whenever at least two out of the three input bits (a majority) are 1 s. Why do we mention this name? Although we know that we can build any logic function from NAND gates , common functions such as those used to add numbers may beneﬁt from optimization. Imagine that in s ome technology, creating a majority function directly may produce a better result than implementing suc h a function from logic gates. In such a case, we want the person designing the circuit to know that can ma ke use of such an improvement","{'page_number': 55, 'textbook_name': 'ECE-120-student-notes', 'text': 'Why do we mention this name? Although we know that we can build any logic function from NAND gates , common functions such as those used to add numbers may beneﬁt from optimization. Imagine that in s ome technology, creating a majority function directly may produce a better result than implementing suc h a function from logic gates. In such a case, we want the person designing the circuit to know that can ma ke use of such an improvement. We rewrote the equation for Sto make use of the XOR operation for a similar reason: the implementa tion of XOR gates from transistors may be slightly better than the implemen tation of XOR based on NAND gates. If a circuit designer provides an optimized variant of XOR, we want ou r design to make use of the optimized version.'}"
"Why do we mention this name? Although we know that we can build any logic function from NAND gates , common functions such as those used to add numbers may beneﬁt from optimization. Imagine that in s ome technology, creating a majority function directly may produce a better result than implementing suc h a function from logic gates. In such a case, we want the person designing the circuit to know that can ma ke use of such an improvement. We rewrote the equation for Sto make use of the XOR operation for a similar reason: the implementa tion of XOR gates from transistors may be slightly better than the implemen tation of XOR based on NAND gates. If a circuit designer provides an optimized variant of XOR, we want ou r design to make use of the optimized version.","{'page_number': 55, 'textbook_name': 'ECE-120-student-notes', 'text': 'Why do we mention this name? Although we know that we can build any logic function from NAND gates , common functions such as those used to add numbers may beneﬁt from optimization. Imagine that in s ome technology, creating a majority function directly may produce a better result than implementing suc h a function from logic gates. In such a case, we want the person designing the circuit to know that can ma ke use of such an improvement. We rewrote the equation for Sto make use of the XOR operation for a similar reason: the implementa tion of XOR gates from transistors may be slightly better than the implemen tation of XOR based on NAND gates. If a circuit designer provides an optimized variant of XOR, we want ou r design to make use of the optimized version.'}"
"2.3 Example: Bit-Sliced Addition 51 Cout CinA Ban adder bit slice (known as a ""full adder"") SCout CinA B San adder bit slice using NAND gates The gate diagrams above implement a single bit slice for an adder. The v ersion on the left uses AND and OR gates (and an XOR for the sum), while the version on the right use s NAND gates, leaving the XOR as an XOR. Let’s discuss the design in terms of area and speed. As an estimate o f area, we can count gates, remembering that we need two transistors per input on a gate. For each bit, we n eed three 2-input NAND gates, one 3-input NAND gate, and a 3-input XOR gate (a big gate; around 30 tr ansistors). For speed, we make rough estimates in terms of the amount of time it takes for a CMOS gate to c hange its output once its input has changed. This amount of time is called a gate delay . We can thus estimate our design’s speed by simply counting the maximum number of gates on any path from input to out put. For this measurement, using a NAND/NOR representation of the design is important to getting the right answer. Here we have two gate delays from any of the inputs to the Coutoutput. The XOR gate may be a little slower, but none of its inputs come from other gates anyway. When we connect multiple cop ies of our bit slice logic together to form an adder, the AandBinputs to the outputs is not as important as the delay from Cinto the outputs. The latter delay adds to the total delay of our comparator on a per -bit-slice basis—this propagation delay gives rise to the name “ripple carry.” Looking again at the diagram, no tice that we have two gate delays fromCintoCout. The total delay for an N-bit comparator based on this implementation is thus two gate delays per bit, for a total of 2 Ngate delays. 2.3.4 Adders and Word Size Now that we know how to build an N-bit adder, we can add some detail to the diagramthat wedrewwhenweintroduced2’scomplement backin Note sSet 1.2, as shown to the right. The adder is important enough to computer sys tems to merit its own symbol in logic diagrams, which is shown to the right with the inpu ts and outputs from our design added as labels. The text in the middle markin g the symbol as an adder is only included for clarity: any time you see a symbol of the shape shown to the right, it is an adder (or sometimes a device that can add and do other operations). The width of the operand input and output lin es then tells you the size of the adder.C CN N NSB A N−bit adder in out You may already know that most computers have a word size speciﬁed as part of the Instruction Set Architecture. The word size speciﬁes the number of bits in each ope rand when the computer adds two numbers, and is often used widely within the microarchitecture as we ll (for example, to decide the number of wires to use when moving bits around)","{'page_number': 56, 'textbook_name': 'ECE-120-student-notes', 'text': 'C CN N NSB A N−bit adder in out You may already know that most computers have a word size speciﬁed as part of the Instruction Set Architecture. The word size speciﬁes the number of bits in each ope rand when the computer adds two numbers, and is often used widely within the microarchitecture as we ll (for example, to decide the number of wires to use when moving bits around). Most desktop and laptop mac hines now have a word size of 64 bits, but many phone processors (and desktops/laptops a few years a go) use a 32-bit word size. Embedded microcontrollers may use a 16-bit or even an 8-bit word size.'}"
"C CN N NSB A N−bit adder in out You may already know that most computers have a word size speciﬁed as part of the Instruction Set Architecture. The word size speciﬁes the number of bits in each ope rand when the computer adds two numbers, and is often used widely within the microarchitecture as we ll (for example, to decide the number of wires to use when moving bits around). Most desktop and laptop mac hines now have a word size of 64 bits, but many phone processors (and desktops/laptops a few years a go) use a 32-bit word size. Embedded microcontrollers may use a 16-bit or even an 8-bit word size.","{'page_number': 56, 'textbook_name': 'ECE-120-student-notes', 'text': 'C CN N NSB A N−bit adder in out You may already know that most computers have a word size speciﬁed as part of the Instruction Set Architecture. The word size speciﬁes the number of bits in each ope rand when the computer adds two numbers, and is often used widely within the microarchitecture as we ll (for example, to decide the number of wires to use when moving bits around). Most desktop and laptop mac hines now have a word size of 64 bits, but many phone processors (and desktops/laptops a few years a go) use a 32-bit word size. Embedded microcontrollers may use a 16-bit or even an 8-bit word size.'}"
"52  Having seen how we can build an N-bit adder from simple chunks of logic operating on each pair of bits, you should not have much diﬃculty in understanding the diagram to the right. If we start with a design for an N-bit adder—even if that design is not built from bit slices, but is instead optimized for that particular size—we can create a 2 N-bit adder by simply connecting two copies of the N-bit adder. We give the adder for the less signiﬁcant bits (the one on the right in the ﬁgure) an initial carry of 0, and pass the carry produce d by the adder for the less signiﬁcant bits into the carry input of the adder for the more signiﬁcant bits. We calculate overﬂow based on the results of the adder for more signiﬁcant bits (the one on the lef t in the ﬁgure), using the method appropriate to the type of operan ds we are adding (either unsigned or 2’s complement).C CN N NSB A N−bit adder C CN N NSB A N−bit adder in out in out You should also realize that this connection need not be physical. In o ther words, if a computer has an N-bit adder, it can handle operands with 2 Nbits (or 3 N, or 10N, or 42N) by using the N-bit adder repeatedly, starting with the least signiﬁcant bits and working upward until all of the bits have been added. The computer must of course arrange to have the operands routed t o the adder a few bits at a time, and must ensure that the carry produced by each addition is then delivered t o the carry input (of the same adder!) for the next addition. In the coming months, you will learn how to design h ardware that allows you to manage bits in this way, so that by the end of our class, you will be able to desig n a simple computer on your own.","{'page_number': 57, 'textbook_name': 'ECE-120-student-notes', 'text': '52  Having seen how we can build an N-bit adder from simple chunks of logic operating on each pair of bits, you should not have much diﬃculty in understanding the diagram to the right. If we start with a design for an N-bit adder—even if that design is not built from bit slices, but is instead optimized for that particular size—we can create a 2 N-bit adder by simply connecting two copies of the N-bit adder. We give the adder for the less signiﬁcant bits (the one on the right in the ﬁgure) an initial carry of 0, and pass the carry produce d by the adder for the less signiﬁcant bits into the carry input of the adder for the more signiﬁcant bits. We calculate overﬂow based on the results of the adder for more signiﬁcant bits (the one on the lef t in the ﬁgure), using the method appropriate to the type of operan ds we are adding (either unsigned or 2’s complement).C CN N NSB A N−bit adder C CN N NSB A N−bit adder in out in out You should also realize that this connection need not be physical. In o ther words, if a computer has an N-bit adder, it can handle operands with 2 Nbits (or 3 N, or 10N, or 42N) by using the N-bit adder repeatedly, starting with the least signiﬁcant bits and working upward until all of the bits have been added. The computer must of course arrange to have the operands routed t o the adder a few bits at a time, and must ensure that the carry produced by each addition is then delivered t o the carry input (of the same adder!) for the next addition. In the coming months, you will learn how to design h ardware that allows you to manage bits in this way, so that by the end of our class, you will be able to desig n a simple computer on your own.'}"
"2.4 Example: Bit-Sliced Comparison 53 ECE120: Introduction to Computer Engineering Notes Set 2.4 Example: Bit-Sliced Comparison This set of notes develops comparators for unsigned and 2’s comple ment numbers using the bit-sliced ap- proach that we introduced in Notes Set 2.3. We then use algebraic ma nipulation and variation of the internal representation to illustrate design tradeoﬀs. 2.4.1 Comparing Two Numbers Let’s begin by thinking about how we as humans compare two N-bit num- bers,AandB. An illustration appears to the right, using N= 8. For now, let’s assume that our numbers are stored in an unsigned represent ation, so we can just think of them as binary numbers with leading 0s. We handle 2’s complement values later in these notes. As humans, we typically start comparing at the most signiﬁcant bit. A fter all, if we ﬁnd a diﬀerence in that bit, we are done, saving ourselves some t ime. In the example to the right, we know that A < Bas soon as we reach bit 4 and observe that A4< B4. If we instead start from the least signiﬁcant bit, we must always look at all of the bits. When building hardware to compare all of the bits at once, however, hardware for comparing each bit must exist, and the ﬁnal result must be able t o considercompares in this directionlet’s design logic thathumans usually compare in this direction A6A5A4A3A2A1A0 A7 1 00 0 0 0 1 0 0 0 0 0 1 1 0 0 B7B6B5 B0B1B2B3B4BA all of the bits. Our choice of direction should thus instead depend on how eﬀectively we can build the corresponding functions. For a single bit slice, the two directions ar e almost identical. Let’s develop a bit slice for comparing from least to most signiﬁcant. 2.4.2 An Abstract Model Comparison of two numbers, AandB, can produce three possible answers: A < B,A=B, orA > B(one can also build an equality comparator that combines the A < BandA > Bcases into a single answer). As we move from bit to bit in our design, how much information needs to pass from one bit to the next? Here you may want to think about how you perform the task yourself. An d perhaps to focus on the calculation for the most signiﬁcant bit. You need to know the values of the two b its that you are comparing. If those two are not equal, you are done. But if the two bits are equal, what d o you do? The answer is fairly simple: pass along the result from the less signiﬁcant bits. Thus our bit slice lo gic for bit Mneeds to be able to accept three possible answers from the bit slice logic for bit M−1 and must be able to pass one of three possible answers to the logic for bit M+1","{'page_number': 58, 'textbook_name': 'ECE-120-student-notes', 'text': 'If those two are not equal, you are done. But if the two bits are equal, what d o you do? The answer is fairly simple: pass along the result from the less signiﬁcant bits. Thus our bit slice lo gic for bit Mneeds to be able to accept three possible answers from the bit slice logic for bit M−1 and must be able to pass one of three possible answers to the logic for bit M+1. Since⌈log2(3)⌉= 2, we need two bits of input and two bits of output in addition to our input bits from numbers AandB. The diagram to the right shows an abstract model of our comparat or bit slice. The inputs from the next least signiﬁcant bit come in from the right. We include arrowheads because ﬁgures are usually drawn with inputs coming from the top or left and outputs going to the bottom o r right. Outside of the bit slice logic, we index these comparison bits usin g the bit number. The bit slice has CM−1 1andCM−1 0provided as inputs and produces CM 1andCM 0as outputs. Internally, we use C1andC0to denote these inputs, and Z1andZ0to denote the outputs. Similarly, theZ ZC C CA BA B C C C1 01 0 0M M 1 1 0comparator bit slice MM MM−1 M−1 bitsAMandBMfrom the numbers AandBare represented internally simply as AandB. The overloading ofmeaningshould not confuseyou, since the context(designing th e logicblockorthinking about the problem as a whole) should always be clear.'}"
"If those two are not equal, you are done. But if the two bits are equal, what d o you do? The answer is fairly simple: pass along the result from the less signiﬁcant bits. Thus our bit slice lo gic for bit Mneeds to be able to accept three possible answers from the bit slice logic for bit M−1 and must be able to pass one of three possible answers to the logic for bit M+1. Since⌈log2(3)⌉= 2, we need two bits of input and two bits of output in addition to our input bits from numbers AandB. The diagram to the right shows an abstract model of our comparat or bit slice. The inputs from the next least signiﬁcant bit come in from the right. We include arrowheads because ﬁgures are usually drawn with inputs coming from the top or left and outputs going to the bottom o r right. Outside of the bit slice logic, we index these comparison bits usin g the bit number. The bit slice has CM−1 1andCM−1 0provided as inputs and produces CM 1andCM 0as outputs. Internally, we use C1andC0to denote these inputs, and Z1andZ0to denote the outputs. Similarly, theZ ZC C CA BA B C C C1 01 0 0M M 1 1 0comparator bit slice MM MM−1 M−1 bitsAMandBMfrom the numbers AandBare represented internally simply as AandB. The overloading ofmeaningshould not confuseyou, since the context(designing th e logicblockorthinking about the problem as a whole) should always be clear.","{'page_number': 58, 'textbook_name': 'ECE-120-student-notes', 'text': 'If those two are not equal, you are done. But if the two bits are equal, what d o you do? The answer is fairly simple: pass along the result from the less signiﬁcant bits. Thus our bit slice lo gic for bit Mneeds to be able to accept three possible answers from the bit slice logic for bit M−1 and must be able to pass one of three possible answers to the logic for bit M+1. Since⌈log2(3)⌉= 2, we need two bits of input and two bits of output in addition to our input bits from numbers AandB. The diagram to the right shows an abstract model of our comparat or bit slice. The inputs from the next least signiﬁcant bit come in from the right. We include arrowheads because ﬁgures are usually drawn with inputs coming from the top or left and outputs going to the bottom o r right. Outside of the bit slice logic, we index these comparison bits usin g the bit number. The bit slice has CM−1 1andCM−1 0provided as inputs and produces CM 1andCM 0as outputs. Internally, we use C1andC0to denote these inputs, and Z1andZ0to denote the outputs. Similarly, theZ ZC C CA BA B C C C1 01 0 0M M 1 1 0comparator bit slice MM MM−1 M−1 bitsAMandBMfrom the numbers AandBare represented internally simply as AandB. The overloading ofmeaningshould not confuseyou, since the context(designing th e logicblockorthinking about the problem as a whole) should always be clear.'}"
"54  2.4.3 A Representation and the First Bit We need to select a representation for our three possible answers before we can design any logic. The representation chosen aﬀects the implementa tion, as we discuss later in these notes. For now, we simply choose the represe ntation to the right, which seems reasonable. Now we can design the logic for the ﬁrst bit (bit 0). In keeping with the bit slice philosophy, in practice we simply use another copy of the full bit slice d esign for bit 0 and attach the C1C0inputs to ground (to denote A=B). Here we tackle the simpler problem as a warm-up exercise.C1C0meaning 0 0 A=B 0 1 A < B 1 0 A > B 1 1 not used The truth table for bit 0 appears to the right (recall that we use Z1andZ0for the output names). Note that the bit 0 function has only two meanin gful inputs— there is no bit to the right of bit 0. If the two inputs AandBare the same, we output equality. Otherwise, we do a 1-bit comparison and use our re presentation mapping to select the outputs. These functions are fairly straight forwardto derive by inspection. They are: Z1=AB Z0=A BA B Z1Z0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 0 These forms should also be intuitive, given the representation that we chose: A > Bif and only if A= 1 andB= 0;A < Bif and only if A= 0 and B= 1. Implementation diagrams for our one-bit functions appear to the right. The diagram to the immediate right shows the implementation as we might initially draw it, and the diagram on the farrightshowsthe implementationZ1 Z0A BZ1 Z0A B converted to NAND/NOR gates for a more accurate estimate of co mplexity when implemented in CMOS. The exercise of designing the logic for bit 0 is also useful in the sense t hat the logic structure illustrated forms the core of the full design in that it identiﬁes the two cases th at matter: A < BandA > B. Now we are ready to design the full function. Let’s start by writing a full truth table, as shown on the left below","{'page_number': 59, 'textbook_name': 'ECE-120-student-notes', 'text': 'The exercise of designing the logic for bit 0 is also useful in the sense t hat the logic structure illustrated forms the core of the full design in that it identiﬁes the two cases th at matter: A < BandA > B. Now we are ready to design the full function. Let’s start by writing a full truth table, as shown on the left below. A B C 1C0Z1Z0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 x x 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 x x 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 x x 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 x xA B C 1C0Z1Z0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 x x 1 1 x xA B C 1C0Z1Z0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 other x x In the truth table, we marked the outputs as “don’t care” (x’s) wh eneverC1C0= 11. You might recall that we ran into problems with our ice cream dispenser control in Notes Se t 2.2. However, in that case we could not safely assume that a user did not push multiple buttons. Here, o ur bit slice logic only accepts inputs'}"
"The exercise of designing the logic for bit 0 is also useful in the sense t hat the logic structure illustrated forms the core of the full design in that it identiﬁes the two cases th at matter: A < BandA > B. Now we are ready to design the full function. Let’s start by writing a full truth table, as shown on the left below. A B C 1C0Z1Z0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 x x 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 x x 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 x x 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 x xA B C 1C0Z1Z0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 x x 1 1 x xA B C 1C0Z1Z0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 other x x In the truth table, we marked the outputs as “don’t care” (x’s) wh eneverC1C0= 11. You might recall that we ran into problems with our ice cream dispenser control in Notes Se t 2.2. However, in that case we could not safely assume that a user did not push multiple buttons. Here, o ur bit slice logic only accepts inputs","{'page_number': 59, 'textbook_name': 'ECE-120-student-notes', 'text': 'The exercise of designing the logic for bit 0 is also useful in the sense t hat the logic structure illustrated forms the core of the full design in that it identiﬁes the two cases th at matter: A < BandA > B. Now we are ready to design the full function. Let’s start by writing a full truth table, as shown on the left below. A B C 1C0Z1Z0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 x x 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 1 x x 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 x x 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 x xA B C 1C0Z1Z0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 x x 1 1 x xA B C 1C0Z1Z0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 other x x In the truth table, we marked the outputs as “don’t care” (x’s) wh eneverC1C0= 11. You might recall that we ran into problems with our ice cream dispenser control in Notes Se t 2.2. However, in that case we could not safely assume that a user did not push multiple buttons. Here, o ur bit slice logic only accepts inputs'}"
"2.4 Example: Bit-Sliced Comparison 55 from other copies of itself (or a ﬁxed value for bit 0), and—assuming that we design the logic correctly—our bit slice never generates the 11 combination. In other words, that input combination is impossible (rather than undesirable or unlikely), so the result produced on the output s is irrelevant. It is tempting to shorten the full truth table by replacing groups of rows. For example, if AB= 01, we know that A < B, so the less signiﬁcant bits (for which the result is represented by t heC1C0inputs) don’t matter. We could write one row with input pattern ABC1C0= 01xx and output pattern Z1Z0= 01. We might also collapse our “don’t care” output patterns: whenever th e input matches ABC1C0=xx11, we don’t care about the output, so Z1Z0=xx. But these two rows overlap in the input space! In other words , some input patterns, such as ABC1C0= 0111, match both of our suggested new rows. Which output shou ld take precedence? The answer is that a reader should not have to guess .Do not use overlapping rows to shorten a truth table. In fact, the ﬁrst of the suggested new rows is not valid: we don’t ne ed to produce output 01 if we seeC1C0= 11. Two valid short forms of this truth table appear to the right of the full table. If you have an “other” entry, as shown in the rightmost table, this entry shou ld always appear as the last row. Normal rows, including rows representing multiple input patterns, are not r equired to be in any particular order. Use whatever order makes the table easiest to read for its purpos e (usually by treating the input pattern as a binary number and ordering rows in increasing numeric order). In order to translate our design into algebra, we transcribe the truthtableintoaK-mapforeachoutput variable,asshowntothe right. You may want to perform this exercise yourself and check that you obtain the same solution. Implicants for each output are marked in the K-maps, giving the following equations: Z1=AB+A C1+B C1 Z0=A B+A C0+B C0C1C0Z1 00 01 11 10 0 0 1100 01 11 10AB 1 10 0 00 0 xxxx 1C1C0Z0 00 01 11 10 0 00 01 11 10AB 0 xxxx 1 01 011 01 00 An implementation based on our equations appears to the right. The ﬁgure makes it easy to see the symmetry between the inputs, which arises from the representation that we’ve chosen. Since the design only uses two-level logic (not counting the inverters on the AandBinputs, since inverters can be viewed as 1-input NAND or NOR gates), convertingto NAND/NOR simply requiresreplac- ing all of the AND and OR gates with NAND gates. Let’s discuss the design’s eﬃciency roughly in terms of area and speed","{'page_number': 60, 'textbook_name': 'ECE-120-student-notes', 'text': 'Since the design only uses two-level logic (not counting the inverters on the AandBinputs, since inverters can be viewed as 1-input NAND or NOR gates), convertingto NAND/NOR simply requiresreplac- ing all of the AND and OR gates with NAND gates. Let’s discuss the design’s eﬃciency roughly in terms of area and speed. As an estimate of area, we can count gates,rememberingthatweneedtwotransistorsperinput onagate. Ourinitialdesignusestwoinverters,six2-input gates, and two 3-input gates. For speed, we make rough estimates in terms of the amount of time it takes for a CMOS gate to change its output once its input has changed. This amount of time is called a gate delay . We can thus estimate our design’sZ1 Z0C0C1a comparator bit slice (first attempt) A B speed by simply counting the maximum number of gates on any path fr om input to output. For this mea- surement, using a NAND/NOR representation of the design is import ant to getting the right answer, but, as we have discussed, the diagram above is equivalent on a gate-for-g ate basis. Here we have three gate delays from the AandBinputs to the outputs (through the inverters). But when we conn ect multiple copies of our bit slice logic together to form a comparator, as shown on the ne xt page, the delay from the AandB inputs to the outputs is not as important as the delay from the C1andC0inputs to the outputs. The latter delay adds to the total delay of our comparatoron a per-bit-slice b asis. Looking again at the diagram, notice that we have only two gate delays from the C1andC0inputs to the outputs. The total delay for an N-bit comparator based on this implementation is thus three gate delays f or bit 0 and two more gate delays per additional bit, for a total of 2 N+1 gate delays.'}"
"Since the design only uses two-level logic (not counting the inverters on the AandBinputs, since inverters can be viewed as 1-input NAND or NOR gates), convertingto NAND/NOR simply requiresreplac- ing all of the AND and OR gates with NAND gates. Let’s discuss the design’s eﬃciency roughly in terms of area and speed. As an estimate of area, we can count gates,rememberingthatweneedtwotransistorsperinput onagate. Ourinitialdesignusestwoinverters,six2-input gates, and two 3-input gates. For speed, we make rough estimates in terms of the amount of time it takes for a CMOS gate to change its output once its input has changed. This amount of time is called a gate delay . We can thus estimate our design’sZ1 Z0C0C1a comparator bit slice (first attempt) A B speed by simply counting the maximum number of gates on any path fr om input to output. For this mea- surement, using a NAND/NOR representation of the design is import ant to getting the right answer, but, as we have discussed, the diagram above is equivalent on a gate-for-g ate basis. Here we have three gate delays from the AandBinputs to the outputs (through the inverters). But when we conn ect multiple copies of our bit slice logic together to form a comparator, as shown on the ne xt page, the delay from the AandB inputs to the outputs is not as important as the delay from the C1andC0inputs to the outputs. The latter delay adds to the total delay of our comparatoron a per-bit-slice b asis. Looking again at the diagram, notice that we have only two gate delays from the C1andC0inputs to the outputs. The total delay for an N-bit comparator based on this implementation is thus three gate delays f or bit 0 and two more gate delays per additional bit, for a total of 2 N+1 gate delays.","{'page_number': 60, 'textbook_name': 'ECE-120-student-notes', 'text': 'Since the design only uses two-level logic (not counting the inverters on the AandBinputs, since inverters can be viewed as 1-input NAND or NOR gates), convertingto NAND/NOR simply requiresreplac- ing all of the AND and OR gates with NAND gates. Let’s discuss the design’s eﬃciency roughly in terms of area and speed. As an estimate of area, we can count gates,rememberingthatweneedtwotransistorsperinput onagate. Ourinitialdesignusestwoinverters,six2-input gates, and two 3-input gates. For speed, we make rough estimates in terms of the amount of time it takes for a CMOS gate to change its output once its input has changed. This amount of time is called a gate delay . We can thus estimate our design’sZ1 Z0C0C1a comparator bit slice (first attempt) A B speed by simply counting the maximum number of gates on any path fr om input to output. For this mea- surement, using a NAND/NOR representation of the design is import ant to getting the right answer, but, as we have discussed, the diagram above is equivalent on a gate-for-g ate basis. Here we have three gate delays from the AandBinputs to the outputs (through the inverters). But when we conn ect multiple copies of our bit slice logic together to form a comparator, as shown on the ne xt page, the delay from the AandB inputs to the outputs is not as important as the delay from the C1andC0inputs to the outputs. The latter delay adds to the total delay of our comparatoron a per-bit-slice b asis. Looking again at the diagram, notice that we have only two gate delays from the C1andC0inputs to the outputs. The total delay for an N-bit comparator based on this implementation is thus three gate delays f or bit 0 and two more gate delays per additional bit, for a total of 2 N+1 gate delays.'}"
"56  00 Z ZC CZ ZC CA BA B C CZ ZC CZ ZC CA BA B C CA BA B C C. . .A BA B C C1 01 01 01 01 0comparator bit1 1 slice 111 1 01 01 01 01 0comparator bit0 0 slice 00 01 0comparator bit1 0comparator bitN−1 N−1N−2 N−2 slice N−1 slice N−2N−1 N−1 N−2 N−2an N−bit unsigned comparator composed of bit slices 2.4.4 Optimizing Our Design We have a fairly good design at this point—good enough for a homewor k or exam problem in this class, certainly—but let’s consider how we might further optimize it. Today, optimization of logic at this level is done mostly by computer-aided design (CAD) tools, but we want yo u to be aware of the sources of optimization potential and the tradeoﬀs involved. And, if the topic in terests you, someone has to continue to improve CAD software! The ﬁrst step is to manipulate our algebra to expose common terms t hat occur due to the design’s symmetry. Starting with our original equation for Z1, we have Z1=AB+A C1+B C1 =AB+/parenleftbig A+B/parenrightbig C1 =AB+A B C 1 Similarly, Z0=A B+AB C0 Notice that the second term in each equation now includes the comple ment of ﬁrst term from the other equation. For example, the Z1equation includes the complement of the ABproduct that we need to compute Z0. We may be able to improve our design by combining these computation s. An implementation based on our new algebraic formulation appears to the right. In this form, we seem to have kept the same num- ber of gates, although we have re- placed the 3-input gates with in- verters. However, the middle in- verters disappear when we convert to NAND/NOR form, as shown be- low to the right. Our new de- sign requires only two inverters and six 2-input gates, a substantial re- duction relative to the original im- plementation. Is there a disadvantage? Yes, but only a slight one. Notice that the path from the AandBinputs to the outputs is nowfour gates(maxi- mum) insteadofthree. Yet the path fromC1andC0to the outputs is still only two gates. Thus, overall, we have merely increased our N-bit comparator’sdelayfrom2 N+1gate delays to 2 N+2 gate delays.C1 Z1 C0Z0A Ba comparator bit slice (optimized) C1 Z1 C0Z0A Ba comparator bit slice (optimized, NAND/NOR)","{'page_number': 61, 'textbook_name': 'ECE-120-student-notes', 'text': '56  00 Z ZC CZ ZC CA BA B C CZ ZC CZ ZC CA BA B C CA BA B C C. . .A BA B C C1 01 01 01 01 0comparator bit1 1 slice 111 1 01 01 01 01 0comparator bit0 0 slice 00 01 0comparator bit1 0comparator bitN−1 N−1N−2 N−2 slice N−1 slice N−2N−1 N−1 N−2 N−2an N−bit unsigned comparator composed of bit slices 2.4.4 Optimizing Our Design We have a fairly good design at this point—good enough for a homewor k or exam problem in this class, certainly—but let’s consider how we might further optimize it. Today, optimization of logic at this level is done mostly by computer-aided design (CAD) tools, but we want yo u to be aware of the sources of optimization potential and the tradeoﬀs involved. And, if the topic in terests you, someone has to continue to improve CAD software! The ﬁrst step is to manipulate our algebra to expose common terms t hat occur due to the design’s symmetry. Starting with our original equation for Z1, we have Z1=AB+A C1+B C1 =AB+/parenleftbig A+B/parenrightbig C1 =AB+A B C 1 Similarly, Z0=A B+AB C0 Notice that the second term in each equation now includes the comple ment of ﬁrst term from the other equation. For example, the Z1equation includes the complement of the ABproduct that we need to compute Z0. We may be able to improve our design by combining these computation s. An implementation based on our new algebraic formulation appears to the right. In this form, we seem to have kept the same num- ber of gates, although we have re- placed the 3-input gates with in- verters. However, the middle in- verters disappear when we convert to NAND/NOR form, as shown be- low to the right. Our new de- sign requires only two inverters and six 2-input gates, a substantial re- duction relative to the original im- plementation. Is there a disadvantage? Yes, but only a slight one. Notice that the path from the AandBinputs to the outputs is nowfour gates(maxi- mum) insteadofthree. Yet the path fromC1andC0to the outputs is still only two gates. Thus, overall, we have merely increased our N-bit comparator’sdelayfrom2 N+1gate delays to 2 N+2 gate delays.C1 Z1 C0Z0A Ba comparator bit slice (optimized) C1 Z1 C0Z0A Ba comparator bit slice (optimized, NAND/NOR)'}"
"2.4 Example: Bit-Sliced Comparison 57 2.4.5 Extending to 2’s Complement What about comparing 2’s complement numbers? Can we make use of t he unsigned comparator that we just designed? Let’s start by thinking about the sign of the numbers AandB. Recall that 2’s complement records a number’s sign in the most signiﬁcant bit. For example, in the 8-bit numb ers shown in the ﬁrst diagram in this set of notes, the sign bits are A7andB7. Let’s denote these sign bits in the general case by AsandBs. Negative numbers have a sign bit equal to 1, and non-negative numb ers have a sign bit equal to 0. The table below outlines an initial evaluation of the four possible combinations of sign bits. AsBsinterpretation solution 0 0 A≥0 ANDB≥0use unsigned comparator on remaining bits 0 1 A≥0 ANDB <0A > B 1 0 A <0 ANDB≥0A < B 1 1 A <0 ANDB <0unknown Whatshouldwedowhenbothnumbersarenegative? Needwe design a completely separate logic circuit? Can we somehow convert a negative value to a positive one? The answer is in fact much simpler. Recall that 2’s complement is deﬁned based on modular arithmetic. Given an N-bit negative number A, the representation for the bits A[N−2 : 0] is the same as the binary (unsigned) representation of A+2N−1. An example appears to the right.A3A2A1A0 B0B1B2B3 1A (−4)1 0 0 B (−2) 1 1 1 0 4 = −4 + 8 6 = −2 + 8 Let’s deﬁne Ar=A+2N−1as the value of the remaining bits for AandBrsimilarly for B. What happens if we just go ahead and compare ArandBrusing an ( N−1)-bit unsigned comparator? If we ﬁnd that Ar< Brwe know that Ar−2N−1< Br−2N−1as well, but that means A < B! We can do the same with either of the other possible results. In other words, simply compar ingArwithBrgives the correct answer for two negative numbers as well. All we need to design is a logic block for the sign bits. At this point, we might write out a K-map, but instead let’s rewrite our high-level table with the new informa- tion, as shown to the right. Looking at the table, notice the similarity to the high- level design for a single bit of an unsigned value. TheAsBssolution 0 0 pass result from less signiﬁcant bits 0 1 A > B 1 0 A < B 1 1 pass result from less signiﬁcant bits only diﬀerence is that the two A/n⌉}ationslash=Bcases are reversed. If we swap AsandBs, the function is identical. We can simply use another bit slice but swap these two inputs. Impleme ntation of an N-bit 2’s complement comparator based on our bit slice comparator is shown below","{'page_number': 62, 'textbook_name': 'ECE-120-student-notes', 'text': 'If we swap AsandBs, the function is identical. We can simply use another bit slice but swap these two inputs. Impleme ntation of an N-bit 2’s complement comparator based on our bit slice comparator is shown below. The blu e circle highlights the only change from the N-bit unsigned comparator, which is to swap the two inputs on the sign bit. 00 Z ZC CZ ZC CA BA B C CZ ZC CA Z ZC CA BA B C CA BB C C. . .A BA B C C1 01 01 01 01 0comparator bit1 1 slice 111 1 01 0N−1 1 01 01 0comparator bit0 0 slice 00 01 0comparator bit slice N−1N−1 N−1 N−11 0comparator bitN−2 N−2 slice N−2N−2 N−2an N−bit 2’s complement comparator composed of bit slices'}"
"If we swap AsandBs, the function is identical. We can simply use another bit slice but swap these two inputs. Impleme ntation of an N-bit 2’s complement comparator based on our bit slice comparator is shown below. The blu e circle highlights the only change from the N-bit unsigned comparator, which is to swap the two inputs on the sign bit. 00 Z ZC CZ ZC CA BA B C CZ ZC CA Z ZC CA BA B C CA BB C C. . .A BA B C C1 01 01 01 01 0comparator bit1 1 slice 111 1 01 0N−1 1 01 01 0comparator bit0 0 slice 00 01 0comparator bit slice N−1N−1 N−1 N−11 0comparator bitN−2 N−2 slice N−2N−2 N−2an N−bit 2’s complement comparator composed of bit slices","{'page_number': 62, 'textbook_name': 'ECE-120-student-notes', 'text': 'If we swap AsandBs, the function is identical. We can simply use another bit slice but swap these two inputs. Impleme ntation of an N-bit 2’s complement comparator based on our bit slice comparator is shown below. The blu e circle highlights the only change from the N-bit unsigned comparator, which is to swap the two inputs on the sign bit. 00 Z ZC CZ ZC CA BA B C CZ ZC CA Z ZC CA BA B C CA BB C C. . .A BA B C C1 01 01 01 01 0comparator bit1 1 slice 111 1 01 0N−1 1 01 01 0comparator bit0 0 slice 00 01 0comparator bit slice N−1N−1 N−1 N−11 0comparator bitN−2 N−2 slice N−2N−2 N−2an N−bit 2’s complement comparator composed of bit slices'}"
"58  2.4.6 Further Optimization Let’s return to the topic of optimization. To what extent did the representation of the three outcomes aﬀect our ability to develop a good bit slice design? Although selecting a good representation can be quite important, for this particular problem most representatio ns lead to similar implementations. Some representations, however, have interesting properties. C onsiderC1C0original alternate 0 0 A=BA=B 0 1 A < B A > B 1 0 A > B not used 1 1 not used A < B the alternate representation on the right, for example (a copy of the original representation is included for comparison). Notice that in the alternate representation, C0= 1 whenever A/n⌉}ationslash=B. Once we have found the numbers to be diﬀerent in some bit, the end result can never be e quality, so perhaps with the right representation—the new one, for example—we might be able to cut d elay in half? An implementation based on the alternate representation appears in thediagramtotheright. Asyoucan see, in terms of gate count, this de- sign replaces one 2-input gate with an inverter and a second 2-input gate with a 3-input gate. The path lengthsarethesame,requiring2 N+ 2 gate delays for an N-bit compara- tor. Overall, it is about the same as our original design.C0Z0Z1C1 A Ba comparator bit slice (alternate representation) Why didn’t it work? Should we consider still other representations? I n fact, none of the possible represen- tations that we might choose for a bit slice can cut the delay down to o ne gate delay per bit. The problem is fundamental, and is related to the nature of CMOS. For a single bit s lice, we deﬁne the incoming and outgoing representations to be the same. We also need to have at le ast one gate in the path to combine theC1andC0inputs with information from the bit slice’s AandBinputs. But all CMOS gates invert the sense of their inputs. Our choices are limited to NAND and NOR. Thus w e need at least two gates in the path to maintain the same representation. One simple answer is to use diﬀerent representations for odd and ev en bits. Instead, we optimize a logic circuit for comparing two bits. We base our design on the alternate r epresentation. The implementation is shown below. The left shows an implementation based on the algebra, and the right shows a NAND/NOR implementation. Estimating by gate count and number of inputs, the two-bit design doesn’t save much over two single bit slices in terms of area. In terms of delay, however, we h ave only two gate delays from C1 andC0to either output. The longest path from the AandBinputs to the outputs is ﬁve gate delays. Thus, for anN-bit comparator built with this design, the total delay is only N+3 gate delays. But Nhas to be even","{'page_number': 63, 'textbook_name': 'ECE-120-student-notes', 'text': 'In terms of delay, however, we h ave only two gate delays from C1 andC0to either output. The longest path from the AandBinputs to the outputs is ﬁve gate delays. Thus, for anN-bit comparator built with this design, the total delay is only N+3 gate delays. But Nhas to be even. 1A 1B 0A 0B C0C1 Z1 Z0a comparator 2−bit slice (alternate representation) 1A 1B 0A 0B C0C1 Z1 Z0a comparator 2−bit slice (alternate representation, NAND/NOR) As you can imagine, continuing to scale up the size of our logic block give s us better performance at the expense of a more complex design. Using the alternate represe ntation may help you to see how one can generalize the approach to larger groups of bits—for example, you may have noticed the two bitwise comparator blocks on the left of the implementations above.'}"
"In terms of delay, however, we h ave only two gate delays from C1 andC0to either output. The longest path from the AandBinputs to the outputs is ﬁve gate delays. Thus, for anN-bit comparator built with this design, the total delay is only N+3 gate delays. But Nhas to be even. 1A 1B 0A 0B C0C1 Z1 Z0a comparator 2−bit slice (alternate representation) 1A 1B 0A 0B C0C1 Z1 Z0a comparator 2−bit slice (alternate representation, NAND/NOR) As you can imagine, continuing to scale up the size of our logic block give s us better performance at the expense of a more complex design. Using the alternate represe ntation may help you to see how one can generalize the approach to larger groups of bits—for example, you may have noticed the two bitwise comparator blocks on the left of the implementations above.","{'page_number': 63, 'textbook_name': 'ECE-120-student-notes', 'text': 'In terms of delay, however, we h ave only two gate delays from C1 andC0to either output. The longest path from the AandBinputs to the outputs is ﬁve gate delays. Thus, for anN-bit comparator built with this design, the total delay is only N+3 gate delays. But Nhas to be even. 1A 1B 0A 0B C0C1 Z1 Z0a comparator 2−bit slice (alternate representation) 1A 1B 0A 0B C0C1 Z1 Z0a comparator 2−bit slice (alternate representation, NAND/NOR) As you can imagine, continuing to scale up the size of our logic block give s us better performance at the expense of a more complex design. Using the alternate represe ntation may help you to see how one can generalize the approach to larger groups of bits—for example, you may have noticed the two bitwise comparator blocks on the left of the implementations above.'}"
"2.5 Using Abstraction to Simplify Problems 59 ECE120: Introduction to Computer Engineering Notes Set 2.5 Using Abstraction to Simplify Problems In this set of notes, we illustrate the use of abstraction to simplify p roblems, then introduce a component called a multiplexer that allows selection among multiple inputs. We begin b y showing how two speciﬁc examples—integer subtraction and identiﬁcation of letters in ASCII —can be implemented using logic func- tions that we have already developed. We also introduce a conceptu al technique for breaking functions into smaller pieces, which allows us to solve several simpler problems and th en to compose a full solution from these partial solutions. Together with the idea of bit-sliced designs that we introduced earlie r, these techniques help to simplify the process of designing logic that operates correctly. The techn iques can, of course, lead to less eﬃcient designs, but correctness is always more important than performance . The potential loss of eﬃciency is often acceptable for three reasons. First, as we mentioned earlier, com puter-aided design tools for optimizing logic functions are fairly eﬀective, and in many cases produce better re sults than human engineers (except in the rare cases in which the human eﬀort required to beat the tools is wor thwhile). Second, as you know from the design of the 2’s complement representation, we may be able to reus e speciﬁc pieces of hardware if we think carefully about how we deﬁne our problems and representations. F inally, many tasks today are executed in software, which is designed to leverage the fairly general logic availa ble via an instruction set architecture. A programmer cannot easily add new logic to a user’s processor. As a result, the hardware used to execute a function typically is not optimized for that function. The approach es shown in this set of notes illustrate how abstraction can be used to design logic. 2.5.1 Subtraction Our discussion of arithmetic implementation has focused so far on ad dition. What about other operations, such as subtraction, multiplication, and division? The latter two requ ire more work, and we will not discuss them in detail until later in our class (if at all). Subtraction, however, can be performed almost trivially using logic t hat we have already designed. Let’s say that we want to calculate the diﬀerence Dbetween two N-bit numbers AandB. In particular, we want to ﬁnd D=A−B. For now, think of A,B, andDas 2’s complement values. Recall how we deﬁned the 2’s complement representation: the N-bit pattern that we use to represent −Bis the same as the base 2 bit pattern for (2N−B), so we can use an adder if we ﬁrst calculate the bit pattern for −B, then add the resulting pattern to A. As you know, our N-bit adder always produces a result that is correct modulo 2N, so the result of such an operation, D= 2N+A−B, is correct so long as the subtraction does not overﬂow","{'page_number': 64, 'textbook_name': 'ECE-120-student-notes', 'text': 'As you know, our N-bit adder always produces a result that is correct modulo 2N, so the result of such an operation, D= 2N+A−B, is correct so long as the subtraction does not overﬂow. How can we calculate 2N−B? The same way that we do by hand! Calculate the 1’s complement, (2N−1)−B, then add 1. The diagram to the right shows how we can use the N-bit adder that we designed in Notes Set 2.3 to build an N-bit subtracter. New elements appear in blue in the ﬁgure—the rest of the logic is just an adder. The box labeled “1’s comp.” calculate s the 1’s complement of the value B, which together with the carry in value of 1 correspond to calculating −B. What’s in the “1’s comp.” box? One inverter per bit in B. That’s all we need to calculate the 1’s complement. You might now ask: does this approach also work for unsigned numbers? The a nswer is yes, absolutely. However, the overﬂow conditions for both 2’s com plement and unsigned subtraction are diﬀerent than the overﬂow condition for either type of addition. What does the carry out of our adder signify, for exam ple? The answer may not be immediately obvious.C CN1’s comp. N NSB A N−bit adderNA B 1 What does the carry out mean? D=A−Bin out Let’s start with the overﬂow condition for unsigned subtraction. O verﬂow means that we cannot represent the result. With an N-bit unsigned number, we have A−B/n⌉}ationslash∈[0,2N−1]. Obviously, the diﬀerence cannot be larger than the upper limit, since Ais representable and we are subtracting a non-negative (unsigned ) value. We can thus assume that overﬂow occurs only when A−B <0. In other words, when A < B.'}"
"As you know, our N-bit adder always produces a result that is correct modulo 2N, so the result of such an operation, D= 2N+A−B, is correct so long as the subtraction does not overﬂow. How can we calculate 2N−B? The same way that we do by hand! Calculate the 1’s complement, (2N−1)−B, then add 1. The diagram to the right shows how we can use the N-bit adder that we designed in Notes Set 2.3 to build an N-bit subtracter. New elements appear in blue in the ﬁgure—the rest of the logic is just an adder. The box labeled “1’s comp.” calculate s the 1’s complement of the value B, which together with the carry in value of 1 correspond to calculating −B. What’s in the “1’s comp.” box? One inverter per bit in B. That’s all we need to calculate the 1’s complement. You might now ask: does this approach also work for unsigned numbers? The a nswer is yes, absolutely. However, the overﬂow conditions for both 2’s com plement and unsigned subtraction are diﬀerent than the overﬂow condition for either type of addition. What does the carry out of our adder signify, for exam ple? The answer may not be immediately obvious.C CN1’s comp. N NSB A N−bit adderNA B 1 What does the carry out mean? D=A−Bin out Let’s start with the overﬂow condition for unsigned subtraction. O verﬂow means that we cannot represent the result. With an N-bit unsigned number, we have A−B/n⌉}ationslash∈[0,2N−1]. Obviously, the diﬀerence cannot be larger than the upper limit, since Ais representable and we are subtracting a non-negative (unsigned ) value. We can thus assume that overﬂow occurs only when A−B <0. In other words, when A < B.","{'page_number': 64, 'textbook_name': 'ECE-120-student-notes', 'text': 'As you know, our N-bit adder always produces a result that is correct modulo 2N, so the result of such an operation, D= 2N+A−B, is correct so long as the subtraction does not overﬂow. How can we calculate 2N−B? The same way that we do by hand! Calculate the 1’s complement, (2N−1)−B, then add 1. The diagram to the right shows how we can use the N-bit adder that we designed in Notes Set 2.3 to build an N-bit subtracter. New elements appear in blue in the ﬁgure—the rest of the logic is just an adder. The box labeled “1’s comp.” calculate s the 1’s complement of the value B, which together with the carry in value of 1 correspond to calculating −B. What’s in the “1’s comp.” box? One inverter per bit in B. That’s all we need to calculate the 1’s complement. You might now ask: does this approach also work for unsigned numbers? The a nswer is yes, absolutely. However, the overﬂow conditions for both 2’s com plement and unsigned subtraction are diﬀerent than the overﬂow condition for either type of addition. What does the carry out of our adder signify, for exam ple? The answer may not be immediately obvious.C CN1’s comp. N NSB A N−bit adderNA B 1 What does the carry out mean? D=A−Bin out Let’s start with the overﬂow condition for unsigned subtraction. O verﬂow means that we cannot represent the result. With an N-bit unsigned number, we have A−B/n⌉}ationslash∈[0,2N−1]. Obviously, the diﬀerence cannot be larger than the upper limit, since Ais representable and we are subtracting a non-negative (unsigned ) value. We can thus assume that overﬂow occurs only when A−B <0. In other words, when A < B.'}"
"60  To calculate the unsigned subtraction overﬂow condition in terms of the bits, recall that our adder is cal- culating 2N+A−B. The carry out represents the 2Nterm. When A≥B, the result of the adder is at least 2N, and we see a carry out, Cout= 1. However, when A < B, the result of the adder is less than 2N, and we see no carry out, Cout= 0.Overﬂow for unsigned subtraction is thus inverted from over ﬂow for unsigned addition : a carry out of 0 indicates an overﬂow for subtraction. What about overﬂow for 2’s complement subtraction? We can use ar guments similar to those that we used to reason about overﬂow of 2’s complement addition to prove that s ubtraction of one negative number from a second negative number can never overﬂow. Nor can subtractio n of a non-negative number from a second non-negative number overﬂow. IfA≥0 andB <0, the subtraction overﬂows iﬀ A−B≥2N−1. Again using similar arguments as before, we can prove that the diﬀerence Dappears to be negative in the case of overﬂow, so the product AN−1BN−1DN−1evaluates to 1 when this type of overﬂow occurs (these variables r epresent the most signiﬁcant bits of the two operands and the diﬀerence; in the case o f 2’s complement, they are also the sign bits). Similarly, if A <0 andB≥0, we have overﬂow when A−B <−2N−1. Here we can prove that D≥0 on overﬂow, so AN−1BN−1DN−1evaluates to 1. Our overﬂow condition for N-bit 2’s complement subtraction is thus given by the following: AN−1BN−1DN−1+AN−1BN−1DN−1 If we calculate all four overﬂow conditions—unsigned and 2’s complem ent, addition and subtraction—and provide some way to choose whether or not to complement Band to control the Cininput, we can use the same hardware for addition and subtraction of either type. 2.5.2 Checking ASCII for Upper-case Letters Let’s now consider how we can check whether or not an ASCII chara cter is an upper-case letter. Let’s call the 7-bit letter C=C6C5C4C3C2C1C0and the function that we want to calculate U(C). The function U should equal 1 whenever Crepresents an upper-case letter, and should equal 0 whenever Cdoes not. In ASCII, the 7-bit patterns from 0x41 through 0x5A correspon d to the letters A through Z in alphabetic order. Perhaps you want to draw a 7-input K-map? Get a few large s heets of paper! Instead, imagine that we’ve written the full 128-rowtruth table. Let’s break the truth t able into pieces","{'page_number': 65, 'textbook_name': 'ECE-120-student-notes', 'text': 'In ASCII, the 7-bit patterns from 0x41 through 0x5A correspon d to the letters A through Z in alphabetic order. Perhaps you want to draw a 7-input K-map? Get a few large s heets of paper! Instead, imagine that we’ve written the full 128-rowtruth table. Let’s break the truth t able into pieces. Each piece will correspond to one speciﬁc pattern of the three high bits C6C5C4, and each piece will have 16 entries for the four low bitsC3C2C1C0. The truth tables for high bits 000, 001, 010, 011, 110, and 111 ar e easy: the function is exactly 0. The other two truth tables appear on the left below. We’v e called the two functions T4andT5, where the subscripts correspond to the binary value of the three high bits of C. C3C2C1C0T4T5 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0T4 C3 C100 01 11 10 00 1 1 1 011 1 1 1 1 111 1 1 101 1 1 10C2 C0 T5 C3 C100 01 11 10 00 1 1 011 1 1 111 1 101 1 1C2 C01 0 0 0 00T4=C3+C2+C1+C0 T5=C3+C2C1+C2C0'}"
"In ASCII, the 7-bit patterns from 0x41 through 0x5A correspon d to the letters A through Z in alphabetic order. Perhaps you want to draw a 7-input K-map? Get a few large s heets of paper! Instead, imagine that we’ve written the full 128-rowtruth table. Let’s break the truth t able into pieces. Each piece will correspond to one speciﬁc pattern of the three high bits C6C5C4, and each piece will have 16 entries for the four low bitsC3C2C1C0. The truth tables for high bits 000, 001, 010, 011, 110, and 111 ar e easy: the function is exactly 0. The other two truth tables appear on the left below. We’v e called the two functions T4andT5, where the subscripts correspond to the binary value of the three high bits of C. C3C2C1C0T4T5 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0T4 C3 C100 01 11 10 00 1 1 1 011 1 1 1 1 111 1 1 101 1 1 10C2 C0 T5 C3 C100 01 11 10 00 1 1 011 1 1 111 1 101 1 1C2 C01 0 0 0 00T4=C3+C2+C1+C0 T5=C3+C2C1+C2C0","{'page_number': 65, 'textbook_name': 'ECE-120-student-notes', 'text': 'In ASCII, the 7-bit patterns from 0x41 through 0x5A correspon d to the letters A through Z in alphabetic order. Perhaps you want to draw a 7-input K-map? Get a few large s heets of paper! Instead, imagine that we’ve written the full 128-rowtruth table. Let’s break the truth t able into pieces. Each piece will correspond to one speciﬁc pattern of the three high bits C6C5C4, and each piece will have 16 entries for the four low bitsC3C2C1C0. The truth tables for high bits 000, 001, 010, 011, 110, and 111 ar e easy: the function is exactly 0. The other two truth tables appear on the left below. We’v e called the two functions T4andT5, where the subscripts correspond to the binary value of the three high bits of C. C3C2C1C0T4T5 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0T4 C3 C100 01 11 10 00 1 1 1 011 1 1 1 1 111 1 1 101 1 1 10C2 C0 T5 C3 C100 01 11 10 00 1 1 011 1 1 111 1 101 1 1C2 C01 0 0 0 00T4=C3+C2+C1+C0 T5=C3+C2C1+C2C0'}"
"2.5 Using Abstraction to Simplify Problems 61 As shown to the right of the truth tables, we can then draw simpler K -maps for T4andT5, and can solve the K-maps to ﬁnd equations for each, as shown to the right (chec k that you get the same answers). How do we merge these results to form our ﬁnal expression for U? We AND each of the term functions ( T4 andT5) with the appropriate minterm for the high bits of C, then OR the results together, as shown here: U=C6C5C4T4+C6C5C4T5 =C6C5C4(C3+C2+C1+C0)+C6C5C4(C3+C2C1+C2C0) Rather than trying to optimize by hand, we can at this point let the CA D tools take over, conﬁdent that we have the right function to identify an upper-case ASCII letter. Breaking the truth table into pieces and using simple logic to reconnec t the pieces is one way to make use of abstraction when solving complex logic problems. In fact, recruiter s for some companies often ask questions that involve using speciﬁc logic elements as building blocks to implement o ther functions. Knowing that you can implement a truth table one piece at a time will help you to solve this t ype of problem. Let’s think about other ways to tackle the problem of calculating U. In Notes Sets 2.3 and 2.4, we developed adders and comparators. Can we make use of these components a s building blocks to check whether C represents an upper-case letter? Yes, of course we can: by com paringCwith the ends of the range of upper-case letters, we can check whether or not Cfalls in that range. The idea is illustrated on the left below using two 7-bit comparators co nstructed as discussed in Notes Set 2.4. The comparators are the black parts of the drawing, while t he blue parts represent our extensions to calculate U. Each comparator is given the value Cas one input. The second value to the comparators is either the letter A (0x41) or the letter Z (0x5A). The meaning of t he 2-bit input to and result of each comparator is given in the table on the right below. The inputs on the r ight of each comparator are set to 0 to ensure that equality is produced if Cmatches the second input ( B). One output from each comparator is then routed to a NOR gate to calculate U. Let’s consider how this combination works. The left comparator compares Cwith the letter A (0x41). If C≥0x41, the comparator produces Z0= 0. In this case, we may have a letter. On the other hand, if C <0x41, the comparator produces Z0= 1, and the NOR gate outputs U= 0, since we do not have a letter in this case. The right comparator c ompares Cwith the letter Z (0x5A). If C≤0x5A, the comparator produces Z1= 0. In this case, we may have a letter","{'page_number': 66, 'textbook_name': 'ECE-120-student-notes', 'text': 'In this case, we may have a letter. On the other hand, if C <0x41, the comparator produces Z0= 1, and the NOR gate outputs U= 0, since we do not have a letter in this case. The right comparator c ompares Cwith the letter Z (0x5A). If C≤0x5A, the comparator produces Z1= 0. In this case, we may have a letter. On the other hand, if C >0x5A, the comparator produces Z1= 1, and the NOR gate outputs U= 0, since we do not have a letter in this case. Only when 0x41 ≤C≤0x5A does U= 1, as desired. B Z ZC C0 07 70x5A discardC A B Z ZC C0 0discard7 70x41 A 1 01 0comparator7−bit1 01 0comparator7−bit UZ1Z0meaning 0 0 A=B 0 1 A < B 1 0 A > B 1 1 not used'}"
"In this case, we may have a letter. On the other hand, if C <0x41, the comparator produces Z0= 1, and the NOR gate outputs U= 0, since we do not have a letter in this case. The right comparator c ompares Cwith the letter Z (0x5A). If C≤0x5A, the comparator produces Z1= 0. In this case, we may have a letter. On the other hand, if C >0x5A, the comparator produces Z1= 1, and the NOR gate outputs U= 0, since we do not have a letter in this case. Only when 0x41 ≤C≤0x5A does U= 1, as desired. B Z ZC C0 07 70x5A discardC A B Z ZC C0 0discard7 70x41 A 1 01 0comparator7−bit1 01 0comparator7−bit UZ1Z0meaning 0 0 A=B 0 1 A < B 1 0 A > B 1 1 not used","{'page_number': 66, 'textbook_name': 'ECE-120-student-notes', 'text': 'In this case, we may have a letter. On the other hand, if C <0x41, the comparator produces Z0= 1, and the NOR gate outputs U= 0, since we do not have a letter in this case. The right comparator c ompares Cwith the letter Z (0x5A). If C≤0x5A, the comparator produces Z1= 0. In this case, we may have a letter. On the other hand, if C >0x5A, the comparator produces Z1= 1, and the NOR gate outputs U= 0, since we do not have a letter in this case. Only when 0x41 ≤C≤0x5A does U= 1, as desired. B Z ZC C0 07 70x5A discardC A B Z ZC C0 0discard7 70x41 A 1 01 0comparator7−bit1 01 0comparator7−bit UZ1Z0meaning 0 0 A=B 0 1 A < B 1 0 A > B 1 1 not used'}"
"62  What if we have only 8-bit adders available for our use, such as those developed in Notes Set 2.3? Can we still cal- culateU? Yes. The diagram shown to the right illustrates the approach, again with black for the adders and blue for our extensions. Here we are actually using the adders as subtracters, but calculating the 1’s complements of the constant values by hand. The “zero extend” box simply adds a leading 0 to our 7-bit ASCII letter. The left adder subtracts the letter A from C: if no carry is produced, we know that C <0x41 and thus Cdoes not represent an upper-case letter, and U= 0. Similarly, the right adder subtracts 0x5B (the letter Z plus one) from C. If a carry is produced, we know that C≥0x5B, and thus Cdoes not represent an upper-case letter, and U= 0. With the right combination of carries(1 from the left and 0 from the right), we obtain U= 1.C C C C8zero extend SB A 1 discard8−bit adder8 880xA4 SB A 1 discard8−bit adder8 87C 0xBE in out in out U Looking carefully at this solution, however, you might be struck by t he fact that we are calculating two sums and then discarding them. Surely such an approach is ineﬃcient? We oﬀer two answers. First, given the design shown above, a good C AD tool recognizesthat the sum outputs of the adders are not being used, and does not generate logic to ca lculate them. The logic for the two carry bits used to calculate Ucan then be optimized. Second, the design shown, including the calcu lation of the sums, is similarin eﬃciency to whathappens at the rateofabout 1015times per second, 24hoursaday, seven days a week, inside processors in data centers processing HTML, X ML, and other types of human-readable Internet traﬃc. Abstraction is a powerful tool. Later in our class, you will learn how to control logical connections b etween hardware blocks so that you can make use of the same hardware for adding, subtracting, chec king for upper-case letters, and so forth. 2.5.3 Checking ASCII for Lower-case Letters Having developed several approaches for checking for an upper- case letter, the task of checking for a lower- case letter should be straightforward. In ASCII, lower-case lett ers are represented by the 7-bit patterns from 0x61 through 0x7A. One can now easily see how more abstract designs make solving similar tasks easier. If we have designed our upper-case checker with 7-variab le K-maps, we must start again with new K-maps for the lower-case checker. If instead we have taken the approach of designing logic for the upper and lower bits of the ASCII character, we can reuse most of that lo gic, since the functions T4andT5are identical when checking for a lower-case character","{'page_number': 67, 'textbook_name': 'ECE-120-student-notes', 'text': 'One can now easily see how more abstract designs make solving similar tasks easier. If we have designed our upper-case checker with 7-variab le K-maps, we must start again with new K-maps for the lower-case checker. If instead we have taken the approach of designing logic for the upper and lower bits of the ASCII character, we can reuse most of that lo gic, since the functions T4andT5are identical when checking for a lower-case character. Recalling the a lgebraic form of U(C), we can then write a function L(C) (a lower-case checker) as shown on the left below. U=C6C5C4T4+C6C5C4T5 L=C6C5C4T4+C6C5C4T5 =C6C5C4(C3+C2+C1+C0)+ C6C5C4(C3+C2C1+C2C0)B Z ZC C0 07 7 discardC A B Z ZC C0 0discard7 7 A0x61 0x7A 1 01 0comparator7−bit1 01 0comparator7−bit L Finally, if we have used a design based on comparators or adders, th e design of a lower-case checker becomes trivial: simply change the numbers that we input to these component s, as shown in the ﬁgure on the right above for the comparator-based design. The only changes from t he upper-case checker design are the inputs to the comparators and the output produced, highlighted with blue text in the ﬁgure.'}"
"One can now easily see how more abstract designs make solving similar tasks easier. If we have designed our upper-case checker with 7-variab le K-maps, we must start again with new K-maps for the lower-case checker. If instead we have taken the approach of designing logic for the upper and lower bits of the ASCII character, we can reuse most of that lo gic, since the functions T4andT5are identical when checking for a lower-case character. Recalling the a lgebraic form of U(C), we can then write a function L(C) (a lower-case checker) as shown on the left below. U=C6C5C4T4+C6C5C4T5 L=C6C5C4T4+C6C5C4T5 =C6C5C4(C3+C2+C1+C0)+ C6C5C4(C3+C2C1+C2C0)B Z ZC C0 07 7 discardC A B Z ZC C0 0discard7 7 A0x61 0x7A 1 01 0comparator7−bit1 01 0comparator7−bit L Finally, if we have used a design based on comparators or adders, th e design of a lower-case checker becomes trivial: simply change the numbers that we input to these component s, as shown in the ﬁgure on the right above for the comparator-based design. The only changes from t he upper-case checker design are the inputs to the comparators and the output produced, highlighted with blue text in the ﬁgure.","{'page_number': 67, 'textbook_name': 'ECE-120-student-notes', 'text': 'One can now easily see how more abstract designs make solving similar tasks easier. If we have designed our upper-case checker with 7-variab le K-maps, we must start again with new K-maps for the lower-case checker. If instead we have taken the approach of designing logic for the upper and lower bits of the ASCII character, we can reuse most of that lo gic, since the functions T4andT5are identical when checking for a lower-case character. Recalling the a lgebraic form of U(C), we can then write a function L(C) (a lower-case checker) as shown on the left below. U=C6C5C4T4+C6C5C4T5 L=C6C5C4T4+C6C5C4T5 =C6C5C4(C3+C2+C1+C0)+ C6C5C4(C3+C2C1+C2C0)B Z ZC C0 07 7 discardC A B Z ZC C0 0discard7 7 A0x61 0x7A 1 01 0comparator7−bit1 01 0comparator7−bit L Finally, if we have used a design based on comparators or adders, th e design of a lower-case checker becomes trivial: simply change the numbers that we input to these component s, as shown in the ﬁgure on the right above for the comparator-based design. The only changes from t he upper-case checker design are the inputs to the comparators and the output produced, highlighted with blue text in the ﬁgure.'}"
"2.5 Using Abstraction to Simplify Problems 63 2.5.4 The Multiplexer Using the more abstract designs for checking ranges of ASCII cha racters, we can go a step further and create a checker for both upper- and lower-case letters. To do so, we ad d another input Sthat allows us to select the function that we want—either the upper-case checker U(C) or the lower-case checker L(C). For this purpose, we make use of a logic block called a multiplexer , ormux. Multiplexers are an important abstraction for digital logic. In general, a multiplexer allows us to use one digital signal to select which of several others is forwarded to an output. The simplest form of the multiplexer is the 2-to-1 multi- plexer shown to the right. The logic diagram illustrates how the mux works. The block has two inputs from the left and one from the top. The top input allows us to choose which of the left inputs is forwarded to the out- put. When the input S= 0, the upper AND gate out- puts 0, and the lower AND gate outputs the value of D0. The OR gate then produces Q= 0+D0=D0. Similarly, when input S= 1, the upper AND gate outputs D1, and the lower AND gate outputs 0. In this case, the OR gate produces Q=D1+0 =D1.D1 D0D0D1 1 0QSS Q a 2−to−1 multiplexer (logic diagram)a 2−to−1 multiplexer (symbolic form) The symbolic form of the mux is a trapezoid with data inputs on the larg er side, an output on the smaller side, and a select input on the angled part of the trapezoid. The labe ls inside the trapezoid indicate the value of the select input Sfor which the adjacent data signal, D1orD0, is copied to the output Q. We can generalize multiplexers in two ways. First, we can extend the s ingle select input to a group of select inputs. An N-bit select input allows selection from amongst 2Ninputs. A 4-to-1 multiplexer is shown below, for example. The logic diagramon the left showshow the 4-to-1 mux o perates. For any combination of S1S0, three of the AND gates produce 0, and the fourth outputs the Dinput corresponding to the interpretation ofSas an unsigned number. Given three zeroes and one Dinput, the OR gate thus reproduces one of theD’s. When S1S0= 10, for example, the third AND gate copies D2, andQ=D2. 2S1 S0 D1 D0 (logic diagram)a 4−to−1 multiplexerD3 D2 QD3 D2 D1 D0S0 S1 a 4−to−1 multiplexer built from 2−to−1 multiplexers1 01 0 1 0QD2D3 D0D1 a 4−to−1 multiplexer (symbolic form)3 2 1 0QS As shown in the middle ﬁgure, a 4-to-1 mux can also be built from three 2-to-1 muxes","{'page_number': 68, 'textbook_name': 'ECE-120-student-notes', 'text': 'Finally, the symbolic form of a 4-to-1 mux appears on the right in the ﬁgure.'}"
"Finally, the symbolic form of a 4-to-1 mux appears on the right in the ﬁgure.","{'page_number': 68, 'textbook_name': 'ECE-120-student-notes', 'text': 'Finally, the symbolic form of a 4-to-1 mux appears on the right in the ﬁgure.'}"
"64  The second way in which we can generalize multiplexers is by using sever al multiplexers of the same type and using the same signals for selection. For example, we might use a s ingle select bit Tto choose between any number of paired inputs. Denote input pair by i Di 1andDi 0. For each pair, we have an output Qi. WhenT= 0,Qi=Di 0for each value of i. And, when T= 1,Qi=Di 1for each value of i. Each value of i requires a 2-to-1 mux with its select input driven by the global select signalT. Returning to the example of the upper- and lower-casechecker, w e can make use oftwo groupsofseven 2-to-1 muxes, all controlled by a single bit select signal S, to choose between the inputs needed for an upper-case checker and those needed for a lower-case checker. Speciﬁc conﬁgurations of multiplexers are often referred to as N-to-Mmultiplexers . Here the value N refers to the number of inputs, and Mrefers to the number of outputs. The number of select bits can th en be calculated as log2(N/M)—N/Mis generally a power of two—and one way to build such a multiplexer is to useMcopies of an ( N/M)-to-1 multiplexer. Let’s extend our upper- and lower-case checker to check for fou r diﬀerent ranges of ASCII characters, as shown below. This design uses two 28-to-7 muxes to create a single c hecker for the four ranges. Each of the muxes in the ﬁgure logically represents seven 4-to-1 muxes. 7 7 7 7 7 7 7 B Z ZC C0 07 7 discardA B Z ZC C0 0discard7 7 AC 20x7A 0x5A 0x39 0x1F 20x61 0x41 0x30 0x00 7 2 1 01 0comparator7−bit1 01 0comparator7−bit R0 1 2 3 0 1 2 3S The table to the right describes the behavior of the checker. When the select input Sis set to 00, the left mux selects the value 0x00, and the right mux selects the value 0x1F, which checks whether the ASCII character repre- sented by Cis a control character. When the selectinput S= 01,themuxesproducetheval- uesneededtocheckwhether Cisanupper-case letter. Similarly, when the select input S= 10,left right comparator comparator S1S0input input R(C) produced 00 0x00 0x1F control character? 01 0x41 0x5A upper-case letter? 10 0x61 0x7A lower-case letter? 11 0x30 0x39 numeric digit? the muxes produce the values needed to check whether Cis a lower-case letter","{'page_number': 69, 'textbook_name': 'ECE-120-student-notes', 'text': 'Similarly, when the select input S= 10,left right comparator comparator S1S0input input R(C) produced 00 0x00 0x1F control character? 01 0x41 0x5A upper-case letter? 10 0x61 0x7A lower-case letter? 11 0x30 0x39 numeric digit? the muxes produce the values needed to check whether Cis a lower-case letter. Finally, when the select inputS= 11, the left mux selects the value 0x30, and the right mux selects t he value 0x39, which checks whether the ASCII character represented by Cis a digit (0 to 9).'}"
"Similarly, when the select input S= 10,left right comparator comparator S1S0input input R(C) produced 00 0x00 0x1F control character? 01 0x41 0x5A upper-case letter? 10 0x61 0x7A lower-case letter? 11 0x30 0x39 numeric digit? the muxes produce the values needed to check whether Cis a lower-case letter. Finally, when the select inputS= 11, the left mux selects the value 0x30, and the right mux selects t he value 0x39, which checks whether the ASCII character represented by Cis a digit (0 to 9).","{'page_number': 69, 'textbook_name': 'ECE-120-student-notes', 'text': 'Similarly, when the select input S= 10,left right comparator comparator S1S0input input R(C) produced 00 0x00 0x1F control character? 01 0x41 0x5A upper-case letter? 10 0x61 0x7A lower-case letter? 11 0x30 0x39 numeric digit? the muxes produce the values needed to check whether Cis a lower-case letter. Finally, when the select inputS= 11, the left mux selects the value 0x30, and the right mux selects t he value 0x39, which checks whether the ASCII character represented by Cis a digit (0 to 9).'}"
"2.6 Sequential Logic 65 ECE120: Introduction to Computer Engineering Notes Set 2.6 Sequential Logic These notes introduce logic components for storing bits, building up from the idea of a pair of cross-coupled inverters through an implementation of a ﬂip-ﬂop, the storage abs tractions used in most modern logic design processes. We then introduce simple forms of timing diagrams, which we use to illustrate the behavior of a logic circuit. After commenting on the beneﬁts of using a clocked syn chronous abstraction when designing systems that store and manipulate bits, we illustrate timing issues an d explain how these are abstracted away in clocked synchronous designs. Sections marked with an asterisk are provided solely for you r interest, but you probably need to learn this material in later classes . 2.6.1 Storing One Bit So far we have discussed only implementation of Boolean functions: g iven some bits as input, how can we design a logic circuit to calculate the result of applying some function t o those bits? The answer to such questions is called combinational logic (sometimes combinatorial logic ), a name stemming from the fact that we are combining existing bits with logic, not storing new bits . You probably already know, however, that combinational logic alone is not suﬃcient to build a computer. We need the ability to store bits, and to change those bits. Logic des igns that make use of stored bits—bits that can be changed, not just wires to high voltage and ground—ar e calledsequential logic . The name comes from the idea that such a system moves through a sequence of stored bit patterns (the current stored bit pattern is called the stateof the system). Consider the diagramto the right. What is it? A 1-input NAND gate, or aninverterdrawnbadly? Ifyouthinkcarefullyabouthowthesetwo gates are built, you will realize that they are the same thing. Conceptually, we use two inverters to store a bit, but in most cases we make use of NA ND gates to simplify the mechanism for changing the stored bit. Take a look at the design to the right. Here we have taken two inverters (drawn as NAND gates) and coupled each gate’s output to the other’s input. What does the circuit do? Let’s make some guesses and see where they take us. Imagine that the value at Qis 0. In that case, the lower gate drives Pto 1. But Pdrives the upper gate, which forcesQto 0. In other words, this combination forms aQ PQ P 0 1 1 0 stable state of the system: once the gates reach this state, the y continue to hold these values. The ﬁrst row of the truth table to the right (outputs only) shows this state. What ifQ= 1, though? In this case, the lower gate forces Pto 0, and the upper gate in turn forces Qto 1. Another stable state! The Q= 1 state appears as the second row of the truth table. We have identiﬁed all of the stable states.5Notice that our cross-coupled inverters can store a bit","{'page_number': 70, 'textbook_name': 'ECE-120-student-notes', 'text': 'The ﬁrst row of the truth table to the right (outputs only) shows this state. What ifQ= 1, though? In this case, the lower gate forces Pto 0, and the upper gate in turn forces Qto 1. Another stable state! The Q= 1 state appears as the second row of the truth table. We have identiﬁed all of the stable states.5Notice that our cross-coupled inverters can store a bit. Unfortu - nately, we have no way to specify which value should be stored, nor t o change the bit’s value once the gates have settled into a stable state. What can we do? 5Most logic families also allow unstable states in which the v alues alternate rapidly between 0 and 1. These metastable states are beyond the scope of our class, but ensuring that th ey do not occur in practice is important for real designs.'}"
"The ﬁrst row of the truth table to the right (outputs only) shows this state. What ifQ= 1, though? In this case, the lower gate forces Pto 0, and the upper gate in turn forces Qto 1. Another stable state! The Q= 1 state appears as the second row of the truth table. We have identiﬁed all of the stable states.5Notice that our cross-coupled inverters can store a bit. Unfortu - nately, we have no way to specify which value should be stored, nor t o change the bit’s value once the gates have settled into a stable state. What can we do? 5Most logic families also allow unstable states in which the v alues alternate rapidly between 0 and 1. These metastable states are beyond the scope of our class, but ensuring that th ey do not occur in practice is important for real designs.","{'page_number': 70, 'textbook_name': 'ECE-120-student-notes', 'text': 'The ﬁrst row of the truth table to the right (outputs only) shows this state. What ifQ= 1, though? In this case, the lower gate forces Pto 0, and the upper gate in turn forces Qto 1. Another stable state! The Q= 1 state appears as the second row of the truth table. We have identiﬁed all of the stable states.5Notice that our cross-coupled inverters can store a bit. Unfortu - nately, we have no way to specify which value should be stored, nor t o change the bit’s value once the gates have settled into a stable state. What can we do? 5Most logic families also allow unstable states in which the v alues alternate rapidly between 0 and 1. These metastable states are beyond the scope of our class, but ensuring that th ey do not occur in practice is important for real designs.'}"
"66  Let’s add an input to the upper gate, as shown to the right. We call the input ¯S. The “S” stands for set—as you will see, our new input allows us to setour stored bit Qto 1. The use ofacomplementednamefortheinputindicates that the input is active low . In other words, the input performs its intended task (setting Q to 1) when its value is 0 (not 1).the complement marking indicates that this input is active low Q PS¯SQ P 10 1 11 0 01 0 Think about what happens when the new input is not active, ¯S= 1. As you know, ANDing any value with 1 produces the same value, so our new input has no eﬀect when ¯S= 1. The ﬁrst two rows of the truth table are simply a copy of our previous table: the circuit can store either b it value when ¯S= 1. What happens when¯S= 0? In that case, the upper gate’s output is forced to 1, and thus the lower gate’s is forced to 0. This third possibility is reﬂected in the last row of the truth table. Now we have the ability to force bit Qto have value 1, but if we want Q= 0, we just have to hope that the circuit happens to settle into that state when we turn on the power. What can we do? As you probably guessed, we add an input to the other gate, as shown to the right. We call the new input ¯R: the input’s pur- pose is to resetbitQto 0, and the input is active low. We extend the truth table to include a row with ¯R= 0 and ¯S= 1, which forces Q= 0 and P= 1.S the complement markings indicate that the inputs are active lowQ Pan R−S latch (stores a single bit) R¯R¯SQ P 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 The circuit that we have drawn has a name: an ¯R-¯S latch. One can also build R-S latches (with active high set and reset inputs). The textbook also shows an ¯R-¯S latch (labeled incorrectly). Can you ﬁgure out how to build an R-S latch yourself? Let’s think a little more about the ¯R-¯S latch. What happens if we set ¯S= 0 and ¯R= 0 at the same time? Nothing bad happens immediately. Looking at the design, both gates produce 1, so Q= 1 and P= 1. The bad part happens later: if we raise both ¯Sand¯Rback to 1 at around the same time, the stored bit may end up in either state.6 We can avoid the problem by adding gates to prevent the two contro l inputs ( ¯Sand¯R) from ever being 1 at the same time. A single inverter might technically suﬃce, but let’s bu ild up the structure shown below, noting that the two inverters in sequence connecting Dto¯Rhave no practical eﬀect at the moment. A truth table is shown to the right of the logic diagram","{'page_number': 71, 'textbook_name': 'ECE-120-student-notes', 'text': '6 We can avoid the problem by adding gates to prevent the two contro l inputs ( ¯Sand¯R) from ever being 1 at the same time. A single inverter might technically suﬃce, but let’s bu ild up the structure shown below, noting that the two inverters in sequence connecting Dto¯Rhave no practical eﬀect at the moment. A truth table is shown to the right of the logic diagram. When D= 0,¯Ris forced to 0, and the bit is reset. Similarly, when D= 1,¯Sis forced to 0, and the bit is set. S RQ PD D¯R¯S Q P 00 1 0 1 11 0 1 0 Unfortunately, except for some interesting timing characteristic s, the new design has the same functionality as a piece of wire. And, if you ask a circuit designer, thin wires also hav e some interesting timing character- istics. What can we do? Rather than having Qalways reﬂect the current value of D, let’s add some extra inputs to the new NAND gates that allow us to control when the value ofDis copied to Q, as shown on the next page. 6Or, worse, in a metastable state, as mentioned earlier.'}"
"6 We can avoid the problem by adding gates to prevent the two contro l inputs ( ¯Sand¯R) from ever being 1 at the same time. A single inverter might technically suﬃce, but let’s bu ild up the structure shown below, noting that the two inverters in sequence connecting Dto¯Rhave no practical eﬀect at the moment. A truth table is shown to the right of the logic diagram. When D= 0,¯Ris forced to 0, and the bit is reset. Similarly, when D= 1,¯Sis forced to 0, and the bit is set. S RQ PD D¯R¯S Q P 00 1 0 1 11 0 1 0 Unfortunately, except for some interesting timing characteristic s, the new design has the same functionality as a piece of wire. And, if you ask a circuit designer, thin wires also hav e some interesting timing character- istics. What can we do? Rather than having Qalways reﬂect the current value of D, let’s add some extra inputs to the new NAND gates that allow us to control when the value ofDis copied to Q, as shown on the next page. 6Or, worse, in a metastable state, as mentioned earlier.","{'page_number': 71, 'textbook_name': 'ECE-120-student-notes', 'text': '6 We can avoid the problem by adding gates to prevent the two contro l inputs ( ¯Sand¯R) from ever being 1 at the same time. A single inverter might technically suﬃce, but let’s bu ild up the structure shown below, noting that the two inverters in sequence connecting Dto¯Rhave no practical eﬀect at the moment. A truth table is shown to the right of the logic diagram. When D= 0,¯Ris forced to 0, and the bit is reset. Similarly, when D= 1,¯Sis forced to 0, and the bit is set. S RQ PD D¯R¯S Q P 00 1 0 1 11 0 1 0 Unfortunately, except for some interesting timing characteristic s, the new design has the same functionality as a piece of wire. And, if you ask a circuit designer, thin wires also hav e some interesting timing character- istics. What can we do? Rather than having Qalways reﬂect the current value of D, let’s add some extra inputs to the new NAND gates that allow us to control when the value ofDis copied to Q, as shown on the next page. 6Or, worse, in a metastable state, as mentioned earlier.'}"
"2.6 Sequential Logic 67 S RQ PD WEa gated D latch (stores a single bit)WE D ¯R¯S Q P 1 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 1 1 1 1 0 TheWE(write enable) input controls whether or not Qmirrors the value of D. The ﬁrst two rows in the truth table are replicated from our “wire” d esign: a value ofWE= 1has no eﬀect on the ﬁrst two NAND gates, and Q=D. A value of WE= 0 forces the ﬁrst two NAND gates to output 1, thus ¯R= 1,¯S= 1, and the bit Qcan occupy either of the two possible states, regardless of the value o fD, as reﬂected in the lower four lines of the truth table. The circuit just shown is called a gated D latch , and is an important mechanismD QQ WE P and Q were always opposites, so we now just write Q (often omitted entirely in drawings)gated D latch symbol for storing state in sequential logic. (Random-access memory use s a slightly diﬀerent technique to connect the cross-coupled inverters, but latches are used for nearly eve ry other application of stored state.) The “D” stands for “data,” meaning that the bit stored is matches the value of the input. Other types of latches (including S-R latches) have been used historically, but D latches are used predominantly today, so we omit discussion of other types. The “gated” qualiﬁer refers to the pre sence of an enable input (we called it WE) to control when the latch copies its input into the stored bit. A symb ol for a gated D latch appears to the right. Note that we have dropped the name Pin favor of ¯Q, sinceP=¯Qin a gated D latch. 2.6.2 The Clock Abstraction High-speed logic designs often use latches directly. Engineers spec ify the number of latches as well as combinational logic functions needed to connect one latch to the ne xt, and the CAD tools optimize the combinational logic. The enable inputs of successive groups of latch es are then driven by what we call a clock signal, a single bit line distributed across most of the chip that alt ernates between 0 and 1 with a regular period. While the clock is 0, one set of latches holds its bit value s ﬁxed, and combinational logic uses those latches as inputs to produce bits that are copied into a secon d set of latches. When the clock switches to 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches. Of course, some of the latches in the ﬁrst and third sets may be the same. The timing of signals in such designs plays a critical role in their correct operation","{'page_number': 72, 'textbook_name': 'ECE-120-student-notes', 'text': 'When the clock switches to 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches. Of course, some of the latches in the ﬁrst and third sets may be the same. The timing of signals in such designs plays a critical role in their correct operation. Fortunately, we have developed powerful abstractions that allow engineers to ignore mu ch of the complexity while thinking about the Boolean logic needed for a given design. Towards that end, we make a simplifying assumption for the rest of o ur class, and for most of your career as an undergraduate: the clock signal is a square wave delivered uniformly across a chip. For example, if the period of a clock is 0.5 nanoseconds (2 GHz), the clock signal is a 1 for 0.25 nanoseconds, then a 0 for 0.25 nanoseconds. We assume that the clock signal changes instan taneously and at the same time across the chip. Such a signal can never exist in the real world: voltages do not change instantaneously, and the phrase “at the same time” may not even make sense at these scales . However, circuit designers can usually provide a clock signal that is close enough, allowing us to forget for n ow that no physical signal can meet our abstract deﬁnition.'}"
"When the clock switches to 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches. Of course, some of the latches in the ﬁrst and third sets may be the same. The timing of signals in such designs plays a critical role in their correct operation. Fortunately, we have developed powerful abstractions that allow engineers to ignore mu ch of the complexity while thinking about the Boolean logic needed for a given design. Towards that end, we make a simplifying assumption for the rest of o ur class, and for most of your career as an undergraduate: the clock signal is a square wave delivered uniformly across a chip. For example, if the period of a clock is 0.5 nanoseconds (2 GHz), the clock signal is a 1 for 0.25 nanoseconds, then a 0 for 0.25 nanoseconds. We assume that the clock signal changes instan taneously and at the same time across the chip. Such a signal can never exist in the real world: voltages do not change instantaneously, and the phrase “at the same time” may not even make sense at these scales . However, circuit designers can usually provide a clock signal that is close enough, allowing us to forget for n ow that no physical signal can meet our abstract deﬁnition.","{'page_number': 72, 'textbook_name': 'ECE-120-student-notes', 'text': 'When the clock switches to 1, the second set of latches stops storing their data inputs and retains their bit values in order to drive other combinational logic, the results of which are copied into a third set of latches. Of course, some of the latches in the ﬁrst and third sets may be the same. The timing of signals in such designs plays a critical role in their correct operation. Fortunately, we have developed powerful abstractions that allow engineers to ignore mu ch of the complexity while thinking about the Boolean logic needed for a given design. Towards that end, we make a simplifying assumption for the rest of o ur class, and for most of your career as an undergraduate: the clock signal is a square wave delivered uniformly across a chip. For example, if the period of a clock is 0.5 nanoseconds (2 GHz), the clock signal is a 1 for 0.25 nanoseconds, then a 0 for 0.25 nanoseconds. We assume that the clock signal changes instan taneously and at the same time across the chip. Such a signal can never exist in the real world: voltages do not change instantaneously, and the phrase “at the same time” may not even make sense at these scales . However, circuit designers can usually provide a clock signal that is close enough, allowing us to forget for n ow that no physical signal can meet our abstract deﬁnition.'}"
"68  The device shown to the right is a master-slave implementation of a positive edge-triggered D ﬂip-ﬂop. As you can see, we have constructed it from two gated D latches with oppo- site senses of write enable. The “D” part of the name has the same mean- ing as with a gated D latch: the bit stored is the same as the one deliveredD QQ WED QQ WED QQ CLOCKa positive edge−triggered D flip−flop (master−slave implementation)X D QQD flip−flop symbol to the input. Other variants of ﬂip-ﬂops have also been built, but th is type dominates designs today. Most are actually generated automatically from hardware “design” langu ages (that is, computer programming languages for hardware design). When the clock is low (0), the ﬁrst latch copies its value from the ﬂip-ﬂ op’sDinput to the midpoint (marked Xin our ﬁgure, but not usually given a name). When the clock is high (1), the second latch copies its value from Xto the ﬂip-ﬂop’s output Q. SinceXcan not change when the clock is high, the result is that the output changes each time the clock changes from 0 to 1, which is called the rising edge orpositive edge(the derivative) of the clock signal. Hence the qualiﬁer “positive edg e-triggered,” which describes the ﬂip-ﬂop’s behavior. The “master-slave”implementation refersto t he use oftwo latches. In practice, ﬂip-ﬂops are almost never built this way. To see a commercial design, look up 74 LS74, which uses six 3-input NAND gates and allows set/reset of the ﬂip-ﬂop (using two extra inputs) . Thetiming diagram to the right illustrates the operation of our ﬂip-ﬂop. In a timing diagram, the horizontal axis represents (continuous) increasing time, and the individual lines represent voltages for logic signals. The relatively simple version shown here uses only binary values for each signal. One can also draw transitions more realistically (as taking ﬁnite time). The dashed vertical lines here represent the times at which the clock rises. To make thenotice that D may change before rising edge QCLKD X(with a master−slave implementation)D copied to X when CLK is low X copied to Q when CLK is high example interesting, we havevaried Dovertwo clock cycles. Notice that even though Drises and falls during the second clock cycle, its value is not copied to the output of our ﬂip -ﬂop. One can build ﬂip-ﬂops that “catch” this kind of behavior (and change to output 1), but we leav e such designs for later in your career. Circuits such as latches and ﬂip-ﬂops are called sequential feedback circuits, and the process by which they are designed is beyond the scope of our course","{'page_number': 73, 'textbook_name': 'ECE-120-student-notes', 'text': 'One can build ﬂip-ﬂops that “catch” this kind of behavior (and change to output 1), but we leav e such designs for later in your career. Circuits such as latches and ﬂip-ﬂops are called sequential feedback circuits, and the process by which they are designed is beyond the scope of our course. The “feedba ck” part of the name refers to the fact that the outputs of some gates are fed back into the inputs of oth ers. Each cycle in a sequential feedback circuit can store one bit. Circuits that merely use latches and ﬂip-ﬂo ps as building blocks are called clocked synchronous sequential circuits . Such designs are still sequential: their behavior depends on the bit s currently stored in the latches and ﬂip-ﬂops. However, their beha vior is substantially simpliﬁed by the use of a clock signal (the “clocked” part of the name) in a way that all ele ments change at the same time (“synchronously”). The value of using ﬂip-ﬂops and assuming a square-wave clock signal with uniform timing may not be clear to you yet, but it bears emphasis. With such assumptions, we can treat time as having discrete values. In other words, time “ticks” along discretely, like integers instead of r eal numbers. We can look at the state of the system, calculate the inputs to our ﬂip-ﬂops through the comb inational logic that drives their Dinputs, and be conﬁdent that, when time moves to the next discrete value, we will know the new bit values stored in our ﬂip-ﬂops, allowing us to repeat the process for the next clock cycle without worrying about exactly when things change. Values change only on the rising edge of the cloc k! Real systems, of course, are not so simple, and we do not have one clock to drive the universe, so engineers must also design systems that interact even though each has its ow n private clock signal (usually with dif- ferent periods).'}"
"One can build ﬂip-ﬂops that “catch” this kind of behavior (and change to output 1), but we leav e such designs for later in your career. Circuits such as latches and ﬂip-ﬂops are called sequential feedback circuits, and the process by which they are designed is beyond the scope of our course. The “feedba ck” part of the name refers to the fact that the outputs of some gates are fed back into the inputs of oth ers. Each cycle in a sequential feedback circuit can store one bit. Circuits that merely use latches and ﬂip-ﬂo ps as building blocks are called clocked synchronous sequential circuits . Such designs are still sequential: their behavior depends on the bit s currently stored in the latches and ﬂip-ﬂops. However, their beha vior is substantially simpliﬁed by the use of a clock signal (the “clocked” part of the name) in a way that all ele ments change at the same time (“synchronously”). The value of using ﬂip-ﬂops and assuming a square-wave clock signal with uniform timing may not be clear to you yet, but it bears emphasis. With such assumptions, we can treat time as having discrete values. In other words, time “ticks” along discretely, like integers instead of r eal numbers. We can look at the state of the system, calculate the inputs to our ﬂip-ﬂops through the comb inational logic that drives their Dinputs, and be conﬁdent that, when time moves to the next discrete value, we will know the new bit values stored in our ﬂip-ﬂops, allowing us to repeat the process for the next clock cycle without worrying about exactly when things change. Values change only on the rising edge of the cloc k! Real systems, of course, are not so simple, and we do not have one clock to drive the universe, so engineers must also design systems that interact even though each has its ow n private clock signal (usually with dif- ferent periods).","{'page_number': 73, 'textbook_name': 'ECE-120-student-notes', 'text': 'One can build ﬂip-ﬂops that “catch” this kind of behavior (and change to output 1), but we leav e such designs for later in your career. Circuits such as latches and ﬂip-ﬂops are called sequential feedback circuits, and the process by which they are designed is beyond the scope of our course. The “feedba ck” part of the name refers to the fact that the outputs of some gates are fed back into the inputs of oth ers. Each cycle in a sequential feedback circuit can store one bit. Circuits that merely use latches and ﬂip-ﬂo ps as building blocks are called clocked synchronous sequential circuits . Such designs are still sequential: their behavior depends on the bit s currently stored in the latches and ﬂip-ﬂops. However, their beha vior is substantially simpliﬁed by the use of a clock signal (the “clocked” part of the name) in a way that all ele ments change at the same time (“synchronously”). The value of using ﬂip-ﬂops and assuming a square-wave clock signal with uniform timing may not be clear to you yet, but it bears emphasis. With such assumptions, we can treat time as having discrete values. In other words, time “ticks” along discretely, like integers instead of r eal numbers. We can look at the state of the system, calculate the inputs to our ﬂip-ﬂops through the comb inational logic that drives their Dinputs, and be conﬁdent that, when time moves to the next discrete value, we will know the new bit values stored in our ﬂip-ﬂops, allowing us to repeat the process for the next clock cycle without worrying about exactly when things change. Values change only on the rising edge of the cloc k! Real systems, of course, are not so simple, and we do not have one clock to drive the universe, so engineers must also design systems that interact even though each has its ow n private clock signal (usually with dif- ferent periods).'}"
"2.6 Sequential Logic 69 2.6.3 Static Hazards: Causes and Cures* Before we forget about the fact that real designs do not provide perfect clocks, let’s explore some of the issues that engineers must sometimes face. We discuss these prima rily to ensure that you appreciate the power of the abstraction that we use in the rest of our course. In later classes (probably our 298, which will absorb material from 385), you may be required to master this mat erial.For now, we provide it simply for your interest. Consider the circuit shown below, for which the output is given by the equation S=AB+¯B¯C. a glitch in SA B CA B C SS B goes low B goes high The timing diagram on the right shows a glitchin the output when the input shifts from ABC= 110 to 100, that is, when Bfalls. The problem lies in the possibility that the upper AND gate, driven byB, might go low before the lower AND gate, driven by ¯B, goes high. In such a case, the OR gate output Sfalls until the second AND gate rises, and the output exhibits a glitch. A circuit that might exhibit a glitch in an output that functionally remain s stable at 1 is said to have a static-1 hazard . The qualiﬁer “static” here refers to the fact that we expect the output to remain static, while the “1” refers to the expected value of the output. The presence of hazards in circuits can be problematic in certain cas es. In domino logic, for example, an output is precharged and kept at 1 until the output of a driving circ uit pulls it to 0, at which point it stays low (like a domino that has been knocked over). If the driving circuit c ontains static-1 hazards, the output may fall in response to a glitch. Similarly, hazards can lead to unreliable behavior in sequential feedba ck circuits. Consider the addition of a feedback loop to the circuit just discussed, as shown in the ﬁgure below. The output of the circuit is now given by the equation S∗=AB+¯B¯CS, whereS∗denotes the state after Sfeeds back through the lower AND gate. In the case discussed previously, the transition fr omABC= 110 to 100, the glitch in S canbreakthe feedback, leaving Sloworunstable. Theresultingsequentialfeedbackcircuitisthus un reliable. unknown/unstableSS CBCBAA Eliminating static hazards from two-level circuits is fairly straightfo rward. The Kar- naugh map to the right corresponds to our original circuit; the solid lines indicate the implicants selected bythe AND gates. Astatic-1hazardis presentw hen twoadjacent 1s in the K-map are not covered by a common implicant. Static-0 hazard s do not occur in two-level SOP circuits.10 00110100 01 11 10 0 1CAB Eliminating static hazards requires merely extending the circuit with c onsensus terms in order to ensure that some AND gate remains high through every transition between input states with output 1.7In the K-map shown, the dashed line indicates the necessary consensus term, A¯C. 7Hazard elimination is not in general simple; we have conside red only two-level circuits.","{'page_number': 74, 'textbook_name': 'ECE-120-student-notes', 'text': '2.6 Sequential Logic 69 2.6.3 Static Hazards: Causes and Cures* Before we forget about the fact that real designs do not provide perfect clocks, let’s explore some of the issues that engineers must sometimes face. We discuss these prima rily to ensure that you appreciate the power of the abstraction that we use in the rest of our course. In later classes (probably our 298, which will absorb material from 385), you may be required to master this mat erial.For now, we provide it simply for your interest. Consider the circuit shown below, for which the output is given by the equation S=AB+¯B¯C. a glitch in SA B CA B C SS B goes low B goes high The timing diagram on the right shows a glitchin the output when the input shifts from ABC= 110 to 100, that is, when Bfalls. The problem lies in the possibility that the upper AND gate, driven byB, might go low before the lower AND gate, driven by ¯B, goes high. In such a case, the OR gate output Sfalls until the second AND gate rises, and the output exhibits a glitch. A circuit that might exhibit a glitch in an output that functionally remain s stable at 1 is said to have a static-1 hazard . The qualiﬁer “static” here refers to the fact that we expect the output to remain static, while the “1” refers to the expected value of the output. The presence of hazards in circuits can be problematic in certain cas es. In domino logic, for example, an output is precharged and kept at 1 until the output of a driving circ uit pulls it to 0, at which point it stays low (like a domino that has been knocked over). If the driving circuit c ontains static-1 hazards, the output may fall in response to a glitch. Similarly, hazards can lead to unreliable behavior in sequential feedba ck circuits. Consider the addition of a feedback loop to the circuit just discussed, as shown in the ﬁgure below. The output of the circuit is now given by the equation S∗=AB+¯B¯CS, whereS∗denotes the state after Sfeeds back through the lower AND gate. In the case discussed previously, the transition fr omABC= 110 to 100, the glitch in S canbreakthe feedback, leaving Sloworunstable. Theresultingsequentialfeedbackcircuitisthus un reliable. unknown/unstableSS CBCBAA Eliminating static hazards from two-level circuits is fairly straightfo rward. The Kar- naugh map to the right corresponds to our original circuit; the solid lines indicate the implicants selected bythe AND gates. Astatic-1hazardis presentw hen twoadjacent 1s in the K-map are not covered by a common implicant. Static-0 hazard s do not occur in two-level SOP circuits.10 00110100 01 11 10 0 1CAB Eliminating static hazards requires merely extending the circuit with c onsensus terms in order to ensure that some AND gate remains high through every transition between input states with output 1.7In the K-map shown, the dashed line indicates the necessary consensus term, A¯C. 7Hazard elimination is not in general simple; we have conside red only two-level circuits.'}"
"70  2.6.4 Dynamic Hazards* Consider an input transition for which we expect to see a change in an output. Under certain timing conditions, the output may not transition smoothly, but instead bo unce between its original value and its new value before coming to rest at the new value. A circuit that might exhibit such behavior is said to contain a dynamic hazard . The qualiﬁer “dynamic” refers to the expected change in the outp ut. Dynamic hazards appear only in more complex circuits, such as the on e shown below. The output of this circuit is deﬁned by the equation Q=¯AB+¯A¯C+¯B¯C+BD. jA B i C hf g Q D Consider the transition from the input state ABCD= 1111 to 1011, in whichBfalls from 1 to 0. For simplicity, assume that each gate has a delay of 1 time unit. If Bgoes low at time T= 0, the table shows the progression over time of logic levels at several intermediate points in the circuit and at the output Q. Each gate merely produces the appropriate output based on its inputs in the previous time step. After one delay , the three gates with Bas a direct input change their outputs (to stable, ﬁnal values). After another delay, at T= 2, the other three gates re-TfghijQ 0000111 1111111 2111000 3111011 4111010 spond to the initial changes and ﬂip their outputs. The resulting cha nges induce another set of changes at T= 3, which in turn causes the output Qto change a ﬁnal time at T= 4. The output column in the table illustrates the possible impact of a dyna mic hazard: rather than a smooth transition from 1 to 0, the output drops to 0, rises back to 1, and ﬁ nally falls to 0 again. The dynamic hazard in thiscasecanbe attributedto thepresenceofastatic hazardin t helogicthat producesintermediatevalue j.","{'page_number': 75, 'textbook_name': 'ECE-120-student-notes', 'text': '70  2.6.4 Dynamic Hazards* Consider an input transition for which we expect to see a change in an output. Under certain timing conditions, the output may not transition smoothly, but instead bo unce between its original value and its new value before coming to rest at the new value. A circuit that might exhibit such behavior is said to contain a dynamic hazard . The qualiﬁer “dynamic” refers to the expected change in the outp ut. Dynamic hazards appear only in more complex circuits, such as the on e shown below. The output of this circuit is deﬁned by the equation Q=¯AB+¯A¯C+¯B¯C+BD. jA B i C hf g Q D Consider the transition from the input state ABCD= 1111 to 1011, in whichBfalls from 1 to 0. For simplicity, assume that each gate has a delay of 1 time unit. If Bgoes low at time T= 0, the table shows the progression over time of logic levels at several intermediate points in the circuit and at the output Q. Each gate merely produces the appropriate output based on its inputs in the previous time step. After one delay , the three gates with Bas a direct input change their outputs (to stable, ﬁnal values). After another delay, at T= 2, the other three gates re-TfghijQ 0000111 1111111 2111000 3111011 4111010 spond to the initial changes and ﬂip their outputs. The resulting cha nges induce another set of changes at T= 3, which in turn causes the output Qto change a ﬁnal time at T= 4. The output column in the table illustrates the possible impact of a dyna mic hazard: rather than a smooth transition from 1 to 0, the output drops to 0, rises back to 1, and ﬁ nally falls to 0 again. The dynamic hazard in thiscasecanbe attributedto thepresenceofastatic hazardin t helogicthat producesintermediatevalue j.'}"
"2.6 Sequential Logic 71 2.6.5 Essential Hazards* Essential hazards are inherent to the function of a circuit and may appear in any impleme ntation. In sequential feedback circuit design, they must be addressed at a lo w level to ensure that variations in logic path lengths ( timing skew ) through a circuit do not expose them. With clocked synchronous c ircuits, essential hazards are abstracted into a single form: clock skew , or disparate clock edge arrival times at a circuit’s ﬂip-ﬂops. An example demonstrates the possible eﬀects: consider the const ruction of a clocked synchronous circuit to recognize 0-1 sequences on an input IN. Output Qshould be held high for one cycle after recognition, that is, untilthe nextrisingclockedge. Adescriptionofstatesandasta tediagramforsuchacircuitappearbelow. S1S0state meaning 00 A nothing, 1, or 11 seen last 01 B 0 seen last 10 C 01 recognized (output high) 11 unusedA1/0 C B0/0 0/0 1/0 0/1 1/1 For three states, we need two (= ⌈log23⌉) ﬂip-ﬂops. Denote the internal state S1S0. The speciﬁc internal state values for each logicalstate (A, B, and C) simplify the implemen tation and the example. A state table and K-maps for the next-state logic appear below. The state table uses one line per state with separate columns for each input combination, making the table more compact t han one with one line per state/input combination. Each column contains the full next-state information , including output. Using this form of the state table, the K-maps can be read directly from the table. IN S1S00 1 00 01/0 00/0 01 01/0 10/0 11 x x 10 01/1 00/1S S 10 0 1S IN0 000 01 11 10 0 1IN0 0 00 x 0 1 xS S 1 0S S 000 01 11Q x1x 0x 1 xS 100 0 0 101 11 10 0 1IN1 1 0+ 0 1 0+ 1 0 Examining the K-maps, we see that the excitation and output equat ions are S+ 1=IN·S0,S+ 0=IN, and Q=S1. An implementation of the circuit using two D ﬂip-ﬂops appears below. Imagine that mistakes in routing or process variations have made the clock signal’s path to ﬂip -ﬂop 1 much longer than its path into ﬂip-ﬂop 0, as illustrated. 01 10QIN CLKD S D S a long, slow wire Due to the long delays, we cannot assume that rising clock edges arr ive at the ﬂip-ﬂops at the same time. The result is called clock skew, and can make the circuit behave improp erly by exposing essential hazards. In the logical B to C transition, for example, we begin in state S1S0= 01 with IN= 1 and the clock edge rising. Assume that the edge reaches ﬂip-ﬂop 0 at time T= 0","{'page_number': 76, 'textbook_name': 'ECE-120-student-notes', 'text': 'The result is called clock skew, and can make the circuit behave improp erly by exposing essential hazards. In the logical B to C transition, for example, we begin in state S1S0= 01 with IN= 1 and the clock edge rising. Assume that the edge reaches ﬂip-ﬂop 0 at time T= 0. After a ﬂip-ﬂop delay ( T= 1),S0goes low. After another AND gate delay ( T= 2), input D1goes low, but the second ﬂip-ﬂop has yet to change state! Finally, at some later time, the clock edge reaches ﬂip-ﬂop 1. Howeve r, the output S1remains at 0, leaving the system in state A rather than state C. Fortunately, in clocked synchronous sequential circuits, all esse ntial hazards are related to clock skew. This fact implies that we can eliminate a signiﬁcant amount of complexity fro m circuit design by doing a good job of distributing the clock signal. It also implies that, as a designer, you should avoid specious addition of logic in a clock path, as you may regret such a decision later, as you tr y to debug the circuit timing.'}"
"The result is called clock skew, and can make the circuit behave improp erly by exposing essential hazards. In the logical B to C transition, for example, we begin in state S1S0= 01 with IN= 1 and the clock edge rising. Assume that the edge reaches ﬂip-ﬂop 0 at time T= 0. After a ﬂip-ﬂop delay ( T= 1),S0goes low. After another AND gate delay ( T= 2), input D1goes low, but the second ﬂip-ﬂop has yet to change state! Finally, at some later time, the clock edge reaches ﬂip-ﬂop 1. Howeve r, the output S1remains at 0, leaving the system in state A rather than state C. Fortunately, in clocked synchronous sequential circuits, all esse ntial hazards are related to clock skew. This fact implies that we can eliminate a signiﬁcant amount of complexity fro m circuit design by doing a good job of distributing the clock signal. It also implies that, as a designer, you should avoid specious addition of logic in a clock path, as you may regret such a decision later, as you tr y to debug the circuit timing.","{'page_number': 76, 'textbook_name': 'ECE-120-student-notes', 'text': 'The result is called clock skew, and can make the circuit behave improp erly by exposing essential hazards. In the logical B to C transition, for example, we begin in state S1S0= 01 with IN= 1 and the clock edge rising. Assume that the edge reaches ﬂip-ﬂop 0 at time T= 0. After a ﬂip-ﬂop delay ( T= 1),S0goes low. After another AND gate delay ( T= 2), input D1goes low, but the second ﬂip-ﬂop has yet to change state! Finally, at some later time, the clock edge reaches ﬂip-ﬂop 1. Howeve r, the output S1remains at 0, leaving the system in state A rather than state C. Fortunately, in clocked synchronous sequential circuits, all esse ntial hazards are related to clock skew. This fact implies that we can eliminate a signiﬁcant amount of complexity fro m circuit design by doing a good job of distributing the clock signal. It also implies that, as a designer, you should avoid specious addition of logic in a clock path, as you may regret such a decision later, as you tr y to debug the circuit timing.'}"
"72  2.6.6 Proof Outline for Clocked Synchronous Design* This section outlines a proof of the claim made regarding clock skew be ing the only source of essential hazards for clocked synchronous sequential circuits. A proof outline suggests the form that a proof might take and provides some of the logical arguments, but is not rigorou s enough to be considered a proof. Here we use a D ﬂip-ﬂop to illustrate a method for identifying essential haz ards (the D ﬂip-ﬂop has no essential hazards, however ), then arguethat the method can be applied generallyto collections ofﬂip-ﬂopsin aclocked synchronous design to show that essential hazards occur only in t he form of clock skew. state low L clock low, last input low high H clock high, last input low pulse low PL clock low, last input high (output high, too) pulse high PH clock high, last input high (output high, too)L L L PH H H PL PHL PL PLL PL PLH PH PHH H PHstate 00 01 11 10CLK  D Consider the sequential feedback state table for a positive edge- triggered D ﬂip-ﬂop, shown above. In designing and analyzing such circuits, we assume that only one input b it changes at a time. The state table consists of one row for each state and one column for each inp ut combination. Within a row, input combinations that have no eﬀect on the internal state of the circu it (that is, those that do not cause any change in the state) are said to be stable; these states are circled . Other states are unstable, and the circuit changes state in response to changes in the inputs. For example, given an initial state L with low output, low clock, and high inputD, the solid arcs trace the reaction of the circuit to a rising clock edge. From the 01 input combin ation, we move along the column to the 11 column, which indicates the new state, PH. Moving down the co lumn to that state’s row, we see that the new state is stable for the input combination 11, and we stop. If PH were not stable, we would continue to move within the column until coming to rest on a stable state. An essential hazard appears in such a table as a diﬀerence between the ﬁnal state when ﬂipping a bit once and the ﬁnal state when ﬂipping a bit thrice in succession. The dashe d arcs in the ﬁgure illustrate the concept: after coming to rest in the PH state, we reset the input t o 01 and move along the PH row to ﬁnd a new state of PL. Moving up the column, we see that the state is sta ble. We then ﬂip the clock a third time and move back along the row to 11, which indicates that PH is again the next state. Moving down the column, we come again to rest in PH, the same state as was reach ed after one ﬂip. Flipping a bit three times rather than once evaluates the impact of timing skew in the circ uit; if a diﬀerent state is reached after two more ﬂips, timing skew could cause unreliable behavior","{'page_number': 77, 'textbook_name': 'ECE-120-student-notes', 'text': 'We then ﬂip the clock a third time and move back along the row to 11, which indicates that PH is again the next state. Moving down the column, we come again to rest in PH, the same state as was reach ed after one ﬂip. Flipping a bit three times rather than once evaluates the impact of timing skew in the circ uit; if a diﬀerent state is reached after two more ﬂips, timing skew could cause unreliable behavior. As you can verify from the table, a D ﬂip-ﬂop has no essential hazards. A group of ﬂip-ﬂops, as might appear in a clocked synchronous circu it, can and usually does have essential hazards, but only dealing with the clock. As you know, the inputs to a clocked synchronous sequential circuit consist of a clock signal and other inputs (either external o f fed back from the ﬂip-ﬂops). Changing an input other than the clock can change the internal state of a ﬂip -ﬂop (of the master-slave variety), but ﬂip-ﬂop designs do not capture the number of input changes in a cloc k cycle beyond one, and changing an input three times is the same as changing it once. Changing the clock, of course, results in a synchronous state machine transition. The detection of essential hazards in a clocked synchronous desig n based on ﬂip-ﬂops thus reduces to exam- ination of the state machine. If the next state of the machine has a ny dependence on the current state, an essential hazard exists, as a second rising clock edge moves the sy stem into a second new state. For a single D ﬂip-ﬂop, the next state is independent of the current state, an d no essential hazards are present.'}"
"We then ﬂip the clock a third time and move back along the row to 11, which indicates that PH is again the next state. Moving down the column, we come again to rest in PH, the same state as was reach ed after one ﬂip. Flipping a bit three times rather than once evaluates the impact of timing skew in the circ uit; if a diﬀerent state is reached after two more ﬂips, timing skew could cause unreliable behavior. As you can verify from the table, a D ﬂip-ﬂop has no essential hazards. A group of ﬂip-ﬂops, as might appear in a clocked synchronous circu it, can and usually does have essential hazards, but only dealing with the clock. As you know, the inputs to a clocked synchronous sequential circuit consist of a clock signal and other inputs (either external o f fed back from the ﬂip-ﬂops). Changing an input other than the clock can change the internal state of a ﬂip -ﬂop (of the master-slave variety), but ﬂip-ﬂop designs do not capture the number of input changes in a cloc k cycle beyond one, and changing an input three times is the same as changing it once. Changing the clock, of course, results in a synchronous state machine transition. The detection of essential hazards in a clocked synchronous desig n based on ﬂip-ﬂops thus reduces to exam- ination of the state machine. If the next state of the machine has a ny dependence on the current state, an essential hazard exists, as a second rising clock edge moves the sy stem into a second new state. For a single D ﬂip-ﬂop, the next state is independent of the current state, an d no essential hazards are present.","{'page_number': 77, 'textbook_name': 'ECE-120-student-notes', 'text': 'We then ﬂip the clock a third time and move back along the row to 11, which indicates that PH is again the next state. Moving down the column, we come again to rest in PH, the same state as was reach ed after one ﬂip. Flipping a bit three times rather than once evaluates the impact of timing skew in the circ uit; if a diﬀerent state is reached after two more ﬂips, timing skew could cause unreliable behavior. As you can verify from the table, a D ﬂip-ﬂop has no essential hazards. A group of ﬂip-ﬂops, as might appear in a clocked synchronous circu it, can and usually does have essential hazards, but only dealing with the clock. As you know, the inputs to a clocked synchronous sequential circuit consist of a clock signal and other inputs (either external o f fed back from the ﬂip-ﬂops). Changing an input other than the clock can change the internal state of a ﬂip -ﬂop (of the master-slave variety), but ﬂip-ﬂop designs do not capture the number of input changes in a cloc k cycle beyond one, and changing an input three times is the same as changing it once. Changing the clock, of course, results in a synchronous state machine transition. The detection of essential hazards in a clocked synchronous desig n based on ﬂip-ﬂops thus reduces to exam- ination of the state machine. If the next state of the machine has a ny dependence on the current state, an essential hazard exists, as a second rising clock edge moves the sy stem into a second new state. For a single D ﬂip-ﬂop, the next state is independent of the current state, an d no essential hazards are present.'}"
"2.7 Registers 73 ECE120: Introduction to Computer Engineering Notes Set 2.7 Registers This set of notes introduces registers, an abstraction used for s torage of groups of bits in digital systems. We introduce some terminology used to describe aspects of registe r design and illustrate the idea of a shift register. The registers shown here are important abstractions f or digital system design. 2.7.1 Registers Aregister is a storage element composed from one or more ﬂip-ﬂops operating on a common clock. In addition to the ﬂip-ﬂops, most regis- ters include logic to control the bits stored by theregister. Forexample,Dﬂip-ﬂopscopytheir inputs at the rising edge ofeach clock cycle, dis- carding whatever bits they have stored before the rising edge (in the previous clock cycle). To enable a ﬂip-ﬂop to retain its value, we might try to hide the rising edge of the clock from the ﬂip-ﬂop, as shown to the right. TheLOADinput controls the clock signals through a method known as clock gating .cQ CLKIN D LOAD IN LOAD QCLK c specious falling edge (has no effect)specious rising edge (causes incorrect load)incorrect output value WhenLOADis high, the circuit reduces to a regular D ﬂip-ﬂop. When LOADis low, the ﬂip-ﬂop clock input,c, is held high, and the ﬂip-ﬂop stores its current value. The problems with clock gating are twofold. First, adding logic to the clock path introduces clock skew, which may cause timing problems later in the development process (or, worse, in future projects that use yo ur circuits as components). Second, in the design shown above, the LOADsignal can only be lowered while the clock is high to prevent spurious ris ing edges from causing incorrect behavior, as shown in the timing diagra m. A better approach is to use a mux and a feedback loop from the ﬂip- ﬂop’s output, as shown in the ﬁgure to the right. When LOADis low, the mux selects the feedback line, and the register reloads its curr ent value. When LOADis high, the mux selects the INinput, and the register loads a new value. The result is similar to a gated D latch with distinct write enable and clock lines.QIN CLKD10LOAD We can use this extended ﬂip-ﬂop as a bit slice for a multi-bit register. A four-bit register of this type is shown to the right. Four data lines—one for each bit—enter the registers from the top of the ﬁgure. When LOAD is low, the logic copies each ﬂip-ﬂop’s value back to its input, 3 0 1 23 0 1 2 Q Q Q QIN IN IN IN CLK1 0 1 0 1 0 1 0 D D D DLOAD and the INinput lines are ignored","{'page_number': 78, 'textbook_name': 'ECE-120-student-notes', 'text': 'A four-bit register of this type is shown to the right. Four data lines—one for each bit—enter the registers from the top of the ﬁgure. When LOAD is low, the logic copies each ﬂip-ﬂop’s value back to its input, 3 0 1 23 0 1 2 Q Q Q QIN IN IN IN CLK1 0 1 0 1 0 1 0 D D D DLOAD and the INinput lines are ignored. When LOADis high, the muxes forward each INline to the corre- sponding ﬂip-ﬂop’s Dinput, allowing the register to load the new 4-bit value. The use of one input line per bit to load a multi-bit register in a single cycle is termed a parallel load .'}"
"A four-bit register of this type is shown to the right. Four data lines—one for each bit—enter the registers from the top of the ﬁgure. When LOAD is low, the logic copies each ﬂip-ﬂop’s value back to its input, 3 0 1 23 0 1 2 Q Q Q QIN IN IN IN CLK1 0 1 0 1 0 1 0 D D D DLOAD and the INinput lines are ignored. When LOADis high, the muxes forward each INline to the corre- sponding ﬂip-ﬂop’s Dinput, allowing the register to load the new 4-bit value. The use of one input line per bit to load a multi-bit register in a single cycle is termed a parallel load .","{'page_number': 78, 'textbook_name': 'ECE-120-student-notes', 'text': 'A four-bit register of this type is shown to the right. Four data lines—one for each bit—enter the registers from the top of the ﬁgure. When LOAD is low, the logic copies each ﬂip-ﬂop’s value back to its input, 3 0 1 23 0 1 2 Q Q Q QIN IN IN IN CLK1 0 1 0 1 0 1 0 D D D DLOAD and the INinput lines are ignored. When LOADis high, the muxes forward each INline to the corre- sponding ﬂip-ﬂop’s Dinput, allowing the register to load the new 4-bit value. The use of one input line per bit to load a multi-bit register in a single cycle is termed a parallel load .'}"
"74  2.7.2 Shift Registers Certain types of registers include logic to manipulate data held within the register. A shift register is an important example of this3 2 1 0CLKSI SO Q Q Q QD D D D type. The simplest shift registeris a series of D ﬂip-ﬂops, with the ou tput of each attached to the input of the next, as shown to the right above. In the circuit shown, a serial inp utSIaccepts a single bit of data per cycle and delivers the bit four cycles later to a serial output SO. Shift registers serve many purposes in modern systems, from the obvious uses of providing a ﬁxed delay and perfo rming bit shifts for processor arithmetic to rate matching between components and reducing the pin count o n programmable logic devices such as ﬁeld programmable gate arrays (FPGAs), the modern form of the p rogrammable logic array mentioned in the textbook. An examplehelps toillustratetheratematchingproblem: historicalI /Obusesused fairlyslowclocks,asthey had to drive signals and be arbitrated over relatively long distances. The Peripheral Control Interconnect (PCI) standard, for example, provided for 33 and 66 MHz bus spee ds. To provide adequate data rates, such buses use many wires in parallel, either 32 or 64 in the case of PCI. In c ontrast, a Gigabit Ethernet (local area network) signal travelling over a ﬁber is clocked at 1.25 GHz, bu t sends only one bit per cycle. Several layers of shift registers sit between the ﬁber and the I/O bus to me diate between the slow, highly parallel signals that travel over the I/O bus and the fast, serial signals th at travel over the ﬁber. The latest variant of PCI, PCIe (e for “express”), uses serial lines at much higher clo ck rates. Returning to the ﬁgure above, imagine that the outputs Qifeed into logic clocked at 1 /4ththe rate of the shift register (and suitably synchronized). Every four cycles, th e ﬂip-ﬂops ﬁll up with another four bits, at which point the outputs are read in parallel. The shift register shown can thus serve to transform serial data to 4-bit-parallel data at one-quarter the clock speed. Unlike the registers discussed earlier, the shift register above does not support parallel load, which prevents it fr om transforming a slow, parallel stream of data into a high-speed serial stream. The use of serial load requires Ncycles for an N-bit register, but can reduce the number of wires needed to support the operation o f the shift register. How would you add support for parallel load? How many additional inputs would be neces sary? The shift register above also shifts continuously, and cannot stor e a value. A set of muxes, analogous to those that we used to control register loading, can be applied to co ntrol shifting, as shown below. 3 0 1 2 Q Q Q QCLK1 0 1 0 1 0 1 0 D D D DSHIFT SI SO Using a 4-to-1 mux, we can construct a shift register with additiona l functionality","{'page_number': 79, 'textbook_name': 'ECE-120-student-notes', 'text': 'A set of muxes, analogous to those that we used to control register loading, can be applied to co ntrol shifting, as shown below. 3 0 1 2 Q Q Q QCLK1 0 1 0 1 0 1 0 D D D DSHIFT SI SO Using a 4-to-1 mux, we can construct a shift register with additiona l functionality. The bit slice at the top of the next page allows us to build a bidirectional shift register with parallel load capability and the ability to retain its value indeﬁnitely. The two-bit control input Cuses a representation that we have chosen for the four operations supported by our shift register, as show n in the table below the bit slice design.'}"
"A set of muxes, analogous to those that we used to control register loading, can be applied to co ntrol shifting, as shown below. 3 0 1 2 Q Q Q QCLK1 0 1 0 1 0 1 0 D D D DSHIFT SI SO Using a 4-to-1 mux, we can construct a shift register with additiona l functionality. The bit slice at the top of the next page allows us to build a bidirectional shift register with parallel load capability and the ability to retain its value indeﬁnitely. The two-bit control input Cuses a representation that we have chosen for the four operations supported by our shift register, as show n in the table below the bit slice design.","{'page_number': 79, 'textbook_name': 'ECE-120-student-notes', 'text': 'A set of muxes, analogous to those that we used to control register loading, can be applied to co ntrol shifting, as shown below. 3 0 1 2 Q Q Q QCLK1 0 1 0 1 0 1 0 D D D DSHIFT SI SO Using a 4-to-1 mux, we can construct a shift register with additiona l functionality. The bit slice at the top of the next page allows us to build a bidirectional shift register with parallel load capability and the ability to retain its value indeﬁnitely. The two-bit control input Cuses a representation that we have chosen for the four operations supported by our shift register, as show n in the table below the bit slice design.'}"
"2.7 Registers 75 The bit slice allows us to build N-bit shift registers by replicating the slice and adding a ﬁxed amount of “ glue logic .” For example, the ﬁgure below represents a 4-bit bidirectional shift register constructed in this way. The mux used for the SOoutput logic is the glue logic needed in addition to the four bit slices. At each rising clock edge, the action speciﬁed by C1C0is taken. When C1C0= 00, the register holds its current value, with the register value appearing on Q[3 : 0] and each ﬂip-ﬂop feeding its output back into its input. For C1C0= 01, the shift register shifts left: the serial input, SI, is fed into ﬂip-ﬂop 0, and Q3is passed to the serial output, SO. Similarly, when C1C0= 11, the shift register shifts right: SIis fed into ﬂip-ﬂop 3, and Q0is passed to SO. Finally, the case C1C0= 10 causes all ﬂip-ﬂops to accept new values from IN[3 : 0], eﬀecting a parallel load.ii−1 i+1i QQ QIN CLK2C D3 0 2 1 C1C0 meaning 00 retain current value 01shift left (low to high) 10load new value (from IN) 11shift right (high to low) 0 3 2 1IN3 IN2 IN1 IN0 Q Q Q Qi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQ bidirectional shift register bitbidirectional shift register bitbidirectional shift register bitbidirectional shift register bit SOC1C0 CLKSI 1 0Qi+1Qi+1Qi+1Qi+1 Several specialized shift operations are used to support data man ipulation in modern processors (CPUs). Essentially, these specializations dictate the glue logic for a shift reg ister as well as the serial input value. The simplest is a logical shift , for which SIis hardwired to 0: incoming bits are always 0. A cyclic shift takesSOand feeds it back into SI, forming a circle of register bits through which the data bits cycle. Finally, an arithmetic shift treats the shift register contents as a number in 2’s complement fo rm. For non-negative numbers and left shifts, an arithmetic shift is the sam e as a logical shift. When a negative number is arithmetically shifted to the right, however, the sign bit is r etained, resulting in a function similar to division by two. The diﬀerence lies in the rounding direction. Division b y two rounds towardszero in most processors:−5/2 gives−2. Arithmetic shift right rounds away from zero for negative numbe rs (and towards zero for positive numbers): −5>>1 gives−3","{'page_number': 80, 'textbook_name': 'ECE-120-student-notes', 'text': 'When a negative number is arithmetically shifted to the right, however, the sign bit is r etained, resulting in a function similar to division by two. The diﬀerence lies in the rounding direction. Division b y two rounds towardszero in most processors:−5/2 gives−2. Arithmetic shift right rounds away from zero for negative numbe rs (and towards zero for positive numbers): −5>>1 gives−3. We transform our previous shift register into one capable of arithmetic shifts by eliminating the serial input and feeding the most s igniﬁcant bit, which represents the sign in 2’s complement form, back into itself for right shifts, as shown below. The bit shifted in for left shifts has been hardwired to 0. 0 3 2 1IN3 IN2 IN1 IN0 Q Q Q Qi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQ bidirectional shift register bitbidirectional shift register bitbidirectional shift register bitbidirectional shift register bit SOC01C CLK0 1 0Qi+1Qi+1Qi+1Qi+1'}"
"When a negative number is arithmetically shifted to the right, however, the sign bit is r etained, resulting in a function similar to division by two. The diﬀerence lies in the rounding direction. Division b y two rounds towardszero in most processors:−5/2 gives−2. Arithmetic shift right rounds away from zero for negative numbe rs (and towards zero for positive numbers): −5>>1 gives−3. We transform our previous shift register into one capable of arithmetic shifts by eliminating the serial input and feeding the most s igniﬁcant bit, which represents the sign in 2’s complement form, back into itself for right shifts, as shown below. The bit shifted in for left shifts has been hardwired to 0. 0 3 2 1IN3 IN2 IN1 IN0 Q Q Q Qi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQ bidirectional shift register bitbidirectional shift register bitbidirectional shift register bitbidirectional shift register bit SOC01C CLK0 1 0Qi+1Qi+1Qi+1Qi+1","{'page_number': 80, 'textbook_name': 'ECE-120-student-notes', 'text': 'When a negative number is arithmetically shifted to the right, however, the sign bit is r etained, resulting in a function similar to division by two. The diﬀerence lies in the rounding direction. Division b y two rounds towardszero in most processors:−5/2 gives−2. Arithmetic shift right rounds away from zero for negative numbe rs (and towards zero for positive numbers): −5>>1 gives−3. We transform our previous shift register into one capable of arithmetic shifts by eliminating the serial input and feeding the most s igniﬁcant bit, which represents the sign in 2’s complement form, back into itself for right shifts, as shown below. The bit shifted in for left shifts has been hardwired to 0. 0 3 2 1IN3 IN2 IN1 IN0 Q Q Q Qi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQi−1QiIN 1C 0C iQ bidirectional shift register bitbidirectional shift register bitbidirectional shift register bitbidirectional shift register bit SOC01C CLK0 1 0Qi+1Qi+1Qi+1Qi+1'}"
"76  ECE120: Introduction to Computer Engineering Notes Set 2.8 Summary of Part 2 of the Course These notes supplement the Patt and Patel textbook, so you will a lso need to read and understand the relevant chapters (see the syllabus) in order to master this mater ial completely. The diﬃculty of learning depends on the type of task involved. Remem bering new terminology is relatively easy, while applying the ideas underlying design decisions shown by exa mple to new problems posed as human tasks is relatively hard. In this short summary, we give you list s at several levels of diﬃculty of what we expect you to be able to do as a result of the last few weeks o f studying (reading, listening, doing homework, discussing your understanding with your classmates, a nd so forth). We’ll start with the skills, and leave the easy stuﬀ for the next page. We expect you to be able to exercise the following skills: •Design a CMOS gate for a simple Boolean function from n-type and p-t ype transistors. •Apply DeMorgan’s laws repeatedly to simplify the form of the compleme nt of a Boolean expression. •Use a K-map to ﬁnd a reasonable expression for a Boolean function ( for example, in POS or SOP form with the minimal number of terms). •More generally, translate Boolean logic functions among concise alge braic, truth table, K-map, and canonical (minterm/maxterm) forms. When designing combinational logic, we expect you to be able to apply t he following design strategies: •Make use of human algorithms (for example, multiplication from additio n). •Determine whether a bit-sliced approach is applicable, and, if so, mak e use of one. •Break truth tables into parts so as to solve each part of a function separately. •Make use of known abstractions (adders, comparators, muxes, or other abstractions available to you) to simplify the problem. And, at the highest level, we expect that you will be able to do the follo wing: •Understand and be able to reason at a high-level about circuit desig n tradeoﬀs between area/cost and performance (and to know that power is also important, but we hav en’t given you any quantiﬁcation methods). •Understand the tradeoﬀs typically made to develop bit-sliced design s—typically, bit-sliced designs are simplerbut biggerand slower—andhowone can developvariantsbetw een the extremes ofthe bit-sliced approach and optimization of functions speciﬁc to an N-bit design. •Understand the pitfalls of marking a function’s value as “don’t care” for some input combinations, and recognize that implementations do not produce “don’t care.” •Understand the tradeoﬀs involved in selecting a representation fo r communicating information between elements in a design, such as the bit slices in a bit-sliced design. •Explain the operation of a latch or a ﬂip-ﬂop, particularly in terms of t he bistable states used to hold a bit","{'page_number': 81, 'textbook_name': 'ECE-120-student-notes', 'text': '” •Understand the tradeoﬀs involved in selecting a representation fo r communicating information between elements in a design, such as the bit slices in a bit-sliced design. •Explain the operation of a latch or a ﬂip-ﬂop, particularly in terms of t he bistable states used to hold a bit. •Understand and be able to articulate the value of the clocked synch ronous design abstraction.'}"
"” •Understand the tradeoﬀs involved in selecting a representation fo r communicating information between elements in a design, such as the bit slices in a bit-sliced design. •Explain the operation of a latch or a ﬂip-ﬂop, particularly in terms of t he bistable states used to hold a bit. •Understand and be able to articulate the value of the clocked synch ronous design abstraction.","{'page_number': 81, 'textbook_name': 'ECE-120-student-notes', 'text': '” •Understand the tradeoﬀs involved in selecting a representation fo r communicating information between elements in a design, such as the bit slices in a bit-sliced design. •Explain the operation of a latch or a ﬂip-ﬂop, particularly in terms of t he bistable states used to hold a bit. •Understand and be able to articulate the value of the clocked synch ronous design abstraction.'}"
"2.8 Summary of Part 2 of the Course 77 You should recognize all of these terms and be able to explain what th ey mean. For the speciﬁc circuits, you should be able to draw them and explain how they work. Actually, w e don’t care whether you can draw something from memory—a full adder, for example—provided that y ou know what a full adder does and can derive a gate diagram correctly for one in a few minutes. Higher- level skills are much more valuable","{'page_number': 82, 'textbook_name': 'ECE-120-student-notes', 'text': '•Boolean functions and logic gates - NOT/inverter - AND - OR - XOR - NAND - NOR - XNOR - majority function •speciﬁc logic circuits - full adder - ripple carry adder - N-to-M multiplexer (mux) - N-to-2N decoder -¯R-¯S latch - R-S latch - gated D latch - master-slave implementation of a positive edge-triggered D ﬂip-ﬂop - (bidirectional) shift register - register supporting parallel load •design metrics - metric - optimal - heuristic - constraints - power, area/cost, performance - computer-aided design (CAD) tools - gate delay •general math concepts - canonical form -N-dimensional hypercube •tools for solving logic problems - truth table - Karnaugh map (K-map) - implicant - prime implicant - bit-slicing - timing diagram•device technology - complementary metal-oxide semiconductor (CMOS) - ﬁeld eﬀect transistor (FET) - transistor gate, source, drain •Boolean logic terms - literal - algebraic properties - dual form, principle of duality - sum, product - minterm, maxterm - sum-of-products (SOP) - product-of-sums (POS) - canonical sum/SOP form - canonical product/POS form - logical equivalence •digital systems terms - word size -N-bit Gray code - combinational/combinatorial logic - two-level logic - “don’t care” outputs (x’s) - sequential logic - state - active low input - set a bit (to 1) - reset a bit (to 0) - master-slave implementation - positive edge-triggered - clock signal - square wave - rising/positive clock edge - falling/negative clock edge - clock gating - clocked synchronous sequential circuit - parallel/serial load of register - logical/arithmetic/cyclic shift'}"
"•Boolean functions and logic gates - NOT/inverter - AND - OR - XOR - NAND - NOR - XNOR - majority function •speciﬁc logic circuits - full adder - ripple carry adder - N-to-M multiplexer (mux) - N-to-2N decoder -¯R-¯S latch - R-S latch - gated D latch - master-slave implementation of a positive edge-triggered D ﬂip-ﬂop - (bidirectional) shift register - register supporting parallel load •design metrics - metric - optimal - heuristic - constraints - power, area/cost, performance - computer-aided design (CAD) tools - gate delay •general math concepts - canonical form -N-dimensional hypercube •tools for solving logic problems - truth table - Karnaugh map (K-map) - implicant - prime implicant - bit-slicing - timing diagram•device technology - complementary metal-oxide semiconductor (CMOS) - ﬁeld eﬀect transistor (FET) - transistor gate, source, drain •Boolean logic terms - literal - algebraic properties - dual form, principle of duality - sum, product - minterm, maxterm - sum-of-products (SOP) - product-of-sums (POS) - canonical sum/SOP form - canonical product/POS form - logical equivalence •digital systems terms - word size -N-bit Gray code - combinational/combinatorial logic - two-level logic - “don’t care” outputs (x’s) - sequential logic - state - active low input - set a bit (to 1) - reset a bit (to 0) - master-slave implementation - positive edge-triggered - clock signal - square wave - rising/positive clock edge - falling/negative clock edge - clock gating - clocked synchronous sequential circuit - parallel/serial load of register - logical/arithmetic/cyclic shift","{'page_number': 82, 'textbook_name': 'ECE-120-student-notes', 'text': '•Boolean functions and logic gates - NOT/inverter - AND - OR - XOR - NAND - NOR - XNOR - majority function •speciﬁc logic circuits - full adder - ripple carry adder - N-to-M multiplexer (mux) - N-to-2N decoder -¯R-¯S latch - R-S latch - gated D latch - master-slave implementation of a positive edge-triggered D ﬂip-ﬂop - (bidirectional) shift register - register supporting parallel load •design metrics - metric - optimal - heuristic - constraints - power, area/cost, performance - computer-aided design (CAD) tools - gate delay •general math concepts - canonical form -N-dimensional hypercube •tools for solving logic problems - truth table - Karnaugh map (K-map) - implicant - prime implicant - bit-slicing - timing diagram•device technology - complementary metal-oxide semiconductor (CMOS) - ﬁeld eﬀect transistor (FET) - transistor gate, source, drain •Boolean logic terms - literal - algebraic properties - dual form, principle of duality - sum, product - minterm, maxterm - sum-of-products (SOP) - product-of-sums (POS) - canonical sum/SOP form - canonical product/POS form - logical equivalence •digital systems terms - word size -N-bit Gray code - combinational/combinatorial logic - two-level logic - “don’t care” outputs (x’s) - sequential logic - state - active low input - set a bit (to 1) - reset a bit (to 0) - master-slave implementation - positive edge-triggered - clock signal - square wave - rising/positive clock edge - falling/negative clock edge - clock gating - clocked synchronous sequential circuit - parallel/serial load of register - logical/arithmetic/cyclic shift'}"
"3.1 Serialization and Finite State Machines 79 ECE120: Introduction to Computer Engineering Notes Set 3.1 Serialization and Finite State Machines The third part of our class builds upon the basic combinational and se quential logic elements that we developed in the second part. After discussing a simple application of stored state to trade between area and performance, we introduce a powerful abstraction for formalizin g and reasoning about digital systems, the Finite State Machine (FSM). General FSM models are broadly applicab le in a range of engineering contexts, including not only hardware and software design but also the design o f control systems and distributed systems. We limit our model so as to avoid circuit timing issues in your ﬁr st exposure, but provide some amount of discussion as to how, when, and why you should eventually learn the more sophisticated models. Through development a range of FSM examples, we illustrate importa nt design issues for these systems and motivateacoupleofmoreadvancedcombinationallogicdevicesthat canbe usedasbuilding blocks. Together with the idea of memory, another form of stored state, these elem ents form the basis for development of our ﬁrst computer. At this point we return to the textbook, in whic h Chapters 4 and 5 provide a solid introduction to the von Neumann model of computing systems and t he LC-3 (Little Computer, version 3) instruction set architecture. By the end of this part of the cours e, you will have seen an example of the boundary between hardware and software, and will be ready to wr ite some instructions yourself. In this set of notes, we cover the ﬁrst few parts of this material. W e begin by describing the conversion of bit-sliced designs into serial designs, which store a single bit slice’s out put in ﬂip-ﬂops and then feed the outputs back into the bit slice in the next cycle. As a speciﬁc example, we use our bit-sliced comparator to discuss tradeoﬀs in area and performance. We introduce Finite S tate Machines and some of the tools used to design them, then develop a handful of simple counter desig ns. Before delving too deeply into FSM design issues, we spend a little time discussing other strategies for c ounter design and placing the material covered in our course in the broader context of digital system des ign. Remember that sections marked with an asterisk are provided solely for your interest, but you may need to learn this material in later classes. 3.1.1 Serialization: General Strategy In previous notes, we discussed and illustrated the development of bit-sliced logic, in which one designs a logic block to handle one bit of a multi-bit operation, then replicates th e bit slice logic to construct a design for the entire operation. We developed ripple carry adders in this wa y in Notes Set 2.3 and both unsigned and 2’s complement comparators in Notes Set 2.4. Another interesting design strategy is serialization : rather than replicating the bit slice, we can use ﬂip- ﬂops to store the bits passed from one bit slice to the next, then pr esent the stored bits to the same bit slice in the next cycle","{'page_number': 84, 'textbook_name': 'ECE-120-student-notes', 'text': 'We developed ripple carry adders in this wa y in Notes Set 2.3 and both unsigned and 2’s complement comparators in Notes Set 2.4. Another interesting design strategy is serialization : rather than replicating the bit slice, we can use ﬂip- ﬂops to store the bits passed from one bit slice to the next, then pr esent the stored bits to the same bit slice in the next cycle. Thus, in a serial design, we only need one copy of th e bit slice logic! The area needed for a serial design is usually much less than for a bit-sliced design, but such a design is also usually slower. After illustrating the general design strategy, we’ll consider thes e tradeoﬀs more carefully in the context of a detailed example. Recall the general bit-sliced design ap- proach, as illustrated to the right. Some number of copies of the logic for a single bit slice are connected in sequence. Each bit slice accepts Pbits of operand input and produces Qbits of external output. Adjacent bit slices receive an addi- tionalMbits of information from the previous bit slice and pass along Mbits to the next bit slice, generallyusing some representation chosen by the designer.P Qsecond bit sliceMP Qlast bit sliceMP QM Moutput logicRinitial values . . .first bit sliceresults per−slice outputsper−slice inputsa general bit−sliced design The ﬁrst bit slice is initialized by passing in constant values, and some ca lculation may be performed on the ﬁnal bit slice’s results to produce Rbits more external output.'}"
"We developed ripple carry adders in this wa y in Notes Set 2.3 and both unsigned and 2’s complement comparators in Notes Set 2.4. Another interesting design strategy is serialization : rather than replicating the bit slice, we can use ﬂip- ﬂops to store the bits passed from one bit slice to the next, then pr esent the stored bits to the same bit slice in the next cycle. Thus, in a serial design, we only need one copy of th e bit slice logic! The area needed for a serial design is usually much less than for a bit-sliced design, but such a design is also usually slower. After illustrating the general design strategy, we’ll consider thes e tradeoﬀs more carefully in the context of a detailed example. Recall the general bit-sliced design ap- proach, as illustrated to the right. Some number of copies of the logic for a single bit slice are connected in sequence. Each bit slice accepts Pbits of operand input and produces Qbits of external output. Adjacent bit slices receive an addi- tionalMbits of information from the previous bit slice and pass along Mbits to the next bit slice, generallyusing some representation chosen by the designer.P Qsecond bit sliceMP Qlast bit sliceMP QM Moutput logicRinitial values . . .first bit sliceresults per−slice outputsper−slice inputsa general bit−sliced design The ﬁrst bit slice is initialized by passing in constant values, and some ca lculation may be performed on the ﬁnal bit slice’s results to produce Rbits more external output.","{'page_number': 84, 'textbook_name': 'ECE-120-student-notes', 'text': 'We developed ripple carry adders in this wa y in Notes Set 2.3 and both unsigned and 2’s complement comparators in Notes Set 2.4. Another interesting design strategy is serialization : rather than replicating the bit slice, we can use ﬂip- ﬂops to store the bits passed from one bit slice to the next, then pr esent the stored bits to the same bit slice in the next cycle. Thus, in a serial design, we only need one copy of th e bit slice logic! The area needed for a serial design is usually much less than for a bit-sliced design, but such a design is also usually slower. After illustrating the general design strategy, we’ll consider thes e tradeoﬀs more carefully in the context of a detailed example. Recall the general bit-sliced design ap- proach, as illustrated to the right. Some number of copies of the logic for a single bit slice are connected in sequence. Each bit slice accepts Pbits of operand input and produces Qbits of external output. Adjacent bit slices receive an addi- tionalMbits of information from the previous bit slice and pass along Mbits to the next bit slice, generallyusing some representation chosen by the designer.P Qsecond bit sliceMP Qlast bit sliceMP QM Moutput logicRinitial values . . .first bit sliceresults per−slice outputsper−slice inputsa general bit−sliced design The ﬁrst bit slice is initialized by passing in constant values, and some ca lculation may be performed on the ﬁnal bit slice’s results to produce Rbits more external output.'}"
"80  We can transform this bit-sliced design to a serial design with a single c opy of the bit slice logic, M+Q ﬂip-ﬂops, and Mgates (and sometimes an inverter). The strategy is illustrated on t he right below. A single copy of the bit slice operates on one set of Pexternal input bits and produces one set of Qexternal output bits each clock cycle. In the design shown, these output bit s are available during the next cycle, after they have been stored in the ﬂip-ﬂops. The Mbits to be passed to the “next” bit slice are also stored in ﬂip-ﬂops, and in the next cycle are provided back to the same physic al bit slice as inputs. The ﬁrst cycle of a multi-cycle operation must be handled slightly diﬀerently, so we add se lection logic and an control signal, F. For the ﬁrst cycle, we apply F= 1, and the initial values are passed into the bit slice. For all other bit s, we apply F= 0, and the values stored in the ﬂip-ﬂops are returned to the bit slic e’s inputs. After all bits have passed through the bit slice—after Ncycles for an N-bit design—the ﬁnal Mbits are stored in the ﬂip-ﬂops, and the results are calculated by the output logic. iBFiBFinitialize to 0 when F=1 initialize to 1 when F=1P select logicbit sliceinitial valuesM MM flip− flopsoutput logicR QQM MFper−slice inputsa serialized bit−sliced design CLK M flip− flopsQresults per−slice outputsB The selection logic merits explanation. Given that the original design in itialized the bits to constant values (0s or 1s), we need only simple logic for selection. The two drawings on the left above illustrate how Bi, the complemented ﬂip-ﬂop output for a bit i, can be combined with the ﬁrst-cycle signal Fto produce an appropriate input for the bit slice. Selection thus requires one extr a gate for each of the Minputs, and we need an inverter for Fif any of the initial values is 1. 3.1.2 Serialization: Comparator Example We now apply the general strategy to a speciﬁc example, our bit-sliced unsigned comparator from Notes Set 2.4. The result is shown to the right. In terms of the general model, the single comparator bit slice ac- ceptsP= 2 bits of inputs each cycle, in this case a single bit from each of the two numbers being compared, presented to the bit slice in increasing order of signiﬁcance. The bit slice produces no external output other than the ﬁnal result ( Q= 0). Two bits ( M= 2) are producedeachcycleby the bit sliceand storedC CZ Z B0B1A Z ZA BB CLK D QQF D QQa serial unsigned comparator 1 01 0i 1 0comparator biti sliceoutput logic into ﬂip ﬂops B1andB0","{'page_number': 85, 'textbook_name': 'ECE-120-student-notes', 'text': 'The bit slice produces no external output other than the ﬁnal result ( Q= 0). Two bits ( M= 2) are producedeachcycleby the bit sliceand storedC CZ Z B0B1A Z ZA BB CLK D QQF D QQa serial unsigned comparator 1 01 0i 1 0comparator biti sliceoutput logic into ﬂip ﬂops B1andB0. These bits represent the relationship between the two numbers c ompared so far (including only the bit alreadyseen by the comparatorbit slice). On th e ﬁrst cycle, when the least signiﬁcant bits ofAandBare being fed into the bit slice, we set F= 1, which forces the C1andC0inputs of the bit slice to 0 independent of the values stored in the ﬂip-ﬂops. In all o ther cycles, F= 0, and the NOR gates set C1=B1andC0=B0. Finally, after Ncycles for an N-bit comparison, the output logic—in this case simply wires, as shown in the dashed box—places the result o f the comparison on the Z1andZ0 outputs ( R= 2 in the general model). The result is encoded in the representatio n deﬁned for constructing the bit slice (see Notes Set 2.4, but the encoding does not matter he re).'}"
"The bit slice produces no external output other than the ﬁnal result ( Q= 0). Two bits ( M= 2) are producedeachcycleby the bit sliceand storedC CZ Z B0B1A Z ZA BB CLK D QQF D QQa serial unsigned comparator 1 01 0i 1 0comparator biti sliceoutput logic into ﬂip ﬂops B1andB0. These bits represent the relationship between the two numbers c ompared so far (including only the bit alreadyseen by the comparatorbit slice). On th e ﬁrst cycle, when the least signiﬁcant bits ofAandBare being fed into the bit slice, we set F= 1, which forces the C1andC0inputs of the bit slice to 0 independent of the values stored in the ﬂip-ﬂops. In all o ther cycles, F= 0, and the NOR gates set C1=B1andC0=B0. Finally, after Ncycles for an N-bit comparison, the output logic—in this case simply wires, as shown in the dashed box—places the result o f the comparison on the Z1andZ0 outputs ( R= 2 in the general model). The result is encoded in the representatio n deﬁned for constructing the bit slice (see Notes Set 2.4, but the encoding does not matter he re).","{'page_number': 85, 'textbook_name': 'ECE-120-student-notes', 'text': 'The bit slice produces no external output other than the ﬁnal result ( Q= 0). Two bits ( M= 2) are producedeachcycleby the bit sliceand storedC CZ Z B0B1A Z ZA BB CLK D QQF D QQa serial unsigned comparator 1 01 0i 1 0comparator biti sliceoutput logic into ﬂip ﬂops B1andB0. These bits represent the relationship between the two numbers c ompared so far (including only the bit alreadyseen by the comparatorbit slice). On th e ﬁrst cycle, when the least signiﬁcant bits ofAandBare being fed into the bit slice, we set F= 1, which forces the C1andC0inputs of the bit slice to 0 independent of the values stored in the ﬂip-ﬂops. In all o ther cycles, F= 0, and the NOR gates set C1=B1andC0=B0. Finally, after Ncycles for an N-bit comparison, the output logic—in this case simply wires, as shown in the dashed box—places the result o f the comparison on the Z1andZ0 outputs ( R= 2 in the general model). The result is encoded in the representatio n deﬁned for constructing the bit slice (see Notes Set 2.4, but the encoding does not matter he re).'}"
"3.1 Serialization and Finite State Machines 81 How does the serial design compare with the bit-sliced design? As an estimate of area, let’s count gates. Our optimized design is replicated to the right for convenience. Each bit slice requires six 2-input gates and two inverters. Assume that each ﬂip-ﬂop requires eight 2-input gates and two inverters, so the se- rial design overall requires 24 gatesC1 Z1 C0Z0A Ba comparator bit slice (optimized, NAND/NOR) and six inverters to handle any number of bits. Thus, for any numbe r of bits N≥4, the serial design is smaller than the bit-sliced design, and the beneﬁt grows with N. What about performance? In Notes Set 2.4, we counted gate delay s for our bit-sliced design. The path fromAorBto the outputs is four gate delays, but the CtoZpaths are all two gate delays. Overall, then, the bit-sliced design requires 2 N+2 gate delays for Nbits. What about the serial design? The performance of the serial design is likely to be much worse for th ree reasons. First, all paths in the design matter, not just the paths from bit slice to bit slice. None of t he inputs can be assumed to be available before the start of the clock cycle, so we must consider all paths f rom input to output. Second, we must also count gate delays for the selection logic as well as the gates embedd ed in the ﬂip-ﬂops. Finally, the result of these calculations may not matter, since the clock speed may well be limited by other logic elsewhere in the system. If we want a common clock for all of our logic, the clock m ust not go faster than the slowest element in the entire system, or some of our logic will not work proper ly. What is the longest path through our serial comparator? Let’s ass ume that the path through a ﬂip-ﬂop is eight gate delays, with four on each side of the clock’s rising edge. Th e inputs AandBare likely to be driven by ﬂip-ﬂops elsewhere in our system, so we conservatively co unt four gate delays to AandBand ﬁve gate delays to C1andC0(the extra one comes from the selection logic). The AandBpaths thus dominate inside the bit slice, adding four more gate delays to the outputs Z1andZ0. Finally, we add the last four gate delays to ﬂip the ﬁrst latch in the ﬂip-ﬂops for a total of 12 gat e delays. If we assume that our serial comparator limits the clock frequency (that is, if everything else in t he system can use a faster clock), we take 12 gate delays per cycle, or 12 Ngate delays to compare two N-bit numbers. You might also notice that adding support for 2’s complement is no long er free. We need extra logic to swap theAandBinputs in the cycle corresponding to the sign bits of AandB. In other cycles, they must remain in the usual order","{'page_number': 86, 'textbook_name': 'ECE-120-student-notes', 'text': 'If we assume that our serial comparator limits the clock frequency (that is, if everything else in t he system can use a faster clock), we take 12 gate delays per cycle, or 12 Ngate delays to compare two N-bit numbers. You might also notice that adding support for 2’s complement is no long er free. We need extra logic to swap theAandBinputs in the cycle corresponding to the sign bits of AandB. In other cycles, they must remain in the usual order. This extra logic is not complex, but adds further delay to the paths. The bit-sliced and serial designs represent two extreme points in a b road space of design possibilities. Op- timization of the entire N-bit logic function (for any metric) represe nts a third extreme. As an engineer, you should realize that you can design systems anywhere in between these points as well. At the end of Notes Set 2.4, for example, we showed a design for a logic slice that co mpares two bits at a time. In general, we can optimize logic for any number of bits and then apply multiple copie s of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generaliza tion of the serialization approach), or in a combination of the two. Sometimes these tradeoﬀs may happen a t a higher level. As mentioned in Notes Set 2.3, computer software uses the carry out of an adder to perform addition of larger groups of bits (over multiple clock cycles) than is supported by the processor ’s adder hardware. In computer system design, engineers often design hardware elements that are gener al enough to support this kind of extension in software. As a concrete example of the possible tradeoﬀs, consider a serial c omparator design based on the 2-bit slice variant. This approach leads to a serial design with 24 gates and 10 in verters, which is not much larger than our earlier serial design. In terms of gate delays, however, the ne w design is identical, meaning that we ﬁnish a comparison in half the time. More realistic area and timing metrics sho w slightly more diﬀerence between the two designs. These diﬀerences can dominate the results if we blin dly scale the idea to handle more bits without thinking carefully about the design. Neither many-input gat es nor gates driving many outputs work well in practice.'}"
"If we assume that our serial comparator limits the clock frequency (that is, if everything else in t he system can use a faster clock), we take 12 gate delays per cycle, or 12 Ngate delays to compare two N-bit numbers. You might also notice that adding support for 2’s complement is no long er free. We need extra logic to swap theAandBinputs in the cycle corresponding to the sign bits of AandB. In other cycles, they must remain in the usual order. This extra logic is not complex, but adds further delay to the paths. The bit-sliced and serial designs represent two extreme points in a b road space of design possibilities. Op- timization of the entire N-bit logic function (for any metric) represe nts a third extreme. As an engineer, you should realize that you can design systems anywhere in between these points as well. At the end of Notes Set 2.4, for example, we showed a design for a logic slice that co mpares two bits at a time. In general, we can optimize logic for any number of bits and then apply multiple copie s of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generaliza tion of the serialization approach), or in a combination of the two. Sometimes these tradeoﬀs may happen a t a higher level. As mentioned in Notes Set 2.3, computer software uses the carry out of an adder to perform addition of larger groups of bits (over multiple clock cycles) than is supported by the processor ’s adder hardware. In computer system design, engineers often design hardware elements that are gener al enough to support this kind of extension in software. As a concrete example of the possible tradeoﬀs, consider a serial c omparator design based on the 2-bit slice variant. This approach leads to a serial design with 24 gates and 10 in verters, which is not much larger than our earlier serial design. In terms of gate delays, however, the ne w design is identical, meaning that we ﬁnish a comparison in half the time. More realistic area and timing metrics sho w slightly more diﬀerence between the two designs. These diﬀerences can dominate the results if we blin dly scale the idea to handle more bits without thinking carefully about the design. Neither many-input gat es nor gates driving many outputs work well in practice.","{'page_number': 86, 'textbook_name': 'ECE-120-student-notes', 'text': 'If we assume that our serial comparator limits the clock frequency (that is, if everything else in t he system can use a faster clock), we take 12 gate delays per cycle, or 12 Ngate delays to compare two N-bit numbers. You might also notice that adding support for 2’s complement is no long er free. We need extra logic to swap theAandBinputs in the cycle corresponding to the sign bits of AandB. In other cycles, they must remain in the usual order. This extra logic is not complex, but adds further delay to the paths. The bit-sliced and serial designs represent two extreme points in a b road space of design possibilities. Op- timization of the entire N-bit logic function (for any metric) represe nts a third extreme. As an engineer, you should realize that you can design systems anywhere in between these points as well. At the end of Notes Set 2.4, for example, we showed a design for a logic slice that co mpares two bits at a time. In general, we can optimize logic for any number of bits and then apply multiple copie s of the resulting logic in space (a generalization of the bit-sliced approach), or in time (a generaliza tion of the serialization approach), or in a combination of the two. Sometimes these tradeoﬀs may happen a t a higher level. As mentioned in Notes Set 2.3, computer software uses the carry out of an adder to perform addition of larger groups of bits (over multiple clock cycles) than is supported by the processor ’s adder hardware. In computer system design, engineers often design hardware elements that are gener al enough to support this kind of extension in software. As a concrete example of the possible tradeoﬀs, consider a serial c omparator design based on the 2-bit slice variant. This approach leads to a serial design with 24 gates and 10 in verters, which is not much larger than our earlier serial design. In terms of gate delays, however, the ne w design is identical, meaning that we ﬁnish a comparison in half the time. More realistic area and timing metrics sho w slightly more diﬀerence between the two designs. These diﬀerences can dominate the results if we blin dly scale the idea to handle more bits without thinking carefully about the design. Neither many-input gat es nor gates driving many outputs work well in practice.'}"
"82  3.1.3 Finite State Machines Aﬁnite state machine (orFSM) is a model for understanding the behavior of a system by describin g the system as occupying one of a ﬁnite set of states, moving betwe en these states in response to external inputs, and producing external outputs. In any given state, a pa rticular input may cause the FSM to move to another state; this combination is called a transition rule . An FSM comprises ﬁve parts: a ﬁnite set of states, a set of possible inputs, a set of possible outputs, a set of transition rules, and methods for calculating outputs. When an FSM is implemented as a digital system, all states must be rep resented as patterns using a ﬁxed number of bits, all inputs must be translated into bits, and all outpu ts must be translated into bits. For a digital FSM, transition rules must be complete ; in other words, given any state of the FSM, and any pattern of input bits, a transition must be deﬁned from that state to another state (transitions from a state to itself, called self-loops , are acceptable). And, of course, calculation of outputs for a dig ital FSM reduces to Boolean logic expressions. In this class, we focus on clocked sync hronous FSM implementations, in which the FSM’s internal state bits are stored in ﬂip-ﬂops. In this section, we introduce the tools used to describe, develop, a nd analyze implementations of FSMs with digital logic. In the next few weeks, we will show you how an FSM can se rve as the central control logic in a computer. At the same time, we will illustrate connections between F SMs and software and will make some connections with other areas of interest in ECE, such as the design and analysis of digital control systems. The table below gives a list of abstract states for a typical keyless entry system for a car. In this case, we have merely named the states rather than specifying the bit pat terns to be used for each state—for this reason, we refer to them as abstract states. The description of the states in the ﬁrst column is an optional element often included in the early design stages for an FSM, when ide ntifying the states needed for the design. A list may also include the outputs for each state. Again, in th e list below, we have speciﬁed these outputs abstractly. By including outputs for each state, we implicit ly assume that outputs depend only on the state of the FSM. We discuss this assumption in more detail later in these notes (see “Machine Models”), but will make the assumption throughout our class. meaning state driver’s door other doors alarm on vehicle locked LOCKED locked locked no driver door unlocked DRIVER unlocked locked no all doors unlocked UNLOCKED unlocked unlocked no alarm sounding ALARM locked locked yes Another tool used with FSMs is the next-state table (sometimes called a state transition table , or just astate table ), which maps the current state and input combination into the next state of the FSM. The abstract variant shown below outlines desired behavior at a high leve l, and is often ambiguous, incomplete, and even inconsistent","{'page_number': 87, 'textbook_name': 'ECE-120-student-notes', 'text': 'The abstract variant shown below outlines desired behavior at a high leve l, and is often ambiguous, incomplete, and even inconsistent. For example, what happens if a user pushes two buttons? What happens if they push unlock while the alarm is sounding? These questions should event ually be considered. However, we can already start to see the intended use of the design: starting f rom a locked car, a user can push “unlock” once to gain entry to the driver’s seat, or push “unlock” twice to op en the car fully for passengers. To lock the car, a user can push the “lock” button at any time. And, if a use r needs help, pressing the “panic” button sets oﬀ an alarm. state action/input next state LOCKED push “unlock” DRIVER DRIVER push “unlock” UNLOCKED (any) push “lock” LOCKED (any) push “panic” ALARM'}"
"The abstract variant shown below outlines desired behavior at a high leve l, and is often ambiguous, incomplete, and even inconsistent. For example, what happens if a user pushes two buttons? What happens if they push unlock while the alarm is sounding? These questions should event ually be considered. However, we can already start to see the intended use of the design: starting f rom a locked car, a user can push “unlock” once to gain entry to the driver’s seat, or push “unlock” twice to op en the car fully for passengers. To lock the car, a user can push the “lock” button at any time. And, if a use r needs help, pressing the “panic” button sets oﬀ an alarm. state action/input next state LOCKED push “unlock” DRIVER DRIVER push “unlock” UNLOCKED (any) push “lock” LOCKED (any) push “panic” ALARM","{'page_number': 87, 'textbook_name': 'ECE-120-student-notes', 'text': 'The abstract variant shown below outlines desired behavior at a high leve l, and is often ambiguous, incomplete, and even inconsistent. For example, what happens if a user pushes two buttons? What happens if they push unlock while the alarm is sounding? These questions should event ually be considered. However, we can already start to see the intended use of the design: starting f rom a locked car, a user can push “unlock” once to gain entry to the driver’s seat, or push “unlock” twice to op en the car fully for passengers. To lock the car, a user can push the “lock” button at any time. And, if a use r needs help, pressing the “panic” button sets oﬀ an alarm. state action/input next state LOCKED push “unlock” DRIVER DRIVER push “unlock” UNLOCKED (any) push “lock” LOCKED (any) push “panic” ALARM'}"
"3.1 Serialization and Finite State Machines 83 Astate transition diagram (ortransition diagram , orstate diagram ), as shown to the right, illustrates the contents of the next-state table graphically, with each state drawn in a circle, and arcsbetween states labeled with the inputcombinationsthatcausethesetransitions from one state to another. Putting the FSM design into this graphical form does not solve the problems with the ab- stract model. The questions that we asked in regard to the next-state table remain unan- swered. Implementing an FSM using digital logic re- quires that we translate the design into bits, eliminate any ambiguity, and complete the speciﬁcation. How many internal bits should we use? What are the possible input val- ues, and how are their meanings represented in bits? What are the possible output val- ues, and how are their meanings represented in bits? We will consider these questions for several examples in the coming weeks.push ""lock"" push ""lock"" push ""lock"" push ""panic""push ""panic""push ""panic""push ""unlock"" push ""lock"" push ""panic""push ""unlock""LOCKED DRIVER ALARM UNLOCKED For now, we simply deﬁne answers for our example design, the keyles s entry system. Given four states, we need at least⌈log2(4)⌉= 2 bits of internal state, which we store in two ﬂip-ﬂops and call S1S0. The table below lists input and output signals and deﬁnes their meaning. outputs Ddriver door; 1 means unlocked Rother doors (remaining doors); 1 means unlocked Aalarm; 1 means alarm is sounding inputs Uunlock button; 1 means it has been pressed Llock button; 1 means it has been pressed Ppanic button; 1 means it has been pressed We can now choose a representation for our states and rewrite th e list of states, using bits both for the states and for the outputs. We also include the meaning of each state for c larity in our example. Note that we can choose the internal representation in any way. Here we have matc hed theDandRoutputs when possible to simplify the output logic needed for the implementation. The order of states in the list is not particularly important, but should be chosen for convenience and clarity (includ ing transcribing bits into to K-maps, for example). driver’s door other doors alarm on meaning state S1S0 D R A vehicle locked LOCKED 00 0 0 0 driver door unlocked DRIVER 10 1 0 0 all doors unlocked UNLOCKED 11 1 1 0 alarm sounding ALARM 01 0 0 1","{'page_number': 88, 'textbook_name': 'ECE-120-student-notes', 'text': '3.1 Serialization and Finite State Machines 83 Astate transition diagram (ortransition diagram , orstate diagram ), as shown to the right, illustrates the contents of the next-state table graphically, with each state drawn in a circle, and arcsbetween states labeled with the inputcombinationsthatcausethesetransitions from one state to another. Putting the FSM design into this graphical form does not solve the problems with the ab- stract model. The questions that we asked in regard to the next-state table remain unan- swered. Implementing an FSM using digital logic re- quires that we translate the design into bits, eliminate any ambiguity, and complete the speciﬁcation. How many internal bits should we use? What are the possible input val- ues, and how are their meanings represented in bits? What are the possible output val- ues, and how are their meanings represented in bits? We will consider these questions for several examples in the coming weeks.push ""lock"" push ""lock"" push ""lock"" push ""panic""push ""panic""push ""panic""push ""unlock"" push ""lock"" push ""panic""push ""unlock""LOCKED DRIVER ALARM UNLOCKED For now, we simply deﬁne answers for our example design, the keyles s entry system. Given four states, we need at least⌈log2(4)⌉= 2 bits of internal state, which we store in two ﬂip-ﬂops and call S1S0. The table below lists input and output signals and deﬁnes their meaning. outputs Ddriver door; 1 means unlocked Rother doors (remaining doors); 1 means unlocked Aalarm; 1 means alarm is sounding inputs Uunlock button; 1 means it has been pressed Llock button; 1 means it has been pressed Ppanic button; 1 means it has been pressed We can now choose a representation for our states and rewrite th e list of states, using bits both for the states and for the outputs. We also include the meaning of each state for c larity in our example. Note that we can choose the internal representation in any way. Here we have matc hed theDandRoutputs when possible to simplify the output logic needed for the implementation. The order of states in the list is not particularly important, but should be chosen for convenience and clarity (includ ing transcribing bits into to K-maps, for example). driver’s door other doors alarm on meaning state S1S0 D R A vehicle locked LOCKED 00 0 0 0 driver door unlocked DRIVER 10 1 0 0 all doors unlocked UNLOCKED 11 1 1 0 alarm sounding ALARM 01 0 0 1'}"
"84  We can also rewrite the next-state table in terms of bits. We use Gra y code order on both axes, as these orders make it more convenient to use K-maps. The values represe nted in this table are the next FSM state given the current state S1S0and the inputs U,L, andP. Our symbols for the next-state bits are S+ 1 andS+ 0. The “+” superscript is a common way of expressing the next value in a discrete series, here induced by the use of clocked synchronous logic in implementing the FSM. In ot her words, S+ 1is the value of S1 in the next clock cycle, and S+ 1in an FSM implemented as a digital system is a Boolean expression based on the current state and the inputs. For our example problem, we w ant to be able to write down expres- sions for S+ 1(S1,S0,U,L,P) andS+ 1(S1,S0,U,L,P), as well as expressions for the output logic U(S1,S0), L(S1,S0), andP(S1,S0). current state ULP S1S0000 001 011 010 110 111 101 100 00 00 01 01 00 00 01 01 10 01 01 01 01 00 00 01 01 01 11 11 01 01 00 00 01 01 11 10 10 01 01 00 00 01 01 11 In the process of writing out the next-statetable, wehavemadede- cisions for all of the questions that we asked earlier regarding the ab- stract state table. These decisions are also reﬂected in the complete state transition diagram shown to the right. The states have been extended with state bits and out- put bits, as S1S0/DRA. You should recognize that we can also leave some questions unanswered by placing x’s (don’t cares) into our table. However, you should also understand at this point that any implementation will produce bits, not x’s, so we must be care- ful not to allow arbitrary choices unless any of the choices allowed is indeed acceptable for our FSM’s purpose. We will discuss this pro- cess and the considerations neces- sary as we cover more FSM design examples.ULP=000,010, or 110 ULP=000,001,011, 111,101, or 100ULP=   010 or   110 ULP=010 or 110ULP=001,011, 101, or 111 ULP=000 or 100ULP=100ULP=100 ULP=010 or 110 ULP=001,011,101, or 111ULP=000 ULP=001,011, 101, or 11100/000LOCKED DRIVER 10/100 ALARM 01/001 11/110UNLOCKED We have deliberately omitted calculation of expressions for the next -state variables S+ 1andS+ 0, and for the outputsU,L, andP. We expect that you are able to do so from the detailed state table a bove, and may assign such an exercise as part of your homework. 3.1","{'page_number': 89, 'textbook_name': 'ECE-120-student-notes', 'text': 'We expect that you are able to do so from the detailed state table a bove, and may assign such an exercise as part of your homework. 3.1.4 Synchronous Counters Acounter is a clocked sequential circuit with a state diagram consisting of a sing le logical cycle. Not all counters are synchronous. In other words, not all ﬂip-ﬂops in a c ounter are required to use the same clock signal. A counter in which all ﬂip-ﬂops do utilize the same clock signal is c alled asynchronous counter . Except for a brief introduction to other types of counters in the n ext section, our class focuses entirely on clocked synchronous designs, including counters.'}"
"We expect that you are able to do so from the detailed state table a bove, and may assign such an exercise as part of your homework. 3.1.4 Synchronous Counters Acounter is a clocked sequential circuit with a state diagram consisting of a sing le logical cycle. Not all counters are synchronous. In other words, not all ﬂip-ﬂops in a c ounter are required to use the same clock signal. A counter in which all ﬂip-ﬂops do utilize the same clock signal is c alled asynchronous counter . Except for a brief introduction to other types of counters in the n ext section, our class focuses entirely on clocked synchronous designs, including counters.","{'page_number': 89, 'textbook_name': 'ECE-120-student-notes', 'text': 'We expect that you are able to do so from the detailed state table a bove, and may assign such an exercise as part of your homework. 3.1.4 Synchronous Counters Acounter is a clocked sequential circuit with a state diagram consisting of a sing le logical cycle. Not all counters are synchronous. In other words, not all ﬂip-ﬂops in a c ounter are required to use the same clock signal. A counter in which all ﬂip-ﬂops do utilize the same clock signal is c alled asynchronous counter . Except for a brief introduction to other types of counters in the n ext section, our class focuses entirely on clocked synchronous designs, including counters.'}"
"3.1 Serialization and Finite State Machines 85 The design of synchronous counter circuits is a fairly straightforw ard exercise given the desired cycle of output patterns. The task can be more c omplex if the internal state bits are allowed to diﬀer from the output bits, so for now we assume that output Ziis equal to internal bit Si. Note that distinction between internal states and outputs is necessary if any output p attern appears more than once in the desired cycle. The cycle ofstates shown to the right correspondsto the states of a 3-bit binary counter. The numbers in the states represent both internal sta te bitsS2S1S0 and output bits Z2Z1Z0. We transcribe this diagram into the next-state table shown on the left below. We then write out K-maps for the next stat e bitsS+ 2, S+ 1, andS+ 0, as shown to the right, and use the K-maps to ﬁnd expressions for these variables in terms of the current state.3−bit binary counter cycle000 110111 101 011 100001 010 S2S1S0S+ 2S+ 1S+ 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0S2+S2 S1S0 00 01 11 10 0 10 10 1 01 0 1S2S1S0+S1 00 01 11 10 0 1 10 1 01 1 0 0S2S1S0+S0 0 000 01 11 10 0 1 1010 11 S+ 2=¯S2S1S0+S2¯S1+S2¯S0=S2⊕(S1S0) S+ 1= S1¯S0+¯S1S0=S1⊕S0 S+ 0= ¯S0 =S0⊕1 The ﬁrst form of the expression for each next-state variable is ta ken directly from the corresponding K-map. We have rewritten each expression to make the emerging pattern m ore obvious. We can also derive the pat- tern intuitively by asking the following: given a binary counter in state SN−1SN−2...Sj+1SjSj−1...S1S0, when does Sjchange in the subsequent state? The answer, of course, is that Sjchanges when all of the bits belowSjare 1. Otherwise, Sjremains the same in the next state. We thus write S+ j=Sj⊕(Sj−1...S1S0) and implement the counter as shown below for a 4-bit design. Note th at the usual order of output bits along the bottom is reversed in the ﬁgure, with the most signiﬁcant bit at t he right rather than the left","{'page_number': 90, 'textbook_name': 'ECE-120-student-notes', 'text': 'Otherwise, Sjremains the same in the next state. We thus write S+ j=Sj⊕(Sj−1...S1S0) and implement the counter as shown below for a 4-bit design. Note th at the usual order of output bits along the bottom is reversed in the ﬁgure, with the most signiﬁcant bit at t he right rather than the left. S3D QQ S1D QQ S2 S0D QQ D QQa 4−bit synchronous binary counter with serial gating Z1 Z2 Z3 Z0CLK The calculation of the left inputs to the XOR gates in the counter sho wn above is performed with a series of two-input AND gates. Each of these gates AND’s another ﬂip-ﬂop v alue into the product. This approach, calledserial gating , implies that an N-bit counter requires more than N−2 gate delays to settle into the next state. An alternative approach, called parallel gating , calculates each input independently with a'}"
"Otherwise, Sjremains the same in the next state. We thus write S+ j=Sj⊕(Sj−1...S1S0) and implement the counter as shown below for a 4-bit design. Note th at the usual order of output bits along the bottom is reversed in the ﬁgure, with the most signiﬁcant bit at t he right rather than the left. S3D QQ S1D QQ S2 S0D QQ D QQa 4−bit synchronous binary counter with serial gating Z1 Z2 Z3 Z0CLK The calculation of the left inputs to the XOR gates in the counter sho wn above is performed with a series of two-input AND gates. Each of these gates AND’s another ﬂip-ﬂop v alue into the product. This approach, calledserial gating , implies that an N-bit counter requires more than N−2 gate delays to settle into the next state. An alternative approach, called parallel gating , calculates each input independently with a","{'page_number': 90, 'textbook_name': 'ECE-120-student-notes', 'text': 'Otherwise, Sjremains the same in the next state. We thus write S+ j=Sj⊕(Sj−1...S1S0) and implement the counter as shown below for a 4-bit design. Note th at the usual order of output bits along the bottom is reversed in the ﬁgure, with the most signiﬁcant bit at t he right rather than the left. S3D QQ S1D QQ S2 S0D QQ D QQa 4−bit synchronous binary counter with serial gating Z1 Z2 Z3 Z0CLK The calculation of the left inputs to the XOR gates in the counter sho wn above is performed with a series of two-input AND gates. Each of these gates AND’s another ﬂip-ﬂop v alue into the product. This approach, calledserial gating , implies that an N-bit counter requires more than N−2 gate delays to settle into the next state. An alternative approach, called parallel gating , calculates each input independently with a'}"
"86  single logic gate, as shown below. The blue inputs to the AND gate for S3highlight the diﬀerence from the previous ﬁgure (note that the two approaches diﬀer only for bits S3and above). With parallel gating, the fan-inof the gates (the number of inputs) and the fan-out of the ﬂip-ﬂop outputs (number of other gates into which an output feeds) grow with the size of the counter. In pr actice, large counters use a combination of these two approaches. S3D QQ S1D QQ S2 S0D QQ D QQa 4−bit synchronous binary counter with parallel gating Z1 Z2 Z3 Z0CLK 3.1.5 Ripple Counters A second class of counter drives some of its ﬂip-ﬂops with a clock sign al and feeds ﬂip-ﬂop outputs into the clock inputs of its remaining ﬂip-ﬂops, possibly through additiona l logic. Such a counter is called a ripple counter , because the eﬀect of a clock edge ripples through the ﬂip-ﬂops. T he delay inherent to the ripple eﬀect, along with the complexity of ensuring that timing issues d o not render the design unreliable, are the major drawbacks of ripple counters. Compared with synch ronous counters, however, ripple counters consume less energy, and are sometimes used for devices with rest ricted energy supplies. General ripple counterscan be trickybecause oftiming issues, but certain types are easy. Consider the design of binary ripple counter. The state d iagram for a 3-bit binary counter is replicated to the right. Looking at the stat es, notice that the least-signiﬁcant bit alternates with each state, while highe r bits ﬂip whenever the next smaller bit (to the right) transitions from one to zero. To take advantage of these properties, we use positive edge-trigge red D ﬂip-ﬂops with their complemented ( ¯Q) outputs wired back to their inputs. The clock input is fed only into the ﬁrst ﬂip-ﬂop, and the complemented output of each ﬂip-ﬂop is also connected to the clock of the next.3−bit binary counter cycle000 110111 101 011 100001 010 An implementation of a 4-bit binary ripple counter appears to the right. The order of bits in the ﬁgure matches the order used for our synchronous binary counters: least signiﬁcant on the left, most signiﬁcant on the right. As you can see from the ﬁgure, the technique gen- eralizes to arbitrarily large binary ripple coun-S0 S1D QQ S2D QQ S3D QQ D QQa 4−bit binary ripple counter Z0 Z1 Z2 Z3CLK ters, but the time required for the outputs to settle after a clock edge scales with the number of ﬂip-ﬂops in the counter","{'page_number': 91, 'textbook_name': 'ECE-120-student-notes', 'text': 'As you can see from the ﬁgure, the technique gen- eralizes to arbitrarily large binary ripple coun-S0 S1D QQ S2D QQ S3D QQ D QQa 4−bit binary ripple counter Z0 Z1 Z2 Z3CLK ters, but the time required for the outputs to settle after a clock edge scales with the number of ﬂip-ﬂops in the counter. On the other hand, an average of only two ﬂip-ﬂops s ee each clock edge (1+1 /2+1/4+...), which reduces the power requirements.8 8Recall that ﬂip-ﬂops record the clock state internally. The logical activity required to record such state consumes ene rgy.'}"
"As you can see from the ﬁgure, the technique gen- eralizes to arbitrarily large binary ripple coun-S0 S1D QQ S2D QQ S3D QQ D QQa 4−bit binary ripple counter Z0 Z1 Z2 Z3CLK ters, but the time required for the outputs to settle after a clock edge scales with the number of ﬂip-ﬂops in the counter. On the other hand, an average of only two ﬂip-ﬂops s ee each clock edge (1+1 /2+1/4+...), which reduces the power requirements.8 8Recall that ﬂip-ﬂops record the clock state internally. The logical activity required to record such state consumes ene rgy.","{'page_number': 91, 'textbook_name': 'ECE-120-student-notes', 'text': 'As you can see from the ﬁgure, the technique gen- eralizes to arbitrarily large binary ripple coun-S0 S1D QQ S2D QQ S3D QQ D QQa 4−bit binary ripple counter Z0 Z1 Z2 Z3CLK ters, but the time required for the outputs to settle after a clock edge scales with the number of ﬂip-ﬂops in the counter. On the other hand, an average of only two ﬂip-ﬂops s ee each clock edge (1+1 /2+1/4+...), which reduces the power requirements.8 8Recall that ﬂip-ﬂops record the clock state internally. The logical activity required to record such state consumes ene rgy.'}"
"3.1 Serialization and Finite State Machines 87 Beginning with the state 0000, at the rising clock edge, the left ( S0) ﬂip-ﬂop toggles to 1. The second ( S1) ﬂip-ﬂop sees this change as a falling clock edge and does nothing, leav ing the counter in state 0001. When the next rising clock edge arrives, the left ﬂip-ﬂop toggles back to 0 , which the second ﬂip-ﬂop sees as a rising clock edge, causing it to toggle to 1. The third ( S2) ﬂip-ﬂop sees the second ﬂip-ﬂop’s change as a falling edge and does nothing, and the state settles as 0010. We leave ver iﬁcation of the remainder of the cycle as an exercise. 3.1.6 Timing Issues* Ripple counters are a form of a more general strategy known as clo ck gating.9Clock gating uses logic to control the visibility of a clock signal to ﬂip-ﬂops (or latches). Histo rically, digital system designers rarely used clock gating techniques because of the complexity introduced for the circuit designers, who must ensure that clock edges are delivered with little skew along a dynamically chang ing set of paths to ﬂip-ﬂops. Today, however, the power beneﬁts of hiding the clock signal from ﬂip-ﬂop s have made clock gating an attractive strategy. Nevertheless, digital logic designers and computer arc hitects still almost never use clock gating strategies directly. In most of the industry, CAD tools insert logic f or clock gating automatically. A handful of companies (such as Intel and Apple/Samsung) design custom cir cuits rather than relying on CAD tools to synthesize hardware designs from standard libraries of element s. In these companies, clock gating is used widely by the circuit design teams, and some input is occasionally neces sary from the higher-level designers. More aggressive gating strategies are also used in modern designs, but these usually require more time to transition between the on and oﬀ states and can be more diﬃcult to g et right automatically (with the tools), hence hardware designers may need to provide high-level informat ion about their designs. A ﬂip-ﬂop that does not see any change in its clock input still has connections to high voltage and ground, and thus allows a small amount of leakage current . In contrast, with power gating , the voltage diﬀerence is removed, and the circuit uses no power at all. Power gating can be tricky—as yo u know, for example, when you turn the power on, you need to make sure that each latch settles into a s table state. Latches may need to be initialized to guarantee that they settle, which requires time after t he power is restored. If you want a deeper understanding of gating issues, take ECE482 , Digital Integrated Circuit Design, or ECE527, System-on-a-Chip Design. 3.1","{'page_number': 92, 'textbook_name': 'ECE-120-student-notes', 'text': 'Power gating can be tricky—as yo u know, for example, when you turn the power on, you need to make sure that each latch settles into a s table state. Latches may need to be initialized to guarantee that they settle, which requires time after t he power is restored. If you want a deeper understanding of gating issues, take ECE482 , Digital Integrated Circuit Design, or ECE527, System-on-a-Chip Design. 3.1.7 Machine Models Before we dive fully into FSM design, we must point out that we have pla ced a somewhat artiﬁcial restriction on the types of FSMs that we use in our course. Historically, this res triction was given a name, and machines of the type that we have discussed are called Moore machines. Howe ver, outside of introductory classes, almost no one cares about this name, nor about the name for the mo re generalmodel used almost universally in hardware design, Mealy machines. What is the diﬀerence? In a Moore machine , outputs depend only on the internal state bits of the FSM (the values stored in the ﬂip-ﬂops). In a Mealy machine , outputs may be expressed as functions both of internal state and FSM inputs. As we illustrate shortly, the bene ﬁt of using input signals to calculate outputs (the Mealy machine model) is that input bits eﬀectively serve as additional system state, which means that the number of internal state bits can be reduced. The disadvantage of including input signals in the expressions for output signals is that timing characteristics of input signals may not be known, whereas an FSM designer may want to guarantee certain timing characterist ics for output signals. In practice, when such timing guarantees are needed, the designe r simply adds state to the FSM to accom- modate the need, and the problem is solved. The coin-counting FSM t hat we designed for our class’ lab assignments, for example, required that we use a Moore machine mo del to avoidsending the servocontrolling the coin’s path an output pulse that was too short to enforce the F SM’s decision about which way to send the coin. By adding more states to the FSM, we were able to hold the s ervo in place, as desired. 9Fall 2012 students: This part may seem a little redundant, bu t we’re going to remove the earlier mention of clock gating in future semesters.'}"
"Power gating can be tricky—as yo u know, for example, when you turn the power on, you need to make sure that each latch settles into a s table state. Latches may need to be initialized to guarantee that they settle, which requires time after t he power is restored. If you want a deeper understanding of gating issues, take ECE482 , Digital Integrated Circuit Design, or ECE527, System-on-a-Chip Design. 3.1.7 Machine Models Before we dive fully into FSM design, we must point out that we have pla ced a somewhat artiﬁcial restriction on the types of FSMs that we use in our course. Historically, this res triction was given a name, and machines of the type that we have discussed are called Moore machines. Howe ver, outside of introductory classes, almost no one cares about this name, nor about the name for the mo re generalmodel used almost universally in hardware design, Mealy machines. What is the diﬀerence? In a Moore machine , outputs depend only on the internal state bits of the FSM (the values stored in the ﬂip-ﬂops). In a Mealy machine , outputs may be expressed as functions both of internal state and FSM inputs. As we illustrate shortly, the bene ﬁt of using input signals to calculate outputs (the Mealy machine model) is that input bits eﬀectively serve as additional system state, which means that the number of internal state bits can be reduced. The disadvantage of including input signals in the expressions for output signals is that timing characteristics of input signals may not be known, whereas an FSM designer may want to guarantee certain timing characterist ics for output signals. In practice, when such timing guarantees are needed, the designe r simply adds state to the FSM to accom- modate the need, and the problem is solved. The coin-counting FSM t hat we designed for our class’ lab assignments, for example, required that we use a Moore machine mo del to avoidsending the servocontrolling the coin’s path an output pulse that was too short to enforce the F SM’s decision about which way to send the coin. By adding more states to the FSM, we were able to hold the s ervo in place, as desired. 9Fall 2012 students: This part may seem a little redundant, bu t we’re going to remove the earlier mention of clock gating in future semesters.","{'page_number': 92, 'textbook_name': 'ECE-120-student-notes', 'text': 'Power gating can be tricky—as yo u know, for example, when you turn the power on, you need to make sure that each latch settles into a s table state. Latches may need to be initialized to guarantee that they settle, which requires time after t he power is restored. If you want a deeper understanding of gating issues, take ECE482 , Digital Integrated Circuit Design, or ECE527, System-on-a-Chip Design. 3.1.7 Machine Models Before we dive fully into FSM design, we must point out that we have pla ced a somewhat artiﬁcial restriction on the types of FSMs that we use in our course. Historically, this res triction was given a name, and machines of the type that we have discussed are called Moore machines. Howe ver, outside of introductory classes, almost no one cares about this name, nor about the name for the mo re generalmodel used almost universally in hardware design, Mealy machines. What is the diﬀerence? In a Moore machine , outputs depend only on the internal state bits of the FSM (the values stored in the ﬂip-ﬂops). In a Mealy machine , outputs may be expressed as functions both of internal state and FSM inputs. As we illustrate shortly, the bene ﬁt of using input signals to calculate outputs (the Mealy machine model) is that input bits eﬀectively serve as additional system state, which means that the number of internal state bits can be reduced. The disadvantage of including input signals in the expressions for output signals is that timing characteristics of input signals may not be known, whereas an FSM designer may want to guarantee certain timing characterist ics for output signals. In practice, when such timing guarantees are needed, the designe r simply adds state to the FSM to accom- modate the need, and the problem is solved. The coin-counting FSM t hat we designed for our class’ lab assignments, for example, required that we use a Moore machine mo del to avoidsending the servocontrolling the coin’s path an output pulse that was too short to enforce the F SM’s decision about which way to send the coin. By adding more states to the FSM, we were able to hold the s ervo in place, as desired. 9Fall 2012 students: This part may seem a little redundant, bu t we’re going to remove the earlier mention of clock gating in future semesters.'}"
"88  Why are we protecting you from the model used in practice? First, t iming issues add complexity to a topic that is complex enough for an introductory course. And, second, most software FSMs are Moore machines, so the abstraction is a useful one in that context, too. In many design contexts, the timing issues implied by a Mealy model can be relatively simple to manage. When working in a single clock domain, all of the input signals come from ﬂ ip-ﬂops in the same domain, and are thus stable for most of the clock cycle. Only rarely does one nee d to keep additional state to improve timing characteristics in these contexts. In contrast, when inter acting across clock domains, more care is sometimes needed to ensure correct behavior. We now illustrate the state reduction beneﬁt of the Mealy machine mo del with a simple example, an FSM that recognizes the pattern of a 0 followed by a 1 on a single input and outputs a 1 when it observes the pattern. As already mentioned, Mealy machines often require fewe r ﬂip-ﬂops. Intuitively, the number of combinations of states and inputs is greater than the number of co mbinations of states alone, and allowing a function to depend on inputs reduces the number of internal sta tes needed. A Mealy implementation of the FSM appears on the left below, and an ex ample timing diagram illustrating the FSM’s behavior is shown on the right. The machine shown below occ upies state A when the last bit seen was a 0, and state B when the last bit seen was a 1. Notice that t he transition arcs in the state diagram are labeled with two values instead of one. Since outputs can depend on input values as well as state, transitions in a Mealy machine are labeled with input/output co mbinations, while states are labeled only with their internal bits (or just their names, as shown below). L abeling states with outputs does not make sense for a Mealy machine, since outputs may vary with inputs. Notice that the outputs indicated on any given transition hold only until that transition is taken (at the ris ing clock edge), as is apparent in the timing diagram. When inputs are asynchronous, that is, not driven b y the same clock signal, output pulses from a Mealy machine can be arbitrarily short, which can lead to proble ms. OUTINCLK OUT falls at rising CLK OUT rises with INA 1/01/1 0/0 0/0B For a Moore machine, we must create a special state in which the out put is high. Doing so requires that we split state B into two states, a state C in which the last two bits seen w ere 01, and a state D in which the last two bits seen were 11. Only state C generates output 1. State D also becomes the starting state for the new state machine. The state diagram on the left below illustrates th e changes, using the transition diagram style that we introduced earlier to represent Moore machines. Not ice in the associated timing diagram that the output pulse lasts a full clock cycle. OUT rises with CLK OUT falls at rising CLKOUTCLK INA/0 C/1 D/001 0 101","{'page_number': 93, 'textbook_name': 'ECE-120-student-notes', 'text': '88  Why are we protecting you from the model used in practice? First, t iming issues add complexity to a topic that is complex enough for an introductory course. And, second, most software FSMs are Moore machines, so the abstraction is a useful one in that context, too. In many design contexts, the timing issues implied by a Mealy model can be relatively simple to manage. When working in a single clock domain, all of the input signals come from ﬂ ip-ﬂops in the same domain, and are thus stable for most of the clock cycle. Only rarely does one nee d to keep additional state to improve timing characteristics in these contexts. In contrast, when inter acting across clock domains, more care is sometimes needed to ensure correct behavior. We now illustrate the state reduction beneﬁt of the Mealy machine mo del with a simple example, an FSM that recognizes the pattern of a 0 followed by a 1 on a single input and outputs a 1 when it observes the pattern. As already mentioned, Mealy machines often require fewe r ﬂip-ﬂops. Intuitively, the number of combinations of states and inputs is greater than the number of co mbinations of states alone, and allowing a function to depend on inputs reduces the number of internal sta tes needed. A Mealy implementation of the FSM appears on the left below, and an ex ample timing diagram illustrating the FSM’s behavior is shown on the right. The machine shown below occ upies state A when the last bit seen was a 0, and state B when the last bit seen was a 1. Notice that t he transition arcs in the state diagram are labeled with two values instead of one. Since outputs can depend on input values as well as state, transitions in a Mealy machine are labeled with input/output co mbinations, while states are labeled only with their internal bits (or just their names, as shown below). L abeling states with outputs does not make sense for a Mealy machine, since outputs may vary with inputs. Notice that the outputs indicated on any given transition hold only until that transition is taken (at the ris ing clock edge), as is apparent in the timing diagram. When inputs are asynchronous, that is, not driven b y the same clock signal, output pulses from a Mealy machine can be arbitrarily short, which can lead to proble ms. OUTINCLK OUT falls at rising CLK OUT rises with INA 1/01/1 0/0 0/0B For a Moore machine, we must create a special state in which the out put is high. Doing so requires that we split state B into two states, a state C in which the last two bits seen w ere 01, and a state D in which the last two bits seen were 11. Only state C generates output 1. State D also becomes the starting state for the new state machine. The state diagram on the left below illustrates th e changes, using the transition diagram style that we introduced earlier to represent Moore machines. Not ice in the associated timing diagram that the output pulse lasts a full clock cycle. OUT rises with CLK OUT falls at rising CLKOUTCLK INA/0 C/1 D/001 0 101'}"
"3.2 Finite State Machine Design Examples, Part I 89 ECE120: Introduction to Computer Engineering Notes Set 3.2 Finite State Machine Design Examples, Part I This set of notes uses a series of examples to illustrate design princip les for the implementation of ﬁnite state machines (FSMs) using digital logic. We begin with an overview of the design process for a digital FSM, from the development of an abstract model through the imple mentation of functions for the next-state variables and output signals. Our ﬁrst few examples cover only the c oncrete aspects: we implement several counters, which illustrate the basic process of translating a concr ete and complete state transition diagram into an implementation based on ﬂip-ﬂops and logic gates. We next con sider a counter with a number of states that is not a power of two, with which we illustrate the need fo r FSM initialization. We then consider the design process as a whole through a more gene ral example of a counter with multiple inputs to control its behavior. We work from an abstract model do wn to an implementation, illustrating how semantic knowledge from the abstract model can be used to sim plify the implementation. Finally, we illustrate how the choice of representation for the FSM’s internal s tate aﬀects the complexity of the imple- mentation. Fortunately, designs that are more intuitive and easier for humans to understand also typically make the best designs in terms of other metrics, such as logic comple xity. 3.2.1 Steps in the Design Process Before we begin exploring designs, let’s talk brieﬂy about the genera l approach that we take when designing an FSM. We follow a six-step process: 1. develop an abstract model 2. specify I/O behavior 3. complete the speciﬁcation 4. choose a state representation 5. calculate logic expressions 6. implement with ﬂip-ﬂops and gates In Step 1, we translate our description in human language into a mode l with states and desired behavior. At this stage, we simply try to capture the intent of the description and are not particularly thorough nor exact. Step 2 begins to formalize the model, starting with its input and outpu t behavior. If we eventually plan to develop an implementation of our FSM as a digital system (which is no t the only choice, of course!), all input and output must consist of bits. Often, input and/or output speciﬁcations may need to match other digital systems to which we plan to connect our FSM. In fact, most problems in developing large digital systems today arise because of incompatibilities when comp osing two or more separately designed pieces (or modules ) into an integrated system. Once we know the I/O behavior for our FSM, in Step 3 we start to mak e any implicit assumptions clear and to make any other decisions necessary to the design. Occasion ally, we may choose to leave something undecided in the hope of simplifying the design with “don’t care” entrie s in the logic formulation. In Step 4, we select an internal representation for the bits neces sary to encode the state of our FSM. In practice, for small designs, this representationcan be selected b y a computer in such a way as to optimize the implementation","{'page_number': 94, 'textbook_name': 'ECE-120-student-notes', 'text': 'Occasion ally, we may choose to leave something undecided in the hope of simplifying the design with “don’t care” entrie s in the logic formulation. In Step 4, we select an internal representation for the bits neces sary to encode the state of our FSM. In practice, for small designs, this representationcan be selected b y a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instr uction set architecture that we study later in this class, humans do most of the work by hand. In the later examp les in this set of notes, we show how even a small design can leverage meaningful information from the de sign when selecting the representation, leading to an implementation that is simpler and is easier to build correct ly. We also show how one can use abstraction to simplify an implementation. By Step 5, our design is a complete speciﬁcation in terms of bits, and w e need merely derive logic expressions for the next-state variables and the output signals. This process is no diﬀerent than for combinational logic, and should already be fairly familiar to you.'}"
"Occasion ally, we may choose to leave something undecided in the hope of simplifying the design with “don’t care” entrie s in the logic formulation. In Step 4, we select an internal representation for the bits neces sary to encode the state of our FSM. In practice, for small designs, this representationcan be selected b y a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instr uction set architecture that we study later in this class, humans do most of the work by hand. In the later examp les in this set of notes, we show how even a small design can leverage meaningful information from the de sign when selecting the representation, leading to an implementation that is simpler and is easier to build correct ly. We also show how one can use abstraction to simplify an implementation. By Step 5, our design is a complete speciﬁcation in terms of bits, and w e need merely derive logic expressions for the next-state variables and the output signals. This process is no diﬀerent than for combinational logic, and should already be fairly familiar to you.","{'page_number': 94, 'textbook_name': 'ECE-120-student-notes', 'text': 'Occasion ally, we may choose to leave something undecided in the hope of simplifying the design with “don’t care” entrie s in the logic formulation. In Step 4, we select an internal representation for the bits neces sary to encode the state of our FSM. In practice, for small designs, this representationcan be selected b y a computer in such a way as to optimize the implementation. However, for large designs, such as the LC-3 instr uction set architecture that we study later in this class, humans do most of the work by hand. In the later examp les in this set of notes, we show how even a small design can leverage meaningful information from the de sign when selecting the representation, leading to an implementation that is simpler and is easier to build correct ly. We also show how one can use abstraction to simplify an implementation. By Step 5, our design is a complete speciﬁcation in terms of bits, and w e need merely derive logic expressions for the next-state variables and the output signals. This process is no diﬀerent than for combinational logic, and should already be fairly familiar to you.'}"
"90  Finally, in Step 6, we translate our logic expressions into gates and us e ﬂip-ﬂops (or registers) to hold the internal state bits of the FSM. In later notes, we use more complex building blocks when implementing an FSM, building up abstractions in order to simplify the design process in much the same way that we have shown for combinational logic. 3.2.2 Example: A Two-Bit Gray Code Counter Let’s begin with a two-bit Gray code counter with no inputs. As we men tioned in Notes Set 2.1, a Gray code is a cycle over all bit patterns of a certain length in which consec utive patterns diﬀer in exactly one bit. For simplicity, our ﬁrst few examples are based on counters and use the internal state of the FSM as the output values. You should already know how to design combinatio nal logic for the outputs if it were necessary. The inputs to a counter, if any, are typically limited to fu nctions such as starting and stopping the counter, controlling the counting direction, and resetting the counter to a particular state. A fully-speciﬁed transition diagram for a two-bit Gray code counter appears below. With no inputs, the states simply form a loop, with the counter moving from one state to the next each cycle. Each state in the diagram is marked with the internal state value S1S0(before the “/”) and the output Z1Z0(after the “/”), which are always equal for this counter. Based on the trans ition diagram, we can ﬁll in the K-maps for the next-state values S+ 1andS+ 0as shown to the right of the transition diagram, then derive algebra ic expressions in the usual way to obtain S+ 1=S0andS+ 0=S1. We then use the next-state logic to develop the implementation shown on the far right, completing our ﬁrst coun ter design. COUNT A COUNT B COUNT C COUNT D 00/00 01/01 11/11 10/100 1S1 S1S0 0 1+ 0 1 10 1S0 S1S0 0 1+ 0 1 01 0D QQ S1 D QQ S0Z1 Z0CLOCK a two−bit Gray code counter 3.2.3 Example: A Three-Bit Gray Code Counter Now we’ll add a third bit to our counter, but again use a Gray code as the basis for the state sequence. A fully-speciﬁed transition diagram for such a counter appears to the right. As before, with no inputs, the states simply form a loop, with the counter moving fromonestatetothenexteachcycle. Eachstateinthe diagramismarkedwiththeinternalstatevalue S2S1S0 (before “/”) and the output Z2Z1Z0(after “/”)","{'page_number': 95, 'textbook_name': 'ECE-120-student-notes', 'text': 'A fully-speciﬁed transition diagram for such a counter appears to the right. As before, with no inputs, the states simply form a loop, with the counter moving fromonestatetothenexteachcycle. Eachstateinthe diagramismarkedwiththeinternalstatevalue S2S1S0 (before “/”) and the output Z2Z1Z0(after “/”).COUNT A COUNT B COUNT C COUNT D 000/000 001/001 011/011 010/010 100/100 101/101 111/111 110/110COUNT E COUNT F COUNT G COUNT H Based on the transition diagram, we can ﬁll in the K-maps for the next-state values S+ 2, S+ 1, andS+ 0as shown to the right, then derive algebraic expressions. The results are more complex this time.S2+S2 S1S0 00 01 11 10 0 100 0 1 1110S2+S1 S1S0 00 01 11 10 0 10 01 1 1 100S2+S0 S1S0 00 01 11 10 0 1 1 01 01 0 1 0 For our next-state logic, we obtain: S+ 2=S2S0+S1S0 S+ 1=S2S0+S1S0 S+ 0=S2S1+S2S1'}"
"A fully-speciﬁed transition diagram for such a counter appears to the right. As before, with no inputs, the states simply form a loop, with the counter moving fromonestatetothenexteachcycle. Eachstateinthe diagramismarkedwiththeinternalstatevalue S2S1S0 (before “/”) and the output Z2Z1Z0(after “/”).COUNT A COUNT B COUNT C COUNT D 000/000 001/001 011/011 010/010 100/100 101/101 111/111 110/110COUNT E COUNT F COUNT G COUNT H Based on the transition diagram, we can ﬁll in the K-maps for the next-state values S+ 2, S+ 1, andS+ 0as shown to the right, then derive algebraic expressions. The results are more complex this time.S2+S2 S1S0 00 01 11 10 0 100 0 1 1110S2+S1 S1S0 00 01 11 10 0 10 01 1 1 100S2+S0 S1S0 00 01 11 10 0 1 1 01 01 0 1 0 For our next-state logic, we obtain: S+ 2=S2S0+S1S0 S+ 1=S2S0+S1S0 S+ 0=S2S1+S2S1","{'page_number': 95, 'textbook_name': 'ECE-120-student-notes', 'text': 'A fully-speciﬁed transition diagram for such a counter appears to the right. As before, with no inputs, the states simply form a loop, with the counter moving fromonestatetothenexteachcycle. Eachstateinthe diagramismarkedwiththeinternalstatevalue S2S1S0 (before “/”) and the output Z2Z1Z0(after “/”).COUNT A COUNT B COUNT C COUNT D 000/000 001/001 011/011 010/010 100/100 101/101 111/111 110/110COUNT E COUNT F COUNT G COUNT H Based on the transition diagram, we can ﬁll in the K-maps for the next-state values S+ 2, S+ 1, andS+ 0as shown to the right, then derive algebraic expressions. The results are more complex this time.S2+S2 S1S0 00 01 11 10 0 100 0 1 1110S2+S1 S1S0 00 01 11 10 0 10 01 1 1 100S2+S0 S1S0 00 01 11 10 0 1 1 01 01 0 1 0 For our next-state logic, we obtain: S+ 2=S2S0+S1S0 S+ 1=S2S0+S1S0 S+ 0=S2S1+S2S1'}"
"3.2 Finite State Machine Design Examples, Part I 91 Notice that the equations for S+ 2andS+ 1share a com- mon term, S1S0. This design does not allow much choice in developing good equations for the next-state logic, but some designs may enable you to reduce the designcomplexity byexplicitly identifying and making use ofcommonalgebraicterms andsub-expressionsfor diﬀerent outputs. In modern design processes, identi- fying such opportunities is generally performed by a computer program, but it’s important to understand howthey arise. Notethat thecommontermbecomesa single AND gate in the implementation of our counter, as shown to the right. Looking at the counter’s implementation diagram, no- tice that the vertical lines carrying the current state values and their inverses back to the next state logic inputs have been carefully ordered to simplify under- standing the diagram. In particular, they are ordered from left to right (on the left side of the ﬁgure) as S0S0S1S1S2S2. When designing any logic diagram, be sure to make use of a reasonable order so as to make it easy for someone (including yourself!) to read and check the correctness of the logic.D QQ S1 D QQ S0D QQ S2 Z0Z1Z2CLOCK a three−bit Gray code counter 3.2.4 Example: A Color Sequencer Early graphics systems used a three-bit red-green-blue (RGB) en coding for colors. The color mapping for such a system is shown to the right. Imagine that you are charged with creating a counter to drive a light through a sequence of colors. The light takes an RGB input as just described , and the desired pattern is oﬀ (black) yellow violet green blue You immediately recognize that you merely need a counter with ﬁve st ates. How many ﬂip-ﬂops will we need? At least three, since ⌈log2(5)⌉= 3. Given that we need three ﬂip-ﬂops, and that the colors we need to p roduce asRGBcolor 000black 001blue 010green 011cyan 100red 101violet 110yellow 111white outputs are all unique bit patterns, we can again choose to use the counter’s internal state directly as our output values. A fully-speciﬁed transition diagram for our color sequencer appears to the right. The states again form a loop, and are marked with the internal state value S2S1S0and the output RGB.000/000BLACK YELLOW VIOLET 110/110 101/101GREEN 010/010BLUE 001/001 As before, we can use the transition dia- gramtoﬁllinK-mapsforthenext-stateval- uesS+ 2,S+ 1, andS+ 0, as shown to the right","{'page_number': 96, 'textbook_name': 'ECE-120-student-notes', 'text': 'The states again form a loop, and are marked with the internal state value S2S1S0and the output RGB.000/000BLACK YELLOW VIOLET 110/110 101/101GREEN 010/010BLUE 001/001 As before, we can use the transition dia- gramtoﬁllinK-mapsforthenext-stateval- uesS+ 2,S+ 1, andS+ 0, as shown to the right. For each of the three states not included in ourtransitiondiagram,we haveinsertedx’sS2+S2 S1S0 00 01 11 10 0 10 11 x 0 x 0 xS2+S1 S1S0 00 01 11 10 0 101 x 0 x x1 0S2+S0 S1S0 00 01 11 10 0 10 1x x 0 x0 1 into the K-maps to indicate “don’t care.” As you know, we can treat e ach x as either a 0 or a 1, whichever produces better results (where “better” usually means simpler eq uations). The terms that we have chosen for our algebraic equations are illustrated in the K-maps. The x’s with in the ellipses become 1s in the implementation, and the x’s outside of the ellipses become 0s.'}"
"The states again form a loop, and are marked with the internal state value S2S1S0and the output RGB.000/000BLACK YELLOW VIOLET 110/110 101/101GREEN 010/010BLUE 001/001 As before, we can use the transition dia- gramtoﬁllinK-mapsforthenext-stateval- uesS+ 2,S+ 1, andS+ 0, as shown to the right. For each of the three states not included in ourtransitiondiagram,we haveinsertedx’sS2+S2 S1S0 00 01 11 10 0 10 11 x 0 x 0 xS2+S1 S1S0 00 01 11 10 0 101 x 0 x x1 0S2+S0 S1S0 00 01 11 10 0 10 1x x 0 x0 1 into the K-maps to indicate “don’t care.” As you know, we can treat e ach x as either a 0 or a 1, whichever produces better results (where “better” usually means simpler eq uations). The terms that we have chosen for our algebraic equations are illustrated in the K-maps. The x’s with in the ellipses become 1s in the implementation, and the x’s outside of the ellipses become 0s.","{'page_number': 96, 'textbook_name': 'ECE-120-student-notes', 'text': 'The states again form a loop, and are marked with the internal state value S2S1S0and the output RGB.000/000BLACK YELLOW VIOLET 110/110 101/101GREEN 010/010BLUE 001/001 As before, we can use the transition dia- gramtoﬁllinK-mapsforthenext-stateval- uesS+ 2,S+ 1, andS+ 0, as shown to the right. For each of the three states not included in ourtransitiondiagram,we haveinsertedx’sS2+S2 S1S0 00 01 11 10 0 10 11 x 0 x 0 xS2+S1 S1S0 00 01 11 10 0 101 x 0 x x1 0S2+S0 S1S0 00 01 11 10 0 10 1x x 0 x0 1 into the K-maps to indicate “don’t care.” As you know, we can treat e ach x as either a 0 or a 1, whichever produces better results (where “better” usually means simpler eq uations). The terms that we have chosen for our algebraic equations are illustrated in the K-maps. The x’s with in the ellipses become 1s in the implementation, and the x’s outside of the ellipses become 0s.'}"
"92  For our next-state logic, we obtain: S+ 2=S2S1+S1S0 S+ 1=S2S0+S1S0 S+ 0=S1 Again our equations for S+ 2andS+ 1share a common term, which becomes a single AND gate in the imple- mentation shown to the right. D QQ S1 D QQ S0D QQ S2CLOCK an RGB color sequencerR G B 3.2.5 Identifying an Initial State Let’s say that you go the lab and build the implementation above, hook it up to the light, and turn it on. Does it work? Sometimes. Sometimes it works perfectly, but sometim es the light glows cyan or red brieﬂy ﬁrst. At other times, the light is an unchanging white. What could be going wrong? Let’s try to understand. We begin by deriving K-maps for the implementation, as shown to the right. In these K-maps, each of the x’s in our design has been replaced by either a 0 or a 1. These entries are highlighted with green italics.S2+S2 S1S0 00 01 11 10 0 10 11 0 0 1 10 S2+S1 S1S0 00 01 11 10 0 101 0 1 0 1 10 S2+S0 S1S0 00 01 11 10 0 10 1 00 1 0 11 Now let’s imagine what might happen if somehow our FSM got into the S2S1S0= 111 state. In such a state, the light would appear white, since RGB=S2S1S0= 111. What happens in the next cycle? Plugging into the equations or looking into the K-maps gives (of course) the s ame answer: the next state is the S+ 2S+ 1S+ 0= 111 state. In other words, the light stays white indeﬁnitely! As an exercise, you should check what happens if the light is red or cyan. We can extend the transition diagram that we developed for our des ign with the extra states possible in the implementation, as shown below. As with the ﬁve states in the design, the extra states are named with the color of light that they produce. BLUE 001/001CYAN 011/011WHITE 111/111RED 100/100 000/000BLACK YELLOW VIOLET 110/110 101/101GREEN 010/010 Notice that the FSM does not move out of the WHITE state (ever). You may at this point wonder whether more careful decisions in selecting our next-state expressions mig ht address this issue. To some extent, yes. For example, if we replace the S2S1term in the equation for S+ 2withS2S0, a decision allowed by the “don’t care” boxes in the K-map for our design, the resulting transition dia gram does not suﬀer from the problem that we’ve found. However, even if we do change our implementation slightly, we need to address another aspect of the problem: how can the FSM ever get into the unexpect ed states?","{'page_number': 97, 'textbook_name': 'ECE-120-student-notes', 'text': '92  For our next-state logic, we obtain: S+ 2=S2S1+S1S0 S+ 1=S2S0+S1S0 S+ 0=S1 Again our equations for S+ 2andS+ 1share a common term, which becomes a single AND gate in the imple- mentation shown to the right. D QQ S1 D QQ S0D QQ S2CLOCK an RGB color sequencerR G B 3.2.5 Identifying an Initial State Let’s say that you go the lab and build the implementation above, hook it up to the light, and turn it on. Does it work? Sometimes. Sometimes it works perfectly, but sometim es the light glows cyan or red brieﬂy ﬁrst. At other times, the light is an unchanging white. What could be going wrong? Let’s try to understand. We begin by deriving K-maps for the implementation, as shown to the right. In these K-maps, each of the x’s in our design has been replaced by either a 0 or a 1. These entries are highlighted with green italics.S2+S2 S1S0 00 01 11 10 0 10 11 0 0 1 10 S2+S1 S1S0 00 01 11 10 0 101 0 1 0 1 10 S2+S0 S1S0 00 01 11 10 0 10 1 00 1 0 11 Now let’s imagine what might happen if somehow our FSM got into the S2S1S0= 111 state. In such a state, the light would appear white, since RGB=S2S1S0= 111. What happens in the next cycle? Plugging into the equations or looking into the K-maps gives (of course) the s ame answer: the next state is the S+ 2S+ 1S+ 0= 111 state. In other words, the light stays white indeﬁnitely! As an exercise, you should check what happens if the light is red or cyan. We can extend the transition diagram that we developed for our des ign with the extra states possible in the implementation, as shown below. As with the ﬁve states in the design, the extra states are named with the color of light that they produce. BLUE 001/001CYAN 011/011WHITE 111/111RED 100/100 000/000BLACK YELLOW VIOLET 110/110 101/101GREEN 010/010 Notice that the FSM does not move out of the WHITE state (ever). You may at this point wonder whether more careful decisions in selecting our next-state expressions mig ht address this issue. To some extent, yes. For example, if we replace the S2S1term in the equation for S+ 2withS2S0, a decision allowed by the “don’t care” boxes in the K-map for our design, the resulting transition dia gram does not suﬀer from the problem that we’ve found. However, even if we do change our implementation slightly, we need to address another aspect of the problem: how can the FSM ever get into the unexpect ed states?'}"
"3.2 Finite State Machine Design Examples, Part I 93 What is the initial state of the three ﬂip-ﬂops in our implementation? The initial state may not even be 0s and 1s unless we have an explicit mechanism for initialization. Initialization can work in two ways. The ﬁrst approach makes use of the ﬂip-ﬂop design. As you know, a ﬂip-ﬂop is built from a pair of latches, and we can make use of the internal reset lines on these latches to force each ﬂip-ﬂop into the 0 state (or the 1 state) using an addi- tional input. Alternatively, we can add some extra logictoourdesign. Consideraddinga few AND gates and a RESET input (active low), as shown in the dashed box in the ﬁgure to the right. In this case, when we assert RESET by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state. The approach taken here is for clarity; one can opti- mize the design, if desired. For exam- ple, we could simply connect RESET as an extra input into the three AND gates on the left rather than adding new ones, with the same eﬀect. We may sometimes want a more pow- erful initialization mechanism—one that allows us to force the FSM into any speciﬁc state in the next cycle. In such a case, we can add multi- plexers to each of our ﬂip-ﬂop in- puts, allowing us to use the INIT input to choose between normal op- eration ( INIT= 0) of the FSM and forcing the FSM into the next state given by I2I1I0(whenINIT= 1).D QQ S1 D QQ S0D QQ S2 an RGB color sequencer with resetCLOCK R G BRESET S2 S1 S0D QQ D QQ D QQI2 I0I1 1 0 1 01 0 an RGB color sequencer with arbitrary initializationINIT CLOCK R G B 3.2.6 Developing an Abstract Model We are now ready to discuss the design process for an FSM from start to ﬁnish. For this ﬁrst abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We nowwantacounterthatallowsustostartandstopthestate no input halt button go button counting counting halted halted halted counting count. What is the mechanism for stopping and starting? To begin ou r design, we could sketch out an abstract next-state table such as the one shown to the right abo ve. In this form of the table, the ﬁrst column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination. The table contains two states, c ounting and halted, and speciﬁes that the design uses two distinct buttons to move between the states.","{'page_number': 98, 'textbook_name': 'ECE-120-student-notes', 'text': '3.2 Finite State Machine Design Examples, Part I 93 What is the initial state of the three ﬂip-ﬂops in our implementation? The initial state may not even be 0s and 1s unless we have an explicit mechanism for initialization. Initialization can work in two ways. The ﬁrst approach makes use of the ﬂip-ﬂop design. As you know, a ﬂip-ﬂop is built from a pair of latches, and we can make use of the internal reset lines on these latches to force each ﬂip-ﬂop into the 0 state (or the 1 state) using an addi- tional input. Alternatively, we can add some extra logictoourdesign. Consideraddinga few AND gates and a RESET input (active low), as shown in the dashed box in the ﬁgure to the right. In this case, when we assert RESET by setting it to 0, the FSM moves to state 000 in the next cycle, putting it into the BLACK state. The approach taken here is for clarity; one can opti- mize the design, if desired. For exam- ple, we could simply connect RESET as an extra input into the three AND gates on the left rather than adding new ones, with the same eﬀect. We may sometimes want a more pow- erful initialization mechanism—one that allows us to force the FSM into any speciﬁc state in the next cycle. In such a case, we can add multi- plexers to each of our ﬂip-ﬂop in- puts, allowing us to use the INIT input to choose between normal op- eration ( INIT= 0) of the FSM and forcing the FSM into the next state given by I2I1I0(whenINIT= 1).D QQ S1 D QQ S0D QQ S2 an RGB color sequencer with resetCLOCK R G BRESET S2 S1 S0D QQ D QQ D QQI2 I0I1 1 0 1 01 0 an RGB color sequencer with arbitrary initializationINIT CLOCK R G B 3.2.6 Developing an Abstract Model We are now ready to discuss the design process for an FSM from start to ﬁnish. For this ﬁrst abstract FSM example, we build upon something that we have already seen: a two-bit Gray code counter. We nowwantacounterthatallowsustostartandstopthestate no input halt button go button counting counting halted halted halted counting count. What is the mechanism for stopping and starting? To begin ou r design, we could sketch out an abstract next-state table such as the one shown to the right abo ve. In this form of the table, the ﬁrst column lists the states, while each of the other columns lists states to which the FSM transitions after a clock cycle for a particular input combination. The table contains two states, c ounting and halted, and speciﬁes that the design uses two distinct buttons to move between the states.'}"
"94  A counter with a single counting state, ofcourse, does not provide much value. We extend the table with four counting states and four halted states, as shown to the right. This version of the table also introduces more formal state names, for which these notes use all capital letters. The upper four states represent uninterrupted counting, in which the counter cycles through these states indeﬁnitely. A user can stop the counter in any state by pressing the “halt” but- ton, causing the counter to retain its current value until the user presses the “go” button. Below the state table is an abstract transition diagram, which provides exactly the same infor- mation in graphical form. Here circles represent states (as labeled) and arcs represent transitions fromonestatetoanotherbasedonaninputcom- bination (which is used to label the arc). We have already implicitly made a few choices about our counter design. First, the counterstate no input halt button go button COUNT A COUNT B HALT A COUNT B COUNT C HALT B COUNT C COUNT D HALT C COUNT D COUNT A HALT D HALT A HALT A COUNT B HALT B HALT B COUNT C HALT C HALT C COUNT D HALT D HALT D COUNT A press haltpress haltpress haltpress haltpress go press go press go press goHALT BCOUNT C HALT D HALT C HALT ACOUNT D COUNT A COUNT B shownretainsthe currentstate ofthe system when “halt”ispres sed. We could instead resetthe counterstate whenever it is restarted, in which case we need only ﬁve states: fou r for counting and one more for a halted counter. Second, we’ve designed the counter to stop when the us er presses “halt” and to resume counting when the user presses “go.” We could instead choose to delay these eﬀects by a cycle. For example, pressing “halt” in state COUNT B could take the counter to state HALT C, and pressing “go” in state HALT C could take the system to state COUNT C. In these notes, we impleme nt only the diagrams shown. 3.2.7 Specifying I/O Behavior We next start to formalize our design by specifying its input and output behavior digitally. Each of the two control buttons provides a single bit of input. The “halt” button we call H, and the “go” button we callG. For the output, we use a two-bit Gray code. With these choices, we can redraw the transition dia- gram as show to the right. In this ﬁgure, the states are marked with output val- uesZ1Z0and transition arcs are labeled in terms of our two input buttons, GandH. The uninterrupted counting cycle is labeled with Hto indicate that it continues until we press H.H H H H G G G GH H H H G G G GCOUNT A /00COUNT B COUNT C COUNT D /10 /01 /11 HALT A /00HALT B /01HALT C /11HALT D /10 3.2.8 Completing the Speciﬁcation Now we need to think about how the system should behave if somethin g outside of our initial expectations occurs","{'page_number': 99, 'textbook_name': 'ECE-120-student-notes', 'text': 'H H H H G G G GH H H H G G G GCOUNT A /00COUNT B COUNT C COUNT D /10 /01 /11 HALT A /00HALT B /01HALT C /11HALT D /10 3.2.8 Completing the Speciﬁcation Now we need to think about how the system should behave if somethin g outside of our initial expectations occurs. Having drawn out a partial transition diagram can help with t his process, since we can use the diagram to systematically consider all possible input conditions from a ll possible states. The state table form can make the missing parts of the speciﬁcation even more obvio us.'}"
"H H H H G G G GH H H H G G G GCOUNT A /00COUNT B COUNT C COUNT D /10 /01 /11 HALT A /00HALT B /01HALT C /11HALT D /10 3.2.8 Completing the Speciﬁcation Now we need to think about how the system should behave if somethin g outside of our initial expectations occurs. Having drawn out a partial transition diagram can help with t his process, since we can use the diagram to systematically consider all possible input conditions from a ll possible states. The state table form can make the missing parts of the speciﬁcation even more obvio us.","{'page_number': 99, 'textbook_name': 'ECE-120-student-notes', 'text': 'H H H H G G G GH H H H G G G GCOUNT A /00COUNT B COUNT C COUNT D /10 /01 /11 HALT A /00HALT B /01HALT C /11HALT D /10 3.2.8 Completing the Speciﬁcation Now we need to think about how the system should behave if somethin g outside of our initial expectations occurs. Having drawn out a partial transition diagram can help with t his process, since we can use the diagram to systematically consider all possible input conditions from a ll possible states. The state table form can make the missing parts of the speciﬁcation even more obvio us.'}"
"3.2 Finite State Machine Design Examples, Part I 95 For our counter, the symmetry between counting states makes the problem substantially simpler. Let’s write out part of a list of states and part of a state table with one counting state and one halt state, as shown to the right. Four values of the inputs HGare possible (recall that Nbits allow 2Npossible patterns). We list the columns in Gray code order, since we may want to transcribe this ta- ble into K-maps later.state description ﬁrst counting state COUNT A counting, output Z1Z0= 00 ﬁrst halted state HALT A halted, output Z1Z0= 00 HG state 00 01 11 10 COUNT A COUNT B unspeciﬁed unspeciﬁed HALT A HALT A HALT A COUNT B unspeciﬁed unspeciﬁed Let’s start with the COUNT A state. We know that if neither button is pressed ( HG= 00), we want the counter to move to the COUNT B state. And, if we press the “halt” b utton (HG= 10), we want the counter to move to the HALT A state. What should happen if a user presses t he “go” button ( HG= 01)? Or if the user presses both buttons ( HG= 11)? Answering these questions is part of fully specifying our desig n. We can choose to leave some parts unspeciﬁed, but any implementation of our system will imply answers , and thus we must be careful. We choose to ignore the “go” button w hile counting, and to have the “halt” button override the “go” button. Thus, if HG= 01 when the counter is in state COUNT A, the counter moves to state COUNT B. And, if HG= 11, the counter moves to state HALT A. Use of explicit bit patterns for the inputs HGmay help you to check that all four possible input values are covered from each state. If you choose to use a transition diagra m instead of a state table, you might even want to add four arcs from each state, each labeled with a speciﬁc v alue ofHG. When two arcs connect the same two states, we can either use multiple labels or can indicate bits t hat do not matter using a don’t-care symbol,x. For example, the arc from state COUNT A to state COUNT B could be labeledHG= 00,01 or HG= 0x. The arc from state COUNT A to state HALT A could be labeled HG= 10,11 orHG= 1x. We can also use logical expressions as labels, but such notation can obs cure unspeciﬁed transitions. Now consider the state HALT A. The transitions speciﬁed so far are that when we press “go”( HG= 01), the counter moves to the COUNT B state, and that the counter remain s halted in state HALT A if no buttons are pressed ( HG= 00). What if the “halt” button is pressed ( HG= 10), or both buttons are pressed (HG= 11)? For consistency, we decide that “halt” overrides “go,” but d oes nothing special if it alone is pressed while the counter is halted","{'page_number': 100, 'textbook_name': 'ECE-120-student-notes', 'text': 'What if the “halt” button is pressed ( HG= 10), or both buttons are pressed (HG= 11)? For consistency, we decide that “halt” overrides “go,” but d oes nothing special if it alone is pressed while the counter is halted. Thus, input patterns HG= 10 and HG= 11 also take state HALT A back to itself. Here the arc could be labeled HG= 00,10,11 or, equivalently, HG= 00,1xorHG=x0,11. To complete our design, we apply the same decisions that we made for the COUNT A state to all of the other counting states, and the decisions that we made for the HALT A state to all of the other halted states. If we had chosen not to specify an answer, an imple- mentation could produce diﬀerent behavior from the diﬀerent counting and/or halted states, which might confuse a user. The resulting design appears to the right.HG=0x HG=1xHG=0x HG=01 HG=01HG=0x HG=0x HG=1x HG=1x HG=1x HG=01 HG=01 HG=x0,11 HG=x0,11 HG=x0,11 HG=x0,11COUNT A /00COUNT B COUNT C COUNT D /10 /01 /11 HALT A /00HALT B /01HALT C /11HALT D /10 3.2.9 Choosing a State Representation Now we need to select a representation for the states. Since our c ounter has eight states, we need at least three (⌈log2(8)⌉= 3) state bits S2S1S0to keep track of the current state. As we show later, the choice of representation for an FSM’s states can dramatically aﬀec t the design complexity . For a design as simple as our counter, you could just let a computer implement all possible r epresentations (there aren’t more than 840, if we consider simple symmetries) and select one according to whatever metrics are interesting. For bigger designs, however, the number of possibilities quickly beco mes impossible to explore completely.'}"
"What if the “halt” button is pressed ( HG= 10), or both buttons are pressed (HG= 11)? For consistency, we decide that “halt” overrides “go,” but d oes nothing special if it alone is pressed while the counter is halted. Thus, input patterns HG= 10 and HG= 11 also take state HALT A back to itself. Here the arc could be labeled HG= 00,10,11 or, equivalently, HG= 00,1xorHG=x0,11. To complete our design, we apply the same decisions that we made for the COUNT A state to all of the other counting states, and the decisions that we made for the HALT A state to all of the other halted states. If we had chosen not to specify an answer, an imple- mentation could produce diﬀerent behavior from the diﬀerent counting and/or halted states, which might confuse a user. The resulting design appears to the right.HG=0x HG=1xHG=0x HG=01 HG=01HG=0x HG=0x HG=1x HG=1x HG=1x HG=01 HG=01 HG=x0,11 HG=x0,11 HG=x0,11 HG=x0,11COUNT A /00COUNT B COUNT C COUNT D /10 /01 /11 HALT A /00HALT B /01HALT C /11HALT D /10 3.2.9 Choosing a State Representation Now we need to select a representation for the states. Since our c ounter has eight states, we need at least three (⌈log2(8)⌉= 3) state bits S2S1S0to keep track of the current state. As we show later, the choice of representation for an FSM’s states can dramatically aﬀec t the design complexity . For a design as simple as our counter, you could just let a computer implement all possible r epresentations (there aren’t more than 840, if we consider simple symmetries) and select one according to whatever metrics are interesting. For bigger designs, however, the number of possibilities quickly beco mes impossible to explore completely.","{'page_number': 100, 'textbook_name': 'ECE-120-student-notes', 'text': 'What if the “halt” button is pressed ( HG= 10), or both buttons are pressed (HG= 11)? For consistency, we decide that “halt” overrides “go,” but d oes nothing special if it alone is pressed while the counter is halted. Thus, input patterns HG= 10 and HG= 11 also take state HALT A back to itself. Here the arc could be labeled HG= 00,10,11 or, equivalently, HG= 00,1xorHG=x0,11. To complete our design, we apply the same decisions that we made for the COUNT A state to all of the other counting states, and the decisions that we made for the HALT A state to all of the other halted states. If we had chosen not to specify an answer, an imple- mentation could produce diﬀerent behavior from the diﬀerent counting and/or halted states, which might confuse a user. The resulting design appears to the right.HG=0x HG=1xHG=0x HG=01 HG=01HG=0x HG=0x HG=1x HG=1x HG=1x HG=01 HG=01 HG=x0,11 HG=x0,11 HG=x0,11 HG=x0,11COUNT A /00COUNT B COUNT C COUNT D /10 /01 /11 HALT A /00HALT B /01HALT C /11HALT D /10 3.2.9 Choosing a State Representation Now we need to select a representation for the states. Since our c ounter has eight states, we need at least three (⌈log2(8)⌉= 3) state bits S2S1S0to keep track of the current state. As we show later, the choice of representation for an FSM’s states can dramatically aﬀec t the design complexity . For a design as simple as our counter, you could just let a computer implement all possible r epresentations (there aren’t more than 840, if we consider simple symmetries) and select one according to whatever metrics are interesting. For bigger designs, however, the number of possibilities quickly beco mes impossible to explore completely.'}"
"96  Fortunately, use of abstraction in selecting a representation also tends to produce better designs for a wide varietyofmetrics(suchasdesigncomplexity,area,powerconsum ption, andperformance). Therightstrategy is thus often to start by selecting a representation that makes se nse to a human, even if it requires more bits than are strictly necessary. The resulting implementation will be easier to design and to debug than an implementation in which only the global behavior has any meaning. Let’s return to our speciﬁc example, the counter. We can use one bit, S2, to record whether or not our counter is counting ( S2= 0) or halted ( S2= 1). The other two bits can then record the counter state in terms of the desired output. Choosing this repre- sentation implies that only wires will be necessary to compute outputs Z1andZ0from the internal state: Z1=S1andZ0=S0. The resulting design, in which states are now labeled with both internal state and outputs ( S2S1S0/Z1Z0) appears to the right. In this version, we have changed the arc labeling to use logi- cal expressions, which can sometimes help us to think about the implementation.H H H H GH GH GH H+G H+G H+G H+GGHH H H HCOUNT A COUNT B COUNT C COUNT D HALT A HALT B HALT C HALT D000/00 001/01 011/11 010/10 100/00 101/01 111/11 110/10 The equivalent state listing and state table appear below. We have or dered the rows of the state table in Gray code order to simplify transcription of K-maps. state S2S1S0 description COUNT A 000 counting, output Z1Z0= 00 COUNT B 001 counting, output Z1Z0= 01 COUNT C 011 counting, output Z1Z0= 11 COUNT D 010 counting, output Z1Z0= 10 HALT A 100 halted, output Z1Z0= 00 HALT B 101 halted, output Z1Z0= 01 HALT C 111 halted, output Z1Z0= 11 HALT D 110 halted, output Z1Z0= 10HG state S2S1S000 01 11 10 COUNT A 000 001 001 100 100 COUNT B 001 011 011 101 101 COUNT C 011 010 010 111 111 COUNT D 010 000 000 110 110 HALT D 110 110 000 110 110 HALT C 111 111 010 111 111 HALT B 101 101 011 101 101 HALT A 100 100 001 100 100 Havingchosena representation,we can go ahead and implement our design in the usual way. As shown to the right, K-maps for the next- state logic are complicated, since we have ﬁve variables and must consider implicants that are not contiguous in the K-maps. The S+ 2 logic is easy enough: we only need two terms, as shown","{'page_number': 101, 'textbook_name': 'ECE-120-student-notes', 'text': 'As shown to the right, K-maps for the next- state logic are complicated, since we have ﬁve variables and must consider implicants that are not contiguous in the K-maps. The S+ 2 logic is easy enough: we only need two terms, as shown. Notice that we have used color and line style to distinguish diﬀerent+S2 S1S0 S2HG 00 01 11 10 000 001 0011 010 0 1 01 110 111 101 10011 100 0 1111 1111 1 111 10 0 00 0 01 1+S1 S1S0 S2HG 00 01 11 10 000 001 1 0011 010 0 1 01 110 111 101 1001 11 1 1 100 0 11 1 110 0 00 1 1 0 000 0 0+S0 S1S0 S21 011 1 01 1 1100 0 0 0 0 0 11 11 1 1 1 1 00 00 0 0 0HG 00 01 11 10 000 001 011 010 110 111 101 100 implicants in the K-maps. Furthermore, the symmetry of the design produces symmetry in the S+ 1and S+ 0formula, so we have used the same color and line style for analogous t erms in these two K-maps. For S+ 1, we need four terms. The green ellipses in the HG= 01 column are part of the same term, as are the two halves of the dashed blue circle. In S+ 0, we still need four terms, but three of them are split into twopiecesintheK-map. Asyoucansee,theutilityoftheK-mapissta rtingtobreakdownwithﬁvevariables.'}"
"As shown to the right, K-maps for the next- state logic are complicated, since we have ﬁve variables and must consider implicants that are not contiguous in the K-maps. The S+ 2 logic is easy enough: we only need two terms, as shown. Notice that we have used color and line style to distinguish diﬀerent+S2 S1S0 S2HG 00 01 11 10 000 001 0011 010 0 1 01 110 111 101 10011 100 0 1111 1111 1 111 10 0 00 0 01 1+S1 S1S0 S2HG 00 01 11 10 000 001 1 0011 010 0 1 01 110 111 101 1001 11 1 1 100 0 11 1 110 0 00 1 1 0 000 0 0+S0 S1S0 S21 011 1 01 1 1100 0 0 0 0 0 11 11 1 1 1 1 00 00 0 0 0HG 00 01 11 10 000 001 011 010 110 111 101 100 implicants in the K-maps. Furthermore, the symmetry of the design produces symmetry in the S+ 1and S+ 0formula, so we have used the same color and line style for analogous t erms in these two K-maps. For S+ 1, we need four terms. The green ellipses in the HG= 01 column are part of the same term, as are the two halves of the dashed blue circle. In S+ 0, we still need four terms, but three of them are split into twopiecesintheK-map. Asyoucansee,theutilityoftheK-mapissta rtingtobreakdownwithﬁvevariables.","{'page_number': 101, 'textbook_name': 'ECE-120-student-notes', 'text': 'As shown to the right, K-maps for the next- state logic are complicated, since we have ﬁve variables and must consider implicants that are not contiguous in the K-maps. The S+ 2 logic is easy enough: we only need two terms, as shown. Notice that we have used color and line style to distinguish diﬀerent+S2 S1S0 S2HG 00 01 11 10 000 001 0011 010 0 1 01 110 111 101 10011 100 0 1111 1111 1 111 10 0 00 0 01 1+S1 S1S0 S2HG 00 01 11 10 000 001 1 0011 010 0 1 01 110 111 101 1001 11 1 1 100 0 11 1 110 0 00 1 1 0 000 0 0+S0 S1S0 S21 011 1 01 1 1100 0 0 0 0 0 11 11 1 1 1 1 00 00 0 0 0HG 00 01 11 10 000 001 011 010 110 111 101 100 implicants in the K-maps. Furthermore, the symmetry of the design produces symmetry in the S+ 1and S+ 0formula, so we have used the same color and line style for analogous t erms in these two K-maps. For S+ 1, we need four terms. The green ellipses in the HG= 01 column are part of the same term, as are the two halves of the dashed blue circle. In S+ 0, we still need four terms, but three of them are split into twopiecesintheK-map. Asyoucansee,theutilityoftheK-mapissta rtingtobreakdownwithﬁvevariables.'}"
"3.2 Finite State Machine Design Examples, Part I 97 3.2.10 Abstracting Design Symmetries Rather than implementing the design as two-level logic, let’s try to ta ke advantage of our design’s symmetry to further simplify the logic (we reduce gate count at the expense o f longer, slower paths). Looking back to the last transition diagram, in which the arcs were lab eled with logical expressions, let’s calculate an expression for when the counter should retain its curr ent value in the next cycle. We call this variableHOLD. In the counting states, when S2= 0, the counter stops (moves into a halted state without changing value) when His true. In the halted states, when S2= 1, the counter stops (stays in a halted state) when H+Gis true. We can thus write HOLD =S2·H+S2·(H+G) HOLD =S2H+S2H+S2G HOLD =H+S2G In other words, the counter should hold its current value (stop co unting) if we press the “halt” button or if the counter was already halted and we didn’t press the “go” button . As desired, the current value of the counter ( S1S0) has no impact on this decision. You may have noticed that the expre ssion we derived for HOLDalso matches S+ 2, the next-state value of S2in the K-map on the previous page. Now let’s re-write our state transition table in terms of HOLD. The left version uses state names for clarity; the right uses state values to help us transcribe K-maps. HOLD state S2S1S0 0 1 COUNT A 000 COUNT B HALT A COUNT B 001 COUNT C HALT B COUNT C 011 COUNT D HALT C COUNT D 010 COUNT A HALT D HALT A 100 COUNT B HALT A HALT B 101 COUNT C HALT B HALT C 111 COUNT D HALT C HALT D 110 COUNT A HALT DHOLD state S2S1S00 1 COUNT A 000 001 100 COUNT B 001 011 101 COUNT C 011 010 111 COUNT D 010 000 110 HALT A 100 001 100 HALT B 101 011 101 HALT C 111 010 111 HALT D 110 000 110 The K-maps based on the HOLDabstrac- tion are shown to the right. As you can see, the necessary logic has been simpliﬁed substantially, requiring only two terms each forbothS+ 1andS+ 0","{'page_number': 102, 'textbook_name': 'ECE-120-student-notes', 'text': 'As you can see, the necessary logic has been simpliﬁed substantially, requiring only two terms each forbothS+ 1andS+ 0. Writingthe next-state logic algebraically, we obtain S+ 2=HOLD S+ 1=HOLD·S0+HOLD·S1 S+ 0=HOLD·S1+HOLD·S0+S2 HOLD  S2 00 00 1111 S1S000 01 11 10 00 1 11 1 0 000 01 11 10+S1 HOLD  S2 0 0 00 S1S000 01 11 10 00 1 100 111 11100 01 11 10+S0 HOLD  S2 0 0 S1S000 01 11 10 0000 01 11 100 0 1 11 111 1 01 0 Notice the similarity between the equations for S+ 1S+ 0and the equations for a 2-to-1 mux: when HOLD= 1, the counter retains its state, and when HOLD= 0, it counts.'}"
"As you can see, the necessary logic has been simpliﬁed substantially, requiring only two terms each forbothS+ 1andS+ 0. Writingthe next-state logic algebraically, we obtain S+ 2=HOLD S+ 1=HOLD·S0+HOLD·S1 S+ 0=HOLD·S1+HOLD·S0+S2 HOLD  S2 00 00 1111 S1S000 01 11 10 00 1 11 1 0 000 01 11 10+S1 HOLD  S2 0 0 00 S1S000 01 11 10 00 1 100 111 11100 01 11 10+S0 HOLD  S2 0 0 S1S000 01 11 10 0000 01 11 100 0 1 11 111 1 01 0 Notice the similarity between the equations for S+ 1S+ 0and the equations for a 2-to-1 mux: when HOLD= 1, the counter retains its state, and when HOLD= 0, it counts.","{'page_number': 102, 'textbook_name': 'ECE-120-student-notes', 'text': 'As you can see, the necessary logic has been simpliﬁed substantially, requiring only two terms each forbothS+ 1andS+ 0. Writingthe next-state logic algebraically, we obtain S+ 2=HOLD S+ 1=HOLD·S0+HOLD·S1 S+ 0=HOLD·S1+HOLD·S0+S2 HOLD  S2 00 00 1111 S1S000 01 11 10 00 1 11 1 0 000 01 11 10+S1 HOLD  S2 0 0 00 S1S000 01 11 10 00 1 100 111 11100 01 11 10+S0 HOLD  S2 0 0 S1S000 01 11 10 0000 01 11 100 0 1 11 111 1 01 0 Notice the similarity between the equations for S+ 1S+ 0and the equations for a 2-to-1 mux: when HOLD= 1, the counter retains its state, and when HOLD= 0, it counts.'}"
"98  An implementation appears below. By using semantic meaning in our cho ice of representation—inparticular the use of S2to record whether the counter is currently halted ( S2= 1) or counting ( S2= 0)—we have enabled ourselves to separate out the logic for deciding whether to advance the counter fairly cleanly from the logic for advancing the counter itself. Only the HOLDbit in the diagram is used to determine whether or not the counter should advance in the current cycle. Let’s check that the implementation matches our original design. St art by verifying that the HOLDvariable is calculated correctly, HOLD=H+S2G, then look back at the K-map for S+ 2in the low-level design to verify that the expression we used does indeed match. D QQ S2 S0S1 D QQD QQ H(halt button) G (go button) this bit records whether or not the counter is currently haltedZ1 Z01 0 1 0a controllable two−bit counterCLOCK HOLD Next, check the mux abstraction. When HOLD= 1, the next-state logic for S+ 1andS+ 0reduces to S+ 1=S1 andS+ 0=S0; in other words, the counter stops counting and simply stays in its c urrent state. When HOLD= 0, these equations become S+ 1=S0andS+ 0=S1, which produces the repeating sequence for S1S0of 00, 01, 11, 10, as desired. You may want to look back at our two- bit Gray code counter design to compare the next-state equations. We can now verify that the implementation produces the correct tr ansition behavior. In the counting states, S2= 0, and the HOLDvalue simpliﬁes to HOLD=H. Until we push the “halt” button, S2remains 0, and and the counter continues to count in the correct sequence. WhenH= 1,HOLD= 1, and the counter stops at its current value ( S+ 2S+ 1S+ 0= 1S1S0, which is shorthand for S+ 2= 1,S+ 1=S1, andS+ 0=S0). In any of the halted states, S2= 1, and we can reduce HOLDtoHOLD=H+G. Here, so long as we press the “halt” button or do not press the “go” button, the c ounter stays in its current state, because HOLD= 1. If we release “halt” and press “go,” we have HOLD= 0, and the counter resumes counting (S+ 2S+ 1S+ 0= 0S0S1, which is shorthand for S+ 2= 0,S+ 1=S0, andS+ 0=S1). We have now veriﬁed the implementation. What if you wanted to build a three-bit Gray code counter with the sa me controls for starting and stopping? You could go back to basics and struggle with six-variable K-maps","{'page_number': 103, 'textbook_name': 'ECE-120-student-notes', 'text': 'We have now veriﬁed the implementation. What if you wanted to build a three-bit Gray code counter with the sa me controls for starting and stopping? You could go back to basics and struggle with six-variable K-maps. Or you could simply copy the HOLD mechanism from the two-bit design above, insert muxes between th e next state logic and the ﬂip-ﬂops of the three-bit Gray code counter that we designed earlier, and contro l the muxes with the HOLDbit. Abstrac- tion is a powerful tool.'}"
"We have now veriﬁed the implementation. What if you wanted to build a three-bit Gray code counter with the sa me controls for starting and stopping? You could go back to basics and struggle with six-variable K-maps. Or you could simply copy the HOLD mechanism from the two-bit design above, insert muxes between th e next state logic and the ﬂip-ﬂops of the three-bit Gray code counter that we designed earlier, and contro l the muxes with the HOLDbit. Abstrac- tion is a powerful tool.","{'page_number': 103, 'textbook_name': 'ECE-120-student-notes', 'text': 'We have now veriﬁed the implementation. What if you wanted to build a three-bit Gray code counter with the sa me controls for starting and stopping? You could go back to basics and struggle with six-variable K-maps. Or you could simply copy the HOLD mechanism from the two-bit design above, insert muxes between th e next state logic and the ﬂip-ﬂops of the three-bit Gray code counter that we designed earlier, and contro l the muxes with the HOLDbit. Abstrac- tion is a powerful tool.'}"
"3.2 Finite State Machine Design Examples, Part I 99 3.2.11 Impact of the State Representation What happens if we choose a bad representation? For the same FSM —the two-bit Gray code counter with start and stop inputs—the table below shows a poorly chosen mappin g from states to internal state repre- sentation. Below the table is a diagram of an implementation using that representation. Verifying that the implementation’s behavior is correct is left as an exercise for the det ermined reader. state S2S1S0 state S2S1S0 COUNT A 000 HALT A 111 COUNT B 101 HALT B 110 COUNT C 011 HALT C 100 COUNT D 010 HALT D 001 D QQ S0D QQ S1D QQ S2 G (go button)a controllable two−bit counter (with poorly chosen state values) Z1 Z0CLOCK H(halt button)","{'page_number': 104, 'textbook_name': 'ECE-120-student-notes', 'text': '3.2 Finite State Machine Design Examples, Part I 99 3.2.11 Impact of the State Representation What happens if we choose a bad representation? For the same FSM —the two-bit Gray code counter with start and stop inputs—the table below shows a poorly chosen mappin g from states to internal state repre- sentation. Below the table is a diagram of an implementation using that representation. Verifying that the implementation’s behavior is correct is left as an exercise for the det ermined reader. state S2S1S0 state S2S1S0 COUNT A 000 HALT A 111 COUNT B 101 HALT B 110 COUNT C 011 HALT C 100 COUNT D 010 HALT D 001 D QQ S0D QQ S1D QQ S2 G (go button)a controllable two−bit counter (with poorly chosen state values) Z1 Z0CLOCK H(halt button)'}"
"100  ECE120: Introduction to Computer Engineering Notes Set 3.3 Design of the Finite State Machine for the Lab This set of notes explains the process that Prof. Jones used to de velop the FSM for the lab. The lab simu- lates a vending machine mechanism for automatically identifying coins ( dimes and quarters only), tracking the amount of money entered by the user, accepting or rejecting coins, and emitting a signal when a total of 35 cents has been accepted. In the lab, we will only drive a light with the “paid in full” signal. Sorry, no candy nor Dew will be distributed! 3.3.1 Physical Design, Sensors, and Timing The physical elements of the lab were designed by Prof. Chris Schmit z and constructed with some help from the ECE shop. A user inserts a coin into a slot at one end of the device . The coin then rolls down a slope towards a gate controlled by a servo. The gate can be raised or lowe red, and determines whether the coin exits from the other side or the bottom of the device. As the coin ro lls, it passes two optical sensors.10One of these sensors is positioned high enough above the slope that a dim e passes beneath the sesnor, allowing the signal Tproduced by the sensor to tell us whether the coin is a dime or a quar ter. The second sensor is positioned so that all coins pass in front of it. The sensor positions a re chosen carefully to ensure that, in the case of a quarter, the coin is still blocking the ﬁrst sensor when it reaches the second sensor. Blocked sensors give a signal of 1 in this design, so the rising edge the signal f rom the second sensor can be used as a “clock” for our FSM. When the rising edge occurs, the signal Tfrom the ﬁrst sensor indicates whether the coin is a quarter ( T= 1) or a dime ( T= 0). A sample timing diagram for the lab appears to the right. The clock signal generated by the lab is not only not a square wave—in other words, the high and low portions are not equal—but is also unlikely to be periodic. Instead, the “cycle” is deﬁned by the time between coin insertions. The T signal serves as the single input to our FSM. In the timingCLK T ACLK low between coins (time varies)CLK high as coin rolls by T rises before CLK next state implements coin acceptance decision (A) diagram, Tis shown as rising and falling before the clock edge. We use positive edg e-triggered ﬂip-ﬂops to implement our FSM, thus the aspect of the relative timing that matte rs to our design is that, when the clock rises, the value of Tis stable and indicates the type of coin entered. The signal Tmay fall before or after the clock does—the two are equivalent for our FSM’s needs. The signal Ain the timing diagram is an output from the FSM, and indicates whether or not the coin should be accepted. This signal controls the servo that drives the gate, and thus determines whether the coin is accepted ( A= 1) as payment or rejected ( A= 0) and returned to the user","{'page_number': 105, 'textbook_name': 'ECE-120-student-notes', 'text': 'The signal Tmay fall before or after the clock does—the two are equivalent for our FSM’s needs. The signal Ain the timing diagram is an output from the FSM, and indicates whether or not the coin should be accepted. This signal controls the servo that drives the gate, and thus determines whether the coin is accepted ( A= 1) as payment or rejected ( A= 0) and returned to the user. Looking at the timing diagram, you should note that our FSM makes a d ecision based on its current state and the input Tand enters a new state at the rising clock edge. The value of Ain the next cycle thus determines the position of the gate when the coin eventually rolls to t he end of the slope. As we said earlier, our FSM is thus a Moore machine: the output Adoes not depend on the input T, but only on the current internal state bits of the the FSM. However, you should also now re alize that making Adepend on Tis not adequate for this lab. If Awere to rise with Tand fall with the rising clock edge (on entry to the next state), or even fall with the falling edge of T, the gate would return to the reject position by the time the coin reached the gate, regardless of our FSM’s decision! 10The full system actually allows four sensors to diﬀerentiat e four types of coins, but our lab uses only two of these sensor s.'}"
"The signal Tmay fall before or after the clock does—the two are equivalent for our FSM’s needs. The signal Ain the timing diagram is an output from the FSM, and indicates whether or not the coin should be accepted. This signal controls the servo that drives the gate, and thus determines whether the coin is accepted ( A= 1) as payment or rejected ( A= 0) and returned to the user. Looking at the timing diagram, you should note that our FSM makes a d ecision based on its current state and the input Tand enters a new state at the rising clock edge. The value of Ain the next cycle thus determines the position of the gate when the coin eventually rolls to t he end of the slope. As we said earlier, our FSM is thus a Moore machine: the output Adoes not depend on the input T, but only on the current internal state bits of the the FSM. However, you should also now re alize that making Adepend on Tis not adequate for this lab. If Awere to rise with Tand fall with the rising clock edge (on entry to the next state), or even fall with the falling edge of T, the gate would return to the reject position by the time the coin reached the gate, regardless of our FSM’s decision! 10The full system actually allows four sensors to diﬀerentiat e four types of coins, but our lab uses only two of these sensor s.","{'page_number': 105, 'textbook_name': 'ECE-120-student-notes', 'text': 'The signal Tmay fall before or after the clock does—the two are equivalent for our FSM’s needs. The signal Ain the timing diagram is an output from the FSM, and indicates whether or not the coin should be accepted. This signal controls the servo that drives the gate, and thus determines whether the coin is accepted ( A= 1) as payment or rejected ( A= 0) and returned to the user. Looking at the timing diagram, you should note that our FSM makes a d ecision based on its current state and the input Tand enters a new state at the rising clock edge. The value of Ain the next cycle thus determines the position of the gate when the coin eventually rolls to t he end of the slope. As we said earlier, our FSM is thus a Moore machine: the output Adoes not depend on the input T, but only on the current internal state bits of the the FSM. However, you should also now re alize that making Adepend on Tis not adequate for this lab. If Awere to rise with Tand fall with the rising clock edge (on entry to the next state), or even fall with the falling edge of T, the gate would return to the reject position by the time the coin reached the gate, regardless of our FSM’s decision! 10The full system actually allows four sensors to diﬀerentiat e four types of coins, but our lab uses only two of these sensor s.'}"
"3.3 Design of the Finite State Machine for the Lab 101 3.3.2 An Abstract Model We start by writing down states for a user’s expected behavior. Given the fairly tight constraints that we have placed on our lab, few combinations are pos-state dime (T= 0) quarter ( T= 1)accept? ( A) paid? ( P) START DIME QUARTER no DIME PAID yes no QUARTER PAID yes no PAID yes yes sible. For a total of35 cents, a user should either insert a dime follow ed by a quarter, or a quarterfollowed by a dime. We begin in a START state, which transitions to states DIME or QUARTER when the user inserts the ﬁrst coin. With no previous coin, we need not specify a value for A. No money has been deposited, so we set output P= 0 in the START state. We next create DIME and QUARTER states co rresponding to the user having entered one coin. The ﬁrst coin should be accepted, bu t more money is needed, so both of these states output A= 1 and P= 0. When a coin of the opposite type is entered, each state moves t o a state called PAID, which we use for the case in which a total of 35 cents has been received. For now, we ignore the possibility that the same type of coin is deposited more than once . Finally, the PAID state accepts the second coin ( A= 1) and indicates that the user has paid the full price of 35 cents ( P= 1). We next extend our design to handle user mistakes. If a user enters a second dime in the DIME state, our FSM should reject the coin. We create a RE- JECTD state and add it as the next state fromstate dime (T= 0) quarter ( T= 1)accept? ( A) paid? ( P) START DIME QUARTER no DIME REJECTD PAID yes no REJECTD REJECTD PAID no no QUARTER PAID REJECTQ yes no REJECTQ PAID REJECTQ no no PAID yes yes DIME when a dime is entered. The REJECTD state rejects the dime ( A= 0) and continues to wait for a quarter ( P= 0). What should we use as next states from REJECTD? If the user enters a third dime (or a fourth, or a ﬁfth, and so on), we want to reject the new dime as we ll. If the user enters a quarter, we want to accept the coin, at which point we have received 35 cents (count ing the ﬁrst dime). We use this reasoning to complete the description of REJECTD. We also create an analogou s state, REJECTQ, to handle a user who inserts more than one quarter. What should happen after a user has paid 35 cents and bought one it em? The FSM at that point is in the PAID state, which delivers the item by setting P= 1","{'page_number': 106, 'textbook_name': 'ECE-120-student-notes', 'text': 'We use this reasoning to complete the description of REJECTD. We also create an analogou s state, REJECTQ, to handle a user who inserts more than one quarter. What should happen after a user has paid 35 cents and bought one it em? The FSM at that point is in the PAID state, which delivers the item by setting P= 1. Given that we want the FSM to allow the user to purchase another item, how should we choose the next states fro m PAID? The behavior that we want from PAID is identical to the behavior that we deﬁned from START. The 35 cents already deposited was used to pay for the item delivered, so the machine is no longer holding any of the user’s money. We can thus simply set the next states from PAID to be DIME when a dime is inserte d and QUARTER when a quarter is inserted. At this point, we make a decision intended primar- ily to simplify the logic needed to build the lab. Without a physical item delivery mechanism with a speciﬁcation forhow its in-state dime (T= 0) quarter ( T= 1)accept? ( A) paid? ( P) PAID DIME QUARTER yes yes DIME REJECTD PAID yes no REJECTD REJECTD PAID no no QUARTER PAID REJECTQ yes no REJECTQ PAID REJECTQ no no put must be driven, the behavior of the output signal Pcan be fairly ﬂexible. For example, we could build a delivery mechanism that used the rising edge of Pto open a chute. In this case, the output P= 0 in the start state is not relevant, and we can merge the state START with the state PAID. The way that we handlePin the lab, we might ﬁnd it strange to have a “paid” light turn on before inserting any money, but keeping the design simple enough for a ﬁrst lab exercise is more import ant. Our ﬁnal abstract state table appears above.'}"
"We use this reasoning to complete the description of REJECTD. We also create an analogou s state, REJECTQ, to handle a user who inserts more than one quarter. What should happen after a user has paid 35 cents and bought one it em? The FSM at that point is in the PAID state, which delivers the item by setting P= 1. Given that we want the FSM to allow the user to purchase another item, how should we choose the next states fro m PAID? The behavior that we want from PAID is identical to the behavior that we deﬁned from START. The 35 cents already deposited was used to pay for the item delivered, so the machine is no longer holding any of the user’s money. We can thus simply set the next states from PAID to be DIME when a dime is inserte d and QUARTER when a quarter is inserted. At this point, we make a decision intended primar- ily to simplify the logic needed to build the lab. Without a physical item delivery mechanism with a speciﬁcation forhow its in-state dime (T= 0) quarter ( T= 1)accept? ( A) paid? ( P) PAID DIME QUARTER yes yes DIME REJECTD PAID yes no REJECTD REJECTD PAID no no QUARTER PAID REJECTQ yes no REJECTQ PAID REJECTQ no no put must be driven, the behavior of the output signal Pcan be fairly ﬂexible. For example, we could build a delivery mechanism that used the rising edge of Pto open a chute. In this case, the output P= 0 in the start state is not relevant, and we can merge the state START with the state PAID. The way that we handlePin the lab, we might ﬁnd it strange to have a “paid” light turn on before inserting any money, but keeping the design simple enough for a ﬁrst lab exercise is more import ant. Our ﬁnal abstract state table appears above.","{'page_number': 106, 'textbook_name': 'ECE-120-student-notes', 'text': 'We use this reasoning to complete the description of REJECTD. We also create an analogou s state, REJECTQ, to handle a user who inserts more than one quarter. What should happen after a user has paid 35 cents and bought one it em? The FSM at that point is in the PAID state, which delivers the item by setting P= 1. Given that we want the FSM to allow the user to purchase another item, how should we choose the next states fro m PAID? The behavior that we want from PAID is identical to the behavior that we deﬁned from START. The 35 cents already deposited was used to pay for the item delivered, so the machine is no longer holding any of the user’s money. We can thus simply set the next states from PAID to be DIME when a dime is inserte d and QUARTER when a quarter is inserted. At this point, we make a decision intended primar- ily to simplify the logic needed to build the lab. Without a physical item delivery mechanism with a speciﬁcation forhow its in-state dime (T= 0) quarter ( T= 1)accept? ( A) paid? ( P) PAID DIME QUARTER yes yes DIME REJECTD PAID yes no REJECTD REJECTD PAID no no QUARTER PAID REJECTQ yes no REJECTQ PAID REJECTQ no no put must be driven, the behavior of the output signal Pcan be fairly ﬂexible. For example, we could build a delivery mechanism that used the rising edge of Pto open a chute. In this case, the output P= 0 in the start state is not relevant, and we can merge the state START with the state PAID. The way that we handlePin the lab, we might ﬁnd it strange to have a “paid” light turn on before inserting any money, but keeping the design simple enough for a ﬁrst lab exercise is more import ant. Our ﬁnal abstract state table appears above.'}"
"102  3.3.3 Picking the Representation We are now ready to choose the state representation for the lab F SM. With ﬁve states, we need three bits of internal state. Prof. Jones decided to leverage human meaning in assigning the bit patterns, as follows: S2type of last coin inserted (0 for dime, 1 for quarter) S1more than one quarter inserted? (1 for yes, 0 for no) S0more than one dime inserted? (1 for yes, 0 for no) These meanings are not easy to apply to all of our states. For exam ple, in the PAID state, the last coin inserted may have been of either type, or of no type at all, since we decided to start our FSM in that state as well. However , for the other four states, the meanings provide a clear and unique set of b it pattern assignments, asshown to the right. We can choose anyof the rema iningfour bit patterns (010, 011, 101, or 111) for the PAID state. In fact, we can choose all of the remaining patterns forthe PAIDstate. We canalwaysrepresentanystatestate S2S1S0 PAID ??? DIME 000 REJECTD 001 QUARTER 100 REJECTQ 110 with more than one pattern if we have spare patterns available. Pro f. Jones used this freedom to simplify the logic design. This particular example is slightly tricky. The four free patterns do n ot share any single bit in common, so we cannot simply insert x’s into all K-map entries for which the next state is PAID. For example, if we insert an x into the K-map for S+ 2, and then choose a function for S+ 2that produces a value of 1 in place of the don’t care, we must also produce a 1 in the corresponding entry of the K-map for S+ 0. Our options for PAID include 101 and 111, but not 100 nor 110. These latter two sta tes have other meanings. Let’s begin by writing a next-state table consisting mostly of bits, as shown to the right. We use this table to write out a K-map for S+ 2as follows: any of the patterns that may be used for the PAID state obey the next-state rules for PAID. Any next-state marked as PAID is marked as don’t care in the K-map,S+ 2S+ 1S+ 0 state S2S1S0T= 0T= 1 PAID PAID 000 100 DIME 000 001 PAID REJECTD 001 001 PAID QUARTER 100 PAID 110 REJECTQ 110 PAID 110+S2 S2S1 00 01 11 10 1100 01 11 10S0T 1 000 01xx 1x 00 1x since we can choose patterns starting with either or both values to represent our PAID state. The result- ing K-map appears to the far right. As shown, we simply set S+ 2=T, which matches our original “meaning” forS2. That is, S2is the type of the last coin inserted","{'page_number': 107, 'textbook_name': 'ECE-120-student-notes', 'text': 'The result- ing K-map appears to the far right. As shown, we simply set S+ 2=T, which matches our original “meaning” forS2. That is, S2is the type of the last coin inserted. Based on ourchoice for S+ 2, we can rewrite the K-map as shownto the right, with green italics and shading marking the values produced for the x’s in the spec iﬁcation. Each of these boxes corresponds to one transition into the PAID state . By specifying the S2 value, we cut the number of possible choices from four to two in each case. For those combinations in which the implementation produces S+ 2= 0, we must choose S+ 1= 1, but are still free to leave S+ 0marked as a don’t care. Similarly, for those combinations in which the implementation produces S+ 2= 1, we must choose S+ 0= 1, but are still free to leave S+ 1marked as a don’t care.+S2 S2S1 00 01 11 10 1100 01 11 10S0T 1 000 01100 1 1 10 0 The K-maps for S+ 1andS+ 0are shown to the right. We have not given algebraic expressions for either, but have indicated our choices by highlighting the resulting replacements of don’t care entries with the values produced by our expressions. At this point, we canreview the state patternsactually produced by each of the four next-state transitions into the PAID state. From the DIME state, we move into the 101 state when the user inserts a+S1 S2S1 00 01 11 10 100 01 11 10S0T 000 0100 1 1 0 0 0 00 0+S0 S2S1 00 01 11 10 00 01 11 10S0T 00 00 11 1 10 00 0 000 0 quarter. The result is the same from the REJECTD state. From the QUARTER state, however, we move into the 010 state when the user inserts a dime. The result is the sam e from the REJECTQ state. We must thus classify both patterns, 101 and 010, as PAID states. The re maining two patterns, 011 and 111, cannot'}"
"The result- ing K-map appears to the far right. As shown, we simply set S+ 2=T, which matches our original “meaning” forS2. That is, S2is the type of the last coin inserted. Based on ourchoice for S+ 2, we can rewrite the K-map as shownto the right, with green italics and shading marking the values produced for the x’s in the spec iﬁcation. Each of these boxes corresponds to one transition into the PAID state . By specifying the S2 value, we cut the number of possible choices from four to two in each case. For those combinations in which the implementation produces S+ 2= 0, we must choose S+ 1= 1, but are still free to leave S+ 0marked as a don’t care. Similarly, for those combinations in which the implementation produces S+ 2= 1, we must choose S+ 0= 1, but are still free to leave S+ 1marked as a don’t care.+S2 S2S1 00 01 11 10 1100 01 11 10S0T 1 000 01100 1 1 10 0 The K-maps for S+ 1andS+ 0are shown to the right. We have not given algebraic expressions for either, but have indicated our choices by highlighting the resulting replacements of don’t care entries with the values produced by our expressions. At this point, we canreview the state patternsactually produced by each of the four next-state transitions into the PAID state. From the DIME state, we move into the 101 state when the user inserts a+S1 S2S1 00 01 11 10 100 01 11 10S0T 000 0100 1 1 0 0 0 00 0+S0 S2S1 00 01 11 10 00 01 11 10S0T 00 00 11 1 10 00 0 000 0 quarter. The result is the same from the REJECTD state. From the QUARTER state, however, we move into the 010 state when the user inserts a dime. The result is the sam e from the REJECTQ state. We must thus classify both patterns, 101 and 010, as PAID states. The re maining two patterns, 011 and 111, cannot","{'page_number': 107, 'textbook_name': 'ECE-120-student-notes', 'text': 'The result- ing K-map appears to the far right. As shown, we simply set S+ 2=T, which matches our original “meaning” forS2. That is, S2is the type of the last coin inserted. Based on ourchoice for S+ 2, we can rewrite the K-map as shownto the right, with green italics and shading marking the values produced for the x’s in the spec iﬁcation. Each of these boxes corresponds to one transition into the PAID state . By specifying the S2 value, we cut the number of possible choices from four to two in each case. For those combinations in which the implementation produces S+ 2= 0, we must choose S+ 1= 1, but are still free to leave S+ 0marked as a don’t care. Similarly, for those combinations in which the implementation produces S+ 2= 1, we must choose S+ 0= 1, but are still free to leave S+ 1marked as a don’t care.+S2 S2S1 00 01 11 10 1100 01 11 10S0T 1 000 01100 1 1 10 0 The K-maps for S+ 1andS+ 0are shown to the right. We have not given algebraic expressions for either, but have indicated our choices by highlighting the resulting replacements of don’t care entries with the values produced by our expressions. At this point, we canreview the state patternsactually produced by each of the four next-state transitions into the PAID state. From the DIME state, we move into the 101 state when the user inserts a+S1 S2S1 00 01 11 10 100 01 11 10S0T 000 0100 1 1 0 0 0 00 0+S0 S2S1 00 01 11 10 00 01 11 10S0T 00 00 11 1 10 00 0 000 0 quarter. The result is the same from the REJECTD state. From the QUARTER state, however, we move into the 010 state when the user inserts a dime. The result is the sam e from the REJECTQ state. We must thus classify both patterns, 101 and 010, as PAID states. The re maining two patterns, 011 and 111, cannot'}"
"3.3 Design of the Finite State Machine for the Lab 103 be reached from any of the states in our design. We might then try to leverage the fact that the next- state patterns from these two states are not relevant (recall that we ﬁxed the next-state patterns for all four of the possible PAID states) to further simplify our logic, but doing so does not provide any advan- tage (you may want to check our claim). The ﬁnal state table is shown to the right. We have included the extra states at the bottom of the table. Wehavespeciﬁedthenext-statelogicfortheseS+ 2S+ 1S+ 0 state S2S1S0T= 0T= 1A P PAID1 010 000 100 1 1 PAID2 101 000 100 1 1 DIME 000 001 101 1 0 REJECTD 001 001 101 0 0 QUARTER 100 010 110 1 0 REJECTQ 110 010 110 0 0 EXTRA1 011 000 100 x x EXTRA2 111 000 100 x x states, but left the output bits as don’t cares. A state transition diagram appears at the bottom of this page. 3.3.4 Testing the Design Having a complete design on paper is a good step forward, but human s make mistakes at all stages. How can we know that a circuit that we build in the lab correctly implements t he FSM that we have outlined in these notes? For the lab design, we have two problems to solve. First, we have not speciﬁed an initialization scheme for the FSM. We may want the FSM to start in one of the PAID states, bu t adding initialization logic to the design may mean requiring you to wire together signiﬁcantly more chip s. Second, we need a sequence of inputs that manages to test that all of the next-state and outpu t logic implementations are correct. Testing sequential logic, including FSMs, is in general extremely diﬃcu lt. In fact, large sequential systems today are generally converted into combinational logic by using shift registers to ﬁll the ﬂip-ﬂops with a particular pattern, executing the logic for one clock cycle, and che cking that the resulting pattern of bits in the ﬂip-ﬂops is correct. This approach is called scan-based testing , and is discussed in ECE 543. You will make use of a similar approach when you test your combinational logic in the second week of the lab, before wiring up the ﬂip-ﬂops. We have designed our FSM to be easy to test (even small FSMs may be challenging) with a brute force approach. In particular, we identify two input sequences that tog ether serve both to initialize and to test a correctly implemented variant of our FSM. Our initialization sequence forces the FSM into a speciﬁc state regardless of its initial state. And our test sequence crosses eve ry transition arc leaving the six valid states. In terms of T, the coin type, we initialize the FSM with the input sequence 001. Notic e that such a sequence takes any initial state into PAID2. For testing, we use the input sequence 111010010001","{'page_number': 108, 'textbook_name': 'ECE-120-student-notes', 'text': 'Our initialization sequence forces the FSM into a speciﬁc state regardless of its initial state. And our test sequence crosses eve ry transition arc leaving the six valid states. In terms of T, the coin type, we initialize the FSM with the input sequence 001. Notic e that such a sequence takes any initial state into PAID2. For testing, we use the input sequence 111010010001. You should trace this sequence, starting from PAID2, on the diagram below to see how the test sequence covers all of the possible arcs. As we test, we need also to observe the AandPoutputs in each state to check the output logic. T=0 T=0 T=1T=1 T=1T=1 T=1T=0 T=0T=0 T=1T=1 T=0T=1T=0 T=0 QTR 100/10PAID1 010/11PAID2 101/11EXTRA1 011/xxEXTRA2 111/xxDIME 000/10REJECTD 001/00 REJECTQ 110/00'}"
"Our initialization sequence forces the FSM into a speciﬁc state regardless of its initial state. And our test sequence crosses eve ry transition arc leaving the six valid states. In terms of T, the coin type, we initialize the FSM with the input sequence 001. Notic e that such a sequence takes any initial state into PAID2. For testing, we use the input sequence 111010010001. You should trace this sequence, starting from PAID2, on the diagram below to see how the test sequence covers all of the possible arcs. As we test, we need also to observe the AandPoutputs in each state to check the output logic. T=0 T=0 T=1T=1 T=1T=1 T=1T=0 T=0T=0 T=1T=1 T=0T=1T=0 T=0 QTR 100/10PAID1 010/11PAID2 101/11EXTRA1 011/xxEXTRA2 111/xxDIME 000/10REJECTD 001/00 REJECTQ 110/00","{'page_number': 108, 'textbook_name': 'ECE-120-student-notes', 'text': 'Our initialization sequence forces the FSM into a speciﬁc state regardless of its initial state. And our test sequence crosses eve ry transition arc leaving the six valid states. In terms of T, the coin type, we initialize the FSM with the input sequence 001. Notic e that such a sequence takes any initial state into PAID2. For testing, we use the input sequence 111010010001. You should trace this sequence, starting from PAID2, on the diagram below to see how the test sequence covers all of the possible arcs. As we test, we need also to observe the AandPoutputs in each state to check the output logic. T=0 T=0 T=1T=1 T=1T=1 T=1T=0 T=0T=0 T=1T=1 T=0T=1T=0 T=0 QTR 100/10PAID1 010/11PAID2 101/11EXTRA1 011/xxEXTRA2 111/xxDIME 000/10REJECTD 001/00 REJECTQ 110/00'}"
"104  ECE120: Introduction to Computer Engineering Notes Set 3.4 Extending Keyless Entry with a Timeout This set of notes builds on the keyless entry control FSM that we de signed earlier. In particular, we use a counter to make the alarm time out, turning itself oﬀ after a ﬁxed am ount of time. The goal of this extension is to illustrate how we can make use of components such as registers and counters as building blocks for our FSMs without fully expanding the design to explicitly illustrate all possib le states. To begin, let’s review the FSM that we designed earlier for key- less entry. The state transition diagram for our design is repli- cated to the right. The four states are labeled with state bits and output bits, S1S0/DRA, where Dindicates that the driver’s door should be unlocked, Rindicates that the rest of the doors should be unlocked, and Aindicates that the alarm should be on. Transi- tion arcs in the diagram are la- beled with concise versions of the inputsULP(using don’t cares), whereUrepresentsan unlock but- ton,Lrepresents a lock button, andPrepresents a panic button. In this design, once a user presses the panic button P, the alarm sounds until the user presses theULP=000 ULP=000,x10 ULP=xx1 ULP=x10ULP=xx1 ULP=100 ULP=xx1,x00ULP=xx1ULP=100 ULP=x10 ULP=x00ULP=x10 (We will add a   timeout here.)00/000LOCKED DRIVER 10/100 ALARM 01/001 11/110UNLOCKED lock button Lto turn it oﬀ. Instead of sounding the alarm indeﬁnitely, we might wan t to turn the alarm oﬀ after a ﬁxed amount of time. In other words, after the system ha s been in the ALARM state for, say, thirty or sixty seconds, we might want to move back to the LOCKED state e ven if the user has not pushed the lock button. The blue annotation in the diagram indicates the arc tha t we must adjust. But thirty or sixty seconds is a large number of clock cycles, and our FSM must keep tra ck of the time. Do we need to draw all of the states? Instead of following the design process that we outlined earlier, let’s think about how we can modify our existing design to incorporate the new functionality. In order to ke ep track of time, we use a binary counter. Let’s say that we want our timeout to be Tcycles. When we enter the alarm state, we want to set the counter’s value to T−1, then let the counter count down until it reaches 0, at which point a timeout occurs. To load the initial value, our counter should have a parallel load capab ility that sets the counter value when inputLD= 1. When LD= 0, the counter counts down. The counter also has an output Zthat indicates that the counter’s value is currently zero, which we can use to indica te a timeout on the alarm","{'page_number': 109, 'textbook_name': 'ECE-120-student-notes', 'text': 'To load the initial value, our counter should have a parallel load capab ility that sets the counter value when inputLD= 1. When LD= 0, the counter counts down. The counter also has an output Zthat indicates that the counter’s value is currently zero, which we can use to indica te a timeout on the alarm. You should be able to build such a counter based on what you have learned earlier in the class. Here, we will assume that we can just make use of it. How many bits do we need in our counter? The answer depends on T. If we add the counter to our design, the state of the counter is technically part of the state of our FSM , but we can treat it somewhat abstractly. For example, we only plan to make use of the counter value in the ALAR M state, so we ignore the counter bits in the three other states. In other words, S1S0= 10 means that the system is in the LOCKED state regardless of the counter’s value.'}"
"To load the initial value, our counter should have a parallel load capab ility that sets the counter value when inputLD= 1. When LD= 0, the counter counts down. The counter also has an output Zthat indicates that the counter’s value is currently zero, which we can use to indica te a timeout on the alarm. You should be able to build such a counter based on what you have learned earlier in the class. Here, we will assume that we can just make use of it. How many bits do we need in our counter? The answer depends on T. If we add the counter to our design, the state of the counter is technically part of the state of our FSM , but we can treat it somewhat abstractly. For example, we only plan to make use of the counter value in the ALAR M state, so we ignore the counter bits in the three other states. In other words, S1S0= 10 means that the system is in the LOCKED state regardless of the counter’s value.","{'page_number': 109, 'textbook_name': 'ECE-120-student-notes', 'text': 'To load the initial value, our counter should have a parallel load capab ility that sets the counter value when inputLD= 1. When LD= 0, the counter counts down. The counter also has an output Zthat indicates that the counter’s value is currently zero, which we can use to indica te a timeout on the alarm. You should be able to build such a counter based on what you have learned earlier in the class. Here, we will assume that we can just make use of it. How many bits do we need in our counter? The answer depends on T. If we add the counter to our design, the state of the counter is technically part of the state of our FSM , but we can treat it somewhat abstractly. For example, we only plan to make use of the counter value in the ALAR M state, so we ignore the counter bits in the three other states. In other words, S1S0= 10 means that the system is in the LOCKED state regardless of the counter’s value.'}"
"3.4 Extending Keyless Entry with a Timeout 105 We expand the ALARM state into Tseparate states based on the value of the counter. As shown to the right, we name the states ALARM( 1) through ALARM(T). All of these alarm states use S1S0= 01, but they can be diﬀerentiated using a “timer” (the counter value). We need to make design decisions about how the arcs entering and lea v- ing the ALARM state in our original design should be used once we have incorporated the timeout. As a ﬁrst step, we decide that all arcs e ntering ALARM from other states now enter ALARM(1). Similarly, if the user presses the panic button Pin any of the ALARM(t) states, the system returns to ALARM(1). Eﬀectively, pressing the panic button rese ts the timer. The only arc leaving the ALARM state goes to the LOCKED state on ULP=x10. We replicate this arc for all ALARM(t) states: the user can push the lock button at any time to silence the alarm. Finally, the self-loop back to the ALARM state on ULP=x00 be- comes the countdown arcs in our expanded states, taking ALARM( t) to ALARM(t+1), and ALARM(T) to LOCKED. Now that we have a complete speciﬁcation for the extended design, we can implement it. We want to reuse our original design as much as possible, but we have three new features that must be considered. First, w hen we enter the ALARM(1) state, we need to set the counter value to T−1. Second, we need the counter value to count downwardwhile in the AL ARM state. Finally, we need to move back to the LOCKEDstate when a time out occurs—that is, when the counter reaches zero.this state, including ULP=xx1 from any ALARM state.All input arcs to ALARM enter (on ULP=x10) are replicated.Outgoing arcs to LOCKED ULP=x00ULP=x00 ULP=x00 LOCKEDtimer=0ALARM(T)ALARM(1) ALARM(2)timer=T−1 timer=T−2. . . The ﬁrst problem is fairly easy. Our counter supports parallel load, and the only value that we need to load isT−1, so we apply the constant bit pattern for T−1 to the load inputs and raise the LDinput whenever we enter the ALARM(1) state. In our original design, we chose to e nter the ALARM state whenever the user pressed P, regardless of the other buttons. Hence we can connect Pdirectly to our counter’s LDinput. The second problem is handled by the counter’s countdown function ality. In the ALARM(t) states, the counter will count down each cycle, moving the system from ALARM( t) to ALARM(t+1). The last problem is slightly trickier, since we need to change S1S0. Notice that S1S0= 01 for the ALARM state and S1S0= 00 for the LOCKED state","{'page_number': 110, 'textbook_name': 'ECE-120-student-notes', 'text': 'The second problem is handled by the counter’s countdown function ality. In the ALARM(t) states, the counter will count down each cycle, moving the system from ALARM( t) to ALARM(t+1). The last problem is slightly trickier, since we need to change S1S0. Notice that S1S0= 01 for the ALARM state and S1S0= 00 for the LOCKED state. Thus, we need only force S0to 0 when a timeout occurs. We can use a single 2-to-1 multiplexer for this purpose. The “0” input of the mux comes from the original S+ 0 logic, and the “1” input is a constant 0. All other state logic remains u nchanged. When does a timeout occur? First, we must be in the ALARM(T) state, so S1S0= 01 and the counter’s Zoutput is raised. Second, the input combination must be ULP=xx0—notice that both ULP=x00 andULP=x10 return to LOCKED from ALARM(T). A single, four-input AND gate thus suﬃc es to obtain the timeout signal, ¯S1S0Z¯P, which we connect to the select input of the mux between the S+ 0logic and the S0ﬂip-ﬂop. The extension thus requires only a counter, a mux, and a gate, as s hown below. S0S logic+ 0S11S logic+S0S1 counter LD ZT−1D QQD QQP L U 0 1 0'}"
"The second problem is handled by the counter’s countdown function ality. In the ALARM(t) states, the counter will count down each cycle, moving the system from ALARM( t) to ALARM(t+1). The last problem is slightly trickier, since we need to change S1S0. Notice that S1S0= 01 for the ALARM state and S1S0= 00 for the LOCKED state. Thus, we need only force S0to 0 when a timeout occurs. We can use a single 2-to-1 multiplexer for this purpose. The “0” input of the mux comes from the original S+ 0 logic, and the “1” input is a constant 0. All other state logic remains u nchanged. When does a timeout occur? First, we must be in the ALARM(T) state, so S1S0= 01 and the counter’s Zoutput is raised. Second, the input combination must be ULP=xx0—notice that both ULP=x00 andULP=x10 return to LOCKED from ALARM(T). A single, four-input AND gate thus suﬃc es to obtain the timeout signal, ¯S1S0Z¯P, which we connect to the select input of the mux between the S+ 0logic and the S0ﬂip-ﬂop. The extension thus requires only a counter, a mux, and a gate, as s hown below. S0S logic+ 0S11S logic+S0S1 counter LD ZT−1D QQD QQP L U 0 1 0","{'page_number': 110, 'textbook_name': 'ECE-120-student-notes', 'text': 'The second problem is handled by the counter’s countdown function ality. In the ALARM(t) states, the counter will count down each cycle, moving the system from ALARM( t) to ALARM(t+1). The last problem is slightly trickier, since we need to change S1S0. Notice that S1S0= 01 for the ALARM state and S1S0= 00 for the LOCKED state. Thus, we need only force S0to 0 when a timeout occurs. We can use a single 2-to-1 multiplexer for this purpose. The “0” input of the mux comes from the original S+ 0 logic, and the “1” input is a constant 0. All other state logic remains u nchanged. When does a timeout occur? First, we must be in the ALARM(T) state, so S1S0= 01 and the counter’s Zoutput is raised. Second, the input combination must be ULP=xx0—notice that both ULP=x00 andULP=x10 return to LOCKED from ALARM(T). A single, four-input AND gate thus suﬃc es to obtain the timeout signal, ¯S1S0Z¯P, which we connect to the select input of the mux between the S+ 0logic and the S0ﬂip-ﬂop. The extension thus requires only a counter, a mux, and a gate, as s hown below. S0S logic+ 0S11S logic+S0S1 counter LD ZT−1D QQD QQP L U 0 1 0'}"
"106  ECE120: Introduction to Computer Engineering Notes Set 3.5 Finite State Machine Design Examples, Part II This set of notes provides several additional examples of FSM desig n. We ﬁrst design an FSM to control a vending machine, introducing encoders and decoders as componen ts that help us to implement our design. We then design a game controller for a logic puzzle implemented as a child ren’s game. Finally, we analyze a digital FSM designed to control the stoplights at the intersection of two roads. 3.5.1 Design of a Vending Machine For the next example, we design an FSM to control a simple vending ma chine. The machine accepts U.S. coins11as payment and oﬀers a choice of three items for sale. What states does such an FSM need? The FSM needs to keep track o f how much money has been inserted in order to decide whether a user can purchase one of the items. Th at information alone is enough for the simplest machine, but let’s create a machine with adjustable item price s. We can use registers to hold the item prices, which we denote P1,P2, andP3. Technically, the item prices are also part of the internal state of th e FSM. However, we leave out discussion (and, indeed, methods) for setting the item prices, so no state wit h a given combination of prices has any transition to a state with a diﬀerent set of item prices. In other wor ds, any given combination of item prices induces a subset of states that operate independently of the sub set induced by a distinct combination of item prices. By abstracting away the prices in this way, we can focus on a general design that allows the owner of the machine to set the prices dynamically. Our machine will not accept pennies, so let’s have the FSM keep track of how much money has been inserted as a multiple of 5 cents (one nickel). The table to the right shows ﬁve types of coins, their value in dollars, and their value in terms of nickels. The most expensive item in the machine might cost a dollar or two, so the FSM must track at least 20 or 40 nickels of value. Let’scoin type value# of nickels nickel $0.05 1 dime $0.10 2 quarter $0.25 5 half dollar $0.50 10 dollar $1.00 20 decide to use six bits to record the number of nickels, which allows the machine to keep track of up to $3.15 (63 nickels). We call the abstract states STATE00 throughSTATE63 , and refer to a state with an inserted value ofNnickels as STATE<N>. Let’s now create a next-state table, as shown at the top of the ne xt page. The user can insert one of the ﬁve coin types, or can pick one of the three items. What should happen if the user inserts more money than the FSM can track? Let’s make the FSM reject such coins. Similarly, if the user tries to buy an item without inserting enough money ﬁrst, the FSM must reject the request","{'page_number': 111, 'textbook_name': 'ECE-120-student-notes', 'text': 'Let’s now create a next-state table, as shown at the top of the ne xt page. The user can insert one of the ﬁve coin types, or can pick one of the three items. What should happen if the user inserts more money than the FSM can track? Let’s make the FSM reject such coins. Similarly, if the user tries to buy an item without inserting enough money ﬁrst, the FSM must reject the request. F or each of the possible input events, we add a condition to separate the FSM states that allow the input even t to be processed as the user desires from those states that do not. For example, if the user inserts a q uarter, those states with N <59 transition to states with value N+5 and accept the quarter. Those states with N≥59 reject the coin and remain in STATE<N>. 11Most countries have small bills or coins in demoninations su itable for vending machine prices, so think about some other currency if you prefer.'}"
"Let’s now create a next-state table, as shown at the top of the ne xt page. The user can insert one of the ﬁve coin types, or can pick one of the three items. What should happen if the user inserts more money than the FSM can track? Let’s make the FSM reject such coins. Similarly, if the user tries to buy an item without inserting enough money ﬁrst, the FSM must reject the request. F or each of the possible input events, we add a condition to separate the FSM states that allow the input even t to be processed as the user desires from those states that do not. For example, if the user inserts a q uarter, those states with N <59 transition to states with value N+5 and accept the quarter. Those states with N≥59 reject the coin and remain in STATE<N>. 11Most countries have small bills or coins in demoninations su itable for vending machine prices, so think about some other currency if you prefer.","{'page_number': 111, 'textbook_name': 'ECE-120-student-notes', 'text': 'Let’s now create a next-state table, as shown at the top of the ne xt page. The user can insert one of the ﬁve coin types, or can pick one of the three items. What should happen if the user inserts more money than the FSM can track? Let’s make the FSM reject such coins. Similarly, if the user tries to buy an item without inserting enough money ﬁrst, the FSM must reject the request. F or each of the possible input events, we add a condition to separate the FSM states that allow the input even t to be processed as the user desires from those states that do not. For example, if the user inserts a q uarter, those states with N <59 transition to states with value N+5 and accept the quarter. Those states with N≥59 reject the coin and remain in STATE<N>. 11Most countries have small bills or coins in demoninations su itable for vending machine prices, so think about some other currency if you prefer.'}"
"5 Finite State Machine Design Examples, Part II 107 ﬁnal state accept release initial state input event condition state coin product STATE<N>no input always STATE<N> — none STATE<N>nickel inserted N <63STATE<N+1>yes none STATE<N>nickel inserted N= 63STATE<N> no none STATE<N>dime inserted N <62STATE<N+2>yes none STATE<N>dime inserted N≥62STATE<N> no none STATE<N>quarter inserted N <59STATE<N+5>yes none STATE<N>quarter inserted N≥59STATE<N> no none STATE<N>half dollar inserted N <54STATE<N+10>yes none STATE<N>half dollar inserted N≥54STATE<N> no none STATE<N>dollar inserted N <44STATE<N+20>yes none STATE<N>dollar inserted N≥44STATE<N> no none STATE<N>item 1 selected N≥P1STATE<N−P1>— 1 STATE<N>item 1 selected N < P 1STATE<N> — none STATE<N>item 2 selected N≥P2STATE<N−P2>— 2 STATE<N>item 2 selected N < P 2STATE<N> — none STATE<N>item 3 selected N≥P3STATE<N−P3>— 3 STATE<N>item 3 selected N < P 3STATE<N> — none We can now begin to formalize the I/O for our machine","{'page_number': 112, 'textbook_name': 'ECE-120-student-notes', 'text': 'Inputs include insertion of coins and selection of items for purchase. Outputs include a signal to accept or reject a n inserted coin as well as signals to release each of the three items. For input to the FSM, we assume that a coin inserted in any given cycle is classiﬁed and delivered to our FSM using the three-bit representat ion shown to the right. For item selection, we assume that the user has acces s to three buttons, B1,B2, andB3, that indicate a desire to purchase the corresponding item. For output, the FSM must produce a signal Aindicating whether a coin should be accepted. To control the release of items that have bee n purchased, the FSM must produce the signals R1,R2, andR3, corresponding to the re-coin type C2C1C0 none 110 nickel 010 dime 000 quarter 011 half dollar 001 dollar 111 lease of each item. Since outputs in our class depend only on state, w e extend the internal state of the FSM to include bits for each of these output signals. The output signals g o high in the cycle after the inputs that generate them. Thus, for example, the accept signal Acorresponds to a coin inserted in the previous cycle, even if a second coin is inserted in the current cycle. This meaning mus t be made clear to whomever builds the mechanical system to return coins. Now we are ready to complete the speciﬁcation. How many states do es the FSM have? With six bits to recordmoney inserted and four bits to driveoutput signals, we hav ea total of 1,024(210) states! Six diﬀerent coin inputs are possible, and the selection buttons allow eight possible combinations, giving 48 transitions from each state. Fortunately, we can use the meaning of the bits t o greatly simplify our analysis. First, note that the current state of the coin accept bit and item r elease bits—the four bits of FSM state that control the outputs—have no eﬀect on the next state of the FSM . Thus, we can consider only the current amount of money in a given state when thinking about the transitions from the state. As you have seen, we can further abstract the states using the number N, the number of nickels currently held by the vending machine. We must still considerall48possibletransitionsfrom STATE<N>. Lookingbackat ourabstractnext-state table, notice that we had only eight types of input events (not coun ting “no input”). If we strictly prioritize these eight possible events, we can safely ignore combinations. Rec all that we adopted a similar strategy for several earlier designs, including the ice cream dispenser in Note s Set 2.2 and the keyless entry system developed in Notes Set 3.1.3.'}"
"Inputs include insertion of coins and selection of items for purchase. Outputs include a signal to accept or reject a n inserted coin as well as signals to release each of the three items. For input to the FSM, we assume that a coin inserted in any given cycle is classiﬁed and delivered to our FSM using the three-bit representat ion shown to the right. For item selection, we assume that the user has acces s to three buttons, B1,B2, andB3, that indicate a desire to purchase the corresponding item. For output, the FSM must produce a signal Aindicating whether a coin should be accepted. To control the release of items that have bee n purchased, the FSM must produce the signals R1,R2, andR3, corresponding to the re-coin type C2C1C0 none 110 nickel 010 dime 000 quarter 011 half dollar 001 dollar 111 lease of each item. Since outputs in our class depend only on state, w e extend the internal state of the FSM to include bits for each of these output signals. The output signals g o high in the cycle after the inputs that generate them. Thus, for example, the accept signal Acorresponds to a coin inserted in the previous cycle, even if a second coin is inserted in the current cycle. This meaning mus t be made clear to whomever builds the mechanical system to return coins. Now we are ready to complete the speciﬁcation. How many states do es the FSM have? With six bits to recordmoney inserted and four bits to driveoutput signals, we hav ea total of 1,024(210) states! Six diﬀerent coin inputs are possible, and the selection buttons allow eight possible combinations, giving 48 transitions from each state. Fortunately, we can use the meaning of the bits t o greatly simplify our analysis. First, note that the current state of the coin accept bit and item r elease bits—the four bits of FSM state that control the outputs—have no eﬀect on the next state of the FSM . Thus, we can consider only the current amount of money in a given state when thinking about the transitions from the state. As you have seen, we can further abstract the states using the number N, the number of nickels currently held by the vending machine. We must still considerall48possibletransitionsfrom STATE<N>. Lookingbackat ourabstractnext-state table, notice that we had only eight types of input events (not coun ting “no input”). If we strictly prioritize these eight possible events, we can safely ignore combinations. Rec all that we adopted a similar strategy for several earlier designs, including the ice cream dispenser in Note s Set 2.2 and the keyless entry system developed in Notes Set 3.1.3.","{'page_number': 112, 'textbook_name': 'ECE-120-student-notes', 'text': 'Inputs include insertion of coins and selection of items for purchase. Outputs include a signal to accept or reject a n inserted coin as well as signals to release each of the three items. For input to the FSM, we assume that a coin inserted in any given cycle is classiﬁed and delivered to our FSM using the three-bit representat ion shown to the right. For item selection, we assume that the user has acces s to three buttons, B1,B2, andB3, that indicate a desire to purchase the corresponding item. For output, the FSM must produce a signal Aindicating whether a coin should be accepted. To control the release of items that have bee n purchased, the FSM must produce the signals R1,R2, andR3, corresponding to the re-coin type C2C1C0 none 110 nickel 010 dime 000 quarter 011 half dollar 001 dollar 111 lease of each item. Since outputs in our class depend only on state, w e extend the internal state of the FSM to include bits for each of these output signals. The output signals g o high in the cycle after the inputs that generate them. Thus, for example, the accept signal Acorresponds to a coin inserted in the previous cycle, even if a second coin is inserted in the current cycle. This meaning mus t be made clear to whomever builds the mechanical system to return coins. Now we are ready to complete the speciﬁcation. How many states do es the FSM have? With six bits to recordmoney inserted and four bits to driveoutput signals, we hav ea total of 1,024(210) states! Six diﬀerent coin inputs are possible, and the selection buttons allow eight possible combinations, giving 48 transitions from each state. Fortunately, we can use the meaning of the bits t o greatly simplify our analysis. First, note that the current state of the coin accept bit and item r elease bits—the four bits of FSM state that control the outputs—have no eﬀect on the next state of the FSM . Thus, we can consider only the current amount of money in a given state when thinking about the transitions from the state. As you have seen, we can further abstract the states using the number N, the number of nickels currently held by the vending machine. We must still considerall48possibletransitionsfrom STATE<N>. Lookingbackat ourabstractnext-state table, notice that we had only eight types of input events (not coun ting “no input”). If we strictly prioritize these eight possible events, we can safely ignore combinations. Rec all that we adopted a similar strategy for several earlier designs, including the ice cream dispenser in Note s Set 2.2 and the keyless entry system developed in Notes Set 3.1.3.'}"
"108  We choose to prioritize purchases over new coin insertions, and to p rioritize item 3 over item 2 over item 1. These prioritizations are strict in the sense that if the user presse sB3, both other buttons are ignored, and any coin inserted is rejected, regardless of whether or not th e user can actually purchase item 3 (the machine may not contain enough money to cover the item price). With the choice of strict prioritization, all transitions from all states become well-deﬁned. We apply the trans ition rules in order of decreasing priority, with conditions, and with don’t-cares for lower-priority inputs. For example, for any of the 16 STATE50 ’s (remember that the four current output bits do not aﬀect trans itions), the table below lists all possible transitions assuming that P3= 60,P2= 10, and P1= 35. next state initial state B3B2B1C2C1C0state A R 3R2R1 STATE50 1 x x xxx STATE50 0 0 0 0 STATE50 0 1 x xxx STATE40 0 0 1 0 STATE50 0 0 1 xxx STATE15 0 0 0 1 STATE50 0 0 0 010 STATE51 1 0 0 0 STATE50 0 0 0 000 STATE52 1 0 0 0 STATE50 0 0 0 011 STATE55 1 0 0 0 STATE50 0 0 0 001 STATE60 1 0 0 0 STATE50 0 0 0 111 STATE50 0 0 0 0 STATE50 0 0 0 110 STATE50 0 0 0 0 Next, we need to choose a state representation. But this task is e ssentially done: each output bit ( A,R1, R2, andR3) is represented with one bit in the internal representation, and th e remaining six bits record the number of nickels held by the vending machine using an unsigned repre sentation. The choice of a numeric representation for the money held is importa nt, as it allows us to use an adder to compute the money held in the next state. 3.5.2 Encoders and Decoders Since we chose to prioritize purchases, let’s begin by building logic to pe rform state transitions for purchases. Ourﬁrsttaskistoimplementprioritizationamongthethreeselection buttons. Forthispurpose, weconstruct a 4-input priority encoder , which generates a signal Pwhenever any of its four input lines is active and encodes the index of the highest active input as a two-bit unsigned n umberS. A truth table for our priority encoder appears on the left below, with K-maps for each of the out put bits on the right. B3B2B1B0P S 1 x x x 1 11 0 1 x x 1 10 0 0 1 x 1 01 0 0 0 1 1 00 0 0 0 0 0 xxB1B0B3B2 00 01 11 10 00 01 11 1011 1 1P 11 1 1 11111 1 10 B1B0B3B2 00 01 11 10 000 01 11 100 11 1S1 110 1 1 111 11x B1B0B3B2 00 01 11 10 00 01 11 1011 1S 10 1 11 1 x0 1 1 0000","{'page_number': 113, 'textbook_name': 'ECE-120-student-notes', 'text': '108  We choose to prioritize purchases over new coin insertions, and to p rioritize item 3 over item 2 over item 1. These prioritizations are strict in the sense that if the user presse sB3, both other buttons are ignored, and any coin inserted is rejected, regardless of whether or not th e user can actually purchase item 3 (the machine may not contain enough money to cover the item price). With the choice of strict prioritization, all transitions from all states become well-deﬁned. We apply the trans ition rules in order of decreasing priority, with conditions, and with don’t-cares for lower-priority inputs. For example, for any of the 16 STATE50 ’s (remember that the four current output bits do not aﬀect trans itions), the table below lists all possible transitions assuming that P3= 60,P2= 10, and P1= 35. next state initial state B3B2B1C2C1C0state A R 3R2R1 STATE50 1 x x xxx STATE50 0 0 0 0 STATE50 0 1 x xxx STATE40 0 0 1 0 STATE50 0 0 1 xxx STATE15 0 0 0 1 STATE50 0 0 0 010 STATE51 1 0 0 0 STATE50 0 0 0 000 STATE52 1 0 0 0 STATE50 0 0 0 011 STATE55 1 0 0 0 STATE50 0 0 0 001 STATE60 1 0 0 0 STATE50 0 0 0 111 STATE50 0 0 0 0 STATE50 0 0 0 110 STATE50 0 0 0 0 Next, we need to choose a state representation. But this task is e ssentially done: each output bit ( A,R1, R2, andR3) is represented with one bit in the internal representation, and th e remaining six bits record the number of nickels held by the vending machine using an unsigned repre sentation. The choice of a numeric representation for the money held is importa nt, as it allows us to use an adder to compute the money held in the next state. 3.5.2 Encoders and Decoders Since we chose to prioritize purchases, let’s begin by building logic to pe rform state transitions for purchases. Ourﬁrsttaskistoimplementprioritizationamongthethreeselection buttons. Forthispurpose, weconstruct a 4-input priority encoder , which generates a signal Pwhenever any of its four input lines is active and encodes the index of the highest active input as a two-bit unsigned n umberS. A truth table for our priority encoder appears on the left below, with K-maps for each of the out put bits on the right. B3B2B1B0P S 1 x x x 1 11 0 1 x x 1 10 0 0 1 x 1 01 0 0 0 1 1 00 0 0 0 0 0 xxB1B0B3B2 00 01 11 10 00 01 11 1011 1 1P 11 1 1 11111 1 10 B1B0B3B2 00 01 11 10 000 01 11 100 11 1S1 110 1 1 111 11x B1B0B3B2 00 01 11 10 00 01 11 1011 1S 10 1 11 1 x0 1 1 0000'}"
"3.5 Finite State Machine Design Examples, Part II 109 From the K-maps, we extract the following equations: P=B3+B2+B1+B0 S1=B3+B2 S0=B3+B2B1 which allow us to implement our encoder as shown to the right. If we connect our buttons B1,B2, andB3to the priority encoder (and feed 0 into the fourth input), it produces a signalPindicatingthattheuseristryingtomakeapurchase and a two-bit signal Sindicating which item the user wants. (logic diagram)a 4−input priority encoderS0S1 B1 B0B2B3 P We also need to build logic to control the item release outputsR1,R2, andR3. An item should be released only when it has been selected (as indicated by the priority encoder signal S) and the vending machine has enough money. For now, let’s leave aside calcula- tion of the item release signal, which we call R, and focus on how we can produce the correctvalues of R1, R2, andR3fromSandR. The component to the right is a decoder with an enable input. A decoder takes an input signal— typically one coded as a binary number—and produces one output for each possible value of the signal. You may notice the similarity with the structure of a mux: when the decoder is en- abled (EN= 1), each of the AND gates produces2S1 S0 D1D2D3 D0EN (logic diagram)a 2−to−4 decoder with enablea 2−to−4 decoder with enable (symbolic form)S D0D1D2D3 0123EN one minterm of the input signal S. In the mux, each of the inputs is then included in one minterm’s AND gate, and the outputs of all AND gates are ORd together. In the d ecoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produ ces exactly one 1 bit on its outputs. When not enabled ( EN= 0), the decoder produces all 0 bits. We use a decoder to generate the releasesignalsfor the vending ma chine by connecting the signal Sproduced bythe priorityencoderto the decoder’s Sinput andconnecting the item releasesignal Rtothe decoder’s EN input. The outputs D1,D2, andD3then correspond to the individual item release signals R1,R2, andR3 for our vending machine.","{'page_number': 114, 'textbook_name': 'ECE-120-student-notes', 'text': '3.5 Finite State Machine Design Examples, Part II 109 From the K-maps, we extract the following equations: P=B3+B2+B1+B0 S1=B3+B2 S0=B3+B2B1 which allow us to implement our encoder as shown to the right. If we connect our buttons B1,B2, andB3to the priority encoder (and feed 0 into the fourth input), it produces a signalPindicatingthattheuseristryingtomakeapurchase and a two-bit signal Sindicating which item the user wants. (logic diagram)a 4−input priority encoderS0S1 B1 B0B2B3 P We also need to build logic to control the item release outputsR1,R2, andR3. An item should be released only when it has been selected (as indicated by the priority encoder signal S) and the vending machine has enough money. For now, let’s leave aside calcula- tion of the item release signal, which we call R, and focus on how we can produce the correctvalues of R1, R2, andR3fromSandR. The component to the right is a decoder with an enable input. A decoder takes an input signal— typically one coded as a binary number—and produces one output for each possible value of the signal. You may notice the similarity with the structure of a mux: when the decoder is en- abled (EN= 1), each of the AND gates produces2S1 S0 D1D2D3 D0EN (logic diagram)a 2−to−4 decoder with enablea 2−to−4 decoder with enable (symbolic form)S D0D1D2D3 0123EN one minterm of the input signal S. In the mux, each of the inputs is then included in one minterm’s AND gate, and the outputs of all AND gates are ORd together. In the d ecoder, the AND gate outputs are the outputs of the decoder. Thus, when enabled, the decoder produ ces exactly one 1 bit on its outputs. When not enabled ( EN= 0), the decoder produces all 0 bits. We use a decoder to generate the releasesignalsfor the vending ma chine by connecting the signal Sproduced bythe priorityencoderto the decoder’s Sinput andconnecting the item releasesignal Rtothe decoder’s EN input. The outputs D1,D2, andD3then correspond to the individual item release signals R1,R2, andR3 for our vending machine.'}"
"110  3.5.3 Vending Machine Implementation We are now ready to implement the FSM to handle purchases, as shown to the right. The current num- ber of nickels, N, is stored in a register in the center of the diagram. Each cycle, Nis fed into a 6-bit adder, which subtracts the price of any purchase re- quested in that cycle. Recall that we chose to record item prices in registers. We avoid the need to negate prices before adding them by storing the negated prices in our registers. Thus, the value of register PRICE1 is−P1, the the value of register PRICE2 is−P2, and the the value of register PRICE3 is −P3. The priority encoder’s Ssignal is then used to se- lect the value of one of these three registers (using a 24-to-6 mux) as the second input to the adder. We use the adder to execute a subtraction, so the carryout Coutis1wheneverthevalue of Nisatleast as great as the amount being subtracted. In that case, the purchase is successful. The AND gate on the left calculatesthe signal Rindicating asuccessful purchase, which is then used to select the next value ofNusing the 12-to-6 mux below the adder. When no item selection buttons are pushed, Pand thus R are both 0, and the mux below the adder keeps N unchanged in the next cycle. Similarly, if P= 1 butNis insuﬃcient, Coutand thus Rare both 0, and again Ndoes not change. Only when P= 1 andCout= 1 is the purchase successful, in which case the price is subtracted from Nin the next cycle.6 6 6 6 6N 6 62 2 26 6BB B R R R66 60 0 R0 PSPRICE3PRICE2PRICE1 6−bit register23 1priority encoder 3 2 16−bit register6−bit register 1−bit register 1−bit register 1−bit register6−bit register 3EN 2 1 00 2 3 1 0 16−bit adderoutC inC The signal Ris also used to enable a decoder that generates the three individual item release outputs. The correct output is generated based on the decoded Ssignal from the priority encoder, and all three output bits are latched into registers to release the purchased item in the n ext cycle. One minor note on the design so far: by hardwiring Cinto 0, we created a problem for items that cost nothing (0 nickels): in that case, Coutis always 0. We could instead store −P1−1 in PRICE1 (and so forth) and feed Pin toCin, but maybe it’s better not to allow free items. How can we support coin insertion? Let’s use the same adder to add each inserted coin’s value to N. The table at the right shows thevalueofeachcoinasa5-bitunsignednumberofnickels. Using this table, we canﬁll in K-mapsforeachbit of V, asshownbelow. Notice that we have marked the two undeﬁned bit patterns for the coin type Cas don’t cares in the K-maps","{'page_number': 115, 'textbook_name': 'ECE-120-student-notes', 'text': 'How can we support coin insertion? Let’s use the same adder to add each inserted coin’s value to N. The table at the right shows thevalueofeachcoinasa5-bitunsignednumberofnickels. Using this table, we canﬁll in K-mapsforeachbit of V, asshownbelow. Notice that we have marked the two undeﬁned bit patterns for the coin type Cas don’t cares in the K-maps.coin type C2C1C0V4V3V2V1V0 none 110 00000 nickel 010 00001 dime 000 00010 quarter 011 00101 half dollar 001 01010 dollar 111 10100 C01C 0 100 01 11 10 00 0 1 0V4 0x x2C C01C 0 100 01 11 10 00 0V 0x x 1 03 C2 C01C 0 100 01 11 10 00 0 1 0V x x 122C C01C 0 100 01 11 10 0 0V 0x x1 1 1 02C C01C 0 100 01 11 10 0 0 0V4 x x1 1 02C'}"
"How can we support coin insertion? Let’s use the same adder to add each inserted coin’s value to N. The table at the right shows thevalueofeachcoinasa5-bitunsignednumberofnickels. Using this table, we canﬁll in K-mapsforeachbit of V, asshownbelow. Notice that we have marked the two undeﬁned bit patterns for the coin type Cas don’t cares in the K-maps.coin type C2C1C0V4V3V2V1V0 none 110 00000 nickel 010 00001 dime 000 00010 quarter 011 00101 half dollar 001 01010 dollar 111 10100 C01C 0 100 01 11 10 00 0 1 0V4 0x x2C C01C 0 100 01 11 10 00 0V 0x x 1 03 C2 C01C 0 100 01 11 10 00 0 1 0V x x 122C C01C 0 100 01 11 10 0 0V 0x x1 1 1 02C C01C 0 100 01 11 10 0 0 0V4 x x1 1 02C","{'page_number': 115, 'textbook_name': 'ECE-120-student-notes', 'text': 'How can we support coin insertion? Let’s use the same adder to add each inserted coin’s value to N. The table at the right shows thevalueofeachcoinasa5-bitunsignednumberofnickels. Using this table, we canﬁll in K-mapsforeachbit of V, asshownbelow. Notice that we have marked the two undeﬁned bit patterns for the coin type Cas don’t cares in the K-maps.coin type C2C1C0V4V3V2V1V0 none 110 00000 nickel 010 00001 dime 000 00010 quarter 011 00101 half dollar 001 01010 dollar 111 10100 C01C 0 100 01 11 10 00 0 1 0V4 0x x2C C01C 0 100 01 11 10 00 0V 0x x 1 03 C2 C01C 0 100 01 11 10 00 0 1 0V x x 122C C01C 0 100 01 11 10 0 0V 0x x1 1 1 02C C01C 0 100 01 11 10 0 0 0V4 x x1 1 02C'}"
"3.5 Finite State Machine Design Examples, Part II 111 Solving the K-maps gives the following equations, which we implement as shown to the right. V4=C2C0 V3=C1C0 V2=C1C0 V1=C1 V0=C2C1C2 C1 C0V4 V3 V2 V0V1 coin value calculator Now we can extend the design to handle coin inser- tion, as shown to the right with new elements high- lighted in blue. The output of the coin value calcu- lator is extended with a leading 0 and then fed into a 12-to-6 mux. When a purchase is requested, P= 1 and the mux forwards the item price to the adder— recall that we chose to give purchases priority over coin insertion. When no purchase is requested, the value of any coin inserted (or 0 when no coin is in- serted) is passed to the adder. Two new gates have been added on the lower left. First, let’s verify that purchases work as before. When a purchase is requested, P= 1, so the NOR gate outputs 0, and the OR gate simple forwards R tocontrolthemuxthatdecideswhetherthepurchase was successful, just as in our original design. When no purchase is made ( P= 0, and R= 0), the adder adds the value of any inserted coin to N. If the addition overﬂows, Cout= 1, and the output of the NOR gate is 0. Note that the NOR gate output is stored as the output Ain the next cycle, so a coin thatcausesoverﬂowintheamountofmoneystoredis rejected. TheORgatealsooutputs0, and Nremains unchanged. If the addition does not overﬂow, the NOR gate outputs a 1, the coin is accepted ( A= 1 in the next cycle), andthe mux allowsthe sum N+V to be written back as the new value of N.6 6 6 6 6N 6 666 6PRICE2 PRICE3PRICE1 2 2 26 6BB B R R R0C V [4:0] [5]0 0 R0 PS3 5 6 A6−bit register6−bit register6−bit register 6−bit register 23 1priority encoder 3 2 11−bit register 1−bit register 1−bit registercoin value calculator 1−bit register 3EN 2 1 00 2 3 1 0 10 1 6−bit adderoutC inC The tables at the top of the next page deﬁne all of the state variab les, inputs, outputs, and internal signals used in the design, and list the number of bits for each variable.","{'page_number': 116, 'textbook_name': 'ECE-120-student-notes', 'text': '3.5 Finite State Machine Design Examples, Part II 111 Solving the K-maps gives the following equations, which we implement as shown to the right. V4=C2C0 V3=C1C0 V2=C1C0 V1=C1 V0=C2C1C2 C1 C0V4 V3 V2 V0V1 coin value calculator Now we can extend the design to handle coin inser- tion, as shown to the right with new elements high- lighted in blue. The output of the coin value calcu- lator is extended with a leading 0 and then fed into a 12-to-6 mux. When a purchase is requested, P= 1 and the mux forwards the item price to the adder— recall that we chose to give purchases priority over coin insertion. When no purchase is requested, the value of any coin inserted (or 0 when no coin is in- serted) is passed to the adder. Two new gates have been added on the lower left. First, let’s verify that purchases work as before. When a purchase is requested, P= 1, so the NOR gate outputs 0, and the OR gate simple forwards R tocontrolthemuxthatdecideswhetherthepurchase was successful, just as in our original design. When no purchase is made ( P= 0, and R= 0), the adder adds the value of any inserted coin to N. If the addition overﬂows, Cout= 1, and the output of the NOR gate is 0. Note that the NOR gate output is stored as the output Ain the next cycle, so a coin thatcausesoverﬂowintheamountofmoneystoredis rejected. TheORgatealsooutputs0, and Nremains unchanged. If the addition does not overﬂow, the NOR gate outputs a 1, the coin is accepted ( A= 1 in the next cycle), andthe mux allowsthe sum N+V to be written back as the new value of N.6 6 6 6 6N 6 666 6PRICE2 PRICE3PRICE1 2 2 26 6BB B R R R0C V [4:0] [5]0 0 R0 PS3 5 6 A6−bit register6−bit register6−bit register 6−bit register 23 1priority encoder 3 2 11−bit register 1−bit register 1−bit registercoin value calculator 1−bit register 3EN 2 1 00 2 3 1 0 10 1 6−bit adderoutC inC The tables at the top of the next page deﬁne all of the state variab les, inputs, outputs, and internal signals used in the design, and list the number of bits for each variable.'}"
112  FSM state PRICE1 6 negated price of item 1 ( −P1) PRICE2 6 negated price of item 2 ( −P2) PRICE3 6 negated price of item 3 ( −P3) N6 value of money in machine (in nickels) A1 stored value of accept coin output R11 stored value of release item 1 output R21 stored value of release item 2 output R31 stored value of release item 3 output internal signals V5 inserted coin value in nickels P1 purchase requested (from priority encoder) S2 item # requested (from priority encoder) R1 release item (purchase approved)inputs B11 item 1 selected for purchase B21 item 2 selected for purchase B31 item 3 selected for purchase C3 coin inserted (see earlier table for meaning) outputs A1 accept inserted coin (last cycle) R11 release item 1 R21 release item 2 R31 release item 3 Note that outputs correspond one-to-one with four bits of FSM state,"{'page_number': 117, 'textbook_name': 'ECE-120-student-notes', 'text': 'The conditions here require that th e farmer be on the same bank as any entity that the player wants the farmer to carry across the river . Next, we specify the I/O interface. For input, the game has ﬁve bu ttons. A reset button Rforces the FSM back into the initial state. The other four buttons cause the farm er to cross the river: BFcrosses alone, BXwith the fox, BGwith the goose, and BCwith the corn. For output, we need position indicators for the four entities, but le t’s assume that we can simply output the current state FXGCand have appropriate images or lights appear on the correct banks of the river. We also need two more indicators: Wfor reaching the winning state, and Lfor reaching a losing state.'}"
"3.5.4 Design of a Game Controller For the next example, imagine that you arepart ofateam building a ga mefor childrento play at Engineering Open House. The game revolves around an old logic problem in which a fa rmer must cross a river in order to reach the market. The farmer is traveling to the market to sell a fox, a goose, and some corn. The farmer has a boat, but the boat is only large enough to carry the fox, the g oose, or the corn along with the farmer. The farmer knows that if he leaves the fox alone with the goose, the fox will eat the goose. Similarly, if the farmer leaves the goose alone with the corn, the goose will eat the c orn. How can the farmer cross the river? Your team decides to build a board illustrating the problem with a river f rom top to bottom and lights illustrating the positions of the farmer (always with the boat), the f ox, the goose, and the corn on either the left bank or the right bank of the river. Everything starts on t he left bank, and the children can play the game until they win by getting everything to the right bank or un til they make a mistake. As the ECE major on your team, you get to design the FSM! Since the four entities (farmer, fox, goose, and corn) can be only on one bank or the other, we can use one bit to represent the location of each entity. Rather than giving the states names, let’s just call a state FXGC. The value of Frepresents the location of the farmer, either on the left bank ( F= 0) or the right bank (F= 1). Using the same representation (0 for the left bank, 1 for the right bank), the value of X represents the location of the fox, Grepresents the location of the goose, and Crepresents the location of the corn. We can now put together an abstract next- state table, as shown to the right. Once the player wins or loses, let’s have the game indi- cate their ﬁnal status and stop accepting re- quests to have the farmer cross the river. We can use a reset button to force the game back into the original state for the next player. Note that we have included conditions for some of the input events, as we did previouslyinitial state input event condition ﬁnal state FXGC no input always FXGC FXGC reset always 0000 FXGC cross alone always ¯FXGC FXGC cross with fox F=X¯F¯XGC FXGC cross with goose F=G¯FX¯GC FXGC cross with corn F=C¯FXG¯C with the vending machine design. The conditions here require that th e farmer be on the same bank as any entity that the player wants the farmer to carry across the river . Next, we specify the I/O interface. For input, the game has ﬁve bu ttons. A reset button Rforces the FSM back into the initial state. The other four buttons cause the farm er to cross the river: BFcrosses alone, BXwith the fox, BGwith the goose, and BCwith the corn","{'page_number': 117, 'textbook_name': 'ECE-120-student-notes', 'text': 'The conditions here require that th e farmer be on the same bank as any entity that the player wants the farmer to carry across the river . Next, we specify the I/O interface. For input, the game has ﬁve bu ttons. A reset button Rforces the FSM back into the initial state. The other four buttons cause the farm er to cross the river: BFcrosses alone, BXwith the fox, BGwith the goose, and BCwith the corn. For output, we need position indicators for the four entities, but le t’s assume that we can simply output the current state FXGCand have appropriate images or lights appear on the correct banks of the river. We also need two more indicators: Wfor reaching the winning state, and Lfor reaching a losing state.'}"
"The conditions here require that th e farmer be on the same bank as any entity that the player wants the farmer to carry across the river . Next, we specify the I/O interface. For input, the game has ﬁve bu ttons. A reset button Rforces the FSM back into the initial state. The other four buttons cause the farm er to cross the river: BFcrosses alone, BXwith the fox, BGwith the goose, and BCwith the corn. For output, we need position indicators for the four entities, but le t’s assume that we can simply output the current state FXGCand have appropriate images or lights appear on the correct banks of the river. We also need two more indicators: Wfor reaching the winning state, and Lfor reaching a losing state.","{'page_number': 117, 'textbook_name': 'ECE-120-student-notes', 'text': 'The conditions here require that th e farmer be on the same bank as any entity that the player wants the farmer to carry across the river . Next, we specify the I/O interface. For input, the game has ﬁve bu ttons. A reset button Rforces the FSM back into the initial state. The other four buttons cause the farm er to cross the river: BFcrosses alone, BXwith the fox, BGwith the goose, and BCwith the corn. For output, we need position indicators for the four entities, but le t’s assume that we can simply output the current state FXGCand have appropriate images or lights appear on the correct banks of the river. We also need two more indicators: Wfor reaching the winning state, and Lfor reaching a losing state.'}"
"3.5 Finite State Machine Design Examples, Part II 113 Now we are ready to complete the speciﬁcation. We could use a strict prioritization of input events, as we did with earlier examples. Instead, in order to vary the designs a bit, we use a strict prioritization among allowed inputs. The reset button Rhas the highest priority, followed by BF,BC,BG, and ﬁnally BX. However, only those buttons that result in an allowed move are cons idered when selecting one button among several pressed in a single clock cycle. As an example, consider the state FXGC = 0101. The farmer is not on the same bank as the fox, nor as the corn, so the BXandBCbuttons are ignored, leading to the next-state table to the right. Notice that BGis accepted even if BC is pressed because the farmer is not on the sameFXGC R B FBXBGBCF+X+G+C+ 0101 1 x x x x 0000 0101 0 1 x x x 1101 0101 0 0 x 1 x 1111 0101 0 0 x 0 x 0101 bank as the corn. As shown later, this approach to prioritization of inputs is equally simple in terms of implementation. Recall that we want to stop the game when the player wins or loses. In these states, only the reset button is accepted. For example, thestate FXGC= 0110isalosingstatebecauseFXGC R B FBXBGBCF+X+G+C+ 0110 1 x x x x 0000 0110 0 x x x x 0110 the farmer has left the fox with the goose on the opposite side of th e river. In this case, the player can reset the game, but other buttons are ignored. As we have already chosen a representation, we now move on to imple ment the FSM. Let’s begin by cal- culating the next state ignoring the reset button and the winning an d losing states, as shown in the logic diagram below. B BBB 2F C G XSP 3 2 1 0C G X F G XCF priority encoder+ + + +3EN 2 1 0 TheleftcolumnofXORgatesdetermineswhetherthefarmerisonth esamebankasthecorn(topgate), goose (middle gate), and fox (bottom gate). The output of each gate is t hen used to mask out the corresponding button: only when the farmer is on the same bank are these button s considered. The adjusted button values are then fed into a priority encoder, which selects the highest prior ity input event according to the scheme that we outlined earlier (from highest to lowest, BF,BC,BG, andBX, ignoring the reset button). The output of the priority encoder is then used to drive another co lumn of XOR gates on the right in order to calculate the next state. If any of the allowed buttons is presse d, the priority encoder outputs P= 1, and the farmer’s bank is changed. If BCis allowed and selected by the priority encoder (only when BFis not pressed), both the farmer and the corn’s banks are ﬂipped. The g oose and the fox are handled in the same way.","{'page_number': 118, 'textbook_name': 'ECE-120-student-notes', 'text': '3.5 Finite State Machine Design Examples, Part II 113 Now we are ready to complete the speciﬁcation. We could use a strict prioritization of input events, as we did with earlier examples. Instead, in order to vary the designs a bit, we use a strict prioritization among allowed inputs. The reset button Rhas the highest priority, followed by BF,BC,BG, and ﬁnally BX. However, only those buttons that result in an allowed move are cons idered when selecting one button among several pressed in a single clock cycle. As an example, consider the state FXGC = 0101. The farmer is not on the same bank as the fox, nor as the corn, so the BXandBCbuttons are ignored, leading to the next-state table to the right. Notice that BGis accepted even if BC is pressed because the farmer is not on the sameFXGC R B FBXBGBCF+X+G+C+ 0101 1 x x x x 0000 0101 0 1 x x x 1101 0101 0 0 x 1 x 1111 0101 0 0 x 0 x 0101 bank as the corn. As shown later, this approach to prioritization of inputs is equally simple in terms of implementation. Recall that we want to stop the game when the player wins or loses. In these states, only the reset button is accepted. For example, thestate FXGC= 0110isalosingstatebecauseFXGC R B FBXBGBCF+X+G+C+ 0110 1 x x x x 0000 0110 0 x x x x 0110 the farmer has left the fox with the goose on the opposite side of th e river. In this case, the player can reset the game, but other buttons are ignored. As we have already chosen a representation, we now move on to imple ment the FSM. Let’s begin by cal- culating the next state ignoring the reset button and the winning an d losing states, as shown in the logic diagram below. B BBB 2F C G XSP 3 2 1 0C G X F G XCF priority encoder+ + + +3EN 2 1 0 TheleftcolumnofXORgatesdetermineswhetherthefarmerisonth esamebankasthecorn(topgate), goose (middle gate), and fox (bottom gate). The output of each gate is t hen used to mask out the corresponding button: only when the farmer is on the same bank are these button s considered. The adjusted button values are then fed into a priority encoder, which selects the highest prior ity input event according to the scheme that we outlined earlier (from highest to lowest, BF,BC,BG, andBX, ignoring the reset button). The output of the priority encoder is then used to drive another co lumn of XOR gates on the right in order to calculate the next state. If any of the allowed buttons is presse d, the priority encoder outputs P= 1, and the farmer’s bank is changed. If BCis allowed and selected by the priority encoder (only when BFis not pressed), both the farmer and the corn’s banks are ﬂipped. The g oose and the fox are handled in the same way.'}"
"114  Next, let’s build a compo- nent to produce the win and lose signals. The one winning state is FXGC= 1111, so we simply need an AND gate. For the lose signalL, we can ﬁll in a K-map and derive an ex- pression, as shown to the right, then implement as shown in the logic dia- gram to the far right. For theK-map, rememberthat the player loses whenever the fox and the goose are on the same side of the river, but opposite from the farmer, or whenever the goose and the corn are on the same side of the river, but opposite from the farmer.1 1 0 000 01 11 10 00 01 11 101 11 1L GC0 0 0 00 0 00FX L=F¯X¯G+¯FXG+ F¯G¯C+¯FGCLC X G F W Finally, we complete our design by integrat- ing the next-state calculation and the win-lose calculation with a couple of muxes, as shown to the right. The lower mux controls the ﬁnal value of the next state: note that it selects be- tween the constant state FXGC= 0000 when the reset button Ris pressed and the output of the upper mux when R= 0. The upper mux is controlled by W+L, and retains the cur- rent state whenever either signal is 1. In other words, once the player has won or lost, the up- per mux prevents further state changes until the resetbutton ispressed. When R,W, andL are all 0, the next state is calculated according to whatever buttons have been pressed.BB B B 4 4 44 44 W L 0000 RW L XF G Cnext−state calculation4−bit register FXGCcalculationwin−lose 0 10 1 3.5.5 Analysis of a Stoplight Controller In this example, we begin with a digital FSM design and analyze it to unde rstand how it works and to verify that its behavior is appropriate. The FSM that we analyze has been d esigned to control the stoplights at the intersection of two roads. For naming purposes, we assume that o ne of the roads runs East and West (EW), and the second runs North and South (NS). The stoplight controller has two inputs, each of which senses vehicle s approaching from either direction on one of the two roads. The input VEW= 1 when a vehicle approaches from either the East or the West, and the input VNS= 1 when a vehicle approaches from either the North or the South. T hese inputs are also active when vehicles are stopped waiting at the corresponding lights . Another three inputs, A,B, andC, control the timing behavior of the system; we do not discuss them h ere except as variables. The outputs of the controller consist of two 2-bit values, LEWandLNS, that specify the light colors for the two roads. In particular, LEWcontrols the lights facing East and West, and LNScontrols the lights facing North and South. The meaning of these outputs is given in the table to the right.Llight color 0xred 10yellow 11green","{'page_number': 119, 'textbook_name': 'ECE-120-student-notes', 'text': '114  Next, let’s build a compo- nent to produce the win and lose signals. The one winning state is FXGC= 1111, so we simply need an AND gate. For the lose signalL, we can ﬁll in a K-map and derive an ex- pression, as shown to the right, then implement as shown in the logic dia- gram to the far right. For theK-map, rememberthat the player loses whenever the fox and the goose are on the same side of the river, but opposite from the farmer, or whenever the goose and the corn are on the same side of the river, but opposite from the farmer.1 1 0 000 01 11 10 00 01 11 101 11 1L GC0 0 0 00 0 00FX L=F¯X¯G+¯FXG+ F¯G¯C+¯FGCLC X G F W Finally, we complete our design by integrat- ing the next-state calculation and the win-lose calculation with a couple of muxes, as shown to the right. The lower mux controls the ﬁnal value of the next state: note that it selects be- tween the constant state FXGC= 0000 when the reset button Ris pressed and the output of the upper mux when R= 0. The upper mux is controlled by W+L, and retains the cur- rent state whenever either signal is 1. In other words, once the player has won or lost, the up- per mux prevents further state changes until the resetbutton ispressed. When R,W, andL are all 0, the next state is calculated according to whatever buttons have been pressed.BB B B 4 4 44 44 W L 0000 RW L XF G Cnext−state calculation4−bit register FXGCcalculationwin−lose 0 10 1 3.5.5 Analysis of a Stoplight Controller In this example, we begin with a digital FSM design and analyze it to unde rstand how it works and to verify that its behavior is appropriate. The FSM that we analyze has been d esigned to control the stoplights at the intersection of two roads. For naming purposes, we assume that o ne of the roads runs East and West (EW), and the second runs North and South (NS). The stoplight controller has two inputs, each of which senses vehicle s approaching from either direction on one of the two roads. The input VEW= 1 when a vehicle approaches from either the East or the West, and the input VNS= 1 when a vehicle approaches from either the North or the South. T hese inputs are also active when vehicles are stopped waiting at the corresponding lights . Another three inputs, A,B, andC, control the timing behavior of the system; we do not discuss them h ere except as variables. The outputs of the controller consist of two 2-bit values, LEWandLNS, that specify the light colors for the two roads. In particular, LEWcontrols the lights facing East and West, and LNScontrols the lights facing North and South. The meaning of these outputs is given in the table to the right.Llight color 0xred 10yellow 11green'}"
"3.5 Finite State Machine Design Examples, Part II 115 Let’s think about the basic operation of the controller. For safety reasons, the controller must ensure that the lights on one or both roads are red at all times. Similarly, if a road h as a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. Finally, for fairness, the controller should alternate green lig hts between the two roads. Now take a look at the logic diagram below. The state of the FSM has be en split into two pieces: a 3-bit registerSand a 6-bit timer. The timer is simply a binary counter that counts dow nward and produces an output of Z= 1 when it reaches 0. Notice that the register Sonly takes a new value when the timer reaches 0, and that the Zsignal from the timer also forces a new value to be loaded into the time r in the next cycle. We can thus think of transitions in the FSM on a cycle by cycle ba sis as consisting of two types. The ﬁrst type simply counts downward for a number of cycles while holding the register Sconstant, while the second changes the value of Sand sets the timer in order to maintain the new value of Sfor some number of cycles. 6 6 IN IN IN S S S L L L L VV66 6 LD LD Z F6 2 1 0 1 2 03−bit register counter)(binary down6−bit timer 1EW 0EW 1NS 0NS NSEW0 1 0 1A B C","{'page_number': 120, 'textbook_name': 'ECE-120-student-notes', 'text': '3.5 Finite State Machine Design Examples, Part II 115 Let’s think about the basic operation of the controller. For safety reasons, the controller must ensure that the lights on one or both roads are red at all times. Similarly, if a road h as a green light, the controller should show a yellow light before showing a red light to give drivers some warning and allow them to slow down. Finally, for fairness, the controller should alternate green lig hts between the two roads. Now take a look at the logic diagram below. The state of the FSM has be en split into two pieces: a 3-bit registerSand a 6-bit timer. The timer is simply a binary counter that counts dow nward and produces an output of Z= 1 when it reaches 0. Notice that the register Sonly takes a new value when the timer reaches 0, and that the Zsignal from the timer also forces a new value to be loaded into the time r in the next cycle. We can thus think of transitions in the FSM on a cycle by cycle ba sis as consisting of two types. The ﬁrst type simply counts downward for a number of cycles while holding the register Sconstant, while the second changes the value of Sand sets the timer in order to maintain the new value of Sfor some number of cycles. 6 6 IN IN IN S S S L L L L VV66 6 LD LD Z F6 2 1 0 1 2 03−bit register counter)(binary down6−bit timer 1EW 0EW 1NS 0NS NSEW0 1 0 1A B C'}"
"116  Let’s look at the next-state logic for S, which feeds intothe INinputs onthe 3-bit register ( S+ 2=IN2and so forth). Notice thatnoneoftheinputstotheFSMdirectly aﬀect these values. The states of Sthus act like a counter. By examining the con- nections, we can derive equations for the next state and draw a transition diagram, as shown to the right. As the ﬁgure shows, there are six states in the loop deﬁned by the next-state logic, with the two remain- ing states converging into the loop after a single cycle. Let’s now examine the outputs for each state in order to understand how the stop- light sequencing works. We derive equa- tions for the outputs that control the lights, asshownto theright, then calculate values and colors for each state, as shown to the far right. For completeness, the ta- ble includes the states outside of the de- sired loop. The lights are all red in both of these states, which is necessary for safety.S+ 2=S2+S0 S+ 1=S2⊕S1 S+ 0=S2 LEW 1=S2S1 LEW 0=S0 LNS 1=S2S1 LNS 0=S0110000 001 011 101111 010100 EW NS light light SLEWLNScolor color 00000 00 red red 11111 01 green red 11010 00 yellow red 01000 00 red red 10101 11 red green 10000 10 red yellow 00101 01 red red 01101 01 red red Now let’s think about how the timer works. As we already noted, the t imer value is set whenever Senters a new state, but it can also be set under other conditions—in particu lar, by the signal Fcalculated at the bottom of the FSM logic diagram. For now, assume that F= 0. In this case, the timer is set only when the state Schanges, and we can ﬁnd the duration of each state by analyzing the muxes. The bot- tom mux selects AwhenS2= 0, and selects the output of the top mux when S2= 1. The top mux selects B whenS0= 1, and selects CwhenS0= 0. Combining these results, we can calculate the duration of the next states of SwhenF= 0, as shown in the table to the right. We can then combine the next state duration with our previous calculation of the state sequencing (also the order in the table) to obtain the durations of each state, also shown in the rightmost column of the table.EW NS next current light light state state Scolor color duration duration 000red red A C 111green red B A 110yellow red C B 010red red A C 101red green B A 100red yellow C B 001red red A — 011red red A — What does Fdo? Analyzing the gates that produce it gives F=S1S0VNS+S1S0VEW","{'page_number': 121, 'textbook_name': 'ECE-120-student-notes', 'text': 'EW NS next current light light state state Scolor color duration duration 000red red A C 111green red B A 110yellow red C B 010red red A C 101red green B A 100red yellow C B 001red red A — 011red red A — What does Fdo? Analyzing the gates that produce it gives F=S1S0VNS+S1S0VEW. If we ignore the two states outside of the main loop for S, the ﬁrst term is 1 only when the lights are green on the East and Wes t roads and the detector for the North and South roads indicates t hat no vehicles are approaching. Similarly, the second term is 1 only when the lights are green on the North and S outh roads and the detector for the East and West roads indicates that no vehicles are approaching. What happens when F= 1? First, the OR gate feeding into the timer’s LDinput produces a 1, meaning that the timer loads a new value instead of counting down. Second, t he OR gate controlling the lower mux selects the Ainput. In other words, the timer is reset to Acycles, corresponding to the initial value for the green light states. In other words, the light stays green until veh icles approach on the other road, plus A more cycles. Unfortunately, the signal Fmay also be 1 in the unused states of S, in which case the lights on both roads may remain red even though cars are waiting on one of the roads. To avoid this behavior, we must be sure to initialize the state Sto one of the six states in the desired loop.'}"
"EW NS next current light light state state Scolor color duration duration 000red red A C 111green red B A 110yellow red C B 010red red A C 101red green B A 100red yellow C B 001red red A — 011red red A — What does Fdo? Analyzing the gates that produce it gives F=S1S0VNS+S1S0VEW. If we ignore the two states outside of the main loop for S, the ﬁrst term is 1 only when the lights are green on the East and Wes t roads and the detector for the North and South roads indicates t hat no vehicles are approaching. Similarly, the second term is 1 only when the lights are green on the North and S outh roads and the detector for the East and West roads indicates that no vehicles are approaching. What happens when F= 1? First, the OR gate feeding into the timer’s LDinput produces a 1, meaning that the timer loads a new value instead of counting down. Second, t he OR gate controlling the lower mux selects the Ainput. In other words, the timer is reset to Acycles, corresponding to the initial value for the green light states. In other words, the light stays green until veh icles approach on the other road, plus A more cycles. Unfortunately, the signal Fmay also be 1 in the unused states of S, in which case the lights on both roads may remain red even though cars are waiting on one of the roads. To avoid this behavior, we must be sure to initialize the state Sto one of the six states in the desired loop.","{'page_number': 121, 'textbook_name': 'ECE-120-student-notes', 'text': 'EW NS next current light light state state Scolor color duration duration 000red red A C 111green red B A 110yellow red C B 010red red A C 101red green B A 100red yellow C B 001red red A — 011red red A — What does Fdo? Analyzing the gates that produce it gives F=S1S0VNS+S1S0VEW. If we ignore the two states outside of the main loop for S, the ﬁrst term is 1 only when the lights are green on the East and Wes t roads and the detector for the North and South roads indicates t hat no vehicles are approaching. Similarly, the second term is 1 only when the lights are green on the North and S outh roads and the detector for the East and West roads indicates that no vehicles are approaching. What happens when F= 1? First, the OR gate feeding into the timer’s LDinput produces a 1, meaning that the timer loads a new value instead of counting down. Second, t he OR gate controlling the lower mux selects the Ainput. In other words, the timer is reset to Acycles, corresponding to the initial value for the green light states. In other words, the light stays green until veh icles approach on the other road, plus A more cycles. Unfortunately, the signal Fmay also be 1 in the unused states of S, in which case the lights on both roads may remain red even though cars are waiting on one of the roads. To avoid this behavior, we must be sure to initialize the state Sto one of the six states in the desired loop.'}"
"3.6 Random Access Memories 117 ECE120: Introduction to Computer Engineering Notes Set 3.6 Random Access Memories This set of notes describes random access memories (RAMs), prov iding slightly more detail than is available in the textbook. We begin with a discussion of the memory abstractio n and the types of memory most commonly used in digital systems, then examine how one can build memo ries (static RAMs) using logic. We next introduce tri-state buﬀers as a way of simplifying ouput con nections, and illustrate how memory chips can be combined to provide larger and wider memories. A more de tailed description of dynamic RAMs ﬁnishes this set. Sections marked with an asterisk are provided solely for you r interest, but you probably need to learn this material in later classes. 3.6.1 Memory A computer memory is a group of storage elements and the logic necessary to move data in and out of the elements. The size of the elements in a memory—called the addressability of the memory—varies from a single binary digit, or bit, to abyte(8 bits) or more. Typically, we refer to data elements larger than a byte aswords, but the size of a word depends on context. Each element in a memory is assigned a unique name, called an address, that allows an external circuit to identify the particular element of interest. These addresses ar e not unlike the street addresses that you use when you send a letter. Unlike street addresses, however, me mory addresses usually have little or no redundancy; eachpossiblecombinationofbitsinanaddressidentiﬁe sadistinctsetofbitsinthememory. The ﬁgure on the right below illustrates the concept. Each house repre sents a storage element and is associated with a unique address. ADDR CSWEDATA−IN DATA−OUT100 101 110 111011 010 001 000k NN memory2   x Nk The memories that we consider in this class have severalproperties in common. These memories support two operations: writeplaces a word of data into an element, and readretrieves a copy of a word of data from an element. The memories are also volatile, which means that the data held by a memory are erased when electrical power is turned oﬀ or fails. Non-volatile forms of memory include magnetic and optical storage media such as DVDs, CD-ROMs, disks, and tapes, capacitive storag e media such as Flash drives, and some programmable logic devices. Finally, the memories considered in this cla ss arerandom access memories (RAMs) , which means that the time required to access an element in the memo ry is independent of the element being accessed. In contrast, serial memories such as magnetic tape require much less time to access data near the current location in the tape than data far aw ay from the current location. The ﬁgure on the left above shows a generic RAM structure. The me mory contains 2kelements of Nbits each. Ak-bit address input, ADDR, identiﬁes the memory element of interest for any particular opera tion. The write enable input, WE, selects the operation to be performed: if WEis high, the operation is a write; if it is low, the operation is a read","{'page_number': 122, 'textbook_name': 'ECE-120-student-notes', 'text': 'The ﬁgure on the left above shows a generic RAM structure. The me mory contains 2kelements of Nbits each. Ak-bit address input, ADDR, identiﬁes the memory element of interest for any particular opera tion. The write enable input, WE, selects the operation to be performed: if WEis high, the operation is a write; if it is low, the operation is a read. Data to be written into an element ar e provided through Ninputs at the top, and data read from an element appear on Noutputs at the bottom. Finally, a chip select input, CS, functions as an enable control for the memory; when CSis low, the memory neither reads nor writes any location. Random access memory further divides into two important types: static RAM , orSRAM, anddynamic RAM, orDRAM. SRAM employs active logic in the form of a two-inverter loop to mainta in stored values.'}"
"The ﬁgure on the left above shows a generic RAM structure. The me mory contains 2kelements of Nbits each. Ak-bit address input, ADDR, identiﬁes the memory element of interest for any particular opera tion. The write enable input, WE, selects the operation to be performed: if WEis high, the operation is a write; if it is low, the operation is a read. Data to be written into an element ar e provided through Ninputs at the top, and data read from an element appear on Noutputs at the bottom. Finally, a chip select input, CS, functions as an enable control for the memory; when CSis low, the memory neither reads nor writes any location. Random access memory further divides into two important types: static RAM , orSRAM, anddynamic RAM, orDRAM. SRAM employs active logic in the form of a two-inverter loop to mainta in stored values.","{'page_number': 122, 'textbook_name': 'ECE-120-student-notes', 'text': 'The ﬁgure on the left above shows a generic RAM structure. The me mory contains 2kelements of Nbits each. Ak-bit address input, ADDR, identiﬁes the memory element of interest for any particular opera tion. The write enable input, WE, selects the operation to be performed: if WEis high, the operation is a write; if it is low, the operation is a read. Data to be written into an element ar e provided through Ninputs at the top, and data read from an element appear on Noutputs at the bottom. Finally, a chip select input, CS, functions as an enable control for the memory; when CSis low, the memory neither reads nor writes any location. Random access memory further divides into two important types: static RAM , orSRAM, anddynamic RAM, orDRAM. SRAM employs active logic in the form of a two-inverter loop to mainta in stored values.'}"
"118  DRAM uses a charged capacitor to store a bit; the charge drains ov er time and must be replaced, giving rise to the qualiﬁer “dynamic.” “Static” thus serves only to diﬀerentiate memories with active logic elements from those with capacitive elements. Both types are volatile, that is , both lose all data when the power supply is removed. We consider both SRAM and DRAM in this course, bu t the details of DRAM operation are beyond our scope. 3.6.2 Static Random Access Memory Staticrandomaccessmemoryisusedforhigh-speedapplicationssu chasprocessorcachesandsomeembedded designs. As SRAM bit density—the number of bits in a given chip area—is signiﬁcantly lower than DRAM bit density, most applications with less demanding speed requirement s use DRAM. The main memory in most computers, for example, is DRAM, whereas the memory on the same chip as a processor is SRAM.12 DRAM is also unavailable when recharging its capacitors, which can be a problem for applications with stringent real-time needs. A diagram of an SRAM cell(a single bit) appears to the right. A dual-inverter loop stores the bit, and is connected to opposing BIT lines through transistors controlled by a SELECT line. The cell works as follows. When SELECT is high, the transistors connect the inverter loop to the bit lines. When writing a cell, the bit lines are held at opposite logic values, forcing the inverters to match the values on the lines and storing the value from the BITinput. When reading a cell, the bit lines are disconnected from other logic, allowing the inverters to drive the lines with their current outputs.BIT BITSELECT The value stored previously is thus copied onto the BITline as an output, and the opposite value is placed on theBITline. When SELECT is low, the transistors disconnect the inverters from the bit lines, a nd the cell holds its current value until SELECT goes high again. The actual operation of an SRAM cell is more complicated than we hav e described. For example, when writing a bit, the BITlines can temporarily connect high voltage to ground (a short). The circuit must be designed carefully to minimize the power consumed during this proc ess. When reading a bit, the BIT lines are pre-charged halfway between high-voltage and ground, a nd analog devices called sense ampliﬁers are used to detect the voltage changes on the BITlines (driven by the inverter loop) as quickly as possible. These analog design issues are outside of the scope of our class. A number of cells are combined into a bit slice , as shown to the right. The labels along the bottom of the ﬁgure are external inputs to the bit slice, and match the labels for the abstract. . . EN 4 ADDR WE DATA−IN DATA−OUTWED Q CS4−to−16 decodercell 0cell cell cell 3cell 12cell 13cell 14cell 15 1 2and write logicread 4−to−16 decoderBB memory discussed earlier. The bit slice in the ﬁgure can be thought of as a 16-address, 1-bit-addressable memory (24×1b). The cells in a bit slice share bit lines and analogread and write logic, which a ppears to the right in the ﬁgure","{'page_number': 123, 'textbook_name': 'ECE-120-student-notes', 'text': 'The bit slice in the ﬁgure can be thought of as a 16-address, 1-bit-addressable memory (24×1b). The cells in a bit slice share bit lines and analogread and write logic, which a ppears to the right in the ﬁgure. Based on the ADDRinput, a decoder sets one cell’s SELECT line high to enable a read or write operation to the cell. The chip select input CSdrives the enable input of the decoder, so none of the memory cells is active when chip select is low ( CS= 0), and exactly one of the memory cells is active when chip select is high (CS= 1). Actual bit slices can contain many more cells than are shown in th e ﬁgure—more cells means less extra logic per cell, but slower memory, since longer wires h ave higher capacitance. 12Chips combining both DRAM and processor logic are available , and are used by some processor manufacturers (such as IBM). Research is underway to couple such logic types more eﬃ ciently by building 3D stacks of chips.'}"
"The bit slice in the ﬁgure can be thought of as a 16-address, 1-bit-addressable memory (24×1b). The cells in a bit slice share bit lines and analogread and write logic, which a ppears to the right in the ﬁgure. Based on the ADDRinput, a decoder sets one cell’s SELECT line high to enable a read or write operation to the cell. The chip select input CSdrives the enable input of the decoder, so none of the memory cells is active when chip select is low ( CS= 0), and exactly one of the memory cells is active when chip select is high (CS= 1). Actual bit slices can contain many more cells than are shown in th e ﬁgure—more cells means less extra logic per cell, but slower memory, since longer wires h ave higher capacitance. 12Chips combining both DRAM and processor logic are available , and are used by some processor manufacturers (such as IBM). Research is underway to couple such logic types more eﬃ ciently by building 3D stacks of chips.","{'page_number': 123, 'textbook_name': 'ECE-120-student-notes', 'text': 'The bit slice in the ﬁgure can be thought of as a 16-address, 1-bit-addressable memory (24×1b). The cells in a bit slice share bit lines and analogread and write logic, which a ppears to the right in the ﬁgure. Based on the ADDRinput, a decoder sets one cell’s SELECT line high to enable a read or write operation to the cell. The chip select input CSdrives the enable input of the decoder, so none of the memory cells is active when chip select is low ( CS= 0), and exactly one of the memory cells is active when chip select is high (CS= 1). Actual bit slices can contain many more cells than are shown in th e ﬁgure—more cells means less extra logic per cell, but slower memory, since longer wires h ave higher capacitance. 12Chips combining both DRAM and processor logic are available , and are used by some processor manufacturers (such as IBM). Research is underway to couple such logic types more eﬃ ciently by building 3D stacks of chips.'}"
"3.6 Random Access Memories 119 A read operation is performed as follows. We set CS= 1 and WE= 0, and place the address of the cell to be read on the ADDRinput. The decoder outputs a 1 on the appropriate cell’s SELECT line, and the read logic reads the bit from the cell and delivers it to its Qoutput, which is then available on the bit slice’sDATA-OUToutput. For a write operation, we set CS= 1 and WE= 1. We again place the address of the cell to be written on theADDRinput and set the value of the bit slice’s DATA-INinput to the value to be written into the memory cell. When the decoder activates the cell’s SELECT line, the write logic writes the new value from itsDinput into the memory cell. Later reads from that cell then produce the new value. . . .D Q WED Q WE D Q WE 2 4. . .. . .. . . WEQ D ADDR[5:4] EN EN DATA−OUT ADDR[3:0] WE DATA−IN CScell 0BB cell cell cell 3cell 12cell 13cell 14cell 15 1 2and write logicreadand write logicread and write logicread and write logicread 4−to−16 decoder 2BB cell cell cell 19cell 28cell 29cell 30cell 31 17 18cell 16BB cell cell cell 35cell 44cell 45cell 46cell 47 33 34cell 32BB cell cell cell 51cell 60cell 61cell 62cell 63 49 50cell 48 2−to−4 decoder 0 1 2 3 Theoutputsofthecellselectiondecodercanbeusedtocontrolm ultiplebitslices,asshownintheﬁgureabove of a 26×1b memory. Selection between bit slices is then based on other bits fr om the address ( ADDR). In the ﬁgure above, a 2-to-4 decoder is used to deliver write request s to one of four bit slices, and a 4-to-1 mux is used to choose the appropriate output bit for read requests. The 4-to-16 decoder now activates one cell in each of the four bit s lices. For a read operation, WE= 0, and the 2-to-4 decoder is not enabled, so it outputs all 0s. All four bit slices thus perform reads, and the desired result bit is forwarded to DATA-OUTby the 4-to-1 mux. The tri-state buﬀer between the mux andDATA-OUTis explained in a later section. For a write operation, exactly one of th e bit slices has itsWEinput set to 1 by the 2-to-4 decoder. That bit slice writes the bit valu e delivered to all bit slices fromDATA-IN. The other three bit slices perform reads, but their results are sim ply discarded. The approach shown above, in which a cell is selected through a two- dimensional indexing scheme, is known ascoincident selection . The qualiﬁer “coincident” arises from the notion that the desired c ell coincides with the intersection of the active row and column outputs from the decoders.","{'page_number': 124, 'textbook_name': 'ECE-120-student-notes', 'text': '3.6 Random Access Memories 119 A read operation is performed as follows. We set CS= 1 and WE= 0, and place the address of the cell to be read on the ADDRinput. The decoder outputs a 1 on the appropriate cell’s SELECT line, and the read logic reads the bit from the cell and delivers it to its Qoutput, which is then available on the bit slice’sDATA-OUToutput. For a write operation, we set CS= 1 and WE= 1. We again place the address of the cell to be written on theADDRinput and set the value of the bit slice’s DATA-INinput to the value to be written into the memory cell. When the decoder activates the cell’s SELECT line, the write logic writes the new value from itsDinput into the memory cell. Later reads from that cell then produce the new value. . . .D Q WED Q WE D Q WE 2 4. . .. . .. . . WEQ D ADDR[5:4] EN EN DATA−OUT ADDR[3:0] WE DATA−IN CScell 0BB cell cell cell 3cell 12cell 13cell 14cell 15 1 2and write logicreadand write logicread and write logicread and write logicread 4−to−16 decoder 2BB cell cell cell 19cell 28cell 29cell 30cell 31 17 18cell 16BB cell cell cell 35cell 44cell 45cell 46cell 47 33 34cell 32BB cell cell cell 51cell 60cell 61cell 62cell 63 49 50cell 48 2−to−4 decoder 0 1 2 3 Theoutputsofthecellselectiondecodercanbeusedtocontrolm ultiplebitslices,asshownintheﬁgureabove of a 26×1b memory. Selection between bit slices is then based on other bits fr om the address ( ADDR). In the ﬁgure above, a 2-to-4 decoder is used to deliver write request s to one of four bit slices, and a 4-to-1 mux is used to choose the appropriate output bit for read requests. The 4-to-16 decoder now activates one cell in each of the four bit s lices. For a read operation, WE= 0, and the 2-to-4 decoder is not enabled, so it outputs all 0s. All four bit slices thus perform reads, and the desired result bit is forwarded to DATA-OUTby the 4-to-1 mux. The tri-state buﬀer between the mux andDATA-OUTis explained in a later section. For a write operation, exactly one of th e bit slices has itsWEinput set to 1 by the 2-to-4 decoder. That bit slice writes the bit valu e delivered to all bit slices fromDATA-IN. The other three bit slices perform reads, but their results are sim ply discarded. The approach shown above, in which a cell is selected through a two- dimensional indexing scheme, is known ascoincident selection . The qualiﬁer “coincident” arises from the notion that the desired c ell coincides with the intersection of the active row and column outputs from the decoders.'}"
"120  The beneﬁt of coincident selection is easily calculated in terms of the n umber of gates required for the decoders. Decoder complexity is roughly equal to the number of ou tputs, as each output is a minterm and requires a unique gate to calculate it. Consider a 1M ×8bRAM chip. The number of addressesis 220, and the total number of memory cells is 8,388,608 (223). One option is to use eight bit slices and a 20-to-1,048,576 decoder, or about 220gates. Alternatively, we can use 8,192 bit slices of 1,024 cells. For the second implementation, we need two 10-to-1024 decoders, or about 211gates. As chip area is roughly proportional to the number of gates, the savings are substantial. Other schem es are possible as well: if we want a more square chip area, we might choose to use 4,096 bit slices of 2,048 cells a long with one 11-to-2048decoder and one 9-to-512 decoder. This approach requires roughly 25% more d ecoder gates than our previous example, but is still far superior to the eight-bit-slice implementation. Memories are typically unclocked devices. However, as you have see n, the circuits are highly structured, which enables engineers to cope with the complexity of sequential fe edback design. Devices used to control memories are typically clocked, and the interaction between the two can be fairly complex. Timing diagrams for reads and writes to SRAM are shown to the right. A write operation appears on the left. In the ﬁrst cycle, the controller raises the chip select signal and places the memory address to be written on the address inputs. Once the memory hashadtimetosetuptheappropriateCLK ADDR CS write cycleWECLK ADDR CS read cycleWE DATA−IN DATA−OUT DATA validADDR valid ADDR valid DATA valid select lines internally, the WEinput is raised, and data are placed on the data inputs. The delay, wh ich is speciﬁed by the memory manufacturer, is necessary to avoid writin g data to the incorrect element within the memory. The timing shown in the ﬁgure rounds this delay up to a sin gle clock cycle, but the actual delay needed depends on the clock speed and the memory’s speciﬁca tion. At some point after new data have been delivered to the memory, the write operation completes w ithin the memory. The time from the application of the address until the (worst-case) completion of th e write operation is called the write cycle of the memory, and is also speciﬁed by the memory manufacturer. O nce the write cycle has passed, the controlling logic lowers WE, waits for the change to settle within the memory, then removes th e address and lowers the chip select signal. The reason for the delay between thes e signal changes is the same: to avoid mistakenly overwriting another memory location. A read operation is quite similar. As shown on the right, the controlling logic places the address on the input lines and raises the chip select signal. No races need be consider ed, as read operations on SRAM do not aﬀect the stored data. After a delay called the read cycle , the data can be read from the data outputs","{'page_number': 125, 'textbook_name': 'ECE-120-student-notes', 'text': 'The reason for the delay between thes e signal changes is the same: to avoid mistakenly overwriting another memory location. A read operation is quite similar. As shown on the right, the controlling logic places the address on the input lines and raises the chip select signal. No races need be consider ed, as read operations on SRAM do not aﬀect the stored data. After a delay called the read cycle , the data can be read from the data outputs. The address can then be removed and the chip select signal lowered . For both reads and writes, the number of cycles required for an op eration depends on a combination of the clock cycle of the controller and the cycle time of the memory. For ex ample, with a 25 nanosecond write cycle and a 10 nanosecondclock cycle, a write requires three cycles . In general, the number of cycles required is given by the formula ⌈memory cycle time /clock cycle time⌉.'}"
"The reason for the delay between thes e signal changes is the same: to avoid mistakenly overwriting another memory location. A read operation is quite similar. As shown on the right, the controlling logic places the address on the input lines and raises the chip select signal. No races need be consider ed, as read operations on SRAM do not aﬀect the stored data. After a delay called the read cycle , the data can be read from the data outputs. The address can then be removed and the chip select signal lowered . For both reads and writes, the number of cycles required for an op eration depends on a combination of the clock cycle of the controller and the cycle time of the memory. For ex ample, with a 25 nanosecond write cycle and a 10 nanosecondclock cycle, a write requires three cycles . In general, the number of cycles required is given by the formula ⌈memory cycle time /clock cycle time⌉.","{'page_number': 125, 'textbook_name': 'ECE-120-student-notes', 'text': 'The reason for the delay between thes e signal changes is the same: to avoid mistakenly overwriting another memory location. A read operation is quite similar. As shown on the right, the controlling logic places the address on the input lines and raises the chip select signal. No races need be consider ed, as read operations on SRAM do not aﬀect the stored data. After a delay called the read cycle , the data can be read from the data outputs. The address can then be removed and the chip select signal lowered . For both reads and writes, the number of cycles required for an op eration depends on a combination of the clock cycle of the controller and the cycle time of the memory. For ex ample, with a 25 nanosecond write cycle and a 10 nanosecondclock cycle, a write requires three cycles . In general, the number of cycles required is given by the formula ⌈memory cycle time /clock cycle time⌉.'}"
"3.6 Random Access Memories 121 3.6.3 Tri-State Buﬀers and Combining Chips Recall the buﬀer symbol—a triangle like an inverter, but with no invers ion bubble—between the mux and theDATA-OUTsignal of the 26×1b memory shown earlier. This tri-state buﬀer serves to disconnect the memory logic from the output line when the memory is not perform ing a read. An implementation diagram for a tri-state buﬀer ap- pears to the right along with the symbolic form and a truth table. The “Z” in the truth table output means high impedance (and is sometimes written “hi-Z”). In other words, there is ef- fectively no electrical con- nection between the tri-state buﬀer and the output OUT. This logical disconnection is achieved by using the outerIN OUTEN tri−state buffer (logic diagram)IN OUT EN tri−state buffer (symbolic form)EN IN OUT 0 x Z 1 0 0 1 1 1 (upper and lower) pair of transistors in the logic diagram. When EN= 0, both transistors turn oﬀ, meaning that regardless of the value of IN,OUTis connected neither to high voltage nor to ground. When EN= 1, both transistors turn on, and the tri-state buﬀer acts as a pair o f back-to-back inverters, copying the signal fromINtoOUT, as shown in the truth table. What beneﬁt does this logical disconnection provide? So long as only o ne memory’s chip select input is high at any time, the same output line can be shared by more than one mem ory without the need for additional multiplexers. Memory chips were often combined in this way to produc e larger memories. The ﬁgure to the right illustrates how larger memories can be con- structed using multiple chips. In the case shown, two2k×N-bitmemories are used to implement a 2k+1×N-bit memory. One of the address bits—in the case shown, the most signiﬁcant bit—is used to drive a decoder that determines which of the two chips is active ( CS= 1). The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CSis low, as desired. TheADDR WE CSADDR CSWEDATA−IN DATA−OUTADDR CSWEDATA−IN DATA−OUT[k][k−1:0]DATA−IN DATA−OUTk k Nk+1NN NNN k memorymemory memory2      x Nk+1 2   x Nk2   x Nk 10 EN    rest of the address bits, as well as the external data inputs and w rite enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories. Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.","{'page_number': 126, 'textbook_name': 'ECE-120-student-notes', 'text': '3.6 Random Access Memories 121 3.6.3 Tri-State Buﬀers and Combining Chips Recall the buﬀer symbol—a triangle like an inverter, but with no invers ion bubble—between the mux and theDATA-OUTsignal of the 26×1b memory shown earlier. This tri-state buﬀer serves to disconnect the memory logic from the output line when the memory is not perform ing a read. An implementation diagram for a tri-state buﬀer ap- pears to the right along with the symbolic form and a truth table. The “Z” in the truth table output means high impedance (and is sometimes written “hi-Z”). In other words, there is ef- fectively no electrical con- nection between the tri-state buﬀer and the output OUT. This logical disconnection is achieved by using the outerIN OUTEN tri−state buffer (logic diagram)IN OUT EN tri−state buffer (symbolic form)EN IN OUT 0 x Z 1 0 0 1 1 1 (upper and lower) pair of transistors in the logic diagram. When EN= 0, both transistors turn oﬀ, meaning that regardless of the value of IN,OUTis connected neither to high voltage nor to ground. When EN= 1, both transistors turn on, and the tri-state buﬀer acts as a pair o f back-to-back inverters, copying the signal fromINtoOUT, as shown in the truth table. What beneﬁt does this logical disconnection provide? So long as only o ne memory’s chip select input is high at any time, the same output line can be shared by more than one mem ory without the need for additional multiplexers. Memory chips were often combined in this way to produc e larger memories. The ﬁgure to the right illustrates how larger memories can be con- structed using multiple chips. In the case shown, two2k×N-bitmemories are used to implement a 2k+1×N-bit memory. One of the address bits—in the case shown, the most signiﬁcant bit—is used to drive a decoder that determines which of the two chips is active ( CS= 1). The decoder is enabled with the chip select signal for the larger memory, so neither chip is enabled when the external CSis low, as desired. TheADDR WE CSADDR CSWEDATA−IN DATA−OUTADDR CSWEDATA−IN DATA−OUT[k][k−1:0]DATA−IN DATA−OUTk k Nk+1NN NNN k memorymemory memory2      x Nk+1 2   x Nk2   x Nk 10 EN    rest of the address bits, as well as the external data inputs and w rite enable signal, are simply delivered to both memories. The external data outputs are also connected to both memories. Ensuring that at most one chip select signal is high at any time guarantees that at most one of the two memory chips drives logic values on the data outputs.'}"
"122  Multiple chips can also be used to construct wider memories, as shown to the right. In the case shown, two 2k×N-bit memories are used to implement a2k×2N-bit memory. Both chips are either active or inac- tive at the same time, sothe external address, write enable, and chip se- lect inputs are routed to both chips. In contrast, the data inputs and out- puts are separate: the left chip han- dles the high Nbits of input on writes and produces the high Nbits of output on reads, while the right chip handles the low Nbits of input and produces the low Nbits of out- put.ADDR WE CSADDR CSWEDATA−IN DATA−OUTADDR CSWEDATA−IN DATA−OUTDATA−IN[2N−1:N] DATA−IN[N−1:0] DATA−OUT[2N−1:N] DATA−OUT[N−1:0]kN N N Nkmemoryk2    x 2N k memory memory2   x Nk2   x Nk    Historically, tri-state buﬀers were also used to reduce the number of pins needed on chips. Pins have long been a scarce resource, and the amount of data that can cross a chip’s pins in a second (the product of the number of pins and the data rate per pin) has not grown nearly a s rapidly as the number of transistors packed into a ﬁxed area. By combining inputs and outputs, chip desig ners were able to halve the number of pins needed. For example, data inputs and outputs of memory were often combined into a single set of data wires, with bidirectional signals. When performing a read from a memo ry chip, the memory chip drove the data pins with the bits being read (tri-state buﬀers on the memory c hip were enabled). When performing a write, other logic such as a processor wrote the value to be stored onto the data pins (tri-state buﬀers were not enabled). 3.6.4 Dynamic Random Access Memory* Dynamicrandomaccessmemory, orDRAM, isusedformainmemoryinc omputersandforotherapplications in which size is more important than speed. While slower than SRAM, DRA M is denser (has more bits per chip area). A substantial part of DRAM density is due to transistor count: typical SRAM cells use six transistors (two for each inverter, and two more to connect the inverters to the bit lines), while DRAM cells use only a single transistor. However, memory designers have a lso made signiﬁcant advances in further miniaturizing DRAM cells to improve density beyond the beneﬁt available from simple transistor count. A diagram of a DRAM cell appears to the right. DRAM storage is capac itive: a bit is stored by charging or not charging a capacitor. The capacitor is attached to aBITline through a transistor controlled by a SELECT line. When SELECT is low, the capacitor is isolated and holds its charge. However, the tra nsistor’s resistance is ﬁnite, and some charge leaks out onto the bit line. Charge also leaks into the substrate on which the transistor is constructed","{'page_number': 127, 'textbook_name': 'ECE-120-student-notes', 'text': 'A diagram of a DRAM cell appears to the right. DRAM storage is capac itive: a bit is stored by charging or not charging a capacitor. The capacitor is attached to aBITline through a transistor controlled by a SELECT line. When SELECT is low, the capacitor is isolated and holds its charge. However, the tra nsistor’s resistance is ﬁnite, and some charge leaks out onto the bit line. Charge also leaks into the substrate on which the transistor is constructed. After some am ount of time, all of thechargedissipates,andthebitislost. Toavoidsuchloss,thecell mustberefreshed periodically by reading the contents and writing them back with active logic.BITSELECT When the SELECT line is high during a write operation, logic driving the bit line forces charg e onto the capacitor or removes all charge from it. For a read operation, the bit line is ﬁrst brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amoun t of charge onto the bit line. The resulting change in voltage is then detected by a sense ampliﬁer at the end of the bit line. A sense amp is analogous to a marble on a mountaintop: a small push causes the ma rble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sen se amp’s output to move rapidly to a logical 0 or 1, depending on the direction of the small change. As me ntioned earlier, sense ampliﬁers also appear in SRAM implementations. While not technically necessary, as t hey are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster .'}"
"A diagram of a DRAM cell appears to the right. DRAM storage is capac itive: a bit is stored by charging or not charging a capacitor. The capacitor is attached to aBITline through a transistor controlled by a SELECT line. When SELECT is low, the capacitor is isolated and holds its charge. However, the tra nsistor’s resistance is ﬁnite, and some charge leaks out onto the bit line. Charge also leaks into the substrate on which the transistor is constructed. After some am ount of time, all of thechargedissipates,andthebitislost. Toavoidsuchloss,thecell mustberefreshed periodically by reading the contents and writing them back with active logic.BITSELECT When the SELECT line is high during a write operation, logic driving the bit line forces charg e onto the capacitor or removes all charge from it. For a read operation, the bit line is ﬁrst brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amoun t of charge onto the bit line. The resulting change in voltage is then detected by a sense ampliﬁer at the end of the bit line. A sense amp is analogous to a marble on a mountaintop: a small push causes the ma rble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sen se amp’s output to move rapidly to a logical 0 or 1, depending on the direction of the small change. As me ntioned earlier, sense ampliﬁers also appear in SRAM implementations. While not technically necessary, as t hey are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster .","{'page_number': 127, 'textbook_name': 'ECE-120-student-notes', 'text': 'A diagram of a DRAM cell appears to the right. DRAM storage is capac itive: a bit is stored by charging or not charging a capacitor. The capacitor is attached to aBITline through a transistor controlled by a SELECT line. When SELECT is low, the capacitor is isolated and holds its charge. However, the tra nsistor’s resistance is ﬁnite, and some charge leaks out onto the bit line. Charge also leaks into the substrate on which the transistor is constructed. After some am ount of time, all of thechargedissipates,andthebitislost. Toavoidsuchloss,thecell mustberefreshed periodically by reading the contents and writing them back with active logic.BITSELECT When the SELECT line is high during a write operation, logic driving the bit line forces charg e onto the capacitor or removes all charge from it. For a read operation, the bit line is ﬁrst brought to an intermediate voltage level (a voltage level between 0 and 1), then SELECT is raised, allowing the capacitor to either pull a small amount of charge from the bit line or to push a small amoun t of charge onto the bit line. The resulting change in voltage is then detected by a sense ampliﬁer at the end of the bit line. A sense amp is analogous to a marble on a mountaintop: a small push causes the ma rble to roll rapidly downhill in the direction of the push. Similarly, a small change in voltage causes a sen se amp’s output to move rapidly to a logical 0 or 1, depending on the direction of the small change. As me ntioned earlier, sense ampliﬁers also appear in SRAM implementations. While not technically necessary, as t hey are with DRAM, the use of a sense amp to react to small changes in voltage makes reads faster .'}"
"3.6 Random Access Memories 123 Each read operation on a DRAM cell brings the voltage on its capacito r closer to the intermediate voltage level, in eﬀect destroying the data in the cell. DRAM is thus said to have destructive reads . To preserve data during a read, the bits must be written back into the cells after a read. For example, the output of the sense ampliﬁers can be used to drive the bit lines, rewriting the cells wit h the appropriate data. At the chip level, typical DRAM inputs and outputs diﬀer from those o f SRAM. Due to the large size and high density ofDRAM, addressesare split into rowand column compon ents and provided througha common set of pins. The DRAM stores the components in registers to suppo rt this approach. Additional inputs, known as the rowandcolumn address strobes —RASandCAS, respectively—are used to indicate when address components are available. As you might guess from the str ucture of coincident selection, DRAM refresh occurs on a row-by-row basis (across bit slices—on column s rather than rows in the ﬁgures earlier in these notes, but the terminology of DRAM is a row). Raising the SELECT line for a row destructively reads the contents of all cells on that row, forcing the cells to be re written and eﬀecting a refresh. The row is thus a natural basis for the refresh cycle. The DRAM data pins pro vide bidirectional signals for reading and writing elements of the DRAM. An output enable input,OE, controls tri-state buﬀers with the DRAM to determine whether or not the DRAM drives the data pins. The WEinput, which controls the type of operation, is also present. Timing diagrams for writes and reads on a historical DRAM implementa- tion appear to the right. In both cases, the row component of the ad- dress is ﬁrst applied to the address pins, then RASis raised. In the next cycle of the controlling logic, the col- umn component is applied to the ad- dress pins, and CASis raised. For a write, as shown on the left, the WEsignal and the data canDATARAS CAS OECLK ADDR read cycle write cycleDATARAS CAS OECLK ADDR hi−ZWE WE validCOL COL ROW validROW also be applied in the second cycle. The DRAM has internal timing and co ntrol logic that prevent races from overwriting an incorrect element (remember that the row and column addresses have to be stored in registers). The DRAM again speciﬁes a write cycle, after which the o peration is guaranteed to be complete. In order, the WE,CAS, andRASsignals are then lowered. For a read operation, the output enable signal, OE, is raised after CASis raised. The DATApins, which should be ﬂoating (in other words, not driven by any logic), are then driven by the DRAM. After the read cycle, valid data appear on the DATApins, and OE,CAS, andRASare lowered in order after the data are read. Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integr ated onto the chips themselves","{'page_number': 128, 'textbook_name': 'ECE-120-student-notes', 'text': 'The DATApins, which should be ﬂoating (in other words, not driven by any logic), are then driven by the DRAM. After the read cycle, valid data appear on the DATApins, and OE,CAS, andRASare lowered in order after the data are read. Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integr ated onto the chips themselves. As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor’s 8Gb (231×4b, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016. The ability to synchronizeto an externalclockhas become prevalen tin the industry, leadingto the somewhat confusing term SDRAM, which stands for synchronous DRAM . The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize acces ses to the external clock without the need for additional logic. The clock provided to the Micron chip just m entioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of t he clock (hence the name DDR, or double data rate ). In addition to row and column components of the address, these ch ips further separate cells into banksand groups of banks. These allow a user to exploit parallelism by starting r eads or writes to separate banks at the same time, thus improving the speed at which data can move in a nd out of the memory. For the 231×4b version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.'}"
"The DATApins, which should be ﬂoating (in other words, not driven by any logic), are then driven by the DRAM. After the read cycle, valid data appear on the DATApins, and OE,CAS, andRASare lowered in order after the data are read. Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integr ated onto the chips themselves. As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor’s 8Gb (231×4b, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016. The ability to synchronizeto an externalclockhas become prevalen tin the industry, leadingto the somewhat confusing term SDRAM, which stands for synchronous DRAM . The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize acces ses to the external clock without the need for additional logic. The clock provided to the Micron chip just m entioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of t he clock (hence the name DDR, or double data rate ). In addition to row and column components of the address, these ch ips further separate cells into banksand groups of banks. These allow a user to exploit parallelism by starting r eads or writes to separate banks at the same time, thus improving the speed at which data can move in a nd out of the memory. For the 231×4b version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.","{'page_number': 128, 'textbook_name': 'ECE-120-student-notes', 'text': 'The DATApins, which should be ﬂoating (in other words, not driven by any logic), are then driven by the DRAM. After the read cycle, valid data appear on the DATApins, and OE,CAS, andRASare lowered in order after the data are read. Modern DRAM chips are substantially more sophisticated than those discussed here, and many of the functions that used to be provided by external logic are now integr ated onto the chips themselves. As an example of modern DRAMs, one can obtain the data sheet for Micron Semiconductor’s 8Gb (231×4b, for example) DDR4 SDRAM, which is 366 pages long as of 11 May 2016. The ability to synchronizeto an externalclockhas become prevalen tin the industry, leadingto the somewhat confusing term SDRAM, which stands for synchronous DRAM . The memory structures themselves are still unclocked, but logic is provided on the chip to synchronize acces ses to the external clock without the need for additional logic. The clock provided to the Micron chip just m entioned can be as fast as 1.6 GHz, and data can be transferred on both the rising and falling edges of t he clock (hence the name DDR, or double data rate ). In addition to row and column components of the address, these ch ips further separate cells into banksand groups of banks. These allow a user to exploit parallelism by starting r eads or writes to separate banks at the same time, thus improving the speed at which data can move in a nd out of the memory. For the 231×4b version of the Micron chip, the cells are structured into 4 groups of 4 banks (16 banks total), each with 131,072 rows and 1,024 columns.'}"
"124  DRAM implementations provide interfaces for specifying refresh op erations in addition to reads and writes. Managingrefreshtimingandexecutionisgenerallyleft toanexterna lDRAM controller. Forthe Micronchip, refresh commands must be issued every 7.8 microseconds at norma l temperatures. Each command refreshes about 220cells, so 8,192 commands refresh the whole chip in less than 64 millisecon ds. Alternatively, the chip can handle refresh on-chip in order to maintain memory content s when the rest of the system is powered down.","{'page_number': 129, 'textbook_name': 'ECE-120-student-notes', 'text': '124  DRAM implementations provide interfaces for specifying refresh op erations in addition to reads and writes. Managingrefreshtimingandexecutionisgenerallyleft toanexterna lDRAM controller. Forthe Micronchip, refresh commands must be issued every 7.8 microseconds at norma l temperatures. Each command refreshes about 220cells, so 8,192 commands refresh the whole chip in less than 64 millisecon ds. Alternatively, the chip can handle refresh on-chip in order to maintain memory content s when the rest of the system is powered down.'}"
"3.7 From FSM to Computer 125 ECE120: Introduction to Computer Engineering Notes Set 3.7 From FSM to Computer The FSM designs we have explored so far have started with a human- based design process in which someone writesdownthe desiredbehaviorintermsofstates, inputs, outpu ts, andtransitions. Suchanapproachmakes it easierto build adigital FSM, sincethe abstractionusedcorrespon dsalmostdirectlyto the implementation. As an alternative, one can start by mapping the desired task into a h igh-level programming language, then using components such as registers, counters, and memories to im plement the variables needed. In this approach, the control structure of the code maps into a high-lev el FSM design. Of course, in order to implement our FSM with digital logic, we eventually still need to map down to bits and gates. In this set of notes, we show how one can transform a piece of code written in a high-level language into an FSM. This process is meant to help you understand how we can design an FSM that executes simple pieces of a ﬂow chart such as assignments, ifstatements, and loops. Later, we generalize this concept and build an FSM that allows the pieces to be executed to be speciﬁed after th e FSM is built—in other words, the FSM executes a program speciﬁed by bits stored in memory. This mor e general model, as you might have already guessed, is a computer. 3.7.1 Specifying the Problem Let’s begin by specifying the problem that we want to solve. Say that we want to ﬁnd the minimum value in a set of 10 integers. Using the C programming language, we can writ e the following fragment of code: int values[10]; /* 10 integers--filled in by other code */ int idx; int min min = values[0]; for (idx = 1; 10 > idx; idx = idx + 1) { if (min > values[idx]) { min = values[idx]; } } /* The minimum value from array is now in min. */ Thecode usesarraynotation, which wehavenotused previouslyin o urclass, solet’s ﬁrstdiscussthe meaning of the code. The code uses three variables. The variable valuesrepresents the 10 values in our set. The suﬃx “[10]” after the variable name tells the compiler that we want an array of 10 integers ( int) indexed from 0 to 9. These integers can be treated as 10 separate variables, but can b e accessed using the single name “ values” along with an index (again, from 0 to 9 in this case). The variable idxholds a loop index that we use to examine each of the values one by one in order to ﬁnd the minimum value in the set. Finally, the variable minholds the smallest known value as the program examines each of the v alues in the set. The program body consists of two statements. We assume that so me other piece of code—one not shown here—has initialized the 10 values in our set before the code above ex ecutes. The ﬁrst statement initializes the minimum known value ( min) to the value stored at index 0 in the array ( values[0] )","{'page_number': 130, 'textbook_name': 'ECE-120-student-notes', 'text': 'Finally, the variable minholds the smallest known value as the program examines each of the v alues in the set. The program body consists of two statements. We assume that so me other piece of code—one not shown here—has initialized the 10 values in our set before the code above ex ecutes. The ﬁrst statement initializes the minimum known value ( min) to the value stored at index 0 in the array ( values[0] ). The second statement is a loop in which the variable indextakes on values from 1 to 9. For each value, an ifstatement compares the current known minimum with the value stored in the arr ay at index given by the idxvariable. If the stored value is smaller, the current known value (again, min) is updated to reﬂect the program’shaving found a smaller value. When the loop ﬁnishes all nine iterations, the va riableminholds the smallest value among the set of 10 integers stored in the valuesarray.'}"
"Finally, the variable minholds the smallest known value as the program examines each of the v alues in the set. The program body consists of two statements. We assume that so me other piece of code—one not shown here—has initialized the 10 values in our set before the code above ex ecutes. The ﬁrst statement initializes the minimum known value ( min) to the value stored at index 0 in the array ( values[0] ). The second statement is a loop in which the variable indextakes on values from 1 to 9. For each value, an ifstatement compares the current known minimum with the value stored in the arr ay at index given by the idxvariable. If the stored value is smaller, the current known value (again, min) is updated to reﬂect the program’shaving found a smaller value. When the loop ﬁnishes all nine iterations, the va riableminholds the smallest value among the set of 10 integers stored in the valuesarray.","{'page_number': 130, 'textbook_name': 'ECE-120-student-notes', 'text': 'Finally, the variable minholds the smallest known value as the program examines each of the v alues in the set. The program body consists of two statements. We assume that so me other piece of code—one not shown here—has initialized the 10 values in our set before the code above ex ecutes. The ﬁrst statement initializes the minimum known value ( min) to the value stored at index 0 in the array ( values[0] ). The second statement is a loop in which the variable indextakes on values from 1 to 9. For each value, an ifstatement compares the current known minimum with the value stored in the arr ay at index given by the idxvariable. If the stored value is smaller, the current known value (again, min) is updated to reﬂect the program’shaving found a smaller value. When the loop ﬁnishes all nine iterations, the va riableminholds the smallest value among the set of 10 integers stored in the valuesarray.'}"
"126  As a ﬁrst step towards designing an FSM to implement the code, we transform the code into a ﬂow chart, as shown to the right. Theprogramagainbeginswithini- tialization, which appears in the second column of the ﬂow chart. The loop in the program translates to the third column of the ﬂow chart, and the ifstatement to the middle comparison and update of min. Our goal is now to design an FSM to implement the ﬂow chart. In or- der to do so, we want to leverage the same kind of abstraction that we used earlier, when extending our keyless en- try system with a timer. Although the timer’s value was technically alsoSTART min > values[idx]?T T idx = idx + 1END  min = values[0] idx = 110 > idx?  min = values[idx]F F part of the FSM’s state, we treated it as data and integrated it into our next-state decisions in only a couple of cases. For our minimum value problem, we have two sources of data. First, a n external program supplies data in the form of a set of 10 integers. If we assume 32-bit integers, the se data technically form 320 input bits! Second, as with the keyless entry system timer, we have data used internally by our FSM, such as the loop index and the current minimum value. These are technically state bits . For both types of data, we treat them abstractly as values rather than thinking of them individually as bits, allowing us to develop our FSM at a high-level and then to implement it using the components that we have developed earlier in our course. 3.7.2 Choosing Components and Identifying States Now we are ready to design an FSM that implements the ﬂow chart. Wh at components do we need, other than our state logic? We use registers and counters to implement th e variables idxandminin the program. For the array values, we use a 16×32-bit memory.13We need a comparator to implement the test for the ifstatement. We choose to use a serial comparator, which allows us t o illustrate again how one logical high-level state can be subdivided into many actual states. To ope rate the serial comparator, we make use of two shift registers that present the comparator with one bit pe r cycle on each input, and a counter to keep track of the comparator’s progress. How do we identify high-level states from our ﬂow chart? Although t he ﬂow chart attempts to break down the program into ‘simple’ steps, one step of a ﬂow chart may sometim es require more than one state in an FSM. Similarly, one FSM state may be able to implement several steps in a ﬂow chart, if those steps can be performed simultaneously. Our design illustrates both possibilities. How we map ﬂow chart elements into FSM states also depends to some degree on what components we use, which is why we began with some discussion of components. In practic e, one can go back and forth between the two, adjusting components to better match the high-level st ates, and adjusting states to better match the desired components","{'page_number': 131, 'textbook_name': 'ECE-120-student-notes', 'text': 'Our design illustrates both possibilities. How we map ﬂow chart elements into FSM states also depends to some degree on what components we use, which is why we began with some discussion of components. In practic e, one can go back and forth between the two, adjusting components to better match the high-level st ates, and adjusting states to better match the desired components. Finally, notethatweareonlyconcernedwithhigh-levelstates, sow edonotneedtoprovidedetails(yet)down to the level of individual clock cycles, but we do want to deﬁne high-le vel states that can be implemented in a ﬁxed number of cycles, or at least a controllable number of cycles . If we cannot specify clearly when transitions occur from an FSM state, we may not be able to implement the state. 13We technically only need a 10 ×32-bit memory, but we round up the size of the address space to reﬂect more realistic memory designs; one can always optimize later.'}"
"Our design illustrates both possibilities. How we map ﬂow chart elements into FSM states also depends to some degree on what components we use, which is why we began with some discussion of components. In practic e, one can go back and forth between the two, adjusting components to better match the high-level st ates, and adjusting states to better match the desired components. Finally, notethatweareonlyconcernedwithhigh-levelstates, sow edonotneedtoprovidedetails(yet)down to the level of individual clock cycles, but we do want to deﬁne high-le vel states that can be implemented in a ﬁxed number of cycles, or at least a controllable number of cycles . If we cannot specify clearly when transitions occur from an FSM state, we may not be able to implement the state. 13We technically only need a 10 ×32-bit memory, but we round up the size of the address space to reﬂect more realistic memory designs; one can always optimize later.","{'page_number': 131, 'textbook_name': 'ECE-120-student-notes', 'text': 'Our design illustrates both possibilities. How we map ﬂow chart elements into FSM states also depends to some degree on what components we use, which is why we began with some discussion of components. In practic e, one can go back and forth between the two, adjusting components to better match the high-level st ates, and adjusting states to better match the desired components. Finally, notethatweareonlyconcernedwithhigh-levelstates, sow edonotneedtoprovidedetails(yet)down to the level of individual clock cycles, but we do want to deﬁne high-le vel states that can be implemented in a ﬁxed number of cycles, or at least a controllable number of cycles . If we cannot specify clearly when transitions occur from an FSM state, we may not be able to implement the state. 13We technically only need a 10 ×32-bit memory, but we round up the size of the address space to reﬂect more realistic memory designs; one can always optimize later.'}"
"3.7 From FSM to Computer 127 Now let’s go through the ﬂow chart and identify states. Initialization ofminandidxneed not occur serially, and the result of the ﬁrst comparison between idxand the constant 10 is known in advance, so we can merge all three operations into a single state, which we call INIT. We can also merge the updates of minandidxinto a second FSM state, which we call COPY. However, the update to minoccurs only when the comparison ( min > value[idx] ) is true. We can use logic to predicate execution of the update. In other words, we can use the output o f the comparator, which is available after the comparator has ﬁnished comparing the two values (in a high-leve l FSM state that we have yet to deﬁne), to determine whether or not the register holding minloads a new value in the COPYstate. Our model of use for this FSM involves external logic ﬁlling the memory (the array of integer values), executing the FSM “code,” and then checking the answer. To suppo rt this use model, we create a FSM state calledWAITfor cycles in which the FSM has no work to do. Later, we also make use of an external input signalSTARTto start the FSM execution. The WAITstate logically corresponds to the “START” bubble in the ﬂow chart. Only the test for the ifstatement remains. Using a serial compara- tor to compare two 32-bit values re- quires 32 cycles. However, we need anadditionalcycleto movevaluesinto ourshiftregisterssothatthecompara- tor can see the ﬁrst bit. Thus our sin- gle comparison operation breaks into two high-level states. In the ﬁrst state, which we call PREP, we copy minto one of the shift registers, copy values[idx] to the other shift regis- ter, and reset the counter that mea- sures the cycles needed for our se- rial comparator. We then move to a second high-level state, which we call COMPARE, in which we feed one bit per cycle from each shift registerto the se- rial comparator. The COMPARE stateSTART min > values[idx]?T T idx = idx + 1END  min = values[0] idx = 110 > idx?  min = values[idx]F FWAITINIT PREP COMPARE COPY executes for 32 cycles, after which the comparator produces th e one-bit answer that we need, and we can move to the COPYstate. The association between the ﬂow chart and the high-level F SM states is illustrated in the ﬁgure shown to the right above. We can now also draw an abstract state diagram for our FSM, as shown to the right. The FSM begins in theWAITstate. After external logic ﬁlls the valuesar- ray, it signals the FSM to begin by raising the START signal. The FSM transitions into the INITstate, and in the next cycle into the PREPstate. From PREP, the FSM always moves to COMPARE, where it remains for 32 cycles while the serial comparator executes a com- parison","{'page_number': 132, 'textbook_name': 'ECE-120-student-notes', 'text': 'The FSM begins in theWAITstate. After external logic ﬁlls the valuesar- ray, it signals the FSM to begin by raising the START signal. The FSM transitions into the INITstate, and in the next cycle into the PREPstate. From PREP, the FSM always moves to COMPARE, where it remains for 32 cycles while the serial comparator executes a com- parison. After COMPARE, the FSM moves to the COPYend of loopnot end of loopWAIT INITSTART signal always PREP COMPARE COPYalways after 32 cycles state, where it remains for one cycle. The transition from COPYdepends on how many loop iterations have executed. If more loop iterations remain, the FSM moves to PREPto execute the next iteration. If the loop is done, the FSM returns to WAITto allow external logic to read the result of the computation.'}"
"The FSM begins in theWAITstate. After external logic ﬁlls the valuesar- ray, it signals the FSM to begin by raising the START signal. The FSM transitions into the INITstate, and in the next cycle into the PREPstate. From PREP, the FSM always moves to COMPARE, where it remains for 32 cycles while the serial comparator executes a com- parison. After COMPARE, the FSM moves to the COPYend of loopnot end of loopWAIT INITSTART signal always PREP COMPARE COPYalways after 32 cycles state, where it remains for one cycle. The transition from COPYdepends on how many loop iterations have executed. If more loop iterations remain, the FSM moves to PREPto execute the next iteration. If the loop is done, the FSM returns to WAITto allow external logic to read the result of the computation.","{'page_number': 132, 'textbook_name': 'ECE-120-student-notes', 'text': 'The FSM begins in theWAITstate. After external logic ﬁlls the valuesar- ray, it signals the FSM to begin by raising the START signal. The FSM transitions into the INITstate, and in the next cycle into the PREPstate. From PREP, the FSM always moves to COMPARE, where it remains for 32 cycles while the serial comparator executes a com- parison. After COMPARE, the FSM moves to the COPYend of loopnot end of loopWAIT INITSTART signal always PREP COMPARE COPYalways after 32 cycles state, where it remains for one cycle. The transition from COPYdepends on how many loop iterations have executed. If more loop iterations remain, the FSM moves to PREPto execute the next iteration. If the loop is done, the FSM returns to WAITto allow external logic to read the result of the computation.'}"
"128  3.7.3 Laying Out Components Our high-level FSM design tells us what our components need to be able to do in any given cycle. For example, when we load new values into the shift registers that provide bits to the serial compara- tor, we always copy mininto one shift register and values[idx] into the sec- ond. Using this information, we can put together our components and simplify our design by ﬁxing the way in which bits ﬂow between them. The ﬁgure at the right shows how we can organize our components. Again, in practice, one goes back and forth think- ing about states, components, and ﬂow from state to state. In these notes, we present only a completed design. Let’s take a detailed look at each of the components. At the upper left ofthe ﬁg- ure is a 4-bit binary counter called IDX to hold the idxvariable. The counter can be reset to 0 using the RSTin- put. Otherwise, the CNTinput controls whether or not the counter increments its value. With this counter design, we can force idxto 0 in the WAITstate and then count upwards in the INITand COPYstates. A memory labeled VALUESto hold the array valuesappears in the upper right of the ﬁgure. The read/write control for the memory is hardwired to 1 (read) in the ﬁgure, and t he data input lines are unattached. To integrate with other logic that can operate our FSM, we need to add more control logic to allow writing into the memory and to attach the data inputs to something that provid es the data bits. The address input of the memory comes always from the IDXcounter value; in other words, whenever we access this memory by making use of the data output lines, we read values[idx] . In the middle left of the ﬁgure is a 32-bit register for the minvariable. It has a control input LDthat determines whether or not it loads a new value at the end of the clock cycle. If a new value is loaded, the new value always corresponds to the output of the VALUESmemory, values[idx] . Recall that minalways changes in the INITstate, and may change in the COPYstate. But the new value stored in minis always values[idx] . Note also that when the FSM completes its task, the result of the c omputation is left in the MINregister for external logic to read (connections for this purpose are not shown in the ﬁgure). Continuing downward in the ﬁgure, we see two right shift registers la beledAandB. Each has a control input LDthat enables a parallel load. Register Aloads from register MIN, and register Bloads from the memory data output ( values[idx] ). These loads are needed in the PREPstate of our FSM. When LDis low, the shift registers simply shifts to the right. The serial output SOmakes the least signiﬁcant bit of each shift register available. Shifting is necessary to feed the serial comparator in the COMPARE state. Below register Ais a 5-bit binary counter called CNT. The counter is used to control the serial comparator in theCOMPARE state","{'page_number': 133, 'textbook_name': 'ECE-120-student-notes', 'text': 'These loads are needed in the PREPstate of our FSM. When LDis low, the shift registers simply shifts to the right. The serial output SOmakes the least signiﬁcant bit of each shift register available. Shifting is necessary to feed the serial comparator in the COMPARE state. Below register Ais a 5-bit binary counter called CNT. The counter is used to control the serial comparator in theCOMPARE state. A reset input RSTallows it to be forced to 0 in the PREPstate. When the counter value is exactly zero, the output Zis high.'}"
"These loads are needed in the PREPstate of our FSM. When LDis low, the shift registers simply shifts to the right. The serial output SOmakes the least signiﬁcant bit of each shift register available. Shifting is necessary to feed the serial comparator in the COMPARE state. Below register Ais a 5-bit binary counter called CNT. The counter is used to control the serial comparator in theCOMPARE state. A reset input RSTallows it to be forced to 0 in the PREPstate. When the counter value is exactly zero, the output Zis high.","{'page_number': 133, 'textbook_name': 'ECE-120-student-notes', 'text': 'These loads are needed in the PREPstate of our FSM. When LDis low, the shift registers simply shifts to the right. The serial output SOmakes the least signiﬁcant bit of each shift register available. Shifting is necessary to feed the serial comparator in the COMPARE state. Below register Ais a 5-bit binary counter called CNT. The counter is used to control the serial comparator in theCOMPARE state. A reset input RSTallows it to be forced to 0 in the PREPstate. When the counter value is exactly zero, the output Zis high.'}"
"3.7 From FSM to Computer 129 The last major component is the serial comparator, which is based o n the design developed in Notes Set 3.1. The two bits to be compared in a cycle come from shift registers AandB. The ﬁrst bit indicator comes from the zero indicator of counter CNT. The comparator actually produces two outputs ( Z1andZ0), but the meaning of the Z1output by itself is A>B. In the diagram, this signal has been labeled THEN. There are two additional elements in the ﬁgure that we have yet to d iscuss. Each simply compares the value in a register with a ﬁxed constant and produces a 1-bit signal. When t he FSM ﬁnishes an iteration of the loop in the COPYstate, it must check the loop condition ( 10>idx) and move either to the PREPstate or, when the loop ﬁnishes, to the WAITstate to let the external logic read the answer from the MINregister. The loop is done when the current iteration count is nine, so we compare IDXwith nine to produce the DONE signal. The other constant comparison is between the counter CNTand the value 31 to produce the LAST signal, which indicates that the serial comparator is on its last cycle o f comparison. In the cycle after LAST is high, the THENoutput of the comparator indicates whether or not A>B. 3.7.4 Control and Data One can think of the components and the interconnections betwee n them as enabling the movement of data between registers, while the high-level FSM controls which data mov e from register to register in each cycle. With this model in mind, we call the components and interconnections for our design the datapath —a term that we will see again when we examine the parts of a computer in the c oming weeks. The datapath requires several inputs to control the operation of the components—the se we can treat as outputs of the FSM. These signals allow the FSM to control the motion of data in the datapath, s o we call them control signals . Similarly, the datapath produces several outputs that we can tre at as inputs to the FSM. The tables below summarize the control signals (left) and outputs (right) from the datapath for our FSM. datapath input meaning IDX.RST resetIDXcounter to 0 IDX.CNT increment IDXcounter MIN.LD load new value into MINregister A.LD load new value into shift register A B.LD load new value into shift register B CNT.RST resetCNTcounterdatapath output meaning based on DONE last loop iteration ﬁnished IDX = 9 LAST serial comparator executing CNT = 31 last cycle THEN ifstatement condition true A>B Using the datapath controls signals and outputs, we can now write a more formal state transition table for the FSM, as shown below. The “actions” column of the table lists t he changes to register and counter values that are made in each of the FSM states. The notation used t o represent the actions is called register transfer language (RTL). The meaning of an individual action is similar to the meaning of the correspondingstatement from our C code or from the ﬂow chart","{'page_number': 134, 'textbook_name': 'ECE-120-student-notes', 'text': 'The “actions” column of the table lists t he changes to register and counter values that are made in each of the FSM states. The notation used t o represent the actions is called register transfer language (RTL). The meaning of an individual action is similar to the meaning of the correspondingstatement from our C code or from the ﬂow chart. For example, in the WAITstate, “IDX←0” means the same thing as “ idx = 0; ”. In particular, both mean that the value currently stored in the IDX counter is overwritten with the number 0 (all 0 bits). The meaning of RTL is slightly diﬀer- ent from the usual interpretation of high-level program- ming languages, however, in terms of when the actions happen. A list of C statements is generally executed one at a time. In contrast, the entire list of RTL actionsstate actions (simultaneous) condition next state WAIT IDX←0 (to read VALUES[0] inINIT) START INIT START WAIT INIT MIN←VALUES[IDX] (IDXis 0 in this state) (always) PREP IDX←IDX+ 1 PREP A←MIN (always) COMPARE B←VALUES[IDX] CNT←0 COMPARE run serial comparator LAST COPY LAST COMPARE COPY THEN:MIN←VALUES[IDX] DONE WAIT IDX←IDX+ 1 DONE PREP'}"
"The “actions” column of the table lists t he changes to register and counter values that are made in each of the FSM states. The notation used t o represent the actions is called register transfer language (RTL). The meaning of an individual action is similar to the meaning of the correspondingstatement from our C code or from the ﬂow chart. For example, in the WAITstate, “IDX←0” means the same thing as “ idx = 0; ”. In particular, both mean that the value currently stored in the IDX counter is overwritten with the number 0 (all 0 bits). The meaning of RTL is slightly diﬀer- ent from the usual interpretation of high-level program- ming languages, however, in terms of when the actions happen. A list of C statements is generally executed one at a time. In contrast, the entire list of RTL actionsstate actions (simultaneous) condition next state WAIT IDX←0 (to read VALUES[0] inINIT) START INIT START WAIT INIT MIN←VALUES[IDX] (IDXis 0 in this state) (always) PREP IDX←IDX+ 1 PREP A←MIN (always) COMPARE B←VALUES[IDX] CNT←0 COMPARE run serial comparator LAST COPY LAST COMPARE COPY THEN:MIN←VALUES[IDX] DONE WAIT IDX←IDX+ 1 DONE PREP","{'page_number': 134, 'textbook_name': 'ECE-120-student-notes', 'text': 'The “actions” column of the table lists t he changes to register and counter values that are made in each of the FSM states. The notation used t o represent the actions is called register transfer language (RTL). The meaning of an individual action is similar to the meaning of the correspondingstatement from our C code or from the ﬂow chart. For example, in the WAITstate, “IDX←0” means the same thing as “ idx = 0; ”. In particular, both mean that the value currently stored in the IDX counter is overwritten with the number 0 (all 0 bits). The meaning of RTL is slightly diﬀer- ent from the usual interpretation of high-level program- ming languages, however, in terms of when the actions happen. A list of C statements is generally executed one at a time. In contrast, the entire list of RTL actionsstate actions (simultaneous) condition next state WAIT IDX←0 (to read VALUES[0] inINIT) START INIT START WAIT INIT MIN←VALUES[IDX] (IDXis 0 in this state) (always) PREP IDX←IDX+ 1 PREP A←MIN (always) COMPARE B←VALUES[IDX] CNT←0 COMPARE run serial comparator LAST COPY LAST COMPARE COPY THEN:MIN←VALUES[IDX] DONE WAIT IDX←IDX+ 1 DONE PREP'}"
"130  for an FSM state is executed simultaneously, at the end of the clock cycle. As you know, an FSM moves from its current state into a new state at the end of every clock cy cle, so actions during diﬀerent cycles usually are associated with diﬀerent states. We can, however, cha nge the value in more than one register at the end of the same clock cycle, so we can execute more than one RTL action in the same state, so long as the actions do not exceed the capabilities of our datapath (the c omponents must be able to support the simultaneous execution of the actions). Some care must be taken w ith states that execute for more than one cycle to ensure that repeating the RTL actions is appropriate. In o ur design, only the WAITandCOMPARE states execute for more than one cycle. The WAITstate resets the IDXcounter repeatedly, which causes no problems. The COMPARE statement has no RTL actions—all of the shifting, comparison, and counting activity needed to do its work occurs within the datapath itself. One additional piece of RTL syntax needs explanation. In the COPYstate, the ﬁrst action begins with “THEN:,” which means that the preﬁxed RTL action occurs only when the THENsignal is high. Recall that theTHENsignal indicates that the comparator has found A>B, so the equivalent C code is “ if (A>B) {min = values[idx] }”. 3.7.5 State Representation and Logic Expressions Let’s think about the representation for the FSM states. The FSM has ﬁve states, so we could use as few as three ﬂip-ﬂops. Instead, we choose to use a one-hot encoding , in which any valid bit pattern has exactly one 1 bit. In other words, we use ﬁve ﬂip-ﬂops instead of three, an d our states are represented with the bit patterns 10000, 01000, 00100, 00010, and 00001. The table below shows the mapping from each high-level state to bot h the ﬁve-bit encoding for the state as well as the six control signals needed for the datapath. For each s tate, the values of the control signals can be found by examining the actions necessary in that state. state S4S3S2S1S0IDX.RST IDX.CNT MIN.LD A.LD B.LD CNT.RST WAIT 1 0 0 0 0 1 0 0 0 0 0 INIT 0 1 0 0 0 0 1 1 0 0 0 PREP 0 0 1 0 0 0 0 0 1 1 1 COMPARE 0 0 0 1 0 0 0 0 0 0 0 COPY 0 0 0 0 1 0 1 THEN 0 0 0 TheWAITstate needs to set IDXto 0 but need not aﬀect other register or counter values, so WAITproduces a 1 only for IDX.RST. TheINITstate needs to load values[0] into theMINregister while simultaneously incrementing the IDXcounter (from 0 to 1), so INITproduces 1s for IDX.CNT andMIN.LD. ThePREPstate loads both shift registers and resets the counter CNTby producing 1s for A","{'page_number': 135, 'textbook_name': 'ECE-120-student-notes', 'text': 'RST. TheINITstate needs to load values[0] into theMINregister while simultaneously incrementing the IDXcounter (from 0 to 1), so INITproduces 1s for IDX.CNT andMIN.LD. ThePREPstate loads both shift registers and resets the counter CNTby producing 1s for A.LD,B.LD, andCNT.RST. The COMPARE state does not change any register values, so it produces all 0s. F inally, the COPYstate increments theIDXcounter while simultaneously loading a new value into the MINregister. The COPYstate produces 1 forIDX.CNT, but must use the signal THENcoming from the datapath to decide whether or not MINis loaded. The advantage of a one-hot encod- ing becomes obvious when we write equations for the six control signals and the next-state logic, as shown to the right. Implementing the logic to complete our design now requires only a handful of small logic gates.IDX.RST=S4 IDX.CNT=S3+S0 MIN.LD=S3+S0·THEN A.LD=S2 B.LD=S2 CNT.RST=S2S+ 4=S4·START+S0·DONE S+ 3=S4·START S+ 2=S3+S0·DONE S+ 1=S2+S1·LAST S+ 0=S1·LAST Notice that the terms in each control signal can be read directly fr om the rows of the state table and OR’d together. The terms in each of the next-state equations repres ent the incoming arcs for the corresponding state. For example, the WAITstate has one self-loop (the ﬁrst term) and a transition arc coming from the COPYstate when the loop is done. These expressions complete our design .'}"
"RST. TheINITstate needs to load values[0] into theMINregister while simultaneously incrementing the IDXcounter (from 0 to 1), so INITproduces 1s for IDX.CNT andMIN.LD. ThePREPstate loads both shift registers and resets the counter CNTby producing 1s for A.LD,B.LD, andCNT.RST. The COMPARE state does not change any register values, so it produces all 0s. F inally, the COPYstate increments theIDXcounter while simultaneously loading a new value into the MINregister. The COPYstate produces 1 forIDX.CNT, but must use the signal THENcoming from the datapath to decide whether or not MINis loaded. The advantage of a one-hot encod- ing becomes obvious when we write equations for the six control signals and the next-state logic, as shown to the right. Implementing the logic to complete our design now requires only a handful of small logic gates.IDX.RST=S4 IDX.CNT=S3+S0 MIN.LD=S3+S0·THEN A.LD=S2 B.LD=S2 CNT.RST=S2S+ 4=S4·START+S0·DONE S+ 3=S4·START S+ 2=S3+S0·DONE S+ 1=S2+S1·LAST S+ 0=S1·LAST Notice that the terms in each control signal can be read directly fr om the rows of the state table and OR’d together. The terms in each of the next-state equations repres ent the incoming arcs for the corresponding state. For example, the WAITstate has one self-loop (the ﬁrst term) and a transition arc coming from the COPYstate when the loop is done. These expressions complete our design .","{'page_number': 135, 'textbook_name': 'ECE-120-student-notes', 'text': 'RST. TheINITstate needs to load values[0] into theMINregister while simultaneously incrementing the IDXcounter (from 0 to 1), so INITproduces 1s for IDX.CNT andMIN.LD. ThePREPstate loads both shift registers and resets the counter CNTby producing 1s for A.LD,B.LD, andCNT.RST. The COMPARE state does not change any register values, so it produces all 0s. F inally, the COPYstate increments theIDXcounter while simultaneously loading a new value into the MINregister. The COPYstate produces 1 forIDX.CNT, but must use the signal THENcoming from the datapath to decide whether or not MINis loaded. The advantage of a one-hot encod- ing becomes obvious when we write equations for the six control signals and the next-state logic, as shown to the right. Implementing the logic to complete our design now requires only a handful of small logic gates.IDX.RST=S4 IDX.CNT=S3+S0 MIN.LD=S3+S0·THEN A.LD=S2 B.LD=S2 CNT.RST=S2S+ 4=S4·START+S0·DONE S+ 3=S4·START S+ 2=S3+S0·DONE S+ 1=S2+S1·LAST S+ 0=S1·LAST Notice that the terms in each control signal can be read directly fr om the rows of the state table and OR’d together. The terms in each of the next-state equations repres ent the incoming arcs for the corresponding state. For example, the WAITstate has one self-loop (the ﬁrst term) and a transition arc coming from the COPYstate when the loop is done. These expressions complete our design .'}"
"3.8 Summary of Part 3 of the Course 131 ECE120: Introduction to Computer Engineering Notes Set 3.8 Summary of Part 3 of the Course Students often ﬁnd this part of the course more challenging than t he earlier parts of the course. In addition to these notes, you should read Chapters 4 and 5 of the Patt and P atel textbook, which cover the von Neumann model, instruction processing, and ISAs. You should recognize all of these terms and be able to explain what th ey mean. For the speciﬁc circuits, you should be able to draw them and explain how they work. Actually, w e don’t care whether you can draw something from memory—a mux, for example—provided that you kno w what a mux does and can derive a gate diagram correctly for one in a few minutes. Higher-level skills ar e much more valuable","{'page_number': 136, 'textbook_name': 'ECE-120-student-notes', 'text': '•digital systems terms – module – fan-in – fan-out – machine models: Moore and Mealy •simple state machines – synchronous counter – ripple counter – serialization (of bit-sliced design) •ﬁnite state machines (FSMs) – states and state representation – transition rule – self-loop – next state (+) notation – meaning of don’t care in input combination – meaning of don’t care in output – unused states and initialization – completeness (with regard to FSM speciﬁcation) – list of (abstract) states – next-state table/state transition table/state table – state transition diagram/transition diagram/state diagram •memory – number of addresses – addressability – read/write logic – serial/random access memory (RAM) – volatile/non-volatile (N-V) – static/dynamic RAM (SRAM/DRAM) – SRAM cell – DRAM cell – design as a collection of cells – coincident selection•von Neumann model – processing unit – register ﬁle – arithmetic logic unit (ALU) – word size – control unit – program counter (PC) – instruction register (IR) – implementation as FSM – input and output units – memory – memory address register (MAR) – memory data register (MDR) – processor datapath – bus – control signal •tri-state buﬀer – meaning of Z/hi-Z output – use in distributed mux •instruction processing - fetch - decode - execute - register transfer language (RTL) •Instruction Set Architecture (ISA) - instruction encoding - ﬁeld (of an encoded instruction) - operation code (opcode) - types of instructions - operations - data movement - control ﬂow - addressing modes - immediate - register - PC-relative - indirect - base + oﬀset'}"
•digital systems terms – module – fan-in – fan-out – machine models: Moore and Mealy •simple state machines – synchronous counter – ripple counter – serialization (of bit-sliced design) •ﬁnite state machines (FSMs) – states and state representation – transition rule – self-loop – next state (+) notation – meaning of don’t care in input combination – meaning of don’t care in output – unused states and initialization – completeness (with regard to FSM speciﬁcation) – list of (abstract) states – next-state table/state transition table/state table – state transition diagram/transition diagram/state diagram •memory – number of addresses – addressability – read/write logic – serial/random access memory (RAM) – volatile/non-volatile (N-V) – static/dynamic RAM (SRAM/DRAM) – SRAM cell – DRAM cell – design as a collection of cells – coincident selection•von Neumann model – processing unit – register ﬁle – arithmetic logic unit (ALU) – word size – control unit – program counter (PC) – instruction register (IR) – implementation as FSM – input and output units – memory – memory address register (MAR) – memory data register (MDR) – processor datapath – bus – control signal •tri-state buﬀer – meaning of Z/hi-Z output – use in distributed mux •instruction processing - fetch - decode - execute - register transfer language (RTL) •Instruction Set Architecture (ISA) - instruction encoding - ﬁeld (of an encoded instruction) - operation code (opcode) - types of instructions - operations - data movement - control ﬂow - addressing modes - immediate - register - PC-relative - indirect - base + oﬀset,"{'page_number': 136, 'textbook_name': 'ECE-120-student-notes', 'text': '•digital systems terms – module – fan-in – fan-out – machine models: Moore and Mealy •simple state machines – synchronous counter – ripple counter – serialization (of bit-sliced design) •ﬁnite state machines (FSMs) – states and state representation – transition rule – self-loop – next state (+) notation – meaning of don’t care in input combination – meaning of don’t care in output – unused states and initialization – completeness (with regard to FSM speciﬁcation) – list of (abstract) states – next-state table/state transition table/state table – state transition diagram/transition diagram/state diagram •memory – number of addresses – addressability – read/write logic – serial/random access memory (RAM) – volatile/non-volatile (N-V) – static/dynamic RAM (SRAM/DRAM) – SRAM cell – DRAM cell – design as a collection of cells – coincident selection•von Neumann model – processing unit – register ﬁle – arithmetic logic unit (ALU) – word size – control unit – program counter (PC) – instruction register (IR) – implementation as FSM – input and output units – memory – memory address register (MAR) – memory data register (MDR) – processor datapath – bus – control signal •tri-state buﬀer – meaning of Z/hi-Z output – use in distributed mux •instruction processing - fetch - decode - execute - register transfer language (RTL) •Instruction Set Architecture (ISA) - instruction encoding - ﬁeld (of an encoded instruction) - operation code (opcode) - types of instructions - operations - data movement - control ﬂow - addressing modes - immediate - register - PC-relative - indirect - base + oﬀset'}"
"132  We expect you to be able to exercise the following skills: •Transform a bit-sliced design into a serial design, and explain the tra deoﬀs involved in terms of area and time required to compute a result. •Based on a transition diagram, implement a synchronous counter fr om ﬂip-ﬂops and logic gates. •Implement a binary ripple counter (but not necessarily a more gener al type of ripple counter) from ﬂip-ﬂops and logic gates. •Given an FSM implemented as digital logic, analyze the FSM to produce a state transition diagram. •Design an FSM to meet an abstract speciﬁcation for a task, including production of speciﬁed output signals, and possibly including selection of appropriate inputs. •Complete the speciﬁcation of an FSM by ensuring that each state inc ludes a transition rule for every possible input combination. •Compose memory chips into larger memory systems, using additional decoders when necessary. •Encode LC-3 instructions into machine code. •Read and understand programs written in LC-3 assembly/machine c ode. At a higher level, we expect that you understand the concepts and ideas suﬃciently well to do the following: •Abstract design symmetries from an FSM speciﬁcation in order to sim plify the implementation. •Make use of a high-level state design, possibly with many sub-state s in each high-level state, to simplify the implementation. •Use counters to insert time-based transitions between states (s uch as timeouts). •Implement an FSM using logic components such as registers, counte rs, comparators, and adders as building blocks. •Explain the basic organization of a computer’s microarchitecture as well as the role played by elements of a von Neumann design in the processing of instructions. •Identify the stages of processing an instruction (such as fetch, decode, getting operands, execution, and writing back results) in a processor control unit state machine diagram. And, at the highest level, we expect that you will be able to do the follo wing: •Explain the diﬀerence between the Moore and Mealy machine models, a s well as why you might ﬁnd each of them useful when designing an FSM. •Understand the need for initialization of an FSM, be able to analyze an d identify potential problems arising from lack of initialization, and be able to extend an implementatio n to include initialization to an appropriate state when necessary. •Understand how the choice of internal state bits for an FSM can aﬀ ect the complexity of the imple- mentation of next-state and output logic, and be able to select a re asonable state assignment. •IdentifyandﬁxdesignﬂawsinsimpleFSMsbyanalyzinganexistingimplem entation, comparingitwith thespeciﬁcation,andremovinganydiﬀerencesbymakinganyneces sarychangestotheimplementation.","{'page_number': 137, 'textbook_name': 'ECE-120-student-notes', 'text': '132  We expect you to be able to exercise the following skills: •Transform a bit-sliced design into a serial design, and explain the tra deoﬀs involved in terms of area and time required to compute a result. •Based on a transition diagram, implement a synchronous counter fr om ﬂip-ﬂops and logic gates. •Implement a binary ripple counter (but not necessarily a more gener al type of ripple counter) from ﬂip-ﬂops and logic gates. •Given an FSM implemented as digital logic, analyze the FSM to produce a state transition diagram. •Design an FSM to meet an abstract speciﬁcation for a task, including production of speciﬁed output signals, and possibly including selection of appropriate inputs. •Complete the speciﬁcation of an FSM by ensuring that each state inc ludes a transition rule for every possible input combination. •Compose memory chips into larger memory systems, using additional decoders when necessary. •Encode LC-3 instructions into machine code. •Read and understand programs written in LC-3 assembly/machine c ode. At a higher level, we expect that you understand the concepts and ideas suﬃciently well to do the following: •Abstract design symmetries from an FSM speciﬁcation in order to sim plify the implementation. •Make use of a high-level state design, possibly with many sub-state s in each high-level state, to simplify the implementation. •Use counters to insert time-based transitions between states (s uch as timeouts). •Implement an FSM using logic components such as registers, counte rs, comparators, and adders as building blocks. •Explain the basic organization of a computer’s microarchitecture as well as the role played by elements of a von Neumann design in the processing of instructions. •Identify the stages of processing an instruction (such as fetch, decode, getting operands, execution, and writing back results) in a processor control unit state machine diagram. And, at the highest level, we expect that you will be able to do the follo wing: •Explain the diﬀerence between the Moore and Mealy machine models, a s well as why you might ﬁnd each of them useful when designing an FSM. •Understand the need for initialization of an FSM, be able to analyze an d identify potential problems arising from lack of initialization, and be able to extend an implementatio n to include initialization to an appropriate state when necessary. •Understand how the choice of internal state bits for an FSM can aﬀ ect the complexity of the imple- mentation of next-state and output logic, and be able to select a re asonable state assignment. •IdentifyandﬁxdesignﬂawsinsimpleFSMsbyanalyzinganexistingimplem entation, comparingitwith thespeciﬁcation,andremovinganydiﬀerencesbymakinganyneces sarychangestotheimplementation.'}"
"4.1 Control Unit Design 133 ECE120: Introduction to Computer Engineering Notes Set 4.1 Control Unit Design Appendix C of the Patt and Patel textbook describes a microarchit ecture for the LC-3 ISA, including a control unit implementation. In this set of notes, we introduce a fe w concepts and strategies for control unit design, using the textbook’s LC-3 microarchitecture to help illus trate them. Several ﬁgures from the textbook are reproduced with permission in these notes as an aid to understanding. The control unit of a computer based on the von Neumann model ca n be viewed as an FSM that fetches instructions from memory and executes them. Many possible impleme ntations exist both for the control unit itself and for the resources that it controls, the other compo nents in the von Neumann model, which we collectively call the datapath . In this set of notes, we discuss two strategies for structured c ontrol unit design and introduce the idea of using memories to encode logic funct ions. Let’sbeginbyrecallingthatthecontrolunitconsistsofthreepart s: ahigh-levelFSMthatcontrolsinstruction processing, a program counter (PC) register that holds the addr ess of the next instruction to be executed, and an instruction register (IR) that holds the current instructio n as it executes. Other von Neumann components provide inputs to the control unit . The memory unit, for example, contains the instructions and data on which the program executes. The pro cessing unit contains a register ﬁle and condition codes (N, Z, and P for the LC-3 ISA). The outputs of the control unit are signals that control operation of the datapath: the processing unit, the memory, and the I/O interfaces. The basic problem that we must solve, then, for control unit design, is to map instruc tion processing and the state of the FSM (including the PC and the IR) into appropriate sequences of control signals for the datapath. 4.1.1 LC-3 Datapath Control Signals As we have skipped over the implementation details of interrupts and privilege of the LC-3 in our class, let’s consider a microarchitecture and datapath without those capabilit ies. The ﬁgure on the next page (Patt and Patel Figure C.3) shows an LC-3 datapath and control signals witho ut support for interrupts and privilege. Some of the datapath con- trol signals mentioned in the textbook are no longer nec- essary in the simpliﬁed de- sign. Let’s discuss the sig- nals that remain and give some examples of how they are used. A list appears to the right. First, we have a set of seven 1-bit control signals (start- ing with “LD.”) that spec- iﬁes whether registers in the datapath load new values. Next, there are four 1-bit signals (starting with “Gate”) for tri-state buﬀers that control access to the bus. These four implement a distributed mux for the bus. Only one value cansignal meaning LD.MAR load new value into memory address register LD.MDR load new value into memory data register LD.IR load new value into instruction register LD","{'page_number': 138, 'textbook_name': 'ECE-120-student-notes', 'text': '”) that spec- iﬁes whether registers in the datapath load new values. Next, there are four 1-bit signals (starting with “Gate”) for tri-state buﬀers that control access to the bus. These four implement a distributed mux for the bus. Only one value cansignal meaning LD.MAR load new value into memory address register LD.MDR load new value into memory data register LD.IR load new value into instruction register LD.BEN load new value into branch enable register LD.REG load new value into register ﬁle LD.CC load new values into condition code registers (N,Z,P) LD.PC load new value into program counter GatePC write program counter value onto bus GateMDR write memory data register onto bus GateALU write arithmetic logic unit result onto bus GateMARMUX write memory address register mux output onto bus PCMUX select value to write to program counter (2 bits) DRMUX select value to write to destination register (2 bits) SR1MUX select register to read from register ﬁle (2 bits) ADDR1MUX select register component of address (1 bit) ADDR2MUX select oﬀset component of address (2 bits) MARMUX select type of address generation (1 bit) ALUK select arithmetic logic unit operation (2 bits) MIO.EN enable memory R.W read or write from memory appear on the bus in any cycle, so at most one of these signals can be 1; others must all be 0 to avoid creating a short.'}"
"”) that spec- iﬁes whether registers in the datapath load new values. Next, there are four 1-bit signals (starting with “Gate”) for tri-state buﬀers that control access to the bus. These four implement a distributed mux for the bus. Only one value cansignal meaning LD.MAR load new value into memory address register LD.MDR load new value into memory data register LD.IR load new value into instruction register LD.BEN load new value into branch enable register LD.REG load new value into register ﬁle LD.CC load new values into condition code registers (N,Z,P) LD.PC load new value into program counter GatePC write program counter value onto bus GateMDR write memory data register onto bus GateALU write arithmetic logic unit result onto bus GateMARMUX write memory address register mux output onto bus PCMUX select value to write to program counter (2 bits) DRMUX select value to write to destination register (2 bits) SR1MUX select register to read from register ﬁle (2 bits) ADDR1MUX select register component of address (1 bit) ADDR2MUX select oﬀset component of address (2 bits) MARMUX select type of address generation (1 bit) ALUK select arithmetic logic unit operation (2 bits) MIO.EN enable memory R.W read or write from memory appear on the bus in any cycle, so at most one of these signals can be 1; others must all be 0 to avoid creating a short.","{'page_number': 138, 'textbook_name': 'ECE-120-student-notes', 'text': '”) that spec- iﬁes whether registers in the datapath load new values. Next, there are four 1-bit signals (starting with “Gate”) for tri-state buﬀers that control access to the bus. These four implement a distributed mux for the bus. Only one value cansignal meaning LD.MAR load new value into memory address register LD.MDR load new value into memory data register LD.IR load new value into instruction register LD.BEN load new value into branch enable register LD.REG load new value into register ﬁle LD.CC load new values into condition code registers (N,Z,P) LD.PC load new value into program counter GatePC write program counter value onto bus GateMDR write memory data register onto bus GateALU write arithmetic logic unit result onto bus GateMARMUX write memory address register mux output onto bus PCMUX select value to write to program counter (2 bits) DRMUX select value to write to destination register (2 bits) SR1MUX select register to read from register ﬁle (2 bits) ADDR1MUX select register component of address (1 bit) ADDR2MUX select oﬀset component of address (2 bits) MARMUX select type of address generation (1 bit) ALUK select arithmetic logic unit operation (2 bits) MIO.EN enable memory R.W read or write from memory appear on the bus in any cycle, so at most one of these signals can be 1; others must all be 0 to avoid creating a short.'}"
134  MEMORYOUTPUT INPUT DSRDDR KBDR ADDR. CTL. LOGICGateMDR MDR LD.MDR INMUXMAR LD.MAR 2KBSRMIO.EN R MEM.ENR.W MIO.ENSEXT SEXT SEXT SEXT[5:0][8:0][10:0]+1GateMARMUX 16 161616 16 1616 1616 161616 16 16ALUB A GateALU16SR2MUXPC + IRZEXT R N Z P LOGICSR2 OUTSR1 OUTREG FILE [7:0] 2PCMUX LD.CCGatePC LD.PC LD.IRMARMUX ALUK16 1616163 32 [4:0]0ADDR1MUX 2ADDR2MUXSR1 SR2LD.REGDR 3 CONTROL,"{'page_number': 139, 'textbook_name': 'ECE-120-student-notes', 'text': '134  MEMORYOUTPUT INPUT DSRDDR KBDR ADDR. CTL. LOGICGateMDR MDR LD.MDR INMUXMAR LD.MAR 2KBSRMIO.EN R MEM.ENR.W MIO.ENSEXT SEXT SEXT SEXT[5:0][8:0][10:0]+1GateMARMUX 16 161616 16 1616 1616 161616 16 16ALUB A GateALU16SR2MUXPC + IRZEXT R N Z P LOGICSR2\r OUTSR1\r OUTREG FILE [7:0] 2PCMUX LD.CCGatePC LD.PC LD.IRMARMUX ALUK16 1616163 32 [4:0]0ADDR1MUX 2ADDR2MUXSR1 SR2LD.REGDR 3 CONTROL'}"
"4.1 Control Unit Design 135 The third group (ending with “MUX”) of signals controls multiplexers in the datapath. The number of bits for each depends on the number of inputs to the mux; the total nu mber of signals is 10. The last two groups of signals control the ALU and the memory, requiring a total of 4 mo re signals. The total of all groups is thus 25 control signals for the datapath without support for priv ilege and interrupts. 4.1.2 Example Control Word: ADD Before we begin to discuss control unit design in more detail, let’s wor k through a couple of examples of implementing speciﬁc RTL with the control signals available. The ﬁgure below (Patt and Patel Figure C.2) shows a state machine for the LC-3 ISA (again without detail on inte rrupts and privilege). R R R R RPC<–BaseR20 To 18PC<–BaseR R7<–PC [IR[11]] 1 012 4 PC<–PC+off1121To 18 To 18To 18 To 18 To 18To 8 (See Figure C.7) RTIMAR <–PC PC<–PC+1 [INT] MDR<–M  IR<–MDRR DR<–SR1+OP2* set CC DR<–SR1&OP2* set CC[BEN] PC<–PC+off9 PC<–MDRMAR<–PC+off9 MDR<–M[MAR ] R R MAR<–MDRMAR<–PC+off9 MDR<–M[MAR] MAR<–MDRMAR<–B+off6 MAR<–PC+off9MAR<–B+off6 MAR<–PC+off9 MDR<–SR DR<–MDR set CCM[MAR]<–MDR18 32 1 5 7 611 30 0 122 29 31 26 2324 25 27To 18 To 18 To 18 To 18To 180 R R MDR<–M[MAR]To 49 (See Figure C.7) 28 30 210 NOTES 16MDR<–M[MAR] R7<–PC B+off6 : Base + SEXT[offset6] PC+off9 : PC + SEXT{offset9] PC+off11 : PC + SEXT[offset11] *OP2 may be SR2 or SEXT[imm5]DR<–NOT(SR) set CC9NOT 14DR<–PC+off9 set CCLEA LD LDR LDI STI STR STJSRADD AND JMPBR1 R RBEN<–IR[11] & N + IR[10] & Z + IR[9] & P [IR[15:12]] 1101 To 1333 35 MAR<–ZEXT[IR[7:0]]15TRAP","{'page_number': 140, 'textbook_name': 'ECE-120-student-notes', 'text': '4.1 Control Unit Design 135 The third group (ending with “MUX”) of signals controls multiplexers in the datapath. The number of bits for each depends on the number of inputs to the mux; the total nu mber of signals is 10. The last two groups of signals control the ALU and the memory, requiring a total of 4 mo re signals. The total of all groups is thus 25 control signals for the datapath without support for priv ilege and interrupts. 4.1.2 Example Control Word: ADD Before we begin to discuss control unit design in more detail, let’s wor k through a couple of examples of implementing speciﬁc RTL with the control signals available. The ﬁgure below (Patt and Patel Figure C.2) shows a state machine for the LC-3 ISA (again without detail on inte rrupts and privilege). R R R R RPC<–BaseR20 To 18PC<–BaseR R7<–PC\r [IR[11]] 1 012 4 PC<–PC+off1121To 18 To 18To 18 To 18 To 18To 8\r (See Figure C.7) RTIMAR <–PC\r PC<–PC+1\r [INT] MDR<–M  IR<–MDRR DR<–SR1+OP2*\r set CC DR<–SR1&OP2*\r set CC[BEN] PC<–PC+off9 PC<–MDRMAR<–PC+off9 MDR<–M[MAR ] R R MAR<–MDRMAR<–PC+off9 MDR<–M[MAR] MAR<–MDRMAR<–B+off6 MAR<–PC+off9MAR<–B+off6 MAR<–PC+off9 MDR<–SR DR<–MDR\r set CCM[MAR]<–MDR18 32 1 5 7 611 30 0 122 29 31 26 2324 25 27To 18 To 18 To 18 To 18To 180 R R MDR<–M[MAR]To 49 (See Figure C.7) 28 30 210 NOTES 16MDR<–M[MAR]\r R7<–PC B+off6 : Base + SEXT[offset6] PC+off9 : PC + SEXT{offset9] PC+off11 : PC + SEXT[offset11] *OP2 may be SR2 or SEXT[imm5]DR<–NOT(SR)\r set CC9NOT 14DR<–PC+off9\r set CCLEA LD LDR LDI STI STR STJSRADD AND JMPBR1 R RBEN<–IR[11] & N + IR[10] & Z + IR[9] & P\r [IR[15:12]] 1101 To 1333 35 MAR<–ZEXT[IR[7:0]]15TRAP'}"
"136  Consider now the state that implements the ADD instruction—state number 1 in the ﬁgure on the previous page, just below and to the left of the decode state. The RTL for t he state is: DR←SR + OP2, set CC. We can think of the 25 control signals that implement the desired RTL as acontrol word for the datapath. Let’s begin with the register load control signals. The RTL writes to t wo types of registers: the register ﬁle and the condition codes. To accomplish these simultaneous writes, L D.REG and LD.CC must be high. No other registers in the datapath should change, so the other ﬁve L D signals—LD.MAR, LD.MDR, LD.IR, LD.BEN, and LD.PC—should be low. What about the bus? In order to write the result of the add operat ion into the register ﬁle, the control signals must allow the ALU to write its result on to the bus. So we need G ateALU=1. And the other Gate signals—GatePC, GateMDR, and GateMARMUX—must all be 0. The con dition codes are also calculated from the value on the bus, but they are calculated on the same value as is written to the register ﬁle (by the deﬁnition of the LC-3 ISA). If the RTL for an FSM state implicitly r equires more than one value to appear on the bus in the same cycle, that state is impossible to impleme nt using the given datapath. Either the datapath or the state machine must be changed in such a case. The textbook’s design has been fairly thoroughly tested and debugged. The earlier ﬁgure of the dat- apath does not show all of the muxes. The remaining muxes appear in the ﬁgure to the right (Patt and Patel Fig- ure C.6). Some of the muxes in the datapath must be used to enable the addition needed for ADD to occur. The DRMUX must select its IR[11:9] input in order to write to the destination reg- ister speciﬁed by the ADD instruction. Similarly, the SR1MUX must select its IR[8:6] input in order toLogic BEN PZNIR[11:9] (c)IR[11:9] 111DR DRMUX110IR[11:9] (b) (a)IR[8:6] 110 SR1MUXSR1 passthe ﬁrstsourceregisterspeciﬁed bythe ADD to the ALUasinp ut A(seethe datapathﬁgure). SR2MUX is always controlled by the mode bit IR[5], so the control unit does not need to generate anything (note that this signal was not in the list given earlier)","{'page_number': 141, 'textbook_name': 'ECE-120-student-notes', 'text': 'SR2MUX is always controlled by the mode bit IR[5], so the control unit does not need to generate anything (note that this signal was not in the list given earlier). The rest of the muxes in th e datapath—PCMUX, ADDR1MUX, ADDR2MUX, andMARMUX—donotmatter, andthesignalscontrollingt hemaredon’tcares. Forexample, since the PC does not change, the output of the PCMUX is simply disca rded, thus which input the PCMUX forwards to its output cannot matter. The ALU must perform an addition, so we must set the operation typ e ALUK appropriately. And memory should not be enabled (MIO.EN=0), in which case the read/write cont rol for memory, R.W, is a don’t care. These 25 signal values together (including seven don’t cares) implem ent the RTL for the single state of execution for an ADD instruction. 4.1.3 Example Control Word: LDR As a second example, consider the ﬁrst state in the sequence that implements the LDR instruction—state number 6 in the ﬁgure on the previous page. The RTL for the state is : MAR←BaseR + oﬀ6, but BaseR is abbreviated to “B” in the state diagram. What is the control word for this state? Let’s again begin with the re gister load control signals. Only the MAR is written by the RTL, so we need LD.MAR=1 and the other load sign als all equal to 0.'}"
"SR2MUX is always controlled by the mode bit IR[5], so the control unit does not need to generate anything (note that this signal was not in the list given earlier). The rest of the muxes in th e datapath—PCMUX, ADDR1MUX, ADDR2MUX, andMARMUX—donotmatter, andthesignalscontrollingt hemaredon’tcares. Forexample, since the PC does not change, the output of the PCMUX is simply disca rded, thus which input the PCMUX forwards to its output cannot matter. The ALU must perform an addition, so we must set the operation typ e ALUK appropriately. And memory should not be enabled (MIO.EN=0), in which case the read/write cont rol for memory, R.W, is a don’t care. These 25 signal values together (including seven don’t cares) implem ent the RTL for the single state of execution for an ADD instruction. 4.1.3 Example Control Word: LDR As a second example, consider the ﬁrst state in the sequence that implements the LDR instruction—state number 6 in the ﬁgure on the previous page. The RTL for the state is : MAR←BaseR + oﬀ6, but BaseR is abbreviated to “B” in the state diagram. What is the control word for this state? Let’s again begin with the re gister load control signals. Only the MAR is written by the RTL, so we need LD.MAR=1 and the other load sign als all equal to 0.","{'page_number': 141, 'textbook_name': 'ECE-120-student-notes', 'text': 'SR2MUX is always controlled by the mode bit IR[5], so the control unit does not need to generate anything (note that this signal was not in the list given earlier). The rest of the muxes in th e datapath—PCMUX, ADDR1MUX, ADDR2MUX, andMARMUX—donotmatter, andthesignalscontrollingt hemaredon’tcares. Forexample, since the PC does not change, the output of the PCMUX is simply disca rded, thus which input the PCMUX forwards to its output cannot matter. The ALU must perform an addition, so we must set the operation typ e ALUK appropriately. And memory should not be enabled (MIO.EN=0), in which case the read/write cont rol for memory, R.W, is a don’t care. These 25 signal values together (including seven don’t cares) implem ent the RTL for the single state of execution for an ADD instruction. 4.1.3 Example Control Word: LDR As a second example, consider the ﬁrst state in the sequence that implements the LDR instruction—state number 6 in the ﬁgure on the previous page. The RTL for the state is : MAR←BaseR + oﬀ6, but BaseR is abbreviated to “B” in the state diagram. What is the control word for this state? Let’s again begin with the re gister load control signals. Only the MAR is written by the RTL, so we need LD.MAR=1 and the other load sign als all equal to 0.'}"
"4.1 Control Unit Design 137 The address (BaseR + oﬀ6) is generated by the address adder, th en passes through the MARMUX to the bus, from which it can be written into the MAR. To allow the MARMUX to w rite to the bus, we set GateMARMUX high and set the other three Gate control signals low. More of the muxes are needed for this state’s RTL than we needed f or the ADD execution state’s RTL. The SR1MUX must again select its IR[8:6] input, this time in order to pass th e BaseR speciﬁed by the instruction to ADDR1MUX. ADDR1MUX must then select the output of the regist er ﬁle in order to pass the BaseR to the address adder. The other input of the address adder shou ld be oﬀ6, which corresponds to the sign- extended version of IR[5:0]. ADDR2MUX must select this input to pass to the address adder. Finally, the MARMUX must select the output of the address adder. The PCMUX a nd DRMUX do not matter for this case, and can be left as don’t cares. Neither the PC nor any registe r in the register ﬁle is written. The output of the ALU is not used, so which operation it performs is ir relevant, and the ALUK controls are also don’t cares. Memory is also not used, so again we set MIO.EN=0. A nd, as before, the R.W control for memory is a don’t care. These 25 signal values together (again includ ing seven don’t cares) implement the RTL for the ﬁrst state of LDR execution. 4.1.4 Hardwired Control Now we are ready to think about how control signals can be generated. As illustrated to the right, instruction processing consists of two steps repeated inﬁnitely: fetch an instruction, then execute the instruction. Let’s say that we choose a ﬁxed number of cycles for each of these two steps. We can then control our system with afetch instructionexecute instruction counter, using the counter’s value and the IR to generate the con trol signals through combinational logic. The PC is used only as data and has little or no direct eﬀect on how the s ystem fetches and processes instructions.14This approach in general is called hardwired control . How many cycles do we need for instruction fetch? How many cycles d o we need for instruction processing? The answers depend on the factors: the complexity of the ISA, an d the complexity of the datapath. Given a simple ISA, we can design a datapath that is powerful enough to process any instruction in a single cycle. The control unit for such a design is an example of single-cycle, hardwired control . While this approach simpliﬁes the control unit, the cycle time for the clock is limit ed by the slowest instruction. The clock must also be slow enough to let memory operations complete in sin gle cycle, both for instruction fetch and for instructions that require a memory access. Such a low clock rate is usually not acceptable","{'page_number': 142, 'textbook_name': 'ECE-120-student-notes', 'text': 'The control unit for such a design is an example of single-cycle, hardwired control . While this approach simpliﬁes the control unit, the cycle time for the clock is limit ed by the slowest instruction. The clock must also be slow enough to let memory operations complete in sin gle cycle, both for instruction fetch and for instructions that require a memory access. Such a low clock rate is usually not acceptable. More generally, we can use a simpler datapath and break both instru ction fetch and instruction processing into multiple steps. Using the datapath in the ﬁgure from Patt and Pa tel, for example, instruction fetch requires three steps, and the number of steps for instruction pr ocessing depends on the type of instruction being processed. The state diagram shown earlier illustrates the st eps for each opcode. Although the datapath is not powerful enough to complete instruc tions in a single cycle, we can build a multi-cycle, hardwired control unit (one based on combinational logic). In fact, this type of contr ol unit is not much more complex than the single-cycle version. For the cont rol unit’s FSM, we can use a binary counter to enumerate ﬁrst the steps of fetch, then the steps o f processing. The counter value along with the IR register can drive combinational logic to produce control signals . And, to avoid processing at the speed of the slowest instruction (the opcode that requires the largest num ber of steps; LDI and STI in the LC-3 state diagram), we can add a reset signal to the counter to force it back to instruction fetch. The FSM counter reset signal is simply another control signal. Finally, we can add one m ore signal that pauses the counter while waiting for a memory operation to complete. The system clock ca n then run at the speed of the logic rather than at the speed of memory. The control unit implementat ion discussed in Appendix C of Patt and Patel is not hardwired, but it does make use of a memory ready signa l to achieve this decoupling between the clock speed of the processor and the access time of the memor y. 14Some ISAs do split the address space into privileged and non- privileged regions, but we ignore that possibility here.'}"
"The control unit for such a design is an example of single-cycle, hardwired control . While this approach simpliﬁes the control unit, the cycle time for the clock is limit ed by the slowest instruction. The clock must also be slow enough to let memory operations complete in sin gle cycle, both for instruction fetch and for instructions that require a memory access. Such a low clock rate is usually not acceptable. More generally, we can use a simpler datapath and break both instru ction fetch and instruction processing into multiple steps. Using the datapath in the ﬁgure from Patt and Pa tel, for example, instruction fetch requires three steps, and the number of steps for instruction pr ocessing depends on the type of instruction being processed. The state diagram shown earlier illustrates the st eps for each opcode. Although the datapath is not powerful enough to complete instruc tions in a single cycle, we can build a multi-cycle, hardwired control unit (one based on combinational logic). In fact, this type of contr ol unit is not much more complex than the single-cycle version. For the cont rol unit’s FSM, we can use a binary counter to enumerate ﬁrst the steps of fetch, then the steps o f processing. The counter value along with the IR register can drive combinational logic to produce control signals . And, to avoid processing at the speed of the slowest instruction (the opcode that requires the largest num ber of steps; LDI and STI in the LC-3 state diagram), we can add a reset signal to the counter to force it back to instruction fetch. The FSM counter reset signal is simply another control signal. Finally, we can add one m ore signal that pauses the counter while waiting for a memory operation to complete. The system clock ca n then run at the speed of the logic rather than at the speed of memory. The control unit implementat ion discussed in Appendix C of Patt and Patel is not hardwired, but it does make use of a memory ready signa l to achieve this decoupling between the clock speed of the processor and the access time of the memor y. 14Some ISAs do split the address space into privileged and non- privileged regions, but we ignore that possibility here.","{'page_number': 142, 'textbook_name': 'ECE-120-student-notes', 'text': 'The control unit for such a design is an example of single-cycle, hardwired control . While this approach simpliﬁes the control unit, the cycle time for the clock is limit ed by the slowest instruction. The clock must also be slow enough to let memory operations complete in sin gle cycle, both for instruction fetch and for instructions that require a memory access. Such a low clock rate is usually not acceptable. More generally, we can use a simpler datapath and break both instru ction fetch and instruction processing into multiple steps. Using the datapath in the ﬁgure from Patt and Pa tel, for example, instruction fetch requires three steps, and the number of steps for instruction pr ocessing depends on the type of instruction being processed. The state diagram shown earlier illustrates the st eps for each opcode. Although the datapath is not powerful enough to complete instruc tions in a single cycle, we can build a multi-cycle, hardwired control unit (one based on combinational logic). In fact, this type of contr ol unit is not much more complex than the single-cycle version. For the cont rol unit’s FSM, we can use a binary counter to enumerate ﬁrst the steps of fetch, then the steps o f processing. The counter value along with the IR register can drive combinational logic to produce control signals . And, to avoid processing at the speed of the slowest instruction (the opcode that requires the largest num ber of steps; LDI and STI in the LC-3 state diagram), we can add a reset signal to the counter to force it back to instruction fetch. The FSM counter reset signal is simply another control signal. Finally, we can add one m ore signal that pauses the counter while waiting for a memory operation to complete. The system clock ca n then run at the speed of the logic rather than at the speed of memory. The control unit implementat ion discussed in Appendix C of Patt and Patel is not hardwired, but it does make use of a memory ready signa l to achieve this decoupling between the clock speed of the processor and the access time of the memor y. 14Some ISAs do split the address space into privileged and non- privileged regions, but we ignore that possibility here.'}"
"138  The ﬁgure to the right illus- trates a general multi-cycle, hardwired control unit. The three blocks on the left are the control unit state. The combinational logic in the middle uses the control unit state along with some dat- apath status bits to com- pute the control signals for the datapath and the extra controls needed for the FSM counter, IR, and PC. The datapathappearstothe right in the ﬁgure.IRLD DATA PCLD DATAlogiccombinational datapathN−cycle binary counter control signals datapath statusPAUSERESET How complex is the combinational logic? As mentioned earlier, we assum e that the PC does not directly aﬀect control. But we still have 16 bits of IR, the FSM counter stat e, and the datapath status signals. Perhaps we need 24-variable K-maps? Here’s where engineering and human design come to the rescue: by careful design of the ISA and the encoding, the authors have mad e many of the datapath control signals for the LC-3 ISA quite simple. For example, the register that appears o n the register ﬁle’s SR2 output is always speciﬁed by IR[2:0]. The SR1 output requires a mux, but the choices a re limited to IR[11:9] and IR[8:6] (and R6 in the design with support for interrupts). Similarly, the des tination register in the register ﬁle is always R7 or IR[11:9] (or, again, R6 when supporting interrupts). T he control signals for an LC-3 datapath depend almost entirely on the state of the control unit FSM (count er bits in a hardwired design) and the opcode IR[15:12]. The control signals are thus reduced to fairly simp le functions. Let’s imagine building a hardwired control unit for the LC-3. Let’s sta rt by being more precise about the number of inputs to the combinational logic. Although most decisions are based on the opcode, the datapath and state diagram shown earlier for the LC-3 ISA do have one instan ce of using another instruction bit to determine behavior. Speciﬁcally, the JSR instruction has two modes , and the control unit uses IR[11] to choose between them. So we need to have ﬁve bits of IR instead of f our as input to our logic. Howmanydatapathstatussignalsareneeded? When thecontrolu nit accessesmemory, itmust waituntil the memory ﬁnishes the access, as indicated by a memory ready signal R . And the control unit must implement the conditional part of conditional branches, for which it uses the datapath’s branch enable signal BEN. These two datapath status signals suﬃce for our design. How many bits do we need for the counter? Instruction fetch requ ires three cycles: one to move the PC to the MAR and increment the PC, a second to read from memory into MDR, and a third to move the instruction bits across the bus from MDR into IR","{'page_number': 143, 'textbook_name': 'ECE-120-student-notes', 'text': 'And the control unit must implement the conditional part of conditional branches, for which it uses the datapath’s branch enable signal BEN. These two datapath status signals suﬃce for our design. How many bits do we need for the counter? Instruction fetch requ ires three cycles: one to move the PC to the MAR and increment the PC, a second to read from memory into MDR, and a third to move the instruction bits across the bus from MDR into IR. Instruction deco ding in a hardwired design is implicit and requires no cycles: since all of our control signals can depend o n the IR, we do not need a cycle to change the FSM state to reﬂect the opcode. Looking at the LC-3 s tate diagram, we see that processing an instruction requires at most ﬁve cycles. In total, at most eight ste ps are needed to fetch and process any LC-3 instruction, so we can use a 3-bit binary counter. We thus have a total of ten bits of input: IR[15:11], R, BEN, and a 3-b it counter. Adding the RESET and PAUSE controls for our FSM counter to the 25 control signals listed earlier, we need to ﬁnd 27 functions on 10 variables. That’s still a lot of big K-maps to solve. Is there an easie r way? 4.1.5 Using a Memory for Logic Functions Consider a case in which you need to compute many functions on a sma ll number of bits, such as we just described for the multi-cycle, hardwired control unit. One strate gy is to use a memory (possibly a read-only memory). A 2m×Nmemory can be viewed as computing Narbitrary functions on mvariables. The functions to be computed are speciﬁed by ﬁlling in the bits of the memo ry. So long as the value of mis fairly small, the memory (especially SRAM) can be fast.'}"
"And the control unit must implement the conditional part of conditional branches, for which it uses the datapath’s branch enable signal BEN. These two datapath status signals suﬃce for our design. How many bits do we need for the counter? Instruction fetch requ ires three cycles: one to move the PC to the MAR and increment the PC, a second to read from memory into MDR, and a third to move the instruction bits across the bus from MDR into IR. Instruction deco ding in a hardwired design is implicit and requires no cycles: since all of our control signals can depend o n the IR, we do not need a cycle to change the FSM state to reﬂect the opcode. Looking at the LC-3 s tate diagram, we see that processing an instruction requires at most ﬁve cycles. In total, at most eight ste ps are needed to fetch and process any LC-3 instruction, so we can use a 3-bit binary counter. We thus have a total of ten bits of input: IR[15:11], R, BEN, and a 3-b it counter. Adding the RESET and PAUSE controls for our FSM counter to the 25 control signals listed earlier, we need to ﬁnd 27 functions on 10 variables. That’s still a lot of big K-maps to solve. Is there an easie r way? 4.1.5 Using a Memory for Logic Functions Consider a case in which you need to compute many functions on a sma ll number of bits, such as we just described for the multi-cycle, hardwired control unit. One strate gy is to use a memory (possibly a read-only memory). A 2m×Nmemory can be viewed as computing Narbitrary functions on mvariables. The functions to be computed are speciﬁed by ﬁlling in the bits of the memo ry. So long as the value of mis fairly small, the memory (especially SRAM) can be fast.","{'page_number': 143, 'textbook_name': 'ECE-120-student-notes', 'text': 'And the control unit must implement the conditional part of conditional branches, for which it uses the datapath’s branch enable signal BEN. These two datapath status signals suﬃce for our design. How many bits do we need for the counter? Instruction fetch requ ires three cycles: one to move the PC to the MAR and increment the PC, a second to read from memory into MDR, and a third to move the instruction bits across the bus from MDR into IR. Instruction deco ding in a hardwired design is implicit and requires no cycles: since all of our control signals can depend o n the IR, we do not need a cycle to change the FSM state to reﬂect the opcode. Looking at the LC-3 s tate diagram, we see that processing an instruction requires at most ﬁve cycles. In total, at most eight ste ps are needed to fetch and process any LC-3 instruction, so we can use a 3-bit binary counter. We thus have a total of ten bits of input: IR[15:11], R, BEN, and a 3-b it counter. Adding the RESET and PAUSE controls for our FSM counter to the 25 control signals listed earlier, we need to ﬁnd 27 functions on 10 variables. That’s still a lot of big K-maps to solve. Is there an easie r way? 4.1.5 Using a Memory for Logic Functions Consider a case in which you need to compute many functions on a sma ll number of bits, such as we just described for the multi-cycle, hardwired control unit. One strate gy is to use a memory (possibly a read-only memory). A 2m×Nmemory can be viewed as computing Narbitrary functions on mvariables. The functions to be computed are speciﬁed by ﬁlling in the bits of the memo ry. So long as the value of mis fairly small, the memory (especially SRAM) can be fast.'}"
"4.1 Control Unit Design 139 Synthesis tools (or hard work) can, of course, produce smaller de signs that use fewer gates. Actually, tools may be able to optimize a ﬁxed design expressed as read-only memory , too. But designing the functions with a memory makes them easier to modify later. If we make a mistake , for example, in computing one of the functions, we need only change a bit or two in the memory instead of solving equations and reoptimizing and replacing logic gates. We can also extend our design if we have spa ce remaining (that is, if the functions are undeﬁned for some combinations of the minputs). The Cray T3D supercomputer, for example, used a similar approach to add new instructions to the Alpha processors on which it was based. This strategyis eﬀective in many contexts, so let’s brieﬂy discuss tw o analogouscases. In software, a memory becomes a lookup table. Before handheld calculators, lookup tables were used by humans to compute transcendental functions such as sines, cosines, logarithms. Co mputer graphics hardware and software used a similar approach for transcendental functions in order to reduc e cost and improve speed. Functions such as counting 1 bits in a word are useful for processor scheduling and networking, but not all ISAs provide this type of instruction. In such cases, lookup tables in software a re often the best solution. In programmable hardware such as Field Programmable Gate Arrays (FPGAs), lookup tables (called LUTs in this context) have played an important role in implementing arbitrar y logic functions. The FPGA is the modern form of the programmable logic array (PLA) mentioned in the textbook, and will be your main tool for developing digital hardware in ECE385. For many years, FP GAs served as a hardware prototyping platform, but many companies today ship their ﬁrst round product s using designs mapped to FPGAs. Why? Chips are more and more expensive to design, and mistakes are cost ly to ﬁx. In contrast, while companies pay more to buy an FPGA than to produce a chip (after the ﬁrst chip !), errors in the design can usually be ﬁxed by sending customers a new version through the Internet. Let’s return to our LC-3 example. Instead of solving the K-maps, w e can use a small memory: 210×27 bits (27,648 bits total). We just need calculate the bits, put them into th e memory, and use the memory to produce the control signals. The “address” input to the memory a re the same 10 bits that we needed for our combinational logic: IR[15:11], R, BEN, and the FSM counter. The data outputs of the memory are the control signals and the RESET and PAUSE inputs to the FSM counter . And we’re done. We can do a little better, though. The datapath in the textbook was designed to work with the textbook’s control unit. If we add a little logic, we can signiﬁcantly simplify our mem ory-based, hardwired implemen- tation. For example, we only need to pause the FSM counter when wa iting for memory","{'page_number': 144, 'textbook_name': 'ECE-120-student-notes', 'text': 'And we’re done. We can do a little better, though. The datapath in the textbook was designed to work with the textbook’s control unit. If we add a little logic, we can signiﬁcantly simplify our mem ory-based, hardwired implemen- tation. For example, we only need to pause the FSM counter when wa iting for memory. If we can produce a control signal that indicates a need to wait for memory, say WAIT -MEM, we can use a couple of gates to compute the FSM counter’s PAUSE signal as WAIT-MEM AND (NOT R). Making this change shrinks our memory to 29×27 bits. The extra two control signals in this case are RESET and WAI T-MEM. Next, look at how BEN is used in the state diagram: the only use is to te rminate the processing of branch instructions when no branch should occur (when BEN=0). We can fo ld that functionality into the FSM counter’s RESET signalby producing a branch reset signal, BR-RES ET, to reset the counter to end a branch and a second signal, INST-DONE, when an instruction is done. The RE SET input for the FSM counter is then (BR-RESET AND (NOT BEN)) OR INST-DONE. And our memory fu rther shrinks to 28×28 bits, where the extra three control signals are WAIT-MEM, BR-RESET, and INST-DONE. Finally, recall that the only need for IR[11] is to implement the two for ms of JSR. But we can add wires to connect SR1 to PCMUX’s fourth input, then control the PCMUX o utput selection using IR[11] when appropriate (using another control signal). With this extension, w e can implement both forms with a single state, writing to both R7 and PC in the same cycle. Our ﬁnal memory c an then be 27×29 bits (3,712 bits total), which is less than one-seventh the number of bits that we ne eded before modifying the datapath.'}"
"And we’re done. We can do a little better, though. The datapath in the textbook was designed to work with the textbook’s control unit. If we add a little logic, we can signiﬁcantly simplify our mem ory-based, hardwired implemen- tation. For example, we only need to pause the FSM counter when wa iting for memory. If we can produce a control signal that indicates a need to wait for memory, say WAIT -MEM, we can use a couple of gates to compute the FSM counter’s PAUSE signal as WAIT-MEM AND (NOT R). Making this change shrinks our memory to 29×27 bits. The extra two control signals in this case are RESET and WAI T-MEM. Next, look at how BEN is used in the state diagram: the only use is to te rminate the processing of branch instructions when no branch should occur (when BEN=0). We can fo ld that functionality into the FSM counter’s RESET signalby producing a branch reset signal, BR-RES ET, to reset the counter to end a branch and a second signal, INST-DONE, when an instruction is done. The RE SET input for the FSM counter is then (BR-RESET AND (NOT BEN)) OR INST-DONE. And our memory fu rther shrinks to 28×28 bits, where the extra three control signals are WAIT-MEM, BR-RESET, and INST-DONE. Finally, recall that the only need for IR[11] is to implement the two for ms of JSR. But we can add wires to connect SR1 to PCMUX’s fourth input, then control the PCMUX o utput selection using IR[11] when appropriate (using another control signal). With this extension, w e can implement both forms with a single state, writing to both R7 and PC in the same cycle. Our ﬁnal memory c an then be 27×29 bits (3,712 bits total), which is less than one-seventh the number of bits that we ne eded before modifying the datapath.","{'page_number': 144, 'textbook_name': 'ECE-120-student-notes', 'text': 'And we’re done. We can do a little better, though. The datapath in the textbook was designed to work with the textbook’s control unit. If we add a little logic, we can signiﬁcantly simplify our mem ory-based, hardwired implemen- tation. For example, we only need to pause the FSM counter when wa iting for memory. If we can produce a control signal that indicates a need to wait for memory, say WAIT -MEM, we can use a couple of gates to compute the FSM counter’s PAUSE signal as WAIT-MEM AND (NOT R). Making this change shrinks our memory to 29×27 bits. The extra two control signals in this case are RESET and WAI T-MEM. Next, look at how BEN is used in the state diagram: the only use is to te rminate the processing of branch instructions when no branch should occur (when BEN=0). We can fo ld that functionality into the FSM counter’s RESET signalby producing a branch reset signal, BR-RES ET, to reset the counter to end a branch and a second signal, INST-DONE, when an instruction is done. The RE SET input for the FSM counter is then (BR-RESET AND (NOT BEN)) OR INST-DONE. And our memory fu rther shrinks to 28×28 bits, where the extra three control signals are WAIT-MEM, BR-RESET, and INST-DONE. Finally, recall that the only need for IR[11] is to implement the two for ms of JSR. But we can add wires to connect SR1 to PCMUX’s fourth input, then control the PCMUX o utput selection using IR[11] when appropriate (using another control signal). With this extension, w e can implement both forms with a single state, writing to both R7 and PC in the same cycle. Our ﬁnal memory c an then be 27×29 bits (3,712 bits total), which is less than one-seventh the number of bits that we ne eded before modifying the datapath.'}"
"140  4.1.6 Microprogrammed Control We are now ready to discuss the second approach to control unit d esign. Take another look at the state diagram for the the LC-3 ISA. Does it remind you of anything? Like a ﬂ owchart, it has relatively few arcs leaving each state—usually only one or two. What if we treat the state diagram as a program? We can use a small m emory to hold microinstructions (another name for control words) and use the FSM state number as the memory address. Without support for interrupts or privilege, and with the datapath extension for JS R mentioned for hardwired control, the LC-3 state machine requiresfewer than 32 states. The datapath has 25 control signals, but we need one more for the datapath extension for JSR. We thus start with 5-bit stat e number (in a register) and a 25×26 bit memory, which we call our control ROM (read-only memory) to distin guish it from the big, slow, von Neumann memory. Each cycle, the microprogrammed control unit applies the FSM state number to the control ROM (no IR bits, just the state number), gets back a set of control signals, and uses them to drive the datapath. To write our microprogram, we need to calculate the control signals for each microinstruction and put them in the control ROM, but we also need to have a way to decide which microinstruction should execute next. We call the latter problem sequencing or microsequencing. Notice that most of the time there’s no choice: we have only onenext microinstruction. One simple approach is then to add the address (t he 5-bitstateID)ofthenextmicroinstructiontothecontrolROM.I nstead of 26 bits per FSM state, we now have 31 bits per FSM state. Sometimes we do need to have two possible next states. When waiting for memory(the von Neumann memory, not the controlROM) to ﬁn ish an access, for example, we want our FSM to stay in the same state, then move to the next state when the access completes. Let’s add a second address to each microinstruction, and add a branch cont rol signal for the microprogram to decide whether we should use the ﬁr st address or the second for the next microinstruction. This design, using a 25×36 bit memory (1,152 bits total), appears to the right.5 5control signals265register (5 bits)FSM state ID55 5 microprogram branch control 32 x 36−bit memoryADDR DATA0 1 The microprogram branch control signal is a Boolean logic ex- pression based on the memory ready signal R and IR[11]. We can implement it with a state ID comparison and a mux, as shown to the right. For the branch instruction execution state, the mux selects input 1, BEN. For all other states, the mux selects input 0, R. When an FSM state has only a single next state, we set both IDs in the control ROM to the ID for that state, so the value of R has no eﬀect.= branch state? microprogram branch control5state IDR BEN 0 1","{'page_number': 145, 'textbook_name': 'ECE-120-student-notes', 'text': '140  4.1.6 Microprogrammed Control We are now ready to discuss the second approach to control unit d esign. Take another look at the state diagram for the the LC-3 ISA. Does it remind you of anything? Like a ﬂ owchart, it has relatively few arcs leaving each state—usually only one or two. What if we treat the state diagram as a program? We can use a small m emory to hold microinstructions (another name for control words) and use the FSM state number as the memory address. Without support for interrupts or privilege, and with the datapath extension for JS R mentioned for hardwired control, the LC-3 state machine requiresfewer than 32 states. The datapath has 25 control signals, but we need one more for the datapath extension for JSR. We thus start with 5-bit stat e number (in a register) and a 25×26 bit memory, which we call our control ROM (read-only memory) to distin guish it from the big, slow, von Neumann memory. Each cycle, the microprogrammed control unit applies the FSM state number to the control ROM (no IR bits, just the state number), gets back a set of control signals, and uses them to drive the datapath. To write our microprogram, we need to calculate the control signals for each microinstruction and put them in the control ROM, but we also need to have a way to decide which microinstruction should execute next. We call the latter problem sequencing or microsequencing. Notice that most of the time there’s no choice: we have only onenext microinstruction. One simple approach is then to add the address (t he 5-bitstateID)ofthenextmicroinstructiontothecontrolROM.I nstead of 26 bits per FSM state, we now have 31 bits per FSM state. Sometimes we do need to have two possible next states. When waiting for memory(the von Neumann memory, not the controlROM) to ﬁn ish an access, for example, we want our FSM to stay in the same state, then move to the next state when the access completes. Let’s add a second address to each microinstruction, and add a branch cont rol signal for the microprogram to decide whether we should use the ﬁr st address or the second for the next microinstruction. This design, using a 25×36 bit memory (1,152 bits total), appears to the right.5 5control signals265register (5 bits)FSM state ID55 5 microprogram branch control 32 x 36−bit memoryADDR DATA0 1 The microprogram branch control signal is a Boolean logic ex- pression based on the memory ready signal R and IR[11]. We can implement it with a state ID comparison and a mux, as shown to the right. For the branch instruction execution state, the mux selects input 1, BEN. For all other states, the mux selects input 0, R. When an FSM state has only a single next state, we set both IDs in the control ROM to the ID for that state, so the value of R has no eﬀect.= branch state? microprogram branch control5state IDR BEN 0 1'}"
"4.1 Control Unit Design 141 What’s missing? Decode! We do have one FSM state in which we need to be able to branch to one of sixteen possible next states, on e for each opcode. Let’s just add another mux and choose the stat e IDs for starting to process each opcode in an easy way, as shown to th e right with extensions highlighted in blue. The textbook assigns the ﬁrst state for processing each opcode the IDs IR[15:12] preceed ed by two 0s (the textbook’s design requires 6-bit state IDs). We adopt the same strategy. For example, the ﬁrst FSM state for processing a n ADD is 00001, and the ﬁrst state for a TRAP is 01111. In each case, the opcode encoding speciﬁes the last four bits. Now we can pick the remaining state IDs arbitrarily and ﬁll in the control ROM with control signals that implement each state’s RTL and the two possible next states. Transitions from the decode sta te are handled by the extra mux. The microprogrammed control unit implementation in Appendix C of Patt and Patel is similar to the one that we have developed here, but is slightly more complex so that it can handle interrupts and privilege.55 5 5register (5 bits)FSM state ID 5 5control signals2632 x 36−bit memorymicroprogram branch control 0 IR[15:12] = decode state?55 ADDR DATA0 1 0 1","{'page_number': 146, 'textbook_name': 'ECE-120-student-notes', 'text': '4.1 Control Unit Design 141 What’s missing? Decode! We do have one FSM state in which we need to be able to branch to one of sixteen possible next states, on e for each opcode. Let’s just add another mux and choose the stat e IDs for starting to process each opcode in an easy way, as shown to th e right with extensions highlighted in blue. The textbook assigns the ﬁrst state for processing each opcode the IDs IR[15:12] preceed ed by two 0s (the textbook’s design requires 6-bit state IDs). We adopt the same strategy. For example, the ﬁrst FSM state for processing a n ADD is 00001, and the ﬁrst state for a TRAP is 01111. In each case, the opcode encoding speciﬁes the last four bits. Now we can pick the remaining state IDs arbitrarily and ﬁll in the control ROM with control signals that implement each state’s RTL and the two possible next states. Transitions from the decode sta te are handled by the extra mux. The microprogrammed control unit implementation in Appendix C of Patt and Patel is similar to the one that we have developed here, but is slightly more complex so that it can handle interrupts and privilege.55 5 5register (5 bits)FSM state ID 5 5control signals2632 x 36−bit memorymicroprogram branch control 0 IR[15:12] = decode state?55 ADDR DATA0 1 0 1'}"
"142  ECE120: Introduction to Computer Engineering Notes Set 4.2 Redundancy and Coding This set of notes introduces the idea of using sparsely populated re presentationsto protect against accidental changes to bits. Today, such representations are used in almost e very type of storage system, from bits on a chip to main memory to disk to archival tapes. We begin our discussio n with examples of representations in which some bit patterns have no meaning, then consider what happ ens when a bit changes accidentally. We next outline a general scheme that allows a digital system to dete ct a single bit error. Building on the mechanism underlying this scheme, we describe a distance metric tha t enables us to think more broadly about both detecting and correcting such errors, and then show a general approach that allows correction of a single bit error. We leave discussion of more sophisticated schemes to classes on coding and information theory. 4.2.1 Sparse Representations Representations used by computers must avoid ambiguity: a single b it pattern in a representation cannot be used to represent more than one value. However, the converse n eed not be true. A representation can have several bit patterns representing the same value, and not all bit p atterns in a representation need be used to represent values. Let’s consider a few example of representations with unused patte rns. Historically, one common class of representationsofthistypewasthoseusedtorepresentindividu aldecimaldigits. Weexaminethreeexamples from this class. The ﬁrst is Binary-coded Decimal (BCD), in which decimal digits are en coded individually using their repre- sentations in the unsigned (binary) representation. Since we have 10 decimal digits, we need 10 patterns, and four bits for each digit. But four bits allow 24= 16 bit patterns. In BCD, the patterns 1010 ,1011,...,1111 are unused. It is important to note that BCD is not the same as the u nsigned representation. The decimal number 732, for example, requires 12 bits when encoded as BCD: 01 11 0011 0010. When written using a 12-bit unsigned representation, 732 is written 001011011100. Op erations on BCD values were implemented in early processors, including the 8086, and are thus still available in t he x86 instruction set architecture today! The second example is an Excess-3 code, in which each decimal digit dis rep- resented by the pattern corresponding to the 4-bit unsigned pat tern ford+3. For example, the digit 4 is represented as 0111, and the digit 7 is repr esented as 1010. The Excess-3 code has some attractive aspects when us ing simple hardware. For example, we can use a 4-bit binary adder to add two d igitsc anddrepresented in the Excess-3 code, and the carry out signal prod uced by the adder is the same as the carry out for the decimal addition, sinc ec+d≥10 is equivalent to ( c+3)+(d+3)≥16. The third example of decimal digit representations is a 2-out-of-5 c ode. In such a code, ﬁve bits are used to encode each digit. Only patterns w ith exactly two 1s are used","{'page_number': 147, 'textbook_name': 'ECE-120-student-notes', 'text': 'The third example of decimal digit representations is a 2-out-of-5 c ode. In such a code, ﬁve bits are used to encode each digit. Only patterns w ith exactly two 1s are used. There are exactly ten such patterns, an d an example representation is shown to the right (more than one assignment of values to patterns has been used in real systems).a 2-out-of-5 digitrepresentation 1 00011 2 00101 3 00110 4 01001 5 01010 6 01100 7 10001 8 10010 9 10100 0 11000 4.2.2 Error Detection Errors in digital systems can occur for many reasons, ranging fro m cosmic ray strikes to defects in chip fabrication to errors in the design of the digital system. As a simple m odel, we assume that an error takes the form of changes to some number of bits. In other words, a bit t hat should have the value 0 instead has the value 1, or a bit that should have the value 1 instead has the value 0. Such an error is called a bit error .'}"
"The third example of decimal digit representations is a 2-out-of-5 c ode. In such a code, ﬁve bits are used to encode each digit. Only patterns w ith exactly two 1s are used. There are exactly ten such patterns, an d an example representation is shown to the right (more than one assignment of values to patterns has been used in real systems).a 2-out-of-5 digitrepresentation 1 00011 2 00101 3 00110 4 01001 5 01010 6 01100 7 10001 8 10010 9 10100 0 11000 4.2.2 Error Detection Errors in digital systems can occur for many reasons, ranging fro m cosmic ray strikes to defects in chip fabrication to errors in the design of the digital system. As a simple m odel, we assume that an error takes the form of changes to some number of bits. In other words, a bit t hat should have the value 0 instead has the value 1, or a bit that should have the value 1 instead has the value 0. Such an error is called a bit error .","{'page_number': 147, 'textbook_name': 'ECE-120-student-notes', 'text': 'The third example of decimal digit representations is a 2-out-of-5 c ode. In such a code, ﬁve bits are used to encode each digit. Only patterns w ith exactly two 1s are used. There are exactly ten such patterns, an d an example representation is shown to the right (more than one assignment of values to patterns has been used in real systems).a 2-out-of-5 digitrepresentation 1 00011 2 00101 3 00110 4 01001 5 01010 6 01100 7 10001 8 10010 9 10100 0 11000 4.2.2 Error Detection Errors in digital systems can occur for many reasons, ranging fro m cosmic ray strikes to defects in chip fabrication to errors in the design of the digital system. As a simple m odel, we assume that an error takes the form of changes to some number of bits. In other words, a bit t hat should have the value 0 instead has the value 1, or a bit that should have the value 1 instead has the value 0. Such an error is called a bit error .'}"
"4.2 Redundancy and Coding 143 Digital systems can be designed with or without tolerance to errors . When an error occurs, no notiﬁcation nor identiﬁcation of the error is provided. Rather, if error toleran ce is needed, the system must be designed to be able to recognize and identify errors automatically. Often, we assume that each of the bits may be in error independently of all of the others, each with some low probab ility. With such an assumption, multiple bit errors are much less likely than single bit errors, and we can focus on designs that tolerate a single bit error. When a bit error occurs, however, we must assume that it c an happen to any of the bits. The use of many patterns to represent a smaller number of values, as is the case in a 2-out-of-5 code, enables a system to perform error detection . Let’s consider what happens when a value represented using a 2-out-of-5 code is subjected to a single bit error. Imagine that w e have the digit 7. In the table on the previous page, notice that the digit 7 is represented with the patte rn 10001. As we mentioned, we must assume that the bit error can occur in any of the ﬁve bits, thus we have ﬁve possible bit patterns after the error occurs. If the error occur s in the ﬁrst bit, we have the pattern 00001. If the error occurs in the second bit, we have the pattern 11001. Th e complete set of possible error patterns is 00001, 11001, 10101, 10011, and 10000. Notice that none of the possible error patterns has exactly two 1s , and thus none of them is a meaningful pattern in our 2-out-of-5 code. In other words, whenever a digit al system represents the digit 7 and a single bit error occurs, the system will be able to detect that an error ha s occurred. What if the system needs to represent a diﬀerent digit? Regardless of which digit is represented, the pattern with no errors has exactly two 1s, by the deﬁnition of our represen tation. If we then ﬂip one of the ﬁve bits by subjecting it to a bit error, the resulting error pattern has eith er one 1 (if the bit error changes a 1 to a 0) or three 1s (if the bit error changes a 0 to a 1). In other words , regardless of which digit is represented, and regardless of which bit has an error, the resulting error patte rn never has a meaning in the 2-out-of-5 code. So this representation enables a digital system to detect an y single bit error! 4.2.3 Parity The ability to detect any single bit error is certainly useful. However, so far we have only shown how to protect ourselves when we want to represent decimal digits. Do we need to develop a separate error-tolerant representation for every type of information that we might want t o represent? Or can we instead come up with a more general approach? The answer to the second question is yes: we can, in fact, systematically transform any representation into a representation that allows d etection of a single bit error. The key to this transformation is the idea of parity","{'page_number': 148, 'textbook_name': 'ECE-120-student-notes', 'text': 'However, so far we have only shown how to protect ourselves when we want to represent decimal digits. Do we need to develop a separate error-tolerant representation for every type of information that we might want t o represent? Or can we instead come up with a more general approach? The answer to the second question is yes: we can, in fact, systematically transform any representation into a representation that allows d etection of a single bit error. The key to this transformation is the idea of parity. Consider an arbitrary representation for some type of information. For each pattern used in the representation, we can count the number of 1s. The resulting count is either odd or even. By adding an extra bit—called a parity bit —to the represen- tation, and selecting the parity bit’s value appropriately for each bit pattern, we can ensure that the count of 1s is odd (called odd parity ) or even (called even parity ) for all values represented. The idea is il- lustrated in the table to the right for the 3-bit unsigned representation. The parity bits are shown in bold.value 3-bit number with odd with even represented unsigned of 1s parity parity 0 000 0 0001 0000 1 001 1 0010 0011 2 010 1 0100 0101 3 011 2 0111 0110 4 100 1 1000 1001 5 101 2 1011 1010 6 110 2 1101 1100 7 111 3 1110 1111 Either approach to selecting the parity bits ensures that any single bit error can be detected. For example, if we choose to use odd parity, a single bit error changes either a 0 int o a 1 or a 1 into a 0. The number of 1s in the resulting error pattern thus diﬀers by exactly one from the original pattern, and the parity of the error pattern is even. But all valid patterns have odd parity, s o any single bit error can be detected by simply counting the number of 1s.'}"
"However, so far we have only shown how to protect ourselves when we want to represent decimal digits. Do we need to develop a separate error-tolerant representation for every type of information that we might want t o represent? Or can we instead come up with a more general approach? The answer to the second question is yes: we can, in fact, systematically transform any representation into a representation that allows d etection of a single bit error. The key to this transformation is the idea of parity. Consider an arbitrary representation for some type of information. For each pattern used in the representation, we can count the number of 1s. The resulting count is either odd or even. By adding an extra bit—called a parity bit —to the represen- tation, and selecting the parity bit’s value appropriately for each bit pattern, we can ensure that the count of 1s is odd (called odd parity ) or even (called even parity ) for all values represented. The idea is il- lustrated in the table to the right for the 3-bit unsigned representation. The parity bits are shown in bold.value 3-bit number with odd with even represented unsigned of 1s parity parity 0 000 0 0001 0000 1 001 1 0010 0011 2 010 1 0100 0101 3 011 2 0111 0110 4 100 1 1000 1001 5 101 2 1011 1010 6 110 2 1101 1100 7 111 3 1110 1111 Either approach to selecting the parity bits ensures that any single bit error can be detected. For example, if we choose to use odd parity, a single bit error changes either a 0 int o a 1 or a 1 into a 0. The number of 1s in the resulting error pattern thus diﬀers by exactly one from the original pattern, and the parity of the error pattern is even. But all valid patterns have odd parity, s o any single bit error can be detected by simply counting the number of 1s.","{'page_number': 148, 'textbook_name': 'ECE-120-student-notes', 'text': 'However, so far we have only shown how to protect ourselves when we want to represent decimal digits. Do we need to develop a separate error-tolerant representation for every type of information that we might want t o represent? Or can we instead come up with a more general approach? The answer to the second question is yes: we can, in fact, systematically transform any representation into a representation that allows d etection of a single bit error. The key to this transformation is the idea of parity. Consider an arbitrary representation for some type of information. For each pattern used in the representation, we can count the number of 1s. The resulting count is either odd or even. By adding an extra bit—called a parity bit —to the represen- tation, and selecting the parity bit’s value appropriately for each bit pattern, we can ensure that the count of 1s is odd (called odd parity ) or even (called even parity ) for all values represented. The idea is il- lustrated in the table to the right for the 3-bit unsigned representation. The parity bits are shown in bold.value 3-bit number with odd with even represented unsigned of 1s parity parity 0 000 0 0001 0000 1 001 1 0010 0011 2 010 1 0100 0101 3 011 2 0111 0110 4 100 1 1000 1001 5 101 2 1011 1010 6 110 2 1101 1100 7 111 3 1110 1111 Either approach to selecting the parity bits ensures that any single bit error can be detected. For example, if we choose to use odd parity, a single bit error changes either a 0 int o a 1 or a 1 into a 0. The number of 1s in the resulting error pattern thus diﬀers by exactly one from the original pattern, and the parity of the error pattern is even. But all valid patterns have odd parity, s o any single bit error can be detected by simply counting the number of 1s.'}"
"144  4.2.4 Hamming Distance Next, let’s think about how we might use representations—wemight a lsothink of them as codes—to protect a system against multiple bit errors. As we have seen with parity, one strategy that we can use to provide such errortolerance is the use of representationsin which only som e of the patterns actually representvalues. Let’s call such patterns code words . In other words, the code words in a representation are those pa tterns that correspond to real values of information. Other patterns in the representation have no meaning. As a tool to help us understand error tolerance, let’s deﬁne a meas ure of the distance between code words in a representation. Given two code words XandY, we can calculate the number NX,Yof bits that must change to transform XintoY. Such a calculation merely requires that we compare the patterns b it by bit and count the number of places in which they diﬀer. Notice that this r elationship is symmetric: the same number of changes are required to transform YintoX, soNY,X=NX,Y. We refer to this number NX,Y as theHamming distance between code word Xand code word Y. The metric is named after Richard Hamming, a computing pioneer and an alumnus of the UIUC Math depar tment. The Hamming distance between two code words tells us how many bit er rors are necessary in order for a digital system to mistake one code word for the other. Given a rep resentation, we can calculate the minimum Hamming distance between any pair of code words used by the representation. The result is called theHamming distance of the representation , and representsthe minimum of bit errorsthat must occur before a system might fail to detect errors in a stored value. The Hamming distance for nearly all of the representations that we introduced in earlier sections is 1. Since more than half of the patterns (and often all of the patterns!) co rrespond to meaningful values, some pairs of code words must diﬀer in only one bit, and these representations cannot tolerate any errors. For example, the decimal value 42 is stored as 101010 using a 6-bit unsigned repre sentation, but any bit error in that pattern produces another valid pattern corresponding to one of the following decimal numbers: 10, 58, 34, 46, 40, 43. Note that the Hamming distance between any two patte rns is not necessarily 1. Rather, the Hamming distance of the unsigned representation, which correspo nds to the minimum between any pair of valid patterns, is 1. In contrast, the Hamming distance of the 2-out-of-5 code that w e discussed earlier is 2. Similarly, the Hamming distance of any representation extended with a parity bit is at least 2. Now let’s think about the problem slightly diﬀerently. Given a particular representation, how many bit errors can we detect in values using that representation? A representation with Hamming distance dcan detect up to d−1bit errors. To understand this claim, start by selecting a code word from the re presentation and changing up to d−1 of the bits","{'page_number': 149, 'textbook_name': 'ECE-120-student-notes', 'text': 'Similarly, the Hamming distance of any representation extended with a parity bit is at least 2. Now let’s think about the problem slightly diﬀerently. Given a particular representation, how many bit errors can we detect in values using that representation? A representation with Hamming distance dcan detect up to d−1bit errors. To understand this claim, start by selecting a code word from the re presentation and changing up to d−1 of the bits. No matter how one chooses to change the bits, these changes cannot result in another code word, since we know that any other code wor d has to require at least dchanges from our original code word, by the deﬁnition of the representation’s Ha mming distance. A digital system using the representation can thus detect up to d−1 errors. However, if dor more errors occur, the system might sometimes fail to detect any error in the stored value. 4.2.5 Error Correction Detection of errors is important, but may sometimes not be enough . What can a digital system do when it detects an error? In some cases, the system may be able to ﬁnd th e original value elsewhere, or may be able to re-compute the value from other values. In other cases, the v alue is simply lost, and the digital system may need to reboot or even shut down until a human can attend to it . Many real systems cannot aﬀord such a luxury. Life-critical systems such as medical equipment and airpla nes should not turn themselves oﬀ and wait for a human’s attention. Space vehicles face a similar dilemma, sinc e no human may be able to reach them. Can we use a strategy similar to the one that we have developed for e rror detection in order to try to perform error correction , recovering the original value? Yes, but the overhead—the extra bits that we need to provide such functionality—is higher.'}"
"Similarly, the Hamming distance of any representation extended with a parity bit is at least 2. Now let’s think about the problem slightly diﬀerently. Given a particular representation, how many bit errors can we detect in values using that representation? A representation with Hamming distance dcan detect up to d−1bit errors. To understand this claim, start by selecting a code word from the re presentation and changing up to d−1 of the bits. No matter how one chooses to change the bits, these changes cannot result in another code word, since we know that any other code wor d has to require at least dchanges from our original code word, by the deﬁnition of the representation’s Ha mming distance. A digital system using the representation can thus detect up to d−1 errors. However, if dor more errors occur, the system might sometimes fail to detect any error in the stored value. 4.2.5 Error Correction Detection of errors is important, but may sometimes not be enough . What can a digital system do when it detects an error? In some cases, the system may be able to ﬁnd th e original value elsewhere, or may be able to re-compute the value from other values. In other cases, the v alue is simply lost, and the digital system may need to reboot or even shut down until a human can attend to it . Many real systems cannot aﬀord such a luxury. Life-critical systems such as medical equipment and airpla nes should not turn themselves oﬀ and wait for a human’s attention. Space vehicles face a similar dilemma, sinc e no human may be able to reach them. Can we use a strategy similar to the one that we have developed for e rror detection in order to try to perform error correction , recovering the original value? Yes, but the overhead—the extra bits that we need to provide such functionality—is higher.","{'page_number': 149, 'textbook_name': 'ECE-120-student-notes', 'text': 'Similarly, the Hamming distance of any representation extended with a parity bit is at least 2. Now let’s think about the problem slightly diﬀerently. Given a particular representation, how many bit errors can we detect in values using that representation? A representation with Hamming distance dcan detect up to d−1bit errors. To understand this claim, start by selecting a code word from the re presentation and changing up to d−1 of the bits. No matter how one chooses to change the bits, these changes cannot result in another code word, since we know that any other code wor d has to require at least dchanges from our original code word, by the deﬁnition of the representation’s Ha mming distance. A digital system using the representation can thus detect up to d−1 errors. However, if dor more errors occur, the system might sometimes fail to detect any error in the stored value. 4.2.5 Error Correction Detection of errors is important, but may sometimes not be enough . What can a digital system do when it detects an error? In some cases, the system may be able to ﬁnd th e original value elsewhere, or may be able to re-compute the value from other values. In other cases, the v alue is simply lost, and the digital system may need to reboot or even shut down until a human can attend to it . Many real systems cannot aﬀord such a luxury. Life-critical systems such as medical equipment and airpla nes should not turn themselves oﬀ and wait for a human’s attention. Space vehicles face a similar dilemma, sinc e no human may be able to reach them. Can we use a strategy similar to the one that we have developed for e rror detection in order to try to perform error correction , recovering the original value? Yes, but the overhead—the extra bits that we need to provide such functionality—is higher.'}"
"4.2 Redundancy and Coding 145 Let’s start by thinking about a code with Hamming distance 2, such as 4-bit 2’s complement with odd parity. We know that such a code can detect one bit error. Can it correct s uch a bit error, too? Imagine that a system has stored the decimal value 6 using the patt ern 01101, where the last bit is the odd parity bit. A bit error occurs, changing the stored pattern to 011 11, which is not a valid pattern, since it has an even number of 1s. But can the system know that the original va lue stored was 6? No, it cannot. The original value may also have been 7, in which case the original pattern was 0111 0, and the bit error occurred in the ﬁnal bit. The original value may also have been -1, 3, or 5. The s ystem has no way of resolving this ambiguity. The same problem arises if a digital system uses a code with Hamming distance dto detect up tod−1 errors. Errorcorrection is possible, however, if we assume that fewer bit e rrors occur (or if we instead use a representation with a larger Hamming distance ). As a simple example, let’s create a representation for the numbers 0 th rough 3 by making three copies of the 2-bit unsigned representation, as sh own to the right. The Hamming distance of the resulting code is 3, so any two bit e rrors can be detected. However, this code also enables us to correct a s ingle bit error. Intuitively, think of the three copies as voting on the right a nswer.value three-copy represented code 0 000000 1 010101 2 101010 3 111111 Since a single bit error can only corrupt one copy, a majority vote alw ays gives the right answer! Tripling the number of bits needed in a representation is not a good general strategy, however. Notice also that “correcting” a pattern with two bit errors can produce the wrong result. Let’s think about the problem in terms of Hamming distance. Assume t hat we use a code with Hamming distance dand imagine that up to kbit errors aﬀect a stored value. The resulting pattern then falls wit hin a neighborhood of distance kfrom the original code word. This neighborhood contains all bit patt erns within Hamming distance kof the original pattern. We can deﬁne such a neighborhood around each code word. Now, since dbit errors are needed to transform a code word into any other cod e word, these neighborhoods aredisjointsolongas2 k≤d−1. Inotherwords,iftheinequalityholds, anybit patterninthe repr esentation can be in at most one code word’s neighborhood. The digital system c an then correct the errors by selecting the unique value identiﬁed by the associated neighborhood. Note th at patterns encountered as a result of up tokbit errors always fall within the original code word’s neighborhood; t he inequality ensures that the neighborhood identiﬁed in this way is unique. We can manipulate the ineq uality to express the number of errors kthat can be corrected in terms of the Hamming distance dof the code","{'page_number': 150, 'textbook_name': 'ECE-120-student-notes', 'text': 'The digital system c an then correct the errors by selecting the unique value identiﬁed by the associated neighborhood. Note th at patterns encountered as a result of up tokbit errors always fall within the original code word’s neighborhood; t he inequality ensures that the neighborhood identiﬁed in this way is unique. We can manipulate the ineq uality to express the number of errors kthat can be corrected in terms of the Hamming distance dof the code. A code with Hamming distance dallows up to⌊d−1 2⌋errors to be corrected , where⌊x⌋represents the integer ﬂoor function on x, or rounding xdown to the nearest integer. 4.2.6 Hamming Codes Hamming also developed a general and eﬃcient approach for extend ing an arbitrary representation to allow correction of a single bit error. The approach yields codes with Hamm ing distance 3. To understand how aHamming code works, think of the bits in the representation as being numbered st arting from 1. For example, if we have seven bits in the code, we might write a bit pattern Xasx7x6x5x4x3x2x1. The bits with indices that are powers of two are parity check bits. Th ese include x1,x2,x4,x8, and so forth. The remaining bits can be used to hold data. For example, we could use a 7-bit Hamming code and map the bits from a 4-bit unsigned representation into bits x7,x6,x5, andx3. Notice that Hamming codes are not so useful for small numbers of bits, but require only logarithmic ove rhead for large numbers of bits. That is, in anN-bit Hamming code, only ⌈log2(N+1)⌉bits are used for parity checks. How are the parity checks deﬁned? Each parity bit is used to provide even parity for those bits with indices for which the index, when written in binary, includes a 1 in the single pos ition in which the parity bit’s index contains a 1. The x1bit, for example, provides even parity on all bits with odd indices. The x2bit provides even parity on x2,x3,x6,x7,x10, and so forth. In a 7-bit Hamming code, for example, x1is chosen so that it has even parity together with x3,x5, andx7. Similarly, x2is chosen so that it has even parity together with x3,x6, andx7. Finally, x4is chosen so that it has even parity together with x5,x6, andx7.'}"
"The digital system c an then correct the errors by selecting the unique value identiﬁed by the associated neighborhood. Note th at patterns encountered as a result of up tokbit errors always fall within the original code word’s neighborhood; t he inequality ensures that the neighborhood identiﬁed in this way is unique. We can manipulate the ineq uality to express the number of errors kthat can be corrected in terms of the Hamming distance dof the code. A code with Hamming distance dallows up to⌊d−1 2⌋errors to be corrected , where⌊x⌋represents the integer ﬂoor function on x, or rounding xdown to the nearest integer. 4.2.6 Hamming Codes Hamming also developed a general and eﬃcient approach for extend ing an arbitrary representation to allow correction of a single bit error. The approach yields codes with Hamm ing distance 3. To understand how aHamming code works, think of the bits in the representation as being numbered st arting from 1. For example, if we have seven bits in the code, we might write a bit pattern Xasx7x6x5x4x3x2x1. The bits with indices that are powers of two are parity check bits. Th ese include x1,x2,x4,x8, and so forth. The remaining bits can be used to hold data. For example, we could use a 7-bit Hamming code and map the bits from a 4-bit unsigned representation into bits x7,x6,x5, andx3. Notice that Hamming codes are not so useful for small numbers of bits, but require only logarithmic ove rhead for large numbers of bits. That is, in anN-bit Hamming code, only ⌈log2(N+1)⌉bits are used for parity checks. How are the parity checks deﬁned? Each parity bit is used to provide even parity for those bits with indices for which the index, when written in binary, includes a 1 in the single pos ition in which the parity bit’s index contains a 1. The x1bit, for example, provides even parity on all bits with odd indices. The x2bit provides even parity on x2,x3,x6,x7,x10, and so forth. In a 7-bit Hamming code, for example, x1is chosen so that it has even parity together with x3,x5, andx7. Similarly, x2is chosen so that it has even parity together with x3,x6, andx7. Finally, x4is chosen so that it has even parity together with x5,x6, andx7.","{'page_number': 150, 'textbook_name': 'ECE-120-student-notes', 'text': 'The digital system c an then correct the errors by selecting the unique value identiﬁed by the associated neighborhood. Note th at patterns encountered as a result of up tokbit errors always fall within the original code word’s neighborhood; t he inequality ensures that the neighborhood identiﬁed in this way is unique. We can manipulate the ineq uality to express the number of errors kthat can be corrected in terms of the Hamming distance dof the code. A code with Hamming distance dallows up to⌊d−1 2⌋errors to be corrected , where⌊x⌋represents the integer ﬂoor function on x, or rounding xdown to the nearest integer. 4.2.6 Hamming Codes Hamming also developed a general and eﬃcient approach for extend ing an arbitrary representation to allow correction of a single bit error. The approach yields codes with Hamm ing distance 3. To understand how aHamming code works, think of the bits in the representation as being numbered st arting from 1. For example, if we have seven bits in the code, we might write a bit pattern Xasx7x6x5x4x3x2x1. The bits with indices that are powers of two are parity check bits. Th ese include x1,x2,x4,x8, and so forth. The remaining bits can be used to hold data. For example, we could use a 7-bit Hamming code and map the bits from a 4-bit unsigned representation into bits x7,x6,x5, andx3. Notice that Hamming codes are not so useful for small numbers of bits, but require only logarithmic ove rhead for large numbers of bits. That is, in anN-bit Hamming code, only ⌈log2(N+1)⌉bits are used for parity checks. How are the parity checks deﬁned? Each parity bit is used to provide even parity for those bits with indices for which the index, when written in binary, includes a 1 in the single pos ition in which the parity bit’s index contains a 1. The x1bit, for example, provides even parity on all bits with odd indices. The x2bit provides even parity on x2,x3,x6,x7,x10, and so forth. In a 7-bit Hamming code, for example, x1is chosen so that it has even parity together with x3,x5, andx7. Similarly, x2is chosen so that it has even parity together with x3,x6, andx7. Finally, x4is chosen so that it has even parity together with x5,x6, andx7.'}"
"146  The table to the right shows the result of em- bedding a 4-bit unsigned representation into a 7-bit Hamming code. A Hamming code provides a convenient way to identify which bit should be corrected when a single bit error occurs. Notice that each bit is protected by a unique subset of the parity bits correspondingto the binary form of the bit’s in- dex. Bit x6, for example, is protected by bits x4 andx2, because the number 6 is written 110 in binary. Ifabit isaﬀected byanerror,the parity bitsthat registertheerrorarethosecorrespond- ing to 1s in the binary number of the index. So if we calculate check bits as 1 to represent an error (odd parity) and 0 to represent no error (even parity), then concatenate those bits into a binary number, we obtain the binary value of the index of the single bit aﬀected by an error (or the number 0 if no error has occurred).4-bit 7-bit value unsigned Hamming represented (x7x6x5x3)x4x2x1code 0 0000 0000000000 1 0001 0110000111 2 0010 1010011001 3 0011 1100011110 4 0100 1100101010 5 0101 1010101101 6 0110 0110110011 7 0111 0000110100 8 1000 1111001011 9 1001 1001001100 10 1010 0101010010 11 1011 0011010101 12 1100 0011100001 13 1101 0101100110 14 1110 1001111000 15 1111 1111111111 Let’s do a couple of examples based on the pattern for the decimal n umber 9, 1001100. First, assume that no error occurs. We calculate check bit c4by checking whether x4,x5,x6, andx7together have even parity. Since no error occurred, they do, so c4= 0. Similarly, for c2we consider x2,x3,x6, andx7. These also have even parity, so c2= 0. Finally, for c1, we consider x1,x3,x5, andx7. As with the others, these together have even parity, so c1= 0. Writing c4c2c1, we obtain 000, and conclude that no error has occurred. Next assume that bit 3 has an error, giving us the pattern 1001000 . In this case, we have again that c4= 0, but the bits corresponding to both c2andc1have odd parity, so c2 = 1 and c1= 1. Now when we write the check bits c4c2c1, we obtain 011, and we are able to recognize that bit 3 has been chan ged. A Hamming code can only correct one bit error, however. If two bit e rrors occur, correction will produce the wrong answer. Let’s imagine that both bits 3 and 5 have been ﬂipp ed in our example pattern for the decimal number 9, producing the pattern 1011000. Calculating the check bits as before and writing them asc4c2c1, we obtain 110, which leads us to incorrectly conclude that bit 6 has b een ﬂipped","{'page_number': 151, 'textbook_name': 'ECE-120-student-notes', 'text': 'A Hamming code can only correct one bit error, however. If two bit e rrors occur, correction will produce the wrong answer. Let’s imagine that both bits 3 and 5 have been ﬂipp ed in our example pattern for the decimal number 9, producing the pattern 1011000. Calculating the check bits as before and writing them asc4c2c1, we obtain 110, which leads us to incorrectly conclude that bit 6 has b een ﬂipped. As a result, we “correct” the pattern to 1111000, which represents the decima l number 14. 4.2.7 SEC-DED Codes We now consider one ﬁnal extension of Hamming codes to enable a sys tem to perform single error correction while also detecting any two bit errors. Such codes are known as Single Error Correction, Double Error Detection (SEC-DED) codes. Creating such a code from a Hamming code is trivial: add a parit y bit covering the entire Hamming code. The extra parity bit increases th e Hamming distance to 4. A Hamming distance of 4 still allows only single bit error correction, but avoids th e problem of Hamming distance 3 codes when two bit errors occur, since patterns at Hamming distance 2 fr om a valid code word cannot be within distance 1 of another code word, and thus cannot be “corrected ” to the wrong result. In fact, one can add a parity bit to any representation with an odd H amming distance to create a new representation with Hamming distance one greater than the origina l representation. To proof this convenient fact, begin with a representation with Hamming distance d, wheredis odd. If we choose two code words from the representation, and their Hamming distance is already gre ater than d, their distance in the new representation will also be greater than d. Adding a parity bit cannot decrease the distance. On the other hand, if the two code words are exactly distance dapart, they must have opposite parity, since they diﬀer by an odd number of bits. Thus the new parity bit will be a 0 for one of t he code words and a 1 for the other, increasing the Hamming distance to d+ 1 in the new representation. Since all pairs of code words have Hamming distance of at least d+1, the new representation also has Hamming distance d+1.'}"
"A Hamming code can only correct one bit error, however. If two bit e rrors occur, correction will produce the wrong answer. Let’s imagine that both bits 3 and 5 have been ﬂipp ed in our example pattern for the decimal number 9, producing the pattern 1011000. Calculating the check bits as before and writing them asc4c2c1, we obtain 110, which leads us to incorrectly conclude that bit 6 has b een ﬂipped. As a result, we “correct” the pattern to 1111000, which represents the decima l number 14. 4.2.7 SEC-DED Codes We now consider one ﬁnal extension of Hamming codes to enable a sys tem to perform single error correction while also detecting any two bit errors. Such codes are known as Single Error Correction, Double Error Detection (SEC-DED) codes. Creating such a code from a Hamming code is trivial: add a parit y bit covering the entire Hamming code. The extra parity bit increases th e Hamming distance to 4. A Hamming distance of 4 still allows only single bit error correction, but avoids th e problem of Hamming distance 3 codes when two bit errors occur, since patterns at Hamming distance 2 fr om a valid code word cannot be within distance 1 of another code word, and thus cannot be “corrected ” to the wrong result. In fact, one can add a parity bit to any representation with an odd H amming distance to create a new representation with Hamming distance one greater than the origina l representation. To proof this convenient fact, begin with a representation with Hamming distance d, wheredis odd. If we choose two code words from the representation, and their Hamming distance is already gre ater than d, their distance in the new representation will also be greater than d. Adding a parity bit cannot decrease the distance. On the other hand, if the two code words are exactly distance dapart, they must have opposite parity, since they diﬀer by an odd number of bits. Thus the new parity bit will be a 0 for one of t he code words and a 1 for the other, increasing the Hamming distance to d+ 1 in the new representation. Since all pairs of code words have Hamming distance of at least d+1, the new representation also has Hamming distance d+1.","{'page_number': 151, 'textbook_name': 'ECE-120-student-notes', 'text': 'A Hamming code can only correct one bit error, however. If two bit e rrors occur, correction will produce the wrong answer. Let’s imagine that both bits 3 and 5 have been ﬂipp ed in our example pattern for the decimal number 9, producing the pattern 1011000. Calculating the check bits as before and writing them asc4c2c1, we obtain 110, which leads us to incorrectly conclude that bit 6 has b een ﬂipped. As a result, we “correct” the pattern to 1111000, which represents the decima l number 14. 4.2.7 SEC-DED Codes We now consider one ﬁnal extension of Hamming codes to enable a sys tem to perform single error correction while also detecting any two bit errors. Such codes are known as Single Error Correction, Double Error Detection (SEC-DED) codes. Creating such a code from a Hamming code is trivial: add a parit y bit covering the entire Hamming code. The extra parity bit increases th e Hamming distance to 4. A Hamming distance of 4 still allows only single bit error correction, but avoids th e problem of Hamming distance 3 codes when two bit errors occur, since patterns at Hamming distance 2 fr om a valid code word cannot be within distance 1 of another code word, and thus cannot be “corrected ” to the wrong result. In fact, one can add a parity bit to any representation with an odd H amming distance to create a new representation with Hamming distance one greater than the origina l representation. To proof this convenient fact, begin with a representation with Hamming distance d, wheredis odd. If we choose two code words from the representation, and their Hamming distance is already gre ater than d, their distance in the new representation will also be greater than d. Adding a parity bit cannot decrease the distance. On the other hand, if the two code words are exactly distance dapart, they must have opposite parity, since they diﬀer by an odd number of bits. Thus the new parity bit will be a 0 for one of t he code words and a 1 for the other, increasing the Hamming distance to d+ 1 in the new representation. Since all pairs of code words have Hamming distance of at least d+1, the new representation also has Hamming distance d+1.'}"
"4.3 Instruction Set Architecture* 147 ECE120: Introduction to Computer Engineering Notes Set 4.3 Instruction Set Architecture* This set of notes discusses tradeoﬀs and design elements of instru ction set architectures (ISAs). The material is beyond the scope of our class, and is provided purely for yo ur interest. Those who ﬁnd these topics interesting may also want to read the ECE391 notes, which describe similar material with a focus on the x86 ISA. As you know, the ISA deﬁnes the interface between software and hardware, abstracting the capabilities of a computer’s datapath and standardizing the format of instructio ns to utilize those capabilities. Successful ISAs are rarely discarded, as success implies the existence of large amounts of software built to use the ISA. Rather, they are extended, and their original forms must be supp orted for decades (consider, for example, the IBM 360 and the Intel x86). Employing sound design principles is t hus imperative in an ISA. 4.3.1 Formats and Fields* The LC-3 ISA employs ﬁxed-length instructions and a load-store ar chitecture, two aspects that help to reduce the design space to a manageable set of choices. In a gener al ISA design, many other options exist for instruction formats. Recall the idea of separating the bits of an instruction into (possibly non-contiguous) ﬁelds. One of the ﬁelds must contain an opcode, which speciﬁes the type of operation to be performed by the instruction. In the LC-3 ISA, most opcodes specify both the type of operation and th e types of arguments to the operation. More generally, many addressing modes are possible for each opera nd, and we can think of the bits that specify the addressing mode as a separate ﬁeld, known as the modeﬁeld. As a simple example, the LC-3’s ADD and AND instructions contain a 1-bit mode ﬁeld that speciﬁes whe ther the second operand of the ADD/AND comes from a register or is an immediate value. Several questions must be answered in order to deﬁne the possible instruction formats for an ISA. First, are instructions ﬁxed-length or variable-length? Second, how many ad dresses are needed for each instruction, and how many of the addresses can be memory addresses? Finally, w hat forms of addresses are possible for each operand? For example, can one use full memory addresses or only limited oﬀsets relative to a register? The answertothe ﬁrstquestiondepends on manyfactors, but se veralclearadvantagesexist forboth answers. Fixed-length instructions are easy to fetch and decode. A processor knows in advance how m any bits must be fetched to fetch a full instruction; fetching the opcode a nd mode ﬁelds in order to decide how many more bits are necessary to complete the instruction may requ ire more than one cycle. Fixing the time necessary for instruction fetch also simpliﬁes pipelining. Finally, ﬁ xed-length instructions simplify the datapath by restricting instructions to the size of the bus and alwa ys fetching properly aligned instructions","{'page_number': 152, 'textbook_name': 'ECE-120-student-notes', 'text': 'Fixing the time necessary for instruction fetch also simpliﬁes pipelining. Finally, ﬁ xed-length instructions simplify the datapath by restricting instructions to the size of the bus and alwa ys fetching properly aligned instructions. As an example of this simpliﬁcation, note that the LC-3 ISA does not s upport addressing for individual bytes, only for 16-bit words. Variable-length instructions also have beneﬁts, however. Variable-length encodings allow more e ﬃcient encodings, saving both memory and disk space. A register transfe r operation, for example, clearly requires fewer bits than addition of values at two direct memory addresses f or storage at a third. Fixed-length in- structions must be ﬁxed at the length of the longest possible instru ction, whereasvariable-length instructions can use lengths appropriate to each mode. The same tradeoﬀ has a nother form in the sense that ﬁxed-length ISAs typically eliminate many addressing modes in order to limit the size o f the instructions. Variable- length instructions thus allow more ﬂexibility; indeed, extensions to a variable-length ISA can incorporate new addressing modes that require longer instructions without aﬀe cting the original ISA. For example, the maximum length of x86 instructions has grown from six bytes in 1978 ( the 8086 ISA) to ﬁfteen bytes in today’s version of the ISA.'}"
"Fixing the time necessary for instruction fetch also simpliﬁes pipelining. Finally, ﬁ xed-length instructions simplify the datapath by restricting instructions to the size of the bus and alwa ys fetching properly aligned instructions. As an example of this simpliﬁcation, note that the LC-3 ISA does not s upport addressing for individual bytes, only for 16-bit words. Variable-length instructions also have beneﬁts, however. Variable-length encodings allow more e ﬃcient encodings, saving both memory and disk space. A register transfe r operation, for example, clearly requires fewer bits than addition of values at two direct memory addresses f or storage at a third. Fixed-length in- structions must be ﬁxed at the length of the longest possible instru ction, whereasvariable-length instructions can use lengths appropriate to each mode. The same tradeoﬀ has a nother form in the sense that ﬁxed-length ISAs typically eliminate many addressing modes in order to limit the size o f the instructions. Variable- length instructions thus allow more ﬂexibility; indeed, extensions to a variable-length ISA can incorporate new addressing modes that require longer instructions without aﬀe cting the original ISA. For example, the maximum length of x86 instructions has grown from six bytes in 1978 ( the 8086 ISA) to ﬁfteen bytes in today’s version of the ISA.","{'page_number': 152, 'textbook_name': 'ECE-120-student-notes', 'text': 'Fixing the time necessary for instruction fetch also simpliﬁes pipelining. Finally, ﬁ xed-length instructions simplify the datapath by restricting instructions to the size of the bus and alwa ys fetching properly aligned instructions. As an example of this simpliﬁcation, note that the LC-3 ISA does not s upport addressing for individual bytes, only for 16-bit words. Variable-length instructions also have beneﬁts, however. Variable-length encodings allow more e ﬃcient encodings, saving both memory and disk space. A register transfe r operation, for example, clearly requires fewer bits than addition of values at two direct memory addresses f or storage at a third. Fixed-length in- structions must be ﬁxed at the length of the longest possible instru ction, whereasvariable-length instructions can use lengths appropriate to each mode. The same tradeoﬀ has a nother form in the sense that ﬁxed-length ISAs typically eliminate many addressing modes in order to limit the size o f the instructions. Variable- length instructions thus allow more ﬂexibility; indeed, extensions to a variable-length ISA can incorporate new addressing modes that require longer instructions without aﬀe cting the original ISA. For example, the maximum length of x86 instructions has grown from six bytes in 1978 ( the 8086 ISA) to ﬁfteen bytes in today’s version of the ISA.'}"
"148  Moving to the last of the three questions posed for instruction for mat deﬁnition, operand address speciﬁca- tion, we explore a range of answersdeveloped over the last few dec ades. Answers are usually chosen based on the number of bits necessary, and we use this metric to organize th e possibilities. The ﬁgure below separates approaches into two dimensions: the vertical dimension divides addr essing into registers and memory, and the horizontal dimension into varieties within each type. more bitsfewer bits implicitmemory""zero page"" relative addressessegmented memoryfull addressesspecial-purpose registersimplicitgeneral-purpose registersneed fewer bits memoryregisterneed more bits As a register ﬁle contains fewer registers than a memory does word s, the use of register operands rather than memory addresses reduces the number of bits required to sp ecify an operand. The LC-3 ISA uses a restricted set of addressing modes to stay within the limit imposed by the use of 16-bit instructions. Both register and memory addresses, however, admit a wide range of imp lementations. Implicit operands of either type require no additional bits for the implicit address. The LC-3 procedure call instruction, JSR, for example, stores the return address in R 7. No bits in the JSR encoding name the R7 register; R7 is used implicitly for every JSR executed. Similarly, the procedure call instructions in many ISAs push the return address onto a stack using an implicit register for the top of stack pointer. Memory addresses can also be implicitly equated to other memory addresses . An increment instruction operating on a memory address, for example, implicitly writes the result back to th e same address. At the opposite extreme, an instruction may include a full address, either to any register in the register ﬁle or to any address in the memory. The term general-purpose registers indicates that registers are used in any operation. Special-purpose registers , in contrast, split the register ﬁle and allow only certain registersto be usedin eachoperation. Forexample, the Motorola6 80x0series, used in earlyApple Macintosh computers, provides distinct sets of address and data registers . Loads and stores use the address registers; arithmetic, logic, and shift operations use the data registers. As a result, each instruction selects from a smaller set of registers and thus requires fewer bits in the instruct ion to name the register for use. As full memory addresses require many more bits than full register addresses, a wider range of techniques has been employed to reduce the length. “Zero page” addresses, as deﬁned in the 6510 (6502) ISA used by Commodore PET’s,15C64’s,16and VIC 20’s, preﬁxed a one-byte address with a zero byte, allowing shorter instructions when memory addresses fell within the ﬁrst 256 memor y locations. Assembly and machine language programmers made heavy use of these locations to produ ce shorter programs. Relative addressing is quite common in the LC-3 ISA, in which many addr esses are PC-relative. Typical commerical ISAs also make use of relative addressing","{'page_number': 153, 'textbook_name': 'ECE-120-student-notes', 'text': 'Assembly and machine language programmers made heavy use of these locations to produ ce shorter programs. Relative addressing is quite common in the LC-3 ISA, in which many addr esses are PC-relative. Typical commerical ISAs also make use of relative addressing. The Alpha ISA , for example, has a PC-relative form of procedure call with a 21-bit oﬀset (plus or minus a megabyte), an d the x86 ISA has a “short” form of branch instructions that uses an 8-bit oﬀset. Segmented memory is a form of relative addressing that uses a regis ter (usually implicit) to provide the high bits of an address and an explicit memory address (or another regis ter) to provide the low bits. In the early x86 ISAs, for example, 20-bit addresses are found by adding a 16- bit segment register extended with four zero bits to a 16-bit oﬀset. 15My computer in junior high school. 16My computer in high school.'}"
"Assembly and machine language programmers made heavy use of these locations to produ ce shorter programs. Relative addressing is quite common in the LC-3 ISA, in which many addr esses are PC-relative. Typical commerical ISAs also make use of relative addressing. The Alpha ISA , for example, has a PC-relative form of procedure call with a 21-bit oﬀset (plus or minus a megabyte), an d the x86 ISA has a “short” form of branch instructions that uses an 8-bit oﬀset. Segmented memory is a form of relative addressing that uses a regis ter (usually implicit) to provide the high bits of an address and an explicit memory address (or another regis ter) to provide the low bits. In the early x86 ISAs, for example, 20-bit addresses are found by adding a 16- bit segment register extended with four zero bits to a 16-bit oﬀset. 15My computer in junior high school. 16My computer in high school.","{'page_number': 153, 'textbook_name': 'ECE-120-student-notes', 'text': 'Assembly and machine language programmers made heavy use of these locations to produ ce shorter programs. Relative addressing is quite common in the LC-3 ISA, in which many addr esses are PC-relative. Typical commerical ISAs also make use of relative addressing. The Alpha ISA , for example, has a PC-relative form of procedure call with a 21-bit oﬀset (plus or minus a megabyte), an d the x86 ISA has a “short” form of branch instructions that uses an 8-bit oﬀset. Segmented memory is a form of relative addressing that uses a regis ter (usually implicit) to provide the high bits of an address and an explicit memory address (or another regis ter) to provide the low bits. In the early x86 ISAs, for example, 20-bit addresses are found by adding a 16- bit segment register extended with four zero bits to a 16-bit oﬀset. 15My computer in junior high school. 16My computer in high school.'}"
"4.3 Instruction Set Architecture* 149 4.3.2 Addressing Architectures* One question remains for the deﬁnition of instruction formats: how many addresses are needed for each instruction, and how many of the addresses can be memory addres ses? The ﬁrst part of this question usually rangesfrom zerotothree, and is rarelyallowedtogobeyond three . The answertothe secondpartdetermines theaddressing architecture implemented by an ISA. We now illustrate the tradeoﬀs between ﬁve d istinct addressing architectures through the use of a running example, t he assignment X=AB+C/D. A binary operator requires two source operands and one destinat ion operand, for a total of three addresses. The ADD instruction, for example, has a 3-address format: ADD A,B,C ; M[A]←M[B]+M[C] or ADD R1,R2,R3 ; R1←R2+R3 If all three addresses can be memory addresses, the ISA is dubbe d amemory-to-memory architecture . Such architectures may have small register sets or even lack a reg ister ﬁle completely. To implement the assignment, we assume the availability of two memory locations, T1 an d T2, for temporary storage: MUL T1,A,B ; T1←M[A]∗M[B] DIV T2,C,D ; T2←M[C]/M[D] ADD X,T1,T2 ; X←M[T1]+M[T2] Theassignmentrequiresonlythreeinstructionstoimplement, bute achinstructioncontainsthreefullmemory addresses, and is thus quite long. At the other extreme is the load-store architecture used by the LC-3 ISA. In a load-store architecture, only loads and stores can use memory addresses; all other operat ions use only registers. As most instructions use only registers, this type of addressing architecture is also calle d aregister-to-register architecture . The example assignment translates to the code shown below, which a ssumes that R1, R2, and R3 are free for use (the instructions are NOTLC-3 instructions, but rather a generic assembly language for a loa d-store architecture). LD R1,A ; R1←M[A] LD R2,B ; R2←M[B] MUL R1,R1,R2 ; R1←R1∗R2 LD R2,C ; R2←M[C] LD R3,D ; R3←M[D] DIV R2,R2,R3 ; R2←R2/R3 ADD R1,R1,R2 ; R1←R1+R2 ST R1,X ; M[X]←R1 Eight instructions are necessary, but no instruction requires mor e than one full memory address, and several use only register addresses, allowing the use of shorter instructio ns","{'page_number': 154, 'textbook_name': 'ECE-120-student-notes', 'text': 'The need to move data in and out of memory explicitly, however, also requires a reasonably large registe r set, as is available in the ARM, Sparc, Alpha, and IA-64 ISAs. Architectures that use other combinations of memory and registe r addresses with 3-address formats are not named. Unary operators and transfer operators require only on e source operand, thus can use a 2-address format (for example, NOT A,B). Binary operations can also use 2-address format if one operand is implicit, as in the following instructions: ADD A,B ; M[A]←M[A]+M[B] or ADD R1,B ; R1←R1+M[B] The second instruction, in which one address is a register and the se cond is a memory address, deﬁnes aregister-memory architecture . As shown by the code on the next page, such architectures strik e a balance between the two architectures just discussed.'}"
"The need to move data in and out of memory explicitly, however, also requires a reasonably large registe r set, as is available in the ARM, Sparc, Alpha, and IA-64 ISAs. Architectures that use other combinations of memory and registe r addresses with 3-address formats are not named. Unary operators and transfer operators require only on e source operand, thus can use a 2-address format (for example, NOT A,B). Binary operations can also use 2-address format if one operand is implicit, as in the following instructions: ADD A,B ; M[A]←M[A]+M[B] or ADD R1,B ; R1←R1+M[B] The second instruction, in which one address is a register and the se cond is a memory address, deﬁnes aregister-memory architecture . As shown by the code on the next page, such architectures strik e a balance between the two architectures just discussed.","{'page_number': 154, 'textbook_name': 'ECE-120-student-notes', 'text': 'The need to move data in and out of memory explicitly, however, also requires a reasonably large registe r set, as is available in the ARM, Sparc, Alpha, and IA-64 ISAs. Architectures that use other combinations of memory and registe r addresses with 3-address formats are not named. Unary operators and transfer operators require only on e source operand, thus can use a 2-address format (for example, NOT A,B). Binary operations can also use 2-address format if one operand is implicit, as in the following instructions: ADD A,B ; M[A]←M[A]+M[B] or ADD R1,B ; R1←R1+M[B] The second instruction, in which one address is a register and the se cond is a memory address, deﬁnes aregister-memory architecture . As shown by the code on the next page, such architectures strik e a balance between the two architectures just discussed.'}"
"150  LD R1,A ; R1←M[A] MUL R1,B ; R1←R1∗M[B] LD R2,C ; R2←M[C] DIV R2,D ; R2←R2/M[D] ADD R1,R2 ; R1←R1+R2 ST R1,X ; M[X]←R1 The assignment now requires six instructions using at most one memo ry address each; like memory-to- memory architectures, register-memory architectures use rela tively few registers. Note that two-register operations are also allowed. Intel’s x86 ISA is a register-memory arc hitecture. Several ISAs of the past17used a special-purpose register called the accumulator for ALU ope rations, and are called accumulator architectures . The accumulator in such architectures is implicitly both a source and the destination for any such operation, allowing a 1-address format for instructions, as shown below. ADD B ; ACC←ACC+M[B] or ST E ; M[E]←ACC Accumulator architectures strike the same balance as register-m emory architectures, but use fewer registers. Note that memory location X is used as a temporary storage location as well as the ﬁnal storage location in the following code: LD A ; ACC←M[A] MUL B ; ACC←ACC∗M[B] ST X ; M[X]←ACC LD C ; ACC←M[C] DIV D ; ACC←ACC/M [D] ADD X ; ACC←ACC+M[X] ST X ; M[X]←ACC The last addressing architecture that we discuss is rarely used for modern general-purpose processors, but may be familiar to you because of its historical use in scientiﬁc and eng ineering calculators. A stack architecture maintains a stack of values and draws all ALU operands from this sta ck, allowing these instructions to use a 0-address format. A special-purpose stack pointer (SP) register points to t he top of the stack in memory, and operations analogous to load ( push) and store ( pop) are provided to move values on and oﬀ the stack","{'page_number': 155, 'textbook_name': 'ECE-120-student-notes', 'text': '17The 6510/6502 as well, if memory serves, as the 8080, Z80, and Z8000, which used to drive parlor video games.'}"
"To implement our example assignment, we ﬁrst t ransform it into postﬁx notation (also called reverse Polish notation): A B * C D / + The resulting sequence of symbols transforms on a one-to-one ba sis into instructions for a stack architecture: PUSH A ; SP←SP−1,M[SP]←M[A] A PUSH B ; SP←SP−1,M[SP]←M[B] B A MUL ; M[SP+1]←M[SP+1]∗M[SP],SP←SP+1 AB PUSH C ; SP←SP−1,M[SP]←M[C] C AB PUSH D ; SP←SP−1,M[SP]←M[D] D C AB DIV ; M[SP+1]←M[SP+1]/M[SP],SP←SP+1 C/D AB ADD ; M[SP+1]←M[SP+1]+M[SP],SP←SP+1 AB+C/D POP X ; M[X]←M[SP],SP←SP+1 The values to the right are the values on the stack, starting with th e top value on the left and progressing downwards, after the completion of each instruction","{'page_number': 155, 'textbook_name': 'ECE-120-student-notes', 'text': '17The 6510/6502 as well, if memory serves, as the 8080, Z80, and Z8000, which used to drive parlor video games.'}"
"17The 6510/6502 as well, if memory serves, as the 8080, Z80, and Z8000, which used to drive parlor video games.","{'page_number': 155, 'textbook_name': 'ECE-120-student-notes', 'text': '17The 6510/6502 as well, if memory serves, as the 8080, Z80, and Z8000, which used to drive parlor video games.'}"
"4.3 Instruction Set Architecture* 151 4.3.3 Common Special-Purpose Registers* This section illustrates the uses of special-purpose registers thro ugh a few examples. Thestack pointer (SP) points to the top of the stack in memory. Most older architectures support push and pop operations that implicitly use the stack pointer. Modern arc hitectures assign a general-purpose register to be the stack pointer and reference it explicitly, althoug h an assembler may support instructions that appear to use implicit operands but in fact translate to machine instructions with explicit reference to the register deﬁned to be the SP. Theprogram counter (PC) points to the next instruction to be executed. Some modern archit ectures expose it as a general-purpose register, although its distinct role in the implementation keeps such a model from becoming as common as the use of a general-purpose register for the SP. Theprocessor status register (PSR) , also known as the processor status word (PSW) , contains all status bits as well as a mode bit indicating whether the processor is o perating in user mode or privileged (operating system) mode. Having a register with this information allo ws more general access than is possible solely through the use of control ﬂow instructions. Thezero register appears in modern architectures of the RISC variety (deﬁned in th e next section of these notes). The register is read-only and serves both as a useful con stant and as a destination for operations performed only for their side-eﬀects (for example, setting statu s bits). The availability of a zero register also allows certain opcodes to serve double duty. A register-to-regist er add instruction becomes a register move instruction when one source operand is zero. Similarly, an immediate a dd instruction becomes an immediate load instruction when one source operand is zero. 4.3.4 Reduced Instruction Set Computers* By the mid-1980’s, the VAX architecture dominated the workstatio n and minicomputer markets, which included most universities. Digital Equipment Corporation, the crea tor of the VAX, was second only to IBM in terms of computer sales. VAXen, as the machines were called, use d microprogrammed control units and supported numerous addressing modes as well as complex instruct ions ranging from “square root” to “ﬁnd roots of polynomial equation.” The impact of increasingly dense integrated circuit technology had b egun to have its eﬀect, however, and in view of increasing processor clock speeds, more and more progra mmers were using high-level languages rather than writing assembly code. Although assembly programmer s often made use of the complex VAX instructions, compilers were usually unable to recognize the corres ponding high-level language constructs and thus were unable to make use of the instructions. Increasing density also led to rapid growth in memory sizes, to the po int that researchers began to question the need for variable-length instructions. Recall that variable-len gth instructions allow shorter codes by providing more eﬃcient instruction encodings. With the trend towar d larger memories, code length was less important. The performance advantage of ﬁxed-length instr uctions, which simpliﬁes the datapath and enables pipelining, on the other hand, was attractive","{'page_number': 156, 'textbook_name': 'ECE-120-student-notes', 'text': 'Recall that variable-len gth instructions allow shorter codes by providing more eﬃcient instruction encodings. With the trend towar d larger memories, code length was less important. The performance advantage of ﬁxed-length instr uctions, which simpliﬁes the datapath and enables pipelining, on the other hand, was attractive. Researchersleveragedtheseideas,whichhadbeenﬂoatingaroun dtheresearchcommunity(andhadappeared in somecommercialarchitectures)to create reduced instruction set computers , orRISCmachines. The competing VAXen were labeled CISCmachines, which stands for complex instruction set computers . RISC machinesemploy ﬁxed-length instructions and a load-storear chitecture, allowingonly a few addressing modes and small oﬀsets. This combination ofdesign decisionsenables deep pipelines and multiple instruction issues in a single cycle (termed superscalar implementations), and fo r years, RISC machines were viewed by many researchers as the proper design for future ISAs. Howeve r, companies such as Intel soon learned to pipeline microoperations after decoding instructions, and CISC arc hitectures now oﬀer competitive if not superior performance in comparison with RISC machines. The VAXen are dead, of course,18having been re- placed by the Alpha, which in turn fell to x86, which is now strugglingwit h ARM to enter the mobile market. 18Unless you talk with customer support employees, for whom no machine ever dies.'}"
"Recall that variable-len gth instructions allow shorter codes by providing more eﬃcient instruction encodings. With the trend towar d larger memories, code length was less important. The performance advantage of ﬁxed-length instr uctions, which simpliﬁes the datapath and enables pipelining, on the other hand, was attractive. Researchersleveragedtheseideas,whichhadbeenﬂoatingaroun dtheresearchcommunity(andhadappeared in somecommercialarchitectures)to create reduced instruction set computers , orRISCmachines. The competing VAXen were labeled CISCmachines, which stands for complex instruction set computers . RISC machinesemploy ﬁxed-length instructions and a load-storear chitecture, allowingonly a few addressing modes and small oﬀsets. This combination ofdesign decisionsenables deep pipelines and multiple instruction issues in a single cycle (termed superscalar implementations), and fo r years, RISC machines were viewed by many researchers as the proper design for future ISAs. Howeve r, companies such as Intel soon learned to pipeline microoperations after decoding instructions, and CISC arc hitectures now oﬀer competitive if not superior performance in comparison with RISC machines. The VAXen are dead, of course,18having been re- placed by the Alpha, which in turn fell to x86, which is now strugglingwit h ARM to enter the mobile market. 18Unless you talk with customer support employees, for whom no machine ever dies.","{'page_number': 156, 'textbook_name': 'ECE-120-student-notes', 'text': 'Recall that variable-len gth instructions allow shorter codes by providing more eﬃcient instruction encodings. With the trend towar d larger memories, code length was less important. The performance advantage of ﬁxed-length instr uctions, which simpliﬁes the datapath and enables pipelining, on the other hand, was attractive. Researchersleveragedtheseideas,whichhadbeenﬂoatingaroun dtheresearchcommunity(andhadappeared in somecommercialarchitectures)to create reduced instruction set computers , orRISCmachines. The competing VAXen were labeled CISCmachines, which stands for complex instruction set computers . RISC machinesemploy ﬁxed-length instructions and a load-storear chitecture, allowingonly a few addressing modes and small oﬀsets. This combination ofdesign decisionsenables deep pipelines and multiple instruction issues in a single cycle (termed superscalar implementations), and fo r years, RISC machines were viewed by many researchers as the proper design for future ISAs. Howeve r, companies such as Intel soon learned to pipeline microoperations after decoding instructions, and CISC arc hitectures now oﬀer competitive if not superior performance in comparison with RISC machines. The VAXen are dead, of course,18having been re- placed by the Alpha, which in turn fell to x86, which is now strugglingwit h ARM to enter the mobile market. 18Unless you talk with customer support employees, for whom no machine ever dies.'}"
"152  4.3.5 Procedure and System Calls* Aprocedure is a sequence of instructions that executes a particular task. Pro cedures are used as build- ing blocks for multiple, larger tasks. The concept of a procedure is f undamental to programming, and appears in some form in every high-level language as well as in most IS As. For our purposes, the terms procedure, subroutine, function, and method are synonymous, although they usually have slightly diﬀerent meanings from the linguistic point of view. Procedure calls are suppor ted through callandreturncontrol ﬂow instructions. The ﬁrst instruction in the code below, for examp le, transfers control to the procedure “DoSomeWork,” which presumably does some work, then returns co ntrol to the instruction following the call. loop: CALL DoSomeWork CMP R6,#1 ; compare return value in R6 to 1 BEQ loop ; keep doing work until R6 is not 1 DoSomeWork: ··· ; set R6 to 0 when all work is done, 1 otherwise RETN The procedure also places a return value in R6, which the instruction following the call compares with immediate value 1. Until the two are not equal (when all work is done) , the branch returns control to the call and executes the procedure again. As you may recall, the call and return use the stack pointer to keep track of nested calls. Sample RTL for these operations appears below. call RTL SP←SP−1 M[SP]←PC PC←procedure startreturn RTL PC←M[SP] SP←SP+1 While an ISA provides the call and return instructions necessary to support procedures, it does not specify howinformationispassedtoorreturnedfromaprocedure. Astan dardforsuch decisionsis usuallydeveloped andincludedindescriptionsofthearchitecture,however. This calling convention speciﬁeshowinformation is passed between a caller and a callee. In particular, it speciﬁes the f ollowing: where arguments must be placed, either in registers or in speciﬁc stack memory locations; whic h registers can be used or changed by the procedure; and where any return value must be placed. The term “calling convention” is also used in the programming language community to describe the conven- tion for deciding what information is passed for a given call operation . For example, are variables passed by value, by pointers to values, or in some other way? However, onc e the things to be sent are decided, the architectural calling convention that we discuss here is used to det ermine where to put the data in order for the callee to be able to ﬁnd it. Calling conventions for architectures with large register sets typic ally pass arguments in registers, and nearly all conventions place the return value in a register. A calling convent ion also divides the register set into caller-saved andcallee-saved registers. Caller-saved registers can be modiﬁed arbitrarily by the called procedure, whereas any value in a callee-saved register must be pr eserved. Similarly, before calling a proce- dure, a caller must preserve the values of any caller saved register s that are needed after the call","{'page_number': 157, 'textbook_name': 'ECE-120-student-notes', 'text': 'A calling convent ion also divides the register set into caller-saved andcallee-saved registers. Caller-saved registers can be modiﬁed arbitrarily by the called procedure, whereas any value in a callee-saved register must be pr eserved. Similarly, before calling a proce- dure, a caller must preserve the values of any caller saved register s that are needed after the call. Registers of both types usually saved on the stack by the appropriate code ( caller or callee).'}"
"A calling convent ion also divides the register set into caller-saved andcallee-saved registers. Caller-saved registers can be modiﬁed arbitrarily by the called procedure, whereas any value in a callee-saved register must be pr eserved. Similarly, before calling a proce- dure, a caller must preserve the values of any caller saved register s that are needed after the call. Registers of both types usually saved on the stack by the appropriate code ( caller or callee).","{'page_number': 157, 'textbook_name': 'ECE-120-student-notes', 'text': 'A calling convent ion also divides the register set into caller-saved andcallee-saved registers. Caller-saved registers can be modiﬁed arbitrarily by the called procedure, whereas any value in a callee-saved register must be pr eserved. Similarly, before calling a proce- dure, a caller must preserve the values of any caller saved register s that are needed after the call. Registers of both types usually saved on the stack by the appropriate code ( caller or callee).'}"
"4.3 Instruction Set Architecture* 153 A typical stack structure appears in the ﬁgure to the right. In pr eparation for a call, a caller ﬁrst stores any caller-saved registers on the stack. A rguments to the procedure to be called are pushed next. The procedure is called nex t, implicitly pushing the return address (the address of the instruction follow ing the call instruc- tion). Finally, the called procedure may allocate space on the stack f or storage of callee-saved registers as well as local variables. As an example, the following calling convention can be applied to an 8-re gister load-store architecture similar to the LC-3 ISA: the ﬁrst three ar guments must be placed in R0 through R2 (in order), with any remaining arguments on t he stack; the return value must be placed in R6; R0 through R2 are caller-save d, as is R6, while R3 through R5 are callee-saved; R7 is used as the stack pointer . The code fragments below use this calling convention to implement a procedure and a call of that procedure.storage for current procedurestorage for more calls return address extra arguments call stacklast proc’s SPsaved valuesSP int add3 (int n1, int n2, int n3) { return (n1 + n2 + n3); } ... printf (“%d”, add3 (10, 20, 30)); by convention: n1 is in R0 n2 is in R1 n3 is in R2 return value is in R6add3: ADD R0,R0,R1 ADD R6,R0,R2 RETN ... PUSH R1 ; save the value in R1 LDI R0,#10 ; marshal arguments LDI R1,#20 LDI R2,#30 CALL add3 MOV R1,R6 ; return value becomes 2nd argument LDI R0,“%d” ; load a pointer to the string CALL printf POP R1 ; restore R1 The add3 procedure takes three integers as arguments, adds th em together, and returns the sum. The procedure is called with the constants 10, 20, and 30, and the resu lt is printed. By the calling convention, when the call is made, R0 must contain the value 10, R1 the value 20, a nd R2 the value 30. We assume that the caller wants to preserve the value of R1, but does not care abo ut R3 or R5. In the assembly language version on the right, R1 is ﬁrst saved to the stack, then the argum ents are marshaled into position, and ﬁnally the call is made. The procedure itself needs no local storage a nd does not change any callee-saved registers, thus must simply add the numbers together and place th e result in R6. After add3 returns, its return value is moved from R6 to R1 in preparation for the call to prin tf. After loading a pointer to the format string into R0, the second call is made, and R1 is restored, c ompleting the translation. System calls are almost identical to procedure calls","{'page_number': 158, 'textbook_name': 'ECE-120-student-notes', 'text': 'The procedure itself needs no local storage a nd does not change any callee-saved registers, thus must simply add the numbers together and place th e result in R6. After add3 returns, its return value is moved from R6 to R1 in preparation for the call to prin tf. After loading a pointer to the format string into R0, the second call is made, and R1 is restored, c ompleting the translation. System calls are almost identical to procedure calls. As with procedure calls, a ca lling convention is used: before invoking a system call, arguments are marshaled into the app ropriate registers or locations in the stack; after a system call returns, any result appears in a pre-s peciﬁed register. The calling convention used for system calls need not be the same as that used for procedure c alls. Rather than a call instruction, system calls are usually initiated with a trapinstruction, and system calls are also known as traps. With many architectures, a system call places the processor in privileged or k ernel mode, and the instructions that im- plement the call are considered to be part of the operating system . The term system call arises from this fact.'}"
"The procedure itself needs no local storage a nd does not change any callee-saved registers, thus must simply add the numbers together and place th e result in R6. After add3 returns, its return value is moved from R6 to R1 in preparation for the call to prin tf. After loading a pointer to the format string into R0, the second call is made, and R1 is restored, c ompleting the translation. System calls are almost identical to procedure calls. As with procedure calls, a ca lling convention is used: before invoking a system call, arguments are marshaled into the app ropriate registers or locations in the stack; after a system call returns, any result appears in a pre-s peciﬁed register. The calling convention used for system calls need not be the same as that used for procedure c alls. Rather than a call instruction, system calls are usually initiated with a trapinstruction, and system calls are also known as traps. With many architectures, a system call places the processor in privileged or k ernel mode, and the instructions that im- plement the call are considered to be part of the operating system . The term system call arises from this fact.","{'page_number': 158, 'textbook_name': 'ECE-120-student-notes', 'text': 'The procedure itself needs no local storage a nd does not change any callee-saved registers, thus must simply add the numbers together and place th e result in R6. After add3 returns, its return value is moved from R6 to R1 in preparation for the call to prin tf. After loading a pointer to the format string into R0, the second call is made, and R1 is restored, c ompleting the translation. System calls are almost identical to procedure calls. As with procedure calls, a ca lling convention is used: before invoking a system call, arguments are marshaled into the app ropriate registers or locations in the stack; after a system call returns, any result appears in a pre-s peciﬁed register. The calling convention used for system calls need not be the same as that used for procedure c alls. Rather than a call instruction, system calls are usually initiated with a trapinstruction, and system calls are also known as traps. With many architectures, a system call places the processor in privileged or k ernel mode, and the instructions that im- plement the call are considered to be part of the operating system . The term system call arises from this fact.'}"
"154  4.3.6 Interrupts and Exceptions* Unexpected processor interruptions arise both from interaction s between a processor and external devices and from errors or unexpected behavior in the program being exec uted. The term interrupt is reserved for asynchronous interruptions generated by other devices, inc luding disk drives, printers, network cards, video cards, keyboards, mice, and any number of other possibilities .Exceptions occur when a processor encounters an unexpected opcode or operand. An undeﬁned inst ruction, for example, gives rise to an ex- ception, as does an attempt to divide by zero. Exceptions usually ca use the current program to terminate, although many operating systems will allow the program to catch the exception and to handle it more intel- ligently. The table below summarizes the characteristicsof the two t ypes and compares them to system calls. type generated by example asynchronous unexpected interrupt external device packet arrived at network card yes yes exception invalid opcode or operand divide by zero no yes trap/system call deliberate, via trap instruction print character to console no no Interruptsoccurasynchronouslywithrespecttotheprogram. Mostdesignsonlyrecognizeinterruptsbetween instructions. In other words, the presence of interrupts is chec ked only after completing an instruction rather than in every cycle. In pipelined designs, however, instructions exe cute simultaneously, and the decision as to which instructions occur “before” an interrupt and which occur “after” must be made by the processor. Exceptions are not asynchronous in the sense that they occur fo r a particular instruction, thus no decision need be made as to instruction ordering. After determining which ins tructions were before an interrupt, a pipelined processor discards the state of any partially executed ins tructions that occur “after” the interrupt and completes all instructions that occur “before.” The terminate d instructions are simply restarted after the interrupt completes. Handling the decision, the termination, an d the completion, however, signiﬁcantly increases the design complexity of the system. The code associatedwith an interrupt, an exception, ora system c all is a form ofprocedurecalled a handler , and is found by looking up the interrupt number, exception number, or trap number in a table of functions calledavector table . Vectortables foreachtype (interrupts, exceptions, and syst emcalls)maybe separate, ormay be combined into asingle table. Interrupts and exceptionssh are aneed to saveall registersand status bits before execution of the corresponding handler code (and to r estore those values afterward). Generally, the values—including the status word register—are placed on the st ack. With system calls, saving and restoring any necessary state is part of the calling convention. A s pecial return from interrupt instruction is used to return control from the interrupt handler to the interr upted code; a similar instruction forces the processor back into user mode when returning from a system call. Interrupts are also interesting in the sense that typical compute rs often have many interrupt-generating devices but only a few interrupts. Interrupts are prioritized by nu mber, and only an interrupt with higher prioritycaninterruptanotherinterrupt. Interruptswithequal orlowerpriorityareblockedwhileaninterrupt executes","{'page_number': 159, 'textbook_name': 'ECE-120-student-notes', 'text': 'Interrupts are also interesting in the sense that typical compute rs often have many interrupt-generating devices but only a few interrupts. Interrupts are prioritized by nu mber, and only an interrupt with higher prioritycaninterruptanotherinterrupt. Interruptswithequal orlowerpriorityareblockedwhileaninterrupt executes. Some interrupts can also be blocked in some architectur es by setting bits in a special-purpose register called an interrupt mask. While an interrupt number is maske d, interrupts of that type are blocked, and can not occur. As several devices may generate interrupts with the same interru pt number, interrupt handlers can be chained together. Each handler corresponds to a particular device. When an interrupt occurs, control is passed to the handler for the ﬁrst device, which accesses device r egisters to determine whether or not that device generated an interrupt. If it did, the appropriate service is provided. If not, or after the service is complete, control is passed to the next handler in the chain, which h andles interrupts from the second device, and so forth until the last handler in the chain completes. At this poin t, registers and processor state are restored and control is returned to the point at which the interru pt occurred.'}"
"Interrupts are also interesting in the sense that typical compute rs often have many interrupt-generating devices but only a few interrupts. Interrupts are prioritized by nu mber, and only an interrupt with higher prioritycaninterruptanotherinterrupt. Interruptswithequal orlowerpriorityareblockedwhileaninterrupt executes. Some interrupts can also be blocked in some architectur es by setting bits in a special-purpose register called an interrupt mask. While an interrupt number is maske d, interrupts of that type are blocked, and can not occur. As several devices may generate interrupts with the same interru pt number, interrupt handlers can be chained together. Each handler corresponds to a particular device. When an interrupt occurs, control is passed to the handler for the ﬁrst device, which accesses device r egisters to determine whether or not that device generated an interrupt. If it did, the appropriate service is provided. If not, or after the service is complete, control is passed to the next handler in the chain, which h andles interrupts from the second device, and so forth until the last handler in the chain completes. At this poin t, registers and processor state are restored and control is returned to the point at which the interru pt occurred.","{'page_number': 159, 'textbook_name': 'ECE-120-student-notes', 'text': 'Interrupts are also interesting in the sense that typical compute rs often have many interrupt-generating devices but only a few interrupts. Interrupts are prioritized by nu mber, and only an interrupt with higher prioritycaninterruptanotherinterrupt. Interruptswithequal orlowerpriorityareblockedwhileaninterrupt executes. Some interrupts can also be blocked in some architectur es by setting bits in a special-purpose register called an interrupt mask. While an interrupt number is maske d, interrupts of that type are blocked, and can not occur. As several devices may generate interrupts with the same interru pt number, interrupt handlers can be chained together. Each handler corresponds to a particular device. When an interrupt occurs, control is passed to the handler for the ﬁrst device, which accesses device r egisters to determine whether or not that device generated an interrupt. If it did, the appropriate service is provided. If not, or after the service is complete, control is passed to the next handler in the chain, which h andles interrupts from the second device, and so forth until the last handler in the chain completes. At this poin t, registers and processor state are restored and control is returned to the point at which the interru pt occurred.'}"
"4.3 Instruction Set Architecture* 155 4.3.7 Control Flow Conditions* Control ﬂow instructions may change the PC, loading it with an addre ss speciﬁed by the instruction. Al- though any addressing mode can be supported, the most common s pecify an address directly in the instruc- tion, use a register as an address, or use an address relative to a r egister. Unconditional control ﬂow instructions typically provided by an ISA include procedure calls and returns, traps, and jumps. Conditional control ﬂow instructions are bran ches, and are logically based on status bits set by two types of instructions: comparisons andbit tests . Comparisons subtract one value from another to set the status bits, whereas bit tests use an AND operation to c heck whether certain bits are set or not in a value. Many ISAs implement status bits as special-purpose registers and im plicitly set them when executing certain instructions. A branch based on R2 being less or equal to R3 can the n be written as shown below. The status bits are set by subtracting R3 from R2 with the ALU. CMP R2,R3 ; R2< R3 :CNZ←110,R2=R3 :CNZ←001, ;R2> R3 :CNZ←000 BLE R1 ; ZXORC= 1 :PC←R1 The status bits are not always implemented as special-purpose regis ters; instead, they may be kept in general-purpose registers or not kept at all. For example, the Alph a ISA stores the results of comparisons in general-purpose registers, and the same branch is instead implem ented as follows: CMPLE R4,R2,R3 ; R2≤R3 :R4←1,R2> R3 :R4←0 BNE R4,R1 ; R4/n⌉}ationslash= 0 :PC←R1 Finally, status bits can be calculated, used, and discarded within a sin gle instruction, in which case the branch is written as follows: BLE R1,R2,R3 ; R2≤R3 :PC←R1 The three approaches have advantages and disadvantages similar to those discussed in the section on ad- dressing architectures: the ﬁrst has the shortest instructions , the second is the most general and simplest to implement, and the third requires the fewest instructions. 4.3.8 Stack Operations* Two types of stack operations are commonly supported. Push and pop are the basic operations in many older architectures, and values can be placed upon or removed fro m the stack using these instructions. In more modern architectures, in which the SP becomes a general-pur pose register, push and pop are replaced with indexed loads and stores, that is, loads and stores using the st ack pointer and an oﬀset as the address for the memory operation. Stack updates are performed using th e ALU, subtracting and adding immediate values from the SP as necessary to allocate and deallocate local sto rage. Stack operations serve three purposes in a typical architecture . The ﬁrst is to support procedure calls, as illustrated in a previous section","{'page_number': 160, 'textbook_name': 'ECE-120-student-notes', 'text': 'Stack updates are performed using th e ALU, subtracting and adding immediate values from the SP as necessary to allocate and deallocate local sto rage. Stack operations serve three purposes in a typical architecture . The ﬁrst is to support procedure calls, as illustrated in a previous section. The second is to provide temporary storage during interrupts, which was also mentioned earlier. The third use of stack operations is to support spill code generated by compilers. Compilers ﬁrst translate high-levellanguagesinto anintermediate representationmuch like a ssemblycodebut with an extremely large (theoretically inﬁnite) register set. The ﬁnal translation step tra nslates this intermediate representation into assembly code for the target architecture, assigning architectu ral registers as necessary. However, as real ISAs support only a ﬁnite number of registers, the compiler must oc casionally spill values into memory. For example, if ten values are in use at some point in the code, but the arc hitecture has only eight registers, spill code must be generated to store the remaining two values on the st ack and to restore them when they are needed.'}"
"Stack updates are performed using th e ALU, subtracting and adding immediate values from the SP as necessary to allocate and deallocate local sto rage. Stack operations serve three purposes in a typical architecture . The ﬁrst is to support procedure calls, as illustrated in a previous section. The second is to provide temporary storage during interrupts, which was also mentioned earlier. The third use of stack operations is to support spill code generated by compilers. Compilers ﬁrst translate high-levellanguagesinto anintermediate representationmuch like a ssemblycodebut with an extremely large (theoretically inﬁnite) register set. The ﬁnal translation step tra nslates this intermediate representation into assembly code for the target architecture, assigning architectu ral registers as necessary. However, as real ISAs support only a ﬁnite number of registers, the compiler must oc casionally spill values into memory. For example, if ten values are in use at some point in the code, but the arc hitecture has only eight registers, spill code must be generated to store the remaining two values on the st ack and to restore them when they are needed.","{'page_number': 160, 'textbook_name': 'ECE-120-student-notes', 'text': 'Stack updates are performed using th e ALU, subtracting and adding immediate values from the SP as necessary to allocate and deallocate local sto rage. Stack operations serve three purposes in a typical architecture . The ﬁrst is to support procedure calls, as illustrated in a previous section. The second is to provide temporary storage during interrupts, which was also mentioned earlier. The third use of stack operations is to support spill code generated by compilers. Compilers ﬁrst translate high-levellanguagesinto anintermediate representationmuch like a ssemblycodebut with an extremely large (theoretically inﬁnite) register set. The ﬁnal translation step tra nslates this intermediate representation into assembly code for the target architecture, assigning architectu ral registers as necessary. However, as real ISAs support only a ﬁnite number of registers, the compiler must oc casionally spill values into memory. For example, if ten values are in use at some point in the code, but the arc hitecture has only eight registers, spill code must be generated to store the remaining two values on the st ack and to restore them when they are needed.'}"
"156  4.3.9 I/O* As a ﬁnal topic, we now consider how a processor connects to othe r devices to allow input and output. We have already discussed interrupts, which are a special form of I /O in which only the signal requesting attention is conveyed to the processor. Communication of data oc curs through instructions similar to loads and stores. A processor is designed with a number of I/O ports —usually read-only or write-only registers to which devices can be attached with opposite semantics. That is, a port is usually written by the processor and read by a device or written by a device and read by the processo r. The question of exactly how I/O ports are accessed is an interestin g one. One option is to create special instructions, such as the inandoutinstructions ofthe x86architecture. Portaddressescan then b e speciﬁed in the same way that memory addresses are speciﬁed, but use a dist inct address space. Just as two sets of special-purpose registers can be separated by the ISA, such a nindependent I/O system separates I/O ports from memory addresses by using distinct instructions for ea ch class of operation. Alternatively, device registers can be accessed using the same load and store instructions as are used to access memory. This approach, known as memory-mapped I/O , requires no new instructions for I/O, but demands that a region of the memory address space be set asid e for I/O. The memory words with those addresses, if they exist, can not be accessed during normal proc essor operations.","{'page_number': 161, 'textbook_name': 'ECE-120-student-notes', 'text': '156  4.3.9 I/O* As a ﬁnal topic, we now consider how a processor connects to othe r devices to allow input and output. We have already discussed interrupts, which are a special form of I /O in which only the signal requesting attention is conveyed to the processor. Communication of data oc curs through instructions similar to loads and stores. A processor is designed with a number of I/O ports —usually read-only or write-only registers to which devices can be attached with opposite semantics. That is, a port is usually written by the processor and read by a device or written by a device and read by the processo r. The question of exactly how I/O ports are accessed is an interestin g one. One option is to create special instructions, such as the inandoutinstructions ofthe x86architecture. Portaddressescan then b e speciﬁed in the same way that memory addresses are speciﬁed, but use a dist inct address space. Just as two sets of special-purpose registers can be separated by the ISA, such a nindependent I/O system separates I/O ports from memory addresses by using distinct instructions for ea ch class of operation. Alternatively, device registers can be accessed using the same load and store instructions as are used to access memory. This approach, known as memory-mapped I/O , requires no new instructions for I/O, but demands that a region of the memory address space be set asid e for I/O. The memory words with those addresses, if they exist, can not be accessed during normal proc essor operations.'}"
"4.4 Summary of Part 4 of the Course 157 ECE120: Introduction to Computer Engineering Notes Set 4.4 Summary of Part 4 of the Course With the exception of control unit design strategies and redundan cy and coding, most of the material in this part of the course is drawn from Patt and Patel Chapters 4 throu gh 7. You may also want to read Patt and Patel’s Appendix C for details of their control unit design. In this short summary, we give youlists at severallevelsof diﬃculty o fwhat we expect you to be able to do as a result of the last few weeks of studying (reading, listening, doing h omework, discussing your understanding with your classmates, and so forth). We’ll start with the easy stuﬀ. You should recognize all of these ter ms and be able to explain what they mean","{'page_number': 162, 'textbook_name': 'ECE-120-student-notes', 'text': '•von Neumann elements - program counter (PC) - instruction register (IR) - memory address register (MAR) - memory data register (MDR) - processor datapath - bus - control signal - instruction processing •Instruction Set Architecture (ISA) - instruction encoding - ﬁeld (in an encoded instruction) - operation code (opcode) •assemblers and assembly code - opcode mnemonic (such as ADD, JMP) - two-pass process - label - symbol table - pseudo-op / directive•systematic decomposition - sequential - conditional - iterative •control unit design strategies - control word / microinstruction - sequencing / microsequencing - hardwired control - single-cycle - multi-cycle - microprogrammed control •error detection and correction – code/sparse representation – code word – bit error – odd/even parity bit – Hamming distance between code words – Hamming distance of a code – Hamming code – SEC-DED'}"
"•von Neumann elements - program counter (PC) - instruction register (IR) - memory address register (MAR) - memory data register (MDR) - processor datapath - bus - control signal - instruction processing •Instruction Set Architecture (ISA) - instruction encoding - ﬁeld (in an encoded instruction) - operation code (opcode) •assemblers and assembly code - opcode mnemonic (such as ADD, JMP) - two-pass process - label - symbol table - pseudo-op / directive•systematic decomposition - sequential - conditional - iterative •control unit design strategies - control word / microinstruction - sequencing / microsequencing - hardwired control - single-cycle - multi-cycle - microprogrammed control •error detection and correction – code/sparse representation – code word – bit error – odd/even parity bit – Hamming distance between code words – Hamming distance of a code – Hamming code – SEC-DED","{'page_number': 162, 'textbook_name': 'ECE-120-student-notes', 'text': '•von Neumann elements - program counter (PC) - instruction register (IR) - memory address register (MAR) - memory data register (MDR) - processor datapath - bus - control signal - instruction processing •Instruction Set Architecture (ISA) - instruction encoding - ﬁeld (in an encoded instruction) - operation code (opcode) •assemblers and assembly code - opcode mnemonic (such as ADD, JMP) - two-pass process - label - symbol table - pseudo-op / directive•systematic decomposition - sequential - conditional - iterative •control unit design strategies - control word / microinstruction - sequencing / microsequencing - hardwired control - single-cycle - multi-cycle - microprogrammed control •error detection and correction – code/sparse representation – code word – bit error – odd/even parity bit – Hamming distance between code words – Hamming distance of a code – Hamming code – SEC-DED'}"
"158  We expect you to be able to exercise the following skills: •Map RTL (register transfer language) operations into control wo rds for a given processor datapath. •Systematically decompose a (simple enough) problem to the level of L C-3 instructions. •Encode LC-3 instructions into machine code. •Read and understand programs written in LC-3 assembly/machine c ode. •Test and debug a small program in LC-3 assembly/machine code. •Be able to calculate the Hamming distance of a code/representation . •Know the relationships between Hamming distance and the abilities to d etect and to correct bit errors. We expect that you will understand the concepts and ideas to the e xtent that you can do the following: •Explain the role of diﬀerent types of instructions in allowing a program mer to express a computation. •Explain the importance of the three types of subdivisions in systema tic decomposition (sequential, conditional, and iterative). •Explainthe processoftransformingassemblycodeintomachinecod e(that is, explainhowanassembler works, including describing the use of the symbol table). •Be able to use parity for error detection, and Hamming codes for er ror correction. At the highest level, we hope that, while you do not have direct subst antial experience in this regard from our class (and should not expect to be tested on these skills), that you will nonetheless be able to begin to do the following when designing combinational logic: •Design and compare implementations using gates, decoders, muxes , and/or memories as appropriate, and including reasoning about the relevant design tradeoﬀs in terms of area and delay. •Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based design, again in- cluding reasoning about the relevant design tradeoﬀs in terms of ar ea and delay. •Design and compare implementations of processor control units us ing both hardwired and micropro- grammed strategies, and again including reasoning about the releva nt design tradeoﬀs in terms of area and delay. •Understandbasictradeoﬀsinthesparsityofcodewordswith erro rdetectionandcorrectioncapabilities.","{'page_number': 163, 'textbook_name': 'ECE-120-student-notes', 'text': '158  We expect you to be able to exercise the following skills: •Map RTL (register transfer language) operations into control wo rds for a given processor datapath. •Systematically decompose a (simple enough) problem to the level of L C-3 instructions. •Encode LC-3 instructions into machine code. •Read and understand programs written in LC-3 assembly/machine c ode. •Test and debug a small program in LC-3 assembly/machine code. •Be able to calculate the Hamming distance of a code/representation . •Know the relationships between Hamming distance and the abilities to d etect and to correct bit errors. We expect that you will understand the concepts and ideas to the e xtent that you can do the following: •Explain the role of diﬀerent types of instructions in allowing a program mer to express a computation. •Explain the importance of the three types of subdivisions in systema tic decomposition (sequential, conditional, and iterative). •Explainthe processoftransformingassemblycodeintomachinecod e(that is, explainhowanassembler works, including describing the use of the symbol table). •Be able to use parity for error detection, and Hamming codes for er ror correction. At the highest level, we hope that, while you do not have direct subst antial experience in this regard from our class (and should not expect to be tested on these skills), that you will nonetheless be able to begin to do the following when designing combinational logic: •Design and compare implementations using gates, decoders, muxes , and/or memories as appropriate, and including reasoning about the relevant design tradeoﬀs in terms of area and delay. •Design and compare implementation as a bit-sliced, serial, pipelined, or tree-based design, again in- cluding reasoning about the relevant design tradeoﬀs in terms of ar ea and delay. •Design and compare implementations of processor control units us ing both hardwired and micropro- grammed strategies, and again including reasoning about the releva nt design tradeoﬀs in terms of area and delay. •Understandbasictradeoﬀsinthesparsityofcodewordswith erro rdetectionandcorrectioncapabilities.'}"
