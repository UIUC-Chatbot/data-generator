text,metadata
"Why the Book Happened
This textbook evolved from EECS 100, the rst computing course for computer
science, computer engineering, and electrical engineering majors at the Univer-
sity of Michigan, Ann Arbor, that Kevin Compton and the rst author introduced
for the rst time in the fall term, 1995.
EECS 100 happened at Michigan because Computer Science and Engi-
neering faculty had been dissatised for many years with the lack of student
comprehension of some very basic concepts. For example, students had a lot
of trouble with pointer variables. Recursion seemed to be magic, beyond
understanding.
We decided in 1993 that the conventional wisdom of starting with a high-
level programming language, which was the way we (and most universities) were","{'page_number': 0, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Why the Book Happened\nThis textbook evolved from EECS 100, the rst computing course for computer\nscience, computer engineering, and electrical engineering majors at the Univer-\nsity of Michigan, Ann Arbor, that Kevin Compton and the rst author introduced\nfor the rst time in the fall term, 1995.\nEECS 100 happened at Michigan because Computer Science and Engi-\nneering faculty had been dissatised for many years with the lack of student\ncomprehension of some very basic concepts. For example, students had a lot\nof trouble with pointer variables. Recursion seemed to be magic, beyond\nunderstanding.\nWe decided in 1993 that the conventional wisdom of starting with a high-\nlevel programming language, which was the way we (and most universities) were'}"
"doing it, had its shortcomings. We decided that the reason students were not get-
ting it was that they were forced to memorize technical details when they did not
understand the basic underpinnings.
Our result was the bottom-up approach taken in this book, where we contin-
ually build on what the student already knows, only memorizing when absolutely
necessary. We did not endorse then and we do not endorse now the popular
information hiding approach when it comes to learning. Information hiding is a
useful productivity enhancement technique after one understands what is going on.
But until one gets to that point, we insist that information hiding gets in the way of
understanding. Thus, we continually build on what has gone before so that nothing
is magic and everything can be tied to the foundation that has already been laid.
We should point out that we do not disagree with the notion of top-down
design. On the contrary, we believe strongly that top-down design is correct
design. But there is a clear dierence between how one approaches a design prob-
lem (after one understands the underlying building blocks) and what it takes to get
to the point where one does understand the building blocks. In short, we believe
in top-down design, but bottom-up learning for understanding.
Major Changes in the Third Edition
The LC-3
A hallmark of our book continues to be the LC-3 ISA, which is small enough to
be described in a few pages and hopefully mastered in a very short time, yet rich
enough to convey the essence of what an ISA provides. It is the LC 3 because
it took us three tries to get it right. Four tries, actually, but the two changes in the
LC-3 ISA since the second edition (i.e., changes to the LEA instruction and to the
TRAP instruction) are so minor that we decided not to call the slightly modied
ISA the LC-4.
The LEA instruction no longer sets condition codes. It used to set condition
codes on the mistaken belief that since LEA stands for Load Eective Address,
it should set condition codes like LD, LDI, and LDR do. We recognize now that
this reason was silly. LD, LDI, and LDR load a register from memory, and so
the condition codes provide useful information  whether the value loaded is
negative, zero, or positive. LEA loads an address into a register, and for that, the
condition codes do not really provide any value. Legacy code written before this
change should still run correctly.
The TRAP instruction no longer stores the linkage back to the calling pro-
gram in R7. Instead, the PC and PSR are pushed onto the system stack and popped
by the RTI instruction (renamed Return from Trap or Interrupt) as the last instruc-
tion in a trap routine. Trap routines now execute in privileged memory (x0000 to
x2FFF). This change allows trap routines to be re-entrant. It does not aect old
code provided the starting address of the trap service routines, obtained from the
Trap Vector Table, is in privileged memory and the terminating instruction of
each trap service routine is changed from RET to RTI.
As before, Appendix A species the LC-3 completely","{'page_number': 1, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Trap routines now execute in privileged memory (x0000 to\nx2FFF). This change allows trap routines to be re-entrant. It does not aect old\ncode provided the starting address of the trap service routines, obtained from the\nTrap Vector Table, is in privileged memory and the terminating instruction of\neach trap service routine is changed from RET to RTI.\nAs before, Appendix A species the LC-3 completely.'}"
"Trap routines now execute in privileged memory (x0000 to
x2FFF). This change allows trap routines to be re-entrant. It does not aect old
code provided the starting address of the trap service routines, obtained from the
Trap Vector Table, is in privileged memory and the terminating instruction of
each trap service routine is changed from RET to RTI.
As before, Appendix A species the LC-3 completely.","{'page_number': 1, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Trap routines now execute in privileged memory (x0000 to\nx2FFF). This change allows trap routines to be re-entrant. It does not aect old\ncode provided the starting address of the trap service routines, obtained from the\nTrap Vector Table, is in privileged memory and the terminating instruction of\neach trap service routine is changed from RET to RTI.\nAs before, Appendix A species the LC-3 completely.'}"
"The Addition of C++
Weve had an ongoing debate about how to extend our approach and textbook
to C++. One of the concerns about C++ is that many of its language features
are too far abstracted from the underlying layers to make for an easy t to our
approach. Another concern is that C++ is such a vast language that any adequate
coverage would require an additional thousand pages. We also didnt want to drop
C completely, as it serves as a de facto development language for systems and
hardware-oriented projects.
We adopted an approach where we cover the common core of C and C++
from Chapters 11 through 19. This common core is similar to what was covered
in the second edition, with some minor updates. Chapter 20 serves as a transition,
which we aspired to make very smooth, to the core concepts of C++. With this
approach, we get to explore the evolution between C and C++, which serves as
a key learning opportunity on what changes were essential to boost programmer
productivity.
In particular, we focus on classes in C++ as an evolution from structures in
C. We discuss classes as a compiler construct, how method calls are made, and
the notion of constructors. We touch upon inheritance, too, but leave the details
for subsequent treatment in follow-on courses.
An important element of C++ is the introduction of container classes in the
Standard Template Library, which is a heavily utilized part of the C++ language.
This provides an opportunity to dive deep into the vector class, which serves as
a continuation of a running example in the second half around the support for
variable-sized arrays in high-level languages, or in particular, Cs lack of support
for them.
Other Important Updates
Although no chapter in the book has remained untouched, some chapters have
been changed more than others. In Chapter 2, we expanded the coverage of the
oating point data type and the conversion of fractions between decimal and
binary numbers in response to several instructors who wanted them. We moved
DeMorgans Laws from Chapter 3 to Chapter 2 because the concept is really about
AND and OR functions and not about digital logic implementation. In Chap-
ter 3, we completely overhauled the description of state, latches, ip-ops, nite
state machines, and our example of a danger sign. We felt the explanations in the
second edition were not as clear as they needed to be, and the concepts are too
important to not get right. We revised Chapter 4 to better introduce the LC-3,
including a dierent set of instructions, leading to our rst complete example of
a computer program.
Our organization of Chapters 8, 9, and 10 was completely overhauled in order
to present essentially the same material in a more understandable way. Although
most of our treatment of data structures waits until we have introduced C in the
second half of the book, we felt it was important to introduce stacks, queues,
and character strings as soon as the students have moved out of programming in
machine language so they can write programs dealing with these data structures","{'page_number': 2, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The Addition of C++\nWeve had an ongoing debate about how to extend our approach and textbook\nto C++. One of the concerns about C++ is that many of its language features\nare too far abstracted from the underlying layers to make for an easy t to our\napproach. Another concern is that C++ is such a vast language that any adequate\ncoverage would require an additional thousand pages. We also didnt want to drop\nC completely, as it serves as a de facto development language for systems and\nhardware-oriented projects.\nWe adopted an approach where we cover the common core of C and C++\nfrom Chapters 11 through 19. This common core is similar to what was covered\nin the second edition, with some minor updates. Chapter 20 serves as a transition,\nwhich we aspired to make very smooth, to the core concepts of C++. With this\napproach, we get to explore the evolution between C and C++, which serves as\na key learning opportunity on what changes were essential to boost programmer\nproductivity.\nIn particular, we focus on classes in C++ as an evolution from structures in\nC. We discuss classes as a compiler construct, how method calls are made, and\nthe notion of constructors. We touch upon inheritance, too, but leave the details\nfor subsequent treatment in follow-on courses.\nAn important element of C++ is the introduction of container classes in the\nStandard Template Library, which is a heavily utilized part of the C++ language.\nThis provides an opportunity to dive deep into the vector class, which serves as\na continuation of a running example in the second half around the support for\nvariable-sized arrays in high-level languages, or in particular, Cs lack of support\nfor them.\nOther Important Updates\nAlthough no chapter in the book has remained untouched, some chapters have\nbeen changed more than others. In Chapter 2, we expanded the coverage of the\noating point data type and the conversion of fractions between decimal and\nbinary numbers in response to several instructors who wanted them. We moved\nDeMorgans Laws from Chapter 3 to Chapter 2 because the concept is really about\nAND and OR functions and not about digital logic implementation. In Chap-\nter 3, we completely overhauled the description of state, latches, ip-ops, nite\nstate machines, and our example of a danger sign. We felt the explanations in the\nsecond edition were not as clear as they needed to be, and the concepts are too\nimportant to not get right. We revised Chapter 4 to better introduce the LC-3,\nincluding a dierent set of instructions, leading to our rst complete example of\na computer program.\nOur organization of Chapters 8, 9, and 10 was completely overhauled in order\nto present essentially the same material in a more understandable way. Although\nmost of our treatment of data structures waits until we have introduced C in the\nsecond half of the book, we felt it was important to introduce stacks, queues,\nand character strings as soon as the students have moved out of programming in\nmachine language so they can write programs dealing with these data structures'}"
"and see how these structures are actually organized in memory. We moved our dis-
cussion of subroutines up to Chapter 8 because of their importance in constructing
richer programs.
We also introduced recursion in Chapter 8, although its main treatment is still
left for the second half of the book. Both the expressive power of recursion and
its misuse are so common in undergraduate curricula that we felt dealing with
it twice, rst while they are engrossed in the bowels of assembly language and
again after moving up to the richness of C, was worthwhile.
Chapter 9 now covers all aspects of I/O in one place, including polling and
interrupt-driven I/O. Although the concept of privilege is present in the second
edition, we have put greater emphasis on it in the third edition. Our coverage
of system calls (the trap routines invoked by the TRAP instruction) appears in
Chapter 9. All of the above reduce Chapter 10 to simply a comprehensive example
that pulls together a lot of the rst half of the book: the simulation of a calculator.
Doing so requires 12 subroutines that are laid out in complete detail. Two con-
cepts that are needed to make this happen are stack arithmetic and ASCII/binary
conversion, so they are included in Chapter 10.
We reworked all the examples in Chapters 11 through 19 to use the latest
ANSI Standard C or C18. We also added more coding examples to further empha-
size points and to provide clarity on complex topics such as pointers, arrays,
recursion, and pointers to pointers in C. In Chapter 16, we added additional
sections on variable-sized arrays in C, and on multidimensional arrays.
Chapter Organization
The book breaks down into two major segments, (a) the underlying structure
of a computer, as manifested in the LC-3; and (b) programming in a high-level
language, in our case C and C++.
The LC-3
We start with the underpinnings that are needed to understand the workings of a
real computer. Chapter 2 introduces the bit and arithmetic and logical operations
on bits. Then we begin to build the structure needed to understand the LC-3.
Chapter 3 takes the student from an MOS transistor, step by step, to a real
memory and a nite state machine.
Our real memory consists of four words of three bits each, rather than
16 gigabytes, which is common in most laptops today. Its description ts on a
single page (Figure 3.20), making it easy for a student to grasp. By the time stu-
dents get there, they have been exposed to all the elements needed to construct the
memory. The nite state machine is needed to understand how a computer pro-
cesses instructions, starting in Chapter 4. Chapter 4 introduces the von Neumann
execution model and enough LC-3 instructions to allow an LC-3 program to be
written. Chapter 5 introduces most of the rest of the LC-3 ISA.","{'page_number': 3, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'and see how these structures are actually organized in memory. We moved our dis-\ncussion of subroutines up to Chapter 8 because of their importance in constructing\nricher programs.\nWe also introduced recursion in Chapter 8, although its main treatment is still\nleft for the second half of the book. Both the expressive power of recursion and\nits misuse are so common in undergraduate curricula that we felt dealing with\nit twice, rst while they are engrossed in the bowels of assembly language and\nagain after moving up to the richness of C, was worthwhile.\nChapter 9 now covers all aspects of I/O in one place, including polling and\ninterrupt-driven I/O. Although the concept of privilege is present in the second\nedition, we have put greater emphasis on it in the third edition. Our coverage\nof system calls (the trap routines invoked by the TRAP instruction) appears in\nChapter 9. All of the above reduce Chapter 10 to simply a comprehensive example\nthat pulls together a lot of the rst half of the book: the simulation of a calculator.\nDoing so requires 12 subroutines that are laid out in complete detail. Two con-\ncepts that are needed to make this happen are stack arithmetic and ASCII/binary\nconversion, so they are included in Chapter 10.\nWe reworked all the examples in Chapters 11 through 19 to use the latest\nANSI Standard C or C18. We also added more coding examples to further empha-\nsize points and to provide clarity on complex topics such as pointers, arrays,\nrecursion, and pointers to pointers in C. In Chapter 16, we added additional\nsections on variable-sized arrays in C, and on multidimensional arrays.\nChapter Organization\nThe book breaks down into two major segments, (a) the underlying structure\nof a computer, as manifested in the LC-3; and (b) programming in a high-level\nlanguage, in our case C and C++.\nThe LC-3\nWe start with the underpinnings that are needed to understand the workings of a\nreal computer. Chapter 2 introduces the bit and arithmetic and logical operations\non bits. Then we begin to build the structure needed to understand the LC-3.\nChapter 3 takes the student from an MOS transistor, step by step, to a real\nmemory and a nite state machine.\nOur real memory consists of four words of three bits each, rather than\n16 gigabytes, which is common in most laptops today. Its description ts on a\nsingle page (Figure 3.20), making it easy for a student to grasp. By the time stu-\ndents get there, they have been exposed to all the elements needed to construct the\nmemory. The nite state machine is needed to understand how a computer pro-\ncesses instructions, starting in Chapter 4. Chapter 4 introduces the von Neumann\nexecution model and enough LC-3 instructions to allow an LC-3 program to be\nwritten. Chapter 5 introduces most of the rest of the LC-3 ISA.'}"
"The LC-3 is a 16-bit architecture that includes physical I/O via keyboard
and monitor, TRAPs to the operating system for handling service calls, con-
ditional branches on (N, Z, and P) condition codes, a subroutine call/return
mechanism, a minimal set of operate instructions (ADD, AND, and NOT), and
various addressing modes for loads and stores (direct, indirect, Base+oset).
Chapter 6 is devoted to programming methodology (stepwise renement)
and debugging, and Chapter 7 is an introduction to assembly language program-
ming. We have developed a simulator and an assembler for the LC-3 that runs on
Windows, Linux, and Mac0S platforms. It can be downloaded from the web at
no charge.
Students use the simulator to test and debug programs written in LC-3
machine language and in LC-3 assembly language. The simulator allows online
debugging (deposit, examine, single-step, set breakpoint, and so on). The sim-
ulator can be used for simple LC-3 machine language and assembly language
programming assignments, which are essential for students to master the concepts
presented throughout the rst ten chapters.
Assembly language is taught, but not to train expert assembly language pro-
grammers. Indeed, if the purpose was to train assembly language programmers,
the material would be presented in an upper-level course, not in an introductory
course for freshmen. Rather, the material is presented in Chapter 7 because it
is consistent with the paradigm of the book. In our bottom-up approach, by the
time the student reaches Chapter 7, he/she can handle the process of transform-
ing assembly language programs to sequences of 0s and 1s. We go through the
process of assembly step by step for a very simple LC-3 Assembler. By hand
assembling, the student (at a very small additional cost in time) reinforces the
important fundamental concept of translation.
It is also the case that assembly language provides a user-friendly notation
to describe machine instructions, something that is particularly useful for writing
programs in Chapters 8, 9, and 10, and for providing many of the explanations in
the second half of the book. Starting in Chapter 11, when we teach the semantics
of C statements, it is far easier for the reader to deal with ADD R1, R2, R3 than
to have to struggle with 0001001010000011.
Chapter 8 introduces three important data structures: the stack, the queue,
and the character string, and shows how they are stored in memory. The sub-
routine call/return mechanism of the LC-3 is presented because of its usefulness
both in manipulating these data structures and in writing programs. We also intro-
duce recursion, a powerful construct that we revisit much more thoroughly in the
second half of the book (in Chapter 17), after the student has acquired a much
stronger capability in high-level language programming. We introduce recursion
here to show by means of a few examples the execution-time tradeos incurred
with recursion as a rst step in understanding when its use makes sense and when
it doesnt","{'page_number': 4, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'We also intro-\nduce recursion, a powerful construct that we revisit much more thoroughly in the\nsecond half of the book (in Chapter 17), after the student has acquired a much\nstronger capability in high-level language programming. We introduce recursion\nhere to show by means of a few examples the execution-time tradeos incurred\nwith recursion as a rst step in understanding when its use makes sense and when\nit doesnt.\nChapter 9 deals with input/output and some basic interaction between the\nprocessor and the operating system. We introduce the notions of priority and\nprivilege, which are central to a systems environment. Our treatment of I/O is\nall physical, using keyboard data and status registers for input and display data\nand status registers for output. We describe both interrupt-driven I/O and I/O'}"
"We also intro-
duce recursion, a powerful construct that we revisit much more thoroughly in the
second half of the book (in Chapter 17), after the student has acquired a much
stronger capability in high-level language programming. We introduce recursion
here to show by means of a few examples the execution-time tradeos incurred
with recursion as a rst step in understanding when its use makes sense and when
it doesnt.
Chapter 9 deals with input/output and some basic interaction between the
processor and the operating system. We introduce the notions of priority and
privilege, which are central to a systems environment. Our treatment of I/O is
all physical, using keyboard data and status registers for input and display data
and status registers for output. We describe both interrupt-driven I/O and I/O","{'page_number': 4, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'We also intro-\nduce recursion, a powerful construct that we revisit much more thoroughly in the\nsecond half of the book (in Chapter 17), after the student has acquired a much\nstronger capability in high-level language programming. We introduce recursion\nhere to show by means of a few examples the execution-time tradeos incurred\nwith recursion as a rst step in understanding when its use makes sense and when\nit doesnt.\nChapter 9 deals with input/output and some basic interaction between the\nprocessor and the operating system. We introduce the notions of priority and\nprivilege, which are central to a systems environment. Our treatment of I/O is\nall physical, using keyboard data and status registers for input and display data\nand status registers for output. We describe both interrupt-driven I/O and I/O'}"
"under program control. Both are supported by our LC-3 simulator so the student
can write interrupt drivers. Finally, we show the actual LC-3 code of the trap ser-
vice routines that the student has invoked with the TRAP instruction starting in
Chapter 4. To handle interrupt-driven I/O and trap service routines, we complete
the description of the LC-3 ISA with details of the operation of the Return from
Trap or Interrupt (RTI) and TRAP instructions.
The rst half of the book concludes with Chapter 10, a comprehensive exam-
ple of a simple calculator that pulls together a lot of what the students have learned
in Chapters 1 through 9.
Programming in C and C++
By the time the student gets to the second part of the textbook, he/she has an
understanding of the layers below. In our coverage of programming in C and
C++, we leverage this foundation by showing the resulting LC-3 code generated
by a compiler with each new concept in C/C++.
We start with the C language because it provides the common, essential
core with C++. The C programming language ts very nicely with our bottom-
up approach. Its low-level nature allows students to see clearly the connection
between software and the underlying hardware. In this book, we focus on basic
concepts such as control structures, functions, and arrays. Once basic program-
ming concepts are mastered, it is a short step for students to learn more advanced
concepts such as objects and abstraction in C++.
Each time a new high-level construct is introduced, the student is shown
the LC-3 code that a compiler would produce. We cover the basic constructs of
C (variables, operators, control, and functions), pointers, arrays, recursion, I/O,
complex data structures, and dynamic allocation. With C++, we cover some basic
improvements over C, classes, and containers.
Chapter 11 is a gentle introduction to high-level programming languages. At
this point, students have dealt heavily with assembly language and can understand
the motivation behind what high-level programming languages provide. Chapter
11 also contains a simple C program, which we use to kick-start the process of
learning C.
Chapter 12 deals with values, variables, constants, and operators. Chapter 13
introduces C control structures. We provide many complete program examples
to give students a sample of how each of these concepts is used in practice. LC-3
code is used to demonstrate how each C construct aects the machine at the lower
levels.
Chapter 14 introduces functions in C. Students are not merely exposed to the
syntax of functions. Rather they learn how functions are actually executed, with
argument-passing using a run-time stack. A number of examples are provided.
In Chapter 15, students are exposed to techniques for testing their code, along
with debugging high-level source code. The ideas of white-box and black-box
testing are discussed.
Chapter 16 teaches pointers and arrays, relying heavily on the students
understanding of how memory is organized. We discuss Cs notions of xed size
and variable-length arrays, along with multidimensional array allocation.","{'page_number': 5, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'under program control. Both are supported by our LC-3 simulator so the student\ncan write interrupt drivers. Finally, we show the actual LC-3 code of the trap ser-\nvice routines that the student has invoked with the TRAP instruction starting in\nChapter 4. To handle interrupt-driven I/O and trap service routines, we complete\nthe description of the LC-3 ISA with details of the operation of the Return from\nTrap or Interrupt (RTI) and TRAP instructions.\nThe rst half of the book concludes with Chapter 10, a comprehensive exam-\nple of a simple calculator that pulls together a lot of what the students have learned\nin Chapters 1 through 9.\nProgramming in C and C++\nBy the time the student gets to the second part of the textbook, he/she has an\nunderstanding of the layers below. In our coverage of programming in C and\nC++, we leverage this foundation by showing the resulting LC-3 code generated\nby a compiler with each new concept in C/C++.\nWe start with the C language because it provides the common, essential\ncore with C++. The C programming language ts very nicely with our bottom-\nup approach. Its low-level nature allows students to see clearly the connection\nbetween software and the underlying hardware. In this book, we focus on basic\nconcepts such as control structures, functions, and arrays. Once basic program-\nming concepts are mastered, it is a short step for students to learn more advanced\nconcepts such as objects and abstraction in C++.\nEach time a new high-level construct is introduced, the student is shown\nthe LC-3 code that a compiler would produce. We cover the basic constructs of\nC (variables, operators, control, and functions), pointers, arrays, recursion, I/O,\ncomplex data structures, and dynamic allocation. With C++, we cover some basic\nimprovements over C, classes, and containers.\nChapter 11 is a gentle introduction to high-level programming languages. At\nthis point, students have dealt heavily with assembly language and can understand\nthe motivation behind what high-level programming languages provide. Chapter\n11 also contains a simple C program, which we use to kick-start the process of\nlearning C.\nChapter 12 deals with values, variables, constants, and operators. Chapter 13\nintroduces C control structures. We provide many complete program examples\nto give students a sample of how each of these concepts is used in practice. LC-3\ncode is used to demonstrate how each C construct aects the machine at the lower\nlevels.\nChapter 14 introduces functions in C. Students are not merely exposed to the\nsyntax of functions. Rather they learn how functions are actually executed, with\nargument-passing using a run-time stack. A number of examples are provided.\nIn Chapter 15, students are exposed to techniques for testing their code, along\nwith debugging high-level source code. The ideas of white-box and black-box\ntesting are discussed.\nChapter 16 teaches pointers and arrays, relying heavily on the students\nunderstanding of how memory is organized. We discuss Cs notions of xed size\nand variable-length arrays, along with multidimensional array allocation.'}"
"Chapter 17 teaches recursion, using the students newly gained knowledge of
functions, stack frames, and the run-time stack. Chapter 18 introduces the details
of I/O functions in C, in particular, streams, variable length argument lists, and
how C I/O is aected by the various format specications. This chapter relies on
the students earlier exposure to physical I/O in Chapter 8. Chapter 19 discusses
structures in C, dynamic memory allocation, and linked lists.
Chapter 20 provides a jump-start on C++ programming by discussing its
roots in C and introducing the idea of classes as a natural evolution from struc-
tures. We also cover the idea of containers in the standard template library, to
enable students to quickly jump into productive programming with C++.
Along the way, we have tried to emphasize good programming style and cod-
ing methodology by means of examples. Novice programmers probably learn at
least as much from the programming examples they read as from the rules they
are forced to study. Insights that accompany these examples are highlighted by
means of lightbulb icons that are included in the margins.
We have found that the concept of pointer variables (Chapter 16) is not at all
a problem. By the time students encounter it, they have a good understanding of
what memory is all about, since they have analyzed the logic design of a small
memory (Chapter 3). They know the dierence, for example, between a memory
locations address and the data stored there.
Recursion ceases to be magic since, by the time a student gets to that point
(Chapter 17), he/she has already encountered all the underpinnings. Students
understand how stacks work at the machine level (Chapter 8), and they understand
the call/return mechanism from their LC-3 machine language programming expe-
rience, and the need for linkages between a called program and the return to the
caller (Chapter 8). From this foundation, it is not a large step to explain functions
by introducing run-time stack frames (Chapter 14), with a lot of the mystery about
argument passing, dynamic declarations, and so on, going away. Since a function
can call a function, it is one additional small step (certainly no magic involved)
for a function to call itself.
The Simulator/Debugger
The importance of the Simulator/Debugger for testing the programs a student
writes cannot be overemphasized. We believe strongly that there is no substi-
tute for hands-on practice testing ones knowledge. It is incredibly fullling
to a students education to write a program that does not work, testing it to
nd out why it does not work, xing the bugs himself/herself, and then see-
ing the program run correctly. To that end, the Simulator/Debugger has been
completely rewritten. It runs on Windows, Linux, and MacOS while present-
ing the same user interface (GUI) regardless of which platform the student is
using. We have improved our incorporation of interrupt-driven I/O into the Sim-
ulators functionality so students can easily write interrupt drivers and invoke
them by interrupting a lower priority executing program. ...in their rst course in
computing!","{'page_number': 6, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Chapter 17 teaches recursion, using the students newly gained knowledge of\nfunctions, stack frames, and the run-time stack. Chapter 18 introduces the details\nof I/O functions in C, in particular, streams, variable length argument lists, and\nhow C I/O is aected by the various format specications. This chapter relies on\nthe students earlier exposure to physical I/O in Chapter 8. Chapter 19 discusses\nstructures in C, dynamic memory allocation, and linked lists.\nChapter 20 provides a jump-start on C++ programming by discussing its\nroots in C and introducing the idea of classes as a natural evolution from struc-\ntures. We also cover the idea of containers in the standard template library, to\nenable students to quickly jump into productive programming with C++.\nAlong the way, we have tried to emphasize good programming style and cod-\ning methodology by means of examples. Novice programmers probably learn at\nleast as much from the programming examples they read as from the rules they\nare forced to study. Insights that accompany these examples are highlighted by\nmeans of lightbulb icons that are included in the margins.\nWe have found that the concept of pointer variables (Chapter 16) is not at all\na problem. By the time students encounter it, they have a good understanding of\nwhat memory is all about, since they have analyzed the logic design of a small\nmemory (Chapter 3). They know the dierence, for example, between a memory\nlocations address and the data stored there.\nRecursion ceases to be magic since, by the time a student gets to that point\n(Chapter 17), he/she has already encountered all the underpinnings. Students\nunderstand how stacks work at the machine level (Chapter 8), and they understand\nthe call/return mechanism from their LC-3 machine language programming expe-\nrience, and the need for linkages between a called program and the return to the\ncaller (Chapter 8). From this foundation, it is not a large step to explain functions\nby introducing run-time stack frames (Chapter 14), with a lot of the mystery about\nargument passing, dynamic declarations, and so on, going away. Since a function\ncan call a function, it is one additional small step (certainly no magic involved)\nfor a function to call itself.\nThe Simulator/Debugger\nThe importance of the Simulator/Debugger for testing the programs a student\nwrites cannot be overemphasized. We believe strongly that there is no substi-\ntute for hands-on practice testing ones knowledge. It is incredibly fullling\nto a students education to write a program that does not work, testing it to\nnd out why it does not work, xing the bugs himself/herself, and then see-\ning the program run correctly. To that end, the Simulator/Debugger has been\ncompletely rewritten. It runs on Windows, Linux, and MacOS while present-\ning the same user interface (GUI) regardless of which platform the student is\nusing. We have improved our incorporation of interrupt-driven I/O into the Sim-\nulators functionality so students can easily write interrupt drivers and invoke\nthem by interrupting a lower priority executing program. ...in their rst course in\ncomputing!'}"
"education, but we feel they are better suited to a later course in computer
architecture and design. This book is not intended for that purpose.
Why LC-3, and Not ARM or RISCV?
We have been asked why we invented the LC-3 ISA, rather than going with ARM,
which seems to be the ISA of choice for most mobile devices, or RISCV, which
has attracted substantial interest over the last few years.
There are many reasons. First, we knew that the ISA we selected would
be the students rst ISA, not his/her last ISA. Between the freshman year and
graduation, the student is likely to encounter several ISAs, most of which are in
commercial products: ARM, RISCV, x86, and POWER, to name a few.
But all the commercial ISAs have details that have no place in an introductory
course but still have to be understood for the student to use them eectively. We
could, of course, have subset an existing ISA, but that always ends up in questions
of what to take out and what to leave in with a result that is not as clean as one
would think at rst blush. Certainly not as clean as what one can get when starting
from scratch. It also creates an issue whenever the student uses an instruction in
an exam or on an assignment that is not in the subset. Not very clean from a
pedagogical sense.
We wanted an ISA that was clean with no special cases to deal with, with as
few opcodes as necessary so the student could spend almost all his/her time on
the fundamental concepts in the course and very little time on the nuances of the
instruction set. The formats of all instructions in the LC-3 t on a single page.
Appendix A provides all the details (i.e., the complete data sheet) of the entire
LC-3 ISA in 25 pages.
We also wanted an instruction set that in addition to containing only a few
instructions was very rich in the breadth of what it embraced. So, we came up
with the LC-3, an instruction set with only 15 four-bit opcodes, a small enough
number that students can absorb the ISA without even trying. For arithmetic, we
have only ADD instead of ADD, SUB, MUL, and DIV. For logical operations,
we have AND and NOT, foregoing OR, XOR, etc. We have no shift or rotate
instructions. In all these cases, the missing opcodes can be implemented with
procedures using the few opcodes that the LC-3 provides. We have loads and
stores with three dierent addressing modes, each addressing mode useful for a
dierent purpose. We have conditional branches, subroutine calls, return from
trap or interrupt, and system calls (the TRAP instruction).
In fact, this sparse set of opcodes is a feature! It drives home the need for
creating more complex functionality out of simple operations, and the need for
abstraction, both of which are core concepts in the book.
Most importantly, we have found from discussions with hundreds of students
that starting with the LC-3 does not put them at a disadvantage in later courses","{'page_number': 7, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In fact, this sparse set of opcodes is a feature! It drives home the need for\ncreating more complex functionality out of simple operations, and the need for\nabstraction, both of which are core concepts in the book.\nMost importantly, we have found from discussions with hundreds of students\nthat starting with the LC-3 does not put them at a disadvantage in later courses.\nOn the contrary: For example, at one campus students were introduced to ARM in\nthe follow-on course, while at another campus, students were introduced to x86.'}"
"In fact, this sparse set of opcodes is a feature! It drives home the need for
creating more complex functionality out of simple operations, and the need for
abstraction, both of which are core concepts in the book.
Most importantly, we have found from discussions with hundreds of students
that starting with the LC-3 does not put them at a disadvantage in later courses.
On the contrary: For example, at one campus students were introduced to ARM in
the follow-on course, while at another campus, students were introduced to x86.","{'page_number': 7, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In fact, this sparse set of opcodes is a feature! It drives home the need for\ncreating more complex functionality out of simple operations, and the need for\nabstraction, both of which are core concepts in the book.\nMost importantly, we have found from discussions with hundreds of students\nthat starting with the LC-3 does not put them at a disadvantage in later courses.\nOn the contrary: For example, at one campus students were introduced to ARM in\nthe follow-on course, while at another campus, students were introduced to x86.'}"
"In both cases, students appreciated starting with the LC-3, and their subsequent
introduction to ARM or x86 was much easier as a result of their rst learning the
fundamental concepts with the LC-3.
A Few Observations
Having now taught the course more than 20 times between us, we note the
following:
Understanding, Not Memorizing
Since the course builds from the bottom up, we have found that less memorization
of seemingly arbitrary rules is required than in traditional programming courses.
Students understand that the rules make sense since by the time a topic is taught,
they have an awareness of how that topic is implemented at the levels below
it. This approach is good preparation for later courses in design, where under-
standing of and insights gained from fundamental underpinnings are essential to
making the required design tradeos.
The Student Debugs the Students Program
We hear complaints from industry all the time about CS graduates not being able
to program. Part of the problem is the helpful teaching assistant, who contributes
far too much of the intellectual content of the students program so the student
never has to really master the art. Our approach is to push the student to do the
job without the teaching assistant (TA). Part of this comes from the bottom-up
approach, where memorizing is minimized and the student builds on what he/she
already knows. Part of this is the simulator, which the student uses from the day
he/she writes his/her rst program. The student is taught debugging from his/her
rst program and is required from the very beginning to use the debugging tools
of the simulator to get his/her programs to work. The combination of the simulator
and the order in which the subject material is taught results in students actually
debugging their own programs instead of taking their programs to the TA for
help ... with the too-frequent result that the TAs end up writing the programs for
the students.
Preparation for the Future: Cutting Through Protective Layers
Professionals who use computers in systems today but remain ignorant of what
is going on underneath are likely to discover the hard way that the eectiveness
of their solutions is impacted adversely by things other than the actual programs
they write. This is true for the sophisticated computer programmer as well as the
sophisticated engineer.
Serious programmers will write more ecient code if they understand what
is going on beyond the statements in their high-level language. Engineers, and not
just computer engineers, are having to interact with their computer systems today","{'page_number': 8, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In both cases, students appreciated starting with the LC-3, and their subsequent\nintroduction to ARM or x86 was much easier as a result of their rst learning the\nfundamental concepts with the LC-3.\nA Few Observations\nHaving now taught the course more than 20 times between us, we note the\nfollowing:\nUnderstanding, Not Memorizing\nSince the course builds from the bottom up, we have found that less memorization\nof seemingly arbitrary rules is required than in traditional programming courses.\nStudents understand that the rules make sense since by the time a topic is taught,\nthey have an awareness of how that topic is implemented at the levels below\nit. This approach is good preparation for later courses in design, where under-\nstanding of and insights gained from fundamental underpinnings are essential to\nmaking the required design tradeos.\nThe Student Debugs the Students Program\nWe hear complaints from industry all the time about CS graduates not being able\nto program. Part of the problem is the helpful teaching assistant, who contributes\nfar too much of the intellectual content of the students program so the student\nnever has to really master the art. Our approach is to push the student to do the\njob without the teaching assistant (TA). Part of this comes from the bottom-up\napproach, where memorizing is minimized and the student builds on what he/she\nalready knows. Part of this is the simulator, which the student uses from the day\nhe/she writes his/her rst program. The student is taught debugging from his/her\nrst program and is required from the very beginning to use the debugging tools\nof the simulator to get his/her programs to work. The combination of the simulator\nand the order in which the subject material is taught results in students actually\ndebugging their own programs instead of taking their programs to the TA for\nhelp ... with the too-frequent result that the TAs end up writing the programs for\nthe students.\nPreparation for the Future: Cutting Through Protective Layers\nProfessionals who use computers in systems today but remain ignorant of what\nis going on underneath are likely to discover the hard way that the eectiveness\nof their solutions is impacted adversely by things other than the actual programs\nthey write. This is true for the sophisticated computer programmer as well as the\nsophisticated engineer.\nSerious programmers will write more ecient code if they understand what\nis going on beyond the statements in their high-level language. Engineers, and not\njust computer engineers, are having to interact with their computer systems today'}"
"come Aboard
1.1 What We Will Try to Do
Welcome to From Bits and Gates to C and Beyond. Our intent is to introduce
you over the next xxx pages to the world of computing. As we do so, we have
one objective above all others: to show you very clearly that there is no magic to
computing. The computer is a deterministic systemevery time we hit it over the
head in the same way and in the same place (provided, of course, it was in the same
starting condition), we get the same response. The computer is not an electronic
genius; on the contrary, if anything, it is an electronic idiot, doing exactly what
we tell it to do. It has no mind of its own.
What appears to be a very complex organism is really just a very large, sys-
tematically interconnected collection of very simple parts. Our job throughout
this book is to introduce you to those very simple parts and, step-by-step, build the
interconnected structure that you know by the name computer. Like a house, we
will start at the bottom, construct the foundation rst, and then go on to add layer
after layer, as we get closer and closer to what most people know as a full-blown
computer. Each time we add a layer, we will explain what we are doing, tying the
new ideas to the underlying fabric. Our goal is that when we are done, you will be
able to write programs in a computer language such as C using the sophisticated
features of that language and to understand what is going on underneath, inside
the computer.
1.2 How We Will Get There
We will start (in Chapter 2) by rst showing that any information processed by
the computer is represented by a sequence of 0s and 1s. That is, we will encode
all information as sequences of 0s and 1s. For example, one encoding of the letter
a that is commonly used is the sequence 01100001. One encoding of the decimal
number 35 is the sequence 00100011. We will see how to perform operations on
such encoded information.","{'page_number': 9, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'come Aboard\n1.1 What We Will Try to Do\nWelcome to From Bits and Gates to C and Beyond. Our intent is to introduce\nyou over the next xxx pages to the world of computing. As we do so, we have\none objective above all others: to show you very clearly that there is no magic to\ncomputing. The computer is a deterministic systemevery time we hit it over the\nhead in the same way and in the same place (provided, of course, it was in the same\nstarting condition), we get the same response. The computer is not an electronic\ngenius; on the contrary, if anything, it is an electronic idiot, doing exactly what\nwe tell it to do. It has no mind of its own.\nWhat appears to be a very complex organism is really just a very large, sys-\ntematically interconnected collection of very simple parts. Our job throughout\nthis book is to introduce you to those very simple parts and, step-by-step, build the\ninterconnected structure that you know by the name computer. Like a house, we\nwill start at the bottom, construct the foundation rst, and then go on to add layer\nafter layer, as we get closer and closer to what most people know as a full-blown\ncomputer. Each time we add a layer, we will explain what we are doing, tying the\nnew ideas to the underlying fabric. Our goal is that when we are done, you will be\nable to write programs in a computer language such as C using the sophisticated\nfeatures of that language and to understand what is going on underneath, inside\nthe computer.\n1.2 How We Will Get There\nWe will start (in Chapter 2) by rst showing that any information processed by\nthe computer is represented by a sequence of 0s and 1s. That is, we will encode\nall information as sequences of 0s and 1s. For example, one encoding of the letter\na that is commonly used is the sequence 01100001. One encoding of the decimal\nnumber 35 is the sequence 00100011. We will see how to perform operations on\nsuch encoded information.'}"
"Once we are comfortable with information represented as codes made up of
0s and 1s and operations (addition, for example) being performed on these repre-
sentations, we will begin the process of showing how a computer works. Starting
in Chapter 3, we will note that the computer is a piece of electronic equipment
and, as such, consists of electronic parts operated by voltages and interconnected
by wires. Every wire in the computer, at every moment in time, is at either a high
voltage or a low voltage. For our representation of 0s and 1s, we do not specify
exactly how high. We only care whether there is or is not a large enough voltage
relative to 0 volts to identify it as a 1. That is, the absence or presence of a rea-
sonable voltage relative to 0 volts is what determines whether it represents the
value 0 or the value 1.
In Chapter 3, we will see how the transistors that make up todays micro-
processor (the heart of the modern computer) work. We will further see how
those transistors are combined into larger structures that perform operations,
such as addition, and into structures that allow us to save information for later
use. In Chapter 4, we will combine these larger structures into the von Neumann
machine, a basic model that describes how a computer works. We will also begin
to study a simple computer, the LC-3. We will continue our study of the LC-3 in
Chapter 5. LC-3 stands for Little Computer 3. We actually started with LC-1 but
needed two more shots at it before (we think) we got it right! The LC-3 has all
the important characteristics of the microprocessors that you may have already
heard of, for example, the Intel 8088, which was used in the rst IBM PCs back
in 1981. Or the Motorola 68000, which was used in the Macintosh, vintage 1984.
Or the Pentium IV, one of the high-performance microprocessors of choice for
the PC in the year 2003. Or todays laptop and desktop microprocessors, the Intel
Core processors  I3, I5, and I7. Or even the ARM microprocessors that are used
in most smartphones today. That is, the LC-3 has all the important characteristics
of these real microprocessors without being so complicated that it gets in the
way of your understanding.
Once we understand how the LC-3 works, the next step is to program it, rst
in its own language (Chapter 5 and Chapter 6), and then in a language called
assembly language that is a little bit easier for humans to work with (Chap-
ter 7). Chapter 8 introduces representations of information more complex than a
simple number  stacks, queues, and character strings, and shows how to imple-
ment them. Chapter 9 deals with the problem of getting information into (input)
and out of (output) the LC-3. Chapter 9 also deals with services provided to a
computer user by the operating system. We conclude the rst half of the book
(Chapter 10) with an extensive example, the simulation of a calculator, an app on
most smartphones today","{'page_number': 10, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Chapter 9 deals with the problem of getting information into (input)\nand out of (output) the LC-3. Chapter 9 also deals with services provided to a\ncomputer user by the operating system. We conclude the rst half of the book\n(Chapter 10) with an extensive example, the simulation of a calculator, an app on\nmost smartphones today.\nIn the second half of the book (Chapters 1120), we turn our attention\nto high-level programming concepts, which we introduce via the C and C++\nprogramming languages. High-level languages enable programmers to more\neectively develop complex software by abstracting away the details of the under-\nlying hardware. C and C++ in particular oer a rich set of programmer-friendly\nconstructs, but they are close enough to the hardware that we can examine\nhow code is transformed to execute on the layers below. Our goal is to enable\nyou to write short, simple programs using the core parts of these programming'}"
"Chapter 9 deals with the problem of getting information into (input)
and out of (output) the LC-3. Chapter 9 also deals with services provided to a
computer user by the operating system. We conclude the rst half of the book
(Chapter 10) with an extensive example, the simulation of a calculator, an app on
most smartphones today.
In the second half of the book (Chapters 1120), we turn our attention
to high-level programming concepts, which we introduce via the C and C++
programming languages. High-level languages enable programmers to more
eectively develop complex software by abstracting away the details of the under-
lying hardware. C and C++ in particular oer a rich set of programmer-friendly
constructs, but they are close enough to the hardware that we can examine
how code is transformed to execute on the layers below. Our goal is to enable
you to write short, simple programs using the core parts of these programming","{'page_number': 10, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Chapter 9 deals with the problem of getting information into (input)\nand out of (output) the LC-3. Chapter 9 also deals with services provided to a\ncomputer user by the operating system. We conclude the rst half of the book\n(Chapter 10) with an extensive example, the simulation of a calculator, an app on\nmost smartphones today.\nIn the second half of the book (Chapters 1120), we turn our attention\nto high-level programming concepts, which we introduce via the C and C++\nprogramming languages. High-level languages enable programmers to more\neectively develop complex software by abstracting away the details of the under-\nlying hardware. C and C++ in particular oer a rich set of programmer-friendly\nconstructs, but they are close enough to the hardware that we can examine\nhow code is transformed to execute on the layers below. Our goal is to enable\nyou to write short, simple programs using the core parts of these programming'}"
"languages, all the while being able to comprehend the transformations required
for your code to execute on the underlying hardware.
Well start with basic topics in C such as variables and operators (Chapter 12),
control structures (Chapter 13), and functions (Chapter 14). Well see that these are
straightforward extensions of concepts introduced in the first half of the textbook.
We then move on to programming concepts in Chapters 1519 that will enable
us to create more powerful pieces of code: Testing and Debugging (Chapter 15),
Pointers and Arrays in C (Chapter 16), Recursion (Chapter 17), Input and Output in
C (Chapter 18), and Data Structures in C (Chapter 19).
Chapters 20 is devoted to C++, which we present as an evolution of the
C programming language. Because the C++ language was initially dened as
a superset of C, many of the concepts covered in Chapters 1119 directly map
onto the C++ language. We will introduce some of the core notions in C++ that
have helped establish C++ as one of the most popular languages for developing
real-world software. Chapter 20 is our Introduction to C++.
In almost all cases, we try to tie high-level C and C++ constructs to the
underlying LC-3 so that you will understand what you demand of the computer
when you use a particular construct in a C or C++ program.
1.3 Two Recurring Themes
Two themes permeate this book that we as professors previously took for granted,
assuming that everyone recognized their value and regularly emphasized them
to students of engineering and computer science. However, it has become clear
to us that from the git-go, we need to make these points explicit. So, we state
them here up front. The two themes are (a) the notion of abstraction and (b) the
importance of not separating in your mind the notions of hardware and software.
Their value to your development as an eective engineer or computer scien-
tist goes well beyond your understanding of how a computer works and how to
program it.
The notion of abstraction is central to all that you will learn and expect to
use in practicing your craft, whether it be in mathematics, physics, any aspect of
engineering, or business. It is hard to think of any body of knowledge where the
notion of abstraction is not critical.
The misguided hardware/software separation is directly related to your
continuing study of computers and your work with them.
We will discuss each in turn.
1.3.1 The Notion of Abstraction
The use of abstraction is all around us. When we get in a taxi and tell the driver,
Take me to the airport, we are using abstraction. If we had to, we could probably
direct the driver each step of the way: Go down this street ten blocks, and make
a left turn. And, when the driver got there, Now take this street ve blocks and
make a right turn. And on and on. You know the details, but it is a lot quicker to
just tell the driver to take you to the airport.","{'page_number': 11, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'languages, all the while being able to comprehend the transformations required\nfor your code to execute on the underlying hardware.\nWell start with basic topics in C such as variables and operators (Chapter 12),\ncontrol structures (Chapter 13), and functions (Chapter 14). Well see that these are\nstraightforward extensions of concepts introduced in the first half of the textbook.\nWe then move on to programming concepts in Chapters 1519 that will enable\nus to create more powerful pieces of code: Testing and Debugging (Chapter 15),\nPointers and Arrays in C (Chapter 16), Recursion (Chapter 17), Input and Output in\nC (Chapter 18), and Data Structures in C (Chapter 19).\nChapters 20 is devoted to C++, which we present as an evolution of the\nC programming language. Because the C++ language was initially dened as\na superset of C, many of the concepts covered in Chapters 1119 directly map\nonto the C++ language. We will introduce some of the core notions in C++ that\nhave helped establish C++ as one of the most popular languages for developing\nreal-world software. Chapter 20 is our Introduction to C++.\nIn almost all cases, we try to tie high-level C and C++ constructs to the\nunderlying LC-3 so that you will understand what you demand of the computer\nwhen you use a particular construct in a C or C++ program.\n1.3 Two Recurring Themes\nTwo themes permeate this book that we as professors previously took for granted,\nassuming that everyone recognized their value and regularly emphasized them\nto students of engineering and computer science. However, it has become clear\nto us that from the git-go, we need to make these points explicit. So, we state\nthem here up front. The two themes are (a) the notion of abstraction and (b) the\nimportance of not separating in your mind the notions of hardware and software.\nTheir value to your development as an eective engineer or computer scien-\ntist goes well beyond your understanding of how a computer works and how to\nprogram it.\nThe notion of abstraction is central to all that you will learn and expect to\nuse in practicing your craft, whether it be in mathematics, physics, any aspect of\nengineering, or business. It is hard to think of any body of knowledge where the\nnotion of abstraction is not critical.\nThe misguided hardware/software separation is directly related to your\ncontinuing study of computers and your work with them.\nWe will discuss each in turn.\n1.3.1 The Notion of Abstraction\nThe use of abstraction is all around us. When we get in a taxi and tell the driver,\nTake me to the airport, we are using abstraction. If we had to, we could probably\ndirect the driver each step of the way: Go down this street ten blocks, and make\na left turn. And, when the driver got there, Now take this street ve blocks and\nmake a right turn. And on and on. You know the details, but it is a lot quicker to\njust tell the driver to take you to the airport.'}"
"Even the statement Go down this street ten blocks  can be broken down
further with instructions on using the accelerator, the steering wheel, watching
out for other vehicles, pedestrians, etc.
Abstraction is a technique for establishing a simpler way for a person to inter-
act with a system, removing the details that are unnecessary for the person to
interact eectively with that system. Our ability to abstract is very much a pro-
ductivity enhancer. It allows us to deal with a situation at a higher level, focusing
on the essential aspects, while keeping the component ideas in the background.
It allows us to be more ecient in our use of time and brain activity. It allows us
to not get bogged down in the detail when everything about the detail is working
just ne.
There is an underlying assumption to this, however: when everything about
the detail is just ne. What if everything about the detail is not just ne? Then,
to be successful, our ability to abstract must be combined with our ability to
un-abstract. Some people use the word deconstructthe ability to go from the
abstraction back to its component parts.
Two stories come to mind.
The rst involves a trip through Arizona the rst author made a long time
ago in the hottest part of the summer. At the time he was living in Palo Alto,
California, where the temperature tends to be mild almost always. He knew
enough to take the car to a mechanic before making the trip and tell him to check
the cooling system. That was the abstraction: cooling system. What he had not
mastered was that the capability of a cooling system for Palo Alto, California,
is not the same as the capability of a cooling system for the summer deserts of
Arizona. The result: two days in Deer Lodge, Arizona (population 3), waiting for
a head gasket to be shipped in.
The second story (perhaps apocryphal) is supposed to have happened during
the infancy of electric power generation. General Electric Co. was having trouble
with one of its huge electric power generators and did not know what to do. On
the front of the generator were lots of dials containing lots of information, and
lots of screws that could be rotated clockwise or counterclockwise as the operator
wished. Something on the other side of the wall of dials and screws was malfunc-
tioning and no one knew what to do. As the story goes, they called in one of the
early giants in the electric power industry. He looked at the dials and listened to
the noises for a minute, then took a small screwdriver from his pocket and rotated
one screw 35 degrees counterclockwise. The problem immediately went away. He
submitted a bill for $1000 (a lot of money in those days) without any elaboration.
The controller found the bill for two minutes work a little unsettling and asked
for further clarication. Back came the new bill:
Turning a screw 35 degrees counterclockwise:
$
0.75
Knowing which screw to turn and by how much:
999.25
In both stories the message is the same. It is more ecient to think of entities
as abstractions. One does not want to get bogged down in details unnecessarily","{'page_number': 12, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The controller found the bill for two minutes work a little unsettling and asked\nfor further clarication. Back came the new bill:\nTurning a screw 35 degrees counterclockwise:\n$\n0.75\nKnowing which screw to turn and by how much:\n999.25\nIn both stories the message is the same. It is more ecient to think of entities\nas abstractions. One does not want to get bogged down in details unnecessarily.\nAnd as long as nothing untoward happens, we are OK. If there had been no trip\nto Arizona, the abstraction cooling system would have been sucient. If the'}"
"The controller found the bill for two minutes work a little unsettling and asked
for further clarication. Back came the new bill:
Turning a screw 35 degrees counterclockwise:
$
0.75
Knowing which screw to turn and by how much:
999.25
In both stories the message is the same. It is more ecient to think of entities
as abstractions. One does not want to get bogged down in details unnecessarily.
And as long as nothing untoward happens, we are OK. If there had been no trip
to Arizona, the abstraction cooling system would have been sucient. If the","{'page_number': 12, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The controller found the bill for two minutes work a little unsettling and asked\nfor further clarication. Back came the new bill:\nTurning a screw 35 degrees counterclockwise:\n$\n0.75\nKnowing which screw to turn and by how much:\n999.25\nIn both stories the message is the same. It is more ecient to think of entities\nas abstractions. One does not want to get bogged down in details unnecessarily.\nAnd as long as nothing untoward happens, we are OK. If there had been no trip\nto Arizona, the abstraction cooling system would have been sucient. If the'}"
"electric power generator never malfunctioned, there would have been no need for
the power engineering gurus deeper understanding.
As we will see, modern computers are composed of transistors. These tran-
sistors are combined to form logic gatesan abstraction that lets us think in
terms of 0s and 1s instead of the varying voltages on the transistors. A logic cir-
cuit is a further abstraction of a combination of gates. When one designs a logic
circuit out of gates, it is much more ecient to not have to think about the inter-
nals of each gate. To do so would slow down the process of designing the logic
circuit. One wants to think of the gate as a component. But if there is a problem
with getting the logic circuit to work, it is often helpful to look at the internal
structure of the gate and see if something about its functioning is causing the
problem.
When one designs a sophisticated computer application program, whether it
be a new spreadsheet program, word processing system, or computer game, one
wants to think of each of the components one is using as an abstraction. If one
spent time thinking about the details of each component when it was not neces-
sary, the distraction could easily prevent the total job from ever getting nished.
But when there is a problem putting the components together, it is often useful to
examine carefully the details of each component in order to uncover the problem.
The ability to abstract is the most important skill. In our view, one should
try to keep the level of abstraction as high as possible, consistent with getting
everything to work eectively. Our approach in this book is to continually raise
the level of abstraction. We describe logic gates in terms of transistors. Once we
understand the abstraction of gates, we no longer think in terms of transistors.
Then we build larger structures out of gates. Once we understand these larger
abstractions, we no longer think in terms of gates.
The Bottom Line
Abstractions allow us to be much more ecient in dealing
with all kinds of situations. It is also true that one can be eective without under-
standing what is below the abstraction as long as everything behaves nicely. So,
one should not pooh-pooh the notion of abstraction. On the contrary, one should
celebrate it since it allows us to be more ecient.
In fact, if we never have to combine a component with anything else into a
larger system, and if nothing can go wrong with the component, then it is perfectly
ne to understand this component only at the level of its abstraction.
But if we have to combine multiple components into a larger system, we
should be careful not to allow their abstractions to be the deepest level of our
understanding. If we dont know the components below the level of their abstrac-
tions, then we are at the mercy of them working together without our intervention.
If they dont work together, and we are unable to go below the level of abstraction,
we are stuck. And that is the state we should take care not to nd ourselves in.
1.3.2 Hardware vs. Software
Many computer scientists and engineers refer to themselves as hardware people
or software people. By hardware, they generally mean the physical computer and","{'page_number': 13, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'electric power generator never malfunctioned, there would have been no need for\nthe power engineering gurus deeper understanding.\nAs we will see, modern computers are composed of transistors. These tran-\nsistors are combined to form logic gatesan abstraction that lets us think in\nterms of 0s and 1s instead of the varying voltages on the transistors. A logic cir-\ncuit is a further abstraction of a combination of gates. When one designs a logic\ncircuit out of gates, it is much more ecient to not have to think about the inter-\nnals of each gate. To do so would slow down the process of designing the logic\ncircuit. One wants to think of the gate as a component. But if there is a problem\nwith getting the logic circuit to work, it is often helpful to look at the internal\nstructure of the gate and see if something about its functioning is causing the\nproblem.\nWhen one designs a sophisticated computer application program, whether it\nbe a new spreadsheet program, word processing system, or computer game, one\nwants to think of each of the components one is using as an abstraction. If one\nspent time thinking about the details of each component when it was not neces-\nsary, the distraction could easily prevent the total job from ever getting nished.\nBut when there is a problem putting the components together, it is often useful to\nexamine carefully the details of each component in order to uncover the problem.\nThe ability to abstract is the most important skill. In our view, one should\ntry to keep the level of abstraction as high as possible, consistent with getting\neverything to work eectively. Our approach in this book is to continually raise\nthe level of abstraction. We describe logic gates in terms of transistors. Once we\nunderstand the abstraction of gates, we no longer think in terms of transistors.\nThen we build larger structures out of gates. Once we understand these larger\nabstractions, we no longer think in terms of gates.\nThe Bottom Line\nAbstractions allow us to be much more ecient in dealing\nwith all kinds of situations. It is also true that one can be eective without under-\nstanding what is below the abstraction as long as everything behaves nicely. So,\none should not pooh-pooh the notion of abstraction. On the contrary, one should\ncelebrate it since it allows us to be more ecient.\nIn fact, if we never have to combine a component with anything else into a\nlarger system, and if nothing can go wrong with the component, then it is perfectly\nne to understand this component only at the level of its abstraction.\nBut if we have to combine multiple components into a larger system, we\nshould be careful not to allow their abstractions to be the deepest level of our\nunderstanding. If we dont know the components below the level of their abstrac-\ntions, then we are at the mercy of them working together without our intervention.\nIf they dont work together, and we are unable to go below the level of abstraction,\nwe are stuck. And that is the state we should take care not to nd ourselves in.\n1.3.2 Hardware vs. Software\nMany computer scientists and engineers refer to themselves as hardware people\nor software people. By hardware, they generally mean the physical computer and'}"
"all the specications associated with it. By software, they generally mean the pro-
grams, whether operating systems like Android, ChromeOS, Linux, or Windows,
or database systems like Access, MongoDB, Oracle, or DB-terric, or applica-
tion programs like Facebook, Chrome, Excel, or Word. The implication is that
the person knows a whole lot about one of these two things and precious little
about the other. Usually, there is the further implication that it is OK to be an
expert at one of these (hardware OR software) and clueless about the other. It is
as if there were a big wall between the hardware (the computer and how it actu-
ally works) and the software (the programs that direct the computer to do their
bidding), and that one should be content to remain on one side of that wall or
the other.
The power of abstraction allows us to usually operate at a level where we
do not have to think about the underlying layers all the time. This is a good thing.
It enables us to be more productive. But if we are clueless about the underlying
layers, then we are not able to take advantage of the nuances of those underlying
layers when it is very important to be able to.
That is not to say that you must work at the lower level of abstraction and not
take advantage of the productivity enhancements that the abstractions provide.
On the contrary, you are encouraged to work at the highest level of abstraction
available to you. But in doing so, if you are able to, at the same time, keep in
mind the underlying levels, you will nd yourself able to do a much better job.
As you approach your study and practice of computing, we urge you to take
the approach that hardware and software are names for components of two parts
of a computing system that work best when they are designed by people who take
into account the capabilities and limitations of both.
Microprocessor designers who understand the needs of the programs that
will execute on the microprocessor they are designing can design much more
eective microprocessors than those who dont. For example, Intel, AMD, ARM,
and other major producers of microprocessors recognized a few years ago that a
large fraction of future programs would contain video clips as part of e-mail,
video games, and full-length movies. They recognized that it would be impor-
tant for such programs to execute eciently. The result: most microprocessors
today contain special hardware capability to process those video clips. Intel
dened additional instructions, initially called their MMX instruction set, and
developed special hardware for it. Motorola, IBM, and Apple did essentially
the same thing, resulting in the AltiVec instruction set and special hardware to
support it.
A similar story can be told about software designers. The designer of a large
computer program who understands the capabilities and limitations of the hard-
ware that will carry out the tasks of that program can design the program so it
executes more eciently than the designer who does not understand the nature of
the hardware. One important task that almost all large software systems need to
carry out is called sorting, where a number of items have to be arranged in some
order","{'page_number': 14, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'A similar story can be told about software designers. The designer of a large\ncomputer program who understands the capabilities and limitations of the hard-\nware that will carry out the tasks of that program can design the program so it\nexecutes more eciently than the designer who does not understand the nature of\nthe hardware. One important task that almost all large software systems need to\ncarry out is called sorting, where a number of items have to be arranged in some\norder. The words in a dictionary are arranged in alphabetical order. Students in\na class are often graded based on a numerical order, according to their scores\non the nal exam. There is a large number of fundamentally dierent programs\none can write to arrange a collection of items in order. Donald Knuth, one of the'}"
"A similar story can be told about software designers. The designer of a large
computer program who understands the capabilities and limitations of the hard-
ware that will carry out the tasks of that program can design the program so it
executes more eciently than the designer who does not understand the nature of
the hardware. One important task that almost all large software systems need to
carry out is called sorting, where a number of items have to be arranged in some
order. The words in a dictionary are arranged in alphabetical order. Students in
a class are often graded based on a numerical order, according to their scores
on the nal exam. There is a large number of fundamentally dierent programs
one can write to arrange a collection of items in order. Donald Knuth, one of the","{'page_number': 14, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'A similar story can be told about software designers. The designer of a large\ncomputer program who understands the capabilities and limitations of the hard-\nware that will carry out the tasks of that program can design the program so it\nexecutes more eciently than the designer who does not understand the nature of\nthe hardware. One important task that almost all large software systems need to\ncarry out is called sorting, where a number of items have to be arranged in some\norder. The words in a dictionary are arranged in alphabetical order. Students in\na class are often graded based on a numerical order, according to their scores\non the nal exam. There is a large number of fundamentally dierent programs\none can write to arrange a collection of items in order. Donald Knuth, one of the'}"
"top computer scientists in the world, devoted 391 pages to the task in The Art
of Computer Programming, vol. 3. Which sorting program works best is often
very dependent on how much the software designer is aware of the underlying
characteristics of the hardware.
The Bottom Line
We believe that whether your inclinations are in the direction
of a computer hardware career or a computer software career, you will be much
more capable if you master both. This book is about getting you started on the
path to mastering both hardware and software. Although we sometimes ignore
making the point explicitly when we are in the trenches of working through a
concept, it really is the case that each sheds light on the other.
When you study data types, a software concept, in C (Chapter 12), you will
understand how the nite word length of the computer, a hardware concept,
aects our notion of data types.
When you study functions in C (Chapter 14), you will be able to tie the rules
of calling a function with the hardware implementation that makes those rules
necessary.
When you study recursion, a powerful algorithmic device (initially in
Chapter 8 and more extensively in Chapter 17), you will be able to tie it to the
hardware. If you take the time to do that, you will better understand when the
additional time to execute a procedure recursively is worth it.
When you study pointer variables in C (in Chapter 16), your knowledge of
computer memory will provide a deeper understanding of what pointers pro-
vide, and very importantly, when they should be used and when they should be
avoided.
When you study data structures in C (in Chapter 19), your knowledge of com-
puter memory will help you better understand what must be done to manipulate
the actual structures in memory eciently.
We realize that most of the terms in the preceding ve short paragraphs may
not be familiar to you yet. That is OK; you can reread this page at the end of the
semester. What is important to know right now is that there are important topics
in the software that are very deeply interwoven with topics in the hardware. Our
contention is that mastering either is easier if you pay attention to both.
Most importantly, most computing problems yield better solutions when the
problem solver has the capability of both at his or her disposal.
1.4 A Computer System
We have used the word computer more than two dozen times in the preceding
pages, and although we did not say so explicitly, we used it to mean a system
consisting of the software (i.e., computer programs) that directs and species the
processing of information and the hardware that performs the actual processing
of information in response to what the software asks the hardware to do. When
we say performing the actual processing, we mean doing the actual additions,
multiplications, and so forth in the hardware that are necessary to get the job","{'page_number': 15, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'top computer scientists in the world, devoted 391 pages to the task in The Art\nof Computer Programming, vol. 3. Which sorting program works best is often\nvery dependent on how much the software designer is aware of the underlying\ncharacteristics of the hardware.\nThe Bottom Line\nWe believe that whether your inclinations are in the direction\nof a computer hardware career or a computer software career, you will be much\nmore capable if you master both. This book is about getting you started on the\npath to mastering both hardware and software. Although we sometimes ignore\nmaking the point explicitly when we are in the trenches of working through a\nconcept, it really is the case that each sheds light on the other.\nWhen you study data types, a software concept, in C (Chapter 12), you will\nunderstand how the nite word length of the computer, a hardware concept,\naects our notion of data types.\nWhen you study functions in C (Chapter 14), you will be able to tie the rules\nof calling a function with the hardware implementation that makes those rules\nnecessary.\nWhen you study recursion, a powerful algorithmic device (initially in\nChapter 8 and more extensively in Chapter 17), you will be able to tie it to the\nhardware. If you take the time to do that, you will better understand when the\nadditional time to execute a procedure recursively is worth it.\nWhen you study pointer variables in C (in Chapter 16), your knowledge of\ncomputer memory will provide a deeper understanding of what pointers pro-\nvide, and very importantly, when they should be used and when they should be\navoided.\nWhen you study data structures in C (in Chapter 19), your knowledge of com-\nputer memory will help you better understand what must be done to manipulate\nthe actual structures in memory eciently.\nWe realize that most of the terms in the preceding ve short paragraphs may\nnot be familiar to you yet. That is OK; you can reread this page at the end of the\nsemester. What is important to know right now is that there are important topics\nin the software that are very deeply interwoven with topics in the hardware. Our\ncontention is that mastering either is easier if you pay attention to both.\nMost importantly, most computing problems yield better solutions when the\nproblem solver has the capability of both at his or her disposal.\n1.4 A Computer System\nWe have used the word computer more than two dozen times in the preceding\npages, and although we did not say so explicitly, we used it to mean a system\nconsisting of the software (i.e., computer programs) that directs and species the\nprocessing of information and the hardware that performs the actual processing\nof information in response to what the software asks the hardware to do. When\nwe say performing the actual processing, we mean doing the actual additions,\nmultiplications, and so forth in the hardware that are necessary to get the job'}"
"done. A more precise term for this hardware is a central processing unit (CPU),
or simply a processor or microprocessor. This textbook is primarily about the
processor and the programs that are executed by the processor.
1.4.1 A (Very) Little History for a (Lot) Better Perspective
Before we get into the detail of how the processor and the software associated
with it work, we should take a moment and note the enormous and unparalleled
leaps of performance that the computing industry has made in the relatively short
time computers have been around. After all, it wasnt until the 1940s that the
first computers showed their faces. One of the first computers was the ENIAC
(the Electronic Numerical Integrator and Calculator), a general purpose electronic
computer that could be reprogrammed for different tasks. It was designed and built
in 19431945 attheUniversityofPennsylvaniabyPresperEckertandhiscolleagues.
It contained more than 17,000 vacuum tubes. It was approximately 8 feet high, more
than 100 feet wide, and about 3 feet deep (about 300 square feet of floor space). It
weighed 30 tons and required 140 kW to operate. Figure 1.1 shows three operators
programming the ENIAC by plugging and unplugging cables and switches.
About 40 years and many computer companies and computers later, in the
early 1980s, the Burroughs A series was born. One of the dozen or so 18-inch
oards that comprise that machine is shown in Figure 1.2. Each board contained
Figure 1.1
The ENIAC, designed and built at University of Pennsylvania, 194345.
cHistorical/Getty Images","{'page_number': 16, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'done. A more precise term for this hardware is a central processing unit (CPU),\nor simply a processor or microprocessor. This textbook is primarily about the\nprocessor and the programs that are executed by the processor.\n1.4.1 A (Very) Little History for a (Lot) Better Perspective\nBefore we get into the detail of how the processor and the software associated\nwith it work, we should take a moment and note the enormous and unparalleled\nleaps of performance that the computing industry has made in the relatively short\ntime computers have been around. After all, it wasnt until the 1940s that the\nfirst computers showed their faces. One of the first computers was the ENIAC\n(the Electronic Numerical Integrator and Calculator), a general purpose electronic\ncomputer that could be reprogrammed for different tasks. It was designed and built\nin 19431945 attheUniversityofPennsylvaniabyPresperEckertandhiscolleagues.\nIt contained more than 17,000 vacuum tubes. It was approximately 8 feet high, more\nthan 100 feet wide, and about 3 feet deep (about 300 square feet of floor space). It\nweighed 30 tons and required 140 kW to operate. Figure 1.1 shows three operators\nprogramming the ENIAC by plugging and unplugging cables and switches.\nAbout 40 years and many computer companies and computers later, in the\nearly 1980s, the Burroughs A series was born. One of the dozen or so 18-inch\noards that comprise that machine is shown in Figure 1.2. Each board contained\nFigure 1.1\nThe ENIAC, designed and built at University of Pennsylvania, 194345.\ncHistorical/Getty Images'}"
"Figure 1.2
A processor board, vintage 1980s. Courtesy of Emilio Salguerio
Fast forward another 30 or so years and we nd many of todays computers on
desktops (Figure 1.3), in laptops (Figure 1.4), and most recently in smartphones
(Figure 1.5). Their relative weights and energy requirements have decreased
enormousl
and the s eed at which the
rocess information has also increased
eno
how
com
Figu
A de
cJo
Futu
Shutterstock","{'page_number': 17, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure 1.2\nA processor board, vintage 1980s. Courtesy of Emilio Salguerio\nFast forward another 30 or so years and we nd many of todays computers on\ndesktops (Figure 1.3), in laptops (Figure 1.4), and most recently in smartphones\n(Figure 1.5). Their relative weights and energy requirements have decreased\nenormousl\nand the s eed at which the\nrocess information has also increased\neno\nhow\ncom\nFigu\nA de\ncJo\nFutu\nShutterstock'}"
"10
c
Figure 1.6
A microprocessor. cPeter Gudella/Shutterstock
The integrated circuit packages that comprise modern digital computers have
also seen phenomenal improvement. An example of one of todays microproces-
sors is shown in Figure 1.6. The rst microprocessor, the Intel 4004 in 1971,
contained 2300 transistors and operated at 106 KHz. By 1992, those numbers
had jumped to 3.1 million transistors at a frequency of 66 MHz on the Intel
Pentium microprocessor, an increase in both parameters of a factor of about 1000.
Todays microprocessors contain upwards of ve billion transistors and can oper-
ate at upwards of 4 GHz, another increase in both parameters of about a factor
of 1000.
This factor of one million since 1971 in both the number of transistors and
the frequency that the microprocessor operates at has had very important impli-
cations. The fact that each operation can be performed in one millionth of the
time it took in 1971 means the microprocessor can do one million things today
in the time it took to do one thing in 1971. The fact that there are more than a
million times as many transistors on a chip means we can do a lot more things at
the same time today than we could in 1971.
The result of all this is we have today computers that seem able to understand
the languages people speak  English, Spanish, Chinese, for example. We have
computers that seem able to recognize faces. Many see this as the magic of arti-
cial intelligence. We will see as we get into the details of how a computer works
that much of what appears to be magic is really due to how blazingly fast very
simple mindless operations (many at the same time) can be carried out.
1.4.2 The Parts of a Computer System
When most people use the word computer, they usually mean more than just
the processor (i.e., CPU) that is in charge of doing what the software directs.","{'page_number': 18, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '10\nc\nFigure 1.6\nA microprocessor. cPeter Gudella/Shutterstock\nThe integrated circuit packages that comprise modern digital computers have\nalso seen phenomenal improvement. An example of one of todays microproces-\nsors is shown in Figure 1.6. The rst microprocessor, the Intel 4004 in 1971,\ncontained 2300 transistors and operated at 106 KHz. By 1992, those numbers\nhad jumped to 3.1 million transistors at a frequency of 66 MHz on the Intel\nPentium microprocessor, an increase in both parameters of a factor of about 1000.\nTodays microprocessors contain upwards of ve billion transistors and can oper-\nate at upwards of 4 GHz, another increase in both parameters of about a factor\nof 1000.\nThis factor of one million since 1971 in both the number of transistors and\nthe frequency that the microprocessor operates at has had very important impli-\ncations. The fact that each operation can be performed in one millionth of the\ntime it took in 1971 means the microprocessor can do one million things today\nin the time it took to do one thing in 1971. The fact that there are more than a\nmillion times as many transistors on a chip means we can do a lot more things at\nthe same time today than we could in 1971.\nThe result of all this is we have today computers that seem able to understand\nthe languages people speak  English, Spanish, Chinese, for example. We have\ncomputers that seem able to recognize faces. Many see this as the magic of arti-\ncial intelligence. We will see as we get into the details of how a computer works\nthat much of what appears to be magic is really due to how blazingly fast very\nsimple mindless operations (many at the same time) can be carried out.\n1.4.2 The Parts of a Computer System\nWhen most people use the word computer, they usually mean more than just\nthe processor (i.e., CPU) that is in charge of doing what the software directs.'}"
"They usually mean the collection of parts that in combination form their computer
system. Today that computer system is often a laptop (see Figure 1.4), augmented
with many additional devices.
A computer system generally includes, in addition to the processor, a key-
board for typing commands, a mouse or keypad or joystick for positioning on
menu entries, a monitor for displaying information that the computer system has
produced, memory for temporarily storing information, disks and USB memory
sticks of one sort or another for storing information for a very long time, even after
the computer has been turned o, connections to other devices such as a printer
for obtaining paper copies of that information, and the collection of programs
(the software) that the user wishes to execute.
All these items help computer users to do their jobs. Without a printer, for
example, the user would have to copy by hand what is displayed on the monitor.
Without a mouse, keypad, or joystick, the user would have to type each command,
rather than simply position the mouse, keypad, or joystick.
So, as we begin our journey, which focuses on the CPU that occupies a small
fraction of 1 square inch of silicon and the software that makes the CPU do our
bidding, we note that the computer systems we use contain a lot of additional
components.
1.5 Two Very Important Ideas
Before we leave this rst chapter, there are two very important ideas that we
would like you to understand, ideas that are at the core of what computing is all
about.
Idea 1:
All computers (the biggest and the smallest, the fastest and the
slowest, the most expensive and the cheapest) are capable of comput-
ing exactly the same things if they are given enough time and enough
memory. That is, anything a fast computer can do, a slow computer can
do also. The slow computer just does it more slowly. A more expensive
computer cannot gure out something that a cheaper computer is unable
to gure out as long as the cheaper computer can access enough mem-
ory. (You may have to go to the store to buy more memory whenever it
runs out of memory in order to keep increasing memory.) All computers
can do exactly the same things. Some computers can do things faster,
but none can do more than any other.
Idea 2:
We describe our problems in English or some other language
spoken by people. Yet the problems are solved by electrons running
around inside the computer. It is necessary to transform our problem
from the language of humans to the voltages that inuence the ow of
electrons. This transformation is really a sequence of systematic trans-
formations, developed and improved over the last 70 years, which
combine to give the computer the ability to carry out what appear to","{'page_number': 19, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'They usually mean the collection of parts that in combination form their computer\nsystem. Today that computer system is often a laptop (see Figure 1.4), augmented\nwith many additional devices.\nA computer system generally includes, in addition to the processor, a key-\nboard for typing commands, a mouse or keypad or joystick for positioning on\nmenu entries, a monitor for displaying information that the computer system has\nproduced, memory for temporarily storing information, disks and USB memory\nsticks of one sort or another for storing information for a very long time, even after\nthe computer has been turned o, connections to other devices such as a printer\nfor obtaining paper copies of that information, and the collection of programs\n(the software) that the user wishes to execute.\nAll these items help computer users to do their jobs. Without a printer, for\nexample, the user would have to copy by hand what is displayed on the monitor.\nWithout a mouse, keypad, or joystick, the user would have to type each command,\nrather than simply position the mouse, keypad, or joystick.\nSo, as we begin our journey, which focuses on the CPU that occupies a small\nfraction of 1 square inch of silicon and the software that makes the CPU do our\nbidding, we note that the computer systems we use contain a lot of additional\ncomponents.\n1.5 Two Very Important Ideas\nBefore we leave this rst chapter, there are two very important ideas that we\nwould like you to understand, ideas that are at the core of what computing is all\nabout.\nIdea 1:\nAll computers (the biggest and the smallest, the fastest and the\nslowest, the most expensive and the cheapest) are capable of comput-\ning exactly the same things if they are given enough time and enough\nmemory. That is, anything a fast computer can do, a slow computer can\ndo also. The slow computer just does it more slowly. A more expensive\ncomputer cannot gure out something that a cheaper computer is unable\nto gure out as long as the cheaper computer can access enough mem-\nory. (You may have to go to the store to buy more memory whenever it\nruns out of memory in order to keep increasing memory.) All computers\ncan do exactly the same things. Some computers can do things faster,\nbut none can do more than any other.\nIdea 2:\nWe describe our problems in English or some other language\nspoken by people. Yet the problems are solved by electrons running\naround inside the computer. It is necessary to transform our problem\nfrom the language of humans to the voltages that inuence the ow of\nelectrons. This transformation is really a sequence of systematic trans-\nformations, developed and improved over the last 70 years, which\ncombine to give the computer the ability to carry out what appear to'}"
"be some very complicated tasks. In reality, these tasks are simple and
straightforward.
The rest of this chapter is devoted to discussing these two ideas.
1.6 Computers as Universal
Computational Devices
It may seem strange that an introductory textbook begins by describing how
computers work. After all, mechanical engineering students begin by studying
physics, not how car engines work. Chemical engineering students begin by
studying chemistry, not oil reneries. Why should computing students begin by
studying computers?
The answer is that computers are dierent. To learn the fundamental prin-
ciples of computing, you must study computers or machines that can do what
computers can do. The reason for this has to do with the notion that computers
are universal computational devices. Lets see what that means.
Before modern computers, there were many kinds of calculating machines.
Some were analog machinesmachines that produced an answer by measuring
some physical quantity such as distance or voltage. For example, a slide rule is
an analog machine that multiplies numbers by sliding one logarithmically graded
ruler next to another. The user can read a logarithmic distance on the sec-
ond ruler. Some early analog adding machines worked by dropping weights on a
scale. The diculty with analog machines is that it is very hard to increase their
accuracy.
This is why digital machinesmachines that perform computations by
manipulating a xed nite set of digits or letterscame to dominate comput-
ing. You are familiar with the distinction between analog and digital watches. An
analog watch has hour and minute hands, and perhaps a second hand. It gives
the time by the positions of its hands, which are really angular measures. Digital
watches give the time in digits. You can increase accuracy just by adding more
digits. For example, if it is important for you to measure time in hundredths of
a second, you can buy a watch that gives a reading like 10:35.16 rather than just
10:35. How would you get an analog watch that would give you an accurate read-
ing to one one-hundredth of a second? You could do it, but it would take a mighty
long second hand! When we talk about computers in this book, we will always
mean digital machines.
Before modern digital computers, the most common digital machines in the
West were adding machines. In other parts of the world another digital machine,
the abacus, was common. Digital adding machines were mechanical or elec-
tromechanical devices that could perform a specic kind of computation: adding
integers. There were also digital machines that could multiply integers. There
were digital machines that could put a stack of cards with punched names in
alphabetical order. The main limitation of all these machines is that they could
do only one specic kind of computation. If you owned only an adding machine","{'page_number': 20, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'be some very complicated tasks. In reality, these tasks are simple and\nstraightforward.\nThe rest of this chapter is devoted to discussing these two ideas.\n1.6 Computers as Universal\nComputational Devices\nIt may seem strange that an introductory textbook begins by describing how\ncomputers work. After all, mechanical engineering students begin by studying\nphysics, not how car engines work. Chemical engineering students begin by\nstudying chemistry, not oil reneries. Why should computing students begin by\nstudying computers?\nThe answer is that computers are dierent. To learn the fundamental prin-\nciples of computing, you must study computers or machines that can do what\ncomputers can do. The reason for this has to do with the notion that computers\nare universal computational devices. Lets see what that means.\nBefore modern computers, there were many kinds of calculating machines.\nSome were analog machinesmachines that produced an answer by measuring\nsome physical quantity such as distance or voltage. For example, a slide rule is\nan analog machine that multiplies numbers by sliding one logarithmically graded\nruler next to another. The user can read a logarithmic distance on the sec-\nond ruler. Some early analog adding machines worked by dropping weights on a\nscale. The diculty with analog machines is that it is very hard to increase their\naccuracy.\nThis is why digital machinesmachines that perform computations by\nmanipulating a xed nite set of digits or letterscame to dominate comput-\ning. You are familiar with the distinction between analog and digital watches. An\nanalog watch has hour and minute hands, and perhaps a second hand. It gives\nthe time by the positions of its hands, which are really angular measures. Digital\nwatches give the time in digits. You can increase accuracy just by adding more\ndigits. For example, if it is important for you to measure time in hundredths of\na second, you can buy a watch that gives a reading like 10:35.16 rather than just\n10:35. How would you get an analog watch that would give you an accurate read-\ning to one one-hundredth of a second? You could do it, but it would take a mighty\nlong second hand! When we talk about computers in this book, we will always\nmean digital machines.\nBefore modern digital computers, the most common digital machines in the\nWest were adding machines. In other parts of the world another digital machine,\nthe abacus, was common. Digital adding machines were mechanical or elec-\ntromechanical devices that could perform a specic kind of computation: adding\nintegers. There were also digital machines that could multiply integers. There\nwere digital machines that could put a stack of cards with punched names in\nalphabetical order. The main limitation of all these machines is that they could\ndo only one specic kind of computation. If you owned only an adding machine'}"
"and wanted to multiply two integers, you had some pencil-and-paper work
to do.
This is why computers are dierent. You can tell a computer how to add num-
bers. You can tell it how to multiply. You can tell it how to alphabetize a list or
perform any computation you like. When you think of a new kind of computation,
you do not have to buy or design a new computer. You just give the old computer
a new set of instructions (a program) to carry out the new computation. This is
why we say the computer is a universal computational device. Computer scien-
tists believe that anything that can be computed, can be computed by a computer
provided it has enough time and enough memory. When we study computers, we
study the fundamentals of all computing. We learn what computation is and what
can be computed.
The idea of a universal computational device is due to Alan Turing. Turing
proposed in 1937 that all computations could be carried out by a particular kind of
machine, which is now called a Turing machine. He gave a mathematical descrip-
tion of this kind of machine, but did not actually build one. Digital computers
were not operating until several years later. Turing was more interested in solv-
ing a philosophical problem: dening computation. He began by looking at the
kinds of actions that people perform when they compute; these include making
marks on paper, writing symbols according to certain rules when other symbols
are present, and so on. He abstracted these actions and specied a mechanism that
could carry them out. He gave some examples of the kinds of things that these
machines could do. One Turing machine could add two integers; another could
multiply two integers.
Figure 1.7 shows what we call black box models of Turing machines that
add and multiply. In each case, the operation to be performed is described in
the box. The data elements on which to operate are shown as inputs to the box.
The r
provi
there
a, b
(Turing machine
that adds)
(Turing machine
that multiplies)
Figure 1.7
Black box models of Turing machines.
Turing proposed that every computation can be performed by some Turing
machine. We call this Turings thesis. Although Turings thesis has never been
proved, there does exist a lot of evidence to suggest it is true. We know, for exam-
ple, that various enhancements one can make to Turing machines do not result in
machines that can compute more.
Perhaps the best argument to support Turings thesis was provided by Turing
himself in his original paper. He said that one way to try to construct a machine
more powerful than any particular Turing machine was to make a machine U
that could simulate all Turing machines. You would simply describe to U the","{'page_number': 21, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'and wanted to multiply two integers, you had some pencil-and-paper work\nto do.\nThis is why computers are dierent. You can tell a computer how to add num-\nbers. You can tell it how to multiply. You can tell it how to alphabetize a list or\nperform any computation you like. When you think of a new kind of computation,\nyou do not have to buy or design a new computer. You just give the old computer\na new set of instructions (a program) to carry out the new computation. This is\nwhy we say the computer is a universal computational device. Computer scien-\ntists believe that anything that can be computed, can be computed by a computer\nprovided it has enough time and enough memory. When we study computers, we\nstudy the fundamentals of all computing. We learn what computation is and what\ncan be computed.\nThe idea of a universal computational device is due to Alan Turing. Turing\nproposed in 1937 that all computations could be carried out by a particular kind of\nmachine, which is now called a Turing machine. He gave a mathematical descrip-\ntion of this kind of machine, but did not actually build one. Digital computers\nwere not operating until several years later. Turing was more interested in solv-\ning a philosophical problem: dening computation. He began by looking at the\nkinds of actions that people perform when they compute; these include making\nmarks on paper, writing symbols according to certain rules when other symbols\nare present, and so on. He abstracted these actions and specied a mechanism that\ncould carry them out. He gave some examples of the kinds of things that these\nmachines could do. One Turing machine could add two integers; another could\nmultiply two integers.\nFigure 1.7 shows what we call black box models of Turing machines that\nadd and multiply. In each case, the operation to be performed is described in\nthe box. The data elements on which to operate are shown as inputs to the box.\nThe r\nprovi\nthere\na, b\n(Turing machine\nthat adds)\n(Turing machine\nthat multiplies)\nFigure 1.7\nBlack box models of Turing machines.\nTuring proposed that every computation can be performed by some Turing\nmachine. We call this Turings thesis. Although Turings thesis has never been\nproved, there does exist a lot of evidence to suggest it is true. We know, for exam-\nple, that various enhancements one can make to Turing machines do not result in\nmachines that can compute more.\nPerhaps the best argument to support Turings thesis was provided by Turing\nhimself in his original paper. He said that one way to try to construct a machine\nmore powerful than any particular Turing machine was to make a machine U\nthat could simulate all Turing machines. You would simply describe to U the'}"
"particular Turing machine you wanted it to simulate, say a machine to add two
integers, give U the input data, and U would compute the appropriate output,
in this case the sum of the inputs. Turing then showed that there was, in fact,
a Turing machine that could do this, so even this attempt to nd something that
could not be computed by Turing machines failed.
e, f, g
v
Turing machine)
Figure 1.8
Black box model of a universal Turing machine.
In specifying U, Turing had provided us with a deep insight: He had given us
the rst description of what computers do. In fact, both a computer (with as much
memory as it wants) and a universal Turing machine can compute exactly the
same things. In both cases, you give the machine a description of a computation
and the data it needs, and the machine computes the appropriate answer. Comput-
ers and universal Turing machines can compute anything that can be computed
because they are programmable.
This is the reason that a big or expensive computer cannot do more than a
small, cheap computer. More money may buy you a faster computer, a monitor
with higher resolution, or a nice sound system. But if you have a small, cheap
computer, you already have a universal computational device.
1.7 How Do We Get the Electrons to
Do the Work?
Figure 1.9 shows the process we must go through to get the electrons (which
actually do the work) to do our bidding. We call the steps of this process the
Levels of Transformation. As we will see, at each level we have choices. If we
ignore any of the levels, our ability to make the best use of our computing system
can be very adversely aected.
1.7.1 The Statement of the Problem
We describe the problems we wish to solve in a natural language. Natural lan-
guages are languages that people speak, like English, French, Japanese, Italian,
and so on. They have evolved over centuries in accordance with their usage.
They are fraught with a lot of things unacceptable for providing instructions to a","{'page_number': 22, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'particular Turing machine you wanted it to simulate, say a machine to add two\nintegers, give U the input data, and U would compute the appropriate output,\nin this case the sum of the inputs. Turing then showed that there was, in fact,\na Turing machine that could do this, so even this attempt to nd something that\ncould not be computed by Turing machines failed.\ne, f, g\nv\nTuring machine)\nFigure 1.8\nBlack box model of a universal Turing machine.\nIn specifying U, Turing had provided us with a deep insight: He had given us\nthe rst description of what computers do. In fact, both a computer (with as much\nmemory as it wants) and a universal Turing machine can compute exactly the\nsame things. In both cases, you give the machine a description of a computation\nand the data it needs, and the machine computes the appropriate answer. Comput-\ners and universal Turing machines can compute anything that can be computed\nbecause they are programmable.\nThis is the reason that a big or expensive computer cannot do more than a\nsmall, cheap computer. More money may buy you a faster computer, a monitor\nwith higher resolution, or a nice sound system. But if you have a small, cheap\ncomputer, you already have a universal computational device.\n1.7 How Do We Get the Electrons to\nDo the Work?\nFigure 1.9 shows the process we must go through to get the electrons (which\nactually do the work) to do our bidding. We call the steps of this process the\nLevels of Transformation. As we will see, at each level we have choices. If we\nignore any of the levels, our ability to make the best use of our computing system\ncan be very adversely aected.\n1.7.1 The Statement of the Problem\nWe describe the problems we wish to solve in a natural language. Natural lan-\nguages are languages that people speak, like English, French, Japanese, Italian,\nand so on. They have evolved over centuries in accordance with their usage.\nThey are fraught with a lot of things unacceptable for providing instructions to a'}"
"15
Devices
Figure 1.9
Levels of transformation.
computer. Most important of these unacceptable attributes is ambiguity. Natural
language is lled with ambiguity. To infer the meaning of a sentence, a listener is
often helped by the tone of voice of the speaker, or at the very least, the context
of the sentence.
An example of ambiguity in English is the sentence, Time ies like an
arrow. At least three interpretations are possible, depending on whether (1) one is
noticing how fast time passes, (2) one is at a track meet for insects, or (3) one is
writing a letter to the Dear Abby of Insectville. In the rst case, a simile; one
is comparing the speed of time passing to the speed of an arrow that has been
released. In the second case, one is telling the timekeeper to do his/her job much
like an arrow would. In the third case, one is relating that a particular group of
ies (time ies, as opposed to fruit ies) are all in love with the same arrow.
Such ambiguity would be unacceptable in instructions provided to a com-
puter. The computer, electronic idiot that it is, can only do as it is told. To tell it to
do something where there are multiple interpretations would cause the computer
to not know which interpretation to follow.","{'page_number': 23, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '15\nDevices\nFigure 1.9\nLevels of transformation.\ncomputer. Most important of these unacceptable attributes is ambiguity. Natural\nlanguage is lled with ambiguity. To infer the meaning of a sentence, a listener is\noften helped by the tone of voice of the speaker, or at the very least, the context\nof the sentence.\nAn example of ambiguity in English is the sentence, Time ies like an\narrow. At least three interpretations are possible, depending on whether (1) one is\nnoticing how fast time passes, (2) one is at a track meet for insects, or (3) one is\nwriting a letter to the Dear Abby of Insectville. In the rst case, a simile; one\nis comparing the speed of time passing to the speed of an arrow that has been\nreleased. In the second case, one is telling the timekeeper to do his/her job much\nlike an arrow would. In the third case, one is relating that a particular group of\nies (time ies, as opposed to fruit ies) are all in love with the same arrow.\nSuch ambiguity would be unacceptable in instructions provided to a com-\nputer. The computer, electronic idiot that it is, can only do as it is told. To tell it to\ndo something where there are multiple interpretations would cause the computer\nto not know which interpretation to follow.'}"
"1.7.2 The Algorithm
The rst step in the sequence of transformations is to transform the natural lan-
guage description of the problem to an algorithm, and in so doing, get rid of the
objectionable characteristics of the natural language. An algorithm is a step-by-
step procedure that is guaranteed to terminate, such that each step is precisely
stated and can be carried out by the computer. There are terms to describe each
of these properties.
We use the term deniteness to describe the notion that each step is precisely
stated. A recipe for excellent pancakes that instructs the preparer to stir until
lumpy lacks deniteness, since the notion of lumpiness is not precise.
We use the term eective computability to describe the notion that each step
can be carried out by a computer. A procedure that instructs the computer to take
the largest prime number lacks eective computability, since there is no largest
prime number.
We use the term niteness to describe the notion that the procedure termi-
nates.
For every problem there are usually many dierent algorithms for solving
that problem. One algorithm may require the fewest steps. Another algorithm
may allow some steps to be performed concurrently. A computer that allows
more than one thing to be done at a time can often solve the problem in less
time, even though it is likely that the total number of steps to be performed has
increased.
1.7.3 The Program
The next step is to transform the algorithm into a computer program in one of the
programming languages that are available. Programming languages are mechan-
ical languages. That is, unlike natural languages, mechanical languages did not
evolve through human discourse. Rather, they were invented for use in specify-
ing a sequence of instructions to a computer. Therefore, mechanical languages do
not suer from failings such as ambiguity that would make them unacceptable for
specifying a computer program.
There are more than 1000 programming languages. Some have been designed
for use with particular applications, such as Fortran for solving scientic calcula-
tions and COBOL for solving business data-processing problems. In the second
half of this book, we will use C and C++, languages that were designed for
manipulating low-level hardware structures.
Other languages are useful for still other purposes. Prolog is the language of
choice for many applications that require the design of an expert system. LISP
was for years the language of choice of a substantial number of people working
on problems dealing with articial intelligence. Pascal is a language invented as
a vehicle for teaching beginning students how to program.
There are two kinds of programming languages, high-level languages and
low-level languages. High-level languages are at a distance (a high level) from
the underlying computer. At their best, they are independent of the computer on
which the programs will execute. We say the language is machine independent.","{'page_number': 24, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '1.7.2 The Algorithm\nThe rst step in the sequence of transformations is to transform the natural lan-\nguage description of the problem to an algorithm, and in so doing, get rid of the\nobjectionable characteristics of the natural language. An algorithm is a step-by-\nstep procedure that is guaranteed to terminate, such that each step is precisely\nstated and can be carried out by the computer. There are terms to describe each\nof these properties.\nWe use the term deniteness to describe the notion that each step is precisely\nstated. A recipe for excellent pancakes that instructs the preparer to stir until\nlumpy lacks deniteness, since the notion of lumpiness is not precise.\nWe use the term eective computability to describe the notion that each step\ncan be carried out by a computer. A procedure that instructs the computer to take\nthe largest prime number lacks eective computability, since there is no largest\nprime number.\nWe use the term niteness to describe the notion that the procedure termi-\nnates.\nFor every problem there are usually many dierent algorithms for solving\nthat problem. One algorithm may require the fewest steps. Another algorithm\nmay allow some steps to be performed concurrently. A computer that allows\nmore than one thing to be done at a time can often solve the problem in less\ntime, even though it is likely that the total number of steps to be performed has\nincreased.\n1.7.3 The Program\nThe next step is to transform the algorithm into a computer program in one of the\nprogramming languages that are available. Programming languages are mechan-\nical languages. That is, unlike natural languages, mechanical languages did not\nevolve through human discourse. Rather, they were invented for use in specify-\ning a sequence of instructions to a computer. Therefore, mechanical languages do\nnot suer from failings such as ambiguity that would make them unacceptable for\nspecifying a computer program.\nThere are more than 1000 programming languages. Some have been designed\nfor use with particular applications, such as Fortran for solving scientic calcula-\ntions and COBOL for solving business data-processing problems. In the second\nhalf of this book, we will use C and C++, languages that were designed for\nmanipulating low-level hardware structures.\nOther languages are useful for still other purposes. Prolog is the language of\nchoice for many applications that require the design of an expert system. LISP\nwas for years the language of choice of a substantial number of people working\non problems dealing with articial intelligence. Pascal is a language invented as\na vehicle for teaching beginning students how to program.\nThere are two kinds of programming languages, high-level languages and\nlow-level languages. High-level languages are at a distance (a high level) from\nthe underlying computer. At their best, they are independent of the computer on\nwhich the programs will execute. We say the language is machine independent.'}"
"All the languages mentioned thus far are high-level languages. Low-level lan-
guages are tied to the computer on which the programs will execute. There is
generally one such low-level language for each computer. That language is called
the assembly language for that computer.
1.7.4 The ISA
The next step is to translate the program into the instruction set of the particular
computer that will be used to carry out the work of the program. The instruction
set architecture (ISA) is the complete specication of the interface between pro-
grams that have been written and the underlying computer hardware that must
carry out the work of those programs.
An analogy that may be helpful in understanding the concept of an ISA is
provided by the automobile. Corresponding to a computer program, represented
as a sequence of 0s and 1s in the case of the computer, is the human sitting in the
drivers seat of a car. Corresponding to the microprocessor hardware is the car
itself. The ISA of the automobile is the specication of everything the human
needs to know to tell the automobile what to do, and everything the automobile
needs to know to carry out the tasks specied by the human driver. For example,
one element of the automobiles ISA is the pedal on the oor known as the
brake, and its function. The human knows that if he/she steps on the brake, the
car will stop. The automobile knows that if it feels pressure from the human on
that pedal, the hardware of the automobile must engage those elements necessary
to stop the car. The full ISA of the car includes the specication of the other
pedals, the steering wheel, the ignition key, the gears, windshield wipers, etc. For
each, the ISA species (a) what the human has to do to tell the automobile what
he/she wants done, and (b) correspondingly, what the automobile will interpret
those actions to mean so it (the automobile) can carry out the specied task.
The ISA of a computer serves the same purpose as the ISA of an auto-
mobile, except instead of the driver and the car, the ISA of a computer species
the interface between the computer program directing the computer hardware
and the hardware carrying out those directions. For example, consider the set of
instructions that the computer can carry outthat is, what operations the com-
puter can perform and where to get the data needed to perform those operations.
The term opcode is used to describe the operation. The term operand is used to
describe individual data values. The ISA species the acceptable representations
for operands. They are called data types. A data type is a representation of an
operand such that the computer can perform operations on that representation.
The ISA species the mechanisms that the computer can use to gure out where
the operands are located. These mechanisms are called addressing modes.
The number of opcodes, data types, and addressing modes specied by an
ISA vary among dierent ISAs. Some ISAs have as few as a half dozen opcodes,
whereas others have as many as several hundred. Some ISAs have only one data
type, while others have more than a dozen","{'page_number': 25, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The ISA species the mechanisms that the computer can use to gure out where\nthe operands are located. These mechanisms are called addressing modes.\nThe number of opcodes, data types, and addressing modes specied by an\nISA vary among dierent ISAs. Some ISAs have as few as a half dozen opcodes,\nwhereas others have as many as several hundred. Some ISAs have only one data\ntype, while others have more than a dozen. Some ISAs have one or two addressing\nmodes, whereas others have more than 20. The x86, the ISA used in the PC, has\nmore than 200 opcodes, more than a dozen data types, and more than two dozen\naddressing modes.'}"
"The ISA species the mechanisms that the computer can use to gure out where
the operands are located. These mechanisms are called addressing modes.
The number of opcodes, data types, and addressing modes specied by an
ISA vary among dierent ISAs. Some ISAs have as few as a half dozen opcodes,
whereas others have as many as several hundred. Some ISAs have only one data
type, while others have more than a dozen. Some ISAs have one or two addressing
modes, whereas others have more than 20. The x86, the ISA used in the PC, has
more than 200 opcodes, more than a dozen data types, and more than two dozen
addressing modes.","{'page_number': 25, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The ISA species the mechanisms that the computer can use to gure out where\nthe operands are located. These mechanisms are called addressing modes.\nThe number of opcodes, data types, and addressing modes specied by an\nISA vary among dierent ISAs. Some ISAs have as few as a half dozen opcodes,\nwhereas others have as many as several hundred. Some ISAs have only one data\ntype, while others have more than a dozen. Some ISAs have one or two addressing\nmodes, whereas others have more than 20. The x86, the ISA used in the PC, has\nmore than 200 opcodes, more than a dozen data types, and more than two dozen\naddressing modes.'}"
"The ISA also species the number of unique locations that comprise the com-
puters memory and the number of individual 0s and 1s that are contained in each
location.
Many ISAs are in use today. The most widely known example is the x86,
introduced by Intel Corporation in 1979 and currently also manufactured by
AMD and other companies. Other ISAs and the companies responsible for them
include ARM and THUMB (ARM), POWER and z/Architecture (IBM), and
SPARC (Oracle).
The translation from a high-level language (such as C) to the ISA of the
computer on which the program will execute (such as x86) is usually done by
a translating program called a compiler. To translate from a program written in
C to the x86 ISA, one would need a C to x86 compiler. For each high-level lan-
guage and each desired target ISA, one must provide a corresponding compiler.
The translation from the unique assembly language of a computer to its ISA
is done by an assembler.
1.7.5 The Microarchitecture
The next step is the implementation of the ISA, referred to as its microarchitec-
ture. The automobile analogy that we used in our discussion of the ISA is also
useful in showing the relationship between an ISA and a microarchitecture that
implements that ISA. The automobiles ISA describes what the driver needs to
know as he/she sits inside the automobile to make the automobile carry out the
drivers wishes. All automobiles have the same ISA. If there are three pedals on
the oor, it does not matter what manufacturer produced the car, the middle one
is always the brake. The one on the right is always the accelerator, and the more
it is depressed, the faster the car will move. Because there is only one ISA for
automobiles, one does not need one drivers license for Buicks and a dierent
drivers license for Hondas.
The microarchitecture (or implementation) of the automobiles ISA, on
the other hand, is about what goes on underneath the hood. Here all automo-
bile makes and models can be dierent, depending on what cost/performance
tradeos the automobile designer made before the car was manufactured. Some
automobiles come with disc brakes, others (in the past, at least) with drums.
Some automobiles have eight cylinders, others run on six cylinders, and still oth-
ers have only four. Some are turbocharged, some are not. Some automobiles can
travel 60 miles on one gallon of gasoline, others are lucky to travel from one gas
station to the next without running out of gas. Some automobiles cost 6000 US
dollars, others cost 200,000 US dollars. In each case, the microarchitecture
of the specic automobile is a result of the automobile designers decisions
regarding the tradeos of cost and performance. The fact that the micro-
architecture of every model or make is dierent is a good reason to take ones
Honda, when it is malfunctioning, to a Honda repair person, and not to a Buick
repair person","{'page_number': 26, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Some automobiles cost 6000 US\ndollars, others cost 200,000 US dollars. In each case, the microarchitecture\nof the specic automobile is a result of the automobile designers decisions\nregarding the tradeos of cost and performance. The fact that the micro-\narchitecture of every model or make is dierent is a good reason to take ones\nHonda, when it is malfunctioning, to a Honda repair person, and not to a Buick\nrepair person.\nIn the previous section, we identied ISAs of several computer manufactur-\ners, including the x86 (Intel), the PowerPC (IBM and Motorola), and THUMB\n(ARM). Each has been implemented by many dierent microarchitectures. For'}"
"Some automobiles cost 6000 US
dollars, others cost 200,000 US dollars. In each case, the microarchitecture
of the specic automobile is a result of the automobile designers decisions
regarding the tradeos of cost and performance. The fact that the micro-
architecture of every model or make is dierent is a good reason to take ones
Honda, when it is malfunctioning, to a Honda repair person, and not to a Buick
repair person.
In the previous section, we identied ISAs of several computer manufactur-
ers, including the x86 (Intel), the PowerPC (IBM and Motorola), and THUMB
(ARM). Each has been implemented by many dierent microarchitectures. For","{'page_number': 26, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Some automobiles cost 6000 US\ndollars, others cost 200,000 US dollars. In each case, the microarchitecture\nof the specic automobile is a result of the automobile designers decisions\nregarding the tradeos of cost and performance. The fact that the micro-\narchitecture of every model or make is dierent is a good reason to take ones\nHonda, when it is malfunctioning, to a Honda repair person, and not to a Buick\nrepair person.\nIn the previous section, we identied ISAs of several computer manufactur-\ners, including the x86 (Intel), the PowerPC (IBM and Motorola), and THUMB\n(ARM). Each has been implemented by many dierent microarchitectures. For'}"
"example, the x86s original implementation in 1979 was the 8086, followed by the
80286, 80386, and 80486 in the 1980s. More recently, in 2001, Intel introduced
the Pentium IV microprocessor. Even more recently, in 2015, Intel introduced
Skylake. Each of these x86 microprocessors has its own microarchitecture.
The story is the same for the PowerPC ISA, with more than a dozen dierent
microprocessors, each having its own microarchitecture.
Each microarchitecture is an opportunity for computer designers to make dif-
ferent tradeos between the cost of the microprocessor, the performance that the
microprocessor will provide, and the energy that is required to power the micro-
processor. Computer design is always an exercise in tradeos, as the designer
opts for higher (or lower) performance, more (or less) energy required, at greater
(or lesser) cost.
1.7.6 The Logic Circuit
The next step is to implement each element of the microarchitecture out of simple
logic circuits. Here also there are choices, as the logic designer decides how to
best make the tradeos between cost and performance. So, for example, even for
an operation as simple as addition, there are several choices of logic circuits to
perform the operation at diering speeds and corresponding costs.
1.7.7 The Devices
Finally, each basic logic circuit is implemented in accordance with the require-
ments of the particular device technology used. So, CMOS circuits are dierent
from NMOS circuits, which are dierent, in turn, from gallium arsenide
circuits.
The Bottom Line
In summary, from the natural language description of a prob-
lem to the electrons that actually solve the problem by moving from one voltage
potential to another, many transformations need to be performed. If we could
speak electron, or if the electrons could understand English, perhaps we could just
walk up to the computer and get the electrons to do our bidding. Since we cant
speak electron and they cant speak English, the best we can do is this systematic
sequence of transformations. At each level of transformation, there are choices as
to how to proceed. Our handling of those choices determines the resulting cost
and performance of our computer.
In this book, we describe each of these transformations. We show how tran-
sistors combine to form logic circuits, how logic circuits combine to form the
microarchitecture, and how the microarchitecture implements a particular ISA.
In our case, the ISA is the LC-3. We complete the process by going from the
English-language description of a problem to a C or C++ program that solves the
problem, and we show how that C or C++ program is translated (i.e., compiled)
to the ISA of the LC-3.
We hope you enjoy the ride.","{'page_number': 27, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'example, the x86s original implementation in 1979 was the 8086, followed by the\n80286, 80386, and 80486 in the 1980s. More recently, in 2001, Intel introduced\nthe Pentium IV microprocessor. Even more recently, in 2015, Intel introduced\nSkylake. Each of these x86 microprocessors has its own microarchitecture.\nThe story is the same for the PowerPC ISA, with more than a dozen dierent\nmicroprocessors, each having its own microarchitecture.\nEach microarchitecture is an opportunity for computer designers to make dif-\nferent tradeos between the cost of the microprocessor, the performance that the\nmicroprocessor will provide, and the energy that is required to power the micro-\nprocessor. Computer design is always an exercise in tradeos, as the designer\nopts for higher (or lower) performance, more (or less) energy required, at greater\n(or lesser) cost.\n1.7.6 The Logic Circuit\nThe next step is to implement each element of the microarchitecture out of simple\nlogic circuits. Here also there are choices, as the logic designer decides how to\nbest make the tradeos between cost and performance. So, for example, even for\nan operation as simple as addition, there are several choices of logic circuits to\nperform the operation at diering speeds and corresponding costs.\n1.7.7 The Devices\nFinally, each basic logic circuit is implemented in accordance with the require-\nments of the particular device technology used. So, CMOS circuits are dierent\nfrom NMOS circuits, which are dierent, in turn, from gallium arsenide\ncircuits.\nThe Bottom Line\nIn summary, from the natural language description of a prob-\nlem to the electrons that actually solve the problem by moving from one voltage\npotential to another, many transformations need to be performed. If we could\nspeak electron, or if the electrons could understand English, perhaps we could just\nwalk up to the computer and get the electrons to do our bidding. Since we cant\nspeak electron and they cant speak English, the best we can do is this systematic\nsequence of transformations. At each level of transformation, there are choices as\nto how to proceed. Our handling of those choices determines the resulting cost\nand performance of our computer.\nIn this book, we describe each of these transformations. We show how tran-\nsistors combine to form logic circuits, how logic circuits combine to form the\nmicroarchitecture, and how the microarchitecture implements a particular ISA.\nIn our case, the ISA is the LC-3. We complete the process by going from the\nEnglish-language description of a problem to a C or C++ program that solves the\nproblem, and we show how that C or C++ program is translated (i.e., compiled)\nto the ISA of the LC-3.\nWe hope you enjoy the ride.'}"
", Data Types, and
rations
2.1 Bits and Data Types
2.1.1 The Bit as the Unit of Information
We noted in Chapter 1 that the computer was organized as a system with several
levels of transformation. A problem stated in a natural language such as English
is actually solved by the electrons moving around inside the components of the
computer.
Inside the computer, millions of very tiny, very fast devices control the move-
ment of those electrons. These devices react to the presence or absence of voltages
in electronic circuits. They could react to the actual values of the voltages, rather
than simply to the presence or absence of voltages. However, this would make
the control and detection circuits more complex than they need to be. It is much
easier to detect simply whether or not a voltage exists at a point in a circuit than
it is to measure exactly what that voltage is.
To understand this, consider any wall outlet in your home. You could measure
the exact voltage it is carrying, whether 120 volts or 115 volts, or 118.6 volts,
for example. However, the detection circuitry to determine only whether there is
a voltage or whether there is no voltage is much simpler. Your nger casually
inserted into the wall socket, for example, will suce.
We symbolically represent the presence of a voltage as 1 and the absence
of a voltage as 0. We refer to each 0 and each 1 as a bit, which is a shortened
form of binary digit. Recall the digits you have been using since you were a
child0, 1, 2, 3,  , 9. There are ten of them, and they are referred to as decimal
digits. In the case of binary digits, there are two of them, 0 and 1.
To be perfectly precise, it is not really the case that the computer dieren-
tiates the absolute absence of a voltage (i.e., 0) from the absolute presence of
a voltage (i.e., 1). Actually, the electronic circuits in the computer dierentiate
voltages close to 0 from voltages far from 0. So, for example, if the computer
expects either a voltage of 1.2 volts or a voltage of 0 volts (1.2 volts signifying
1 and 0 volts signifying 0), then a voltage of 1.0 volts will be taken as a 1 and
0.2 volts will be taken as a 0.","{'page_number': 28, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': ', Data Types, and\nrations\n2.1 Bits and Data Types\n2.1.1 The Bit as the Unit of Information\nWe noted in Chapter 1 that the computer was organized as a system with several\nlevels of transformation. A problem stated in a natural language such as English\nis actually solved by the electrons moving around inside the components of the\ncomputer.\nInside the computer, millions of very tiny, very fast devices control the move-\nment of those electrons. These devices react to the presence or absence of voltages\nin electronic circuits. They could react to the actual values of the voltages, rather\nthan simply to the presence or absence of voltages. However, this would make\nthe control and detection circuits more complex than they need to be. It is much\neasier to detect simply whether or not a voltage exists at a point in a circuit than\nit is to measure exactly what that voltage is.\nTo understand this, consider any wall outlet in your home. You could measure\nthe exact voltage it is carrying, whether 120 volts or 115 volts, or 118.6 volts,\nfor example. However, the detection circuitry to determine only whether there is\na voltage or whether there is no voltage is much simpler. Your nger casually\ninserted into the wall socket, for example, will suce.\nWe symbolically represent the presence of a voltage as 1 and the absence\nof a voltage as 0. We refer to each 0 and each 1 as a bit, which is a shortened\nform of binary digit. Recall the digits you have been using since you were a\nchild0, 1, 2, 3,  , 9. There are ten of them, and they are referred to as decimal\ndigits. In the case of binary digits, there are two of them, 0 and 1.\nTo be perfectly precise, it is not really the case that the computer dieren-\ntiates the absolute absence of a voltage (i.e., 0) from the absolute presence of\na voltage (i.e., 1). Actually, the electronic circuits in the computer dierentiate\nvoltages close to 0 from voltages far from 0. So, for example, if the computer\nexpects either a voltage of 1.2 volts or a voltage of 0 volts (1.2 volts signifying\n1 and 0 volts signifying 0), then a voltage of 1.0 volts will be taken as a 1 and\n0.2 volts will be taken as a 0.'}"
"honewire,onecandifferentiateonlytwothings.Oneofthemcanbeassigned
e 0, the other can be assigned the value 1. But to get useful work done by
puter, it is necessary to be able to differentiate a large number of distinct
and to assign each of them a unique representation. We can accomplish
ombining many wires, that is, many bits. For example, if we use eight bits
(corresponding to the voltage present on each of eight wires), we can represent one
particularvalueas01001110,andanothervalueas11100111.Infact,ifwearelimited
toeightbits,wecandifferentiateatmostonly256(i.e., 28) differentthings.Ingeneral,
with k bits, we can distinguish at most 2k distinct items. Each pattern of these k bits
is a code; that is, it corresponds to a particular item (or value).
2.1.2 Data Types
There are many ways to represent the same value. For example, the number ve
can be written as a 5. This is the standard decimal notation that you are used to.
The value ve can also be represented by someone holding up one hand, with all
ngers and thumb extended. The person is saying, The number I wish to com-
municate can be determined by counting the number of ngers I am showing. A
written version of that scheme would be the value 11111. This notation has a name
alsounary. The Romans had yet another notation for vethe character V. We
momentarily that a fourth notation for ve is the binary representation
01.
s not enough simply to represent values; we must be able to operate on
alues. We say a particular representation is a data type if there are oper-
n the computer that can operate on information that is encoded in that
ntation. Each instruction set architecture (ISA) has its own set of data types
own set of instructions that can operate on those data types. In this book,
mainly use two data types: 2s complement integers for representing posi-
tive and negative integers that we wish to perform arithmetic on, and ASCII codes
for representing characters that we wish to input to a computer via the keyboard or
output from the computer via a monitor. Both data types will be explained shortly.
There are other representations of information that could be used, and indeed
that are present in most computers. Recall the scientic notation from high
school chemistry where you were admonished to represent the decimal num-
ber 621 as 6.21102. There are computers that represent numbers in that form,
and they provide operations that can operate on numbers so represented. That
data type is usually called oating point. We will examine its representation in
Section 2.7.1.
2.2 Integer Data Types
2.2.1 Unsigned Integers
The rst representation of information, or data type, that we shall look at is the
unsigned integer. As its name suggests, an unsigned integer has no sign (plus or
minus) associated with it. An unsigned integer just has a magnitude. Unsigned","{'page_number': 29, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'honewire,onecandifferentiateonlytwothings.Oneofthemcanbeassigned\ne 0, the other can be assigned the value 1. But to get useful work done by\nputer, it is necessary to be able to differentiate a large number of distinct\nand to assign each of them a unique representation. We can accomplish\nombining many wires, that is, many bits. For example, if we use eight bits\n(corresponding to the voltage present on each of eight wires), we can represent one\nparticularvalueas01001110,andanothervalueas11100111.Infact,ifwearelimited\ntoeightbits,wecandifferentiateatmostonly256(i.e., 28) differentthings.Ingeneral,\nwith k bits, we can distinguish at most 2k distinct items. Each pattern of these k bits\nis a code; that is, it corresponds to a particular item (or value).\n2.1.2 Data Types\nThere are many ways to represent the same value. For example, the number ve\ncan be written as a 5. This is the standard decimal notation that you are used to.\nThe value ve can also be represented by someone holding up one hand, with all\nngers and thumb extended. The person is saying, The number I wish to com-\nmunicate can be determined by counting the number of ngers I am showing. A\nwritten version of that scheme would be the value 11111. This notation has a name\nalsounary. The Romans had yet another notation for vethe character V. We\nmomentarily that a fourth notation for ve is the binary representation\n01.\ns not enough simply to represent values; we must be able to operate on\nalues. We say a particular representation is a data type if there are oper-\nn the computer that can operate on information that is encoded in that\nntation. Each instruction set architecture (ISA) has its own set of data types\nown set of instructions that can operate on those data types. In this book,\nmainly use two data types: 2s complement integers for representing posi-\ntive and negative integers that we wish to perform arithmetic on, and ASCII codes\nfor representing characters that we wish to input to a computer via the keyboard or\noutput from the computer via a monitor. Both data types will be explained shortly.\nThere are other representations of information that could be used, and indeed\nthat are present in most computers. Recall the scientic notation from high\nschool chemistry where you were admonished to represent the decimal num-\nber 621 as 6.21102. There are computers that represent numbers in that form,\nand they provide operations that can operate on numbers so represented. That\ndata type is usually called oating point. We will examine its representation in\nSection 2.7.1.\n2.2 Integer Data Types\n2.2.1 Unsigned Integers\nThe rst representation of information, or data type, that we shall look at is the\nunsigned integer. As its name suggests, an unsigned integer has no sign (plus or\nminus) associated with it. An unsigned integer just has a magnitude. Unsigned'}"
"integers have many uses in a computer. If we wish to perform a task some spe-
cic number of times, unsigned integers enable us to keep track of this number
easily by simply counting how many times we have performed the task. Unsigned
integers also provide a means for identifying dierent memory locations in the
computer in the same way that house numbers dierentiate 129 Main Street from
131 Main Street. I dont recall ever seeing a house number with a minus sign in
front of it.
We can represent unsigned integers as strings of binary digits. To do this, we
use a positional notation much like the decimal system that you have been using
since you were three years old.
You are familiar with the decimal number 329, which also uses positional
notation. The 3 is worth much more than the 9, even though the absolute value of
3 standing alone is only worth 1/3 the value of 9 standing alone. This is because,
as you know, the 3 stands for 300 (3102) due to its position in the decimal string
329, while the 9 stands for 9  100.
Instead of using decimal digits, we can represent unsigned integers using just
the binary digits 0 and 1. Here the base is 2, rather than 10. So, for example, if
we have ve bits (binary digits) available to represent our values, the number 5,
which we mentioned earlier, is represented as 00101, corresponding to
0  24 + 0  23 + 1  22 + 0  21 + 1  20
With k bits, we can represent in this positional notation exactly 2k integers, rang-
ing from 0 to 2k  1. Figure 2.1 shows the ve-bit representations for the integers
from 0 to 31.
2.2.2 Signed Integers
To do useful arithmetic, however, it is often (although not always) necessary to
be able to deal with negative quantities as well as positive. We could take our 2k
distinct patterns of k bits and separate them in half, half for positive integers and
half for negative integers. In this way, with ve-bit codes, instead of representing
integers from 0 to +31, we could choose to represent positive integers from +1
to +15 and negative integers from 1 to 15. There are 30 such integers. Since
25 is 32, we still have two 5-bit codes unassigned. One of them, 00000, we would
presumably assign to the value 0, giving us the full range of integer values from
15 to +15. That leaves one 5-bit code left over, and there are dierent ways to
assign this code, as we will see momentarily.
We are still left with the problem of determining what codes to assign to what
values. That is, we have 32 codes, but which value should go with which code?
Positive integers are represented in the straightforward positional scheme.
Since there are k bits, and we wish to use exactly half of the 2k codes to represent
the integers from 0 to 2k1  1, all positive integers will have a leading 0 in their
representation. In our example of Figure 2.1 (with k = 5), the largest positive
integer +15 is represented as 01111.
Note that in all three signed data types shown in Figure 2","{'page_number': 30, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Since there are k bits, and we wish to use exactly half of the 2k codes to represent\nthe integers from 0 to 2k1  1, all positive integers will have a leading 0 in their\nrepresentation. In our example of Figure 2.1 (with k = 5), the largest positive\ninteger +15 is represented as 01111.\nNote that in all three signed data types shown in Figure 2.1 , the represen-\ntation for 0 and all the positive integers start with a leading 0. What about the\nrepresentations for the negative integers (in our ve-bit example, 1 to 15)?'}"
"Since there are k bits, and we wish to use exactly half of the 2k codes to represent
the integers from 0 to 2k1  1, all positive integers will have a leading 0 in their
representation. In our example of Figure 2.1 (with k = 5), the largest positive
integer +15 is represented as 01111.
Note that in all three signed data types shown in Figure 2.1 , the represen-
tation for 0 and all the positive integers start with a leading 0. What about the
representations for the negative integers (in our ve-bit example, 1 to 15)?","{'page_number': 30, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Since there are k bits, and we wish to use exactly half of the 2k codes to represent\nthe integers from 0 to 2k1  1, all positive integers will have a leading 0 in their\nrepresentation. In our example of Figure 2.1 (with k = 5), the largest positive\ninteger +15 is represented as 01111.\nNote that in all three signed data types shown in Figure 2.1 , the represen-\ntation for 0 and all the positive integers start with a leading 0. What about the\nrepresentations for the negative integers (in our ve-bit example, 1 to 15)?'}"
"11111
31
15
0
1
Figure 2.1
Four representations of integers.
The rst thought that usually comes to mind is: If a leading 0 signies a positive
integer, how about letting a leading 1 signify a negative integer? The result is
the signed-magnitude data type shown in Figure 2.1. A second thought (which
was actually used on some early computers such as the Control Data Corpora-
tion 6600) was the following: Let a negative number be represented by taking the
representation of the positive number having the same magnitude, and ipping
all the bits. That is, if the original representation had a 0, replace it with a 1; if
it originally had a 1, replace it with a 0. For example, since +5 is represented as
00101, we designate 5 as 11010. This data type is referred to in the computer
engineering community as 1s complement and is also shown in Figure 2.1.","{'page_number': 31, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '11111\n31\n15\n0\n1\nFigure 2.1\nFour representations of integers.\nThe rst thought that usually comes to mind is: If a leading 0 signies a positive\ninteger, how about letting a leading 1 signify a negative integer? The result is\nthe signed-magnitude data type shown in Figure 2.1. A second thought (which\nwas actually used on some early computers such as the Control Data Corpora-\ntion 6600) was the following: Let a negative number be represented by taking the\nrepresentation of the positive number having the same magnitude, and ipping\nall the bits. That is, if the original representation had a 0, replace it with a 1; if\nit originally had a 1, replace it with a 0. For example, since +5 is represented as\n00101, we designate 5 as 11010. This data type is referred to in the computer\nengineering community as 1s complement and is also shown in Figure 2.1.'}"
"At this point, you might think that a computer designer could assign any bit
pattern to represent any integer he or she wants. And you would be right! Unfor-
tunately, that could complicate matters when we try to build an electronic circuit
to add two integers. In fact, the signed-magnitude and 1s complement data types
both require unnecessarily cumbersome hardware to do addition. Because com-
puter designers knew what it would take to design a circuit to add two integers,
they chose representations that simplied the circuit. The result is the 2s comple-
ment data type, also shown in Figure 2.1. It is used on just about every computer
manufactured today.
2.3 2s Complement Integers
We see in Figure 2.1 the representations of the integers from 16 to +15 for the
2s complement data type. Why were those representations chosen?
The positive integers, we saw, are represented in the straightforward posi-
tional scheme. With ve bits, we use exactly half of the 25 codes to represent 0
and the positive integers from 1 to 24  1.
The choice of representations for the negative integers was based, as we said
previously, on the wish to keep the logic circuits as simple as possible. Almost
all computers use the same basic mechanism to perform addition. It is called an
arithmetic and logic unit, usually known by its acronym ALU. We will get into
the actual structure of the ALU in Chapters 3 and 4. What is relevant right now
is that an ALU has two inputs and one output. It performs addition by adding the
binary bit patterns at its inputs, producing a bit pattern at its output that is the
sum of the two input bit patterns.
For example, if the ALU processe
were 00110 and 00101, the result (o
addition is as follows:
00110
00101
01011
The addition of two binary strings is performed in the same way the addi-
tion of two decimal strings is performed, from right to left, column by column.
If the addition in a column generates a carry, the carry is added to the column
immediately to its left.
What is particularly relevant is that the binary ALU does not know (and does
not care) what the two patterns it is adding represent. It simply adds the two binary
patterns. Since the binary ALU only ADDs and does not CARE, it would be nice
if our assignment of codes to the integers resulted in the ALU producing correct
results when it added two integers.
For starters, it would be nice if, when the ALU adds the representation for an
arbitrary integer to the representation of the integer having the same magnitude
but opposite sign, the sum would be 0. That is, if the inputs to the ALU are the
representations of non-zero integers A and A, the output of the ALU should
be 00000.","{'page_number': 32, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'At this point, you might think that a computer designer could assign any bit\npattern to represent any integer he or she wants. And you would be right! Unfor-\ntunately, that could complicate matters when we try to build an electronic circuit\nto add two integers. In fact, the signed-magnitude and 1s complement data types\nboth require unnecessarily cumbersome hardware to do addition. Because com-\nputer designers knew what it would take to design a circuit to add two integers,\nthey chose representations that simplied the circuit. The result is the 2s comple-\nment data type, also shown in Figure 2.1. It is used on just about every computer\nmanufactured today.\n2.3 2s Complement Integers\nWe see in Figure 2.1 the representations of the integers from 16 to +15 for the\n2s complement data type. Why were those representations chosen?\nThe positive integers, we saw, are represented in the straightforward posi-\ntional scheme. With ve bits, we use exactly half of the 25 codes to represent 0\nand the positive integers from 1 to 24  1.\nThe choice of representations for the negative integers was based, as we said\npreviously, on the wish to keep the logic circuits as simple as possible. Almost\nall computers use the same basic mechanism to perform addition. It is called an\narithmetic and logic unit, usually known by its acronym ALU. We will get into\nthe actual structure of the ALU in Chapters 3 and 4. What is relevant right now\nis that an ALU has two inputs and one output. It performs addition by adding the\nbinary bit patterns at its inputs, producing a bit pattern at its output that is the\nsum of the two input bit patterns.\nFor example, if the ALU processe\nwere 00110 and 00101, the result (o\naddition is as follows:\n00110\n00101\n01011\nThe addition of two binary strings is performed in the same way the addi-\ntion of two decimal strings is performed, from right to left, column by column.\nIf the addition in a column generates a carry, the carry is added to the column\nimmediately to its left.\nWhat is particularly relevant is that the binary ALU does not know (and does\nnot care) what the two patterns it is adding represent. It simply adds the two binary\npatterns. Since the binary ALU only ADDs and does not CARE, it would be nice\nif our assignment of codes to the integers resulted in the ALU producing correct\nresults when it added two integers.\nFor starters, it would be nice if, when the ALU adds the representation for an\narbitrary integer to the representation of the integer having the same magnitude\nbut opposite sign, the sum would be 0. That is, if the inputs to the ALU are the\nrepresentations of non-zero integers A and A, the output of the ALU should\nbe 00000.'}"
"To accomplish that, the 2s complement data type species the representation
for each negative integer so that when the ALU adds it to the representation of
the positive integer of the same magnitude, the result will be the representation
for 0. For example, since 00101 is the representation of +5, 11011 is chosen as
the representation for 5.
Moreover, and actually more importantly, as we sequence through represen-
tations of 15 to +15, the ALU is adding 00001 to each successive representation.
We can express this mathematically as:
REPRESENTATION(value + 1) =
REPRESENTATION(value) + REPRESENTATION(1).
This is sucient to guarantee (as long as we do not get a result larger than
+15 or smaller than 15) that the binary ALU will perform addition correctly.
Note in particular the representations for 1 and 0, that is, 11111 and 00000.
When we add 00001 to the representation for 1, we do get 00000, but we also
generate a carry. That carry, however, does not inuence the result. That is, the
correct result of adding 00001 to the representation for 1 is 0, not 100000.
Therefore, the carry is ignored. In fact, because the carry obtained by adding
00001 to 11111 is ignored, the carry can always be ignored when dealing with
2s complement arithmetic.
Note: If we know the representation for A, a shortcut for guring out the
representation for A(A  0) is as follows: Flip all the bits of A (the ocial term
for ip is complement), and add 1 to the complement of A. The sum of A and
Exa
10011
00000
You may have noticed that the addition of 01101 and 10011, in addition to
producing 00000, also produces a carry out of the ve-bit ALU. That is, the binary
addition of 01101 and 10011 is really 100000. However, as we saw previously,
this carry out can be ignored in the case of the 2s complement data type.
At this point, we have identied in our ve-bit scheme 15 positive inte-
gers. We have constructed 15 negative integers. We also have a representation","{'page_number': 33, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'To accomplish that, the 2s complement data type species the representation\nfor each negative integer so that when the ALU adds it to the representation of\nthe positive integer of the same magnitude, the result will be the representation\nfor 0. For example, since 00101 is the representation of +5, 11011 is chosen as\nthe representation for 5.\nMoreover, and actually more importantly, as we sequence through represen-\ntations of 15 to +15, the ALU is adding 00001 to each successive representation.\nWe can express this mathematically as:\nREPRESENTATION(value + 1) =\nREPRESENTATION(value) + REPRESENTATION(1).\nThis is sucient to guarantee (as long as we do not get a result larger than\n+15 or smaller than 15) that the binary ALU will perform addition correctly.\nNote in particular the representations for 1 and 0, that is, 11111 and 00000.\nWhen we add 00001 to the representation for 1, we do get 00000, but we also\ngenerate a carry. That carry, however, does not inuence the result. That is, the\ncorrect result of adding 00001 to the representation for 1 is 0, not 100000.\nTherefore, the carry is ignored. In fact, because the carry obtained by adding\n00001 to 11111 is ignored, the carry can always be ignored when dealing with\n2s complement arithmetic.\nNote: If we know the representation for A, a shortcut for guring out the\nrepresentation for A(A  0) is as follows: Flip all the bits of A (the ocial term\nfor ip is complement), and add 1 to the complement of A. The sum of A and\nExa\n10011\n00000\nYou may have noticed that the addition of 01101 and 10011, in addition to\nproducing 00000, also produces a carry out of the ve-bit ALU. That is, the binary\naddition of 01101 and 10011 is really 100000. However, as we saw previously,\nthis carry out can be ignored in the case of the 2s complement data type.\nAt this point, we have identied in our ve-bit scheme 15 positive inte-\ngers. We have constructed 15 negative integers. We also have a representation'}"
"for 0. With k = 5, we can uniquely identify 32 distinct quantities, and we have
accounted for only 31 (15+15+1). The remaining representation is 10000. What
value shall we assign to it?
We note that 1 is 11111, 2 is 11110, 3 is 11101, and so on. If we continue
this, we note that 15 is 10001. Note that, as in the case of the positive represen-
tations, as we sequence backwards from representations of 1 to 15, the ALU
is subtracting 00001 from each successive representation. Thus, it is convenient
to assign to 10000 the value 16; that is the value one gets by subtracting 00001
from 10001 (the representation for 15).
In Chapter 5 we will specify a computer that we aectionately have named
the LC-3 (for Little Computer 3). The LC-3 operates on 16-bit values. Therefore,
the 2s complement integers that can be represented in the LC-3 are the integers
from 32,768 to +32,767.
2.4 Conversion Between Binary and
Decimal
It is often useful to convert numbers between the 2s complement data type
the computer likes and the decimal representation that you have used all
your life.
2.4.1 Binary to Decimal Conversion
We convert a 2s complement representation of an integer to a decimal represen-
tation as follows: For purposes of illustration, we will assume our number can be
represented in eight bits, corresponding to decimal integer values from 128 to
+127.
Recall that an eight-bit 2s complement integer takes the form
b7 b6 b5 b4 b3 b2 b1 b0
where each of the bits bi is either 0 or 1.
1. Examine the leading bit b7. If it is a 0, the integer is positive, and we can
begin evaluating its magnitude. If it is a 1, the integer is negative. In that
case, we need to rst obtain the 2s complement representation of the
positive number having the same magnitude. We do this by ipping all the
bits and adding 1.
2. The magnitude is simply
b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20
In either case, we obtain the decimal magnitude by simply adding the
powers of 2 that have coecients of 1.
3. Finally, if the original number is negative, we ax a minus sign in front.
Done!","{'page_number': 34, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'for 0. With k = 5, we can uniquely identify 32 distinct quantities, and we have\naccounted for only 31 (15+15+1). The remaining representation is 10000. What\nvalue shall we assign to it?\nWe note that 1 is 11111, 2 is 11110, 3 is 11101, and so on. If we continue\nthis, we note that 15 is 10001. Note that, as in the case of the positive represen-\ntations, as we sequence backwards from representations of 1 to 15, the ALU\nis subtracting 00001 from each successive representation. Thus, it is convenient\nto assign to 10000 the value 16; that is the value one gets by subtracting 00001\nfrom 10001 (the representation for 15).\nIn Chapter 5 we will specify a computer that we aectionately have named\nthe LC-3 (for Little Computer 3). The LC-3 operates on 16-bit values. Therefore,\nthe 2s complement integers that can be represented in the LC-3 are the integers\nfrom 32,768 to +32,767.\n2.4 Conversion Between Binary and\nDecimal\nIt is often useful to convert numbers between the 2s complement data type\nthe computer likes and the decimal representation that you have used all\nyour life.\n2.4.1 Binary to Decimal Conversion\nWe convert a 2s complement representation of an integer to a decimal represen-\ntation as follows: For purposes of illustration, we will assume our number can be\nrepresented in eight bits, corresponding to decimal integer values from 128 to\n+127.\nRecall that an eight-bit 2s complement integer takes the form\nb7 b6 b5 b4 b3 b2 b1 b0\nwhere each of the bits bi is either 0 or 1.\n1. Examine the leading bit b7. If it is a 0, the integer is positive, and we can\nbegin evaluating its magnitude. If it is a 1, the integer is negative. In that\ncase, we need to rst obtain the 2s complement representation of the\npositive number having the same magnitude. We do this by ipping all the\nbits and adding 1.\n2. The magnitude is simply\nb6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20\nIn either case, we obtain the decimal magnitude by simply adding the\npowers of 2 that have coecients of 1.\n3. Finally, if the original number is negative, we ax a minus sign in front.\nDone!'}"
"Exa
.
3. The decimal integer value corresponding to 11000111 is 57.
2.4.2 Decimal to Binary Conversion
Converting from decimal to 2s complement is a little more complicated. The
crux of the method is to note that a positive binary number is odd if the rightmost
digit is 1 and even if the rightmost digit is 0.
Consider again our generic eight-bit representation:
b7  27 + b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20
We can illustrate the conversion best by rst working through an example.
Suppose we wish to convert the value +105 to a 2s complement binary code.
We note that +105 is positive. We rst nd values for bi, representing the mag-
nitude 105. Since the value is positive, we will then obtain the 2s complement
result by simply appending b7, which we know is 0.
Our rst step is to nd values for bi that satisfy the following:
105 = b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20
Since 105 is odd, we know that b0 is 1. We subtract 1 from both sides of the
equation, yielding
104 = b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21
We next divide both sides of the equation by 2, yielding
52 = b6  25 + b5  24 + b4  23 + b3  22 + b2  21 + b1  20
Since 52 is even, b1, the only coecient not multiplied by a power of 2, must be
equal to 0.
We iterate this process, each time subtracting the rightmost digit from both
sides of the equation, then dividing both sides by 2, and nally noting whether
the new decimal number on the left side is odd or even. Continuing where we left
o, with
52 = b6  25 + j5  24 + b4  23 + b3  22 + b2  21
the process produces, in turn:
26 = b6  24 + b5  23 + b4  22 + b3  21 + b2  20","{'page_number': 35, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Exa\n.\n3. The decimal integer value corresponding to 11000111 is 57.\n2.4.2 Decimal to Binary Conversion\nConverting from decimal to 2s complement is a little more complicated. The\ncrux of the method is to note that a positive binary number is odd if the rightmost\ndigit is 1 and even if the rightmost digit is 0.\nConsider again our generic eight-bit representation:\nb7  27 + b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20\nWe can illustrate the conversion best by rst working through an example.\nSuppose we wish to convert the value +105 to a 2s complement binary code.\nWe note that +105 is positive. We rst nd values for bi, representing the mag-\nnitude 105. Since the value is positive, we will then obtain the 2s complement\nresult by simply appending b7, which we know is 0.\nOur rst step is to nd values for bi that satisfy the following:\n105 = b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20\nSince 105 is odd, we know that b0 is 1. We subtract 1 from both sides of the\nequation, yielding\n104 = b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21\nWe next divide both sides of the equation by 2, yielding\n52 = b6  25 + b5  24 + b4  23 + b3  22 + b2  21 + b1  20\nSince 52 is even, b1, the only coecient not multiplied by a power of 2, must be\nequal to 0.\nWe iterate this process, each time subtracting the rightmost digit from both\nsides of the equation, then dividing both sides by 2, and nally noting whether\nthe new decimal number on the left side is odd or even. Continuing where we left\no, with\n52 = b6  25 + j5  24 + b4  23 + b3  22 + b2  21\nthe process produces, in turn:\n26 = b6  24 + b5  23 + b4  22 + b3  21 + b2  20'}"
"Therefore, b2 = 0.
13 = b6  23 + b5  22 + b4  21 + b3  20
Therefore, b3 = 1.
6 = b6  22 + b5  21 + b4  20
Therefore, b4 = 0.
3 = b6  21 + b5  20
Therefore, b5 = 1.
1 = b6  20
Therefore, b6 = 1, and we are done. The binary representation is 01101001.
Lets summarize the process. If we are given a decimal integer value N, we
construct the 2s complement representation as follows:
1. We rst obtain the binary representation of the magnitude of N by forming
the equation
N = b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20
and repeating the following, until the left side of the equation is 0:
a. If N is odd, the rightmost bit is 1. If N is even, the rightmost bit is 0.
b. Subtract 1 or 0 (according to whether N is odd or even) from N, remove
the least signicant term from the right side, and divide both sides of
the equation by 2.
Each iteration produces the value of one coecient bi.
2. If the original decimal number is positive, append a leading 0 sign bit, and
you are done.
3. If the original decimal number is negative, append a leading 0 and then
form the negative of this 2s complement representation, and then you
are done.
2.4.3 Extending Conversion to Numbers with Fractional Parts
What if the number we wish to convert is not an integer, but instead has a
fractional part. How do we handle that wrinkle?
Binary to decimal
The binary to decimal case is straightforward. In a positional
notation system, the number
0.b1b2b3b4
shows four bits to the right of the binary point, representing (when the cor-
responding bi = 1) the values 0.5, 0.25, 0.125, and 0.0625. To complete the","{'page_number': 36, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Therefore, b2 = 0.\n13 = b6  23 + b5  22 + b4  21 + b3  20\nTherefore, b3 = 1.\n6 = b6  22 + b5  21 + b4  20\nTherefore, b4 = 0.\n3 = b6  21 + b5  20\nTherefore, b5 = 1.\n1 = b6  20\nTherefore, b6 = 1, and we are done. The binary representation is 01101001.\nLets summarize the process. If we are given a decimal integer value N, we\nconstruct the 2s complement representation as follows:\n1. We rst obtain the binary representation of the magnitude of N by forming\nthe equation\nN = b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20\nand repeating the following, until the left side of the equation is 0:\na. If N is odd, the rightmost bit is 1. If N is even, the rightmost bit is 0.\nb. Subtract 1 or 0 (according to whether N is odd or even) from N, remove\nthe least signicant term from the right side, and divide both sides of\nthe equation by 2.\nEach iteration produces the value of one coecient bi.\n2. If the original decimal number is positive, append a leading 0 sign bit, and\nyou are done.\n3. If the original decimal number is negative, append a leading 0 and then\nform the negative of this 2s complement representation, and then you\nare done.\n2.4.3 Extending Conversion to Numbers with Fractional Parts\nWhat if the number we wish to convert is not an integer, but instead has a\nfractional part. How do we handle that wrinkle?\nBinary to decimal\nThe binary to decimal case is straightforward. In a positional\nnotation system, the number\n0.b1b2b3b4\nshows four bits to the right of the binary point, representing (when the cor-\nresponding bi = 1) the values 0.5, 0.25, 0.125, and 0.0625. To complete the'}"
"conversion to decimal, we simply add those values where the corresponding bi =
1. For example, if the fractional part of the binary representation is
. 1 0 1 1
we would add 0.5 plus 0.125 plus 0.0625, or 0.6875.
Decimal to binary
The decimal to binary case requires a little more work. Sup-
pose we wanted to convert 0.421 to binary. As we did for integer conversion, we
rst form the equation
0.421 = b1  21 + b2  22 + b3  23 + b4  24 + ...
In the case of converting a decimal integer value to binary, we divided by 2 and
assigned a 1 or 0 to the coecient of 20 depending on whether the number on the
left of the equal sign is odd or even. Here (i.e., in the case of converting a decimal
fraction to binary), we multiply both sides of the equation by 2 and assign a 1 or
a 0 to the coecient of 20 depending on whether the left side of the equation is
greater than or equal to 1 or whether the left side is less than 1. Do you see why?
Since
0.842 = b1  20 + b2  21 + b3  22 + b4  23 + ...
we assign b1 = 0. Continuing,
1.684 = b2  20 + b3  21 + b4  22 + ...
so we assign b2 = 1 and subtract 1 from both sides of the equation, yielding
0.684 = b3  21 + b4  22 + ...
Multiplying by 2, we get
1.368 = b3  20 + b4  21 + ...
so we assign b3 = 1 and subtract 1 from both sides of the equation, yielding
0.368 = b4  20 + ...
which assigns 0 to b4. We can continue this process indenitely, until we are
simply too tired to go on, or until the left side = 0, in which case all bits to
the right of where we stop are 0s. In our case, stopping with four bits, we have
converted 0.421 decimal to 0.0110 in binary.
2.5 Operations on Bits
Part I: Arithmetic
2.5.1 Addition and Subtraction
Arithmetic on 2s complement numbers is very much like the arithmetic on
decimal numbers that you have been doing for a long time.","{'page_number': 37, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'conversion to decimal, we simply add those values where the corresponding bi =\n1. For example, if the fractional part of the binary representation is\n. 1 0 1 1\nwe would add 0.5 plus 0.125 plus 0.0625, or 0.6875.\nDecimal to binary\nThe decimal to binary case requires a little more work. Sup-\npose we wanted to convert 0.421 to binary. As we did for integer conversion, we\nrst form the equation\n0.421 = b1  21 + b2  22 + b3  23 + b4  24 + ...\nIn the case of converting a decimal integer value to binary, we divided by 2 and\nassigned a 1 or 0 to the coecient of 20 depending on whether the number on the\nleft of the equal sign is odd or even. Here (i.e., in the case of converting a decimal\nfraction to binary), we multiply both sides of the equation by 2 and assign a 1 or\na 0 to the coecient of 20 depending on whether the left side of the equation is\ngreater than or equal to 1 or whether the left side is less than 1. Do you see why?\nSince\n0.842 = b1  20 + b2  21 + b3  22 + b4  23 + ...\nwe assign b1 = 0. Continuing,\n1.684 = b2  20 + b3  21 + b4  22 + ...\nso we assign b2 = 1 and subtract 1 from both sides of the equation, yielding\n0.684 = b3  21 + b4  22 + ...\nMultiplying by 2, we get\n1.368 = b3  20 + b4  21 + ...\nso we assign b3 = 1 and subtract 1 from both sides of the equation, yielding\n0.368 = b4  20 + ...\nwhich assigns 0 to b4. We can continue this process indenitely, until we are\nsimply too tired to go on, or until the left side = 0, in which case all bits to\nthe right of where we stop are 0s. In our case, stopping with four bits, we have\nconverted 0.421 decimal to 0.0110 in binary.\n2.5 Operations on Bits\nPart I: Arithmetic\n2.5.1 Addition and Subtraction\nArithmetic on 2s complement numbers is very much like the arithmetic on\ndecimal numbers that you have been doing for a long time.'}"
"Addition still proceeds from right to left, one digit at a time. At each point,
we generate a sum digit and a carry. Instead of generating a carry after 9 (since
9 is the largest decimal digit), we generate a carry after 1 (since 1 is the largest
binary digit).
Example 2.3
Using our ve-bit notation, what is 11 + 3?
The decimal value 11 is represented as 01011
The decimal value 3 is represented as
00011
The sum, which is the value 14, is
01110
Subtraction is simply addition, preceded by determining the negative of the
number to be subtracted. That is, A  B is simply A + (B).
Example 2.4
What is 14  9?
The decimal value 14 is represented as
01110
The decimal value 9 is represented as
01001
First we form the negative, that is, -9: 10111
Adding 14 to -9, we get
01110
10111
which results in the value 5.
00101
Note again that the carry out is ignored.
Exam
What happens when we add a number to itself (e.g., x + x)?
Lets assume for this example eight-bit codes, which would allow us to represent
integers from 128 to 127. Consider a value for x, the integer 59, represented as
00111011. If we add 59 to itself, we get the code 01110110. Note that the bits have
all shifted to the left by one position. Is that a curiosity, or will that happen all the
time as long as the sum x + x is not too large to represent with the available number
of bits?
Using our positional notation, the number 59 is
0  26 + 1  25 + 1  24 + 1  23 + 0  22 + 1  21 + 1  20
The sum 59 + 59 is 2  59, which, in our representation, is
2  (0  26 + 1  25 + 1  24 + 1  23 + 0  22 + 1  21 + 1  20)
But that is nothing more than
0  27 + 1  26 + 1  25 + 1  24 + 0  23 + 1  22 + 1  21
which shifts each digit one position to the left. Thus, adding a number to itself
(provided there are enough bits to represent the result) is equivalent to shifting the
representation one bit position to the left.","{'page_number': 38, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Addition still proceeds from right to left, one digit at a time. At each point,\nwe generate a sum digit and a carry. Instead of generating a carry after 9 (since\n9 is the largest decimal digit), we generate a carry after 1 (since 1 is the largest\nbinary digit).\nExample 2.3\nUsing our ve-bit notation, what is 11 + 3?\nThe decimal value 11 is represented as 01011\nThe decimal value 3 is represented as\n00011\nThe sum, which is the value 14, is\n01110\nSubtraction is simply addition, preceded by determining the negative of the\nnumber to be subtracted. That is, A  B is simply A + (B).\nExample 2.4\nWhat is 14  9?\nThe decimal value 14 is represented as\n01110\nThe decimal value 9 is represented as\n01001\nFirst we form the negative, that is, -9: 10111\nAdding 14 to -9, we get\n01110\n10111\nwhich results in the value 5.\n00101\nNote again that the carry out is ignored.\nExam\nWhat happens when we add a number to itself (e.g., x + x)?\nLets assume for this example eight-bit codes, which would allow us to represent\nintegers from 128 to 127. Consider a value for x, the integer 59, represented as\n00111011. If we add 59 to itself, we get the code 01110110. Note that the bits have\nall shifted to the left by one position. Is that a curiosity, or will that happen all the\ntime as long as the sum x + x is not too large to represent with the available number\nof bits?\nUsing our positional notation, the number 59 is\n0  26 + 1  25 + 1  24 + 1  23 + 0  22 + 1  21 + 1  20\nThe sum 59 + 59 is 2  59, which, in our representation, is\n2  (0  26 + 1  25 + 1  24 + 1  23 + 0  22 + 1  21 + 1  20)\nBut that is nothing more than\n0  27 + 1  26 + 1  25 + 1  24 + 0  23 + 1  22 + 1  21\nwhich shifts each digit one position to the left. Thus, adding a number to itself\n(provided there are enough bits to represent the result) is equivalent to shifting the\nrepresentation one bit position to the left.'}"
"2.5.2 Sign-Extension
It is often useful to represent a small number with fewer bits. For example, rather
than represent the value 5 as 0000000000000101, there are times when it makes
nse to use only six bits to represent the value 5: 000101. There is little
n, since we are all used to adding leading zeros without aecting the
a number. A check for $456.78 and a check for $0000456.78 are checks
he same value.
at about negative representations? We obtained the negative representa-
its positive counterpart by complementing the positive representation
ing 1. Thus, the representation for 5, given that 5 is represented as
, is 111011. If 5 is represented as 0000000000000101, then the represen-
tation for 5 is 1111111111111011. In the same way that leading 0s do not aect
the value of a positive number, leading 1s do not aect the value of a negative
number.
In order to add representations of dierent lengths, it is rst necessary to
represent them with the same number of bits. For example, suppose we wish to
add the number 13 to 5, where 13 is represented as 0000000000001101 and 5
is represented as 111011. If we do not represent the two values with the same
number of bits, we have
0000000000001101
+ 111011
When we attempt to perform the addition, what shall we do with the missing bits
in the representation for 5? If we take the absence of a bit to be a 0, then we are
no longer adding 5 to 13. On the contrary, if we take the absence of bits to be 0s,
we have changed the 5 to the number represented as 0000000000111011, that
is, +59. Not surprisingly, then, our result turns out to be the representation for 72.
However, if we understand that a 6-bit 5 and a 16-bit 5 dier only in the
number of meaningless leading 1s, then we rst extend the value of 5 to 16 bits
before we perform the addition. Thus, we have
0000000000001101
+ 1111111111111011
0000000000001000
result is +8, as we should expect.
e value of a positive number does not change if we extend the sign bit
any bit positions to the left as desired. Similarly, the value of a negative
r does not change by extending the sign bit 1 as many bit positions to the
desired. Since in both cases it is the sign bit that is extended, we refer
to the operation as Sign-EXTension, often abbreviated SEXT. Sign-extension is
performed in order to be able to operate on representations of dierent lengths.
It does not aect the values of the numbers being represented.
2.5.3 Overow
Up to now, we have always insisted that the sum of two integers be small enough
to be represented by the available bits. What happens if such is not the case?","{'page_number': 39, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '2.5.2 Sign-Extension\nIt is often useful to represent a small number with fewer bits. For example, rather\nthan represent the value 5 as 0000000000000101, there are times when it makes\nnse to use only six bits to represent the value 5: 000101. There is little\nn, since we are all used to adding leading zeros without aecting the\na number. A check for $456.78 and a check for $0000456.78 are checks\nhe same value.\nat about negative representations? We obtained the negative representa-\nits positive counterpart by complementing the positive representation\ning 1. Thus, the representation for 5, given that 5 is represented as\n, is 111011. If 5 is represented as 0000000000000101, then the represen-\ntation for 5 is 1111111111111011. In the same way that leading 0s do not aect\nthe value of a positive number, leading 1s do not aect the value of a negative\nnumber.\nIn order to add representations of dierent lengths, it is rst necessary to\nrepresent them with the same number of bits. For example, suppose we wish to\nadd the number 13 to 5, where 13 is represented as 0000000000001101 and 5\nis represented as 111011. If we do not represent the two values with the same\nnumber of bits, we have\n0000000000001101\n+ 111011\nWhen we attempt to perform the addition, what shall we do with the missing bits\nin the representation for 5? If we take the absence of a bit to be a 0, then we are\nno longer adding 5 to 13. On the contrary, if we take the absence of bits to be 0s,\nwe have changed the 5 to the number represented as 0000000000111011, that\nis, +59. Not surprisingly, then, our result turns out to be the representation for 72.\nHowever, if we understand that a 6-bit 5 and a 16-bit 5 dier only in the\nnumber of meaningless leading 1s, then we rst extend the value of 5 to 16 bits\nbefore we perform the addition. Thus, we have\n0000000000001101\n+ 1111111111111011\n0000000000001000\nresult is +8, as we should expect.\ne value of a positive number does not change if we extend the sign bit\nany bit positions to the left as desired. Similarly, the value of a negative\nr does not change by extending the sign bit 1 as many bit positions to the\ndesired. Since in both cases it is the sign bit that is extended, we refer\nto the operation as Sign-EXTension, often abbreviated SEXT. Sign-extension is\nperformed in order to be able to operate on representations of dierent lengths.\nIt does not aect the values of the numbers being represented.\n2.5.3 Overow\nUp to now, we have always insisted that the sum of two integers be small enough\nto be represented by the available bits. What happens if such is not the case?'}"
"You are undoubtedly familiar with the odometer on the front dashboard of
your automobile. It keeps track of how many miles your car has been drivenbut
only up to a point. In the old days, when the odometer registered 99992 and you
drove it 100 miles, its new reading became 00092. A brand new car! The problem,
as you know, is that the largest value the odometer could store was 99999, so the
value 100092 showed up as 00092. The carry out of the ten-thousands digit was
lost. (Of course, if you grew up in Boston, the carry out was not lost at allit
was in full display in the rusted chrome all over the car.)
We say the odometer overowed. Representing 100092 as 00092 is unaccept-
able. As more and more cars lasted more than 100,000 miles, car makers felt the
pressure to add a digit to the odometer. Today, practically all cars overow at
1,000,000 miles, rather than 100,000 miles.
The odometer provides an example of unsigned arithmetic. The miles you
add are always positive miles. The odometer reads 000129 and you drive 50 miles.
The odometer now reads 000179. Overow is a carry out of the leading digit.
In the case of signed arithmetic, or more particularly, 2s complement
arithmetic, overow is a little more subtle.
Lets return to our ve-bit 2s complement data type, which allowed us to
represent integers from 16 to +15. Suppose we wish to add +9 and +11. Our
arithmetic takes the following form:
01001
01011
10100
Note that the sum is larger than +15, and therefore too large to represent with
our 2s complement scheme. The fact that the number is too large means that the
number is larger than 01111, the largest positive number we can represent with
a ve-bit 2s complement data type. Note that because our positive result was
larger than +15, it generated a carry into the leading bit position. But this bit
position is used to indicate the sign of a value. Thus, detecting that the result is
too large is an easy matter. Since we are adding two positive numbers, the result
must be positive. Since the ALU has produced a negative result, something must
be wrong. The thing that is wrong is that the sum of the two positive numbers
is too large to be represented with the available bits. We say that the result has
overowed the capacity of the representation.
Suppose instead, we had started with negative numbers, for example, 12
and 6. In this case, our arithmetic takes the following form:
10100
11010
01110
Here, too, the result has overowed the capacity of the machine, since 12 + 6
equals 18, which is more negative than 16, the negative number with the
largest allowable magnitude. The ALU obliges by producing a positive result.
Again, this is easy to detect since the sum of two negative numbers cannot be
positive.","{'page_number': 40, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'You are undoubtedly familiar with the odometer on the front dashboard of\nyour automobile. It keeps track of how many miles your car has been drivenbut\nonly up to a point. In the old days, when the odometer registered 99992 and you\ndrove it 100 miles, its new reading became 00092. A brand new car! The problem,\nas you know, is that the largest value the odometer could store was 99999, so the\nvalue 100092 showed up as 00092. The carry out of the ten-thousands digit was\nlost. (Of course, if you grew up in Boston, the carry out was not lost at allit\nwas in full display in the rusted chrome all over the car.)\nWe say the odometer overowed. Representing 100092 as 00092 is unaccept-\nable. As more and more cars lasted more than 100,000 miles, car makers felt the\npressure to add a digit to the odometer. Today, practically all cars overow at\n1,000,000 miles, rather than 100,000 miles.\nThe odometer provides an example of unsigned arithmetic. The miles you\nadd are always positive miles. The odometer reads 000129 and you drive 50 miles.\nThe odometer now reads 000179. Overow is a carry out of the leading digit.\nIn the case of signed arithmetic, or more particularly, 2s complement\narithmetic, overow is a little more subtle.\nLets return to our ve-bit 2s complement data type, which allowed us to\nrepresent integers from 16 to +15. Suppose we wish to add +9 and +11. Our\narithmetic takes the following form:\n01001\n01011\n10100\nNote that the sum is larger than +15, and therefore too large to represent with\nour 2s complement scheme. The fact that the number is too large means that the\nnumber is larger than 01111, the largest positive number we can represent with\na ve-bit 2s complement data type. Note that because our positive result was\nlarger than +15, it generated a carry into the leading bit position. But this bit\nposition is used to indicate the sign of a value. Thus, detecting that the result is\ntoo large is an easy matter. Since we are adding two positive numbers, the result\nmust be positive. Since the ALU has produced a negative result, something must\nbe wrong. The thing that is wrong is that the sum of the two positive numbers\nis too large to be represented with the available bits. We say that the result has\noverowed the capacity of the representation.\nSuppose instead, we had started with negative numbers, for example, 12\nand 6. In this case, our arithmetic takes the following form:\n10100\n11010\n01110\nHere, too, the result has overowed the capacity of the machine, since 12 + 6\nequals 18, which is more negative than 16, the negative number with the\nlargest allowable magnitude. The ALU obliges by producing a positive result.\nAgain, this is easy to detect since the sum of two negative numbers cannot be\npositive.'}"
"Note that the sum of a negative number and a positive number never presents
a problem. Why is that? See Exercise 2.25.
2.6 Operations on Bits
Part II: Logical Operations
We have seen that it is possible to perform arithmetic (e.g., add, subtract) on val-
ues represented as binary patterns. Another class of operations useful to perform
on binary patterns is the set of logical operations.
2.6.1 A Logical Variable
Logical operations operate on logical variables. A logical variable can have one
of two values, 0 or 1. The name logical is a historical one; it comes from the fact
that the two values 0 and 1 can represent the two logical values false and true,
but the use of logical operations has traveled far from this original meaning.
There are several basic logic functions, and most ALUs perform all of them.
2.6.2 The AND Function
AND is a binary logical function. This means it requires two pieces of input data.
Said another way, AND requires two source operands. Each source is a logical
variable, taking the value 0 or 1. The output of AND is 1 only if both sources have
the value 1. Otherwise, the output is 0. We can think of the AND operation as the
ALL operation; that is, the output is 1 only if ALL two inputs are 1. Otherwise,
the output is 0.
A convenient mechanism for representing the behavior of a logical operation
is the truth table. A truth table consists of n + 1 columns and 2n rows. The rst
n columns correspond to the n source operands. Since each source operand is a
logical variable and can have one of two values, there are 2n unique values that
these source operands can have. Each such set of values (sometimes called an
input combination) is represented as one row of the truth table. The nal column
in the truth table shows the output for each input combination.
In the case of a two-input AND function, the truth table has two columns for
source operands, and four (22) rows for unique input combinations.
A
B
AND
0
0
0
0
1
0
1
0
0
1
1
1
We can apply the logical operation AND to two bit patterns of m bits each. This
involves applying the operation individually and independently to each pair of","{'page_number': 41, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Note that the sum of a negative number and a positive number never presents\na problem. Why is that? See Exercise 2.25.\n2.6 Operations on Bits\nPart II: Logical Operations\nWe have seen that it is possible to perform arithmetic (e.g., add, subtract) on val-\nues represented as binary patterns. Another class of operations useful to perform\non binary patterns is the set of logical operations.\n2.6.1 A Logical Variable\nLogical operations operate on logical variables. A logical variable can have one\nof two values, 0 or 1. The name logical is a historical one; it comes from the fact\nthat the two values 0 and 1 can represent the two logical values false and true,\nbut the use of logical operations has traveled far from this original meaning.\nThere are several basic logic functions, and most ALUs perform all of them.\n2.6.2 The AND Function\nAND is a binary logical function. This means it requires two pieces of input data.\nSaid another way, AND requires two source operands. Each source is a logical\nvariable, taking the value 0 or 1. The output of AND is 1 only if both sources have\nthe value 1. Otherwise, the output is 0. We can think of the AND operation as the\nALL operation; that is, the output is 1 only if ALL two inputs are 1. Otherwise,\nthe output is 0.\nA convenient mechanism for representing the behavior of a logical operation\nis the truth table. A truth table consists of n + 1 columns and 2n rows. The rst\nn columns correspond to the n source operands. Since each source operand is a\nlogical variable and can have one of two values, there are 2n unique values that\nthese source operands can have. Each such set of values (sometimes called an\ninput combination) is represented as one row of the truth table. The nal column\nin the truth table shows the output for each input combination.\nIn the case of a two-input AND function, the truth table has two columns for\nsource operands, and four (22) rows for unique input combinations.\nA\nB\nAND\n0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n1\nWe can apply the logical operation AND to two bit patterns of m bits each. This\ninvolves applying the operation individually and independently to each pair of'}"
"bits in the two source operands. For example, if a and b in Example 2.6 are 16-
bit patterns, then c is the AND of a and b. This operation is often called a bit-
wise AND because the operation is applied to each pair of bits individually and
independently.
Example 2.6
If c is the AND of a and b, where a=0011101001101001 and b=0101100100100001,
what is c?
We form the AND of a and b by bit-wise ANDing the two values.
That means individually ANDing each pair of bits ai and bi to form ci. For
example, since a0=1 and b0=1, c0 is the AND of a0 and b0, which is 1.
Since a6=1 and b6=0, c6 is the AND of a6 and b6, which is 0.
The complete solution for c is
a: 0011101001101001
b: 0101100100100001
c: 0001100000100001
Exam
Suppose we have an eight-bit patternlets call it Ain which the rightmost two
bits have particular signicance. The computer could be asked to do one of four tasks
depending on the value stored in the two rightmost bits of A. Can we isolate those
two bits?
Yes, we can, using a bit mask. A bit mask is a binary pattern that enables the bits
of A to be separated into two partsgenerally the part you care about and the part
you wish to ignore. In this case, the bit mask 00000011 ANDed with A produces 0 in
bit positions 7 through 2, and the original values of bits 1 and 0 of A in bit positions
1 and 0. The bit mask is said to mask out the values in bit positions 7 through 2.
If A is 01010110, the AND of A and the bit mask 00000011 is 00000010. If A is
11111100, the AND of A and the bit mask 00000011 is 00000000.
That is, the result of ANDing any eight-bit pattern with the mask 00000011 is
one of the four patterns: 00000000, 00000001, 00000010, or 00000011. The result
of ANDing with the mask is to highlight the two bits that are relevant.
2.6.3 The OR Function
OR is also a binary logical function. It requires two source operands, both of
which are logical variables. The output of OR is 1 if any source has the value 1.
Only if both sources are 0 is the output 0. We can think of the OR operation as
the ANY operation; that is, the output is 1 if ANY of the two inputs are 1.
The truth table for a two-input OR function is
A
B
OR
0
0
0
0
1
1
1
0
1
1
1
1
In the same way that we applied the logical operation AND to two m-bit patterns,
we can apply the OR operation bit-wise to two m-bit patterns.","{'page_number': 42, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'bits in the two source operands. For example, if a and b in Example 2.6 are 16-\nbit patterns, then c is the AND of a and b. This operation is often called a bit-\nwise AND because the operation is applied to each pair of bits individually and\nindependently.\nExample 2.6\nIf c is the AND of a and b, where a=0011101001101001 and b=0101100100100001,\nwhat is c?\nWe form the AND of a and b by bit-wise ANDing the two values.\nThat means individually ANDing each pair of bits ai and bi to form ci. For\nexample, since a0=1 and b0=1, c0 is the AND of a0 and b0, which is 1.\nSince a6=1 and b6=0, c6 is the AND of a6 and b6, which is 0.\nThe complete solution for c is\na: 0011101001101001\nb: 0101100100100001\nc: 0001100000100001\nExam\nSuppose we have an eight-bit patternlets call it Ain which the rightmost two\nbits have particular signicance. The computer could be asked to do one of four tasks\ndepending on the value stored in the two rightmost bits of A. Can we isolate those\ntwo bits?\nYes, we can, using a bit mask. A bit mask is a binary pattern that enables the bits\nof A to be separated into two partsgenerally the part you care about and the part\nyou wish to ignore. In this case, the bit mask 00000011 ANDed with A produces 0 in\nbit positions 7 through 2, and the original values of bits 1 and 0 of A in bit positions\n1 and 0. The bit mask is said to mask out the values in bit positions 7 through 2.\nIf A is 01010110, the AND of A and the bit mask 00000011 is 00000010. If A is\n11111100, the AND of A and the bit mask 00000011 is 00000000.\nThat is, the result of ANDing any eight-bit pattern with the mask 00000011 is\none of the four patterns: 00000000, 00000001, 00000010, or 00000011. The result\nof ANDing with the mask is to highlight the two bits that are relevant.\n2.6.3 The OR Function\nOR is also a binary logical function. It requires two source operands, both of\nwhich are logical variables. The output of OR is 1 if any source has the value 1.\nOnly if both sources are 0 is the output 0. We can think of the OR operation as\nthe ANY operation; that is, the output is 1 if ANY of the two inputs are 1.\nThe truth table for a two-input OR function is\nA\nB\nOR\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n1\n1\nIn the same way that we applied the logical operation AND to two m-bit patterns,\nwe can apply the OR operation bit-wise to two m-bit patterns.'}"
"Example 2.8
If c is the OR of a and b, where a=0011101001101001 and b=0101100100100001,
as before, what is c?
We form the OR of a and b by bit-wise ORing the two values. That means
individually ORing each pair of bits ai and bi to form ci. For example, since a0=1
and b0=1, c0 is the OR of a0 and b0, which is 1. Since a6=1 and b6=0, c6 is the OR
of a6 and b6, which is also 1.
The complete solution for c is
a: 0011101001101001
b: 0101100100100001
c: 0111101101101001
Sometimes this OR operation is referred to as the inclusive-OR in order to distinguish
it from the exclusive-OR function, which we will discuss momentarily.
2.6.4 The NOT Function
NOT is a unary logical function. This means it operates on only one source
operand. It is also known as the complement operation. The output is formed by
complementing the input. We sometimes say the output is formed by inverting
the input. A 1 input results in a 0 output. A 0 input results in a 1 output.
The truth table for the NOT function is
A
NOT
0
1
1
0
In the same way that we applied the logical operation AND and OR to two m-bit
patterns, we can apply the NOT operation bit-wise to one m-bit pattern. If a is as
before, then c is the NOT of a.
a: 0011101001101001
c: 1100010110010110
2.6.5 The Exclusive-OR Function
Exclusive-OR, often abbreviated XOR, is a binary logical function. It, too,
requires two source operands, both of which are logical variables. The output
of XOR is 1 if one (but not both) of the two sources is 1. The output of XOR is 0
if both sources are 1 or if neither source is 1. In other words, the output of XOR is
1 if the two sources are dierent. The output is 0 if the two sources are the same.
The truth table for the XOR function is
A
B
XOR
0
0
0
0
1
1
1
0
1
1
1
0","{'page_number': 43, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Example 2.8\nIf c is the OR of a and b, where a=0011101001101001 and b=0101100100100001,\nas before, what is c?\nWe form the OR of a and b by bit-wise ORing the two values. That means\nindividually ORing each pair of bits ai and bi to form ci. For example, since a0=1\nand b0=1, c0 is the OR of a0 and b0, which is 1. Since a6=1 and b6=0, c6 is the OR\nof a6 and b6, which is also 1.\nThe complete solution for c is\na: 0011101001101001\nb: 0101100100100001\nc: 0111101101101001\nSometimes this OR operation is referred to as the inclusive-OR in order to distinguish\nit from the exclusive-OR function, which we will discuss momentarily.\n2.6.4 The NOT Function\nNOT is a unary logical function. This means it operates on only one source\noperand. It is also known as the complement operation. The output is formed by\ncomplementing the input. We sometimes say the output is formed by inverting\nthe input. A 1 input results in a 0 output. A 0 input results in a 1 output.\nThe truth table for the NOT function is\nA\nNOT\n0\n1\n1\n0\nIn the same way that we applied the logical operation AND and OR to two m-bit\npatterns, we can apply the NOT operation bit-wise to one m-bit pattern. If a is as\nbefore, then c is the NOT of a.\na: 0011101001101001\nc: 1100010110010110\n2.6.5 The Exclusive-OR Function\nExclusive-OR, often abbreviated XOR, is a binary logical function. It, too,\nrequires two source operands, both of which are logical variables. The output\nof XOR is 1 if one (but not both) of the two sources is 1. The output of XOR is 0\nif both sources are 1 or if neither source is 1. In other words, the output of XOR is\n1 if the two sources are dierent. The output is 0 if the two sources are the same.\nThe truth table for the XOR function is\nA\nB\nXOR\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0'}"
"In the same way that we applied the logical operation AND to two m-bit patterns,
we can apply the XOR operation bit-wise to two m-bit patterns.
Example 2.9
If a and b are 16-bit patterns as before, then c (shown here) is the XOR of a and b.
a: 0011101001101001
b: 0101100100100001
c: 0110001101001000
Note the distinction between the truth table for XOR shown here and the truth table
for OR shown earlier. In the case of exclusive-OR, if both source operands are 1, the
output is 0. That is, the output is 1 if the rst operand is 1 but the second operand is
not 1 or if the second operand is 1 but the rst operand is not 1. The term exclusive
is used because the output is 1 if only one of the two sources is 1. The OR function,
on the other hand, produces an output 1 if only one of the two sources is 1, or if both
sources are 1. Ergo, the name inclusive-OR.
Example 2.10
Suppose we wish to know if two patterns are identical. Since the XOR function pro-
duces a 0 only if the corresponding pair of bits is identical, two patterns are identical
if the output of the XOR is all 0s.
2.6.6 DeMorgans Laws
There are two well-known relationships between AND functions and OR func-
t

1
0
1
0
0
0
0
1
1
1
0
1
Figure 2.2
DeMorgans Law.","{'page_number': 44, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In the same way that we applied the logical operation AND to two m-bit patterns,\nwe can apply the XOR operation bit-wise to two m-bit patterns.\nExample 2.9\nIf a and b are 16-bit patterns as before, then c (shown here) is the XOR of a and b.\na: 0011101001101001\nb: 0101100100100001\nc: 0110001101001000\nNote the distinction between the truth table for XOR shown here and the truth table\nfor OR shown earlier. In the case of exclusive-OR, if both source operands are 1, the\noutput is 0. That is, the output is 1 if the rst operand is 1 but the second operand is\nnot 1 or if the second operand is 1 but the rst operand is not 1. The term exclusive\nis used because the output is 1 if only one of the two sources is 1. The OR function,\non the other hand, produces an output 1 if only one of the two sources is 1, or if both\nsources are 1. Ergo, the name inclusive-OR.\nExample 2.10\nSuppose we wish to know if two patterns are identical. Since the XOR function pro-\nduces a 0 only if the corresponding pair of bits is identical, two patterns are identical\nif the output of the XOR is all 0s.\n2.6.6 DeMorgans Laws\nThere are two well-known relationships between AND functions and OR func-\nt\n\n1\n0\n1\n0\n0\n0\n0\n1\n1\n1\n0\n1\nFigure 2.2\nDeMorgans Law.'}"
"the
B =
fun
repr
A AND B = A OR B
We can also state this behavior in English:
It is not the case that both A and B are false is equivalent to saying At least
one of A and B is true.
This equivalence is known as one of two DeMorgans Laws. Question: Is there
a similar result if one inverts both inputs to an OR function, and then inverts the
output?
2.6.7 The Bit Vector
We have discussed the AND, OR, and NOT functions performed on m-bit pat-
terns, where each of the m bits is a logical value (0 or 1) and the operations are per-
formed bit-wise (i.e., individually and independently). We have also discussed the
use of an m-bit bit mask, where our choice of 0 or 1 for each bit allows us to isolate
the bits we are interested in focusing on and ignore the bits that dont matter.
An m-bit pattern where each bit has a logical value (0 or 1) independent of
the other bits is called a bit vector. It is a convenient mechanism for identifying
a property such that some of the bits identify the presence of the property and
other bits identify the absence of the property.
There are many uses for bit vectors. The most common use is a bit mask,
as we saw in Example 2.7. In that example, we had an eight-bit value, and we
wanted to focus on bit 1 and bit 0 of that value. We did not care about the other
bits. Performing the AND of that value with the bit mask 00000011 caused bit 7
through bit 2 to be ignored, resulting in the AND function producing 00000000,
00000001, 00000010, or 00000011, depending on the values of bit 1 and bit 0.
The bit mask is a bit vector, where the property of each of the bits is whether or
not we care about that bit. In Example 2.7, we only cared about bit 1 and bit 0.
Another use of a bit mask could involve a 16-bit 2s complement integer.
Suppose the only thing we cared about was whether the integer was odd or even
and whether it was positive or negative. The bit vector 1000000000000001 has a 1 in
bit 15 that is used to identify a number as positive or negative, and a 1 in bit 0 that is
used to identify if the integer is odd or even. If we perform the AND of this bit vector
with a 16-bit 2s complement integer, we would get one of four results, depending
on whether the integer was positive or negative and odd or even:
0000000000000000
0000000000000001
1000000000000000
1000000000000001
Another common use of bit vectors involves managing a complex system
made up of several units, each of which is individually and independently either","{'page_number': 45, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'the\nB =\nfun\nrepr\nA AND B = A OR B\nWe can also state this behavior in English:\nIt is not the case that both A and B are false is equivalent to saying At least\none of A and B is true.\nThis equivalence is known as one of two DeMorgans Laws. Question: Is there\na similar result if one inverts both inputs to an OR function, and then inverts the\noutput?\n2.6.7 The Bit Vector\nWe have discussed the AND, OR, and NOT functions performed on m-bit pat-\nterns, where each of the m bits is a logical value (0 or 1) and the operations are per-\nformed bit-wise (i.e., individually and independently). We have also discussed the\nuse of an m-bit bit mask, where our choice of 0 or 1 for each bit allows us to isolate\nthe bits we are interested in focusing on and ignore the bits that dont matter.\nAn m-bit pattern where each bit has a logical value (0 or 1) independent of\nthe other bits is called a bit vector. It is a convenient mechanism for identifying\na property such that some of the bits identify the presence of the property and\nother bits identify the absence of the property.\nThere are many uses for bit vectors. The most common use is a bit mask,\nas we saw in Example 2.7. In that example, we had an eight-bit value, and we\nwanted to focus on bit 1 and bit 0 of that value. We did not care about the other\nbits. Performing the AND of that value with the bit mask 00000011 caused bit 7\nthrough bit 2 to be ignored, resulting in the AND function producing 00000000,\n00000001, 00000010, or 00000011, depending on the values of bit 1 and bit 0.\nThe bit mask is a bit vector, where the property of each of the bits is whether or\nnot we care about that bit. In Example 2.7, we only cared about bit 1 and bit 0.\nAnother use of a bit mask could involve a 16-bit 2s complement integer.\nSuppose the only thing we cared about was whether the integer was odd or even\nand whether it was positive or negative. The bit vector 1000000000000001 has a 1 in\nbit 15 that is used to identify a number as positive or negative, and a 1 in bit 0 that is\nused to identify if the integer is odd or even. If we perform the AND of this bit vector\nwith a 16-bit 2s complement integer, we would get one of four results, depending\non whether the integer was positive or negative and odd or even:\n0000000000000000\n0000000000000001\n1000000000000000\n1000000000000001\nAnother common use of bit vectors involves managing a complex system\nmade up of several units, each of which is individually and independently either'}"
"busy or available. The system could be a manufacturing plant where each unit is
a particular machine. Or the system could be a taxicab network where each unit
is a particular taxicab. In both cases, it is important to identify which units are
busy and which are available so that work can be properly assigned.
Say we have m such units. We can keep track of these m units with a bit
vector, where a bit is 1 if the unit is free and 0 if the unit is busy.
Example 2.11
Suppose we have eight machines that we want to monitor with respect to their avail-
ability. We can keep track of them with an eight-bit BUSYNESS bit vector, where a
bit is 1 if the unit is free and 0 if the unit is busy. The bits are labeled, from right to
left, from 0 to 7.
The BUSYNESS bit vector 11000010 corresponds to the situation where only
units 7, 6, and 1 are free and therefore available for work assignment.
Suppose work is assigned to unit 7. We update our BUSYNESS bit vector by per-
forming the logical AND, where our two sources are the current bit vector 11000010
and the bit mask 01111111. The purpose of the bit mask is to clear bit 7 of the
BUSYNESS bit vector, while leaving alone the values corresponding to all the other
units. The result is the bit vector 01000010, indicating that unit 7 is now busy.
Suppose unit 5 nishes its task and becomes idle. We can update the
BUSYNESS bit vector by performing the logical OR of it with the bit mask
00100000. The result is 01100010, indicating that unit 5 is now available.
2.7 Other Representations
There are many other representations of information that are used in computers.
Two that are among the most useful are the oating point data type and ASCII
codes. We will describe both in this section. We will also describe a notation
called hexadecimal that, although not a data type, is convenient for humans to
use when dealing with long strings of 0s and 1s.
2.7.1 Floating Point Data Type (Greater Range, Less Precision)
Most of the arithmetic in this book uses integer values. The LC-3 computer, which
you will start studying in Chapter 4, uses the 16-bit, 2s complement integer data
type. That data type provides one bit to identify whether the number is positive
or negative and 15 bits to represent the magnitude of the value. With 16 bits used
in this way, we can express integer values between 32,768 and +32,767, that
is, between 215 and +215  1. We say the precision of our value is 15 bits, and
the range is 216. As you learned in high school chemistry class, sometimes we
need to express much larger numbers, but we do not require so many digits of
precision. In fact, recall the value 6.0221023, which you may have been required
to memorize back then. The range needed to express the value 1023 is far greater
than the largest value 2151 that is available with 16-bit 2s complement integers.
On the other hand, the 15 bits of precision available with 16-bit 2s complement
integers are overkill","{'page_number': 46, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In fact, recall the value 6.0221023, which you may have been required\nto memorize back then. The range needed to express the value 1023 is far greater\nthan the largest value 2151 that is available with 16-bit 2s complement integers.\nOn the other hand, the 15 bits of precision available with 16-bit 2s complement\nintegers are overkill. We need only enough bits to express four signicant decimal\ndigits (6022).'}"
"In fact, recall the value 6.0221023, which you may have been required
to memorize back then. The range needed to express the value 1023 is far greater
than the largest value 2151 that is available with 16-bit 2s complement integers.
On the other hand, the 15 bits of precision available with 16-bit 2s complement
integers are overkill. We need only enough bits to express four signicant decimal
digits (6022).","{'page_number': 46, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In fact, recall the value 6.0221023, which you may have been required\nto memorize back then. The range needed to express the value 1023 is far greater\nthan the largest value 2151 that is available with 16-bit 2s complement integers.\nOn the other hand, the 15 bits of precision available with 16-bit 2s complement\nintegers are overkill. We need only enough bits to express four signicant decimal\ndigits (6022).'}"
"So we have a problem. We have more bits than we need for precision. But we
dont have enough bits to represent the range.
The oating point data type solves the problem. Instead of using all the bits
to represent the precision of a value, the oating point data type allocates some of
the bits to the range of values (i.e., how big or how small) that can be expressed.
The rest of the bits (except for the sign bit) are used for precision.
Most ISAs today specify more than one oating point data type. One of them,
usually called oat, consists of 32 bits, allocated as follows:
I
Figure 2.3
The 32-bit oating point data type.
2.7.1.1 Normalized Form
Like Avogadros number that you learned years ago, the oating point data type
represents numbers expressed in scientic notation, and mostly in normalized
form:
N = (1)S  1. fraction  2exponent127, 1  exponent  254
where S, fraction, and exponent are the binary numbers in the elds of Figure 2.3.
We say mostly in normalized form because (as noted in the equation) the data
type represents a oating point number in normalized form only if the eight-bit
exponent is restricted to the 254 unsigned integer values, 1 (00000001) through
254 (11111110).
As you know, with eight bits, one can represent 256 values uniquely. For the
other two integer values 0 (00000000) and 255 (11111111), the oating point
data type does not represent normalized numbers. We will explain what it does
represent in Section 2.7.1.2 and Section 2.7.1.3.
Recall again Avogadros number: (a) an implied + sign (often left out when
the value is positive), (b) four decimal digits 6.022 in normalized form (one non-
zero decimal digit 6 before the decimal point) times (c) the radix 10 raised to
the power 23. The computers 32-bit oating point data type, on the other hand,
consists of (a) a sign bit (positive or negative), (b) 24 binary digits in normalized
form (one non-zero binary digit to the left of the binary point) times (c) the radix
2 raised to an exponent expressed in eight bits.
We determine the value of the 32-bit oating point representation shown in
Figure 2.3 by examining its three parts.","{'page_number': 47, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'So we have a problem. We have more bits than we need for precision. But we\ndont have enough bits to represent the range.\nThe oating point data type solves the problem. Instead of using all the bits\nto represent the precision of a value, the oating point data type allocates some of\nthe bits to the range of values (i.e., how big or how small) that can be expressed.\nThe rest of the bits (except for the sign bit) are used for precision.\nMost ISAs today specify more than one oating point data type. One of them,\nusually called oat, consists of 32 bits, allocated as follows:\nI\nFigure 2.3\nThe 32-bit oating point data type.\n2.7.1.1 Normalized Form\nLike Avogadros number that you learned years ago, the oating point data type\nrepresents numbers expressed in scientic notation, and mostly in normalized\nform:\nN = (1)S  1. fraction  2exponent127, 1  exponent  254\nwhere S, fraction, and exponent are the binary numbers in the elds of Figure 2.3.\nWe say mostly in normalized form because (as noted in the equation) the data\ntype represents a oating point number in normalized form only if the eight-bit\nexponent is restricted to the 254 unsigned integer values, 1 (00000001) through\n254 (11111110).\nAs you know, with eight bits, one can represent 256 values uniquely. For the\nother two integer values 0 (00000000) and 255 (11111111), the oating point\ndata type does not represent normalized numbers. We will explain what it does\nrepresent in Section 2.7.1.2 and Section 2.7.1.3.\nRecall again Avogadros number: (a) an implied + sign (often left out when\nthe value is positive), (b) four decimal digits 6.022 in normalized form (one non-\nzero decimal digit 6 before the decimal point) times (c) the radix 10 raised to\nthe power 23. The computers 32-bit oating point data type, on the other hand,\nconsists of (a) a sign bit (positive or negative), (b) 24 binary digits in normalized\nform (one non-zero binary digit to the left of the binary point) times (c) the radix\n2 raised to an exponent expressed in eight bits.\nWe determine the value of the 32-bit oating point representation shown in\nFigure 2.3 by examining its three parts.'}"
"The sign bit S is just a single binary digit, 0 for positive numbers, 1 for neg-
ative numbers. The formula contains the factor 1S, which evaluates to +1 if
S = 0, and 1 if S = 1.
The 23 fraction bits form the 24-bit quantity 1.fraction, where normalized
form demands exactly one non-zero binary digit to the left of the binary point.
Since there exists only one non-zero binary digit (i.e., the value 1), it is unneces-
sary to explicitly store that bit in our 32-bit oating point format. In fact that is
how we get 24 bits of precision, the 1 to the left of the binary point that is always
present in normalized numbers and so is unnecessary to store, and the 23 bits of
fraction that are actually part of the 32-bit data type.
The eight exponent bits are encoded in what we call an excess code, named
for the notion that one can get the *real* exponent by treating the code as
an unsigned integer and subtracting the excess (sometimes called the bias). In
the case of the IEEE Floating Point that almost everyone uses, that excess (or
bias) is 127 for 32-bit oating point numbers. Thus, an exponent eld contain-
ing 10000110 corresponds to the exponent +7 (since 10000110 represents the
unsigned integer 134, from which we subtract 127, yielding +7). An exponent
eld containing 00000111 corresponds to the exponent 120 (since 00000111
represents the unsigned integer 7, from which we subtract 127, yielding 120).
The exponent eld gives us numbers as large as 2+127 for an exponent eld con-
taining 254 (11111110) and as small as 2126 for an exponent eld containing
1 (00000001).
Example 2.12
What does the oating point data type
00111101100000000000000000000000
represent?
The leading bit is a 0. This signies a positive number. The next eight bits
represent the unsigned number 123. If we subtract 127, we get the actual expo-
nent 4. The last 23 bits are all 0. Therefore, the number being represented is
+1.000000000000000000000000  24, which is 1
16.
Example 2.13
How is the number 6 5
8 represented in the oating point data type?
First, we express 6 5
8 as a binary number: 110.101.
(1  22 + 1  21 + 0  20 + 1  21 + 0  22 + 1  23)
Then we normalize the value, yielding 1.10101  22.
The sign bit is 1, reecting the fact that 6 5
8 is a negative number. The exponent
eld contains 10000001, the unsigned number 129, reecting the fact that the real
exponent is +2 (129127 = +2). The fraction is the 23 bits of precision, after remov-
ing the leading 1. That is, the fraction is 10101000000000000000000. The result is
the number 6 5
8, expressed as a oating point number:
1 10000001 10101000000000000000000","{'page_number': 48, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The sign bit S is just a single binary digit, 0 for positive numbers, 1 for neg-\native numbers. The formula contains the factor 1S, which evaluates to +1 if\nS = 0, and 1 if S = 1.\nThe 23 fraction bits form the 24-bit quantity 1.fraction, where normalized\nform demands exactly one non-zero binary digit to the left of the binary point.\nSince there exists only one non-zero binary digit (i.e., the value 1), it is unneces-\nsary to explicitly store that bit in our 32-bit oating point format. In fact that is\nhow we get 24 bits of precision, the 1 to the left of the binary point that is always\npresent in normalized numbers and so is unnecessary to store, and the 23 bits of\nfraction that are actually part of the 32-bit data type.\nThe eight exponent bits are encoded in what we call an excess code, named\nfor the notion that one can get the *real* exponent by treating the code as\nan unsigned integer and subtracting the excess (sometimes called the bias). In\nthe case of the IEEE Floating Point that almost everyone uses, that excess (or\nbias) is 127 for 32-bit oating point numbers. Thus, an exponent eld contain-\ning 10000110 corresponds to the exponent +7 (since 10000110 represents the\nunsigned integer 134, from which we subtract 127, yielding +7). An exponent\neld containing 00000111 corresponds to the exponent 120 (since 00000111\nrepresents the unsigned integer 7, from which we subtract 127, yielding 120).\nThe exponent eld gives us numbers as large as 2+127 for an exponent eld con-\ntaining 254 (11111110) and as small as 2126 for an exponent eld containing\n1 (00000001).\nExample 2.12\nWhat does the oating point data type\n00111101100000000000000000000000\nrepresent?\nThe leading bit is a 0. This signies a positive number. The next eight bits\nrepresent the unsigned number 123. If we subtract 127, we get the actual expo-\nnent 4. The last 23 bits are all 0. Therefore, the number being represented is\n+1.000000000000000000000000  24, which is 1\n16.\nExample 2.13\nHow is the number 6 5\n8 represented in the oating point data type?\nFirst, we express 6 5\n8 as a binary number: 110.101.\n(1  22 + 1  21 + 0  20 + 1  21 + 0  22 + 1  23)\nThen we normalize the value, yielding 1.10101  22.\nThe sign bit is 1, reecting the fact that 6 5\n8 is a negative number. The exponent\neld contains 10000001, the unsigned number 129, reecting the fact that the real\nexponent is +2 (129127 = +2). The fraction is the 23 bits of precision, after remov-\ning the leading 1. That is, the fraction is 10101000000000000000000. The result is\nthe number 6 5\n8, expressed as a oating point number:\n1 10000001 10101000000000000000000'}"
"Example 2.14
The following three examples provide further illustrations of the interpretation of
the 32-bit oating point data type according to the rules of the IEEE standard.
0 10000011 00101000000000000000000 is 1.00101  24 = 18.5
The exponent eld contains the unsigned number 131. Since 131  127 is 4, the
exponent is +4. Combining a 1 to the left of the binary point with the fraction eld
to the right of the binary point yields 1.00101. If we move the binary point four
positions to the right, we get 10010.1, which is 18.5.
1 10000010 00101000000000000000000 is 1  1.00101  23 = 9.25
The sign bit is 1, signifying a negative number. The exponent is 130, signifying an
exponent of 130  127, or +3. Combining a 1 to the left of the binary point with
the fraction eld to the right of the binary point yields 1.00101. Moving the binary
point three positions to the right, we get 1001.01, which is 9.25.
0 11111110 11111111111111111111111 is 2128
The sign is +. The exponent is 254  127, or +127. Combining a 1 to the left
of the binary point with the fraction eld to the right of the binary point yields
1.11111111  1, which is approximately 2. Therefore, the result is approximately
2128.
2.7.1.2 Innities
We noted above that the oating point data type represented numbers expressed
in scientic notation in normalized form provided the exponent eld does not
contain 00000000 or 11111111.
If the exponent eld contains 11111111, we use the oating point data type to
represent various things, among them the notion of innity. Innity is represented
by the exponent eld containing all 1s and the fraction eld containing all 0s. We
represent positive innity if the sign bit is 0 and negative innity if the sign bit is 1.
2.7.1.3 Subnormal Numbers
The smallest number that can be represented in normalized form is
N = 1.00000000000000000000000  2126
What about numbers smaller than 2126 but larger than 0? We call such num-
bers subnormal numbers because they cannot be represented in normalized form.
The largest subnormal number is
N = 0.11111111111111111111111  2126
The smallest subnormal number is
N = 0.00000000000000000000001  2126, i.e., 223  2126 which is 2149.
Note that the largest subnormal number is 2126 minus 2149. Do you see why
that is the case?","{'page_number': 49, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Example 2.14\nThe following three examples provide further illustrations of the interpretation of\nthe 32-bit oating point data type according to the rules of the IEEE standard.\n0 10000011 00101000000000000000000 is 1.00101  24 = 18.5\nThe exponent eld contains the unsigned number 131. Since 131  127 is 4, the\nexponent is +4. Combining a 1 to the left of the binary point with the fraction eld\nto the right of the binary point yields 1.00101. If we move the binary point four\npositions to the right, we get 10010.1, which is 18.5.\n1 10000010 00101000000000000000000 is 1  1.00101  23 = 9.25\nThe sign bit is 1, signifying a negative number. The exponent is 130, signifying an\nexponent of 130  127, or +3. Combining a 1 to the left of the binary point with\nthe fraction eld to the right of the binary point yields 1.00101. Moving the binary\npoint three positions to the right, we get 1001.01, which is 9.25.\n0 11111110 11111111111111111111111 is 2128\nThe sign is +. The exponent is 254  127, or +127. Combining a 1 to the left\nof the binary point with the fraction eld to the right of the binary point yields\n1.11111111  1, which is approximately 2. Therefore, the result is approximately\n2128.\n2.7.1.2 Innities\nWe noted above that the oating point data type represented numbers expressed\nin scientic notation in normalized form provided the exponent eld does not\ncontain 00000000 or 11111111.\nIf the exponent eld contains 11111111, we use the oating point data type to\nrepresent various things, among them the notion of innity. Innity is represented\nby the exponent eld containing all 1s and the fraction eld containing all 0s. We\nrepresent positive innity if the sign bit is 0 and negative innity if the sign bit is 1.\n2.7.1.3 Subnormal Numbers\nThe smallest number that can be represented in normalized form is\nN = 1.00000000000000000000000  2126\nWhat about numbers smaller than 2126 but larger than 0? We call such num-\nbers subnormal numbers because they cannot be represented in normalized form.\nThe largest subnormal number is\nN = 0.11111111111111111111111  2126\nThe smallest subnormal number is\nN = 0.00000000000000000000001  2126, i.e., 223  2126 which is 2149.\nNote that the largest subnormal number is 2126 minus 2149. Do you see why\nthat is the case?'}"
"Subnormal numbers are numbers of the form
N = (1)S  0. fraction  2126
We represent them with an exponent eld of 00000000. The fraction eld is
represented in the same way as with normalized numbers. That is, if the expo-
nent eld contains 00000000, the exponent is 126, and the signicant digits are
obtained by starting with a leading 0, followed by a binary point, followed by the
23 bits of the fraction eld.
Example 2.15
What number corresponds to the following oating point representation?
0 00000000 00001000000000000000000
Answer: The leading 0 means the number is positive. The next eight bits, a zero
exponent, means the exponent is 126, and the bit to the left of the binary point is 0.
The last 23 bits form the number 0.00001000000000000000000, which equals 25.
Thus, the number represented is 25  2126, which is 2131.
Including subnormal numbers allows very, very tiny numbers to be repre-
sented.
A detailed understanding of IEEE Floating Point Arithmetic is well beyond
what should be expected in this rst course. Our purpose in including this sec-
tion in the textbook is to at least let you know that there is, in addition to 2s
complement integers, another very important data type available in almost all
ISAs, which is called oating point; it allows very large and very tiny numbers to
be expressed at the cost of reducing the number of binary digits of precision.
2.7.2 ASCII Codes
Another representation of information is the standard code that almost all com-
puter equipment manufacturers have agreed to use for transferring characters
between the main computer processing unit and the input and output devices.
That code is an eight-bit code referred to as ASCII. ASCII stands for Ameri-
can Standard Code for Information Interchange. It (ASCII) greatly simplies the
interface between a keyboard manufactured by one company, a computer made
by another company, and a monitor made by a third company.
Each key on the keyboard is identied by its unique ASCII code. So, for
example, the digit 3 is represented as 00110011, the digit 2 is 00110010, the
lowercase e is 01100101, and the ENTER key is 00001101. The entire set of
eight-bit ASCII codes is listed in Figure E.2 of Appendix E. When you type a key
on the keyboard, the corresponding eight-bit code is stored and made available to
the computer. Where it is stored and how it gets into the computer are discussed
in Chapter 9.
Most keys are associated with more than one code. For example, the ASCII
code for the letter E is 01000101, and the ASCII code for the letter e is 01100101.","{'page_number': 50, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Subnormal numbers are numbers of the form\nN = (1)S  0. fraction  2126\nWe represent them with an exponent eld of 00000000. The fraction eld is\nrepresented in the same way as with normalized numbers. That is, if the expo-\nnent eld contains 00000000, the exponent is 126, and the signicant digits are\nobtained by starting with a leading 0, followed by a binary point, followed by the\n23 bits of the fraction eld.\nExample 2.15\nWhat number corresponds to the following oating point representation?\n0 00000000 00001000000000000000000\nAnswer: The leading 0 means the number is positive. The next eight bits, a zero\nexponent, means the exponent is 126, and the bit to the left of the binary point is 0.\nThe last 23 bits form the number 0.00001000000000000000000, which equals 25.\nThus, the number represented is 25  2126, which is 2131.\nIncluding subnormal numbers allows very, very tiny numbers to be repre-\nsented.\nA detailed understanding of IEEE Floating Point Arithmetic is well beyond\nwhat should be expected in this rst course. Our purpose in including this sec-\ntion in the textbook is to at least let you know that there is, in addition to 2s\ncomplement integers, another very important data type available in almost all\nISAs, which is called oating point; it allows very large and very tiny numbers to\nbe expressed at the cost of reducing the number of binary digits of precision.\n2.7.2 ASCII Codes\nAnother representation of information is the standard code that almost all com-\nputer equipment manufacturers have agreed to use for transferring characters\nbetween the main computer processing unit and the input and output devices.\nThat code is an eight-bit code referred to as ASCII. ASCII stands for Ameri-\ncan Standard Code for Information Interchange. It (ASCII) greatly simplies the\ninterface between a keyboard manufactured by one company, a computer made\nby another company, and a monitor made by a third company.\nEach key on the keyboard is identied by its unique ASCII code. So, for\nexample, the digit 3 is represented as 00110011, the digit 2 is 00110010, the\nlowercase e is 01100101, and the ENTER key is 00001101. The entire set of\neight-bit ASCII codes is listed in Figure E.2 of Appendix E. When you type a key\non the keyboard, the corresponding eight-bit code is stored and made available to\nthe computer. Where it is stored and how it gets into the computer are discussed\nin Chapter 9.\nMost keys are associated with more than one code. For example, the ASCII\ncode for the letter E is 01000101, and the ASCII code for the letter e is 01100101.'}"
"Both are associated with the same key, although in one case the Shift key is also
depressed while in the other case, it is not.
In order to display a particular character on the monitor, the computer must
transfer the ASCII code for that character to the electronics associated with the
monitor. That, too, is discussed in Chapter 9.
2.7.3 Hexadecimal Notation
We have seen that information can be represented as 2s complement integers,
as bit vectors, in oating point format, or as an ASCII code. There are other
representations also, but we will leave them for another book. However, before
we leave this topic, we would like to introduce you to a representation that is used
more as a convenience for humans than as a data type to support operations being
performed by the computer. This is the hexadecimal notation. As we will see, it
evolves nicely from the positional binary notation and is useful for dealing with
long strings of binary digits without making errors.
It will be particularly useful in dealing with the LC-3 where 16-bit binary
strings will be encountered often.
An example of such a binary string is
0011110101101110
Lets try an experiment. Cover the preceding 16-bit binary string of 0s and 1s
with one hand, and try to write it down from memory. How did you do? Hex-
adecimal notation is about being able to do this without making mistakes. We
shall see how.
In general, a 16-bit binary string takes the form
a15 a14 a13 a12 a11 a10 a9 a8 a7 a6 a5 a4 a3 a2 a1 a0
where each of the bits ai is either 0 or 1.
If we think of this binary string as an unsigned integer, its value can be
computed as
a15  215 + a14  214 + a13  213 + a12  212 + a11  211 + a10  210
+ a9  29 + a8  28 + a7  27 + a6  26 + a5  25 + a4  24 + a3  23
+ a2  22 + a1  21 + a0  20
We can factor 212 from the rst four terms, 28 from the second four terms, 24
from the third set of four terms, and 20 from the last four terms, yielding
212(a15  23 + a14  22 + a13  21 + a12  20)
+ 28(a11  23 + a10  22 + a9  21 + a8  20)
+ 24(a7  23 + a6  22 + a5  21 + a4  20)
+ 20(a3  23 + a2  22 + a1  21 + a0  20)
Note that the largest value inside a set of parentheses is 15, which would be the
case if each of the four bits is 1. If we replace what is inside each square bracket","{'page_number': 51, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Both are associated with the same key, although in one case the Shift key is also\ndepressed while in the other case, it is not.\nIn order to display a particular character on the monitor, the computer must\ntransfer the ASCII code for that character to the electronics associated with the\nmonitor. That, too, is discussed in Chapter 9.\n2.7.3 Hexadecimal Notation\nWe have seen that information can be represented as 2s complement integers,\nas bit vectors, in oating point format, or as an ASCII code. There are other\nrepresentations also, but we will leave them for another book. However, before\nwe leave this topic, we would like to introduce you to a representation that is used\nmore as a convenience for humans than as a data type to support operations being\nperformed by the computer. This is the hexadecimal notation. As we will see, it\nevolves nicely from the positional binary notation and is useful for dealing with\nlong strings of binary digits without making errors.\nIt will be particularly useful in dealing with the LC-3 where 16-bit binary\nstrings will be encountered often.\nAn example of such a binary string is\n0011110101101110\nLets try an experiment. Cover the preceding 16-bit binary string of 0s and 1s\nwith one hand, and try to write it down from memory. How did you do? Hex-\nadecimal notation is about being able to do this without making mistakes. We\nshall see how.\nIn general, a 16-bit binary string takes the form\na15 a14 a13 a12 a11 a10 a9 a8 a7 a6 a5 a4 a3 a2 a1 a0\nwhere each of the bits ai is either 0 or 1.\nIf we think of this binary string as an unsigned integer, its value can be\ncomputed as\na15  215 + a14  214 + a13  213 + a12  212 + a11  211 + a10  210\n+ a9  29 + a8  28 + a7  27 + a6  26 + a5  25 + a4  24 + a3  23\n+ a2  22 + a1  21 + a0  20\nWe can factor 212 from the rst four terms, 28 from the second four terms, 24\nfrom the third set of four terms, and 20 from the last four terms, yielding\n212(a15  23 + a14  22 + a13  21 + a12  20)\n+ 28(a11  23 + a10  22 + a9  21 + a8  20)\n+ 24(a7  23 + a6  22 + a5  21 + a4  20)\n+ 20(a3  23 + a2  22 + a1  21 + a0  20)\nNote that the largest value inside a set of parentheses is 15, which would be the\ncase if each of the four bits is 1. If we replace what is inside each square bracket'}"
"with a symbol representing its value (from 0 to 15), and we replace 212 with its
equivalent 163, 28 with 162, 24 with 161, and 20 with 160, we have
h3  163 + h2  162 + h1  161 + h0  160
where h3, for example, is a symbol representing
a15  23 + a14  22 + a13  21 + a12  20
Since the symbols must represent values from 0 to 15, we assign symbols to these
values as follows: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F. That is, we represent
0000 with the symbol 0, 0001 with the symbol 1,  1001 with 9, 1010 with A,
1011 with B,  1111 with F. The resulting notation is called hexadecimal, or
base 16.
So, for example, if the hex digits E92F represent a 16-bit 2s complement
integer, is the value of that integer positive or negative? How do you know?
Now, then, what is this hexadecimal representation good for, anyway? It
seems like just another way to represent a number without adding any benet.
Lets return to the exercise where you tried to write from memory the string
0011110101101110
If we had rst broken the string at four-bit boundaries
0011
1101
0110
1110
and then converted each four-bit string to its equivalent hex digit
3
D
6
E
it would have been no problem to jot down (with the string covered) 3D6E.
In summary, although hexadecimal notation can be used to perform base-
16 arithmetic, it is mainly used as a convenience for humans. It can be used to
represent binary strings that are integers or oating point numbers or sequences
of ASCII codes, or bit vectors. It simply reduces the number of digits by a factor
of
res
2.1
2.
2.
requiring additional bits for each students unique bit pattern?","{'page_number': 52, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'with a symbol representing its value (from 0 to 15), and we replace 212 with its\nequivalent 163, 28 with 162, 24 with 161, and 20 with 160, we have\nh3  163 + h2  162 + h1  161 + h0  160\nwhere h3, for example, is a symbol representing\na15  23 + a14  22 + a13  21 + a12  20\nSince the symbols must represent values from 0 to 15, we assign symbols to these\nvalues as follows: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F. That is, we represent\n0000 with the symbol 0, 0001 with the symbol 1,  1001 with 9, 1010 with A,\n1011 with B,  1111 with F. The resulting notation is called hexadecimal, or\nbase 16.\nSo, for example, if the hex digits E92F represent a 16-bit 2s complement\ninteger, is the value of that integer positive or negative? How do you know?\nNow, then, what is this hexadecimal representation good for, anyway? It\nseems like just another way to represent a number without adding any benet.\nLets return to the exercise where you tried to write from memory the string\n0011110101101110\nIf we had rst broken the string at four-bit boundaries\n0011\n1101\n0110\n1110\nand then converted each four-bit string to its equivalent hex digit\n3\nD\n6\nE\nit would have been no problem to jot down (with the string covered) 3D6E.\nIn summary, although hexadecimal notation can be used to perform base-\n16 arithmetic, it is mainly used as a convenience for humans. It can be used to\nrepresent binary strings that are integers or oating point numbers or sequences\nof ASCII codes, or bit vectors. It simply reduces the number of digits by a factor\nof\nres\n2.1\n2.\n2.\nrequiring additional bits for each students unique bit pattern?'}"
"ital Logic Structures
I
n Chapter 1, we stated that computers were built from very large numbers
of very simple structures. For example, Intels Broadwell-E5 microproces-
sor, introduced in 2016, contained more than seven billion transistors. Similarly,
IBMs Power9 microprocessor, introduced in 2017, contained eight billion tran-
sistors. In this chapter, we will explain how the MOS transistor works (as a logic
element), show how these transistors are connected to form logic gates, and then
show how logic gates are interconnected to form larger units that are needed to
construct a computer. In Chapter 4, we will connect those larger units and form
a computer.
But rst, the transistor.
3.1 The Transistor
Most computers today or rather most microprocessors (which form the core of the
computer) are constructed out of MOS transistors. MOS stands for metal-oxide
semiconductor. The electrical properties of metal-oxide semiconductors are well
beyond the scope of what we want to understand in this course. They are below
our lowest level of abstraction, which means that if somehow transistors start
misbehaving, we are at their mercy. However, it is unlikely in this course that we
will have any problems from the transistors.
Still, it is useful to know that there are two types of MOS transistors: P-type
and N-type. They both operate logically, very similar to the way wall switches
work.
Figure 3.1 shows the most basic of electrical circuits. It consists of (1) a
power supply (in this case, the 120 volts that come into your house if you live in
the United States, or the 220 volts if you live in most of the rest of the world),
(2) a wall switch, and (3) a lamp (plugged into an outlet in the wall). In order for
the lamp to glow, electrons must ow; in order for electrons to ow, there must be
a closed circuit from the power supply to the lamp and back to the power supply.","{'page_number': 53, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'ital Logic Structures\nI\nn Chapter 1, we stated that computers were built from very large numbers\nof very simple structures. For example, Intels Broadwell-E5 microproces-\nsor, introduced in 2016, contained more than seven billion transistors. Similarly,\nIBMs Power9 microprocessor, introduced in 2017, contained eight billion tran-\nsistors. In this chapter, we will explain how the MOS transistor works (as a logic\nelement), show how these transistors are connected to form logic gates, and then\nshow how logic gates are interconnected to form larger units that are needed to\nconstruct a computer. In Chapter 4, we will connect those larger units and form\na computer.\nBut rst, the transistor.\n3.1 The Transistor\nMost computers today or rather most microprocessors (which form the core of the\ncomputer) are constructed out of MOS transistors. MOS stands for metal-oxide\nsemiconductor. The electrical properties of metal-oxide semiconductors are well\nbeyond the scope of what we want to understand in this course. They are below\nour lowest level of abstraction, which means that if somehow transistors start\nmisbehaving, we are at their mercy. However, it is unlikely in this course that we\nwill have any problems from the transistors.\nStill, it is useful to know that there are two types of MOS transistors: P-type\nand N-type. They both operate logically, very similar to the way wall switches\nwork.\nFigure 3.1 shows the most basic of electrical circuits. It consists of (1) a\npower supply (in this case, the 120 volts that come into your house if you live in\nthe United States, or the 220 volts if you live in most of the rest of the world),\n(2) a wall switch, and (3) a lamp (plugged into an outlet in the wall). In order for\nthe lamp to glow, electrons must ow; in order for electrons to ow, there must be\na closed circuit from the power supply to the lamp and back to the power supply.'}"
"Figure 3.1
A simple electric circuit showing the use of a wall switch.
The lamp can be turned on and o by simply manipulating the wall switch to
make or break the closed circuit.
Instead of the wall switch, we could use an N-type or a P-type MOS transistor
to make or break the closed circuit. Figure 3.2 shows a schematic rendering of
an N-type transistor (a) by itself, and (b) in a circuit. Note (Figure 3.2a) that
the transistor has three terminals. They are called the gate, the source, and the
drain. The reasons for the names source and drain are not of interest to us in this
course. What is of interest is the fact that if the gate of the N-type transistor is
supplied with 1.2 volts, the connection from source to drain acts like a piece of
wire. We say (in the language of electricity) that we have a short circuit between
the source and drain. If the gate of the N-type transistor is supplied with 0 volts,
the connection between the source and drain is broken. We say that between the
source and drain we have an open circuit.
Figure 3.2 shows the N-type transistor in a circuit with a battery and a bulb.
When the gate is supplied with 1.2 volts, the transistor acts like a piece of wire,
completing the circuit and causing the bulb to glow. When the gate is supplied
Figure 3.2
The N-type MOS transistor.","{'page_number': 54, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure 3.1\nA simple electric circuit showing the use of a wall switch.\nThe lamp can be turned on and o by simply manipulating the wall switch to\nmake or break the closed circuit.\nInstead of the wall switch, we could use an N-type or a P-type MOS transistor\nto make or break the closed circuit. Figure 3.2 shows a schematic rendering of\nan N-type transistor (a) by itself, and (b) in a circuit. Note (Figure 3.2a) that\nthe transistor has three terminals. They are called the gate, the source, and the\ndrain. The reasons for the names source and drain are not of interest to us in this\ncourse. What is of interest is the fact that if the gate of the N-type transistor is\nsupplied with 1.2 volts, the connection from source to drain acts like a piece of\nwire. We say (in the language of electricity) that we have a short circuit between\nthe source and drain. If the gate of the N-type transistor is supplied with 0 volts,\nthe connection between the source and drain is broken. We say that between the\nsource and drain we have an open circuit.\nFigure 3.2 shows the N-type transistor in a circuit with a battery and a bulb.\nWhen the gate is supplied with 1.2 volts, the transistor acts like a piece of wire,\ncompleting the circuit and causing the bulb to glow. When the gate is supplied\nFigure 3.2\nThe N-type MOS transistor.'}"
"Figure 3.2c is a shorthand notation for describing the circuit of Figure 3.2b.
Rather than always showing the power supply and the complete circuit, electri-
cal engineers usually show only the terminals of the power supply. The fact that
the power supply itself provides the completion of the completed circuit is well
understood, and so is not usually shown.
The P-type transistor works in exactly the opposite fashion from the N-type
transistor. Figure 3.3 shows the schematic representation of a P-type transistor.
When the gate is supplied with 0 volts, the P-type transistor acts (more or less)
like a piece of wire, closing the circuit. When the gate is supplied with 1.2 volts,
the P-type transistor acts like an open circuit. Because the P-type and N-type
t
s
Drain
Figure 3.3
A P-type MOS transistor.
3.2 Logic Gates
One step up from the transistor is the logic gate. That is, we construct basic logic
structures out of individual MOS transistors. In Chapter 2, we studied the behav-
ior of the AND, the OR, and the NOT functions. In this chapter, we construct
transistor circuits that implement each of these functions. The corresponding
circuits are called AND, OR, and NOT gates.
3.2.1 The NOT Gate (Inverter)
Figure 3.4 shows the simplest logic structure that exists in a computer. It is con-
structed from two MOS transistors, one P-type and one N-type. Figure 3.4a is the
schematic representation of that circuit. Figure 3.4b shows the behavior of the
circuit if the input is supplied with 0 volts. Note that the P-type transistor acts
like a short circuit and the N-type transistor acts like an open circuit. The output
is, therefore, connected to 1.2 volts. On the other hand, if the input is supplied
with 1.2 volts, the P-type transistor acts like an open circuit, but the N-type tran-
sistor acts like a short circuit. The output in this case is connected to ground (i.e.,
0 volts). The complete behavior of the circuit can be described by means of a
table, as shown in Figure 3.4c. If we replace 0 volts with the symbol 0 and 1.2","{'page_number': 55, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure 3.2c is a shorthand notation for describing the circuit of Figure 3.2b.\nRather than always showing the power supply and the complete circuit, electri-\ncal engineers usually show only the terminals of the power supply. The fact that\nthe power supply itself provides the completion of the completed circuit is well\nunderstood, and so is not usually shown.\nThe P-type transistor works in exactly the opposite fashion from the N-type\ntransistor. Figure 3.3 shows the schematic representation of a P-type transistor.\nWhen the gate is supplied with 0 volts, the P-type transistor acts (more or less)\nlike a piece of wire, closing the circuit. When the gate is supplied with 1.2 volts,\nthe P-type transistor acts like an open circuit. Because the P-type and N-type\nt\ns\nDrain\nFigure 3.3\nA P-type MOS transistor.\n3.2 Logic Gates\nOne step up from the transistor is the logic gate. That is, we construct basic logic\nstructures out of individual MOS transistors. In Chapter 2, we studied the behav-\nior of the AND, the OR, and the NOT functions. In this chapter, we construct\ntransistor circuits that implement each of these functions. The corresponding\ncircuits are called AND, OR, and NOT gates.\n3.2.1 The NOT Gate (Inverter)\nFigure 3.4 shows the simplest logic structure that exists in a computer. It is con-\nstructed from two MOS transistors, one P-type and one N-type. Figure 3.4a is the\nschematic representation of that circuit. Figure 3.4b shows the behavior of the\ncircuit if the input is supplied with 0 volts. Note that the P-type transistor acts\nlike a short circuit and the N-type transistor acts like an open circuit. The output\nis, therefore, connected to 1.2 volts. On the other hand, if the input is supplied\nwith 1.2 volts, the P-type transistor acts like an open circuit, but the N-type tran-\nsistor acts like a short circuit. The output in this case is connected to ground (i.e.,\n0 volts). The complete behavior of the circuit can be described by means of a\ntable, as shown in Figure 3.4c. If we replace 0 volts with the symbol 0 and 1.2'}"
"1.2 volts
.  
0 volts
      
1      0
Figure 3.4
A CMOS inverter.
volts with the symbol 1, we have the truth table (Figure 3.4d) for the complement
or NOT function, which we studied in Chapter 2.
In other words, we have just shown how to construct an electronic circuit that
implements the NOT logic function discussed in Chapter 2. We call this circuit
a NOT gate, or an inverter.
3.2.2 OR and NOR Gates
Figure 3.5 illustrates a NOR gate. Figure 3.5a is a schematic of a circuit that
implements a NOR gate. It contains two P-type and two N-type transistors.
Figure 3.5b shows the behavior of the circuit if A is supplied with 0 volts
and B is supplied with 1.2 volts. In this case, the lower of the two P-type transis-
tors produces an open circuit, and the output C is disconnected from the 1.2-volt
power supply. However, the leftmost N-type transistor acts like a piece of wire,
connecting the output C to 0 volts.
Note that if both A and B are supplied with 0 volts, the two P-type transistors
conduct, and the output C is connected to 1.2 volts. Note further that there is no
ambiguity here, since both N-type transistors act as open circuits, and so C is
disconnected from ground.
If either A or B is supplied with 1.2 volts, the corresponding P-type transistor
results in an open circuit. That is sucient to break the connection from C to
the 1.2-volt source. However, 1.2 volts supplied to the gate of one of the N-type","{'page_number': 56, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '1.2 volts\n.  \n0 volts\n      \n1      0\nFigure 3.4\nA CMOS inverter.\nvolts with the symbol 1, we have the truth table (Figure 3.4d) for the complement\nor NOT function, which we studied in Chapter 2.\nIn other words, we have just shown how to construct an electronic circuit that\nimplements the NOT logic function discussed in Chapter 2. We call this circuit\na NOT gate, or an inverter.\n3.2.2 OR and NOR Gates\nFigure 3.5 illustrates a NOR gate. Figure 3.5a is a schematic of a circuit that\nimplements a NOR gate. It contains two P-type and two N-type transistors.\nFigure 3.5b shows the behavior of the circuit if A is supplied with 0 volts\nand B is supplied with 1.2 volts. In this case, the lower of the two P-type transis-\ntors produces an open circuit, and the output C is disconnected from the 1.2-volt\npower supply. However, the leftmost N-type transistor acts like a piece of wire,\nconnecting the output C to 0 volts.\nNote that if both A and B are supplied with 0 volts, the two P-type transistors\nconduct, and the output C is connected to 1.2 volts. Note further that there is no\nambiguity here, since both N-type transistors act as open circuits, and so C is\ndisconnected from ground.\nIf either A or B is supplied with 1.2 volts, the corresponding P-type transistor\nresults in an open circuit. That is sucient to break the connection from C to\nthe 1.2-volt source. However, 1.2 volts supplied to the gate of one of the N-type'}"
"1
1
0
1.2 volts
0 volts
1.2 volts
A
B
Figure 3.5
The NOR gate.
transistors is sucient to cause that transistor to conduct, resulting in C being
connected to ground (i.e., 0 volts).
Figure 3.5c summarizes the complete behavior of the circuit of Figure 3.5a.
It shows the behavior of the circuit for each of the four pairs of voltages that can
be supplied to A and B. That is,
A = 0 volts,
B = 0 volts
A = 0 volts,
B = 1.2 volts
A = 1.2 volts,
B = 0 volts
A = 1.2 volts,
B = 1.2 volts
If we replace the voltages with their logical equivalents, we have the truth
table of Figure 3.5d. Note that the output C is exactly the opposite of the logical
OR function that we studied in Chapter 2. In fact, it is the NOT-OR function,
more typically abbreviated as NOR. We refer to the circuit that implements the
NOR function as a NOR gate.
If we augment the circuit of Figure 3.5a by adding an inverter at its output, as
shown in Figure 3.6a, we have at the output D the logical function OR. Figure 3.6a
is the circuit for an OR gate. Figure 3.6b describes the behavior of this circuit if
the input variable A is set to 0 and the input variable B is set to 1. Figure 3.6c
shows the circuits truth table.","{'page_number': 57, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '1\n1\n0\n1.2 volts\n0 volts\n1.2 volts\nA\nB\nFigure 3.5\nThe NOR gate.\ntransistors is sucient to cause that transistor to conduct, resulting in C being\nconnected to ground (i.e., 0 volts).\nFigure 3.5c summarizes the complete behavior of the circuit of Figure 3.5a.\nIt shows the behavior of the circuit for each of the four pairs of voltages that can\nbe supplied to A and B. That is,\nA = 0 volts,\nB = 0 volts\nA = 0 volts,\nB = 1.2 volts\nA = 1.2 volts,\nB = 0 volts\nA = 1.2 volts,\nB = 1.2 volts\nIf we replace the voltages with their logical equivalents, we have the truth\ntable of Figure 3.5d. Note that the output C is exactly the opposite of the logical\nOR function that we studied in Chapter 2. In fact, it is the NOT-OR function,\nmore typically abbreviated as NOR. We refer to the circuit that implements the\nNOR function as a NOR gate.\nIf we augment the circuit of Figure 3.5a by adding an inverter at its output, as\nshown in Figure 3.6a, we have at the output D the logical function OR. Figure 3.6a\nis the circuit for an OR gate. Figure 3.6b describes the behavior of this circuit if\nthe input variable A is set to 0 and the input variable B is set to 1. Figure 3.6c\nshows the circuits truth table.'}"
"64
A
B
1    0      0     1
1    1      0     1
Figure 3.6
The OR gate.
3.2.3 Why We Cant Simply Connect P-Type to Ground
Some bright students have looked at our implementation of the OR gate (a NOR-
gate followed by an inverter) and asked the question, why cant we simply connect
the transistors as shown in Figure 3.7a?
Logically, it looks very tempting. Four transistors instead of six. Unfortu-
nately, the electrical properties of transistors make this problematic. When we
connect a P-type transistor to 1.2 volts or an N-type transistor to ground, there is
no voltage across the transistor, resulting in outputs as shown in Figure 3.5, for
example, of 0 volts or 1.2 volts, depending on the input voltages to A and B. How-
ever, when we connect a P-type transistor to ground or an N-type transistor to 1.2
volts, because of the electrical characteristics of the transistors, we get what is
usually referred to as a transmission voltage of approximately 0.5 volts across the
transistor. This results in the output of the transistor circuit of Figure 3.7 being
0.5 volts + 0.5 volts, or 1.0 volt if A and B are both 0, and 0.7 volts (1.2 volts
minus 0.5 volts) otherwise. Figure 3.7b shows the actual voltages in the resulting
truth table, rather than 0s and 1s. That is, even though the transistor circuit looks
like it would work, the transmission voltages across the transistors would yield
an output voltage of 1 volt for a logical 0 and 0.7 volts for a logical 1. Not what
we would like for an OR gate!","{'page_number': 58, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '64\nA\nB\n1    0      0     1\n1    1      0     1\nFigure 3.6\nThe OR gate.\n3.2.3 Why We Cant Simply Connect P-Type to Ground\nSome bright students have looked at our implementation of the OR gate (a NOR-\ngate followed by an inverter) and asked the question, why cant we simply connect\nthe transistors as shown in Figure 3.7a?\nLogically, it looks very tempting. Four transistors instead of six. Unfortu-\nnately, the electrical properties of transistors make this problematic. When we\nconnect a P-type transistor to 1.2 volts or an N-type transistor to ground, there is\nno voltage across the transistor, resulting in outputs as shown in Figure 3.5, for\nexample, of 0 volts or 1.2 volts, depending on the input voltages to A and B. How-\never, when we connect a P-type transistor to ground or an N-type transistor to 1.2\nvolts, because of the electrical characteristics of the transistors, we get what is\nusually referred to as a transmission voltage of approximately 0.5 volts across the\ntransistor. This results in the output of the transistor circuit of Figure 3.7 being\n0.5 volts + 0.5 volts, or 1.0 volt if A and B are both 0, and 0.7 volts (1.2 volts\nminus 0.5 volts) otherwise. Figure 3.7b shows the actual voltages in the resulting\ntruth table, rather than 0s and 1s. That is, even though the transistor circuit looks\nlike it would work, the transmission voltages across the transistors would yield\nan output voltage of 1 volt for a logical 0 and 0.7 volts for a logical 1. Not what\nwe would like for an OR gate!'}"
"Figure 3.7
An OR gate (not really!).
3.2.4 AND and NAND Gates
Figure 3.8 shows an AND gate. Note that if either A or B is supplied with 0 volts,
there is a direct connection from C to the 1.2-volt power supply. The fact that C
Figure 3.8
The AND gate.","{'page_number': 59, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure 3.7\nAn OR gate (not really!).\n3.2.4 AND and NAND Gates\nFigure 3.8 shows an AND gate. Note that if either A or B is supplied with 0 volts,\nthere is a direct connection from C to the 1.2-volt power supply. The fact that C\nFigure 3.8\nThe AND gate.'}"
"(e) NOR gate 
(d) NAND gate 
Figure 3.9
Basic logic gates.
Again, we note that there is no ambiguity. The fact that at least one of the two
inputs A or B is supplied with 0 volts means that at least one of the two N-type
transistors whose gates are connected to A or B is open, and that consequently, C
is disconnected from ground. Furthermore, the fact that C is at 1.2 volts means
the P-type transistor whose gate is connected to C is open-circuited. Therefore,
D is not connected to 1.2 volts.
On the other hand, if both A and B are supplied with 1.2 volts, then both
of their corresponding P-type transistors are open. However, their corresponding
N-type transistors act like pieces of wire, providing a direct connection from C
to ground. Because C is at ground, the rightmost P-type transistor acts like a
closed circuit, forcing D to 1.2 volts.
Figure 3.8b is a truth table that summarizes the behavior of the circuit of
Figure 3.8a. Note that the circuit is an AND gate. The circuit shown within the
dashed lines (i.e., having output C) is a NOT-AND gate, which we generally
abbreviate as NAND.
The gates just discussed are very common in digital logic circuits and in
digital computers. There are billions of inverters (NOT gates) in Intels Skylake
microprocessor. As a convenience, we can represent each of these gates by stan-
dard symbols, as shown in Figure 3.9. The bubble shown in the inverter, NAND,
and NOR gates signies the complement (i.e., NOT) function. From now on, we
will not draw circuits showing the individual transistors. Instead, we will raise
our level of abstraction and use the symbols shown in Figure 3.9.
3.2.5 Gates with More Than Two Inputs
Before we leave the topic of logic gates, we should note that the notion of AND,
OR, NAND, and NOR gates extends to larger numbers of inputs. One could build
a three-input AND gate or a four-input OR gate, for example. An n-input AND
gate has an output value of 1 only if ALL n input variables have values of 1. If
any of the n inputs has a value of 0, the output of the n-input AND gate is 0. An
n-input OR gate has an output value of 1 if ANY of the n input variables has a
value of 1. That is, an n-input OR gate has an output value of 0 only if ALL n
input variables have values of 0.","{'page_number': 60, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '(e) NOR gate \n(d) NAND gate \nFigure 3.9\nBasic logic gates.\nAgain, we note that there is no ambiguity. The fact that at least one of the two\ninputs A or B is supplied with 0 volts means that at least one of the two N-type\ntransistors whose gates are connected to A or B is open, and that consequently, C\nis disconnected from ground. Furthermore, the fact that C is at 1.2 volts means\nthe P-type transistor whose gate is connected to C is open-circuited. Therefore,\nD is not connected to 1.2 volts.\nOn the other hand, if both A and B are supplied with 1.2 volts, then both\nof their corresponding P-type transistors are open. However, their corresponding\nN-type transistors act like pieces of wire, providing a direct connection from C\nto ground. Because C is at ground, the rightmost P-type transistor acts like a\nclosed circuit, forcing D to 1.2 volts.\nFigure 3.8b is a truth table that summarizes the behavior of the circuit of\nFigure 3.8a. Note that the circuit is an AND gate. The circuit shown within the\ndashed lines (i.e., having output C) is a NOT-AND gate, which we generally\nabbreviate as NAND.\nThe gates just discussed are very common in digital logic circuits and in\ndigital computers. There are billions of inverters (NOT gates) in Intels Skylake\nmicroprocessor. As a convenience, we can represent each of these gates by stan-\ndard symbols, as shown in Figure 3.9. The bubble shown in the inverter, NAND,\nand NOR gates signies the complement (i.e., NOT) function. From now on, we\nwill not draw circuits showing the individual transistors. Instead, we will raise\nour level of abstraction and use the symbols shown in Figure 3.9.\n3.2.5 Gates with More Than Two Inputs\nBefore we leave the topic of logic gates, we should note that the notion of AND,\nOR, NAND, and NOR gates extends to larger numbers of inputs. One could build\na three-input AND gate or a four-input OR gate, for example. An n-input AND\ngate has an output value of 1 only if ALL n input variables have values of 1. If\nany of the n inputs has a value of 0, the output of the n-input AND gate is 0. An\nn-input OR gate has an output value of 1 if ANY of the n input variables has a\nvalue of 1. That is, an n-input OR gate has an output value of 0 only if ALL n\ninput variables have values of 0.'}"
"1      1      0       0
1      1      1       1
Figure 3.10
A three-input AND gate.
Figure 3.10 illustrates a three-input AND gate. Figure 3.10a shows its truth
table. Figure 3.10b shows the symbol for a three-input AND gate.
Question: Can you draw a transistor-level circuit for a three-input AND gate?
How about a four-input AND gate? How about a four-input OR gate?
3.3 Combinational Logic Circuits
Now that we understand the workings of the basic logic gates, the next step
is to build some of the logic structures that are important components of the
microarchitecture of a computer.
There are fundamentally two kinds of logic structures, those that include the
storage of information and those that do not. In Sections 3.4, 3.5, and 3.6, we
will deal with structures that store information. In this section, we will deal with
structures that do not store information. These structures are sometimes referred
to as decision elements. Usually, they are referred to as combinational logic struc-
tures because their outputs are strictly dependent on the combination of input
values that are being applied to the structure right now. Their outputs are not at
all dependent on any past history of information that is stored internally, since no
information can be stored internally in a combinational logic circuit.
We will next examine three useful combinational logic circuits: a decoder, a
mux, and a one-bit adder.
3.3.1 Decoder
Figure 3.11 shows a logic gate implementation of a two-input decoder. A decoder
has the property that exactly one of its outputs is 1 and all the rest are 0s. The one
output that is logically 1 is the output corresponding to the input pattern that it is
expected to detect. In general, decoders have n inputs and 2n outputs. We say the
output line that detects the input pattern is asserted. That is, that output line has
the value 1, rather than 0 as is the case for all the other output lines. In Figure 3.11,
note that for each of the four possible combinations of inputs A and B, exactly one
output has the value 1 at any one time. In Figure 3.11b, the input to the decoder
is 10, resulting in the third output line being asserted.","{'page_number': 61, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '1      1      0       0\n1      1      1       1\nFigure 3.10\nA three-input AND gate.\nFigure 3.10 illustrates a three-input AND gate. Figure 3.10a shows its truth\ntable. Figure 3.10b shows the symbol for a three-input AND gate.\nQuestion: Can you draw a transistor-level circuit for a three-input AND gate?\nHow about a four-input AND gate? How about a four-input OR gate?\n3.3 Combinational Logic Circuits\nNow that we understand the workings of the basic logic gates, the next step\nis to build some of the logic structures that are important components of the\nmicroarchitecture of a computer.\nThere are fundamentally two kinds of logic structures, those that include the\nstorage of information and those that do not. In Sections 3.4, 3.5, and 3.6, we\nwill deal with structures that store information. In this section, we will deal with\nstructures that do not store information. These structures are sometimes referred\nto as decision elements. Usually, they are referred to as combinational logic struc-\ntures because their outputs are strictly dependent on the combination of input\nvalues that are being applied to the structure right now. Their outputs are not at\nall dependent on any past history of information that is stored internally, since no\ninformation can be stored internally in a combinational logic circuit.\nWe will next examine three useful combinational logic circuits: a decoder, a\nmux, and a one-bit adder.\n3.3.1 Decoder\nFigure 3.11 shows a logic gate implementation of a two-input decoder. A decoder\nhas the property that exactly one of its outputs is 1 and all the rest are 0s. The one\noutput that is logically 1 is the output corresponding to the input pattern that it is\nexpected to detect. In general, decoders have n inputs and 2n outputs. We say the\noutput line that detects the input pattern is asserted. That is, that output line has\nthe value 1, rather than 0 as is the case for all the other output lines. In Figure 3.11,\nnote that for each of the four possible combinations of inputs A and B, exactly one\noutput has the value 1 at any one time. In Figure 3.11b, the input to the decoder\nis 10, resulting in the third output line being asserted.'}"
"A
B
1,  if   A, B  is  11
0
Figure 3.11
A two-input decoder.
The decoder is useful in determining how to interpret a bit pattern. We will
see in Chapter 5 that the work to be carried out by each instruction in the LC-
3 computer is determined by a four-bit pattern that is the part of the instruction
called the opcode, A 4-to-16 decoder is a simple combinational logic structure
for identifying what work is to be performed by each instruction.
3.3.2 Mux
Figure 3.12a shows a logic gate implementation of a two-input multiplexer, more
Figure 3.12
A 2-to-1 mux.","{'page_number': 62, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'A\nB\n1,  if   A, B  is  11\n0\nFigure 3.11\nA two-input decoder.\nThe decoder is useful in determining how to interpret a bit pattern. We will\nsee in Chapter 5 that the work to be carried out by each instruction in the LC-\n3 computer is determined by a four-bit pattern that is the part of the instruction\ncalled the opcode, A 4-to-16 decoder is a simple combinational logic structure\nfor identifying what work is to be performed by each instruction.\n3.3.2 Mux\nFigure 3.12a shows a logic gate implementation of a two-input multiplexer, more\nFigure 3.12\nA 2-to-1 mux.'}"
"OUT
Figure 3.13
A four-input mux.
The mux of Figure 3.12 works as follows: Suppose S = 0, as shown in
Figure 3.12b. Since the output of an AND gate is 0 unless all inputs are 1, the out-
put of the rightmost AND gate is 0. Also, the output of the leftmost AND gate is
whatever the input A is. That is, if A = 0, then the output of the leftmost AND gate
is 0, and if A = 1, then the output of the leftmost AND gate is 1. Since the output
of the rightmost AND gate is 0, it has no eect on the OR gate. Consequently,
the output at C is exactly the same as the output of the leftmost AND gate. The
net result of all this is that if S = 0, the output C is identical to the input A.
On the other hand, if S = 1, it is B that is ANDed with 1, resulting in the
output of the OR gate having the value of B.
In summary, the output C is always connected to either the input A or the
input Bwhich one depends on the value of the select line S. We say S selects the
source of the mux (either A or B) to be routed through to the output C. Figure 3.12c
shows the standard representation for a mux.
In general, a mux consists of 2n inputs and n select lines. Figure 3.13a
shows a gate-level description of a four-input mux. It requires two select lines.
Figure 3.13b shows the standard representation for a four-input mux.
Question: Can you construct the gate-level representation for an eight-input
mux? How many select lines must you have?
3.3.3 A One-Bit Adder (a.k.a. a Full Adder)
Recall in Chapter 2, we discussed binary addition. A simple algorithm for binary
addition is to proceed as you have always done in the case of decimal addition,
from right to left, one column at a time, adding the two digits from the two values
plus the carry in, and generating a sum digit and a carry to the next column. The
only dierence here (with binary addition) is you get a carry after 1, rather than
after 9.
Figure 3.14 is a truth table that describes the result of binary addition on one
column of bits within two n-bit operands. At each column, there are three values","{'page_number': 63, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'OUT\nFigure 3.13\nA four-input mux.\nThe mux of Figure 3.12 works as follows: Suppose S = 0, as shown in\nFigure 3.12b. Since the output of an AND gate is 0 unless all inputs are 1, the out-\nput of the rightmost AND gate is 0. Also, the output of the leftmost AND gate is\nwhatever the input A is. That is, if A = 0, then the output of the leftmost AND gate\nis 0, and if A = 1, then the output of the leftmost AND gate is 1. Since the output\nof the rightmost AND gate is 0, it has no eect on the OR gate. Consequently,\nthe output at C is exactly the same as the output of the leftmost AND gate. The\nnet result of all this is that if S = 0, the output C is identical to the input A.\nOn the other hand, if S = 1, it is B that is ANDed with 1, resulting in the\noutput of the OR gate having the value of B.\nIn summary, the output C is always connected to either the input A or the\ninput Bwhich one depends on the value of the select line S. We say S selects the\nsource of the mux (either A or B) to be routed through to the output C. Figure 3.12c\nshows the standard representation for a mux.\nIn general, a mux consists of 2n inputs and n select lines. Figure 3.13a\nshows a gate-level description of a four-input mux. It requires two select lines.\nFigure 3.13b shows the standard representation for a four-input mux.\nQuestion: Can you construct the gate-level representation for an eight-input\nmux? How many select lines must you have?\n3.3.3 A One-Bit Adder (a.k.a. a Full Adder)\nRecall in Chapter 2, we discussed binary addition. A simple algorithm for binary\naddition is to proceed as you have always done in the case of decimal addition,\nfrom right to left, one column at a time, adding the two digits from the two values\nplus the carry in, and generating a sum digit and a carry to the next column. The\nonly dierence here (with binary addition) is you get a carry after 1, rather than\nafter 9.\nFigure 3.14 is a truth table that describes the result of binary addition on one\ncolumn of bits within two n-bit operands. At each column, there are three values'}"
"1
1
1
1
1
Figure 3.14
The truth table for a one-bit adder.
that must be added: one bit from each of the two operands A and B and the carry
from the previous column. We designate these three bits as Ai, Bi, and Ci. There
are two results, the sum bit (Si) and the carry over to the next column, Ci+1. Note
that if only one of the three bits equals 1, we get a sum of 1, and no carry (i.e.,
Ci+1 = 0). If two of the three bits equal 1, we get a sum of 0, and a carry of 1. If
all three bits equal 1, the sum is 3, which in binary corresponds to a sum of 1 and
a carry of 1.
Figure 3.15 shows a logic gate implementation of a one-bit adder. Note that
each AND gate in Figure 3.15 produces an output 1 for exactly one of the eight
input combinations of Ai, Bi, and Ci. The output of the OR gate for Ci+1 must be 1
in e
pro
Figure 3.15
Gate-level description of a one-bit adder.","{'page_number': 64, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '1\n1\n1\n1\n1\nFigure 3.14\nThe truth table for a one-bit adder.\nthat must be added: one bit from each of the two operands A and B and the carry\nfrom the previous column. We designate these three bits as Ai, Bi, and Ci. There\nare two results, the sum bit (Si) and the carry over to the next column, Ci+1. Note\nthat if only one of the three bits equals 1, we get a sum of 1, and no carry (i.e.,\nCi+1 = 0). If two of the three bits equal 1, we get a sum of 0, and a carry of 1. If\nall three bits equal 1, the sum is 3, which in binary corresponds to a sum of 1 and\na carry of 1.\nFigure 3.15 shows a logic gate implementation of a one-bit adder. Note that\neach AND gate in Figure 3.15 produces an output 1 for exactly one of the eight\ninput combinations of Ai, Bi, and Ci. The output of the OR gate for Ci+1 must be 1\nin e\npro\nFigure 3.15\nGate-level description of a one-bit adder.'}"
"S0
S1
S2
S3
Figure 3.16
A circuit for adding two 4-bit binary numbers.
are the outputs of the AND gates corresponding to those input combinations.
Similarly, the inputs to the OR gate that generates Si are the outputs of the AND
gates corresponding to the input combinations that require an output 1 for Si in
the truth table of Figure 3.14.
Note that since the input combination 000 does not result in an output 1 for
either Ci+1 or Si, its corresponding AND gate is not an input to either of the two
OR gates.
Figure 3.16 shows a circuit for adding two 4-bit binary numbers, using four
of the one-bit adder circuits of Figure 3.15. Note that the carry out of column i is
an input to the addition performed in column i + 1.
If we wish to implement a logic circuit that adds two 16-bit numbers, we can
do so with a circuit of 16 one-bit adders.
We should point out that historically the logic circuit of Figure 3.15 that
provides three inputs (Ai, Bi, and Ci) and two outputs (the sum bit Si and the
carry over to the next column Ci+1) has generally been referred to as a full adder
to dierentiate it from another structure, which is called a half adder. The dis-
tinction between the two is the carry bit. Note that the carry into the rightmost
column in Figure 3.16 is 0. That is, in the rightmost circuit, S0 and C1 depend only
on two inputs, A0 and B0. Since that circuit depends on only two inputs, it has been
referred to as a half adder. Since the other circuits depend on all three inputs, they
are referred to as full adders. We prefer the term one-bit adder as a simpler term
for describing what is happening in each column.
3.3.4 The Programmable Logic Array (PLA)
Figure 3.17 illustrates a very common building block for implementing any col-
lection of logic functions one wishes to implement. The building block is called
a programmable logic array (PLA). It consists of an array of AND gates (called
an AND array) followed by an array of OR gates (called an OR array). The num-
ber of AND gates corresponds to the number of input combinations (rows) in
the truth table. For n-input logic functions, we need a PLA with 2n n-input AND","{'page_number': 65, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'S0\nS1\nS2\nS3\nFigure 3.16\nA circuit for adding two 4-bit binary numbers.\nare the outputs of the AND gates corresponding to those input combinations.\nSimilarly, the inputs to the OR gate that generates Si are the outputs of the AND\ngates corresponding to the input combinations that require an output 1 for Si in\nthe truth table of Figure 3.14.\nNote that since the input combination 000 does not result in an output 1 for\neither Ci+1 or Si, its corresponding AND gate is not an input to either of the two\nOR gates.\nFigure 3.16 shows a circuit for adding two 4-bit binary numbers, using four\nof the one-bit adder circuits of Figure 3.15. Note that the carry out of column i is\nan input to the addition performed in column i + 1.\nIf we wish to implement a logic circuit that adds two 16-bit numbers, we can\ndo so with a circuit of 16 one-bit adders.\nWe should point out that historically the logic circuit of Figure 3.15 that\nprovides three inputs (Ai, Bi, and Ci) and two outputs (the sum bit Si and the\ncarry over to the next column Ci+1) has generally been referred to as a full adder\nto dierentiate it from another structure, which is called a half adder. The dis-\ntinction between the two is the carry bit. Note that the carry into the rightmost\ncolumn in Figure 3.16 is 0. That is, in the rightmost circuit, S0 and C1 depend only\non two inputs, A0 and B0. Since that circuit depends on only two inputs, it has been\nreferred to as a half adder. Since the other circuits depend on all three inputs, they\nare referred to as full adders. We prefer the term one-bit adder as a simpler term\nfor describing what is happening in each column.\n3.3.4 The Programmable Logic Array (PLA)\nFigure 3.17 illustrates a very common building block for implementing any col-\nlection of logic functions one wishes to implement. The building block is called\na programmable logic array (PLA). It consists of an array of AND gates (called\nan AND array) followed by an array of OR gates (called an OR array). The num-\nber of AND gates corresponds to the number of input combinations (rows) in\nthe truth table. For n-input logic functions, we need a PLA with 2n n-input AND'}"
"72
Figure 3.17
A programmable logic array.
gates. In Figure 3.17, we have 23 three-input AND gates, corresponding to three
logical input variables. The number of OR gates corresponds to the number of
logic functions we wish to implement, that is, the number of output columns in
the truth table. The implementation algorithm is simply to connect the output of
an AND gate to the input of an OR gate if the corresponding row of the truth table
produces an output 1 for that output column. Hence the notion of programmable.
That is, we say we program the connections from AND gate outputs to OR gate
inputs to implement our desired logic functions.
Figure 3.15 shows seven AND gates connected to two OR gates since our
requirement was to implement two functions (sum and carry) of three input vari-
ables. Figure 3.17 shows a PLA that can implement any four functions of three
variables by appropriately connecting AND gate outputs to OR gate inputs. That
is, any function of three variables can be implemented by connecting the outputs
of all AND gates corresponding to input combinations for which the output is 1
to inputs of one of the OR gates. Thus, we could implement the one-bit adder by
programming the two OR gates in Figure 3.17 whose outputs are W and X by
connecting or not connecting the outputs of the AND gates to the inputs of those
two OR gates as specied by the two output columns of Figure 3.14.
3.3.5 Logical Completeness
Before we leave the topic of combinational logic circuits, it is worth noting an
important property of building blocks for logic circuits: logical completeness. We
showed in Section 3.3.4 that any logic function we wished to implement could
be accomplished with a PLA. We saw that the PLA consists of only AND gates,
OR gates, and inverters. That means that any logic function can be implemented,
provided that enough AND, OR, and NOT gates are available. We say that the set
of gates {AND, OR, NOT} is logically complete because we can build a circuit
to carry out the specication of any truth table we wish without using any other","{'page_number': 66, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '72\nFigure 3.17\nA programmable logic array.\ngates. In Figure 3.17, we have 23 three-input AND gates, corresponding to three\nlogical input variables. The number of OR gates corresponds to the number of\nlogic functions we wish to implement, that is, the number of output columns in\nthe truth table. The implementation algorithm is simply to connect the output of\nan AND gate to the input of an OR gate if the corresponding row of the truth table\nproduces an output 1 for that output column. Hence the notion of programmable.\nThat is, we say we program the connections from AND gate outputs to OR gate\ninputs to implement our desired logic functions.\nFigure 3.15 shows seven AND gates connected to two OR gates since our\nrequirement was to implement two functions (sum and carry) of three input vari-\nables. Figure 3.17 shows a PLA that can implement any four functions of three\nvariables by appropriately connecting AND gate outputs to OR gate inputs. That\nis, any function of three variables can be implemented by connecting the outputs\nof all AND gates corresponding to input combinations for which the output is 1\nto inputs of one of the OR gates. Thus, we could implement the one-bit adder by\nprogramming the two OR gates in Figure 3.17 whose outputs are W and X by\nconnecting or not connecting the outputs of the AND gates to the inputs of those\ntwo OR gates as specied by the two output columns of Figure 3.14.\n3.3.5 Logical Completeness\nBefore we leave the topic of combinational logic circuits, it is worth noting an\nimportant property of building blocks for logic circuits: logical completeness. We\nshowed in Section 3.3.4 that any logic function we wished to implement could\nbe accomplished with a PLA. We saw that the PLA consists of only AND gates,\nOR gates, and inverters. That means that any logic function can be implemented,\nprovided that enough AND, OR, and NOT gates are available. We say that the set\nof gates {AND, OR, NOT} is logically complete because we can build a circuit\nto carry out the specication of any truth table we wish without using any other'}"
"kind of gate. That is, the set of gates {AND, OR, and NOT} is logically complete
because a barrel of AND gates, a barrel of OR gates, and a barrel of NOT gates are
sucient to build a logic circuit that carries out the specication of any desired
truth table. The barrels may have to be big, but the point is, we do not need any
other kind of gate to do the job.
Question: Is there any single two-input logic gate that is logically complete?
For example, is the NAND gate logically complete? Hint: Can I implement a
NOT gate with a NAND gate? If yes, can I then implement an AND gate using a
NAND gate followed by a NOT gate? If yes, can I implement an OR gate using
just AND gates and NOT gates?
If all of the above is true, then the NAND gate is logically complete, and I
can implement any desired logic function as described by its truth table with a
barrel of NAND gates.
3.4 Basic Storage Elements
Recall our statement at the beginning of Section 3.3 that there are two kinds of
logic structures, those that involve the storage of information and those that do
not. We have discussed three examples of those that do not: the decoder, the mux,
and the full adder. Now we are ready to discuss logic structures that do include
the storage of information.
3.4.1 The R-S Latch
A simple example of a storage element is the R-S latch. It can store one bit of
information, a 0 or a 1. The R-S latch can be implemented in many ways, the
simplest being the one shown in Figure 3.18. Two 2-input NAND gates are con-
nected such that the output of each is connected to one of the inputs of the other.
The remaining inputs S and R are normally held at a logic level 1.
The R-S latch
ets its name from the old desi nations for settin
the latch
R
b
Figure 3.18
An R-S latch.
The Quiescent State
We describe the quiescent (or quiet) state of a latch as
the state when the latch is storing a value, either 0 or 1, and nothing is trying to","{'page_number': 67, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'kind of gate. That is, the set of gates {AND, OR, and NOT} is logically complete\nbecause a barrel of AND gates, a barrel of OR gates, and a barrel of NOT gates are\nsucient to build a logic circuit that carries out the specication of any desired\ntruth table. The barrels may have to be big, but the point is, we do not need any\nother kind of gate to do the job.\nQuestion: Is there any single two-input logic gate that is logically complete?\nFor example, is the NAND gate logically complete? Hint: Can I implement a\nNOT gate with a NAND gate? If yes, can I then implement an AND gate using a\nNAND gate followed by a NOT gate? If yes, can I implement an OR gate using\njust AND gates and NOT gates?\nIf all of the above is true, then the NAND gate is logically complete, and I\ncan implement any desired logic function as described by its truth table with a\nbarrel of NAND gates.\n3.4 Basic Storage Elements\nRecall our statement at the beginning of Section 3.3 that there are two kinds of\nlogic structures, those that involve the storage of information and those that do\nnot. We have discussed three examples of those that do not: the decoder, the mux,\nand the full adder. Now we are ready to discuss logic structures that do include\nthe storage of information.\n3.4.1 The R-S Latch\nA simple example of a storage element is the R-S latch. It can store one bit of\ninformation, a 0 or a 1. The R-S latch can be implemented in many ways, the\nsimplest being the one shown in Figure 3.18. Two 2-input NAND gates are con-\nnected such that the output of each is connected to one of the inputs of the other.\nThe remaining inputs S and R are normally held at a logic level 1.\nThe R-S latch\nets its name from the old desi nations for settin\nthe latch\nR\nb\nFigure 3.18\nAn R-S latch.\nThe Quiescent State\nWe describe the quiescent (or quiet) state of a latch as\nthe state when the latch is storing a value, either 0 or 1, and nothing is trying to'}"
"change that value. This is the case when inputs S and R both have the logic value
1. In Figure 3.18 the letter a designates the value that is currently stored in the
latch, which we also refer to as the output of the latch.
Consider rst the case where the value stored and therefore the output a is
1. Since that means the value A is 1 (and since we know the input R is 1 because
we are in the quiescent state), the NAND gates output b must be 0. That, in turn,
means B must be 0, which results in the output a equal to 1. As long as the inputs
S and R remain 1, the state of the circuit will not change. That is, the R-S latch
will continue to store the value 1 (the value of the output a).
If, on the other hand, we assume the output a is 0, then A must be 0, and
the out ut b must be 1. That, in turn, results in B equal to 1, and combined with
t S equal to 1 (again due to quiescence), results in the output a equal to
n, as long as the inputs S and R remain 1, the state of the circuit will not
. In this case, we say the R-S latch stores the value 0.
the Latch to a 1 or a 0
The latch can be set to 1 by momentarily setting
provided we keep the value of R at 1. Similarly, the latch can be set to 0
entarily setting R to 0, provided we keep the value of S at 1. In order for
the R-S latch to work properly, both S and R must never be allowed to be set to 0
at the same time.
We use the term set to denote setting a variable to 0 or 1, as in set to 0 or
set to 1. In addition, we often use the term clear to denote the act of setting a
variable to 0.
If we set S to 0 for a very brief period of time, this causes a to equal 1, which
in turn causes A to equal 1. Since R is also 1, the output at b must be 0. This causes
B to be 0, which in turn makes a equal to 1. If, after that very brief period of time,
we now return S to 1, it does not aect a. Why? Answer: Since B is also 0, and
since only one input 0 to a NAND gate is enough to guarantee that the output of
the NAND gate is 1, the latch will continue to store a 1 long after S returns to 1.
In the same way, we can clear the latch (set the latch to 0) by setting R to 0
for a very short period of time.
We should point out that if both S and R were allowed to be set to 0 at the same
time, the outputs a and b would both be 1, and the nal state of the latch would
depend on the electrical properties of the transistors making up the gates and
not on the logic being performed. How the electrical properties of the transistors
would determine the nal state in this case is a subject we will have to leave for
a later semester","{'page_number': 68, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'We should point out that if both S and R were allowed to be set to 0 at the same\ntime, the outputs a and b would both be 1, and the nal state of the latch would\ndepend on the electrical properties of the transistors making up the gates and\nnot on the logic being performed. How the electrical properties of the transistors\nwould determine the nal state in this case is a subject we will have to leave for\na later semester. :-(\nFinally, we should note that when a digital circuit is powered on, the latch\ncan be in either of its two states, 0 or 1. It does not matter which state since we\nnever use that information until after we have set it to 1 or 0.\n3.4.2 The Gated D Latch\nTo be useful, it is necessary to control when a latch is set and when it is cleared.\nA simple way to accomplish this is with the gated latch.\nFigure 3.19 shows a logic circuit that implements a gated D latch. It consists\nof the R-S latch of Figure 3.18, plus two additional NAND gates that allow the'}"
"We should point out that if both S and R were allowed to be set to 0 at the same
time, the outputs a and b would both be 1, and the nal state of the latch would
depend on the electrical properties of the transistors making up the gates and
not on the logic being performed. How the electrical properties of the transistors
would determine the nal state in this case is a subject we will have to leave for
a later semester. :-(
Finally, we should note that when a digital circuit is powered on, the latch
can be in either of its two states, 0 or 1. It does not matter which state since we
never use that information until after we have set it to 1 or 0.
3.4.2 The Gated D Latch
To be useful, it is necessary to control when a latch is set and when it is cleared.
A simple way to accomplish this is with the gated latch.
Figure 3.19 shows a logic circuit that implements a gated D latch. It consists
of the R-S latch of Figure 3.18, plus two additional NAND gates that allow the","{'page_number': 68, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'We should point out that if both S and R were allowed to be set to 0 at the same\ntime, the outputs a and b would both be 1, and the nal state of the latch would\ndepend on the electrical properties of the transistors making up the gates and\nnot on the logic being performed. How the electrical properties of the transistors\nwould determine the nal state in this case is a subject we will have to leave for\na later semester. :-(\nFinally, we should note that when a digital circuit is powered on, the latch\ncan be in either of its two states, 0 or 1. It does not matter which state since we\nnever use that information until after we have set it to 1 or 0.\n3.4.2 The Gated D Latch\nTo be useful, it is necessary to control when a latch is set and when it is cleared.\nA simple way to accomplish this is with the gated latch.\nFigure 3.19 shows a logic circuit that implements a gated D latch. It consists\nof the R-S latch of Figure 3.18, plus two additional NAND gates that allow the'}"
"R
Figure 3.19
A gated D latch.
latch to be set to the value of D, but only when WE is asserted (i.e., when WE
equals 1). WE stands for write enable. When WE is not asserted (i.e., when WE
equals 0), the outputs S and R are both equal to 1. Since S and R are inputs to the
R-S latch, if they are kept at 1, the value stored in the latch remains unchanged,
as we explained in Section 3.4.1. When WE is momentarily set to 1, exactly one
of the outputs S or R is set to 0, depending on the value of D. If D equals 1, then S
is set to 0. If D equals 0, then both inputs to the lower NAND gate are 1, resulting
in R being set to 0. As we saw earlier, if S is set to 0, the R-S latch is set to 1. If R
is set to 0, the R-S latch is set to 0. Thus, the R-S latch is set to 1 or 0 according
to whether D is 1 or 0. When WE returns to 0, S and R return to 1, and the value
stored in the R-S latch persists.
3.5 The Concept of Memory
We now have all the tools we need to describe one of the most important struc-
tures in the electronic digital computer, its memory. We will see in Chapter 4
how memory ts into the basic scheme of computer processing, and you will see
throughout the rest of the book and indeed the rest of your work with computers
how important the concept of memory is to computing.
Memory is made up of a (usually large) number of locations, each uniquely
identiable and each having the ability to store a value. We refer to the unique
identier associated with each memory location as its address. We refer to the
number of bits of information stored in each location as its addressability.
For example, an advertisement for a laptop computer might say, This com-
puter comes with 2 gigabytes of memory. Actually, most ads generally use
the abbreviation 2 GB (or, often: 2 Gig). This statement means, as we will
explain momentarily, that the laptop includes two billion memory locations, each
containing one byte of information.
3.5.1 Address Space
We refer to the total number of uniquely identiable locations as the memorys
address space. A 2 GB memory, for example, refers to a memory that consists of
two billion uniquely identiable memory locations.","{'page_number': 69, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'R\nFigure 3.19\nA gated D latch.\nlatch to be set to the value of D, but only when WE is asserted (i.e., when WE\nequals 1). WE stands for write enable. When WE is not asserted (i.e., when WE\nequals 0), the outputs S and R are both equal to 1. Since S and R are inputs to the\nR-S latch, if they are kept at 1, the value stored in the latch remains unchanged,\nas we explained in Section 3.4.1. When WE is momentarily set to 1, exactly one\nof the outputs S or R is set to 0, depending on the value of D. If D equals 1, then S\nis set to 0. If D equals 0, then both inputs to the lower NAND gate are 1, resulting\nin R being set to 0. As we saw earlier, if S is set to 0, the R-S latch is set to 1. If R\nis set to 0, the R-S latch is set to 0. Thus, the R-S latch is set to 1 or 0 according\nto whether D is 1 or 0. When WE returns to 0, S and R return to 1, and the value\nstored in the R-S latch persists.\n3.5 The Concept of Memory\nWe now have all the tools we need to describe one of the most important struc-\ntures in the electronic digital computer, its memory. We will see in Chapter 4\nhow memory ts into the basic scheme of computer processing, and you will see\nthroughout the rest of the book and indeed the rest of your work with computers\nhow important the concept of memory is to computing.\nMemory is made up of a (usually large) number of locations, each uniquely\nidentiable and each having the ability to store a value. We refer to the unique\nidentier associated with each memory location as its address. We refer to the\nnumber of bits of information stored in each location as its addressability.\nFor example, an advertisement for a laptop computer might say, This com-\nputer comes with 2 gigabytes of memory. Actually, most ads generally use\nthe abbreviation 2 GB (or, often: 2 Gig). This statement means, as we will\nexplain momentarily, that the laptop includes two billion memory locations, each\ncontaining one byte of information.\n3.5.1 Address Space\nWe refer to the total number of uniquely identiable locations as the memorys\naddress space. A 2 GB memory, for example, refers to a memory that consists of\ntwo billion uniquely identiable memory locations.'}"
"Actually, the number two billion is only an approximation, due to the way we
specify memory locations. Since everything else in the computer is represented
by sequences of 0s and 1s, it should not be surprising that memory locations are
identied by binary addresses as well. With n bits of address, we can uniquely
identify 2n locations. Ten bits provide 1024 locations, which is approximately
1000. If we have 20 bits to represent each address, we have 220 uniquely identi-
able locations, which is approximately one million. With 30 bits, we have 230
locations, which is approximately one billion. In the same way we use the prexes
kilo to represent 210 (approximately 1000) and mega to represent 220 (approx-
imately one million), we use the prex giga to represent 230 (approximately
one billion). Thus, 2 giga really corresponds to the number of uniquely iden-
tiable locations that can be specied with 31 address bits. We say the address
space is 231, which is exactly 2,147,483,648 locations, rather than 2,000,000,000,
although we colloquially refer to it as two billion.
3.5.2 Addressability
The number of bits stored in each memory location is the memorys addressabil-
ity. A 2-gigabyte memory (written 2GB) is a memory consisting of 2,147,483,648
memory locations, each containing one byte (i.e., eight bits) of storage. Most
memories are byte-addressable. The reason is historical; most computers got their
start processing data, and one character stroke on the keyboard corresponds to one
8-bit ASCII code, as we learned in Chapter 2. If the memory is byte-addressable,
then each ASCII character occupies one location in memory. Uniquely identi-
fying each byte of memory allows individual bytes of stored information to be
changed easily.
Many computers that have been designed specically to perform large scien-
tic calculations are 64-bit addressable. This is due to the fact that numbers used
in scientic calculations are often represented as 64-bit oating-point quantities.
Recall that we discussed the oating-point data type in Chapter 2. Since scientic
calculations are likely to use numbers that require 64 bits to represent them, it is
reasonable to design a memory for such a computer that stores one such number
in each uniquely identiable memory location.
3.5.3 A 22-by-3-Bit Memory
Figure 3.20 illustrates a memory of size 22 by 3 bits. That is, the memory
has an address space of four locations and an addressability of three bits.
A memory of size 22 requires two bits to specify the address. We describe
the two-bit address as A[1:0]. A memory of addressability three stores three
bits of information in each memory location. We describe the three bits
of data as D[2:0]. In both cases, our notation A[high:low] and D[high:low]
reects the fact that we have numbered the bits of address and data from
right to left, in order, starting with the rightmost bit, which is numbered 0","{'page_number': 70, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'A memory of addressability three stores three\nbits of information in each memory location. We describe the three bits\nof data as D[2:0]. In both cases, our notation A[high:low] and D[high:low]\nreects the fact that we have numbered the bits of address and data from\nright to left, in order, starting with the rightmost bit, which is numbered 0.\nThe notation [high:low] means a sequence of high  low + 1 bits such that\nhigh is the bit number of the leftmost (or high) bit number in the sequence\nand low is the bit number of the rightmost (or low) bit number in the sequence.'}"
"A memory of addressability three stores three
bits of information in each memory location. We describe the three bits
of data as D[2:0]. In both cases, our notation A[high:low] and D[high:low]
reects the fact that we have numbered the bits of address and data from
right to left, in order, starting with the rightmost bit, which is numbered 0.
The notation [high:low] means a sequence of high  low + 1 bits such that
high is the bit number of the leftmost (or high) bit number in the sequence
and low is the bit number of the rightmost (or low) bit number in the sequence.","{'page_number': 70, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'A memory of addressability three stores three\nbits of information in each memory location. We describe the three bits\nof data as D[2:0]. In both cases, our notation A[high:low] and D[high:low]\nreects the fact that we have numbered the bits of address and data from\nright to left, in order, starting with the rightmost bit, which is numbered 0.\nThe notation [high:low] means a sequence of high  low + 1 bits such that\nhigh is the bit number of the leftmost (or high) bit number in the sequence\nand low is the bit number of the rightmost (or low) bit number in the sequence.'}"
"D [1]
D[2]
D[0]
Figure 3.20
A 22-by-3-bit memory.
Accesses of memory require decoding the address bits. Note that the address
decoder takes as input the address bits A[1:0] and asserts exactly one of its four
outputs, corresponding to the word line being addressed. In Figure 3.20, each row
of the memory corresponds to a unique three-bit word, thus the term word line.
Memory can be read by applying the address A[1:0], which asserts the word line
to be read. Note that each bit of the memory is ANDed with its word line and then
ORed with the corresponding bits of the other words. Since only one word line
can be asserted at a time, this is eectively a mux with the output of the decoder
providing the select function to each bit line. Thus, the appropriate word is read
at D[2:0].
Figure 3.21 shows the process of reading location 3. The code for 3 is 11.
The address A[1:0]=11 is decoded, and the bottom word line is asserted. Note
that the three other decoder outputs are not asserted. That is, they have the value
0. The value stored in location 3 is 101. These three bits are each ANDed with
their word line producing the bits 101, which are supplied to the three output
OR gates. Note that all other inputs to the OR gates are 0, since they have been
produced by ANDing with their unasserted word lines. The result is that D[2:0]
= 101. That is, the value stored in location 3 is output by the OR gates. Memory","{'page_number': 71, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'D [1]\nD[2]\nD[0]\nFigure 3.20\nA 22-by-3-bit memory.\nAccesses of memory require decoding the address bits. Note that the address\ndecoder takes as input the address bits A[1:0] and asserts exactly one of its four\noutputs, corresponding to the word line being addressed. In Figure 3.20, each row\nof the memory corresponds to a unique three-bit word, thus the term word line.\nMemory can be read by applying the address A[1:0], which asserts the word line\nto be read. Note that each bit of the memory is ANDed with its word line and then\nORed with the corresponding bits of the other words. Since only one word line\ncan be asserted at a time, this is eectively a mux with the output of the decoder\nproviding the select function to each bit line. Thus, the appropriate word is read\nat D[2:0].\nFigure 3.21 shows the process of reading location 3. The code for 3 is 11.\nThe address A[1:0]=11 is decoded, and the bottom word line is asserted. Note\nthat the three other decoder outputs are not asserted. That is, they have the value\n0. The value stored in location 3 is 101. These three bits are each ANDed with\ntheir word line producing the bits 101, which are supplied to the three output\nOR gates. Note that all other inputs to the OR gates are 0, since they have been\nproduced by ANDing with their unasserted word lines. The result is that D[2:0]\n= 101. That is, the value stored in location 3 is output by the OR gates. Memory'}"
"78
D[1]
D[2]
D[0]
  
1
0
1
Figure 3.21
Reading location 3 in our 22-by-3-bit memory.
can be written in a similar fashion. The address specied by A[1:0] is presented to
the address decoder, resulting in the correct word line being asserted. With write
enable (WE) also asserted, the three bits D[2:0] can be written into the three gated
latches corresponding to that word line.
3.6 Sequential Logic Circuits
In Section 3.3, we discussed digital logic structures that process information
(decision structures, we call them) wherein the outputs depend solely on the val-
ues that are present on the inputs now. Examples are muxes, decoders, and full
adders. We call these structures combinational logic circuits. In these circuits,
there is no sense of the past. Indeed, there is no capability for storing any infor-
mation about anything that happened before the present time. In Sections 3.4
and 3.5, we described structures that do store informationin Section 3.4, some
basic storage elements, and in Section 3.5, a simple 22-by-3-bit memory.","{'page_number': 72, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '78\nD[1]\nD[2]\nD[0]\n  \n1\n0\n1\nFigure 3.21\nReading location 3 in our 22-by-3-bit memory.\ncan be written in a similar fashion. The address specied by A[1:0] is presented to\nthe address decoder, resulting in the correct word line being asserted. With write\nenable (WE) also asserted, the three bits D[2:0] can be written into the three gated\nlatches corresponding to that word line.\n3.6 Sequential Logic Circuits\nIn Section 3.3, we discussed digital logic structures that process information\n(decision structures, we call them) wherein the outputs depend solely on the val-\nues that are present on the inputs now. Examples are muxes, decoders, and full\nadders. We call these structures combinational logic circuits. In these circuits,\nthere is no sense of the past. Indeed, there is no capability for storing any infor-\nmation about anything that happened before the present time. In Sections 3.4\nand 3.5, we described structures that do store informationin Section 3.4, some\nbasic storage elements, and in Section 3.5, a simple 22-by-3-bit memory.'}"
"Figure 3.22
Sequential logic circuit block diagram.
In this section, we discuss digital logic structures that can both process infor-
mation (i.e., make decisions) and store information. That is, these structures base
their decisions not only on the input values now present, but also (and this is
very important) on what has happened before. These structures are usually called
sequential logic circuits. They are distinguishable from combinational logic cir-
cuits because, unlike combinational logic circuits, they contain storage elements
that allow them to keep track of prior history information. Figure 3.22 shows a
block diagram of a sequential logic circuit. Note the storage elements. Note also
that the output can be dependent on both the inputs now and the values stored in
the storage elements. The values stored in the storage elements reect the history
of what has happened before.
Sequential logic circuits are used to implement a very important class of
mechanisms called nite state machines. We use nite state machines in essen-
tially all branches of engineering. For example, they are used as controllers of
electrical systems, mechanical systems, and aeronautical systems. A trac light
controller that sets the trac light to red, yellow, or green depends on the light
that is currently on (history information) and input information from sensors such
as trip wires on the road, a timer keeping track of how long the current light has
been on, and perhaps optical devices that are monitoring trac.
We will see in Chapter 4 when we introduce the von Neumann model of a
computer that a nite state machine is at the heart of the computer. It controls the
processing of information by the computer.
3.6.1 A Simple Example: The Combination Lock
A simple example shows the dierence between combinational logic structures
and sequential logic structures. Suppose one wishes to secure a bicycle with a
lock, but does not want to carry a key. A common solution is the combination
lock. The person memorizes a combination and uses it to open the lock. Two
common types of locks are shown in Figure 3.23.
In Figure 3.23a, the lock consists of a dial, with the numbers from 0 to 30
equally spaced around its circumference. To open the lock, one needs to know
the combination. One such combination could be: R13-L22-R3. If this were
the case, one would open the lock by turning the dial two complete turns to the
right (clockwise), and then continuing until the dial points to 13, followed by one","{'page_number': 73, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure 3.22\nSequential logic circuit block diagram.\nIn this section, we discuss digital logic structures that can both process infor-\nmation (i.e., make decisions) and store information. That is, these structures base\ntheir decisions not only on the input values now present, but also (and this is\nvery important) on what has happened before. These structures are usually called\nsequential logic circuits. They are distinguishable from combinational logic cir-\ncuits because, unlike combinational logic circuits, they contain storage elements\nthat allow them to keep track of prior history information. Figure 3.22 shows a\nblock diagram of a sequential logic circuit. Note the storage elements. Note also\nthat the output can be dependent on both the inputs now and the values stored in\nthe storage elements. The values stored in the storage elements reect the history\nof what has happened before.\nSequential logic circuits are used to implement a very important class of\nmechanisms called nite state machines. We use nite state machines in essen-\ntially all branches of engineering. For example, they are used as controllers of\nelectrical systems, mechanical systems, and aeronautical systems. A trac light\ncontroller that sets the trac light to red, yellow, or green depends on the light\nthat is currently on (history information) and input information from sensors such\nas trip wires on the road, a timer keeping track of how long the current light has\nbeen on, and perhaps optical devices that are monitoring trac.\nWe will see in Chapter 4 when we introduce the von Neumann model of a\ncomputer that a nite state machine is at the heart of the computer. It controls the\nprocessing of information by the computer.\n3.6.1 A Simple Example: The Combination Lock\nA simple example shows the dierence between combinational logic structures\nand sequential logic structures. Suppose one wishes to secure a bicycle with a\nlock, but does not want to carry a key. A common solution is the combination\nlock. The person memorizes a combination and uses it to open the lock. Two\ncommon types of locks are shown in Figure 3.23.\nIn Figure 3.23a, the lock consists of a dial, with the numbers from 0 to 30\nequally spaced around its circumference. To open the lock, one needs to know\nthe combination. One such combination could be: R13-L22-R3. If this were\nthe case, one would open the lock by turning the dial two complete turns to the\nright (clockwise), and then continuing until the dial points to 13, followed by one'}"
"80
(a)
(b)
Figure 3.23
Combination locks.
complete turn to the left (counterclockwise), and then continuing until the dial
points to 22, followed by turning the dial again to the right (clockwise) until it
points to 3. At that point, the lock opens. What is important here is the sequence
of the turns. The lock will not open, for example if one performed two turns to the
right, and then stopped on 22 (instead of 13), followed by one complete turn to
the left, ending on 13, followed by one turn to the right, ending on 3. That is, even
though the nal position of the dial is 3, and even though R22-L13-R3 uses the
same three numbers as the combination R13-L22-R3, the lock would not open.
Why? Because the lock stores the previous rotations and makes its decision (open
or dont open) on the basis of the the history of the past operations, that is, on the
correct sequence being performed.
Another type of lock is shown in Figure 3.23b. The mechanism consists of
(usually) four wheels, each containing the digits 0 through 9. When the digits
are lined up properly, the lock will open. In this case, the combination is the set
of four digits. Whether or not this lock opens is totally independent of the past
rotations of the four wheels. The lock does not care at all about past rotations.
The only thing important is the current value of each of the four wheels. This is
a simple example of a combinational structure.
It is curious that in our everyday speech, both mechanisms are referred to
as combination locks. In fact, only the lock of Figure 3.23b is a combinational
lock. The lock of Figure 3.23a would be better called a sequential lock!
3.6.2 The Concept of State
For the mechanism of Figure 3.23a to work properly, it has to keep track of the
sequence of rotations leading up to the opening of the lock. In particular, it has
to dierentiate the correct sequence R13-L22-R3 from all other sequences. For
example, R22-L13-R3 must not be allowed to open the lock. Likewise, R10-L22-
R3 must also not be allowed to open the lock.
For the lock of Figure 3.23a to work, it must identify several relevant
situations, as follows:
A. The lock is not open, and NO relevant operations have been
performed.
B. The lock is not open, but the user has just completed the
R13 operation.","{'page_number': 74, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '80\n(a)\n(b)\nFigure 3.23\nCombination locks.\ncomplete turn to the left (counterclockwise), and then continuing until the dial\npoints to 22, followed by turning the dial again to the right (clockwise) until it\npoints to 3. At that point, the lock opens. What is important here is the sequence\nof the turns. The lock will not open, for example if one performed two turns to the\nright, and then stopped on 22 (instead of 13), followed by one complete turn to\nthe left, ending on 13, followed by one turn to the right, ending on 3. That is, even\nthough the nal position of the dial is 3, and even though R22-L13-R3 uses the\nsame three numbers as the combination R13-L22-R3, the lock would not open.\nWhy? Because the lock stores the previous rotations and makes its decision (open\nor dont open) on the basis of the the history of the past operations, that is, on the\ncorrect sequence being performed.\nAnother type of lock is shown in Figure 3.23b. The mechanism consists of\n(usually) four wheels, each containing the digits 0 through 9. When the digits\nare lined up properly, the lock will open. In this case, the combination is the set\nof four digits. Whether or not this lock opens is totally independent of the past\nrotations of the four wheels. The lock does not care at all about past rotations.\nThe only thing important is the current value of each of the four wheels. This is\na simple example of a combinational structure.\nIt is curious that in our everyday speech, both mechanisms are referred to\nas combination locks. In fact, only the lock of Figure 3.23b is a combinational\nlock. The lock of Figure 3.23a would be better called a sequential lock!\n3.6.2 The Concept of State\nFor the mechanism of Figure 3.23a to work properly, it has to keep track of the\nsequence of rotations leading up to the opening of the lock. In particular, it has\nto dierentiate the correct sequence R13-L22-R3 from all other sequences. For\nexample, R22-L13-R3 must not be allowed to open the lock. Likewise, R10-L22-\nR3 must also not be allowed to open the lock.\nFor the lock of Figure 3.23a to work, it must identify several relevant\nsituations, as follows:\nA. The lock is not open, and NO relevant operations have been\nperformed.\nB. The lock is not open, but the user has just completed the\nR13 operation.'}"
"C. The lock is not open, but the user has just completed R13,
followed by L22.
D. The lock is open, since the user has just completed R13,
followed by L22, followed by R3.
We have labeled these four situations A, B, C, and D. We refer to each of these
situations as the state of the lock.
The notion of state is a very important concept in computer engineering, and
actually, in just about all branches of engineering. The state of a mechanism
more generally, the state of a systemis a snapshot of that system in which all
relevant items are explicitly expressed.
That is: The state of a system is a snapshot of all the relevant elements of the
system at the moment the snapshot is taken.
In the case of the lock of Figure 3.23a, there are four states A, B, C, and D.
Either the lock is open (State D), or if it is not open, we have already performed
either zero (State A), one (State B), or two (State C) correct operations. This is
the sum total of all possible states that can exist.
Question: Why are there exactly four states needed to describe the combina-
tion lock of Figure 3.23a? Can you think of a snapshot of the combination lock
after an operation (Rn or Ln) that requires a fth state because it is not covered
by one of the four states A, B, C, or D?
There are many examples of systems that you are familiar with that can be
easily described by means of states.
The state of a game of basketball can be described by the scoreboard in the
basketball arena. Figure 3.24 shows the state of the basketball game as Texas 73,
Oklahoma 68, 7 minutes and 38 seconds left in the second half, 14 seconds left on
the s
fouls
ball
Figure 3.24
An example of a state.","{'page_number': 75, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'C. The lock is not open, but the user has just completed R13,\nfollowed by L22.\nD. The lock is open, since the user has just completed R13,\nfollowed by L22, followed by R3.\nWe have labeled these four situations A, B, C, and D. We refer to each of these\nsituations as the state of the lock.\nThe notion of state is a very important concept in computer engineering, and\nactually, in just about all branches of engineering. The state of a mechanism\nmore generally, the state of a systemis a snapshot of that system in which all\nrelevant items are explicitly expressed.\nThat is: The state of a system is a snapshot of all the relevant elements of the\nsystem at the moment the snapshot is taken.\nIn the case of the lock of Figure 3.23a, there are four states A, B, C, and D.\nEither the lock is open (State D), or if it is not open, we have already performed\neither zero (State A), one (State B), or two (State C) correct operations. This is\nthe sum total of all possible states that can exist.\nQuestion: Why are there exactly four states needed to describe the combina-\ntion lock of Figure 3.23a? Can you think of a snapshot of the combination lock\nafter an operation (Rn or Ln) that requires a fth state because it is not covered\nby one of the four states A, B, C, or D?\nThere are many examples of systems that you are familiar with that can be\neasily described by means of states.\nThe state of a game of basketball can be described by the scoreboard in the\nbasketball arena. Figure 3.24 shows the state of the basketball game as Texas 73,\nOklahoma 68, 7 minutes and 38 seconds left in the second half, 14 seconds left on\nthe s\nfouls\nball\nFigure 3.24\nAn example of a state.'}"
"82
(a)
(b)
(c)
Figure 3.25
Three states in a tic-tac-toe machine.
a two-point shot, the new state would be described by the updated scoreboard.
That is, the score would then be Texas 75, Oklahoma 68, the time remaining in
the game would be 7 minutes and 26 seconds, the shot clock would be back to 25
seconds, and Oklahoma would have the ball.
The game of tic-tac-toe can also be described in accordance with the notion
of state. Recall that the game is played by two people (or, in our case, a person
and the computer). The state is a snapshot of the game in progress each time the
computer asks the person to make a move. The game is played as follows: There
are nine locations on the diagram. The person and then the computer take turns
placing an X (the person) and an O (the computer) in an empty location. The
person goes rst. The winner is the rst to place three symbols (three Xs for the
person, three Os for the computer) in a straight line, either vertically, horizontally,
or diagonally.
The initial state, before either the person or the computer has had a turn, is
shown in Figure 3.25a. Figure 3.25b shows a possible state of the game when the
person is prompted for a second move, if he/she put an X in the upper left corner
as his/her rst move, and the computer followed with an O in the middle square
as its rst move. Figure 3.25c shows a possible state of the game when the person
is prompted for a third move if he/she put an X in the upper right corner on the
second move, and the computer followed by putting its second O in the upper
middle location.
One nal example: a very old soft drink machine, when drinks sold for
15 cents, and the machine would only take nickels (5 cents) and dimes (10 cents)
and not be able to give change.
The state of the machine can be described as the amount of money inserted,
and whether the machine is open (so one can remove a bottle). There are only
three possible states:
A. The lock is open, so a bottle can be (or has been!) removed.
B. The lock is not open, but 5 cents has been inserted.
C. The lock is not open, but 10 cents has been inserted.
3.6.3 The Finite State Machine and Its State Diagram
We have seen that a state is a snapshot of all relevant parts of a system at a
particular point in time. At other times, that system can be in other states. We
have described four systems: a combination lock, a basketball game, a tic-tac-
toe machine, and a very old soft drink machine when a bottle of cola cost only","{'page_number': 76, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '82\n(a)\n(b)\n(c)\nFigure 3.25\nThree states in a tic-tac-toe machine.\na two-point shot, the new state would be described by the updated scoreboard.\nThat is, the score would then be Texas 75, Oklahoma 68, the time remaining in\nthe game would be 7 minutes and 26 seconds, the shot clock would be back to 25\nseconds, and Oklahoma would have the ball.\nThe game of tic-tac-toe can also be described in accordance with the notion\nof state. Recall that the game is played by two people (or, in our case, a person\nand the computer). The state is a snapshot of the game in progress each time the\ncomputer asks the person to make a move. The game is played as follows: There\nare nine locations on the diagram. The person and then the computer take turns\nplacing an X (the person) and an O (the computer) in an empty location. The\nperson goes rst. The winner is the rst to place three symbols (three Xs for the\nperson, three Os for the computer) in a straight line, either vertically, horizontally,\nor diagonally.\nThe initial state, before either the person or the computer has had a turn, is\nshown in Figure 3.25a. Figure 3.25b shows a possible state of the game when the\nperson is prompted for a second move, if he/she put an X in the upper left corner\nas his/her rst move, and the computer followed with an O in the middle square\nas its rst move. Figure 3.25c shows a possible state of the game when the person\nis prompted for a third move if he/she put an X in the upper right corner on the\nsecond move, and the computer followed by putting its second O in the upper\nmiddle location.\nOne nal example: a very old soft drink machine, when drinks sold for\n15 cents, and the machine would only take nickels (5 cents) and dimes (10 cents)\nand not be able to give change.\nThe state of the machine can be described as the amount of money inserted,\nand whether the machine is open (so one can remove a bottle). There are only\nthree possible states:\nA. The lock is open, so a bottle can be (or has been!) removed.\nB. The lock is not open, but 5 cents has been inserted.\nC. The lock is not open, but 10 cents has been inserted.\n3.6.3 The Finite State Machine and Its State Diagram\nWe have seen that a state is a snapshot of all relevant parts of a system at a\nparticular point in time. At other times, that system can be in other states. We\nhave described four systems: a combination lock, a basketball game, a tic-tac-\ntoe machine, and a very old soft drink machine when a bottle of cola cost only'}"
"15 cents. The behavior of each of these systems can be specied by a nite state
machine, and represented as a state diagram.
A nite state machine consists of ve elements:
1. a nite number of states
2. a nite number of external inputs
3. a nite number of external outputs
4. an explicit specication of all state transitions
5. an explicit specication of what determines each external
output value.
The set of states represents all possible situations (or snapshots) that the sys-
tem can be in. Each state transition describes what it takes to get from one state
to another.
Lets examine the nite state machines for these four systems.
The Combination Lock
A state diagram is a convenient representation of a
nite state machine. Figure 3.26 is a state diagram for the combination lock.
Recall, we identied four states A, B, C, and D. Which state we are in depends
on the progress we have made in getting from a random initial state to the lock
being open. In the state diagram of Figure 3.26, each circle corresponds to one
of the four states, A, B, C, or D.
The external inputs are R13, L22, R3, and R-other-than-13, L-other-than-22,
and R-other-than-3.
The external output is either the lock is open or the lock is not open. (One
logical variable will suce to describe that!) As shown in the state diagram, in
states A, B, and C, the combination lock is locked. In state D, the combination
lo
th
Figure 3.26
State diagram of the combination lock of Figure 3.23a.","{'page_number': 77, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '15 cents. The behavior of each of these systems can be specied by a nite state\nmachine, and represented as a state diagram.\nA nite state machine consists of ve elements:\n1. a nite number of states\n2. a nite number of external inputs\n3. a nite number of external outputs\n4. an explicit specication of all state transitions\n5. an explicit specication of what determines each external\noutput value.\nThe set of states represents all possible situations (or snapshots) that the sys-\ntem can be in. Each state transition describes what it takes to get from one state\nto another.\nLets examine the nite state machines for these four systems.\nThe Combination Lock\nA state diagram is a convenient representation of a\nnite state machine. Figure 3.26 is a state diagram for the combination lock.\nRecall, we identied four states A, B, C, and D. Which state we are in depends\non the progress we have made in getting from a random initial state to the lock\nbeing open. In the state diagram of Figure 3.26, each circle corresponds to one\nof the four states, A, B, C, or D.\nThe external inputs are R13, L22, R3, and R-other-than-13, L-other-than-22,\nand R-other-than-3.\nThe external output is either the lock is open or the lock is not open. (One\nlogical variable will suce to describe that!) As shown in the state diagram, in\nstates A, B, and C, the combination lock is locked. In state D, the combination\nlo\nth\nFigure 3.26\nState diagram of the combination lock of Figure 3.23a.'}"
"on each arc species which state the system is coming from and which state it is
going to. We refer to the state the system is coming from as the current state,
and the state it is going to as the next state. The combination lock has eight state
transitions. Associated with each transition is the input that causes the transition
from the current state to the next state. For example, R13 causes the transition
from state A to state B.
A couple of things are worth noting. First, it is usually the case that from a
current state there are multiple transitions to next states. The state transition that
occurs depends on both the current state and the value of the external input. For
example, if the combination lock is in state B, and the input is L22, the next state
is state C. If the current state is state B and the input is anything other than L22,
the next state is state A. In short, the next state is determined by the combination
of the current state and the current external input.
The output values of a system can also be determined by the combination of
the current state and the value of the current external input. However, as is the case
for the combination lock, where states A, B, and C specify the lock is locked,
and state D species the lock is unlocked, the output can also be determined
solely by the current state of the system. In all the systems we will study in this
book, the output values will be specied solely by the current state of the system.
A Very Old Soft Drink Machine
Figure 3.27 is the state diagram for the soft
drink machine.
The soft drink machine has only three states: 5 cents has been inserted,
10 cents has been inserted, and at least 15 cents has been inserted. Transitions
are caused by the insertion (the input) of a nickel or a dime. The output is asso-
Figure 3.27
State diagram of the soft drink machine.
A Basketball Game
We could similarly draw a state diagram for the basketball
game we described earlier, where each state would be one possible conguration
of the scoreboard. A transition would occur if either the referee blew a whistle or
the other team got the ball. We showed earlier the transition that would be caused
by Texas scoring a two-point shot. Clearly, the number of states in the nite state
machine describing a basketball game would be huge.","{'page_number': 78, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'on each arc species which state the system is coming from and which state it is\ngoing to. We refer to the state the system is coming from as the current state,\nand the state it is going to as the next state. The combination lock has eight state\ntransitions. Associated with each transition is the input that causes the transition\nfrom the current state to the next state. For example, R13 causes the transition\nfrom state A to state B.\nA couple of things are worth noting. First, it is usually the case that from a\ncurrent state there are multiple transitions to next states. The state transition that\noccurs depends on both the current state and the value of the external input. For\nexample, if the combination lock is in state B, and the input is L22, the next state\nis state C. If the current state is state B and the input is anything other than L22,\nthe next state is state A. In short, the next state is determined by the combination\nof the current state and the current external input.\nThe output values of a system can also be determined by the combination of\nthe current state and the value of the current external input. However, as is the case\nfor the combination lock, where states A, B, and C specify the lock is locked,\nand state D species the lock is unlocked, the output can also be determined\nsolely by the current state of the system. In all the systems we will study in this\nbook, the output values will be specied solely by the current state of the system.\nA Very Old Soft Drink Machine\nFigure 3.27 is the state diagram for the soft\ndrink machine.\nThe soft drink machine has only three states: 5 cents has been inserted,\n10 cents has been inserted, and at least 15 cents has been inserted. Transitions\nare caused by the insertion (the input) of a nickel or a dime. The output is asso-\nFigure 3.27\nState diagram of the soft drink machine.\nA Basketball Game\nWe could similarly draw a state diagram for the basketball\ngame we described earlier, where each state would be one possible conguration\nof the scoreboard. A transition would occur if either the referee blew a whistle or\nthe other team got the ball. We showed earlier the transition that would be caused\nby Texas scoring a two-point shot. Clearly, the number of states in the nite state\nmachine describing a basketball game would be huge.'}"
"Also clearly, the number of legitimate transitions from one state to another is
small, compared to the number of arcs one could draw connecting arbitrary pairs
of states. For example, there is no arc from a score of Texas 68, Oklahoma 67 to
Texas 75, Oklahoma 91, since no single input can cause that transition. The input
is the activity that occurred on the basketball court since the last transition. Some
input values are: Texas scored two points, Oklahoma scored three points, Texas
stole the ball, Oklahoma successfully rebounded a Texas shot, and so forth.
The output is the nal result of the game. The output has three values: Game
still in progress, Texas wins, Oklahoma wins.
Question: Can one have an arc from a state where the score is Texas 30,
Oklahoma 28 to a state where the score is tied, Texas 30, Oklahoma 30? Is it
possible to have two states, one where Texas is ahead 30-28 and the other where
the score is tied 30-30, but no arc between the two?
A Tic-Tac-Toe Machine
We could also draw a state diagram for a tic-tac-toe
machine, in our case when a person is playing against a computer. Each state is a
representation of the position of the game when the person is asked to put an X
into one of the empty cells. Figure 3.25 shows three states. The transition from the
state of Figure 3.25a to the state of Figure 3.25b is caused by the person putting
an X in the top left cell, followed by the computer putting an O in the center cell.
The transition from the state of Figure 3.25b to the state of Figure 3.25c is caused
by the person putting an X in the top right cell, followed by the computer putting
an O in the top middle cell.
Since there are nine cells, and each state has an X, an O, or nothing in each
cell, there must be fewer than 39 states in the tic-tac-toe machine. Clearly there
are far fewer than that, due to various constraints of the game.
There are nine inputs, corresponding to the nine cells a person can put an
X in. There are three outputs: (a) game still in progress, (b) person wins, and
(c) computer wins.
3.6.4 The Synchronous Finite State Machine
Up to now a transition from a current state to a next state in our nite state machine
happened when it happened. For example, a person could insert a nickel into the
soft drink machine and then wait 10 seconds or 10 minutes before inserting the
next coin into the machine. And the soft drink machine would not complain. It
would not dispense the soft drink until 15 cents was inserted, but it would wait
patiently as long as necessary for the 15 cents to be inserted. That is, there is no
xed amount of time between successive inputs to the nite state machine. This
is true in the case of all four systems we have discussed. We say these systems are
asynchronous because there is nothing synchronizing when each state transition
must occur.
However, almost no computers work that way","{'page_number': 79, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'It\nwould not dispense the soft drink until 15 cents was inserted, but it would wait\npatiently as long as necessary for the 15 cents to be inserted. That is, there is no\nxed amount of time between successive inputs to the nite state machine. This\nis true in the case of all four systems we have discussed. We say these systems are\nasynchronous because there is nothing synchronizing when each state transition\nmust occur.\nHowever, almost no computers work that way. On the contrary, we say that\ncomputers are synchronous because the state transitions take place, one after the\nother, at identical xed units of time. They are controlled by a synchronous nite\nstate machine. We will save for Chapter 4 and beyond the state transitions that\noccur at identical, xed units of time that control a computer. In this chapter, we'}"
"It
would not dispense the soft drink until 15 cents was inserted, but it would wait
patiently as long as necessary for the 15 cents to be inserted. That is, there is no
xed amount of time between successive inputs to the nite state machine. This
is true in the case of all four systems we have discussed. We say these systems are
asynchronous because there is nothing synchronizing when each state transition
must occur.
However, almost no computers work that way. On the contrary, we say that
computers are synchronous because the state transitions take place, one after the
other, at identical xed units of time. They are controlled by a synchronous nite
state machine. We will save for Chapter 4 and beyond the state transitions that
occur at identical, xed units of time that control a computer. In this chapter, we","{'page_number': 79, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'It\nwould not dispense the soft drink until 15 cents was inserted, but it would wait\npatiently as long as necessary for the 15 cents to be inserted. That is, there is no\nxed amount of time between successive inputs to the nite state machine. This\nis true in the case of all four systems we have discussed. We say these systems are\nasynchronous because there is nothing synchronizing when each state transition\nmust occur.\nHowever, almost no computers work that way. On the contrary, we say that\ncomputers are synchronous because the state transitions take place, one after the\nother, at identical xed units of time. They are controlled by a synchronous nite\nstate machine. We will save for Chapter 4 and beyond the state transitions that\noccur at identical, xed units of time that control a computer. In this chapter, we'}"
"will take on a simpler task, the design of a trac controller, an admittedly simpler
structure, but one that is also controlled by a synchronous nite state machine.
It is worth pointing out that both the four asynchronous nite state machines
discussed above and the synchronous nite state machine that controls a digital
computer share an important characteristic: They carry out work, one state tran-
sition at a time, moving closer to a goal. In the case of the combination lock, as
long as you make the correct moves, each state transition takes us closer to the
lock opening. In the case of the soft drink machine, each state transition takes us
closer to enjoying the taste of the soft drink. In the case of a computer, each state
transition takes us closer to solving a problem by processing a computer program
that someone has written.
3.6.5 The Clock
A synchronous nite state machine transitions from its current state to its next
state after an identical xed interval of time. Control of that synchronous behavior
is in part the responsibility of the clock circuit.
A clock circuit produces a signal, commonly referred to as THE clock, whose
value alternates between 0 volts and some specied xed voltage. In digital logic
terms, the clock is a signal whose value alternates between 0 and 1. Figure 3.28
shows the value of the clock signal as a function of time. Each of the repeated
sequence of identical intervals is referred to as a clock cycle. A clock cycle starts
when the clock signal transitions from 0 to 1 and ends the next time the clock
signal transitions from 0 to 1.
We will see in Chapter 5 and beyond that in each clock cycle, a computer
can perform a piece of useful work. When people say their laptop computers run
at a frequency of 2 gigahertz, they are saying their laptop computers perform
two billion pieces of work each second since 2 gigahertz means two billion clock
cycles each second, each clock cycle lasting for just one-half of a nanosecond. The
synchronous nite state machine makes one state transition each clock cycle.
We will show by means of a trac signal controller how the clock signal
controls the transition, xed clock cycle after xed clock cycle, from one state to
the next.
1
0
Figure 3.28
A clock signal.","{'page_number': 80, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'will take on a simpler task, the design of a trac controller, an admittedly simpler\nstructure, but one that is also controlled by a synchronous nite state machine.\nIt is worth pointing out that both the four asynchronous nite state machines\ndiscussed above and the synchronous nite state machine that controls a digital\ncomputer share an important characteristic: They carry out work, one state tran-\nsition at a time, moving closer to a goal. In the case of the combination lock, as\nlong as you make the correct moves, each state transition takes us closer to the\nlock opening. In the case of the soft drink machine, each state transition takes us\ncloser to enjoying the taste of the soft drink. In the case of a computer, each state\ntransition takes us closer to solving a problem by processing a computer program\nthat someone has written.\n3.6.5 The Clock\nA synchronous nite state machine transitions from its current state to its next\nstate after an identical xed interval of time. Control of that synchronous behavior\nis in part the responsibility of the clock circuit.\nA clock circuit produces a signal, commonly referred to as THE clock, whose\nvalue alternates between 0 volts and some specied xed voltage. In digital logic\nterms, the clock is a signal whose value alternates between 0 and 1. Figure 3.28\nshows the value of the clock signal as a function of time. Each of the repeated\nsequence of identical intervals is referred to as a clock cycle. A clock cycle starts\nwhen the clock signal transitions from 0 to 1 and ends the next time the clock\nsignal transitions from 0 to 1.\nWe will see in Chapter 5 and beyond that in each clock cycle, a computer\ncan perform a piece of useful work. When people say their laptop computers run\nat a frequency of 2 gigahertz, they are saying their laptop computers perform\ntwo billion pieces of work each second since 2 gigahertz means two billion clock\ncycles each second, each clock cycle lasting for just one-half of a nanosecond. The\nsynchronous nite state machine makes one state transition each clock cycle.\nWe will show by means of a trac signal controller how the clock signal\ncontrols the transition, xed clock cycle after xed clock cycle, from one state to\nthe next.\n1\n0\nFigure 3.28\nA clock signal.'}"
"3.6.6 Example: A Danger Sign
Many electrical, mechanical, and aeronautical systems are controlled by a syn-
chronous nite state machine. In this section, we will design the complete logic
needed for a synchronous nite state machine to control a trac danger sign.
Figure 3.29 shows the danger sign as it will be placed on the highway. Note the
sign says, Danger, Move Right. The sign contains ve lights (labeled 1 through
5 in the gure).
The purpose of our synchronous nite state machine (a.k.a. a controller) is to
direct the behavior of our system. In our case, the system is the set of lights on the
trac danger sign. The controllers job is to have the ve lights ash on and o
to warn automobile drivers to move to the right. The controller is equipped with
a switch. When the switch is in the ON position, the controller directs the lights
as follows: During one unit of time, all lights will be o. In the next unit of time,
lights 1 and 2 will be on. The next unit of time, lights 1, 2, 3, and 4 will be on.
Then all ve lights will be on. Then the sequence repeats: no lights on, followed
by 1 and 2 on, followed by 1, 2, 3, and 4 on, and so forth. Each unit of time lasts
one
dir
the
lig
Figure 3.29
A trafc danger sign.
The State Diagram for the Danger Sign Controller
Figure 3.30 is a state dia-
gram for the synchronous nite state machine that controls the lights. There are
four states, one for each of the four conditions corresponding to which lights are
on. Note that the outputs (whether each light is on or o) are determined by the
current state of the system.
If the switch is on (input = 1), the transition from each state to the next
state happens at one-second intervals, causing the lights to ash in the sequence","{'page_number': 81, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '3.6.6 Example: A Danger Sign\nMany electrical, mechanical, and aeronautical systems are controlled by a syn-\nchronous nite state machine. In this section, we will design the complete logic\nneeded for a synchronous nite state machine to control a trac danger sign.\nFigure 3.29 shows the danger sign as it will be placed on the highway. Note the\nsign says, Danger, Move Right. The sign contains ve lights (labeled 1 through\n5 in the gure).\nThe purpose of our synchronous nite state machine (a.k.a. a controller) is to\ndirect the behavior of our system. In our case, the system is the set of lights on the\ntrac danger sign. The controllers job is to have the ve lights ash on and o\nto warn automobile drivers to move to the right. The controller is equipped with\na switch. When the switch is in the ON position, the controller directs the lights\nas follows: During one unit of time, all lights will be o. In the next unit of time,\nlights 1 and 2 will be on. The next unit of time, lights 1, 2, 3, and 4 will be on.\nThen all ve lights will be on. Then the sequence repeats: no lights on, followed\nby 1 and 2 on, followed by 1, 2, 3, and 4 on, and so forth. Each unit of time lasts\none\ndir\nthe\nlig\nFigure 3.29\nA trafc danger sign.\nThe State Diagram for the Danger Sign Controller\nFigure 3.30 is a state dia-\ngram for the synchronous nite state machine that controls the lights. There are\nfour states, one for each of the four conditions corresponding to which lights are\non. Note that the outputs (whether each light is on or o) are determined by the\ncurrent state of the system.\nIf the switch is on (input = 1), the transition from each state to the next\nstate happens at one-second intervals, causing the lights to ash in the sequence'}"
"88
1
Figure 3.30
State diagram for the danger sign controller.
described. If the switch is turned o (input = 0), the state always transitions to
state A, the all o state.
The Sequential Logic Circuit for the Danger Sign Controller
Recall that
Fig
Fig
to
Clock
Figure 3.31
Sequential logic circuit for the danger sign controller.","{'page_number': 82, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '88\n1\nFigure 3.30\nState diagram for the danger sign controller.\ndescribed. If the switch is turned o (input = 0), the state always transitions to\nstate A, the all o state.\nThe Sequential Logic Circuit for the Danger Sign Controller\nRecall that\nFig\nFig\nto\nClock\nFigure 3.31\nSequential logic circuit for the danger sign controller.'}"
"First, the two external inputs: the switch and the clock. The switch determines
whether the nite state machine will transition through the four states or whether
it will transition to state A, where all lights are o. The other input (the clock)
controls the transition from state A to B, B to C, C to D, and D to A by controlling
the state of the storage elements. We will see how, momentarily.
Second, there are two storage elements for storing state information. Since
there are four states, and since each storage element can store one bit of informa-
tion, the four states are identied by the contents of the two storage elements: A
(00), B (01), C (10), and D (11). Storage element 2 contains the high bit; storage
element 1 contains the low bit. For example, the danger sign controller is in state
B when storage element 2 is 0 and storage element 1 is 1.
Third, combinational logic circuit 1 shows that the on/o behavior of the
lights is controlled by the storage elements. That is, the input to the combinational
logic circuit is from the two storage elements, that is, the current state of the nite
state machine.
Finally, combinational logic circuit 2 shows that the transition from the cur-
rent state to the next state depends on the two storage elements and the switch. If
the switch is on, the output of combinational logic circuit 2 depends on the state
of the two storage elements.
The Combinational Logic
Figure 3.32 shows the logic that implements com-
binational lo ic circuits 1 and 2.
exter
two s
Figure 3.32
Combinational logic circuits 1 and 2.","{'page_number': 83, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'First, the two external inputs: the switch and the clock. The switch determines\nwhether the nite state machine will transition through the four states or whether\nit will transition to state A, where all lights are o. The other input (the clock)\ncontrols the transition from state A to B, B to C, C to D, and D to A by controlling\nthe state of the storage elements. We will see how, momentarily.\nSecond, there are two storage elements for storing state information. Since\nthere are four states, and since each storage element can store one bit of informa-\ntion, the four states are identied by the contents of the two storage elements: A\n(00), B (01), C (10), and D (11). Storage element 2 contains the high bit; storage\nelement 1 contains the low bit. For example, the danger sign controller is in state\nB when storage element 2 is 0 and storage element 1 is 1.\nThird, combinational logic circuit 1 shows that the on/o behavior of the\nlights is controlled by the storage elements. That is, the input to the combinational\nlogic circuit is from the two storage elements, that is, the current state of the nite\nstate machine.\nFinally, combinational logic circuit 2 shows that the transition from the cur-\nrent state to the next state depends on the two storage elements and the switch. If\nthe switch is on, the output of combinational logic circuit 2 depends on the state\nof the two storage elements.\nThe Combinational Logic\nFigure 3.32 shows the logic that implements com-\nbinational lo ic circuits 1 and 2.\nexter\ntwo s\nFigure 3.32\nCombinational logic circuits 1 and 2.'}"
"First, let us look at the outputs that control the lights. As we have said, there
are only three outputs necessary to control the lights. Light 5 is controlled by
the output of the AND gate labeled V, since the only time light 5 is on is when
the controller is in state 11. Lights 3 and 4 are controlled by the output of the
OR gate labeled X, since there are two states in which those lights are on, those
labeled 10 and 11. Why are lights 1 and 2 controlled by the output of the OR gate
labeled W? See Exercise 3.42.
Next, let us look at the internal outputs that control the storage elements,
which specify the next state of the controller. Storage element 2 should be set
to 1 for the next clock cycle if the next state is 10 or 11. This is true only if the
switch is on and the current state is either 01 or 10. Therefore, the output signal
that will make storage element 2 be 1 in the next clock cycle is the output of the
OR gate labeled Y. Why is the next state of storage element 1 controlled by the
output of the OR gate labeled Z? See Exercise 3.42.
The Two Storage Elements
In order for the danger sign controller to work, the
state transitions must occur once per second when the switch is on.
A Problem with Gated Latches as Storage Elements
What would happen if the
storage elements were gated D latches? If the two storage elements were gated
D latches, when the write enable signal (the clock) is 1, the output of OR gates
Y and Z would immediately change the bits stored in the two gated D latches.
This would produce new input values to the three AND gates that are input to
OR gates Y and Z, producing new outputs that would be applied to the inputs of
the gated latches, which would in turn change the bits stored in the gated latches,
which would in turn mean new inputs to the three AND gates and new outputs
of OR gates Y and Z. This would happen again and again, continually changing
the bits stored in the two storage elements as long as the Write Enable signal to
the gated D latches was asserted. The result: We have no idea what the state of the
nite state machine would be for the next clock cycle. And, even in the current
clock cycle, the state of the storage elements would change so fast that the ve
lights would behave erratically.
The problem is the gated D latch. We want the output of OR gates Y and Z
to transition to the next state at the end of the current clock cycle and allow the
current state to remain unchanged until then. That is, we do not want the input
to the storage elements to take eect until the end of the current clock cycle.
Since the output of a gated D latch changes immediately in response to its input
if the Write Enable signal is asserted, it cannot be the storage element for our
synchronous nite state machine. We need storage elements that allow us to read
the current state throughout the current clock cycle, and not write the next state
values into the storage elements until the beginning of the next clock cycle","{'page_number': 84, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Since the output of a gated D latch changes immediately in response to its input\nif the Write Enable signal is asserted, it cannot be the storage element for our\nsynchronous nite state machine. We need storage elements that allow us to read\nthe current state throughout the current clock cycle, and not write the next state\nvalues into the storage elements until the beginning of the next clock cycle.\nThe Flip-Flop to the Rescue\nIt is worth repeating: To prevent the above from\nhappening, we need storage elements that allow us to read the current state\nthroughout the current clock cycle, and not write the next state values into the\nstorage elements until the beginning of the next clock cycle. That is, the function'}"
"Since the output of a gated D latch changes immediately in response to its input
if the Write Enable signal is asserted, it cannot be the storage element for our
synchronous nite state machine. We need storage elements that allow us to read
the current state throughout the current clock cycle, and not write the next state
values into the storage elements until the beginning of the next clock cycle.
The Flip-Flop to the Rescue
It is worth repeating: To prevent the above from
happening, we need storage elements that allow us to read the current state
throughout the current clock cycle, and not write the next state values into the
storage elements until the beginning of the next clock cycle. That is, the function","{'page_number': 84, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Since the output of a gated D latch changes immediately in response to its input\nif the Write Enable signal is asserted, it cannot be the storage element for our\nsynchronous nite state machine. We need storage elements that allow us to read\nthe current state throughout the current clock cycle, and not write the next state\nvalues into the storage elements until the beginning of the next clock cycle.\nThe Flip-Flop to the Rescue\nIt is worth repeating: To prevent the above from\nhappening, we need storage elements that allow us to read the current state\nthroughout the current clock cycle, and not write the next state values into the\nstorage elements until the beginning of the next clock cycle. That is, the function'}"
"Clock
Figure 3.33
A master/slave ip-op.
to be performed during a single clock cycle involves reading and writing a partic-
ular variable. Reading must be allowed throughout the clock cycle, and writing
must occur at the end of the clock cycle.
A ip-op can accomplish that. One example of a ip-op is the master/slave
ip-op shown in Figure 3.33. The master/slave ip-op can be constructed out
of two gated D latches, one referred to as the master, the other referred to as the
slave. Note that the write enable signal of the master is 1 when the clock is 0, and
the write enable signal of the slave is 1 when the clock is 1.
Figure 3.34 is a timing diagram for the master/slave ip-op, which shows
how and why the master/slave ip-op solves the problem. A timing diagram
shows time passing from left to right. Note that clock cycle n starts at the time
labeled 1 and ends at the time labeled 4. Clock cycle n+1 starts at the time
labeled 4.
Consider clock cycle n, which we will discuss in terms of its rst half A, its
second half B, and the four time points labeled 1, 2, 3, and 4.
At the start of each clock cycle, the outputs of the storage elements are the
outputs of the two slave latches. These outputs (starting at time 1) are input to
the AND gates, resulting in OR gates Y and Z producing the next state values for
the storage elements (at time 2). The timing diagram shows the propagation delay
of the combinational logic, that is, the time it takes for the combinational logic
to produce outputs of OR gates Y and Z. Although OR gates Y and Z produce
the Next State value sometime during half-cycle A, the write enable signal to the
master latches is 0, so the next state cannot be written into the master latches.
At the start of half-cycle B (at time 3), the clock signal is 0, which means
the write enable signal to the master latches is 1, and the master latches can be
written. However, during the half-cycle B, the write enable to the slave latches is
0, so the slave latches cannot write the new information now stored in the master
latches.
At the start of clock cycle n+1 (at time 4), the write enable signal to the slave
latches is 1, so the slave latches can store the next state value that was created by
the combinational logic during clock cycle n. This becomes the current state for
clock cycle n+1.","{'page_number': 85, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Clock\nFigure 3.33\nA master/slave ip-op.\nto be performed during a single clock cycle involves reading and writing a partic-\nular variable. Reading must be allowed throughout the clock cycle, and writing\nmust occur at the end of the clock cycle.\nA ip-op can accomplish that. One example of a ip-op is the master/slave\nip-op shown in Figure 3.33. The master/slave ip-op can be constructed out\nof two gated D latches, one referred to as the master, the other referred to as the\nslave. Note that the write enable signal of the master is 1 when the clock is 0, and\nthe write enable signal of the slave is 1 when the clock is 1.\nFigure 3.34 is a timing diagram for the master/slave ip-op, which shows\nhow and why the master/slave ip-op solves the problem. A timing diagram\nshows time passing from left to right. Note that clock cycle n starts at the time\nlabeled 1 and ends at the time labeled 4. Clock cycle n+1 starts at the time\nlabeled 4.\nConsider clock cycle n, which we will discuss in terms of its rst half A, its\nsecond half B, and the four time points labeled 1, 2, 3, and 4.\nAt the start of each clock cycle, the outputs of the storage elements are the\noutputs of the two slave latches. These outputs (starting at time 1) are input to\nthe AND gates, resulting in OR gates Y and Z producing the next state values for\nthe storage elements (at time 2). The timing diagram shows the propagation delay\nof the combinational logic, that is, the time it takes for the combinational logic\nto produce outputs of OR gates Y and Z. Although OR gates Y and Z produce\nthe Next State value sometime during half-cycle A, the write enable signal to the\nmaster latches is 0, so the next state cannot be written into the master latches.\nAt the start of half-cycle B (at time 3), the clock signal is 0, which means\nthe write enable signal to the master latches is 1, and the master latches can be\nwritten. However, during the half-cycle B, the write enable to the slave latches is\n0, so the slave latches cannot write the new information now stored in the master\nlatches.\nAt the start of clock cycle n+1 (at time 4), the write enable signal to the slave\nlatches is 1, so the slave latches can store the next state value that was created by\nthe combinational logic during clock cycle n. This becomes the current state for\nclock cycle n+1.'}"
"nal to the master latches is now 0, the state of the
Thus, although the write enable signal to the slave
t change because the master latches cannot change.
slave latches contains the current state of the system
cle and produces the inputs to the six AND gates in
ts. Their state changes at the start of the clock cycle
by storing the next state information created by the combinational logic during
the previous cycle but does not change again during the clock cycle. The reason
they do not change again during the clock cycle is as follows: During half-cycle
A, the master latches cannot change, so the slave latches continue to see the state
information that is the current state for the new clock cycle. During half-cycle B,
the slave latches cannot change because the clock signal is 0.
Meanwhile, during half-cycle B, the master latches can store the next state
information produced by the combinational logic, but they cannot write it into
the slave latches until the start of the next clock cycle, when it becomes the state
information for the next clock cycle.
3.7 Preview of Coming Attractions:
The Data Path of the LC-3
In Chapter 5, we will specify a computer, which we call the LC-3, and you will
have the opportunity to write computer programs to execute on it. We close out
Chapter 3 with a discussion of Figure 3.35, the data path of the LC-3 computer.
The data path consists of all the logic structures that combine to process
information in the core of the computer. Right now, Figure 3.35 is undoubtedly
more than a little intimidating, but you should not be concerned by that. You are
not ready to analyze it yet. That will come in Chapter 5. We have included it
here, however, to show you that you are already familiar with many of the basic
structures that make up a computer. For example, you see ve MUXes in the data
path, and you already know how they work. Also, an adder (shown as the ALU
symbol with a + sign inside) and an ALU. You know how those elements are
constructed from gates.
One element that we have not identified explicitly yet is a register. A register
is simply a set of n flip-flops that collectively are used to store one n-bit value. In
Figure 3.35, PC, IR, MAR, and MDR are all 16-bit registers that store 16 bits of
information each. The block labeled REG FILE consists of eight registers that each
store 16 bits of information. As you know, one bit of information can be stored in
one flip-flop. Therefore, each of these registers consists of 16 flip-flops. The data
path also shows three 1-bit registers, N, Z, and P. Those registers require only one
flip-flop each. In fact, a register can be any size that we need. The size depends only
on the number of bits we need to represent the value we wish to store.
One way to implement registers is with master/slave ip-ops. Figure 3.36
shows a four-bit register made up of four master/slave ip-ops","{'page_number': 86, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The data\npath also shows three 1-bit registers, N, Z, and P. Those registers require only one\nflip-flop each. In fact, a register can be any size that we need. The size depends only\non the number of bits we need to represent the value we wish to store.\nOne way to implement registers is with master/slave ip-ops. Figure 3.36\nshows a four-bit register made up of four master/slave ip-ops. We usually need\nip-ops, rather than latches, because it is usually important to be able to both\nread the contents of a register throughout a clock cycle and also store a new value'}"
"The data
path also shows three 1-bit registers, N, Z, and P. Those registers require only one
flip-flop each. In fact, a register can be any size that we need. The size depends only
on the number of bits we need to represent the value we wish to store.
One way to implement registers is with master/slave ip-ops. Figure 3.36
shows a four-bit register made up of four master/slave ip-ops. We usually need
ip-ops, rather than latches, because it is usually important to be able to both
read the contents of a register throughout a clock cycle and also store a new value","{'page_number': 86, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The data\npath also shows three 1-bit registers, N, Z, and P. Those registers require only one\nflip-flop each. In fact, a register can be any size that we need. The size depends only\non the number of bits we need to represent the value we wish to store.\nOne way to implement registers is with master/slave ip-ops. Figure 3.36\nshows a four-bit register made up of four master/slave ip-ops. We usually need\nip-ops, rather than latches, because it is usually important to be able to both\nread the contents of a register throughout a clock cycle and also store a new value'}"
"von Neumann
el
W
e are now ready to raise our level of abstraction another notch. We will
build on the logic structures that we studied in Chapter 3, both decision
elements and storage elements, to construct the basic computer model rst pro-
posed in the 1940s, usually referred to as the von Neumann machine. ...and, we
will write our rst computer program in the ISA of the LC-3.
4.1 Basic Components
To get a task done by a computer, we need two things: (a) a computer pro-
gram that species what the computer must do to perform the task, and (b) the
computer that is to carry out the task.
A computer program consists of a set of instructions, each specifying a well-
dened piece of work for the computer to carry out. The instruction is the smallest
piece of work specied in a computer program. That is, the computer either car-
ries out the work specied by an instruction or it does not. The computer does
not have the luxury of carrying out only a piece of an instruction.
John von Neumann proposed a fundamental model of a computer for process-
ing computer programs in 1946. Figure 4.1 shows its basic components. We have
taken a little poetic license and added a few of our own minor embellishments
to von Neumanns original diagram. The von Neumann model consists of ve
parts: memory, a processing unit, input, output, and a control unit. The computer
program is contained in the computers memory. The data the program needs to
carry out the work of the program is either contained in the programs memory
or is obtained from the input devices. The results of the programs execution are
provided by the output devices. The order in which the instructions are carried
out is performed by the control unit.
We will describe each of the ve parts of the von Neumann model in greater
detail.","{'page_number': 87, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'von Neumann\nel\nW\ne are now ready to raise our level of abstraction another notch. We will\nbuild on the logic structures that we studied in Chapter 3, both decision\nelements and storage elements, to construct the basic computer model rst pro-\nposed in the 1940s, usually referred to as the von Neumann machine. ...and, we\nwill write our rst computer program in the ISA of the LC-3.\n4.1 Basic Components\nTo get a task done by a computer, we need two things: (a) a computer pro-\ngram that species what the computer must do to perform the task, and (b) the\ncomputer that is to carry out the task.\nA computer program consists of a set of instructions, each specifying a well-\ndened piece of work for the computer to carry out. The instruction is the smallest\npiece of work specied in a computer program. That is, the computer either car-\nries out the work specied by an instruction or it does not. The computer does\nnot have the luxury of carrying out only a piece of an instruction.\nJohn von Neumann proposed a fundamental model of a computer for process-\ning computer programs in 1946. Figure 4.1 shows its basic components. We have\ntaken a little poetic license and added a few of our own minor embellishments\nto von Neumanns original diagram. The von Neumann model consists of ve\nparts: memory, a processing unit, input, output, and a control unit. The computer\nprogram is contained in the computers memory. The data the program needs to\ncarry out the work of the program is either contained in the programs memory\nor is obtained from the input devices. The results of the programs execution are\nprovided by the output devices. The order in which the instructions are carried\nout is performed by the control unit.\nWe will describe each of the ve parts of the von Neumann model in greater\ndetail.'}"
"122
PC
Figure 4.1
The von Neumann model, overall block diagram.
4.1.1 Memory
Recall that in Chapter 3 we examined a simple 22-by-3-bit memory that was con-
structed out of gates and latches. A more realistic memory for one of todays
computer systems is 234 by 8 bits. That is, a typical memory in todays world of
computers consists of 234 distinct memory locations, each of which is capable
of storing eight bits of information. We say that such a memory has an address
space of 234 uniquely identiable locations, and an addressability of eight bits.
We refer to such a memory as a 16-gigabyte memory (abbreviated, 16 GB). The
16 giga refers to the 234 locations, and the byte refers to the eight bits stored
in each location. The term is 16 giga because 16 is 24 and giga is the term we use
to represent 230, which is approximately one billion; 24 times 230 = 234. A byte is
the word we use to describe eight bits, much the way we use the word gallon to
describe four quarts.
We note (as we will note again and again) that with k bits, we can represent
uniquely 2k items. Thus, to uniquely identify 234 memory locations, each loca-
tion must have its own 34-bit address. In Chapter 5, we will begin the complete
denition of the LC-3 computer. We will see that the memory address space of
the LC-3 is 216, and the addressability is 16 bits.
Recall from Chapter 3 that we access memory by providing the address from
which we wish to read, or to which we wish to write. To read the contents of a","{'page_number': 88, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '122\nPC\nFigure 4.1\nThe von Neumann model, overall block diagram.\n4.1.1 Memory\nRecall that in Chapter 3 we examined a simple 22-by-3-bit memory that was con-\nstructed out of gates and latches. A more realistic memory for one of todays\ncomputer systems is 234 by 8 bits. That is, a typical memory in todays world of\ncomputers consists of 234 distinct memory locations, each of which is capable\nof storing eight bits of information. We say that such a memory has an address\nspace of 234 uniquely identiable locations, and an addressability of eight bits.\nWe refer to such a memory as a 16-gigabyte memory (abbreviated, 16 GB). The\n16 giga refers to the 234 locations, and the byte refers to the eight bits stored\nin each location. The term is 16 giga because 16 is 24 and giga is the term we use\nto represent 230, which is approximately one billion; 24 times 230 = 234. A byte is\nthe word we use to describe eight bits, much the way we use the word gallon to\ndescribe four quarts.\nWe note (as we will note again and again) that with k bits, we can represent\nuniquely 2k items. Thus, to uniquely identify 234 memory locations, each loca-\ntion must have its own 34-bit address. In Chapter 5, we will begin the complete\ndenition of the LC-3 computer. We will see that the memory address space of\nthe LC-3 is 216, and the addressability is 16 bits.\nRecall from Chapter 3 that we access memory by providing the address from\nwhich we wish to read, or to which we wish to write. To read the contents of a'}"
"111
Figure 4.2
Location 6 contains the value 4; location 4 contains the value 6.
memory location, we rst place the address of that location in the memorys
address register (MAR) and then interrogate the computers memory. The
information stored in the location having that address will be placed in the
memorys data register (MDR). To write (or store) a value in a memory location,
we rst write the address of the memory location in the MAR, and the value to be
stored in the MDR. We then interrogate the computers memory with the write
enable signal asserted. The information contained in the MDR will be written
into the memory location whose address is in the MAR.
Before we leave the notion of memory for the moment, let us again emphasize
the two characteristics of a memory location: its address and what is stored there.
Figure 4.2 shows a representation of a memory consisting of eight locations. Its
addresses are shown at the left, numbered in binary from 0 to 7. Each location
contains eight bits of information. Note that the value 6 is stored in the memory
location whose address is 4, and the value 4 is stored in the memory location
whose address is 6. These represent two very dierent situations.
Finally, an analogy: the post oce boxes in your local post oce. The box
number is like the memory locations address. Each box number is unique. The
information stored in the memory location is like the letters contained in the post
oce box. As time goes by, what is contained in the post oce box at any par-
ticular moment can change. But the box number remains the same. So, too, with
each memory location. The value stored in that location can be changed, but the
locations memory address remains unchanged.
4.1.2 Processing Unit
The actual processing of information in the computer is carried out by the
processing unit. The processing unit in a modern computer can consist of many
sophisticated complex functional units, each performing one particular operation
(divide, square root, etc.). The simplest processing unit, and the one normally
thought of when discussing the basic von Neumann model, is the ALU. ALU is the
abbreviation for Arithmetic and Logic Unit, so called because it is usually capa-
ble of performing basic arithmetic functions (like ADD and SUBTRACT) and
basic logic operations (like bit-wise AND, OR, and NOT) that we have already
studied in Chapter 2. We will see in Chapter 5 that the LC-3 has an ALU, which","{'page_number': 89, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '111\nFigure 4.2\nLocation 6 contains the value 4; location 4 contains the value 6.\nmemory location, we rst place the address of that location in the memorys\naddress register (MAR) and then interrogate the computers memory. The\ninformation stored in the location having that address will be placed in the\nmemorys data register (MDR). To write (or store) a value in a memory location,\nwe rst write the address of the memory location in the MAR, and the value to be\nstored in the MDR. We then interrogate the computers memory with the write\nenable signal asserted. The information contained in the MDR will be written\ninto the memory location whose address is in the MAR.\nBefore we leave the notion of memory for the moment, let us again emphasize\nthe two characteristics of a memory location: its address and what is stored there.\nFigure 4.2 shows a representation of a memory consisting of eight locations. Its\naddresses are shown at the left, numbered in binary from 0 to 7. Each location\ncontains eight bits of information. Note that the value 6 is stored in the memory\nlocation whose address is 4, and the value 4 is stored in the memory location\nwhose address is 6. These represent two very dierent situations.\nFinally, an analogy: the post oce boxes in your local post oce. The box\nnumber is like the memory locations address. Each box number is unique. The\ninformation stored in the memory location is like the letters contained in the post\noce box. As time goes by, what is contained in the post oce box at any par-\nticular moment can change. But the box number remains the same. So, too, with\neach memory location. The value stored in that location can be changed, but the\nlocations memory address remains unchanged.\n4.1.2 Processing Unit\nThe actual processing of information in the computer is carried out by the\nprocessing unit. The processing unit in a modern computer can consist of many\nsophisticated complex functional units, each performing one particular operation\n(divide, square root, etc.). The simplest processing unit, and the one normally\nthought of when discussing the basic von Neumann model, is the ALU. ALU is the\nabbreviation for Arithmetic and Logic Unit, so called because it is usually capa-\nble of performing basic arithmetic functions (like ADD and SUBTRACT) and\nbasic logic operations (like bit-wise AND, OR, and NOT) that we have already\nstudied in Chapter 2. We will see in Chapter 5 that the LC-3 has an ALU, which'}"
"can perform ADD, AND, and NOT operations. Two of these (ADD and AND)
we will discuss in this chapter.
The ALU normally processes data elements of a xed size referred to as the
word length of the computer. The data elements are called words. For example,
to perform ADD, the ALU receives two words as inputs and produces a single
word (the sum) as output. Each ISA has its own word length, depending on the
intended use of the computer.
Most microprocessors today that are used in PCs or workstations have a word
length of 64 bits (as is the case with Intels Core processors) or 32 bits (as is
the case with Intels Atom processors). Even most microprocessors now used in
cell phones have 64-bit word lengths, such as Apples A7 through A11 processors,
and Qualcomms SnapDragon processors. However, the microprocessors used in
very inexpensive applications often have word lengths of as little as 16 or even
8 bits.
In the LC-3, the ALU processes 16-bit words. We say the LC-3 has a word
length of 16 bits.
It is almost always the case that a computer provides some small amount of
storage very close to the ALU to allow results to be temporarily stored if they
will be needed to produce additional results in the near future. For example, if a
computer is to calculate (A+B)C, it could store the result of A+B in memory, and
then subsequently read it in order to multiply that result by C. However, the time
it takes to access memory is long compared to the time it takes to perform the
ADD or MULTIPLY. Almost all computers, therefore, have temporary storage
for storing the result of A + B in order to avoid the much longer access time that
would be necessary when it came time to multiply. The most common form of
temporary storage is a set of registers, like the register described in Section 3.7.
Typically, the size of each register is identical to the size of values processed
by the ALU; that is, they each contain one word. The LC-3 has eight registers
(R0, R1,  R7), each containing 16 bits.
Current microprocessors typically contain 32 registers, each consisting of 32
or 64 bits, depending on the architecture. These serve the same purpose as the
eight 16-bit registers in the LC-3. However, the importance of temporary storage
for values that most modern computers will need shortly means many computers
today have an additional set of special-purpose registers consisting of 128 bits of
information to handle special needs. Those special needs we will have to save for
later in your studies.
4.1.3 Input and Output
In order for a computer to process information, the information must get into
the computer. In order to use the results of that processing, those results must
be displayed in some fashion outside the computer. Many devices exist for the
purposes of input and output. They are generically referred to in computer jar-
gon as peripherals because they are in some sense accessories to the processing
function. Nonetheless, they are no less important.
In the LC-3 we will have the two most basic input and output devices","{'page_number': 90, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In order to use the results of that processing, those results must\nbe displayed in some fashion outside the computer. Many devices exist for the\npurposes of input and output. They are generically referred to in computer jar-\ngon as peripherals because they are in some sense accessories to the processing\nfunction. Nonetheless, they are no less important.\nIn the LC-3 we will have the two most basic input and output devices. For\ninput, we will use the keyboard; for output, we will use the monitor.'}"
"In order to use the results of that processing, those results must
be displayed in some fashion outside the computer. Many devices exist for the
purposes of input and output. They are generically referred to in computer jar-
gon as peripherals because they are in some sense accessories to the processing
function. Nonetheless, they are no less important.
In the LC-3 we will have the two most basic input and output devices. For
input, we will use the keyboard; for output, we will use the monitor.","{'page_number': 90, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In order to use the results of that processing, those results must\nbe displayed in some fashion outside the computer. Many devices exist for the\npurposes of input and output. They are generically referred to in computer jar-\ngon as peripherals because they are in some sense accessories to the processing\nfunction. Nonetheless, they are no less important.\nIn the LC-3 we will have the two most basic input and output devices. For\ninput, we will use the keyboard; for output, we will use the monitor.'}"
"There are, of course, many other input and output devices in computer sys-
tems today. For input we have among other things the mouse, digital scanners,
and shopping mall kiosks to help you navigate the shopping mall. For output we
have among other things printers, LED displays, disks, and shopping mall kiosks
to help you navigate the shopping mall. :-) In the old days, a lot of input and out-
put was carried out by punched cards. Fortunately, for those who would have to
lug around boxes of cards, the use of punched cards has largely disappeared.
4.1.4 Control Unit
The control unit is like the conductor of an orchestra; it is in charge of making
all the other parts of the computer play together. As we will see when we describe
the step-by-step process of executing a computer program, it is the control unit
that keeps track of both where we are within the process of executing the program
and where we are in the process of executing each instruction.
To keep track of which instruction is being executed, the control unit has an
instruction register to contain that instruction. To keep track of which instruc-
tion is to be processed next, the control unit has a register that contains the next
instructions address. For historical reasons, that register is called the program
counter (abbreviated PC), although a better name for it would be the instruction
pointer, since the contents of this register is, in some sense, pointing to the
next instruction to be processed. Curiously, Intel does in fact call that register the
instruction pointer, but the simple elegance of that name has not caught on.
4.2 The LC-3: An Example
von Neumann Machine
In Chapter 5, we will specify in detail the LC-3, a simple computer that we
will study extensively. We have already shown you its data path in Chapter 3
(Figure 3.35) and identied several of its structures in Section 4.1. In this sec-
tion, we will pull together all the parts of the LC-3 we need to describe it as a von
Neumann computer (see Figure 4.3).
We constructed Figure 4.3 by starting with the LC-3s full data path
(Figure 3.35) and removing all elements that are not essential to pointing out
the ve basic components of the von Neumann model.
Note that there are two kinds of arrowheads in Figure 4.3: lled-in and
not-lled-in. Filled-in arrowheads denote data elements that ow along the cor-
responding paths. Not-lled-in arrowheads denote control signals that control the
processing of the data elements. For example, the box labeled ALU in the pro-
cessing unit processes two 16-bit values and produces a 16-bit result. The two
sources and the result are all data, and are designated by lled-in arrowheads.
The operation performed on those two 16-bit data elements (it is labeled ALUK)
is part of the controltherefore, a not-lled-in arrowhead.
MEMORY consists of the storage elements, along with the Memory
Address Register (MAR) for addressing individual locations and the","{'page_number': 91, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'There are, of course, many other input and output devices in computer sys-\ntems today. For input we have among other things the mouse, digital scanners,\nand shopping mall kiosks to help you navigate the shopping mall. For output we\nhave among other things printers, LED displays, disks, and shopping mall kiosks\nto help you navigate the shopping mall. :-) In the old days, a lot of input and out-\nput was carried out by punched cards. Fortunately, for those who would have to\nlug around boxes of cards, the use of punched cards has largely disappeared.\n4.1.4 Control Unit\nThe control unit is like the conductor of an orchestra; it is in charge of making\nall the other parts of the computer play together. As we will see when we describe\nthe step-by-step process of executing a computer program, it is the control unit\nthat keeps track of both where we are within the process of executing the program\nand where we are in the process of executing each instruction.\nTo keep track of which instruction is being executed, the control unit has an\ninstruction register to contain that instruction. To keep track of which instruc-\ntion is to be processed next, the control unit has a register that contains the next\ninstructions address. For historical reasons, that register is called the program\ncounter (abbreviated PC), although a better name for it would be the instruction\npointer, since the contents of this register is, in some sense, pointing to the\nnext instruction to be processed. Curiously, Intel does in fact call that register the\ninstruction pointer, but the simple elegance of that name has not caught on.\n4.2 The LC-3: An Example\nvon Neumann Machine\nIn Chapter 5, we will specify in detail the LC-3, a simple computer that we\nwill study extensively. We have already shown you its data path in Chapter 3\n(Figure 3.35) and identied several of its structures in Section 4.1. In this sec-\ntion, we will pull together all the parts of the LC-3 we need to describe it as a von\nNeumann computer (see Figure 4.3).\nWe constructed Figure 4.3 by starting with the LC-3s full data path\n(Figure 3.35) and removing all elements that are not essential to pointing out\nthe ve basic components of the von Neumann model.\nNote that there are two kinds of arrowheads in Figure 4.3: lled-in and\nnot-lled-in. Filled-in arrowheads denote data elements that ow along the cor-\nresponding paths. Not-lled-in arrowheads denote control signals that control the\nprocessing of the data elements. For example, the box labeled ALU in the pro-\ncessing unit processes two 16-bit values and produces a 16-bit result. The two\nsources and the result are all data, and are designated by lled-in arrowheads.\nThe operation performed on those two 16-bit data elements (it is labeled ALUK)\nis part of the controltherefore, a not-lled-in arrowhead.\nMEMORY consists of the storage elements, along with the Memory\nAddress Register (MAR) for addressing individual locations and the'}"
"126
OUTPUT
INPUT
MEMORY
Figure 4.3
The LC-3 as an example of the von Neumann model.
Memory Data Register (MDR) for holding the contents of a memory
location on its way to/from the storage. Note that the MAR contains 16 bits,
reecting the fact that the memory address space of the LC-3 is 216
memory locations. The MDR contains 16 bits, reecting the fact that each
memory location contains 16 bitsthat is, the LC-3 is 16-bit addressable.
INPUT/OUTPUT consists of a keyboard and a monitor. The simplest
keyboard requires two registers: a keyboard data register (KBDR) for
holding the ASCII codes of keys struck and a keyboard status register
(KBSR) for maintaining status information about the keys struck. The
simplest monitor also requires two registers: a display data register (DDR)
for holding the ASCII code of something to be displayed on the screen and","{'page_number': 92, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '126\nOUTPUT\nINPUT\nMEMORY\nFigure 4.3\nThe LC-3 as an example of the von Neumann model.\nMemory Data Register (MDR) for holding the contents of a memory\nlocation on its way to/from the storage. Note that the MAR contains 16 bits,\nreecting the fact that the memory address space of the LC-3 is 216\nmemory locations. The MDR contains 16 bits, reecting the fact that each\nmemory location contains 16 bitsthat is, the LC-3 is 16-bit addressable.\nINPUT/OUTPUT consists of a keyboard and a monitor. The simplest\nkeyboard requires two registers: a keyboard data register (KBDR) for\nholding the ASCII codes of keys struck and a keyboard status register\n(KBSR) for maintaining status information about the keys struck. The\nsimplest monitor also requires two registers: a display data register (DDR)\nfor holding the ASCII code of something to be displayed on the screen and'}"
"a display status register (DSR) for maintaining associated status
information. These input and output registers will be discussed in detail
in Chapter 9.
THE PROCESSING UNIT consists of a functional unit (ALU) that
performs arithmetic and logic operations and eight registers (R0,  R7) for
storing temporary values that will be needed in the near future as operands
for subsequent instructions. The LC-3 ALU can perform one arithmetic
operation (addition) and two logical operations (bitwise AND and bitwise
NOT).
THE CONTROL UNIT consists of all the structures needed to manage
the processing that is carried out by the computer. Its most important
structure is the nite state machine, which directs all the activity. Recall the
nite state machines in Section 3.6. Processing is carried out step by step,
or rather, clock cycle by clock cycle. Note the CLK input to the nite state
machine in Figure 4.3. It species how long each clock cycle lasts. The
instruction register (IR) is also an input to the nite state machine since
the LC-3 instruction being processed determines what activities must be
carried out. The program counter (PC) is also a part of the control unit;
it keeps track of the next instruction to be executed after the current
instruction nishes.
Note that all the external outputs of the nite state machine in Figure 4.3 have
arrowheads that are not lled in. These outputs control the processing through-
out the computer. For example, one of these outputs (two bits) is ALUK, which
controls the operation performed in the ALU (ADD, AND, or NOT) during the
current clock cycle. Another output is GateALU, which determines whether or
not the output of the ALU is provided to the processor bus during the current
clock cycle.
The complete description of the data path, control, and nite state machine
for one implementation of the LC-3 is the subject of Appendix C.
4.3 Instruction Processing
The central idea in the von Neumann model of computer processing is that the
program and data are both stored as sequences of bits in the computers memory,
and the program is executed one instruction at a time under the direction of the
control unit.
4.3.1 The Instruction
The most basic unit of computer processing is the instruction. It is made up of two
parts, the opcode (what the instruction does) and the operands (who it does it to!).
There are fundamentally three kinds of instructions: operates, data move-
ment, and control, although many ISAs have some special instructions that are
necessary for those ISAs. Operate instructions operate on data. The LC-3 has
three operate instructions: one arithmetic (ADD) and two logicals (AND and","{'page_number': 93, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'a display status register (DSR) for maintaining associated status\ninformation. These input and output registers will be discussed in detail\nin Chapter 9.\nTHE PROCESSING UNIT consists of a functional unit (ALU) that\nperforms arithmetic and logic operations and eight registers (R0,  R7) for\nstoring temporary values that will be needed in the near future as operands\nfor subsequent instructions. The LC-3 ALU can perform one arithmetic\noperation (addition) and two logical operations (bitwise AND and bitwise\nNOT).\nTHE CONTROL UNIT consists of all the structures needed to manage\nthe processing that is carried out by the computer. Its most important\nstructure is the nite state machine, which directs all the activity. Recall the\nnite state machines in Section 3.6. Processing is carried out step by step,\nor rather, clock cycle by clock cycle. Note the CLK input to the nite state\nmachine in Figure 4.3. It species how long each clock cycle lasts. The\ninstruction register (IR) is also an input to the nite state machine since\nthe LC-3 instruction being processed determines what activities must be\ncarried out. The program counter (PC) is also a part of the control unit;\nit keeps track of the next instruction to be executed after the current\ninstruction nishes.\nNote that all the external outputs of the nite state machine in Figure 4.3 have\narrowheads that are not lled in. These outputs control the processing through-\nout the computer. For example, one of these outputs (two bits) is ALUK, which\ncontrols the operation performed in the ALU (ADD, AND, or NOT) during the\ncurrent clock cycle. Another output is GateALU, which determines whether or\nnot the output of the ALU is provided to the processor bus during the current\nclock cycle.\nThe complete description of the data path, control, and nite state machine\nfor one implementation of the LC-3 is the subject of Appendix C.\n4.3 Instruction Processing\nThe central idea in the von Neumann model of computer processing is that the\nprogram and data are both stored as sequences of bits in the computers memory,\nand the program is executed one instruction at a time under the direction of the\ncontrol unit.\n4.3.1 The Instruction\nThe most basic unit of computer processing is the instruction. It is made up of two\nparts, the opcode (what the instruction does) and the operands (who it does it to!).\nThere are fundamentally three kinds of instructions: operates, data move-\nment, and control, although many ISAs have some special instructions that are\nnecessary for those ISAs. Operate instructions operate on data. The LC-3 has\nthree operate instructions: one arithmetic (ADD) and two logicals (AND and'}"
"NOT). Data movement instructions move information from the processing unit
to and from memory and to and from input/output devices. The LC-3 has six data
movement instructions.
Control instructions are necessary for altering the sequential processing of
instructions. That is, normally the next instruction executed is the instruction
contained in the next memory location. If a program consists of instructions
1,2,3,4...10 located in memory locations A, A+1, A+2, ...A+9, normally the
instructions would be executed in the sequence 1,2,3...10. We will see before we
leave Chapter 4, however, that sometimes we will want to change the sequence.
Control instructions enable us to do that.
An LC-3 instruction consists of 16 bits (one word), numbered from left to
right, bit [15] to bit [0]. Bits [15:12] contain the opcode. This means there are at
most 24 distinct opcodes. Actually, we use only 15 of the possible four-bit codes.
One is reserved for some future use. Bits [11:0] are used to gure out where the
operands are.
Exa
s a
or a
as e
er
e con en s o reg s er
o
e con en s o","{'page_number': 94, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'NOT). Data movement instructions move information from the processing unit\nto and from memory and to and from input/output devices. The LC-3 has six data\nmovement instructions.\nControl instructions are necessary for altering the sequential processing of\ninstructions. That is, normally the next instruction executed is the instruction\ncontained in the next memory location. If a program consists of instructions\n1,2,3,4...10 located in memory locations A, A+1, A+2, ...A+9, normally the\ninstructions would be executed in the sequence 1,2,3...10. We will see before we\nleave Chapter 4, however, that sometimes we will want to change the sequence.\nControl instructions enable us to do that.\nAn LC-3 instruction consists of 16 bits (one word), numbered from left to\nright, bit [15] to bit [0]. Bits [15:12] contain the opcode. This means there are at\nmost 24 distinct opcodes. Actually, we use only 15 of the possible four-bit codes.\nOne is reserved for some future use. Bits [11:0] are used to gure out where the\noperands are.\nExa\ns a\nor a\nas e\ner\ne con en s o reg s er\no\ne con en s o'}"
"4.3.2 The Instruction Cycle (NOT the Clock Cycle!)
Instructions are processed under the direction of the control unit in a very sys-
tematic, step-by-step manner. The entire sequence of steps needed to process an
instruction is called the instruction cycle. The instruction cycle consists of six
sequential phases, each phase requiring zero or more steps. We say zero steps
to indicate that most computers have been designed such that not all instructions
require all six phases. We will discuss this momentarily. But rst, we will examine
the six phases of the instruction cycle:
FETCH
DECODE
EVALUATE ADDRESS
FETCH OPERANDS
EXECUTE
STORE RESULT
The process is as follows (again refer to Figure 4.3, our simplied version of
the LC-3 data path):
4.3.2.1 FETCH
The FETCH phase obtains the next instruction from memory and loads it into
the instruction register (IR) of the control unit. Recall that a computer program
consists of a number of instructions, that each instruction is represented by a
sequence of bits, and that the entire program (in the von Neumann model) is stored
in the computers memory. In order to carry out the work of an instruction, we
must rst identify where it is. The program counter (PC) contains the address of
the next instruction to be processed. Thus, the FETCH phase takes the following
steps:
First the MAR is loaded with the contents of the PC.
Next, the memory is interrogated, which results
in the next instruction being placed by the memory
into the MDR.
Finally, the IR is loaded with the contents
of the MDR.
We are now ready for the next phase, decoding the instruction. However, when
the instruction nishes execution, and we wish to fetch the next instruction, we
would like the PC to contain the address of the next instruction. This is accom-
plished by having the FETCH phase perform one more task: incrementing the
PC. In that way, after the current instruction nishes, the FETCH phase of the
next instruction will load into the IR the contents of the next memory location,
provided the execution of the current instruction does not involve changing the
value in the PC.
The complete description of the FETCH phase is as follows:
Step 1:
Load the MAR with the contents of the PC, and
simultaneously increment the PC.","{'page_number': 95, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '4.3.2 The Instruction Cycle (NOT the Clock Cycle!)\nInstructions are processed under the direction of the control unit in a very sys-\ntematic, step-by-step manner. The entire sequence of steps needed to process an\ninstruction is called the instruction cycle. The instruction cycle consists of six\nsequential phases, each phase requiring zero or more steps. We say zero steps\nto indicate that most computers have been designed such that not all instructions\nrequire all six phases. We will discuss this momentarily. But rst, we will examine\nthe six phases of the instruction cycle:\nFETCH\nDECODE\nEVALUATE ADDRESS\nFETCH OPERANDS\nEXECUTE\nSTORE RESULT\nThe process is as follows (again refer to Figure 4.3, our simplied version of\nthe LC-3 data path):\n4.3.2.1 FETCH\nThe FETCH phase obtains the next instruction from memory and loads it into\nthe instruction register (IR) of the control unit. Recall that a computer program\nconsists of a number of instructions, that each instruction is represented by a\nsequence of bits, and that the entire program (in the von Neumann model) is stored\nin the computers memory. In order to carry out the work of an instruction, we\nmust rst identify where it is. The program counter (PC) contains the address of\nthe next instruction to be processed. Thus, the FETCH phase takes the following\nsteps:\nFirst the MAR is loaded with the contents of the PC.\nNext, the memory is interrogated, which results\nin the next instruction being placed by the memory\ninto the MDR.\nFinally, the IR is loaded with the contents\nof the MDR.\nWe are now ready for the next phase, decoding the instruction. However, when\nthe instruction nishes execution, and we wish to fetch the next instruction, we\nwould like the PC to contain the address of the next instruction. This is accom-\nplished by having the FETCH phase perform one more task: incrementing the\nPC. In that way, after the current instruction nishes, the FETCH phase of the\nnext instruction will load into the IR the contents of the next memory location,\nprovided the execution of the current instruction does not involve changing the\nvalue in the PC.\nThe complete description of the FETCH phase is as follows:\nStep 1:\nLoad the MAR with the contents of the PC, and\nsimultaneously increment the PC.'}"
"Step 2:
Interrogate memory, resulting in the instruction
being placed in the MDR.
Step 3:
Load the IR with the contents of the MDR.
Each of these steps is under the direction of the control unit, much like, as we said
previously, the instruments in an orchestra are under the control of a conductors
baton. Each stroke of the conductors baton corresponds to one machine cycle.
We will see in Section 4.3.5 that the amount of time taken by each machine cycle
is one clock cycle. In fact, we often use the two terms interchangeably. Step 1
takes one clock cycle. Step 2 could take one clock cycle or many clock cycles,
depending on how long it takes to access the computers memory. Step 3 takes
one clock cycle. In a modern digital computer, a clock cycle takes a very small
fraction of a second.
Indeed, a 3.1 GHz Intel Core i7 completes 3.1 billion clock cycles in one
second. Said another way, one clock cycle takes 0.322 billionths of a second
(0.322 nanoseconds). Recall that the light bulb that is helping you read this text
is switching on and o at the rate of 60 times a second. Thus, in the time it takes
a light bulb to switch on and o once, todays computers can complete more than
51 million clock cycles!
4.3.2.2 DECODE
The DECODE phase examines the instruction in order to gure out what
the microarchitecture is being asked to do. Recall the decoders we studied in
Chapter 3. In the LC-3, a 4-to-16 decoder identies which of the 16 opcodes is to
be processed (even though one of the 16 is not used!). Input is the four-bit opcode
IR [15:12]. The output line asserted is the one corresponding to the opcode at
the input. Depending on which output of the decoder is asserted, the remaining
12 bits identify what else is needed to process that instruction.
4.3.2.3 EVALUATE ADDRESS
This phase computes the address of the memory location that is needed to pro-
cess the instruction. Recall the example of the LD instruction: The LD instruction
causes a value stored in memory to be loaded into a register. In that exam-
ple, the address was obtained by sign-extending bits [8:0] of the instruction to
16 bits and adding that value to the current contents of the PC. This calculation
was performed during the EVALUATE ADDRESS phase. It is worth noting that
not all instructions access memory to load or store data. For example, we have
already seen that the ADD and AND instructions in the LC-3 obtain their source
operands from registers or from the instruction itself and store the result of the
ADD or AND instruction in a register. For those instructions, the EVALUATE
ADDRESS phase is not needed.
4.3.2.4 FETCH OPERANDS
This phase obtains the source operands needed to process the instruction. In the
LD example, this phase took two steps: loading MAR with the address calculated
in the EVALUATE ADDRESS phase and reading memory that resulted in the
source operand being placed in MDR.","{'page_number': 96, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Step 2:\nInterrogate memory, resulting in the instruction\nbeing placed in the MDR.\nStep 3:\nLoad the IR with the contents of the MDR.\nEach of these steps is under the direction of the control unit, much like, as we said\npreviously, the instruments in an orchestra are under the control of a conductors\nbaton. Each stroke of the conductors baton corresponds to one machine cycle.\nWe will see in Section 4.3.5 that the amount of time taken by each machine cycle\nis one clock cycle. In fact, we often use the two terms interchangeably. Step 1\ntakes one clock cycle. Step 2 could take one clock cycle or many clock cycles,\ndepending on how long it takes to access the computers memory. Step 3 takes\none clock cycle. In a modern digital computer, a clock cycle takes a very small\nfraction of a second.\nIndeed, a 3.1 GHz Intel Core i7 completes 3.1 billion clock cycles in one\nsecond. Said another way, one clock cycle takes 0.322 billionths of a second\n(0.322 nanoseconds). Recall that the light bulb that is helping you read this text\nis switching on and o at the rate of 60 times a second. Thus, in the time it takes\na light bulb to switch on and o once, todays computers can complete more than\n51 million clock cycles!\n4.3.2.2 DECODE\nThe DECODE phase examines the instruction in order to gure out what\nthe microarchitecture is being asked to do. Recall the decoders we studied in\nChapter 3. In the LC-3, a 4-to-16 decoder identies which of the 16 opcodes is to\nbe processed (even though one of the 16 is not used!). Input is the four-bit opcode\nIR [15:12]. The output line asserted is the one corresponding to the opcode at\nthe input. Depending on which output of the decoder is asserted, the remaining\n12 bits identify what else is needed to process that instruction.\n4.3.2.3 EVALUATE ADDRESS\nThis phase computes the address of the memory location that is needed to pro-\ncess the instruction. Recall the example of the LD instruction: The LD instruction\ncauses a value stored in memory to be loaded into a register. In that exam-\nple, the address was obtained by sign-extending bits [8:0] of the instruction to\n16 bits and adding that value to the current contents of the PC. This calculation\nwas performed during the EVALUATE ADDRESS phase. It is worth noting that\nnot all instructions access memory to load or store data. For example, we have\nalready seen that the ADD and AND instructions in the LC-3 obtain their source\noperands from registers or from the instruction itself and store the result of the\nADD or AND instruction in a register. For those instructions, the EVALUATE\nADDRESS phase is not needed.\n4.3.2.4 FETCH OPERANDS\nThis phase obtains the source operands needed to process the instruction. In the\nLD example, this phase took two steps: loading MAR with the address calculated\nin the EVALUATE ADDRESS phase and reading memory that resulted in the\nsource operand being placed in MDR.'}"
"In the ADD example, this phase consisted of obtaining the source operands
from R2 and R6. In most current microprocessors, this phase (for the ADD
instruction) can be done at the same time the instruction is being executed (the
fth phase of the instruction cycle). Exactly how we can speed up the processing
of an instruction in this way is a fascinating subject, but it is one we are forced to
leave for later in your education.
4.3.2.5 EXECUTE
This phase carries out the execution of the instruction. In the ADD example, this
phase consisted of the step of performing the addition in the ALU.
4.3.2.6 STORE RESULT
The nal phase of an instructions execution. The result is written to its designated
destination. In the case of the ADD instruction, in many computers this action is
performed during the EXECUTE phase. That is, in many computers, including
the LC-3, an ADD instruction can fetch its source operands, perform the ADD in
the ALU, and store the result in the destination register all in a single clock cycle.
A separate STORE RESULT phase is not needed.
Once the instruction cycle has been completed, the control unit begins the
instruction cycle for the next instruction, starting from the top with the FETCH
phase. Since the PC was updated during the previous instruction cycle, it contains
at this point the address of the instruction stored in the next sequential memory
location. Thus, the next sequential instruction is fetched next. Processing con-
tinues in this way until something breaks this sequential ow, or the program
nishes execution.
It is worth noting again that although the instruction cycle consists of six
phases, not all instructions require all six phases. As already pointed out, the LC-
3 ADD instruction does not require a separate EVALUATE ADDRESS phase or
a separate STORE RESULT phase. The LC-3 LD instruction does not require an
EXECUTE phase. On the other hand, there are instructions in other ISAs that
require all six phases.
Example 4.4
ADD [eax], edx
This is an example of an Intel x86 instruction that requires
all six phases of the instruction cycle. All instructions require the rst two phases,
FETCH and DECODE. This instruction uses the eax register to calculate the address
of a memory location (EVALUATE ADDRESS). The contents of that memory
location is then read (FETCH OPERAND), added to the contents of the edx reg-
ister (EXECUTE), and the result written into the memory location that originally
contained the rst source operand (STORE RESULT).
4.3.3 Changing the Sequence of Execution
Everything we have said thus far happens when a computer program is executed
in sequence. That is, the rst instruction is executed, then the second instruction
is executed, followed by the third instruction, and so on.","{'page_number': 97, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In the ADD example, this phase consisted of obtaining the source operands\nfrom R2 and R6. In most current microprocessors, this phase (for the ADD\ninstruction) can be done at the same time the instruction is being executed (the\nfth phase of the instruction cycle). Exactly how we can speed up the processing\nof an instruction in this way is a fascinating subject, but it is one we are forced to\nleave for later in your education.\n4.3.2.5 EXECUTE\nThis phase carries out the execution of the instruction. In the ADD example, this\nphase consisted of the step of performing the addition in the ALU.\n4.3.2.6 STORE RESULT\nThe nal phase of an instructions execution. The result is written to its designated\ndestination. In the case of the ADD instruction, in many computers this action is\nperformed during the EXECUTE phase. That is, in many computers, including\nthe LC-3, an ADD instruction can fetch its source operands, perform the ADD in\nthe ALU, and store the result in the destination register all in a single clock cycle.\nA separate STORE RESULT phase is not needed.\nOnce the instruction cycle has been completed, the control unit begins the\ninstruction cycle for the next instruction, starting from the top with the FETCH\nphase. Since the PC was updated during the previous instruction cycle, it contains\nat this point the address of the instruction stored in the next sequential memory\nlocation. Thus, the next sequential instruction is fetched next. Processing con-\ntinues in this way until something breaks this sequential ow, or the program\nnishes execution.\nIt is worth noting again that although the instruction cycle consists of six\nphases, not all instructions require all six phases. As already pointed out, the LC-\n3 ADD instruction does not require a separate EVALUATE ADDRESS phase or\na separate STORE RESULT phase. The LC-3 LD instruction does not require an\nEXECUTE phase. On the other hand, there are instructions in other ISAs that\nrequire all six phases.\nExample 4.4\nADD [eax], edx\nThis is an example of an Intel x86 instruction that requires\nall six phases of the instruction cycle. All instructions require the rst two phases,\nFETCH and DECODE. This instruction uses the eax register to calculate the address\nof a memory location (EVALUATE ADDRESS). The contents of that memory\nlocation is then read (FETCH OPERAND), added to the contents of the edx reg-\nister (EXECUTE), and the result written into the memory location that originally\ncontained the rst source operand (STORE RESULT).\n4.3.3 Changing the Sequence of Execution\nEverything we have said thus far happens when a computer program is executed\nin sequence. That is, the rst instruction is executed, then the second instruction\nis executed, followed by the third instruction, and so on.'}"
"We have identied two types of instructions, the ADD and AND, which are
examples of operate instructions in that they operate on data, and the LD, which
is an example of a data movement instruction in that it moves data from one
place to another. There are other examples of both operate instructions and data
movement instructions, as we will discover in Chapter 5 when we study the LC-3
in greater detail.
There is a third type of instruction, the control instruction, whose purpose is
to change the sequence of instruction execution. For example, there are times, as
we shall see very soon, when it is desirable to rst execute the rst instruction,
then the second, then the third, then the rst again, the second again, then the third
again, then the rst for the third time, the second for the third time, and so on. As
we know, each instruction cycle starts with loading the MAR with the PC. Thus,
if we wish to change the sequence of instructions executed, we must change the
contents of the PC between the time it is incremented (during the FETCH phase
of one instruction) and the start of the FETCH phase of the next instruction.
Control instructions perform that function by loading the PC during the
EXECUTE phase, which wipes out the incremented PC that was loaded during
the FETCH phase. The result is that, at the start of the next instruction cycle,
when the computer accesses the PC to obtain the address of an instruction to
fetch, it will get the address loaded during the previous instructions EXECUTE
phase, rather than the next sequential instruction in the computers program.
The most common control instruction is the conditional branch (BR), which
either changes the contents of the PC or does not change the contents of the PC,
depending on the result of a previous instruction (usually the instruction that is
executed immediately before the conditional branch instruction).
Example 4.5
The BR Instruction
The BR instruction consists of three parts, the opcode (bits
[15:12] = 0000), the condition to be tested (bits [11:9]), and the addressing mode bits
(bits [8:0]) that are used to form the address to be loaded into the PC if the result of
the previous instruction agrees with the test specied by bits [11:9]. The addressing
mode, i.e., the mechanism used to determine the actual address, is the same one we
used in the LD instruction. Bits [8:0] are sign-extended to 16 bits and then added to
the current contents of the PC.
Suppose the BR instruction shown below is located in memory location x36C9.
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
0
0
0
0
1
0
1
1
1
1
1
1
1
0
1
0
BR
condition
6
The opcode 0000 identies the instruction as a conditional branch. Bits [11:9] = 101
species that the test to be performed on the most recent result is whether or not that
result is something other than 0","{'page_number': 98, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Bits [11:9] = 101\nspecies that the test to be performed on the most recent result is whether or not that\nresult is something other than 0. In Chapter 5 we will describe in detail all the tests\nthat can be performed on the most recent result. For now, we will just use one test:\nIs the result not zero? Bits [8:0] is the value 6.\nAssume the previous instruction executed (in memory location x36C8) was an\nADD instruction and the result of the ADD was 0. Since the test not-zero failed,\nthe BR instruction would do nothing during its EXECUTE phase, and so the next\n(continued on next page)'}"
"Bits [11:9] = 101
species that the test to be performed on the most recent result is whether or not that
result is something other than 0. In Chapter 5 we will describe in detail all the tests
that can be performed on the most recent result. For now, we will just use one test:
Is the result not zero? Bits [8:0] is the value 6.
Assume the previous instruction executed (in memory location x36C8) was an
ADD instruction and the result of the ADD was 0. Since the test not-zero failed,
the BR instruction would do nothing during its EXECUTE phase, and so the next
(continued on next page)","{'page_number': 98, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Bits [11:9] = 101\nspecies that the test to be performed on the most recent result is whether or not that\nresult is something other than 0. In Chapter 5 we will describe in detail all the tests\nthat can be performed on the most recent result. For now, we will just use one test:\nIs the result not zero? Bits [8:0] is the value 6.\nAssume the previous instruction executed (in memory location x36C8) was an\nADD instruction and the result of the ADD was 0. Since the test not-zero failed,\nthe BR instruction would do nothing during its EXECUTE phase, and so the next\n(continued on next page)'}"
"instruction executed would be the instruction at M[x36CA], the address formed by
incrementing the PC during the FETCH phase of the BR instructions instruction
cycle.
On the other hand, if the result of the ADD instruction is not 0, then the test
succeeds, causing the BR instruction to load PC with x36C4, the address formed by
sign-extending bits [8:0] to 16 bits and adding that value (-6) to the incremented PC
(x36CA).
Thus, the next instruction executed after the BR instruction at x36C9 is either
the instruction at x36CA or the one at x36C4, depending on whether the result of the
ADD instruction was zero or not zero.
4.3.4 Control of the Instruction Cycle
The instruction cycle is controlled by a synchronous nite state machine. An
abbreviated version of its state diagram, highlighting a few of the LC-3 instruc-
tions discussed in this chapter, is shown in Figure 4.4. As is the case with the
nite state machines studied in Section 3.6, each state corresponds to one machine
cycle of activity that takes one clock cycle to perform. The processing controlled
by each state is described within the node representing that state. The arcs show
the next state transitions.
Processing starts with State 1. The FETCH phase takes three clock cycles,
corresponding to the three steps described earlier. In the rst clock cycle, the
MAR is loaded with the contents of the PC, and the PC is incremented. In order
for the contents of the PC to be loaded into the MAR (see Figure 4.3), the nite
state machine must assert GatePC and LD.MAR. GatePC connects the PC to the
processor bus. LD.MAR, the write enable signal of the MAR register, loads the
contents of the bus into the MAR at the end of the current clock cycle. (Registers
are loaded at the end of the clock cycle if the corresponding control signal is
asserted.)
In order for the PC to be incremented (again, see Figure 4.3), the nite
state machine must assert the PCMUX select lines to choose the output of the
box labeled +1 and must also assert the LD.PC signal to load the output of the
PCMUX into the PC at the end of the current cycle.
The nite state machine then goes to State 2. Here, the MDR is loaded with
the instruction, which is read from memory.
In State 3, the instruction is transferred from the MDR to the instruction
register (IR). This requires the nite state machine to assert GateMDR and LD.IR,
which causes the IR to be loaded at the end of the clock cycle, concluding the
FETCH phase of the instruction cycle.
The DECODE phase takes one clock cycle. In State 4, using the external
input IR, and in particular the opcode bits of the instruction, the nite state
machine can go to the appropriate next state for processing instructions depend-
ing on the particular opcode in IR [15:12]. Three of the 15 paths out of State 4
are shown","{'page_number': 99, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The DECODE phase takes one clock cycle. In State 4, using the external\ninput IR, and in particular the opcode bits of the instruction, the nite state\nmachine can go to the appropriate next state for processing instructions depend-\ning on the particular opcode in IR [15:12]. Three of the 15 paths out of State 4\nare shown. Processing continues clock cycle by clock cycle until the instruction\ncompletes execution, and the next state logic returns the nite state machine to\nState 1.'}"
"The DECODE phase takes one clock cycle. In State 4, using the external
input IR, and in particular the opcode bits of the instruction, the nite state
machine can go to the appropriate next state for processing instructions depend-
ing on the particular opcode in IR [15:12]. Three of the 15 paths out of State 4
are shown. Processing continues clock cycle by clock cycle until the instruction
completes execution, and the next state logic returns the nite state machine to
State 1.","{'page_number': 99, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The DECODE phase takes one clock cycle. In State 4, using the external\ninput IR, and in particular the opcode bits of the instruction, the nite state\nmachine can go to the appropriate next state for processing instructions depend-\ning on the particular opcode in IR [15:12]. Three of the 15 paths out of State 4\nare shown. Processing continues clock cycle by clock cycle until the instruction\ncompletes execution, and the next state logic returns the nite state machine to\nState 1.'}"
"As has already been discussed, it is sometimes necessary not to execute
the next sequential instruction but rather to access another location to nd the
next instruction to execute. As we have said, instructions that change the ow of
instruction processing in this way are called control instructions. In the case of
the conditional branch instruction (BR), at the end of its instruction cycle, the
PC contains one of two addresses: either the incremented PC that was loaded in
State 1 or the new address computed from sign-extending bits [8:0] of the BR
instruction and adding it to the PC, which was loaded in State 63. Which address
gets loaded into the PC depends on the test of the most recent result.
A
endix C contains a full descri tion of the im lementation of the
L
of
m
o a e
o a e
o a e
Figure 4.4
An abbreviated state diagram of the LC-3.","{'page_number': 100, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'As has already been discussed, it is sometimes necessary not to execute\nthe next sequential instruction but rather to access another location to nd the\nnext instruction to execute. As we have said, instructions that change the ow of\ninstruction processing in this way are called control instructions. In the case of\nthe conditional branch instruction (BR), at the end of its instruction cycle, the\nPC contains one of two addresses: either the incremented PC that was loaded in\nState 1 or the new address computed from sign-extending bits [8:0] of the BR\ninstruction and adding it to the PC, which was loaded in State 63. Which address\ngets loaded into the PC depends on the test of the most recent result.\nA\nendix C contains a full descri tion of the im lementation of the\nL\nof\nm\no a e\no a e\no a e\nFigure 4.4\nAn abbreviated state diagram of the LC-3.'}"
"state diagram would be able to control, clock cycle by clock cycle, all the steps
required to execute all the phases of every instruction cycle. Since each instruc-
tion cycle ends by returning to State 1, the nite state machine can process, clock
cycle by clock cycle, a complete computer program.
4.3.5 Halting the Computer (the TRAP Instruction)
From everything we have said, it appears that the computer will continue
processing instructions, carrying out the instruction cycle again and again,
ad nauseum. Since the computer does not have the capacity to be bored, must this
continue until someone pulls the plug and disconnects power to the computer?
Usually, user programs execute under the control of an operating system.
Linux, DOS, MacOS, and Windows are all examples of operating systems.
Operating systems are just computer programs themselves. As far as the com-
puter is concerned, the instruction cycle continues whether a user program is
being processed or the operating system is being processed. This is ne as
far as user programs are concerned since each user program terminates with a
control instruction that changes the PC to again start processing the operating
systemoften to initiate the execution of another user program.
But what if we actually want to stop this potentially innite sequence of
instruction cycles? Recall our analogy to the conductors baton, beating at the
rate of billions of clock cycles per second. Stopping the instruction sequencing
requires stopping the conductors baton. We have pointed out many times that
there is inside the computer a component that corresponds very closely to the
conductors baton. It is called the clock, and it denes the amount of time each
machine cycle takes. We saw in Chapter 3 that the clock enables the synchronous
nite state machine to continue on to the next clock cycle. In Chapter 3 the next
clock cycle corresponded to the next state of the danger sign we designed. Here
the next clock cycle corresponds to the next state of the instruction cycle, which
is either the next state of the current phase of the instruction cycle or the rst state
of the next phase of the instruction cycle. Stopping the instruction cycle requires
stopping the clock.
Figure 4.5a shows a block diagram of the clock circuit, consisting primarily
of a clock generator and a RUN latch. The clock generator is a crystal oscillator,
a piezoelectric device that you may have studied in your physics or chemistry
class. F
tion of
Figure
clock c
Cr
o
Cl
gen
Figure 4.5
The clock circuit and its control.","{'page_number': 101, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'state diagram would be able to control, clock cycle by clock cycle, all the steps\nrequired to execute all the phases of every instruction cycle. Since each instruc-\ntion cycle ends by returning to State 1, the nite state machine can process, clock\ncycle by clock cycle, a complete computer program.\n4.3.5 Halting the Computer (the TRAP Instruction)\nFrom everything we have said, it appears that the computer will continue\nprocessing instructions, carrying out the instruction cycle again and again,\nad nauseum. Since the computer does not have the capacity to be bored, must this\ncontinue until someone pulls the plug and disconnects power to the computer?\nUsually, user programs execute under the control of an operating system.\nLinux, DOS, MacOS, and Windows are all examples of operating systems.\nOperating systems are just computer programs themselves. As far as the com-\nputer is concerned, the instruction cycle continues whether a user program is\nbeing processed or the operating system is being processed. This is ne as\nfar as user programs are concerned since each user program terminates with a\ncontrol instruction that changes the PC to again start processing the operating\nsystemoften to initiate the execution of another user program.\nBut what if we actually want to stop this potentially innite sequence of\ninstruction cycles? Recall our analogy to the conductors baton, beating at the\nrate of billions of clock cycles per second. Stopping the instruction sequencing\nrequires stopping the conductors baton. We have pointed out many times that\nthere is inside the computer a component that corresponds very closely to the\nconductors baton. It is called the clock, and it denes the amount of time each\nmachine cycle takes. We saw in Chapter 3 that the clock enables the synchronous\nnite state machine to continue on to the next clock cycle. In Chapter 3 the next\nclock cycle corresponded to the next state of the danger sign we designed. Here\nthe next clock cycle corresponds to the next state of the instruction cycle, which\nis either the next state of the current phase of the instruction cycle or the rst state\nof the next phase of the instruction cycle. Stopping the instruction cycle requires\nstopping the clock.\nFigure 4.5a shows a block diagram of the clock circuit, consisting primarily\nof a clock generator and a RUN latch. The clock generator is a crystal oscillator,\na piezoelectric device that you may have studied in your physics or chemistry\nclass. F\ntion of\nFigure\nclock c\nCr\no\nCl\ngen\nFigure 4.5\nThe clock circuit and its control.'}"
"If the RUN latch is in the 1 state (i.e., Q = 1), the output of the clock circuit
is the same as the output of the clock generator. If the RUN latch is in the 0 state
(i.e., Q = 0), the output of the clock circuit is 0.
Thus, stopping the instruction cycle requires only clearing the RUN latch.
Every computer has some mechanism for doing that. In some older machines, it
is done by executing a HALT instruction. In the LC-3, as in many other machines,
it is done under control of the operating system, as we will see in Chapter 9. For
now it is enough to know that if a user program requires help from the operating
system, it requests that help with the TRAP instruction (opcode = 1111) and an
eight-bit code called a trap vector, which identies the help that the user program
needs. The eight-bit code x25 tells the operating system that the program has
nished executing and the computer can stop processing instructions.
Question: If a HALT instruction can clear the RUN latch, thereby stopping
the instruction cycle, what instruction is needed to set the RUN latch, thereby
reinitiating the instruction cycle? Hint: This is a trick question!
4.4 Our First Program:
A Multiplication Algorithm
We now have all that we need to write our rst program. We have a data movement
instruction LD to load data from memory into a register, and we have two operate
instructions, ADD for performing arithmetic and AND for performing a bit-wise
logical operation. We have a control instruction BR for loading the PC with an
address dierent from the incremented PC so the instruction to be executed next
will NOT be the instruction in the next sequential location in memory. And we
have the TRAP instruction (a.k.a. system call) that allows us to ask the operating
system for help, in this case to stop the computer. With all that under our belt, we
can write our rst program.
Suppose the computer does not know how to multiply two positive integers.
In the old days, that was true for a lot of computers! They had ADD instructions,
but they did not have multiply instructions. What to do? Suppose we wanted to
multiply 5 times 4. Even if we do not know how to multiply, if we know that 5
times 4 is 5+5+5+5, and the computer has an ADD instruction, we can write a
program that can multiply. All we have to do is add 5 to itself four times.
Figure 4.6 illustrates the process.
Let us assume that memory location x3007, abbreviated M[x3007], contains
the value 5, and M[x3008] contains the value 4. We start by copying the two
values from memory to the two registers R1 and R2. We are going to accumulate
the results of the additions in R3, so we initialize R3 to 0. Then we add 5 to R3,
and subtract 1 from R2 so we will know how many more times we will need to
add 5 to R3. We keep doing this (adding 5 to R3 and subtracting 1 from R2) until
R2 contains the value 0","{'page_number': 102, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'We start by copying the two\nvalues from memory to the two registers R1 and R2. We are going to accumulate\nthe results of the additions in R3, so we initialize R3 to 0. Then we add 5 to R3,\nand subtract 1 from R2 so we will know how many more times we will need to\nadd 5 to R3. We keep doing this (adding 5 to R3 and subtracting 1 from R2) until\nR2 contains the value 0. That tells us that we have added 5 to R3 four times and\nwe are done, so we HALT the computer. R3 contains the value 20, the result of\nour multiplication.\nFigure 4.7 shows the actual LC-3 program, stored in memory locations x3000\nto x3008.'}"
"We start by copying the two
values from memory to the two registers R1 and R2. We are going to accumulate
the results of the additions in R3, so we initialize R3 to 0. Then we add 5 to R3,
and subtract 1 from R2 so we will know how many more times we will need to
add 5 to R3. We keep doing this (adding 5 to R3 and subtracting 1 from R2) until
R2 contains the value 0. That tells us that we have added 5 to R3 four times and
we are done, so we HALT the computer. R3 contains the value 20, the result of
our multiplication.
Figure 4.7 shows the actual LC-3 program, stored in memory locations x3000
to x3008.","{'page_number': 102, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'We start by copying the two\nvalues from memory to the two registers R1 and R2. We are going to accumulate\nthe results of the additions in R3, so we initialize R3 to 0. Then we add 5 to R3,\nand subtract 1 from R2 so we will know how many more times we will need to\nadd 5 to R3. We keep doing this (adding 5 to R3 and subtracting 1 from R2) until\nR2 contains the value 0. That tells us that we have added 5 to R3 four times and\nwe are done, so we HALT the computer. R3 contains the value 20, the result of\nour multiplication.\nFigure 4.7 shows the actual LC-3 program, stored in memory locations x3000\nto x3008.'}"
"138
Stop
Figure 4.6
Flowchart for an algorithm that multiplies two positive integers.
The program counter, which keeps track of the next instruction to be
executed, initially contains the address x3000.
To move the data from memory locations M[x3007] and M[x3008] to R1 and
R2, we use the data movement instruction LD. The LC-3 computer executes the
LD instruction in M[x3000] by sign-extending the oset (in this case 6) to 16 bits,
adding it to the incremented PC (in this case x3001 since we incremented the PC
F gure
.
A program that multiplies without a multiply instruction.","{'page_number': 103, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '138\nStop\nFigure 4.6\nFlowchart for an algorithm that multiplies two positive integers.\nThe program counter, which keeps track of the next instruction to be\nexecuted, initially contains the address x3000.\nTo move the data from memory locations M[x3007] and M[x3008] to R1 and\nR2, we use the data movement instruction LD. The LC-3 computer executes the\nLD instruction in M[x3000] by sign-extending the oset (in this case 6) to 16 bits,\nadding it to the incremented PC (in this case x3001 since we incremented the PC\nF gure\n.\nA program that multiplies without a multiply instruction.'}"
"LC-3
I
n Chapter 4, we discussed the basic components of a computerits mem-
ory, its processing unit, including the associated temporary storage (usually
a set of registers), input and output devices, and the control unit that directs the
activity of all the units (including itself!). We also studied the six phases of the
instruction cycleFETCH, DECODE, ADDRESS EVALUATION, OPERAND
FETCH, EXECUTE, and STORE RESULT. We used elements of the LC-3 to
illustrate some of the concepts. In fact, we introduced ve opcodes: two operate
instructions (ADD and AND), one data movement instruction (LD), and two con-
trol instructions (BR and TRAP). We are now ready to study the LC-3 in much
greater detail.
Recall from Chapter 1 that the ISA is the interface between what the soft-
ware commands and what the hardware actually carries out. In this chapter, we
will point out most of the important features of the ISA of the LC-3. (A few ele-
ments we will leave for Chapter 8 and Chapter 9.) You will need these features
to write programs in the LC-3s own language, that is, in the LC-3s machine
language.
A complete description of the ISA of the LC-3 is contained in Appendix A.
5.1 The ISA: Overview
The ISA species all the information about the computer that the software has
to be aware of. In other words, the ISA species everything in the computer
that is available to a programmer when he/she writes programs in the com-
puters own machine language. Most people, however, do not write programs
in the computers own machine language, but rather opt for writing programs in
a high-level language like C++ or Python (or Fortran or COBOL, which have
been around for more than 50 years). Thus, the ISA also species everything
in the computer that is needed by someone (a compiler writer) who wishes to
translate programs written in a high-level language into the machine language of
the computer.","{'page_number': 104, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'LC-3\nI\nn Chapter 4, we discussed the basic components of a computerits mem-\nory, its processing unit, including the associated temporary storage (usually\na set of registers), input and output devices, and the control unit that directs the\nactivity of all the units (including itself!). We also studied the six phases of the\ninstruction cycleFETCH, DECODE, ADDRESS EVALUATION, OPERAND\nFETCH, EXECUTE, and STORE RESULT. We used elements of the LC-3 to\nillustrate some of the concepts. In fact, we introduced ve opcodes: two operate\ninstructions (ADD and AND), one data movement instruction (LD), and two con-\ntrol instructions (BR and TRAP). We are now ready to study the LC-3 in much\ngreater detail.\nRecall from Chapter 1 that the ISA is the interface between what the soft-\nware commands and what the hardware actually carries out. In this chapter, we\nwill point out most of the important features of the ISA of the LC-3. (A few ele-\nments we will leave for Chapter 8 and Chapter 9.) You will need these features\nto write programs in the LC-3s own language, that is, in the LC-3s machine\nlanguage.\nA complete description of the ISA of the LC-3 is contained in Appendix A.\n5.1 The ISA: Overview\nThe ISA species all the information about the computer that the software has\nto be aware of. In other words, the ISA species everything in the computer\nthat is available to a programmer when he/she writes programs in the com-\nputers own machine language. Most people, however, do not write programs\nin the computers own machine language, but rather opt for writing programs in\na high-level language like C++ or Python (or Fortran or COBOL, which have\nbeen around for more than 50 years). Thus, the ISA also species everything\nin the computer that is needed by someone (a compiler writer) who wishes to\ntranslate programs written in a high-level language into the machine language of\nthe computer.'}"
"The ISA species the memory organization, register set, and instruction set,
including the opcodes, data types, and addressing modes of the instructions in
the instruction set.
5.1.1 Memory Organization
The LC-3 memory has an address space of 216 (i.e., 65,536) locations, and an
addressability of 16 bits. Not all 65,536 addresses are actually used for memory
locations, but we will leave that discussion for Chapter 9. Since the normal unit
of data that is processed in the LC-3 is 16 bits, we refer to 16 bits as one word,
and we say the LC-3 is word-addressable.
5.1.2 Registers
Since it usually takes far more than one clock cycle to obtain data from mem-
ory, the LC-3 provides (like almost all computers) additional temporary storage
locations that can be accessed in a single clock cycle.
The most common type of temporary storage locations, and the one used in
the LC-3, is a set of registers. Each register in the set is called a general purpose
register (GPR). Like memory locations, registers store information that can be
operated on later. The number of bits stored in each register is usually one word.
In the LC-3, this means 16 bits.
Registers must be uniquely identiable. The LC-3 species eight GPRs, each
identied by a three-bit register number. They are referred to as R0, R1,  R7.
Figure 5.1 shows a snapshot of the LC-3s register set, sometimes called a register
le, with the eight values 1, 3, 5, 7, 2, 4, 6, and 8 stored in R0,  R7,
respectively.
Figure 5.1
A snapshot of the LC-3s register le.","{'page_number': 105, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The ISA species the memory organization, register set, and instruction set,\nincluding the opcodes, data types, and addressing modes of the instructions in\nthe instruction set.\n5.1.1 Memory Organization\nThe LC-3 memory has an address space of 216 (i.e., 65,536) locations, and an\naddressability of 16 bits. Not all 65,536 addresses are actually used for memory\nlocations, but we will leave that discussion for Chapter 9. Since the normal unit\nof data that is processed in the LC-3 is 16 bits, we refer to 16 bits as one word,\nand we say the LC-3 is word-addressable.\n5.1.2 Registers\nSince it usually takes far more than one clock cycle to obtain data from mem-\nory, the LC-3 provides (like almost all computers) additional temporary storage\nlocations that can be accessed in a single clock cycle.\nThe most common type of temporary storage locations, and the one used in\nthe LC-3, is a set of registers. Each register in the set is called a general purpose\nregister (GPR). Like memory locations, registers store information that can be\noperated on later. The number of bits stored in each register is usually one word.\nIn the LC-3, this means 16 bits.\nRegisters must be uniquely identiable. The LC-3 species eight GPRs, each\nidentied by a three-bit register number. They are referred to as R0, R1,  R7.\nFigure 5.1 shows a snapshot of the LC-3s register set, sometimes called a register\nle, with the eight values 1, 3, 5, 7, 2, 4, 6, and 8 stored in R0,  R7,\nrespectively.\nFigure 5.1\nA snapshot of the LC-3s register le.'}"
"Fi
0
0
0
1
0
1
0
0
0
0
0
0
0
0
0
1
ADD
R2
R0
R1
where the two sources of the ADD instruction are specied in bits [8:6] and
bits [2:0]. The destination of the ADD result is specied in bits [11:9]. Figure 5.2
shows the contents of the register le of Figure 5.1 AFTER the instruction
ADD R2, R1, R0.
is executed.
5.1.3 The Instruction Set
Recall from Chapter 4 that an instruction is made up of two things, its opcode
(what the instruction is asking the computer to do) and its operands (who the
computer is expected to do it to!). The instruction set is dened by its set of
opcodes, data types, and addressing modes. The addressing modes determine
where the operands are located. The data type is the representation of the operands
in 0s and 1s.
The instruction ADD R2, R0, R1 has an opcode ADD, one addressing mode
(register mode), and one data type (2s complement integer). The instruction
directs the computer to perform a 2s complement integer addition and speci-
es the locations (GPRs) where the computer is expected to nd the operands
and the location (a GPR) where the computer is to write the result.
We saw in Chapter 4 that the ADD instruction can also have two addressing
modes (register mode and immediate mode), where one of the two operands is
literally contained in bits [4:0] of the instruction.
Figure 5.3 lists all the instructions of the LC-3, the bit encoding [15:12] for
each opcode, and the format of each instruction. Some of them you will recognize
from Chapter 4. Many others will be explained in Sections 5.2, 5.3, and 5.4.","{'page_number': 106, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Fi\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\nADD\nR2\nR0\nR1\nwhere the two sources of the ADD instruction are specied in bits [8:6] and\nbits [2:0]. The destination of the ADD result is specied in bits [11:9]. Figure 5.2\nshows the contents of the register le of Figure 5.1 AFTER the instruction\nADD R2, R1, R0.\nis executed.\n5.1.3 The Instruction Set\nRecall from Chapter 4 that an instruction is made up of two things, its opcode\n(what the instruction is asking the computer to do) and its operands (who the\ncomputer is expected to do it to!). The instruction set is dened by its set of\nopcodes, data types, and addressing modes. The addressing modes determine\nwhere the operands are located. The data type is the representation of the operands\nin 0s and 1s.\nThe instruction ADD R2, R0, R1 has an opcode ADD, one addressing mode\n(register mode), and one data type (2s complement integer). The instruction\ndirects the computer to perform a 2s complement integer addition and speci-\nes the locations (GPRs) where the computer is expected to nd the operands\nand the location (a GPR) where the computer is to write the result.\nWe saw in Chapter 4 that the ADD instruction can also have two addressing\nmodes (register mode and immediate mode), where one of the two operands is\nliterally contained in bits [4:0] of the instruction.\nFigure 5.3 lists all the instructions of the LC-3, the bit encoding [15:12] for\neach opcode, and the format of each instruction. Some of them you will recognize\nfrom Chapter 4. Many others will be explained in Sections 5.2, 5.3, and 5.4.'}"
"5.1.4 Opcodes
Some ISAs have a very large number of opcodes, one for each of a very large
number of tasks that a program may wish to carry out. The x86 ISA has more than
200 opcodes. Other ISAs have a very small set of opcodes. Some ISAs have specific
opcodes to help with processing scientific calculations. For example, the Hewlett
Packard Precision Architecture can specify the compound operation (A  B) + C
with one opcode; that is, a multiply, followed by an add on three source operands
A, B, and C. Other ISAs have instructions that process video images obtained from
the World Wide Web. The Intel x86 ISA added a number of instructions which they
originally called MMX instructions because they eXtended the ISA to assist with
MultiMedia applications that use the web. Still other ISAs have specific opcodes to
help with handling the tasks of the operating system. For example, the VAX ISA,
popular in the 1980s, used a single opcode instead of a long sequence of instructions
that other computers used to save the information associated with a program that
was in the middle of executing prior to switching to another program. The decision
as to which instructions to include or leave out of an ISA is usually a hotly debated
topic in a company when a new ISA is being specified.
The LC-3 ISA has 15 instructions, each identied by its unique opcode. The
opcode is specied in bits [15:12] of the instruction. Since four bits are used
to specify the opcode, 16 distinct opcodes are possible. However, the LC-3 ISA
species only 15 opcodes. The code 1101 has been left unspecied, reserved for
some future need that we are not able to anticipate today.
As we already discussed briey in Chapter 4, there are three dierent types
of instructions, which means three dierent types of opcodes: operates, data
movement, and control. Operate instructions process information. Data move-
ment instructions move information between memory and the registers and
between registers/memory and input/output devices. Control instructions change
the sequence of instructions that will be executed. That is, they enable the exe-
cution of an instruction other than the one that is stored in the next sequential
location in memory.
5.1.5 Data Types
As we rst pointed out in Section 2.1.2, a data type is a representation of infor-
mation such that the ISA has opcodes that operate on that representation. There
are many ways to represent the same information in a computer. That should not
surprise us, since in our daily lives, we regularly represent the same information
in many dierent ways. For example, a child, when asked how old he is, might
hold up three ngers, signifying that he is 3 years old. If the child is particularly
precocious, he might write the decimal digit 3 to indicate his age. Or, if the child
is a CS or CE major at the university, he might write 0000000000000011, the
16-bit binary representation for 3. If he is a chemistry major, he might write
3.0  100","{'page_number': 107, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'For example, a child, when asked how old he is, might\nhold up three ngers, signifying that he is 3 years old. If the child is particularly\nprecocious, he might write the decimal digit 3 to indicate his age. Or, if the child\nis a CS or CE major at the university, he might write 0000000000000011, the\n16-bit binary representation for 3. If he is a chemistry major, he might write\n3.0  100. All four represent the same value: 3.\nIn addition to the representation of a single number by dierent bit patterns\nin dierent data types, it is also the case that the same bit pattern can corre-\nspond to dierent numbers, depending on the data type. For example, the 16'}"
"For example, a child, when asked how old he is, might
hold up three ngers, signifying that he is 3 years old. If the child is particularly
precocious, he might write the decimal digit 3 to indicate his age. Or, if the child
is a CS or CE major at the university, he might write 0000000000000011, the
16-bit binary representation for 3. If he is a chemistry major, he might write
3.0  100. All four represent the same value: 3.
In addition to the representation of a single number by dierent bit patterns
in dierent data types, it is also the case that the same bit pattern can corre-
spond to dierent numbers, depending on the data type. For example, the 16","{'page_number': 107, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'For example, a child, when asked how old he is, might\nhold up three ngers, signifying that he is 3 years old. If the child is particularly\nprecocious, he might write the decimal digit 3 to indicate his age. Or, if the child\nis a CS or CE major at the university, he might write 0000000000000011, the\n16-bit binary representation for 3. If he is a chemistry major, he might write\n3.0  100. All four represent the same value: 3.\nIn addition to the representation of a single number by dierent bit patterns\nin dierent data types, it is also the case that the same bit pattern can corre-\nspond to dierent numbers, depending on the data type. For example, the 16'}"
"bits 0011000100110000 represent the 2s complement integer 12,592, the ASCII
code for 10, and a bit vector such that b13, b12, b7, b4, and b3 have the relevant
property of the bit vector.
That should also not surprise us, since in our daily lives, the same represen-
tation can correspond to multiple interpretations, as is the case with a red light.
When you see it on the roadway while you are driving, it means you should stop.
When you see it at Centre Bell where the Montreal Canadiens play hockey, it
means someone has just scored a goal.
Every opcode will interpret the bit patterns of its operands according to the
data type it is designed to support. In the case of the ADD opcode, for example,
the hardware will interpret the bit patterns of its operands as 2s complement
integers. Therefore, if a programmer stored the bit pattern 0011000100110000 in
R3, thinking that the bit pattern represented the integer 10, the instruction ADD
R4, R3, #10 would write the integer 12,602 into R4, and not the ASCII code for
the integer 20. Why? Because the opcode ADD interprets the bit patterns of its
operands as 2s complement integers, and not ASCII codes, regardless what the
person creating those numbers intended.
5.1.6 Addressing Modes
An addressing mode is a mechanism for specifying where the operand is located.
An operand can generally be found in one of three places: in memory, in a register,
or as a part of the instruction. If the operand is a part of the instruction, we refer to
it as a literal or as an immediate operand. The term literal comes from the fact that
the bits of the instruction literally form the operand. The term immediate comes
from the fact that we can obtain the operand immediately from the instruction,
that is, we dont have to look elsewhere for it.
The LC-3 supports ve addressing modes: immediate (or literal), register,
and three memory addressing modes: PC-relative, indirect, and Base+oset. We
will see in Section 5.2 that operate instructions use two addressing modes: register
and immediate. We will see in Section 5.3 that data movement instructions use
four of the ve addressing modes.
5.1.7 Condition Codes
One nal item will complete our overview of the ISA of the LC-3: condition
codes. The LC-3 has three single-bit registers that are individually set (set to
1) or cleared (set to 0) each time one of the eight general purpose registers is
written into as a result of execution of one of the operate instructions or one of
the load instructions. Each operate instruction performs a computation and writes
the result into a general purpose register. Each load instruction reads the contents
of a memory location and writes the value found there into a general purpose
register. We will discuss all the operate instructions in Section 5.2 and all the
load instructions in Section 5.3.
The three single-bit registers are called N, Z, and P, corresponding to their
meaning: negative, zero, and positive","{'page_number': 108, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Each operate instruction performs a computation and writes\nthe result into a general purpose register. Each load instruction reads the contents\nof a memory location and writes the value found there into a general purpose\nregister. We will discuss all the operate instructions in Section 5.2 and all the\nload instructions in Section 5.3.\nThe three single-bit registers are called N, Z, and P, corresponding to their\nmeaning: negative, zero, and positive. Each time a GPR is written by an operate\nor a load instruction, the N, Z, and P one-bit registers are individually set to 0'}"
"Each operate instruction performs a computation and writes
the result into a general purpose register. Each load instruction reads the contents
of a memory location and writes the value found there into a general purpose
register. We will discuss all the operate instructions in Section 5.2 and all the
load instructions in Section 5.3.
The three single-bit registers are called N, Z, and P, corresponding to their
meaning: negative, zero, and positive. Each time a GPR is written by an operate
or a load instruction, the N, Z, and P one-bit registers are individually set to 0","{'page_number': 108, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Each operate instruction performs a computation and writes\nthe result into a general purpose register. Each load instruction reads the contents\nof a memory location and writes the value found there into a general purpose\nregister. We will discuss all the operate instructions in Section 5.2 and all the\nload instructions in Section 5.3.\nThe three single-bit registers are called N, Z, and P, corresponding to their\nmeaning: negative, zero, and positive. Each time a GPR is written by an operate\nor a load instruction, the N, Z, and P one-bit registers are individually set to 0'}"
"or 1, corresponding to whether the result written to the GPR is negative, zero,
or positive. That is, if the result is negative, the N register is set, and Z and P
are cleared. If the result is zero, Z is set and N and P are cleared. If the result is
positive, P is set and N and Z are cleared.
The set of three single-bit registers are referred to as condition codes because
the condition of those bits are used to change the sequence of execution of the
instructions in a computer program. Many ISAs use condition codes to change
the execution sequence. SPARC and x86 are two examples. We will show how
the LC-3 does it in Section 5.4.
5.2 Operate Instructions
5.2.1 ADD, AND, and NOT
Operate instructions process data. Arithmetic operations (like ADD, SUB, MUL,
and DIV) and logical operations (like AND, OR, NOT, XOR) are common
examples. The LC-3 has three operate instructions: ADD, AND, and NOT.
The NOT (opcode = 1001) instruction is the only operate instruction that
performs a unary operation, that is, the operation requires one source operand.
The
resul
its s
speci
I
instr
NOT
R3
R5
R3 will contain 1010111100001111.
Figure 5.4 shows the key parts of the data path that are used to perform the
NOT instruction shown here. Since NOT is a unary operation, only the A input
of the ALU is relevant. It is sourced from R5. The control signal to the ALU
directs the ALU to perform the bit-wise complement operation. The output of
the ALU (the result of the operation) is stored in R3 and the condition codes are
set, completing the execution of the NOT instruction.
Recall from Chapter 4 that the ADD (opcode = 0001) and AND (opcode =
0101) instructions both perform binary operations; they require two 16-bit source
operands. The ADD instruction performs a 2s complement addition of its two
source operands. The AND instruction performs a bit-wise AND of each pair
of bits of its two 16-bit operands. Like the NOT, the ADD and AND use the
register addressing mode for one of the source operands and for the destina-
tion operand. Bits [8:6] specify the source register, and bits [11:9] specify the
destination register (where the result will be written).","{'page_number': 109, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'or 1, corresponding to whether the result written to the GPR is negative, zero,\nor positive. That is, if the result is negative, the N register is set, and Z and P\nare cleared. If the result is zero, Z is set and N and P are cleared. If the result is\npositive, P is set and N and Z are cleared.\nThe set of three single-bit registers are referred to as condition codes because\nthe condition of those bits are used to change the sequence of execution of the\ninstructions in a computer program. Many ISAs use condition codes to change\nthe execution sequence. SPARC and x86 are two examples. We will show how\nthe LC-3 does it in Section 5.4.\n5.2 Operate Instructions\n5.2.1 ADD, AND, and NOT\nOperate instructions process data. Arithmetic operations (like ADD, SUB, MUL,\nand DIV) and logical operations (like AND, OR, NOT, XOR) are common\nexamples. The LC-3 has three operate instructions: ADD, AND, and NOT.\nThe NOT (opcode = 1001) instruction is the only operate instruction that\nperforms a unary operation, that is, the operation requires one source operand.\nThe\nresul\nits s\nspeci\nI\ninstr\nNOT\nR3\nR5\nR3 will contain 1010111100001111.\nFigure 5.4 shows the key parts of the data path that are used to perform the\nNOT instruction shown here. Since NOT is a unary operation, only the A input\nof the ALU is relevant. It is sourced from R5. The control signal to the ALU\ndirects the ALU to perform the bit-wise complement operation. The output of\nthe ALU (the result of the operation) is stored in R3 and the condition codes are\nset, completing the execution of the NOT instruction.\nRecall from Chapter 4 that the ADD (opcode = 0001) and AND (opcode =\n0101) instructions both perform binary operations; they require two 16-bit source\noperands. The ADD instruction performs a 2s complement addition of its two\nsource operands. The AND instruction performs a bit-wise AND of each pair\nof bits of its two 16-bit operands. Like the NOT, the ADD and AND use the\nregister addressing mode for one of the source operands and for the destina-\ntion operand. Bits [8:6] specify the source register, and bits [11:9] specify the\ndestination register (where the result will be written).'}"
"152
Figure 5.4
Data path relevant to the execution of NOT R3, R5.
5.2.2 Immediates
The second source operand for both ADD and AND instructions (as also dis-
cussed in Chapter 4) can be specied by either register mode or as an immediate
operand. Bit [5] determines which. If bit [5] is 0, then the second source operand
uses a register, and bits [2:0] specify which register. In that case, bits [4:3] are set
0
0
0
1
0
0
1
1
0
0
0
0
0
1
0
1
ADD
R1
R4
R5
If bit [5] is 1, the second source operand is contained within the instruction.
In that case the second source operand is obtained by sign-extending bits [4:0] to
16 bits before performing the ADD or AND. The result of the ADD (or AND)
instruction is written to the destination register and the condition codes are set,","{'page_number': 110, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '152\nFigure 5.4\nData path relevant to the execution of NOT R3, R5.\n5.2.2 Immediates\nThe second source operand for both ADD and AND instructions (as also dis-\ncussed in Chapter 4) can be specied by either register mode or as an immediate\noperand. Bit [5] determines which. If bit [5] is 0, then the second source operand\nuses a register, and bits [2:0] specify which register. In that case, bits [4:3] are set\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\nADD\nR1\nR4\nR5\nIf bit [5] is 1, the second source operand is contained within the instruction.\nIn that case the second source operand is obtained by sign-extending bits [4:0] to\n16 bits before performing the ADD or AND. The result of the ADD (or AND)\ninstruction is written to the destination register and the condition codes are set,'}"
"Figure 5.5
Data path relevant to the execution of ADD R1, R4, #2.
completing the execution of the ADD (or AND) instruction. Figure 5.5 shows the
key parts of the data path that are used to perform the instruction
ADD R1, R4, #-2.
Since the immediate operand in an ADD or AND instruction must t in bits
[4:0]
operan
imme
Wha
ANSWER:
Register 2 is cleared (i.e., set to all 0s).","{'page_number': 111, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure 5.5\nData path relevant to the execution of ADD R1, R4, #2.\ncompleting the execution of the ADD (or AND) instruction. Figure 5.5 shows the\nkey parts of the data path that are used to perform the instruction\nADD R1, R4, #-2.\nSince the immediate operand in an ADD or AND instruction must t in bits\n[4:0]\noperan\nimme\nWha\nANSWER:\nRegister 2 is cleared (i.e., set to all 0s).'}"
"154
E
E
stion: What distasteful result is also produced by this sequence? How can it
easily be avoided?
5.2.3 The LEA Instruction (Although Not Really an Operate)
Where to put the LEA instruction is a matter for debate (when you have nothing
more important to do!). It does not really operate on data, it simply loads a register
with an address. It clearly does not move data from memory to a register, nor is it
a control instruction. We had to put it somewhere, so we chose to discuss it here!
LEA (opcode = 1110) loads the register specied by bits [11:9] of the
instruction with the value formed by adding the incremented program counter
to the sign-extended bits [8:0] of the instruction. We saw this method of con-
structing an address in Chapter 4 with the LD instruction. However, in this
case, the instruction does not access memory, it simply loads the computed
address into a register. Perhaps a better name for this opcode would be CEA (for
Compute Eective Address). However, since many microprocessors in industry
that have this instruction in their ISAs call it LEA (for Load Eective Address),
we have chosen to use the same acronym.","{'page_number': 112, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '154\nE\nE\nstion: What distasteful result is also produced by this sequence? How can it\neasily be avoided?\n5.2.3 The LEA Instruction (Although Not Really an Operate)\nWhere to put the LEA instruction is a matter for debate (when you have nothing\nmore important to do!). It does not really operate on data, it simply loads a register\nwith an address. It clearly does not move data from memory to a register, nor is it\na control instruction. We had to put it somewhere, so we chose to discuss it here!\nLEA (opcode = 1110) loads the register specied by bits [11:9] of the\ninstruction with the value formed by adding the incremented program counter\nto the sign-extended bits [8:0] of the instruction. We saw this method of con-\nstructing an address in Chapter 4 with the LD instruction. However, in this\ncase, the instruction does not access memory, it simply loads the computed\naddress into a register. Perhaps a better name for this opcode would be CEA (for\nCompute Eective Address). However, since many microprocessors in industry\nthat have this instruction in their ISAs call it LEA (for Load Eective Address),\nwe have chosen to use the same acronym.'}"
"Figure 5.6
Data path relevant to the execution of LEA R5, #3.
We shall see shortly that the LEA instruction is useful to initialize a regis-
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
1
1
1
0
1
0
1
1
1
1
1
1
1
1
0
1
LEA
R5
3
R5 will contain x4016 after the instruction at x4018 is executed. Question: Why
will R5 not contain the address x4015?
Figure 5.6 shows the relevant parts of the data path required to execute the
LEA instruction. Note that the value to be loaded into the register does not involve
any access to memory. ...nor does it have any eect on the condition codes.
5.3 Data Movement Instructions
Data movement instructions move information between the general purpose reg-
isters and memory and between the registers and the input/output devices. We will
ignore for now the business of moving information from input devices to registers
and from registers to output devices. This will be an important part of Chapter 9.
In this chapter, we will conne ourselves to moving information between memory
and the general purpose registers.","{'page_number': 113, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure 5.6\nData path relevant to the execution of LEA R5, #3.\nWe shall see shortly that the LEA instruction is useful to initialize a regis-\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n1\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\nLEA\nR5\n3\nR5 will contain x4016 after the instruction at x4018 is executed. Question: Why\nwill R5 not contain the address x4015?\nFigure 5.6 shows the relevant parts of the data path required to execute the\nLEA instruction. Note that the value to be loaded into the register does not involve\nany access to memory. ...nor does it have any eect on the condition codes.\n5.3 Data Movement Instructions\nData movement instructions move information between the general purpose reg-\nisters and memory and between the registers and the input/output devices. We will\nignore for now the business of moving information from input devices to registers\nand from registers to output devices. This will be an important part of Chapter 9.\nIn this chapter, we will conne ourselves to moving information between memory\nand the general purpose registers.'}"
"The process of moving information from memory to a register is called a
load, and the process of moving information from a register to memory is called a
store. In both cases, the information in the location containing the source operand
remains unchanged. In both cases, the location of the destination operand is over-
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
opcode
DR or SR
Addr Gen bits
Data movement instructions require two operands, a source and a destination.
The source is the data to be moved; the destination is the location where it is
moved to. One of these locations is a register, the other is a memory location or
an input/output device. In this chapter we will assume the second operand is in
memory. In Chapter 9 we will study the cases where the second operand is an
input or output device.
Bits [11:9] specify one of these operands, the register. If the instruction is a
load, DR refers to the destination general purpose register that will contain the
value after it is read from memory (at the completion of the instruction cycle). If
the instruction is a store, SR refers to the register that contains the value that will
be written to memory.
Bits [8:0] contain the address generation bits. That is, bits [8:0] contain infor-
mation that is used to compute the 16-bit address of the second operand. In the
case of the LC-3s data movement instructions, there are three ways to interpret
bits [8:0]. They are collectively called addressing modes. The opcode species
how to interpret bits [8:0]. That is, the LC-3s opcode species which of the three
addressing modes should be used to obtain the address of the operand from bits
[8:0] of the instruction.
5.3.1 PC-Relative Mode
LD (opcode = 0010) and ST (opcode = 0011) specify the PC-relative address-
ing mode. We have already discussed this addressing mode in Chapter 4. It is
so named because bits [8:0] of the instruction specify an oset relative to the
PC. The memory address is computed by sign-extending bits [8:0] to 16 bits and
adding the result to the incremented PC. The incremented PC is the contents of
the program counter after the FETCH phase, that is, after the PC has been incre-
mented. If the instruction is LD, the computed address (PC + oset) species the
memory location to be accessed. Its contents is loaded into the register specied
by bits [11:9] of the instruction. If the instruction is ST, the contents of the regis-
ter specied by bits [11:9] of the instruction is written into the memory location
whose address is PC + oset. ...and the N, Z, and P one-bit condition codes are
set depending on whether the value loaded is negative, positive, or zero.","{'page_number': 114, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The process of moving information from memory to a register is called a\nload, and the process of moving information from a register to memory is called a\nstore. In both cases, the information in the location containing the source operand\nremains unchanged. In both cases, the location of the destination operand is over-\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nopcode\nDR or SR\nAddr Gen bits\nData movement instructions require two operands, a source and a destination.\nThe source is the data to be moved; the destination is the location where it is\nmoved to. One of these locations is a register, the other is a memory location or\nan input/output device. In this chapter we will assume the second operand is in\nmemory. In Chapter 9 we will study the cases where the second operand is an\ninput or output device.\nBits [11:9] specify one of these operands, the register. If the instruction is a\nload, DR refers to the destination general purpose register that will contain the\nvalue after it is read from memory (at the completion of the instruction cycle). If\nthe instruction is a store, SR refers to the register that contains the value that will\nbe written to memory.\nBits [8:0] contain the address generation bits. That is, bits [8:0] contain infor-\nmation that is used to compute the 16-bit address of the second operand. In the\ncase of the LC-3s data movement instructions, there are three ways to interpret\nbits [8:0]. They are collectively called addressing modes. The opcode species\nhow to interpret bits [8:0]. That is, the LC-3s opcode species which of the three\naddressing modes should be used to obtain the address of the operand from bits\n[8:0] of the instruction.\n5.3.1 PC-Relative Mode\nLD (opcode = 0010) and ST (opcode = 0011) specify the PC-relative address-\ning mode. We have already discussed this addressing mode in Chapter 4. It is\nso named because bits [8:0] of the instruction specify an oset relative to the\nPC. The memory address is computed by sign-extending bits [8:0] to 16 bits and\nadding the result to the incremented PC. The incremented PC is the contents of\nthe program counter after the FETCH phase, that is, after the PC has been incre-\nmented. If the instruction is LD, the computed address (PC + oset) species the\nmemory location to be accessed. Its contents is loaded into the register specied\nby bits [11:9] of the instruction. If the instruction is ST, the contents of the regis-\nter specied by bits [11:9] of the instruction is written into the memory location\nwhose address is PC + oset. ...and the N, Z, and P one-bit condition codes are\nset depending on whether the value loaded is negative, positive, or zero.'}"
"15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
0
0
1
0
0
1
0
1
1
0
1
0
1
1
1
1
LD
R2
x1AF
Figure 5.7 shows the relevant parts of the data path required to execute this
instruction. The three steps of the LD instruction are identied. In step 1, the
incremented PC (x4019) is added to the sign-extended value contained in IR [8:0]
(xFFAF), and the result (x3FC8) is loaded into the MAR. In step 2, memory is
read and the contents of x3FC8 is loaded into the MDR. Suppose the value stored
in x3FC8 is 5. In step 3, the value 5 is loaded into R2, and the NZP condition codes
are set, completing the instruction cycle.
Note that the address of the memory operand is limited to a small range of
the total memory. That is, the address can only be within +256 or 255 locations
of the LD or ST instruction. This is the range provided by the sign-extended value
Figure 5.7
Data path relevant to execution of LD R2, x1AF.","{'page_number': 115, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\nLD\nR2\nx1AF\nFigure 5.7 shows the relevant parts of the data path required to execute this\ninstruction. The three steps of the LD instruction are identied. In step 1, the\nincremented PC (x4019) is added to the sign-extended value contained in IR [8:0]\n(xFFAF), and the result (x3FC8) is loaded into the MAR. In step 2, memory is\nread and the contents of x3FC8 is loaded into the MDR. Suppose the value stored\nin x3FC8 is 5. In step 3, the value 5 is loaded into R2, and the NZP condition codes\nare set, completing the instruction cycle.\nNote that the address of the memory operand is limited to a small range of\nthe total memory. That is, the address can only be within +256 or 255 locations\nof the LD or ST instruction. This is the range provided by the sign-extended value\nFigure 5.7\nData path relevant to execution of LD R2, x1AF.'}"
"5.3.2 Indirect Mode
LDI (opcode = 1010) and STI (opcode = 1011) specify the indirect address-
ing mode. An address is rst formed exactly the same way as with LD and ST.
However, instead of this address being the address of the operand to be loaded
or stored, it is the address of the address of the operand to be loaded or stored.
Hence the name indirect. Note that the address of the operand can be anywhere

LDI
R3
x1CC
is in x4A1B, and the contents of x49E8 is x2110, execution of this instruction
results in the contents of x2110 being loaded into R3.
Figure 5.8 shows the relevant parts of the data path required to execute this
inst
of a
IR [
is i
Figure 5.8
Data path relevant to the execution of LDI R3, x1CC.","{'page_number': 116, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '5.3.2 Indirect Mode\nLDI (opcode = 1010) and STI (opcode = 1011) specify the indirect address-\ning mode. An address is rst formed exactly the same way as with LD and ST.\nHowever, instead of this address being the address of the operand to be loaded\nor stored, it is the address of the address of the operand to be loaded or stored.\nHence the name indirect. Note that the address of the operand can be anywhere\n\nLDI\nR3\nx1CC\nis in x4A1B, and the contents of x49E8 is x2110, execution of this instruction\nresults in the contents of x2110 being loaded into R3.\nFigure 5.8 shows the relevant parts of the data path required to execute this\ninst\nof a\nIR [\nis i\nFigure 5.8\nData path relevant to the execution of LDI R3, x1CC.'}"
"contents of x2110 being loaded into R3. In step 3, since x2110 is not the operand,
but the address of the operand, it is loaded into the MAR. In step 4, memory is
again read, and the MDR again loaded. This time the MDR is loaded with the
contents of x2110. Suppose the value 1 is stored in memory location x2110. In
step 5, the contents of the MDR (i.e., 1) is loaded into R3 and the NZP condition
codes are set, completing the instruction cycle.
5.3.3 Base+ofset Mode
LDR (opcode = 0110) and STR (opcode = 0111) specify the Base+oset
addr
oper
The s
speci
I
with
instr
IR
Figure 5.9
Data path relevant to the execution of LDR R1, R2, x1D.","{'page_number': 117, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'contents of x2110 being loaded into R3. In step 3, since x2110 is not the operand,\nbut the address of the operand, it is loaded into the MAR. In step 4, memory is\nagain read, and the MDR again loaded. This time the MDR is loaded with the\ncontents of x2110. Suppose the value 1 is stored in memory location x2110. In\nstep 5, the contents of the MDR (i.e., 1) is loaded into R3 and the NZP condition\ncodes are set, completing the instruction cycle.\n5.3.3 Base+ofset Mode\nLDR (opcode = 0110) and STR (opcode = 0111) specify the Base+oset\naddr\noper\nThe s\nspeci\nI\nwith\ninstr\nIR\nFigure 5.9\nData path relevant to the execution of LDR R1, R2, x1D.'}"
"contained in IR [5:0] (x001D), and the result (x2362) is loaded into the MAR.
Second, memory is read, and the contents of x2362 is loaded into the MDR.
Suppose the value stored in memory location x2362 is x0F0F. Third, and nally,
the contents of the MDR (in this case, x0F0F) is loaded into R1 and the NZP
condition codes are set, completing the execution of the LDR instruction.
Note that the Base+oset addressing mode also allows the address of the
operand to be anywhere in the computers memory.
5.3.4 An Example
We conclude our study of addressing modes with a comprehensive example.
Assume the contents of memory locations x30F6 through x30FC are as shown in
Figure 5.10, and the PC contains x30F6. We will examine the eects of carrying
out the seven instructions starting at location x30FC.
Since the PC points initially to location x30F6, the rst instruction to be
executed is the one stored in location x30F6. The opcode of that instruction is
1110, load eective address (LEA). LEA loads the register specied by bits [11:9]
with the address formed by sign-extending bits [8:0] of the instruction and adding
the result to the incremented PC. The 16-bit value obtained by sign-extending
bits [8:0] of the instruction is xFFFD. The incremented PC is x30F7. Therefore,
at the end of execution of the LEA instruction, R1 contains x30F4, and the PC
contains x30F7.
Next, the instruction stored in location x30F7 is executed. Since the opcode
0001 species ADD, the sign-extended immediate in bits [4:0] (since bit [5] is
1) is added to the contents of the register specied in bits [8:6], and the result
is written to the register specied by bits [11:9]. Since the previous instruction
wrote x30F4 into R1, and the sign-extended immediate value is x000E, the sum
is x3102. At the end of execution of this instruction, R2 contains x3102, and the
PC contains x30F8. R1 still contains x30F4.
Next, the instruction stored in x30F8. The opcode 0011 species the ST
instruction, which stores the contents of the register specied by bits [11:9]
(R2)
addr
(x3
Figure 5.10
A code fragment illustrating the three addressing modes.","{'page_number': 118, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'contained in IR [5:0] (x001D), and the result (x2362) is loaded into the MAR.\nSecond, memory is read, and the contents of x2362 is loaded into the MDR.\nSuppose the value stored in memory location x2362 is x0F0F. Third, and nally,\nthe contents of the MDR (in this case, x0F0F) is loaded into R1 and the NZP\ncondition codes are set, completing the execution of the LDR instruction.\nNote that the Base+oset addressing mode also allows the address of the\noperand to be anywhere in the computers memory.\n5.3.4 An Example\nWe conclude our study of addressing modes with a comprehensive example.\nAssume the contents of memory locations x30F6 through x30FC are as shown in\nFigure 5.10, and the PC contains x30F6. We will examine the eects of carrying\nout the seven instructions starting at location x30FC.\nSince the PC points initially to location x30F6, the rst instruction to be\nexecuted is the one stored in location x30F6. The opcode of that instruction is\n1110, load eective address (LEA). LEA loads the register specied by bits [11:9]\nwith the address formed by sign-extending bits [8:0] of the instruction and adding\nthe result to the incremented PC. The 16-bit value obtained by sign-extending\nbits [8:0] of the instruction is xFFFD. The incremented PC is x30F7. Therefore,\nat the end of execution of the LEA instruction, R1 contains x30F4, and the PC\ncontains x30F7.\nNext, the instruction stored in location x30F7 is executed. Since the opcode\n0001 species ADD, the sign-extended immediate in bits [4:0] (since bit [5] is\n1) is added to the contents of the register specied in bits [8:6], and the result\nis written to the register specied by bits [11:9]. Since the previous instruction\nwrote x30F4 into R1, and the sign-extended immediate value is x000E, the sum\nis x3102. At the end of execution of this instruction, R2 contains x3102, and the\nPC contains x30F8. R1 still contains x30F4.\nNext, the instruction stored in x30F8. The opcode 0011 species the ST\ninstruction, which stores the contents of the register specied by bits [11:9]\n(R2)\naddr\n(x3\nFigure 5.10\nA code fragment illustrating the three addressing modes.'}"
"(xFFFB). Therefore, at the end of execution of the ST instruction, memory loca-
tion x30F4 (i.e., x30F9 + xFFFB) contains the value stored in R2 (x3102) and
the PC contains x30F9.
Next the instruction at x30F9. The AND instruction, with an immediate
operand x0000. At the end of execution, R2 contains the value 0, and the PC
contains x30FA.
At x30FA, the opcode 0001 species the ADD instruction. After execution,
R2 contains the value 5, and the PC contains x30FB.
At x30FB, the opcode 0111 signies the STR instruction. STR (like LDR)
uses the Base+oset addressing mode. The memory address is obtained by
adding the contents of the BASE register (specied by bits [8:6]) to the sign-
extended oset contained in bits [5:0]. In this case, bits [8:6] specify R1, which
contains x30F4. The 16-bit sign-extended oset is x000E. Since x30F4 + x000E
is x3102, the memory address is x3102. The STR instruction stores into x3102
the contents of the register specied by bits [11:9], in this case R2. Since R2 con-
tains the value 5, at the end of execution of this instruction, M[x3102] contains
the value 5, and the PC contains x30FC.
Finally the instruction at x30FC. The opcode 1010 species LDI. LDI (like
STI) uses the indirect addressing mode. The memory address is obtained by rst
forming an address as is done in the PC-relative addressing mode. Bits [8:0] are
sign-extended to 16 bits (xFFF7) and added to the incremented PC (x30FD).
Their sum (x30F4) is the address of the operand address. Since M[x30F4] con-
tains x3102, x3102 is the operand address. The LDI instruction loads the value
found at this address (in this case 5) into the register identied by bits [11:9] of
the instruction (in this case R3). At the end of execution of this instruction, R3
contains the value 5 and the PC contains x30FD.
5.4 Control Instructions
Control instructions change the sequence of instructions to be executed. If there
were no control instructions, the next instruction fetched after the current instruc-
tion nishes would always be the instruction located in the next sequential
memory location. As you know, this is because the PC is incremented in the
FETCH phase of each instruction cycle. We have already seen in the program of
Section 4.4 that it is often useful to be able to break that sequence.
The LC-3 has ve opcodes that enable the sequential execution ow to
be broken: conditional branch, unconditional jump, subroutine call (sometimes
called function), TRAP, and RTI (Return from Trap or Interrupt)","{'page_number': 119, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'As you know, this is because the PC is incremented in the\nFETCH phase of each instruction cycle. We have already seen in the program of\nSection 4.4 that it is often useful to be able to break that sequence.\nThe LC-3 has ve opcodes that enable the sequential execution ow to\nbe broken: conditional branch, unconditional jump, subroutine call (sometimes\ncalled function), TRAP, and RTI (Return from Trap or Interrupt). In this sec-\ntion, we will deal almost entirely with the most common control instruction, the\nconditional branch. We will also discuss the unconditional jump and the TRAP\ninstruction. The TRAP instruction, often called service call, is useful because\nit allows a programmer to get help from the operating system to do things that\nthe typical programmer does not fully understand how to do. Typical examples:\ngetting information into the computer from input devices, displaying information\nto output devices, and stopping the computer. The TRAP instruction breaks the'}"
"As you know, this is because the PC is incremented in the
FETCH phase of each instruction cycle. We have already seen in the program of
Section 4.4 that it is often useful to be able to break that sequence.
The LC-3 has ve opcodes that enable the sequential execution ow to
be broken: conditional branch, unconditional jump, subroutine call (sometimes
called function), TRAP, and RTI (Return from Trap or Interrupt). In this sec-
tion, we will deal almost entirely with the most common control instruction, the
conditional branch. We will also discuss the unconditional jump and the TRAP
instruction. The TRAP instruction, often called service call, is useful because
it allows a programmer to get help from the operating system to do things that
the typical programmer does not fully understand how to do. Typical examples:
getting information into the computer from input devices, displaying information
to output devices, and stopping the computer. The TRAP instruction breaks the","{'page_number': 119, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'As you know, this is because the PC is incremented in the\nFETCH phase of each instruction cycle. We have already seen in the program of\nSection 4.4 that it is often useful to be able to break that sequence.\nThe LC-3 has ve opcodes that enable the sequential execution ow to\nbe broken: conditional branch, unconditional jump, subroutine call (sometimes\ncalled function), TRAP, and RTI (Return from Trap or Interrupt). In this sec-\ntion, we will deal almost entirely with the most common control instruction, the\nconditional branch. We will also discuss the unconditional jump and the TRAP\ninstruction. The TRAP instruction, often called service call, is useful because\nit allows a programmer to get help from the operating system to do things that\nthe typical programmer does not fully understand how to do. Typical examples:\ngetting information into the computer from input devices, displaying information\nto output devices, and stopping the computer. The TRAP instruction breaks the'}"
"sequential execution of a user program to start a sequence of instructions in the
operating system. How the TRAP instruction does this, and in fact, most of the
discussion of the TRAP instruction and all of the discussion of the subroutine
call and the return from interrupt we will leave for Chapters 8 and 9.
5.4.1 Conditional Branches
Of the ve instructions which change the execution ow from the next sequential
instruction to an instruction located someplace else in the program, only one of
them decides each time it is executed whether to execute the next instruction in
sequence or whether to execute an instruction from outside that sequence. The
instruction that makes that decision each time it is executed is the conditional
branch instruction BR (opcode = 0000).
Like all instructions in the LC-3, the PC is incremented during the FETCH
phase of its instruction cycle. Based on the execution of previous instructions in
the program, the conditional branchs EXECUTE phase either does nothing or it
loads the PC with the address of the instruction it wishes to execute next. If the
conditional branch instruction does nothing during the EXECUTE phase, then
the incremented PC will remain unchanged, and the next instruction executed
will be
Th
change
reect
Th
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
0
0
0
0
n
z
p
PCoset
Bits [11], [10], and [9] are associated with the three condition codes, N, Z, and P.
As you know, the three operate instructions (ADD, AND, and NOT) and
the three load instructions (LD, LDI, and LDR) in the LC-3 write values into
general purpose registers, and also set the three condition codes in accordance
with whether the value written is negative, zero, or positive.
The conditional branch instruction uses that information to determine
whether or not to depart from the usual sequential execution of instructions that
we get as a result of incrementing the PC during the FETCH phase of each
instruction.
We said (without explanation) in the computer program we studied in
Section 4.4 that if bits [11:9] of the conditional branch instruction are 101, we
will depart from the usual sequential execution if the last value written into a reg-
ister by one of the six instructions listed above is not 0. We are now ready to see
exactly what causes that to happen.
During the EXECUTE phase of the BR instruction cycle, the processor
examines the condition codes whose associated bits in the instruction, bits [11:9],
are 1. Note the lower case n, z, and p in bits [11:9] of the BR instruction for-
mat shown above. If bit [11] is 1, condition code N is examined. If bit [10] is 1,","{'page_number': 120, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'sequential execution of a user program to start a sequence of instructions in the\noperating system. How the TRAP instruction does this, and in fact, most of the\ndiscussion of the TRAP instruction and all of the discussion of the subroutine\ncall and the return from interrupt we will leave for Chapters 8 and 9.\n5.4.1 Conditional Branches\nOf the ve instructions which change the execution ow from the next sequential\ninstruction to an instruction located someplace else in the program, only one of\nthem decides each time it is executed whether to execute the next instruction in\nsequence or whether to execute an instruction from outside that sequence. The\ninstruction that makes that decision each time it is executed is the conditional\nbranch instruction BR (opcode = 0000).\nLike all instructions in the LC-3, the PC is incremented during the FETCH\nphase of its instruction cycle. Based on the execution of previous instructions in\nthe program, the conditional branchs EXECUTE phase either does nothing or it\nloads the PC with the address of the instruction it wishes to execute next. If the\nconditional branch instruction does nothing during the EXECUTE phase, then\nthe incremented PC will remain unchanged, and the next instruction executed\nwill be\nTh\nchange\nreect\nTh\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n0\n0\n0\n0\nn\nz\np\nPCoset\nBits [11], [10], and [9] are associated with the three condition codes, N, Z, and P.\nAs you know, the three operate instructions (ADD, AND, and NOT) and\nthe three load instructions (LD, LDI, and LDR) in the LC-3 write values into\ngeneral purpose registers, and also set the three condition codes in accordance\nwith whether the value written is negative, zero, or positive.\nThe conditional branch instruction uses that information to determine\nwhether or not to depart from the usual sequential execution of instructions that\nwe get as a result of incrementing the PC during the FETCH phase of each\ninstruction.\nWe said (without explanation) in the computer program we studied in\nSection 4.4 that if bits [11:9] of the conditional branch instruction are 101, we\nwill depart from the usual sequential execution if the last value written into a reg-\nister by one of the six instructions listed above is not 0. We are now ready to see\nexactly what causes that to happen.\nDuring the EXECUTE phase of the BR instruction cycle, the processor\nexamines the condition codes whose associated bits in the instruction, bits [11:9],\nare 1. Note the lower case n, z, and p in bits [11:9] of the BR instruction for-\nmat shown above. If bit [11] is 1, condition code N is examined. If bit [10] is 1,'}"
"condition code Z is examined. If bit [9] is 1, condition code P is examined. If any
of bits [11:9] are 0, the associated condition codes are not examined. If any of the
condition codes that are examined are set (i.e., equal to 1), then the PC is loaded
with the address obtained in the EVALUATE ADDRESS phase. If none of the
condition codes that are examined are set, the incremented PC is left unchanged,
and the next sequential instruction will be fetched at the start of the next
instruction cycle.
The address obtained during the EVALUATE ADDRESS phase of the
instruction cycle is generated using the PC-relative addressing mode.
In our example in Section 4.4, the ADD instruction in memory location
x3004 subtracted 1 from R2, wrote the result to R2, and set the condition codes.
The BR instruction in memory location x3005 shows bits [11:9] = 101. Since bit
[11] is 1, if the N bit is set, the result of the ADD must have been negative. Since
bit [9] is also 1, if the P bit is set, the result must have been positive. Since bit
[10] is 0, we do not examine the Z bit. Thus if the previous result is positive or
negative (i.e., not 0), the PC is loaded with x3003, the address calculated in the
EVALUATE ADDRESS phase of the branch instruction.
Recall that the program of Figure 4.7 used R2 to keep track of the number
of times the number 5 was added to R3. As long as we were not done with all
our additions, the result of subtracting 1 from R2 was not zero. When we were
done with our additions, subtracting 1 from R2 produced the result 0, so Z was
set to 1, N and P were set to 0. At that point, bits [11:9] checked the N and P
condition codes which were 0, so the incremented PC was not changed, and the
instruction at location x3006, a trap to the operating system to halt the computer,
was executed next.
Lets Lo
at x4027,
15
0
0
0
0
0
1
0
0
1
1
0
1
1
0
0
1
BR
n
z
p
x0D9
Figure 5.11 shows the data path elements that are required to execute this
instruction. Note the logic required to determine whether the sequential instruc-
tion ow should be broken. Each of the three AND gates corresponds to one of
the three condition codes. The output of the AND gate is 1 if the corresponding
condition code is 1 and if the associated bit in the instruction directs the hardware
to check that condition code. If any of the three AND gates have an output 1, the
OR gate has an output 1, indicating that the sequential instruction ow should
be broken, and the PC should be loaded with the address evaluated during the
EVALUATE ADDRESS phase of the instruction cycle","{'page_number': 121, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The output of the AND gate is 1 if the corresponding\ncondition code is 1 and if the associated bit in the instruction directs the hardware\nto check that condition code. If any of the three AND gates have an output 1, the\nOR gate has an output 1, indicating that the sequential instruction ow should\nbe broken, and the PC should be loaded with the address evaluated during the\nEVALUATE ADDRESS phase of the instruction cycle.\nIn the case of the conditional branch instruction at x4027, the answer is yes,\nand the PC is loaded with x4101, replacing x4028, which had been loaded into\nthe PC during the FETCH phase of the BR instruction.'}"
"The output of the AND gate is 1 if the corresponding
condition code is 1 and if the associated bit in the instruction directs the hardware
to check that condition code. If any of the three AND gates have an output 1, the
OR gate has an output 1, indicating that the sequential instruction ow should
be broken, and the PC should be loaded with the address evaluated during the
EVALUATE ADDRESS phase of the instruction cycle.
In the case of the conditional branch instruction at x4027, the answer is yes,
and the PC is loaded with x4101, replacing x4028, which had been loaded into
the PC during the FETCH phase of the BR instruction.","{'page_number': 121, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The output of the AND gate is 1 if the corresponding\ncondition code is 1 and if the associated bit in the instruction directs the hardware\nto check that condition code. If any of the three AND gates have an output 1, the\nOR gate has an output 1, indicating that the sequential instruction ow should\nbe broken, and the PC should be loaded with the address evaluated during the\nEVALUATE ADDRESS phase of the instruction cycle.\nIn the case of the conditional branch instruction at x4027, the answer is yes,\nand the PC is loaded with x4101, replacing x4028, which had been loaded into\nthe PC during the FETCH phase of the BR instruction.'}"
"164
Yes!
Figure 5.11
Data path relevant to the execution of BRz x0D9.
Another Example.
If all three bits [11:9] are 1, then all three condition codes
are examined. In this case, since the last result stored into a register had to be
either negative, zero, or positive (there are no other choices!), one of the three
Question: What happens if all three bits [11:9] in the BR instruction are 0?","{'page_number': 122, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '164\nYes!\nFigure 5.11\nData path relevant to the execution of BRz x0D9.\nAnother Example.\nIf all three bits [11:9] are 1, then all three condition codes\nare examined. In this case, since the last result stored into a register had to be\neither negative, zero, or positive (there are no other choices!), one of the three\nQuestion: What happens if all three bits [11:9] in the BR instruction are 0?'}"
"5.4.2 Two Methods of Loop Control
We saw in Section 4.4 in our multiplication program that we repeatedly executed
a sequence of instructions until the value in a register was zero. We call that
sequence a loop body, and each time the loop body is executed we call it one
iteration of the loop body. The BR instruction at the end of the sequence controls
the number of times the loop body is executed. There are two common ways to
control the number of iterations.
Figure 5.12
An algorithm for adding integers using a counter for loop control.
First, as in all algorithms, we must initialize our variables. That is, we must
set up the initial values of the variables that the computer will use in executing the
program that solves the problem. There are three such variables: the address of
the next integer to be added (assigned to R1), the running sum (assigned to R3),
and the number of integers left to be added (assigned to R2). The three variables
are initialized as follows: The address of the rst integer to be added is put in R1.
R3, which will keep track of the running sum, is initialized to 0. R2, which will
keep track of the number of integers left to be added, is initialized to 12. Then
the process of adding begins.
The program repeats the process of loading into R4 one of the 12 integers
and adding it to R3. Each time we perform the ADD, we increment R1 so it will
point to (i.e., contain the address of) the next number to be added and decrement
R2 so we will know how many numbers still need to be added. When R2 becomes
zero, the Z condition code is set, and we can detect that we are done.
The 10-instruction program shown in Figure 5.13 accomplishes the task.
The details of the program execution are as follows: The program starts
with PC = x3000. The rst instruction (at location x3000) initializes R1 with","{'page_number': 123, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '5.4.2 Two Methods of Loop Control\nWe saw in Section 4.4 in our multiplication program that we repeatedly executed\na sequence of instructions until the value in a register was zero. We call that\nsequence a loop body, and each time the loop body is executed we call it one\niteration of the loop body. The BR instruction at the end of the sequence controls\nthe number of times the loop body is executed. There are two common ways to\ncontrol the number of iterations.\nFigure 5.12\nAn algorithm for adding integers using a counter for loop control.\nFirst, as in all algorithms, we must initialize our variables. That is, we must\nset up the initial values of the variables that the computer will use in executing the\nprogram that solves the problem. There are three such variables: the address of\nthe next integer to be added (assigned to R1), the running sum (assigned to R3),\nand the number of integers left to be added (assigned to R2). The three variables\nare initialized as follows: The address of the rst integer to be added is put in R1.\nR3, which will keep track of the running sum, is initialized to 0. R2, which will\nkeep track of the number of integers left to be added, is initialized to 12. Then\nthe process of adding begins.\nThe program repeats the process of loading into R4 one of the 12 integers\nand adding it to R3. Each time we perform the ADD, we increment R1 so it will\npoint to (i.e., contain the address of) the next number to be added and decrement\nR2 so we will know how many numbers still need to be added. When R2 becomes\nzero, the Z condition code is set, and we can detect that we are done.\nThe 10-instruction program shown in Figure 5.13 accomplishes the task.\nThe details of the program execution are as follows: The program starts\nwith PC = x3000. The rst instruction (at location x3000) initializes R1 with'}"
"166
x3009
0
0
0
0
1
1
1
1 1 1 1
1 1 0 1 0
BRnzp x3004
Figure 5.13
A program that implements the algorithm of Figure 5.12.
the address x3100. (The incremented PC is x3001; the sign-extended PCoset is
x00FF.)
The instruction at x3001 clears R3. R3 will keep track of the running sum,
so it must start with the value 0. As we said previously, this is called initializing
the SUM to zero.
The instructions at x3002 and x3003 initialize R2 to 12, the number of inte-
gers to be added. R2 will keep track of how many numbers have already been
added. This will be done (by the instruction in x3008) by decrementing R2 after
each addition takes place.
The instruction at x3004 is a conditional branch instruction. Note that bit
[10] is a 1. That means that the Z condition code will be examined. If it is set, we
know R2 must have just been decremented to 0. That means there are no more
numbers to be added, and we are done. If it is clear, we know we still have work
to do, and we continue with another iteration of the loop body.
The instruction at x3005 loads the next integer into R4, and the instruction
at x3006 adds it to R3.
The instructions at x3007 and x3008 perform the necessary bookkeeping.
The instruction at x3007 increments R1, so R1 will point to the next location in
memory containing an integer to be added. The instruction at x3008 decrements
R2, which is keeping track of the number of integers still to be added, and sets
the condition codes.
The instruction at x3009 is an unconditional branch, since bits [11:9] are all 1.
It loads the PC with x3004. It also does not aect the condition codes, so the next
instruction to be executed (the conditional branch at x3004) will be based on the
instruction executed at x3008.
This is worth saying again. The conditional branch instruction at x3004 fol-
lows the instruction at x3009, which does not aect condition codes, which in
turn follows the instruction at x3008. Thus, the conditional branch instruction
at x3004 will be based on the condition codes set by the instruction at x3008.
The instruction at x3008 sets the condition codes based on the value produced
by decrementing R2. As long as there are still integers to be added, the ADD
instruction at x3008 will produce a value greater than zero and therefore clear
the Z condition code. The conditional branch instruction at x3004 examines the","{'page_number': 124, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '166\nx3009\n0\n0\n0\n0\n1\n1\n1\n1 1 1 1\n1 1 0 1 0\nBRnzp x3004\nFigure 5.13\nA program that implements the algorithm of Figure 5.12.\nthe address x3100. (The incremented PC is x3001; the sign-extended PCoset is\nx00FF.)\nThe instruction at x3001 clears R3. R3 will keep track of the running sum,\nso it must start with the value 0. As we said previously, this is called initializing\nthe SUM to zero.\nThe instructions at x3002 and x3003 initialize R2 to 12, the number of inte-\ngers to be added. R2 will keep track of how many numbers have already been\nadded. This will be done (by the instruction in x3008) by decrementing R2 after\neach addition takes place.\nThe instruction at x3004 is a conditional branch instruction. Note that bit\n[10] is a 1. That means that the Z condition code will be examined. If it is set, we\nknow R2 must have just been decremented to 0. That means there are no more\nnumbers to be added, and we are done. If it is clear, we know we still have work\nto do, and we continue with another iteration of the loop body.\nThe instruction at x3005 loads the next integer into R4, and the instruction\nat x3006 adds it to R3.\nThe instructions at x3007 and x3008 perform the necessary bookkeeping.\nThe instruction at x3007 increments R1, so R1 will point to the next location in\nmemory containing an integer to be added. The instruction at x3008 decrements\nR2, which is keeping track of the number of integers still to be added, and sets\nthe condition codes.\nThe instruction at x3009 is an unconditional branch, since bits [11:9] are all 1.\nIt loads the PC with x3004. It also does not aect the condition codes, so the next\ninstruction to be executed (the conditional branch at x3004) will be based on the\ninstruction executed at x3008.\nThis is worth saying again. The conditional branch instruction at x3004 fol-\nlows the instruction at x3009, which does not aect condition codes, which in\nturn follows the instruction at x3008. Thus, the conditional branch instruction\nat x3004 will be based on the condition codes set by the instruction at x3008.\nThe instruction at x3008 sets the condition codes based on the value produced\nby decrementing R2. As long as there are still integers to be added, the ADD\ninstruction at x3008 will produce a value greater than zero and therefore clear\nthe Z condition code. The conditional branch instruction at x3004 examines the'}"
"Z condition code. As long as Z is clear, the PC will not be aected, and the next
iteration of the loop body will begin. That is, the next instruction cycle will start
with an instruction fetch from x3005.
The conditional branch instruction causes the execution sequence to follow:
x3000, x3001, x3002, x3003, x3004, x3005, x3006, x3007, x3008, x3009, x3004,
x3005,x3006,x3007,x3008,x3009,x3004,x3005,andsoon.Theloopbodyconsists
of the instructions at x3005 to x3009. When the value in R2 becomes 0, the PC is
loaded with x300A, and the program continues at x300A with its next activity.
You may have noticed that we can remove the branch instruction at x3004
if we replace the unconditional branch instruction at x3009 with a conditional
branch that tests for not 0 (i.e., bits [11:9]=101), and branches to the instruc-
tion currently located in x3005. It is tempting to do that since it decreases the
loop body by one instruction. BUT, we admonish you not to do that! The pro-
gram as shown obeys the rules of structured programming that we will discuss
in Chapter 6. The shortcut does work for this simple example, but it breaks the
methodology of structured programming. You do not want to get in the habit of
taking such shortcuts, since for larger programs it is a clear invitation to disaster.
More on this in Chapter 6.
Finally, it is worth noting that we could have written a program to add these
12 integers without any control instructions. We still would have needed the LEA
instruction in x3000 to initialize R1. We would not have needed the instruction
at x3001 to initialize the running sum, nor the instructions at x3002 and x3003
to initialize the number of integers left to be added. We could have loaded the
contents of x3100 directly into R3, and then repeatedly (by incrementing R1),
loaded subsequent integers into R4 and adding R4 to the running sum in R3 11
more times! After the addition of the twelfth integer, we would go on to the next
task, as does the example of Figure 5.13 with the branch instruction in x3004.
Unfortunately, instead of a 10-instruction program, we would have a 35-
instruction program. Moreover, if we had wished to add 100 integers without any
control instructions instead of 12, we would have had a 299-instruction program
instead of 10. The control instructions in the example of Figure 5.13 permit the
reuse of sequences of code (the loop body) by breaking the sequential instruction
execution ow.
Loop Control with a Sentinel
The example above controls the number of times
the loop body executes by means of a counter. We knew we wanted to execute the
loop 12 times, so we simply set a counter to 12, and then after each execution of
the loop, we decremented the counter and checked to see if it was zero","{'page_number': 125, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '13 permit the\nreuse of sequences of code (the loop body) by breaking the sequential instruction\nexecution ow.\nLoop Control with a Sentinel\nThe example above controls the number of times\nthe loop body executes by means of a counter. We knew we wanted to execute the\nloop 12 times, so we simply set a counter to 12, and then after each execution of\nthe loop, we decremented the counter and checked to see if it was zero. If it was\nnot zero, we set the PC to the start of the loop and continued with another iteration.\nA second method for controlling the number of executions of a loop is to use\na sentinel. This method is particularly eective if we do not know ahead of time\nhow many iterations we will want to perform. Each iteration is usually based on\nprocessing a value. We append to our sequence of values to be processed a value\nthat we know ahead of time can never occur (i.e., the sentinel). For example, if\nwe are adding a sequence of numbers, a sentinel could be a letter A or a *, that is,\nsomething that is not a number. Our loop test is simply a test for the occurrence\nof the sentinel. When we nd it, we know we are done.'}"
"13 permit the
reuse of sequences of code (the loop body) by breaking the sequential instruction
execution ow.
Loop Control with a Sentinel
The example above controls the number of times
the loop body executes by means of a counter. We knew we wanted to execute the
loop 12 times, so we simply set a counter to 12, and then after each execution of
the loop, we decremented the counter and checked to see if it was zero. If it was
not zero, we set the PC to the start of the loop and continued with another iteration.
A second method for controlling the number of executions of a loop is to use
a sentinel. This method is particularly eective if we do not know ahead of time
how many iterations we will want to perform. Each iteration is usually based on
processing a value. We append to our sequence of values to be processed a value
that we know ahead of time can never occur (i.e., the sentinel). For example, if
we are adding a sequence of numbers, a sentinel could be a letter A or a *, that is,
something that is not a number. Our loop test is simply a test for the occurrence
of the sentinel. When we nd it, we know we are done.","{'page_number': 125, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '13 permit the\nreuse of sequences of code (the loop body) by breaking the sequential instruction\nexecution ow.\nLoop Control with a Sentinel\nThe example above controls the number of times\nthe loop body executes by means of a counter. We knew we wanted to execute the\nloop 12 times, so we simply set a counter to 12, and then after each execution of\nthe loop, we decremented the counter and checked to see if it was zero. If it was\nnot zero, we set the PC to the start of the loop and continued with another iteration.\nA second method for controlling the number of executions of a loop is to use\na sentinel. This method is particularly eective if we do not know ahead of time\nhow many iterations we will want to perform. Each iteration is usually based on\nprocessing a value. We append to our sequence of values to be processed a value\nthat we know ahead of time can never occur (i.e., the sentinel). For example, if\nwe are adding a sequence of numbers, a sentinel could be a letter A or a *, that is,\nsomething that is not a number. Our loop test is simply a test for the occurrence\nof the sentinel. When we nd it, we know we are done.'}"
"168
x
nzp x
Figure 5.15
A program that implements the algorithm of Figure 5.14.
Suppose we know the values stored in locations x3100 to x310B are all pos-
itive. Then we could use any negative number as a sentinel. Lets say the sentinel
stored at memory address x310C is 1. The resulting owchart for this solution
is shown in Figure 5.14, and the resulting program is shown in Figure 5.15.
As before, the instruction at x3000 loads R1 with the address of the rst value
to be added, and the instruction at x3001 initializes R3 (which keeps track of the
sum) to 0.
At x3002, we load the contents of the next memory location into R4. If the
sentinel is loaded, the N condition code is set.
The conditional branch at x3003 examines the N condition code. If N=1, PC
is loaded with x3008 and onto the next task. If N=0, R4 must contain a valid
number to be added. In this case, the number is added to R3 (x3004), R1 is
incremented to point to the next memory location (x3005), R4 is loaded with
the contents of the next memory location (x3006), and the PC is loaded with
x3003 to begin the next iteration (x3007).","{'page_number': 126, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '168\nx\nnzp x\nFigure 5.15\nA program that implements the algorithm of Figure 5.14.\nSuppose we know the values stored in locations x3100 to x310B are all pos-\nitive. Then we could use any negative number as a sentinel. Lets say the sentinel\nstored at memory address x310C is 1. The resulting owchart for this solution\nis shown in Figure 5.14, and the resulting program is shown in Figure 5.15.\nAs before, the instruction at x3000 loads R1 with the address of the rst value\nto be added, and the instruction at x3001 initializes R3 (which keeps track of the\nsum) to 0.\nAt x3002, we load the contents of the next memory location into R4. If the\nsentinel is loaded, the N condition code is set.\nThe conditional branch at x3003 examines the N condition code. If N=1, PC\nis loaded with x3008 and onto the next task. If N=0, R4 must contain a valid\nnumber to be added. In this case, the number is added to R3 (x3004), R1 is\nincremented to point to the next memory location (x3005), R4 is loaded with\nthe contents of the next memory location (x3006), and the PC is loaded with\nx3003 to begin the next iteration (x3007).'}"
"5.4.3 The JMP Instruction
The conditional branch instruction, for all its capability, does have one unfor-
tunate limitation. The next instruction executed must be within the range of
addresses that can be computed by adding the incremented PC to the sign-
extended oset obtained from bits [8:0] of the instruction. Since bits [8:0] specify
a 2s complement integer, the next instruction executed after the conditional
branch can be at most +256 or 255 locations from the branch instruction itself.
What if we would like to execute next an instruction that is 2000 locations
from the current instruction? We cannot t the value 2000 into the nine-bit eld;
ergo, the conditional branch instruction does not work.
The LC-3 ISA does provide an instruction JMP (opcode = 1100) that can do
the job.
ed
addr
1
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
JMP
BaseR
R2 contains the value x6600, and the PC contains x4000, then the instruction at
x4000 (the JMP instruction) will be executed, followed by the instruction located
at x6600. Since registers contain 16 bits (the full address space of memory), the
JMP instruction has no limitation on where the next instruction to be executed
must reside.
5.4.4 The TRAP Instruction
We will discuss the details of how the TRAP instruction works in Chapter 9.
However, because it will be useful long before that to get data into and out of the
computer, we discuss the TRAP instruction here. The TRAP (opcode = 1111)
instruction changes the PC to a memory address that is part of the operating
system so that the operating system will perform some task on behalf of the
program that is executing. In the language of operating system jargon, we say
the TRAP instruction invokes an operating system service call. Bits [7:0] of the
TRAP instruction form the tra vector an ei ht-bit code that identies the ser-
vice cal
Table
the LC-
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
1
1
1
1
0
0
0
0
trapvector
Once the operating system is nished performing the service call, the pro-
gram counter is set to the address of the instruction following the TRAP instruc-
tion, and the program continues. In this way, a program can, during its execution,","{'page_number': 127, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '5.4.3 The JMP Instruction\nThe conditional branch instruction, for all its capability, does have one unfor-\ntunate limitation. The next instruction executed must be within the range of\naddresses that can be computed by adding the incremented PC to the sign-\nextended oset obtained from bits [8:0] of the instruction. Since bits [8:0] specify\na 2s complement integer, the next instruction executed after the conditional\nbranch can be at most +256 or 255 locations from the branch instruction itself.\nWhat if we would like to execute next an instruction that is 2000 locations\nfrom the current instruction? We cannot t the value 2000 into the nine-bit eld;\nergo, the conditional branch instruction does not work.\nThe LC-3 ISA does provide an instruction JMP (opcode = 1100) that can do\nthe job.\ned\naddr\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\nJMP\nBaseR\nR2 contains the value x6600, and the PC contains x4000, then the instruction at\nx4000 (the JMP instruction) will be executed, followed by the instruction located\nat x6600. Since registers contain 16 bits (the full address space of memory), the\nJMP instruction has no limitation on where the next instruction to be executed\nmust reside.\n5.4.4 The TRAP Instruction\nWe will discuss the details of how the TRAP instruction works in Chapter 9.\nHowever, because it will be useful long before that to get data into and out of the\ncomputer, we discuss the TRAP instruction here. The TRAP (opcode = 1111)\ninstruction changes the PC to a memory address that is part of the operating\nsystem so that the operating system will perform some task on behalf of the\nprogram that is executing. In the language of operating system jargon, we say\nthe TRAP instruction invokes an operating system service call. Bits [7:0] of the\nTRAP instruction form the tra vector an ei ht-bit code that identies the ser-\nvice cal\nTable\nthe LC-\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n1\n1\n1\n1\n0\n0\n0\n0\ntrapvector\nOnce the operating system is nished performing the service call, the pro-\ngram counter is set to the address of the instruction following the TRAP instruc-\ntion, and the program continues. In this way, a program can, during its execution,'}"
"request services from the operating system and continue processing after each
such service is performed. The services we will require for now are
* Input a character from the keyboard (trapvector = x23).
* Output a character to the monitor (trapvector = x21).
* Halt the program (trapvector = x25).
5.5 Another Example: Counting
Occurrences of a Character
We will nish our introduction to the ISA of the LC-3 with another example
program. Suppose we would like to be able to input a character from the keyboard,
then count the number of occurrences of that character in a le, and nally display
that count on the monitor. We will simplify the problem by assuming that the
number of occurrences of any character that we would be interested in is small
enough that it can be expressed with a single decimal digit. That is, there will be
at most nine occurrences. This simplication allows us to not have to worry about
complex conversion routines between the binary count and the ASCII display on
the monitora subject we will get into in Chapter 10, but not today.
Figure 5.16 is a owchart of the algorithm that solves this problem. Note
that each step is expressed both in English and also (in parentheses) in terms of
an LC-3 implementation.
The rst step is (as always) to initialize all the variables. This means pro-
viding starting values (called initial values) for R0, R1, R2, and R3, the four
registers the computer will use to execute the program that will solve the prob-
lem. R2 will keep track of the number of occurrences; in Figure 5.16, it is referred
to as Count. It is initialized to zero. R3 will point to the next character in the le
that is being examined. We refer to it as a pointer since it points to (i.e., contains
the address of) the location where the next character of the le that we wish to
examine resides. The pointer is initialized with the address of the rst character
in the le. R0 will hold the character that is being counted; we will input that
character from the keyboard and put it in R0. R1 will hold, in turn, each character
that we get from the le being examined.
We should also note that there is no requirement that the le we are examining
be close to or far away from the program we are developing. For example, it is
perfectly reasonable for the program we are developing to start at x3000 and the
le we are examining to start at x9000. If that were the case, in the initialization
process, R3 would be initialized to x9000.
The next step is to count the number of occurrences of the input character.
This is done by processing, in turn, each character in the le being examined, until
the le is exhausted. Processing each character requires one iteration of a loop.
Recall from Section 5.4.3 that there are two common methods for keeping track
of iterations of a loop. We will use the sentinel method, using the ASCII code for
EOT (End of Transmission) (00000100) as the sentinel. A table of ASCII codes
is in Appendix E.","{'page_number': 128, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'request services from the operating system and continue processing after each\nsuch service is performed. The services we will require for now are\n* Input a character from the keyboard (trapvector = x23).\n* Output a character to the monitor (trapvector = x21).\n* Halt the program (trapvector = x25).\n5.5 Another Example: Counting\nOccurrences of a Character\nWe will nish our introduction to the ISA of the LC-3 with another example\nprogram. Suppose we would like to be able to input a character from the keyboard,\nthen count the number of occurrences of that character in a le, and nally display\nthat count on the monitor. We will simplify the problem by assuming that the\nnumber of occurrences of any character that we would be interested in is small\nenough that it can be expressed with a single decimal digit. That is, there will be\nat most nine occurrences. This simplication allows us to not have to worry about\ncomplex conversion routines between the binary count and the ASCII display on\nthe monitora subject we will get into in Chapter 10, but not today.\nFigure 5.16 is a owchart of the algorithm that solves this problem. Note\nthat each step is expressed both in English and also (in parentheses) in terms of\nan LC-3 implementation.\nThe rst step is (as always) to initialize all the variables. This means pro-\nviding starting values (called initial values) for R0, R1, R2, and R3, the four\nregisters the computer will use to execute the program that will solve the prob-\nlem. R2 will keep track of the number of occurrences; in Figure 5.16, it is referred\nto as Count. It is initialized to zero. R3 will point to the next character in the le\nthat is being examined. We refer to it as a pointer since it points to (i.e., contains\nthe address of) the location where the next character of the le that we wish to\nexamine resides. The pointer is initialized with the address of the rst character\nin the le. R0 will hold the character that is being counted; we will input that\ncharacter from the keyboard and put it in R0. R1 will hold, in turn, each character\nthat we get from the le being examined.\nWe should also note that there is no requirement that the le we are examining\nbe close to or far away from the program we are developing. For example, it is\nperfectly reasonable for the program we are developing to start at x3000 and the\nle we are examining to start at x9000. If that were the case, in the initialization\nprocess, R3 would be initialized to x9000.\nThe next step is to count the number of occurrences of the input character.\nThis is done by processing, in turn, each character in the le being examined, until\nthe le is exhausted. Processing each character requires one iteration of a loop.\nRecall from Section 5.4.3 that there are two common methods for keeping track\nof iterations of a loop. We will use the sentinel method, using the ASCII code for\nEOT (End of Transmission) (00000100) as the sentinel. A table of ASCII codes\nis in Appendix E.'}"
"172
x3013
0
0
0
0
0
0
0 0 0 0 1 1 0 0 0 0 ASCII TEMPLATE
Figure 5.17
A machine language program that implements the algorithm of Figure 5.16.
In each iteration of the loop, the contents of R1 is rst compared to the ASCII
code for EOT. If they are equal, the loop is exited, and the program moves on to
the nal step, displaying on the screen the number of occurrences. If not, there is
work to do. R1 (the current character under examination) is compared to R0 (the
character input from the keyboard). If they match, R2 is incremented. In either
case, we move on to getting the next character. The pointer R3 is incremented, the
next character is loaded into R1, and the program returns to the test that checks
for the sentinel at the end of the le.
When the end of the file is reached, all the characters have been examined, and
the count is contained as a binary number in R2. In order to display the count on the
monitor, it is first converted to an ASCII code. Since we have assumed the count
is less than 10, we can do this by putting a leading 0011 in front of the four-bit
binary representation of the count. Note in Figure E.2 the relationship between the
binaryvalueofeachdecimaldigitbetween0and9anditscorrespondingASCIIcode.
Finally, the count is output to the monitor, and the program terminates.
Figure 5.17 is a machine language program that implements the owchart of
Figure 5.16.
First the initialization steps. The instruction at x3000 clears R2 by ANDing
it with x0000. The instruction at x3001 loads the starting address of the le to be
examined into R3. Again, we note that this le can be anywhere in memory. Prior
to starting execution at x3000, some sequence of instructions must have stored the
rst address of this le in x3012. Location x3002 contains the TRAP instruction,","{'page_number': 129, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '172\nx3013\n0\n0\n0\n0\n0\n0\n0 0 0 0 1 1 0 0 0 0 ASCII TEMPLATE\nFigure 5.17\nA machine language program that implements the algorithm of Figure 5.16.\nIn each iteration of the loop, the contents of R1 is rst compared to the ASCII\ncode for EOT. If they are equal, the loop is exited, and the program moves on to\nthe nal step, displaying on the screen the number of occurrences. If not, there is\nwork to do. R1 (the current character under examination) is compared to R0 (the\ncharacter input from the keyboard). If they match, R2 is incremented. In either\ncase, we move on to getting the next character. The pointer R3 is incremented, the\nnext character is loaded into R1, and the program returns to the test that checks\nfor the sentinel at the end of the le.\nWhen the end of the file is reached, all the characters have been examined, and\nthe count is contained as a binary number in R2. In order to display the count on the\nmonitor, it is first converted to an ASCII code. Since we have assumed the count\nis less than 10, we can do this by putting a leading 0011 in front of the four-bit\nbinary representation of the count. Note in Figure E.2 the relationship between the\nbinaryvalueofeachdecimaldigitbetween0and9anditscorrespondingASCIIcode.\nFinally, the count is output to the monitor, and the program terminates.\nFigure 5.17 is a machine language program that implements the owchart of\nFigure 5.16.\nFirst the initialization steps. The instruction at x3000 clears R2 by ANDing\nit with x0000. The instruction at x3001 loads the starting address of the le to be\nexamined into R3. Again, we note that this le can be anywhere in memory. Prior\nto starting execution at x3000, some sequence of instructions must have stored the\nrst address of this le in x3012. Location x3002 contains the TRAP instruction,'}"
"which requests the operating system to perform a service call on behalf of this pro-
gram. The function requested, as identied by the eight-bit trapvector 00100011
(i.e., x23), is to load into R0 the ASCII code of the next character typed on the
keyboard. Table A.2 lists trapvectors for all operating system service calls that
can be performed on behalf of a user program. The instruction at x3003 loads the
character pointed to by R3 into R1.
Then the process of examining characters begins. We start (x3004) by sub-
tracting 4 (the ASCII code for EOT) from R1 and storing it in R4. If the result
is zero, the end of the le has been reached, and it is time to output the count.
The instruction at x3005 conditionally branches to x300E, where the process of
outputting the count begins.
If R4 is not equal to zero, the character in R1 is legitimate and must be
examined. The sequence of instructions at locations x3006, x3007, and x3008
determines whether the contents of R1 and R0 are identical. Taken together, the
three instructions compute
R0  R1
This produces all zeros only if the bit patterns of R1 and R0 are identical. If the
bit patterns are not identical, the conditional branch at x3009 branches to x300B;
that is, it skips the instruction at x300A, which increments the counter (R2).
The instruction at x300B increments R3, so it will point to the next charac-
ter in the le being examined, the instruction at x300C loads that character into
R1, and the instruction at x300D unconditionally takes us back to x3004 to start
processing that character.
When the sentinel (EOT) is nally detected, the process of outputting the
count begins (at x300E). The instruction at x300E loads 00110000 into R0, and
the instruction at x300F adds the count to R0. This converts the binary represen-
tation of the count (in R2) to the ASCII representation of the count (in R0). The
instruction at x3010 invokes a TRAP to the operating system to output the con-
tents of R0 to the monitor. When that is done and the program resumes execution,
the instruction at x3011 invokes a TRAP instruction to terminate the program.
Question: Can you improve the execution of the above program? Hint: How
many times are the instructions at x3006 and x3007 executed. What small change
will decrease the total number of instructions that have to be executed.
5.6 The Data Path Revisited
Before we leave Chapter 5, let us revisit the data path diagram that we rst
encountered in Chapter 3 (Figure 3.35). Many of the structures we have seen
earlier in this chapter in Figures 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, and 5.11. We repro-
duce the data path diagram as Figure 5.18","{'page_number': 130, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '5.6 The Data Path Revisited\nBefore we leave Chapter 5, let us revisit the data path diagram that we rst\nencountered in Chapter 3 (Figure 3.35). Many of the structures we have seen\nearlier in this chapter in Figures 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, and 5.11. We repro-\nduce the data path diagram as Figure 5.18. Note at the outset that there are two\nkinds of arrows in the data path, those with arrowheads lled in and those with\narrowheads not lled in. Filled-in arrowheads designate information that is pro-\ncessed. Unlled-in arrowheads designate control signals. Control signals emanate\nfrom the block labeled Finite State Machine. The connections from the nite\nstate machine to most control signals have been left o Figure 5.18 to reduce\nunnecessary clutter in the diagram.'}"
"5.6 The Data Path Revisited
Before we leave Chapter 5, let us revisit the data path diagram that we rst
encountered in Chapter 3 (Figure 3.35). Many of the structures we have seen
earlier in this chapter in Figures 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, and 5.11. We repro-
duce the data path diagram as Figure 5.18. Note at the outset that there are two
kinds of arrows in the data path, those with arrowheads lled in and those with
arrowheads not lled in. Filled-in arrowheads designate information that is pro-
cessed. Unlled-in arrowheads designate control signals. Control signals emanate
from the block labeled Finite State Machine. The connections from the nite
state machine to most control signals have been left o Figure 5.18 to reduce
unnecessary clutter in the diagram.","{'page_number': 130, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '5.6 The Data Path Revisited\nBefore we leave Chapter 5, let us revisit the data path diagram that we rst\nencountered in Chapter 3 (Figure 3.35). Many of the structures we have seen\nearlier in this chapter in Figures 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, and 5.11. We repro-\nduce the data path diagram as Figure 5.18. Note at the outset that there are two\nkinds of arrows in the data path, those with arrowheads lled in and those with\narrowheads not lled in. Filled-in arrowheads designate information that is pro-\ncessed. Unlled-in arrowheads designate control signals. Control signals emanate\nfrom the block labeled Finite State Machine. The connections from the nite\nstate machine to most control signals have been left o Figure 5.18 to reduce\nunnecessary clutter in the diagram.'}"
"5.6.1 Basic Components of the Data Path
5.6.1.1 The Global Bus
The most obvious item on the data path diagram is the heavy black structure
with arrowheads at both ends. This represents the data paths global bus. The
LC-3 global bus consists of 16 wires and associated electronics. It allows one
structure to transfer up to 16 bits of information to another structure by making the
necessary electronic connections on the bus. Exactly one value can be transferred
on the bus at one time. Note that each structure that supplies values to the bus
has a triangle just behind its input arrow to the bus. This triangle (called a tri-
state device) allows the computers control logic to enable exactly one supplier to
provide information to the bus at any one time. The structure wishing to obtain the
value being supplied can do so by asserting its LD.x (load enable) signal (recall
our discussion of gated latches in Section 3.4.2). Not all computers have a single
global bus. The pros and cons of a single global bus is yet another topic that will
have to wait for later in your education.
5.6.1.2 Memory
One of the most important parts of any computer is the memory that contains
both instructions and data. Memory is accessed by loading the memory address
register (MAR) with the address of the location to be accessed. To perform a load,
control signals then read the contents of that memory location, and the result of
that read is delivered by the memory to the memory data register (MDR). On the
other hand, to perform a store, what is to be stored is loaded into the MDR. Then
the control signals assert a write enable (WE) signal in order to store the value
contained in MDR in the memory location specied by MAR.
5.6.1.3 The ALU and the Register File
The ALU is the processing element. It has two inputs, source 1 from a register and
source 2 from either a register or the sign-extended immediate value provided by
the instruction. The registers (R0 through R7) can provide two values: source 1,
which is controlled by the three-bit register number SR1, and source 2, which is
controlled by the three-bit register number SR2. SR1 and SR2 are elds in the
LC-3 operate instructions. The selection of a second register operand or a sign-
extended immediate operand is determined by bit [5] of the LC-3 instruction.
Note the mux that provides source 2 to the ALU. The select line of that mux is
bit [5] of the LC-3 operate instruction.
The results of an ALU operation are (a) a result that is stored in one of the
registers, and (b) the three single-bit condition codes. Note that the ALU can supply
16 bits to the bus, and that value can then be written into the register specified by the
three-bit register number DR. Also, note that the 16 bits supplied to the bus are also
input to logic that determines whether that 16-bit value is negative, zero, or positive.
The three one-bit condition code registers N, Z, and P are set accordingly.
5.6.1","{'page_number': 131, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Note that the ALU can supply\n16 bits to the bus, and that value can then be written into the register specified by the\nthree-bit register number DR. Also, note that the 16 bits supplied to the bus are also\ninput to logic that determines whether that 16-bit value is negative, zero, or positive.\nThe three one-bit condition code registers N, Z, and P are set accordingly.\n5.6.1.4 The PC and the PCMUX\nAt the start of each instruction cycle, the PC supplies to the MAR over the\nglobal bus the address of the instruction to be fetched. In addition, the PC, in\nturn, is supplied via the three-to-one PCMUX. During the FETCH phase of the'}"
"Note that the ALU can supply
16 bits to the bus, and that value can then be written into the register specified by the
three-bit register number DR. Also, note that the 16 bits supplied to the bus are also
input to logic that determines whether that 16-bit value is negative, zero, or positive.
The three one-bit condition code registers N, Z, and P are set accordingly.
5.6.1.4 The PC and the PCMUX
At the start of each instruction cycle, the PC supplies to the MAR over the
global bus the address of the instruction to be fetched. In addition, the PC, in
turn, is supplied via the three-to-one PCMUX. During the FETCH phase of the","{'page_number': 131, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Note that the ALU can supply\n16 bits to the bus, and that value can then be written into the register specified by the\nthree-bit register number DR. Also, note that the 16 bits supplied to the bus are also\ninput to logic that determines whether that 16-bit value is negative, zero, or positive.\nThe three one-bit condition code registers N, Z, and P are set accordingly.\n5.6.1.4 The PC and the PCMUX\nAt the start of each instruction cycle, the PC supplies to the MAR over the\nglobal bus the address of the instruction to be fetched. In addition, the PC, in\nturn, is supplied via the three-to-one PCMUX. During the FETCH phase of the'}"
"instruction cycle, the PC is incremented and written into the PC. That is shown
as the rightmost input to the PCMUX.
If the current instruction is a control instruction, then the relevant source of
the PCMUX depends on which control instruction is currently being processed.
If the current instruction is a conditional branch and the branch is taken, then the
PC is loaded with the incremented PC + PCoset (the 16-bit value obtained by
sign-extending IR [8:0]). Note that this addition takes place in the special adder
and not in the ALU. The output of the adder is the middle input to PCMUX. The
third input to PCMUX is obtained from the global bus. Its use will become clear
after we discuss other control instructions in Chapters 9.
5.6.1.5 The MARMUX
As you know, memory is accessed by supplying the address to the MAR. The
MARMUX controls which of two sources will supply the MAR with the appro-
priate address during the execution of a load, a store, or a TRAP instruction. The
right input to the MARMUX is obtained by adding either the incremented PC or
a base register to zero or a literal value supplied by the IR. Whether the PC or a
base register and what literal value depends on which opcode is being processed.
The control signal ADDR1MUX species the PC or base register. The control
signal ADDR2MUX species which of four values is to be added. The left input
to MARMUX provides the zero-extended trapvector, which is needed to invoke
service calls, and will be discussed in detail in Chapter 9.
5.6.2 The Instruction Cycle Specic to the LC-3
We co
instruct
locatio
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
0
1
1
0
0
1
1
0
1
0
0
0
0
1
0
0
LDR
R3
R2
4
Suppose the LC-3 has just completed processing the instruction at x3455, which
happened to be an ADD instruction.
5.6.2.1 FETCH
As you know, the instruction cycle starts with the FETCH phase. That is, the
instruction is obtained by accessing memory with the address contained in the PC.
In the rst cycle, the contents of the PC is loaded via the global bus into the MAR,
and the PC is incremented and loaded into the PC. At the end of this cycle, the
PC contains x3457. In the next cycle (if memory can provide information in one
cycle), the memory is read, and the instruction 0110011010000100 is loaded into
the MDR. In the next cycle, the contents of the MDR is loaded into the instruction
register (IR), completing the FETCH phase.
5.6.2.2 DECODE
In the next cycle, the contents of the IR is decoded, resulting in the control
logic providing the correct control signals (unlled arrowheads) to control the","{'page_number': 132, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'instruction cycle, the PC is incremented and written into the PC. That is shown\nas the rightmost input to the PCMUX.\nIf the current instruction is a control instruction, then the relevant source of\nthe PCMUX depends on which control instruction is currently being processed.\nIf the current instruction is a conditional branch and the branch is taken, then the\nPC is loaded with the incremented PC + PCoset (the 16-bit value obtained by\nsign-extending IR [8:0]). Note that this addition takes place in the special adder\nand not in the ALU. The output of the adder is the middle input to PCMUX. The\nthird input to PCMUX is obtained from the global bus. Its use will become clear\nafter we discuss other control instructions in Chapters 9.\n5.6.1.5 The MARMUX\nAs you know, memory is accessed by supplying the address to the MAR. The\nMARMUX controls which of two sources will supply the MAR with the appro-\npriate address during the execution of a load, a store, or a TRAP instruction. The\nright input to the MARMUX is obtained by adding either the incremented PC or\na base register to zero or a literal value supplied by the IR. Whether the PC or a\nbase register and what literal value depends on which opcode is being processed.\nThe control signal ADDR1MUX species the PC or base register. The control\nsignal ADDR2MUX species which of four values is to be added. The left input\nto MARMUX provides the zero-extended trapvector, which is needed to invoke\nservice calls, and will be discussed in detail in Chapter 9.\n5.6.2 The Instruction Cycle Specic to the LC-3\nWe co\ninstruct\nlocatio\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\nLDR\nR3\nR2\n4\nSuppose the LC-3 has just completed processing the instruction at x3455, which\nhappened to be an ADD instruction.\n5.6.2.1 FETCH\nAs you know, the instruction cycle starts with the FETCH phase. That is, the\ninstruction is obtained by accessing memory with the address contained in the PC.\nIn the rst cycle, the contents of the PC is loaded via the global bus into the MAR,\nand the PC is incremented and loaded into the PC. At the end of this cycle, the\nPC contains x3457. In the next cycle (if memory can provide information in one\ncycle), the memory is read, and the instruction 0110011010000100 is loaded into\nthe MDR. In the next cycle, the contents of the MDR is loaded into the instruction\nregister (IR), completing the FETCH phase.\n5.6.2.2 DECODE\nIn the next cycle, the contents of the IR is decoded, resulting in the control\nlogic providing the correct control signals (unlled arrowheads) to control the'}"
"5.6.2.4 OPERAND FETCH
In the next cycle (or more than one, if memory access takes more than one cycle),
the value at that address is loaded into the MDR.
5.6.2.5 EXECUTE
The LDR instruction does not require an EXECUTE phase, so this phase takes
zero cycles.
5.
5.
5.
5.
same way as in the LC-3.","{'page_number': 133, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '5.6.2.4 OPERAND FETCH\nIn the next cycle (or more than one, if memory access takes more than one cycle),\nthe value at that address is loaded into the MDR.\n5.6.2.5 EXECUTE\nThe LDR instruction does not require an EXECUTE phase, so this phase takes\nzero cycles.\n5.\n5.\n5.\n5.\nsame way as in the LC-3.'}"
"ramming
W
e are now ready to develop programs to solve problems with the com-
puter. In this chapter we attempt to do two things: rst, we develop a
methodology for constructing programs to solve problems (Section 6.1, Problem
Solving), and second, we develop a methodology for xing those programs (Sec-
tion 6.2, Debugging) under the likely condition that we did not get everything
right the rst time.
There is a long tradition that the errors present in programs are referred to as
bugs, and the process of removing those errors is called debugging. The opportu-
nities for introducing bugs into a complicated program are so great that it usually
takes much more time to get the program to work correctly (debugging) than it
does to create the program in the rst place.
6.1 Problem Solving
6.1.1 Systematic Decomposition
Recall from Chapter 1 that in order for electrons to solve a problem, we need to go
through several levels of transformation to get from a natural language description
of the problem (in our case English, although many of you might prefer Italian,
Mandarin, Hindi, or something else) to something electrons can deal with. Once
we have a natural language description of the problem, the next step is to trans-
form the problem statement into an algorithm. That is, the next step is to transform
the problem statement into a step-by-step procedure that has the properties of def-
initeness (each step is precisely stated), eective computability (each step can be
carried out by a computer), and niteness (the procedure terminates).
In the late 1960s, the concept of structured programming emerged as a way
to dramatically improve the ability of average programmers to take a complex
description of a problem and systematically decompose it into smaller and smaller
manageable units so that they could ultimately write a program that executed cor-
rectly. The methodology has also been called systematic decomposition because
the larger tasks are systematically broken down into smaller ones.","{'page_number': 134, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'ramming\nW\ne are now ready to develop programs to solve problems with the com-\nputer. In this chapter we attempt to do two things: rst, we develop a\nmethodology for constructing programs to solve problems (Section 6.1, Problem\nSolving), and second, we develop a methodology for xing those programs (Sec-\ntion 6.2, Debugging) under the likely condition that we did not get everything\nright the rst time.\nThere is a long tradition that the errors present in programs are referred to as\nbugs, and the process of removing those errors is called debugging. The opportu-\nnities for introducing bugs into a complicated program are so great that it usually\ntakes much more time to get the program to work correctly (debugging) than it\ndoes to create the program in the rst place.\n6.1 Problem Solving\n6.1.1 Systematic Decomposition\nRecall from Chapter 1 that in order for electrons to solve a problem, we need to go\nthrough several levels of transformation to get from a natural language description\nof the problem (in our case English, although many of you might prefer Italian,\nMandarin, Hindi, or something else) to something electrons can deal with. Once\nwe have a natural language description of the problem, the next step is to trans-\nform the problem statement into an algorithm. That is, the next step is to transform\nthe problem statement into a step-by-step procedure that has the properties of def-\niniteness (each step is precisely stated), eective computability (each step can be\ncarried out by a computer), and niteness (the procedure terminates).\nIn the late 1960s, the concept of structured programming emerged as a way\nto dramatically improve the ability of average programmers to take a complex\ndescription of a problem and systematically decompose it into smaller and smaller\nmanageable units so that they could ultimately write a program that executed cor-\nrectly. The methodology has also been called systematic decomposition because\nthe larger tasks are systematically broken down into smaller ones.'}"
"We will nd the systematic decomposition model a useful technique for
designing computer programs to carry out complex tasks.
6.1.2 The Three Constructs: Sequential, Conditional, Iterative
Systematic decomposition is the process of taking a task, that is, a unit of work
(see Figure 6.1a), and breaking it into smaller units of work such that the collec-
tion of smaller units carries out the same task as the one larger unit. The idea is
that if one starts with a large, complex task and applies this process again and
again, one will end up with very small units of work and consequently be able to
easily write a program to carry out each of these small units of work. The process
is also referred to as stepwise renement, because the process is applied one step
at a time, and each step renes one of the tasks that is still too complex into a
collection of simpler subtasks.
The idea is to replace each larger unit of work with a construct that correctly
decomposes it. There are basically three constructs for doing this: sequential,
conditional, and iterative.
The sequential construct (Figure 6.1b) is the one to use if the designated
task can be broken down into two subtasks, one following the other. That is, the
computer is to carry out the rst subtask completely, then go on and carry out the
second subtask completely never going back to the rst subtask after starting
Figure 6.1
The basic constructs of structured programming.","{'page_number': 135, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'We will nd the systematic decomposition model a useful technique for\ndesigning computer programs to carry out complex tasks.\n6.1.2 The Three Constructs: Sequential, Conditional, Iterative\nSystematic decomposition is the process of taking a task, that is, a unit of work\n(see Figure 6.1a), and breaking it into smaller units of work such that the collec-\ntion of smaller units carries out the same task as the one larger unit. The idea is\nthat if one starts with a large, complex task and applies this process again and\nagain, one will end up with very small units of work and consequently be able to\neasily write a program to carry out each of these small units of work. The process\nis also referred to as stepwise renement, because the process is applied one step\nat a time, and each step renes one of the tasks that is still too complex into a\ncollection of simpler subtasks.\nThe idea is to replace each larger unit of work with a construct that correctly\ndecomposes it. There are basically three constructs for doing this: sequential,\nconditional, and iterative.\nThe sequential construct (Figure 6.1b) is the one to use if the designated\ntask can be broken down into two subtasks, one following the other. That is, the\ncomputer is to carry out the rst subtask completely, then go on and carry out the\nsecond subtask completely never going back to the rst subtask after starting\nFigure 6.1\nThe basic constructs of structured programming.'}"
"condition is true, the computer is to carry out one subtask. If the condition is not true,
the computer is to carry out a different subtask. Either subtask may be vacuous; that
is, it may do nothing. Regardless, after the correct subtask is completed, the program
moves onward. The program never goes back and retests the condition.
The iterative construct (Figure 6.1d) is the one to use if the task consists of
doing a subtask a number of times, but only as long as some condition is true. If
the condition is true, do the subtask. After the subtask is nished, go back and
test the condition again. As long as the result of the condition tested is true, the
program continues to carry out the same subtask again and again. The rst time
the test is not true, the program proceeds onward.
Note in Figure 6.1 that whatever the task of Figure 6.1a, work starts with the
arrow into the top of the box representing the task and finishes with the arrow out
of the bottom of the box. There is no mention of what goes oninside the box. In each
of the three possible decompositions of Figure 6.1a (i.e., Figure 6.1b, c, and d), there
is exactly one entrance into the construct and exactly one exit out of the construct.
Thus, it is easy to replace any task of the form of Figure 6.1a with whichever of its
three decompositions apply. We will see how with several examples.
6.1.3 LC-3 Control Instructions to Implement
Figure 6.2
Use of LC-3 control instructions to implement structured programming.","{'page_number': 136, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'condition is true, the computer is to carry out one subtask. If the condition is not true,\nthe computer is to carry out a different subtask. Either subtask may be vacuous; that\nis, it may do nothing. Regardless, after the correct subtask is completed, the program\nmoves onward. The program never goes back and retests the condition.\nThe iterative construct (Figure 6.1d) is the one to use if the task consists of\ndoing a subtask a number of times, but only as long as some condition is true. If\nthe condition is true, do the subtask. After the subtask is nished, go back and\ntest the condition again. As long as the result of the condition tested is true, the\nprogram continues to carry out the same subtask again and again. The rst time\nthe test is not true, the program proceeds onward.\nNote in Figure 6.1 that whatever the task of Figure 6.1a, work starts with the\narrow into the top of the box representing the task and finishes with the arrow out\nof the bottom of the box. There is no mention of what goes oninside the box. In each\nof the three possible decompositions of Figure 6.1a (i.e., Figure 6.1b, c, and d), there\nis exactly one entrance into the construct and exactly one exit out of the construct.\nThus, it is easy to replace any task of the form of Figure 6.1a with whichever of its\nthree decompositions apply. We will see how with several examples.\n6.1.3 LC-3 Control Instructions to Implement\nFigure 6.2\nUse of LC-3 control instructions to implement structured programming.'}"
"decomposition constructs. That is, Figure 6.2b, c, and d corresponds respectively
to the three constructs shown in Figure 6.1b, c, and d.
We use the letters A, B, C, and D to represent addresses in memory containing
LC-3 instructions. The letter A, for example, represents the address of the rst
LC-3 instruction to be executed in all three cases, since it is the starting address
of the task to be decomposed (shown in Figure 6.2a).
Figure 6.2b illustrates the control ow of the sequential decomposition. Note
that no control instructions are needed since the PC is incremented from Address
B1 to Address B1+1. The program continues to execute instructions through
address D1. It does not return to the rst subtask.
Figure 6.2c illustrates the control ow of the conditional decomposition.
First, a condition is generated, resulting in the setting of one of the condition
codes. This condition is tested by the conditional branch instruction at Address
B2. If the condition is true, the PC is set to Address C2+1, and subtask 1 is
executed. (Note: x equals 1 + the number of instructions in subtask 2.) If the
condition is false, the PC (which had been incremented during the FETCH phase
of the branch instruction) fetches the instruction at Address B2+1, and subtask
2 is executed. Subtask 2 terminates in a branch instruction that at address C2
unconditionally branches to D2+1. (Note: y equals the number of instructions in
subtask 1.)
Figure 6.2d illustrates the control ow of the iterative decomposition. As
in the case of the conditional construct, rst a condition is generated, a condi-
tion code is set, and a conditional branch instruction is executed. In this case,
the condition bits of the instruction at address B3 are set to cause a conditional
branch if the condition generated is false. If the condition is false, the PC is set
to address D3+1. (Note: z equals 1 + the number of instructions in the subtask
in Figure 6.2d.) On the other hand, as long as the condition is true, the PC will
be incremented to B3+1, and the subtask will be executed. The subtask termi-
nates in an unconditional branch instruction at address D3, which sets the PC to
A to again generate and test the condition. (Note: w equals the total number of
instructions in the decomposition shown as Figure 6.2d.)
Now, we are ready to move on to an example.
6.1.4 The Character Count Example from Chapter 5, Revisited
Recall the example of Section 5.5. The statement of the problem is as follows:
We wish to input a character from the keyboard, count the number of occurrences
of that character in a le, and display that count on the monitor.
The systematic decomposition of this English language statement of the
problem to the nal LC-3 implementation is shown in Figure 6.3. Figure 6.3a
is a brief statement of the problem","{'page_number': 137, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '1.4 The Character Count Example from Chapter 5, Revisited\nRecall the example of Section 5.5. The statement of the problem is as follows:\nWe wish to input a character from the keyboard, count the number of occurrences\nof that character in a le, and display that count on the monitor.\nThe systematic decomposition of this English language statement of the\nproblem to the nal LC-3 implementation is shown in Figure 6.3. Figure 6.3a\nis a brief statement of the problem.\nIn order to solve the problem, it is always a good idea rst to examine exactly\nwhat is being asked for, and what is available to help solve the problem. In this\ncase, the statement of the problem says that we will get the character of inter-\nest from the keyboard, and that we must examine all the characters in a le and\ndetermine how many are identical to the character obtained from the keyboard.\nFinally, we must output the result.'}"
"1.4 The Character Count Example from Chapter 5, Revisited
Recall the example of Section 5.5. The statement of the problem is as follows:
We wish to input a character from the keyboard, count the number of occurrences
of that character in a le, and display that count on the monitor.
The systematic decomposition of this English language statement of the
problem to the nal LC-3 implementation is shown in Figure 6.3. Figure 6.3a
is a brief statement of the problem.
In order to solve the problem, it is always a good idea rst to examine exactly
what is being asked for, and what is available to help solve the problem. In this
case, the statement of the problem says that we will get the character of inter-
est from the keyboard, and that we must examine all the characters in a le and
determine how many are identical to the character obtained from the keyboard.
Finally, we must output the result.","{'page_number': 137, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '1.4 The Character Count Example from Chapter 5, Revisited\nRecall the example of Section 5.5. The statement of the problem is as follows:\nWe wish to input a character from the keyboard, count the number of occurrences\nof that character in a le, and display that count on the monitor.\nThe systematic decomposition of this English language statement of the\nproblem to the nal LC-3 implementation is shown in Figure 6.3. Figure 6.3a\nis a brief statement of the problem.\nIn order to solve the problem, it is always a good idea rst to examine exactly\nwhat is being asked for, and what is available to help solve the problem. In this\ncase, the statement of the problem says that we will get the character of inter-\nest from the keyboard, and that we must examine all the characters in a le and\ndetermine how many are identical to the character obtained from the keyboard.\nFinally, we must output the result.'}"
"Figure 6.3
Stepwise renement of the character count program (Fig. 6.3 continued on
next page.)
To do this, we will need to examine in turn all the characters in a le, we will
need to compare each to the character we input from the keyboard, and we will
need a counter to increment each time we get a match.
We will need registers to hold all these pieces of information:
1. The character input from the keyboard.
2. Where we are (a pointer) in our scan of the le.
3. The character in the le that is currently being examined.
4. The count of the number of occurrences.
We will also need to know when we have reached the end of the le.","{'page_number': 138, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure 6.3\nStepwise renement of the character count program (Fig. 6.3 continued on\nnext page.)\nTo do this, we will need to examine in turn all the characters in a le, we will\nneed to compare each to the character we input from the keyboard, and we will\nneed a counter to increment each time we get a match.\nWe will need registers to hold all these pieces of information:\n1. The character input from the keyboard.\n2. Where we are (a pointer) in our scan of the le.\n3. The character in the le that is currently being examined.\n4. The count of the number of occurrences.\nWe will also need to know when we have reached the end of the le.'}"
"208
Figure 6.3
Stepwise renement of the character count program (Fig. 6.3 continued on
next page.)
The problem decomposes naturally (using the sequential construct) into three
parts as shown in Figure 6.3b: (A) initialization, which includes keyboard input
of the character to be counted, (B) the process of determining how many occur-
rences of the character are present in the le, and (C) displaying the count on the
monitor.
We have seen the importance of proper initialization in several examples
already. Before a computer program can get to the crux of the problem, it must
have the correct initial values. These initial values do not just show up in the GPRs
by magic. They get there as a result of the rst set of steps in every algorithm: the
initialization of its variables.
In this particular algorithm, initialization (as we said in Chapter 5) consists
of starting the counter at 0, setting the pointer to the address of the rst character
in the le to be examined, getting an input character from the keyboard, and get-
ting the rst character from the le. Collectively, these four steps comprise the
initialization of the algorithm shown in Figure 6.3b as A.","{'page_number': 139, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '208\nFigure 6.3\nStepwise renement of the character count program (Fig. 6.3 continued on\nnext page.)\nThe problem decomposes naturally (using the sequential construct) into three\nparts as shown in Figure 6.3b: (A) initialization, which includes keyboard input\nof the character to be counted, (B) the process of determining how many occur-\nrences of the character are present in the le, and (C) displaying the count on the\nmonitor.\nWe have seen the importance of proper initialization in several examples\nalready. Before a computer program can get to the crux of the problem, it must\nhave the correct initial values. These initial values do not just show up in the GPRs\nby magic. They get there as a result of the rst set of steps in every algorithm: the\ninitialization of its variables.\nIn this particular algorithm, initialization (as we said in Chapter 5) consists\nof starting the counter at 0, setting the pointer to the address of the rst character\nin the le to be examined, getting an input character from the keyboard, and get-\nting the rst character from the le. Collectively, these four steps comprise the\ninitialization of the algorithm shown in Figure 6.3b as A.'}"
"Figure 6.3
Stepwise renement of the character count program (continued Fig. 6.3
from previous page.)
Figure 6.3c decomposes B into an iteration construct, such that as long as
there are characters in the le to examine, the loop iterates. B1 shows what gets
accomplished in each iteration. The character is tested and the count incremented
if there is a match. Then the next character is prepared for examination. Recall
from Chapter 5 that there are two basic techniques for controlling the number","{'page_number': 140, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure 6.3\nStepwise renement of the character count program (continued Fig. 6.3\nfrom previous page.)\nFigure 6.3c decomposes B into an iteration construct, such that as long as\nthere are characters in the le to examine, the loop iterates. B1 shows what gets\naccomplished in each iteration. The character is tested and the count incremented\nif there is a match. Then the next character is prepared for examination. Recall\nfrom Chapter 5 that there are two basic techniques for controlling the number'}"
"of iterations of a loop: the sentinel method and the use of a counter. Since we
are unlikely to know how many characters there are in a random le, and since
each le ends with an end of text (EOT) character, our choice is obvious. We use
the sentinel method, that is, testing each character to see if we are examining a
character in the le or the EOT character.
Figure 6.3c also shows the initialization step in greater detail. Four LC-3
registers (R0, R1, R2, and R3) have been specied to handle the four requirements
of the algorithm: the input character from the keyboard, the current character
being tested, the counter, and the pointer to the next character to be tested.
Figure 6.3d decomposes both B1 and C using the sequential construct in both
cases. In the case of B1, rst the current character is tested (B2), and the counter
incremented if we have a match, and then the next character is fetched (B3). In
the case of C, rst the count is prepared for display by converting it from a 2s
complement integer to an ASCII code (C1), and then the actual character output
is performed (C2).
6.2 Debugging
Debugging a program is pretty much applied common sense. A simple example
comes to mind: You are driving to a place you have never visited, and somewhere
along the way you made a wrong turn. What do you do now? One common driv-
ing debugging technique is to wander aimlessly, hoping to nd your way back.
When that does not work, and you are nally willing to listen to the person sitting
next to you, you turn around and return to some known position on the route.
Then, using a map (very dicult for some people), you follow the directions pro-
vided, periodically comparing where you are (from landmarks you see out the
window) with where the map says you should be, until you reach your desired
destination.
Debugging is somewhat like that. A logical error in a program can make
you take a wrong turn. The simplest way to keep track of where you are as","{'page_number': 141, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'of iterations of a loop: the sentinel method and the use of a counter. Since we\nare unlikely to know how many characters there are in a random le, and since\neach le ends with an end of text (EOT) character, our choice is obvious. We use\nthe sentinel method, that is, testing each character to see if we are examining a\ncharacter in the le or the EOT character.\nFigure 6.3c also shows the initialization step in greater detail. Four LC-3\nregisters (R0, R1, R2, and R3) have been specied to handle the four requirements\nof the algorithm: the input character from the keyboard, the current character\nbeing tested, the counter, and the pointer to the next character to be tested.\nFigure 6.3d decomposes both B1 and C using the sequential construct in both\ncases. In the case of B1, rst the current character is tested (B2), and the counter\nincremented if we have a match, and then the next character is fetched (B3). In\nthe case of C, rst the count is prepared for display by converting it from a 2s\ncomplement integer to an ASCII code (C1), and then the actual character output\nis performed (C2).\n6.2 Debugging\nDebugging a program is pretty much applied common sense. A simple example\ncomes to mind: You are driving to a place you have never visited, and somewhere\nalong the way you made a wrong turn. What do you do now? One common driv-\ning debugging technique is to wander aimlessly, hoping to nd your way back.\nWhen that does not work, and you are nally willing to listen to the person sitting\nnext to you, you turn around and return to some known position on the route.\nThen, using a map (very dicult for some people), you follow the directions pro-\nvided, periodically comparing where you are (from landmarks you see out the\nwindow) with where the map says you should be, until you reach your desired\ndestination.\nDebugging is somewhat like that. A logical error in a program can make\nyou take a wrong turn. The simplest way to keep track of where you are as'}"
"compared to where you want to be is to trace the program. This consists of
keeping track of the sequence of instructions that have been executed and the
results produced by each instruction executed. When you examine the sequence
of instructions executed, you can detect errors in the ow of the program. When
you compare what each instruction has done to what it is supposed to do, you
can detect logical errors in the program. In short, when the behavior of the pro-
gram as it is executing is dierent from what it should be doing, you know there
is a bug.
A useful technique is to partition the program into parts, often referred to as
modules, and examine the results that have been computed at the end of execu-
tion of each module. In fact, the structured programming approach discussed in
Section 6.1 can help you determine where in the programs execution you should
examine results. This allows you to systematically get to the point where you
are focusing your attention on the instruction or instructions that are causing the
problem.
6.2.1 Debugging Operations
Many sophisticated debugging tools are oered in the marketplace, and
undoubtedly you will use many of them in the years ahead. In Chapter 15, for
example, we will examine debugging techniques using a source-level debugger
for C.
Right now, however, we wish to stay at the level of the machine architecture,
so we will see what we can accomplish with a few very elementary interactive
debugging operations. We will set breakpoints, single-step, and examine the state
of a program written in the LC-3 ISA.
In Chapter 15, we will see these same concepts again: breakpoints, single-
stepping, and examining program state that we are introducing here, but applied
to a C program, instead of the 0s and 1s of a program written in the LC-3 ISA.
When debugging interactively, the user sits in front of the keyboard and mon-
itor and issues commands to the computer. In our case, this means operating an
LC-3 simulator, using the menu available with the simulator. It is important to be
able to:
1. Write values into memory locations and into registers.
2. Execute instruction sequences in a program.
3. Stop execution when desired.
4. Examine what is in memory and registers at any point in the program.
These few simple operations will go a long way toward debugging programs.
6.2.1.1 Set Values
In order to test the execution of a part of a program in isolation without having
to worry about parts of the program that come before it, it is useful to rst write
values in memory and in registers that would have been written by earlier parts of
the program. For example, suppose one module in your program supplies input
from a keyboard, and a subsequent module operates on that input. Suppose you","{'page_number': 142, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'compared to where you want to be is to trace the program. This consists of\nkeeping track of the sequence of instructions that have been executed and the\nresults produced by each instruction executed. When you examine the sequence\nof instructions executed, you can detect errors in the ow of the program. When\nyou compare what each instruction has done to what it is supposed to do, you\ncan detect logical errors in the program. In short, when the behavior of the pro-\ngram as it is executing is dierent from what it should be doing, you know there\nis a bug.\nA useful technique is to partition the program into parts, often referred to as\nmodules, and examine the results that have been computed at the end of execu-\ntion of each module. In fact, the structured programming approach discussed in\nSection 6.1 can help you determine where in the programs execution you should\nexamine results. This allows you to systematically get to the point where you\nare focusing your attention on the instruction or instructions that are causing the\nproblem.\n6.2.1 Debugging Operations\nMany sophisticated debugging tools are oered in the marketplace, and\nundoubtedly you will use many of them in the years ahead. In Chapter 15, for\nexample, we will examine debugging techniques using a source-level debugger\nfor C.\nRight now, however, we wish to stay at the level of the machine architecture,\nso we will see what we can accomplish with a few very elementary interactive\ndebugging operations. We will set breakpoints, single-step, and examine the state\nof a program written in the LC-3 ISA.\nIn Chapter 15, we will see these same concepts again: breakpoints, single-\nstepping, and examining program state that we are introducing here, but applied\nto a C program, instead of the 0s and 1s of a program written in the LC-3 ISA.\nWhen debugging interactively, the user sits in front of the keyboard and mon-\nitor and issues commands to the computer. In our case, this means operating an\nLC-3 simulator, using the menu available with the simulator. It is important to be\nable to:\n1. Write values into memory locations and into registers.\n2. Execute instruction sequences in a program.\n3. Stop execution when desired.\n4. Examine what is in memory and registers at any point in the program.\nThese few simple operations will go a long way toward debugging programs.\n6.2.1.1 Set Values\nIn order to test the execution of a part of a program in isolation without having\nto worry about parts of the program that come before it, it is useful to rst write\nvalues in memory and in registers that would have been written by earlier parts of\nthe program. For example, suppose one module in your program supplies input\nfrom a keyboard, and a subsequent module operates on that input. Suppose you'}"
"want to test the second module before you have nished debugging the rst mod-
ule. If you know that the keyboard input module ends up with an ASCII code in
R0, you can test the module that operates on that input by rst writing an ASCII
code into R0.
6.2.1.2 Execute Sequences
It is important to be able to execute a sequence of instructions and then stop
execution in order to examine the values that the program has computed as a
result of executing that sequence. Three simple mechanisms are usually available
for doing this: run, step, and set breakpoints.
The Run command causes the program to execute until something makes it
stop. This can be either a HALT instruction or a breakpoint.
The Step command causes the program to execute a xed number of instruc-
tions and then stop. The interactive user enters the number of instructions he/she
wishes the simulator to execute before it stops. When that number is 1, the com-
puter executes one instruction, then stops. Executing one instruction and then
stopping is called single-stepping. It allows the person debugging the program to
examine the individual results of each instruction executed.
The Set Breakpoint command causes the program to stop execution at a
specic instruction in a program. Executing the debugging command Set Break-
point consists of adding an address to a list maintained by the simulator. During
the FETCH phase of each instruction, the simulator compares the PC with the
addresses in that list. If there is a match, execution stops. Thus, the eect of setting
a breakpoint is to allow execution to proceed until the PC contains an address that
has been set as a breakpoint. This is useful if one wishes to know what has been
computed up to a particular point in the program. One sets a breakpoint at that
address in the program and executes the Run command. The program executes
until that point and then stops so the user can examine what has been computed
up to that point. (When one no longer wishes to have the program stop execution
at that point, the breakpoint can be removed by executing the Clear Breakpoint
command.)
6.2.1.3 Display Values
Finally, it is useful to examine the results of execution when the simulator has
stopped execution. The Display command allows the user to examine the contents
of any memory location or any register.
6.2.2 Use of an Interactive Debugger
We conclude this chapter with four examples, showing how the use of interactive
debugging operations can help us nd errors in a program. We have chosen the
following four errors: (1) incorrectly setting the loop control so that the loop exe-
cutes an incorrect number of times, (2) confusing the load instruction 0010, which
loads a register with the contents of a memory location, with the load eective
address instruction 1110, which loads a register with the address of a memory
location, (3) forgetting which instructions set the condition codes, resulting in","{'page_number': 143, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'want to test the second module before you have nished debugging the rst mod-\nule. If you know that the keyboard input module ends up with an ASCII code in\nR0, you can test the module that operates on that input by rst writing an ASCII\ncode into R0.\n6.2.1.2 Execute Sequences\nIt is important to be able to execute a sequence of instructions and then stop\nexecution in order to examine the values that the program has computed as a\nresult of executing that sequence. Three simple mechanisms are usually available\nfor doing this: run, step, and set breakpoints.\nThe Run command causes the program to execute until something makes it\nstop. This can be either a HALT instruction or a breakpoint.\nThe Step command causes the program to execute a xed number of instruc-\ntions and then stop. The interactive user enters the number of instructions he/she\nwishes the simulator to execute before it stops. When that number is 1, the com-\nputer executes one instruction, then stops. Executing one instruction and then\nstopping is called single-stepping. It allows the person debugging the program to\nexamine the individual results of each instruction executed.\nThe Set Breakpoint command causes the program to stop execution at a\nspecic instruction in a program. Executing the debugging command Set Break-\npoint consists of adding an address to a list maintained by the simulator. During\nthe FETCH phase of each instruction, the simulator compares the PC with the\naddresses in that list. If there is a match, execution stops. Thus, the eect of setting\na breakpoint is to allow execution to proceed until the PC contains an address that\nhas been set as a breakpoint. This is useful if one wishes to know what has been\ncomputed up to a particular point in the program. One sets a breakpoint at that\naddress in the program and executes the Run command. The program executes\nuntil that point and then stops so the user can examine what has been computed\nup to that point. (When one no longer wishes to have the program stop execution\nat that point, the breakpoint can be removed by executing the Clear Breakpoint\ncommand.)\n6.2.1.3 Display Values\nFinally, it is useful to examine the results of execution when the simulator has\nstopped execution. The Display command allows the user to examine the contents\nof any memory location or any register.\n6.2.2 Use of an Interactive Debugger\nWe conclude this chapter with four examples, showing how the use of interactive\ndebugging operations can help us nd errors in a program. We have chosen the\nfollowing four errors: (1) incorrectly setting the loop control so that the loop exe-\ncutes an incorrect number of times, (2) confusing the load instruction 0010, which\nloads a register with the contents of a memory location, with the load eective\naddress instruction 1110, which loads a register with the address of a memory\nlocation, (3) forgetting which instructions set the condition codes, resulting in'}"
"a branch instruction testing the wrong condition, and (4) not covering all possible
cases of input values.
6.2.2.1 Example 1: Multiplying Without a Multiply Instruction
x
x
x
x
x
Figure 6.4
Debugging Example 1. An LC-3 program to multiply (without a Multiply
instruction).
If we examine the program instruction by instruction, we note that the pro-
gram rst clears R2 (i.e., initializes R2 to 0) and then attempts to perform the
multiplication by adding R4 to itself a number of times equal to the initial value
in R5. Each time an add is performed, R5 is decremented. When R5 = 0, the
program terminates.
It looks like the program should work! Upon execution, however, we nd that
if R4 initially contains the integer 10 and R5 initially contains the integer 3, the
program produces the result 40. What went wrong?
Our rst thought is to trace the program. Before we do that, we note that the
program assumes positive integers in R4 and R5. Using the Set Values command,
we put the value 10 in R4 and the value 3 in R5.
It is also useful to annotate each instruction with some algorithmic descrip-
tion of exactly what each instruction is doing. While this can be very tedious
and not very helpful in a 10,000-instruction program, it often can be very helpful
after one has isolated a bug to within a few instructions. There is a big dierence
between quickly eyeballing a sequence of instructions and stating precisely what
each instruction is doing. Quickly eyeballing often results in mistaking what one
eyeballs! Stating precisely usually does not. We have included in Figure 6.4, next
to each instruction, such an annotation.
Figure 6.5a shows a trace of the program, which we can obtain by single-
stepping. The column labeled PC shows the contents of the PC at the start of
each instruction. R2, R4, and R5 show the values in those three registers at the
start of each instruction.
A quick look at the trace shows that the loop body was executed four times,
rather than three. That suggests that the condition codes for our branch instruction
could have been set incorrectly. From there it is a short step to noting that the
branch should have been taken only when R5 was positive, and not when R5 is 0.
That is, bit [10]=1 in the branch instruction caused the extra iteration of the loop.","{'page_number': 144, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'a branch instruction testing the wrong condition, and (4) not covering all possible\ncases of input values.\n6.2.2.1 Example 1: Multiplying Without a Multiply Instruction\nx\nx\nx\nx\nx\nFigure 6.4\nDebugging Example 1. An LC-3 program to multiply (without a Multiply\ninstruction).\nIf we examine the program instruction by instruction, we note that the pro-\ngram rst clears R2 (i.e., initializes R2 to 0) and then attempts to perform the\nmultiplication by adding R4 to itself a number of times equal to the initial value\nin R5. Each time an add is performed, R5 is decremented. When R5 = 0, the\nprogram terminates.\nIt looks like the program should work! Upon execution, however, we nd that\nif R4 initially contains the integer 10 and R5 initially contains the integer 3, the\nprogram produces the result 40. What went wrong?\nOur rst thought is to trace the program. Before we do that, we note that the\nprogram assumes positive integers in R4 and R5. Using the Set Values command,\nwe put the value 10 in R4 and the value 3 in R5.\nIt is also useful to annotate each instruction with some algorithmic descrip-\ntion of exactly what each instruction is doing. While this can be very tedious\nand not very helpful in a 10,000-instruction program, it often can be very helpful\nafter one has isolated a bug to within a few instructions. There is a big dierence\nbetween quickly eyeballing a sequence of instructions and stating precisely what\neach instruction is doing. Quickly eyeballing often results in mistaking what one\neyeballs! Stating precisely usually does not. We have included in Figure 6.4, next\nto each instruction, such an annotation.\nFigure 6.5a shows a trace of the program, which we can obtain by single-\nstepping. The column labeled PC shows the contents of the PC at the start of\neach instruction. R2, R4, and R5 show the values in those three registers at the\nstart of each instruction.\nA quick look at the trace shows that the loop body was executed four times,\nrather than three. That suggests that the condition codes for our branch instruction\ncould have been set incorrectly. From there it is a short step to noting that the\nbranch should have been taken only when R5 was positive, and not when R5 is 0.\nThat is, bit [10]=1 in the branch instruction caused the extra iteration of the loop.'}"
"214
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
0
1
BR
n
z
p
3
We should also note that we could have saved a lot of the work of tracing the
program by using a breakpoint. That is, instead of examining the results of each
instruction, if we set a breakpoint at x3203, we would examine the results of
each iteration of the loop. Setting a breakpoint to stop the program after each
iteration of the loop is often enough to have us see the problem (and debug the pro-
gram) without the tedium of single-stepping each iteration of the loop. Figure 6.5b
shows the results of tracing the program, where each step is one iteration of the
loop. We see that the loop executed four times instead of three, immediately
identifying the bug.","{'page_number': 145, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '214\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\nBR\nn\nz\np\n3\nWe should also note that we could have saved a lot of the work of tracing the\nprogram by using a breakpoint. That is, instead of examining the results of each\ninstruction, if we set a breakpoint at x3203, we would examine the results of\neach iteration of the loop. Setting a breakpoint to stop the program after each\niteration of the loop is often enough to have us see the problem (and debug the pro-\ngram) without the tedium of single-stepping each iteration of the loop. Figure 6.5b\nshows the results of tracing the program, where each step is one iteration of the\nloop. We see that the loop executed four times instead of three, immediately\nidentifying the bug.'}"
"216
Figure 6.8
Debugging Example 2. A trace of the rst four instructions of the Add
program.
x3002, the loop control (R4), which counts the number of values added to R1, is
initialized to #10. The program subtracts 1 each time through the loop and repeats
until R4 contains 0. In x3003, the base register (R2) is initialized to the starting
location of the values to be added: x3100.
From there, each time through the loop, one value is loaded into R3 (in
x3004), the base register is incremented to get ready for the next iteration (x3005),
the value in R3 is added to R1, which contains the running sum (x3006), the
counter is decremented (x3007), the P bit is tested, and if true, the PC is set to
x3004 to begin the next iteration of the loop body (x3008). After ten times through
the loop, R4 contains 0, the P bit is 0, the branch is not taken, and the program
terminates (x3009).
It looks like the program should work. However, when we execute the pro-
gram and then check the value in R1, we nd the number x0024, which is not
x8135, the sum of the numbers stored in locations x3100 to x3109. What went
wrong?
We turn to the debugger and trace the program. Figure 6.8 shows a trace of
the rst four instructions executed. Note that after the instruction at x3003 has
executed, R2 contains x3107, not x3100 as we had expected. The problem is
that the opcode 0010 loaded the contents of M[x3100] (i.e., x3107) into R2, not
the address x3100. The result was to add the ten numbers starting at M[x3107]
instead of the ten numbers starting at M[x3100].
Our mistake: We used the wrong opcode. We should have used the opcode
1110, which would have loaded R2 with the address x3100. We correct the bug
by replacing the opcode 0010 with 1110, and the program runs correctly.
6.2.2.3 Example 3: Does a Sequence of Memory Locations Contain a 5?
The program of Figure 6.9 has been written to examine the contents of the ten
memory locations starting at address x3100 and to store a 1 in R0 if any of them
contains a 5 and a 0 in R0 if none of them contains a 5.
The program is supposed to do the following: The rst six instructions (at
x3000 to x3005) initialize R0 to 1, R1 to 5, and R3 to 10. The instruction at
x3006 initializes R4 to the address (x3100) of the rst location to be tested, and
x3007 loads the contents of x3100 into R2.
The instructions at x3008 and x3009 determine if R2 contains the value 5 by
adding 5 to R2 and branching to x300F if the result is 0","{'page_number': 146, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The instruction at\nx3006 initializes R4 to the address (x3100) of the rst location to be tested, and\nx3007 loads the contents of x3100 into R2.\nThe instructions at x3008 and x3009 determine if R2 contains the value 5 by\nadding 5 to R2 and branching to x300F if the result is 0. Since R0 is initialized\nto 1, the program terminates with R0 reporting the presence of a 5 among the\nlocations tested.'}"
"The instruction at
x3006 initializes R4 to the address (x3100) of the rst location to be tested, and
x3007 loads the contents of x3100 into R2.
The instructions at x3008 and x3009 determine if R2 contains the value 5 by
adding 5 to R2 and branching to x300F if the result is 0. Since R0 is initialized
to 1, the program terminates with R0 reporting the presence of a 5 among the
locations tested.","{'page_number': 146, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The instruction at\nx3006 initializes R4 to the address (x3100) of the rst location to be tested, and\nx3007 loads the contents of x3100 into R2.\nThe instructions at x3008 and x3009 determine if R2 contains the value 5 by\nadding 5 to R2 and branching to x300F if the result is 0. Since R0 is initialized\nto 1, the program terminates with R0 reporting the presence of a 5 among the\nlocations tested.'}"
"x3010
0
0
1
1
0
0
0
1
0
0
0
0
0
0
0
0
x3100
Figure 6.9
Debugging Example 3. An LC-3 program to detect the presence of a 5.
x300A increments R4, preparing to load the next value. x300B decrements
R3, indicating the number of values remaining to be tested. x300C loads the next
value into R2. x300D branches back to x3008 to repeat the process if R3 still
indicates more values to be tested. If R3 = 0, we have exhausted our tests, so R0
is set to 0 (x300E), and the program terminates (x300F).
When we run the program for some sample data that contains a 5 in one of
the memory locations, the program terminates with R0 = 0, indicating there were
no 5s in locations x3100 to x310A.
What went wrong? We examine a trace of the program, with a breakpoint set
at x300D. The results are shown in Figure 6.10.
The rst time the PC is at x300D, we have already tested the value stored in
x3100, we have loaded 7 (the contents of x3101) into R2, and R3 indicates there
are still nine values to be tested. R4 contains the address from which we most
recently loaded R2.
The second time the PC is at x300D, we have loaded 32 (the contents of
x3102) into R2, and R3 indicates there are eight values still to be tested. The
third time the PC is at x300D, we have loaded 0 (the contents of x3103) into R2,
Figure 6.10
Debugging Example 3. Tracing Example 3 with a breakpoint at x300D.","{'page_number': 147, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'x3010\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\nx3100\nFigure 6.9\nDebugging Example 3. An LC-3 program to detect the presence of a 5.\nx300A increments R4, preparing to load the next value. x300B decrements\nR3, indicating the number of values remaining to be tested. x300C loads the next\nvalue into R2. x300D branches back to x3008 to repeat the process if R3 still\nindicates more values to be tested. If R3 = 0, we have exhausted our tests, so R0\nis set to 0 (x300E), and the program terminates (x300F).\nWhen we run the program for some sample data that contains a 5 in one of\nthe memory locations, the program terminates with R0 = 0, indicating there were\nno 5s in locations x3100 to x310A.\nWhat went wrong? We examine a trace of the program, with a breakpoint set\nat x300D. The results are shown in Figure 6.10.\nThe rst time the PC is at x300D, we have already tested the value stored in\nx3100, we have loaded 7 (the contents of x3101) into R2, and R3 indicates there\nare still nine values to be tested. R4 contains the address from which we most\nrecently loaded R2.\nThe second time the PC is at x300D, we have loaded 32 (the contents of\nx3102) into R2, and R3 indicates there are eight values still to be tested. The\nthird time the PC is at x300D, we have loaded 0 (the contents of x3103) into R2,\nFigure 6.10\nDebugging Example 3. Tracing Example 3 with a breakpoint at x300D.'}"
"The error in the program occurred because the branch instruction immedi-
ately followed the load instruction that set the condition codes based on what was
loaded. That wiped out the condition codes set by the iteration control instruction
at x300B, which was keeping track of the number of iterations left to do. Since
the branch instruction should branch if there are still more memory locations to
examine, the branch instruction should have immediately followed the iteration
control instruction and NOT the load instruction which also sets condition codes.
A conditional branch instruction should be considered the second instruction
in a pair of instructions.
Instruction A
; sets the condition codes
BR instruction ; branches based on the condition codes
The first instruction in the pair (Instruction A) sets the condition codes. The
second instruction (BR) branches or not, depending on the condition codes set by
instruction A. It is important to never insert any instruction that sets condition codes
between instruction A and the branch instruction, since doing so will wipe out the
condition codes set by instruction A that are needed by the branch instruction.
Since the branch at x300D was based on the value loaded into R2, instead
of how many values remained to be tested, the third time the branch instruction
was executed, it was not taken when it should have been. If we interchange the
instructions at x300B and x300C, the branch instruction at x300D immediately
follows the iteration control instruction, and the program executes correctly.
It is also worth noting that the branch at x300D coincidentally behaved cor-
rectly the rst two times it executed because the load instruction at x300C loaded
positive values into R2. The bug did not produce incorrect behavior until the third
iteration. It would be nice if bugs would manifest themselves the rst time they
are encountered, but that is often not the case. Coincidences do occur, which adds
to the challenges of debugging.
6.2.2.4 Example 4: Finding the First 1 in a Word
Our last example contains an error that is usually one of the hardest to nd, as we
Addr
x300
x300
x300
x300
x300
x300
x300
x300
x300
x300
Figure 6.11
Debugging Example 4. An LC-3 program to nd the rst 1 in a word.","{'page_number': 148, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The error in the program occurred because the branch instruction immedi-\nately followed the load instruction that set the condition codes based on what was\nloaded. That wiped out the condition codes set by the iteration control instruction\nat x300B, which was keeping track of the number of iterations left to do. Since\nthe branch instruction should branch if there are still more memory locations to\nexamine, the branch instruction should have immediately followed the iteration\ncontrol instruction and NOT the load instruction which also sets condition codes.\nA conditional branch instruction should be considered the second instruction\nin a pair of instructions.\nInstruction A\n; sets the condition codes\nBR instruction ; branches based on the condition codes\nThe first instruction in the pair (Instruction A) sets the condition codes. The\nsecond instruction (BR) branches or not, depending on the condition codes set by\ninstruction A. It is important to never insert any instruction that sets condition codes\nbetween instruction A and the branch instruction, since doing so will wipe out the\ncondition codes set by instruction A that are needed by the branch instruction.\nSince the branch at x300D was based on the value loaded into R2, instead\nof how many values remained to be tested, the third time the branch instruction\nwas executed, it was not taken when it should have been. If we interchange the\ninstructions at x300B and x300C, the branch instruction at x300D immediately\nfollows the iteration control instruction, and the program executes correctly.\nIt is also worth noting that the branch at x300D coincidentally behaved cor-\nrectly the rst two times it executed because the load instruction at x300C loaded\npositive values into R2. The bug did not produce incorrect behavior until the third\niteration. It would be nice if bugs would manifest themselves the rst time they\nare encountered, but that is often not the case. Coincidences do occur, which adds\nto the challenges of debugging.\n6.2.2.4 Example 4: Finding the First 1 in a Word\nOur last example contains an error that is usually one of the hardest to nd, as we\nAddr\nx300\nx300\nx300\nx300\nx300\nx300\nx300\nx300\nx300\nx300\nFigure 6.11\nDebugging Example 4. An LC-3 program to nd the rst 1 in a word.'}"
"example, if the location examined contained 0010000110000000, the program
would terminate with R1 = 13. If the location contained 0000000000000110,
the program would terminate with R1 = 2.
The program Figure 6.11 is supposed to work as follows (and it usually does):
x3000 and x3001 initialize R1 to 15, the bit number of the leftmost bit.
x3002 loads R2 with the contents of x3400, the bit pattern to be exam-
ined. Since x3400 is too far from x3000 for a LD instruction, the load indirect
instruction is used, obtaining the location of the bit pattern in x3009.
x3003 tests the most signicant bit of the bit pattern (bit [15]), and if it is
a 1, branches to x3008, where the program terminates with R1=15. If the most
signicant bit is 0, the branch is not taken, and processing continues at x3004.
The loop body, locations x3004 to x3007, does two things. First (x3004), it
subtracts 1 from R1, yielding the bit number of the next bit to the right. Second
(x3005), it adds R2 to itself, resulting in the contents of R2 shifting left one bit,
resulting in the next bit to the right being shifted into the bit [15] position. Third
(x3006), the BR instruction tests the new bit [15], and if it is a 1, branches to
x3008, where the program halts with R1 containing the actual bit number of the
current leftmost bit. If the new bit [15] is 0, x3007 is an unconditional branch to
x3004 for the next iteration of the loop body.
The process continues until the rst 1 is found. The program works correctly
almost all the time. However, when we ran the program on our data, the program
Figure 6.12
Debugging Example 4. A Trace of Debugging Example 4 with a breakpoint
at x3007.","{'page_number': 149, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'example, if the location examined contained 0010000110000000, the program\nwould terminate with R1 = 13. If the location contained 0000000000000110,\nthe program would terminate with R1 = 2.\nThe program Figure 6.11 is supposed to work as follows (and it usually does):\nx3000 and x3001 initialize R1 to 15, the bit number of the leftmost bit.\nx3002 loads R2 with the contents of x3400, the bit pattern to be exam-\nined. Since x3400 is too far from x3000 for a LD instruction, the load indirect\ninstruction is used, obtaining the location of the bit pattern in x3009.\nx3003 tests the most signicant bit of the bit pattern (bit [15]), and if it is\na 1, branches to x3008, where the program terminates with R1=15. If the most\nsignicant bit is 0, the branch is not taken, and processing continues at x3004.\nThe loop body, locations x3004 to x3007, does two things. First (x3004), it\nsubtracts 1 from R1, yielding the bit number of the next bit to the right. Second\n(x3005), it adds R2 to itself, resulting in the contents of R2 shifting left one bit,\nresulting in the next bit to the right being shifted into the bit [15] position. Third\n(x3006), the BR instruction tests the new bit [15], and if it is a 1, branches to\nx3008, where the program halts with R1 containing the actual bit number of the\ncurrent leftmost bit. If the new bit [15] is 0, x3007 is an unconditional branch to\nx3004 for the next iteration of the loop body.\nThe process continues until the rst 1 is found. The program works correctly\nalmost all the time. However, when we ran the program on our data, the program\nFigure 6.12\nDebugging Example 4. A Trace of Debugging Example 4 with a breakpoint\nat x3007.'}"
"Each time the PC contained the address x3007, R1 contained a value smaller
by 1 than the previous time. The reason is as follows: After R1 was decremented
and the value in R2 shifted left, the bit tested was a 0, and so the program did not
terminate. This continued for values in R1 equal to 14, 13, 12, 11, 10, 9, 8, 7, 6,
5, 4, 3, 2, 1, 0, 1, 2, 3, 4, and so forth.
The problem was that the initial value in x3400 was x0000. The program
worked ne as long as there was at least one 1 present. For the case where x3400
contained all zeros, the conditional branch at x3006 was never taken, and so the
program continued with execution of x3007, then x3004, x3005, x3006, x3007,
and then back again to x3004. There was no way to break out of the sequence
x3004, x3005, x3006, x3007, and back again to x3004. We call the sequence
x3004 to x3007 a loop. Because there is no way for the program execution to break
out of this loop, we call it an innite loop. Thus, the program never terminates,
and so we can never get the correct answer.
Again, we emphasize that this is often the hardest error to detect because it
is as we said earlier a corner case. The programmer assumed that at least one bit
.
.
.
eciently multiplies two integers and places the result in R3. Show the","{'page_number': 150, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Each time the PC contained the address x3007, R1 contained a value smaller\nby 1 than the previous time. The reason is as follows: After R1 was decremented\nand the value in R2 shifted left, the bit tested was a 0, and so the program did not\nterminate. This continued for values in R1 equal to 14, 13, 12, 11, 10, 9, 8, 7, 6,\n5, 4, 3, 2, 1, 0, 1, 2, 3, 4, and so forth.\nThe problem was that the initial value in x3400 was x0000. The program\nworked ne as long as there was at least one 1 present. For the case where x3400\ncontained all zeros, the conditional branch at x3006 was never taken, and so the\nprogram continued with execution of x3007, then x3004, x3005, x3006, x3007,\nand then back again to x3004. There was no way to break out of the sequence\nx3004, x3005, x3006, x3007, and back again to x3004. We call the sequence\nx3004 to x3007 a loop. Because there is no way for the program execution to break\nout of this loop, we call it an innite loop. Thus, the program never terminates,\nand so we can never get the correct answer.\nAgain, we emphasize that this is often the hardest error to detect because it\nis as we said earlier a corner case. The programmer assumed that at least one bit\n.\n.\n.\neciently multiplies two integers and places the result in R3. Show the'}"
"embly Language
B
y now, you are probably a little tired of 1s and 0s and keeping track of 0001
meaning ADD and 1001 meaning NOT. Also, wouldnt it be nice if we
could refer to a memory location by some meaningful symbolic name instead of
memorizing its 16-bit address? And wouldnt it be nice if we could represent each
instruction in some more easily comprehensible way, instead of having to keep
track of which bit of an instruction conveys which individual piece of information
about that instruction? It turns out that help is on the way.
In this chapter, we introduce assembly language, a mechanism that does all
of the above, and more.
7.1 Assembly Language
Programming
Moving Up a Level
Recall the levels of transformation identied in Figure 1.9 of Chapter 1. Algo-
rithms are transformed into programs described in some mechanical language.
This mechanical language can be, as it is in Chapter 5, the machine language of a
particular computer. Recall that a program is in a computers machine language
if every instruction in the program is from the ISA of that computer.
On the other hand, the mechanical language can be more user-friendly. We
generally partition mechanical languages into two classes, high-level and low-
level. Of the two, high-level languages are much more user-friendly. Examples
are C, C++, Java, Fortran, COBOL, Python, plus more than a thousand others.
Instructions in a high-level language almost (but not quite) resemble statements
in a natural language such as English. High-level languages tend to be ISA inde-
pendent. That is, once you learn how to program in C (or Fortran or Python)
for one ISA, it is a small step to write programs in C (or Fortran or Python) for
another ISA.","{'page_number': 151, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'embly Language\nB\ny now, you are probably a little tired of 1s and 0s and keeping track of 0001\nmeaning ADD and 1001 meaning NOT. Also, wouldnt it be nice if we\ncould refer to a memory location by some meaningful symbolic name instead of\nmemorizing its 16-bit address? And wouldnt it be nice if we could represent each\ninstruction in some more easily comprehensible way, instead of having to keep\ntrack of which bit of an instruction conveys which individual piece of information\nabout that instruction? It turns out that help is on the way.\nIn this chapter, we introduce assembly language, a mechanism that does all\nof the above, and more.\n7.1 Assembly Language\nProgramming\nMoving Up a Level\nRecall the levels of transformation identied in Figure 1.9 of Chapter 1. Algo-\nrithms are transformed into programs described in some mechanical language.\nThis mechanical language can be, as it is in Chapter 5, the machine language of a\nparticular computer. Recall that a program is in a computers machine language\nif every instruction in the program is from the ISA of that computer.\nOn the other hand, the mechanical language can be more user-friendly. We\ngenerally partition mechanical languages into two classes, high-level and low-\nlevel. Of the two, high-level languages are much more user-friendly. Examples\nare C, C++, Java, Fortran, COBOL, Python, plus more than a thousand others.\nInstructions in a high-level language almost (but not quite) resemble statements\nin a natural language such as English. High-level languages tend to be ISA inde-\npendent. That is, once you learn how to program in C (or Fortran or Python)\nfor one ISA, it is a small step to write programs in C (or Fortran or Python) for\nanother ISA.'}"
"Before a program written in a high-level language can be executed, it must
be translated into a program in the ISA of the computer on which it is expected to
execute. It is often the case that each statement in the high-level language species
several instructions in the ISA of the computer. In Chapter 11, we will introduce
the high-level language C, and in Chapters 12 through 19, we will show the rela-
tionship between various statements in C and their corresponding translations to
LC-3 code. In this chapter, however, we will only move up a small step from the
ISA we dealt with in Chapter 5.
A small step up from the ISA of a machine is that ISAs assembly language.
Assembly language is a low-level language. There is no confusing an instruc-
tion in a low-level language with a statement in English. Each assembly language
instruction usually species a single instruction in the ISA. Unlike high-level lan-
guages, which are usually ISA independent, low-level languages are very much
ISA dependent. In fact, it is usually the case that each ISA has only one assembly
language.
The purpose of assembly language is to make the programming process more
user-friendly than programming in machine language (i.e., in the ISA of the com-
puter with which we are dealing), while still providing the programmer with
detailed control over the instructions that the computer can execute. So, for exam-
ple, while still retaining control over the detailed instructions the computer is to
carry out, we are freed from having to remember what opcode is 0001 and what
opcode is 1001, or what is being stored in memory location 0011111100001010
and what is being stored in location 0011111100000101. Assembly languages
let us use mnemonic devices for opcodes, such as ADD for 0001 and NOT for
1001, and they let us give meaningful symbolic names to memory locations, such
as SUM or PRODUCT, rather than use the memory locations 16-bit addresses.
This makes it easier to dierentiate which memory location is keeping track of a
SUM and which memory location is keeping track of a PRODUCT. We call these
names symbolic addresses.
We will see, starting in Chapter 11, that when we take the larger step of
moving up to a higher-level language (such as C), programming will be even more
user-friendly, but in doing so, we will relinquish some control over exactly which
detailed ISA instructions are to be carried out to accomplish the work specied
by a high-level language statement.
7.2 An Assembly Language Program
We will begin our study of the LC-3 assembly language by means of an example.
The program in Figure 7.1 multiplies the integer initially stored in NUMBER
by 6 by adding the integer to itself six times. For example, if the integer is 123,
the program computes the product by adding 123+123+123+123+123+123.
Where have you seen that before? :-)
The program consists of 21 lines of code. We have added a line number to
each line of the program in order to be able to refer to individual lines easily.
This is a common practice","{'page_number': 152, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '1 multiplies the integer initially stored in NUMBER\nby 6 by adding the integer to itself six times. For example, if the integer is 123,\nthe program computes the product by adding 123+123+123+123+123+123.\nWhere have you seen that before? :-)\nThe program consists of 21 lines of code. We have added a line number to\neach line of the program in order to be able to refer to individual lines easily.\nThis is a common practice. These line numbers are not part of the program. Ten\nlines start with a semicolon, designating that they are strictly for the benet of'}"
"1 multiplies the integer initially stored in NUMBER
by 6 by adding the integer to itself six times. For example, if the integer is 123,
the program computes the product by adding 123+123+123+123+123+123.
Where have you seen that before? :-)
The program consists of 21 lines of code. We have added a line number to
each line of the program in order to be able to refer to individual lines easily.
This is a common practice. These line numbers are not part of the program. Ten
lines start with a semicolon, designating that they are strictly for the benet of","{'page_number': 152, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '1 multiplies the integer initially stored in NUMBER\nby 6 by adding the integer to itself six times. For example, if the integer is 123,\nthe program computes the product by adding 123+123+123+123+123+123.\nWhere have you seen that before? :-)\nThe program consists of 21 lines of code. We have added a line number to\neach line of the program in order to be able to refer to individual lines easily.\nThis is a common practice. These line numbers are not part of the program. Ten\nlines start with a semicolon, designating that they are strictly for the benet of'}"
"15
.END
Figure 7.1
An assembly language program.
the human reader. More on this momentarily. Seven lines (06, 07, 08, 0C, 0D, 0E,
and 10) specify assembly language instructions to be translated into machine lan-
guage instructions of the LC-3, which will be executed when the program runs.
The remaining four lines (05, 12, 13, and 15) contain pseudo-ops, which are mes-
sages from the programmer to the translation program to help in the translation
process. The translation program is called an assembler (in this case the LC-3
assembler), and the translation process is called assembly.
7.2.1 Instructions
Instead of an instruction being 16 0s and 1s, as is the case in the LC-3 ISA, an
instruction in assembly language consists of four parts, as follows:
Label
Opcode
Operands
; Comment
Two of the parts (Label and Comment) are optional. More on that momentarily.
7.2.1.1 Opcodes and Operands
Two of the parts (Opcode and Operands) are mandatory. For an assembly lan-
guage instruction to correspond to an instruction in the LC-3 ISA, it must have
an Opcode (the thing the instruction is to do), and the appropriate number of
Operands (the things it is supposed to do it to). Not surprisingly, this was exactly
what we encountered in Chapter 5 when we studied the LC-3 ISA.
The Opcode is a symbolic name for the opcode of the corresponding LC-3
instruction. The idea is that it is easier to remember an operation by the symbolic","{'page_number': 153, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '15\n.END\nFigure 7.1\nAn assembly language program.\nthe human reader. More on this momentarily. Seven lines (06, 07, 08, 0C, 0D, 0E,\nand 10) specify assembly language instructions to be translated into machine lan-\nguage instructions of the LC-3, which will be executed when the program runs.\nThe remaining four lines (05, 12, 13, and 15) contain pseudo-ops, which are mes-\nsages from the programmer to the translation program to help in the translation\nprocess. The translation program is called an assembler (in this case the LC-3\nassembler), and the translation process is called assembly.\n7.2.1 Instructions\nInstead of an instruction being 16 0s and 1s, as is the case in the LC-3 ISA, an\ninstruction in assembly language consists of four parts, as follows:\nLabel\nOpcode\nOperands\n; Comment\nTwo of the parts (Label and Comment) are optional. More on that momentarily.\n7.2.1.1 Opcodes and Operands\nTwo of the parts (Opcode and Operands) are mandatory. For an assembly lan-\nguage instruction to correspond to an instruction in the LC-3 ISA, it must have\nan Opcode (the thing the instruction is to do), and the appropriate number of\nOperands (the things it is supposed to do it to). Not surprisingly, this was exactly\nwhat we encountered in Chapter 5 when we studied the LC-3 ISA.\nThe Opcode is a symbolic name for the opcode of the corresponding LC-3\ninstruction. The idea is that it is easier to remember an operation by the symbolic'}"
"name ADD, AND, or LDR than by the four-bit quantity 0001, 0101, or 0110.
Figure 5.3 (also Figure A.2) lists the Opcodes of the 15 LC-3 instructions.
Pages 658 through 673 show the assembly language representations for the 15
LC-3 instructions.
The number of operands depends on the operation being performed. For
example, the ADD instruction (line 0C in the program of Figure 7.1) requires
three operands (two sources to obtain the numbers to be added, and one desti-
nation to designate where the result is to be stored). All three operands must be
explicitly identied in the instruction.
AGAIN
ADD
R3,R3,R2
In this case the operands to be added are obtained from register 2 and from register
3. The result is to be placed in register 3. We represent each of the registers 0
through 7 as R0, R1, R2,  , R7, rather than 000, 001, 010,  , 111.
The LD instruction (line 07 of the program in Figure 7.1) requires two
operands (the memory location from which the value is to be read and the destina-
tion register that is to contain the value after the instruction nishes execution). In
LC-3 assembly language, we assign symbolic names called labels to the memory
locations so we will not have to remember their explicit 16-bit addresses. In this
case, the location from which the value is to be read is given the label NUMBER.
The destination (i.e., where the value is to be loaded) is register 2.
LD
R2, NUMBER
As we discussed in Section 5.1.6, operands can be obtained from registers, from
memory, or they may be literal (i.e., immediate) values in the instruction. In the
case of register operands, the registers are explicitly represented (such as R2 and
R3 in line 0C). In the case of memory operands, the symbolic name of the mem-
ory location is explicitly represented (such as NUMBER in line 07 and SIX in line
06). In the case of immediate operands, the actual value is explicitly represented
(such as the value 0 in line 08).
AND
R3, R3, #0 ; Clear R3. It will contain the product.
A literal value must contain a symbol identifying the representation base of the
number. We use # for decimal, x for hexadecimal, and b for binary. Sometimes
there is no ambiguity, such as in the case 3F0A, which is a hex number. Nonethe-
less, we write it as x3F0A. Sometimes there is ambiguity, such as in the case
1000. x1000 represents the decimal number 4096, b1000 represents the decimal
number 8, and #1000 represents the decimal number 1000.
7.2.1.2 Labels
Labels are symbolic names that are used to identify memory locations that are
referred to explicitly in the program. In LC-3 assembly language, a label consists
of from 1 to 20 alphanumeric characters (i.e","{'page_number': 154, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Sometimes there is ambiguity, such as in the case\n1000. x1000 represents the decimal number 4096, b1000 represents the decimal\nnumber 8, and #1000 represents the decimal number 1000.\n7.2.1.2 Labels\nLabels are symbolic names that are used to identify memory locations that are\nreferred to explicitly in the program. In LC-3 assembly language, a label consists\nof from 1 to 20 alphanumeric characters (i.e., each character is a capital or lower-\ncase letter of the English alphabet, or a decimal digit), starting with a letter of the\nalphabet.\nHowever, not all sequences of characters that follow these rules can be used\nas labels. You know that computer programs cannot tolerate ambiguity. So ADD,'}"
"Sometimes there is ambiguity, such as in the case
1000. x1000 represents the decimal number 4096, b1000 represents the decimal
number 8, and #1000 represents the decimal number 1000.
7.2.1.2 Labels
Labels are symbolic names that are used to identify memory locations that are
referred to explicitly in the program. In LC-3 assembly language, a label consists
of from 1 to 20 alphanumeric characters (i.e., each character is a capital or lower-
case letter of the English alphabet, or a decimal digit), starting with a letter of the
alphabet.
However, not all sequences of characters that follow these rules can be used
as labels. You know that computer programs cannot tolerate ambiguity. So ADD,","{'page_number': 154, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Sometimes there is ambiguity, such as in the case\n1000. x1000 represents the decimal number 4096, b1000 represents the decimal\nnumber 8, and #1000 represents the decimal number 1000.\n7.2.1.2 Labels\nLabels are symbolic names that are used to identify memory locations that are\nreferred to explicitly in the program. In LC-3 assembly language, a label consists\nof from 1 to 20 alphanumeric characters (i.e., each character is a capital or lower-\ncase letter of the English alphabet, or a decimal digit), starting with a letter of the\nalphabet.\nHowever, not all sequences of characters that follow these rules can be used\nas labels. You know that computer programs cannot tolerate ambiguity. So ADD,'}"
"NOT, x1000, R4, and other character strings that have specic meanings in an
LC-3 program cannot be used as labels. They could confuse the LC-3 assembler
as it tries to translate the LC-3 assembly language program into a program in the
LC-3 ISA. Such not-allowed character strings are often referred to as reserved
words.
NOW, Under21, R2D2, R785, and C3PO are all examples of legitimate LC-3
assembly language labels.
We said we give a label (i.e., a symbolic name) to a memory location if we
explicitly refer to it in the program. There are two reasons for explicitly referring
to a memory location.
1. The location is the target of a branch instruction (e.g., AGAIN in line 0C).
That is, the label AGAIN identies the location of the instruction that will
be executed next if the branch is taken.
2. The location contains a value that is loaded or stored (e.g., NUMBER in
line 12, and SIX in line 13).
Note the location AGAIN (identied in line 0C) is specically referenced by
the branch instruction in line 0E.
BRp
AGAIN
If the result of ADD R1,R1,#1 is positive (which results in the P bit being
set), then the program branches to the location explicitly referenced as AGAIN
to perform another iteration.
The location NUMBER is specically referenced by the load instruction
in line 07. The value stored in the memory location explicitly referenced as
NUMBER is loaded into R2.
If a location in the program is not explicitly referenced, then there is no need
to give it a label.
7.2.1.3 Comments
Comments are messages intended only for human consumption. They have no
eect on the translation process and indeed are not acted on by the LC-3 assem-
bler. They are identied in the program by semicolons. A semicolon signies
that the rest of the line is a comment and is to be ignored by the assembler. If the
semicolon is the rst nonblank character on the line, the entire line is ignored. If
the semicolon follows the operands of an instruction, then only the comment is
ignored by the assembler.
The purpose of comments is to make the program more comprehensible to
the human reader. Comments help explain a nonintuitive aspect of an instruction
or a set of instructions. In lines 08 and 09, the comment Clear R3; it will contain
the product lets the reader know that the instruction on line 08 is initializing
R3 prior to accumulating the product of the two numbers. While the purpose of
line 08 may be obvious to the programmer today, it may not be the case two years
from now, after the programmer has written an additional 30,000 instructions and
cannot remember why he/she wrote AND R3,R3,#0. It may also be the case that
two years from now, the programmer no longer works for the company, and the","{'page_number': 155, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'NOT, x1000, R4, and other character strings that have specic meanings in an\nLC-3 program cannot be used as labels. They could confuse the LC-3 assembler\nas it tries to translate the LC-3 assembly language program into a program in the\nLC-3 ISA. Such not-allowed character strings are often referred to as reserved\nwords.\nNOW, Under21, R2D2, R785, and C3PO are all examples of legitimate LC-3\nassembly language labels.\nWe said we give a label (i.e., a symbolic name) to a memory location if we\nexplicitly refer to it in the program. There are two reasons for explicitly referring\nto a memory location.\n1. The location is the target of a branch instruction (e.g., AGAIN in line 0C).\nThat is, the label AGAIN identies the location of the instruction that will\nbe executed next if the branch is taken.\n2. The location contains a value that is loaded or stored (e.g., NUMBER in\nline 12, and SIX in line 13).\nNote the location AGAIN (identied in line 0C) is specically referenced by\nthe branch instruction in line 0E.\nBRp\nAGAIN\nIf the result of ADD R1,R1,#1 is positive (which results in the P bit being\nset), then the program branches to the location explicitly referenced as AGAIN\nto perform another iteration.\nThe location NUMBER is specically referenced by the load instruction\nin line 07. The value stored in the memory location explicitly referenced as\nNUMBER is loaded into R2.\nIf a location in the program is not explicitly referenced, then there is no need\nto give it a label.\n7.2.1.3 Comments\nComments are messages intended only for human consumption. They have no\neect on the translation process and indeed are not acted on by the LC-3 assem-\nbler. They are identied in the program by semicolons. A semicolon signies\nthat the rest of the line is a comment and is to be ignored by the assembler. If the\nsemicolon is the rst nonblank character on the line, the entire line is ignored. If\nthe semicolon follows the operands of an instruction, then only the comment is\nignored by the assembler.\nThe purpose of comments is to make the program more comprehensible to\nthe human reader. Comments help explain a nonintuitive aspect of an instruction\nor a set of instructions. In lines 08 and 09, the comment Clear R3; it will contain\nthe product lets the reader know that the instruction on line 08 is initializing\nR3 prior to accumulating the product of the two numbers. While the purpose of\nline 08 may be obvious to the programmer today, it may not be the case two years\nfrom now, after the programmer has written an additional 30,000 instructions and\ncannot remember why he/she wrote AND R3,R3,#0. It may also be the case that\ntwo years from now, the programmer no longer works for the company, and the'}"
"Another purpose of comments is to make the visual presentation of a program
easier to understand. That is, comments are used to separate pieces of a program
from each other to make the program more readable. Lines of code that work
together to compute a single result are placed on successive lines, but they are
separated from the rest of the program by blank lines. For example, note that
lines 0C through 0E, which together form the loop body that is the crux of this
computer program, are separated from the rest of the code by lines 0B and 0F.
There is nothing on lines 0B and 0F other than the semicolons in the rst column.
Incidentally, another opportunity to make a program easier to read is the judi-
cious use of white space, accomplished by adding extra spaces to a line that are
ignored by the assemblerfor example, having all the opcodes start in the same
column on the page, whether or not the instruction has a label.
7.2.2 Pseudo-Ops (Assembler Directives)
The LC-3 assembler is a program that takes as input a string of characters repre-
senting a computer program written in LC-3 assembly language and translates it
into a program in the ISA of the LC-3. Pseudo-ops help the assembler perform
that task.
The more formal name for a pseudo-op is assembler directive. It is called a
pseudo-op because, like its Greek root pseudes (which means false), it does
not refer to an operation that will be performed by the program during execution.
Rather, the pseudo-op is strictly a message from the assembly language program-
mer to the assembler to help the assembler in the assembly process. Once the
assembler handles the message, the pseudo-op is discarded. The LC-3 assem-
bly language contains ve pseudo-ops that we will nd useful in our assembly
language programming: .ORIG, .FILL, .BLKW, .STRINGZ, and .END. All are
easily recognizable by the dot as their rst character.
7.2.2.1 .ORIG
.ORIG tells the assembler where in memory to place the LC-3 program. In line 05,
.ORIG x3050 says, place the rst LC-3 ISA instruction in location x3050. As a
result, 0010001000001100 (the translated LD R1,SIX instruction) is put in loca-
tion x3050, and the rest of the translated LC-3 program is placed in the subsequent
sequential locations in memory. For example, if the program consists of x100 LC-
3 instructions, and .ORIG says to put the rst instruction in x3050, the remaining
xFF instructions are placed in locations x3051 to x314F.","{'page_number': 156, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Another purpose of comments is to make the visual presentation of a program\neasier to understand. That is, comments are used to separate pieces of a program\nfrom each other to make the program more readable. Lines of code that work\ntogether to compute a single result are placed on successive lines, but they are\nseparated from the rest of the program by blank lines. For example, note that\nlines 0C through 0E, which together form the loop body that is the crux of this\ncomputer program, are separated from the rest of the code by lines 0B and 0F.\nThere is nothing on lines 0B and 0F other than the semicolons in the rst column.\nIncidentally, another opportunity to make a program easier to read is the judi-\ncious use of white space, accomplished by adding extra spaces to a line that are\nignored by the assemblerfor example, having all the opcodes start in the same\ncolumn on the page, whether or not the instruction has a label.\n7.2.2 Pseudo-Ops (Assembler Directives)\nThe LC-3 assembler is a program that takes as input a string of characters repre-\nsenting a computer program written in LC-3 assembly language and translates it\ninto a program in the ISA of the LC-3. Pseudo-ops help the assembler perform\nthat task.\nThe more formal name for a pseudo-op is assembler directive. It is called a\npseudo-op because, like its Greek root pseudes (which means false), it does\nnot refer to an operation that will be performed by the program during execution.\nRather, the pseudo-op is strictly a message from the assembly language program-\nmer to the assembler to help the assembler in the assembly process. Once the\nassembler handles the message, the pseudo-op is discarded. The LC-3 assem-\nbly language contains ve pseudo-ops that we will nd useful in our assembly\nlanguage programming: .ORIG, .FILL, .BLKW, .STRINGZ, and .END. All are\neasily recognizable by the dot as their rst character.\n7.2.2.1 .ORIG\n.ORIG tells the assembler where in memory to place the LC-3 program. In line 05,\n.ORIG x3050 says, place the rst LC-3 ISA instruction in location x3050. As a\nresult, 0010001000001100 (the translated LD R1,SIX instruction) is put in loca-\ntion x3050, and the rest of the translated LC-3 program is placed in the subsequent\nsequential locations in memory. For example, if the program consists of x100 LC-\n3 instructions, and .ORIG says to put the rst instruction in x3050, the remaining\nxFF instructions are placed in locations x3051 to x314F.'}"
"7.2
An Assembly Language Program
237
7.2.2.2 .FILL
.FILL tells the assembler to set aside the next location in the program and initial-
ize it with the value of the operand. The value can be either a number or a label.
In line 13, the ninth location in the resulting LC-3 program is initialized to the
value x0006.
7.2.2.3 .BLKW
.BLKW tells the assembler to set aside some number of sequential memory loca-
tions (i.e., a BLocK of Words) in the program. The actual number is the operand
of the .BLKW pseudo-op. In line 12, the pseudo-op instructs the assembler to set
aside one location in memory (and, incidentally, to label it NUMBER).
The pseudo-op .BLKW is particularly useful when the actual value of the
operand is not yet known. In our example we assumed the number in location
NUMBER was 123. How did it get there? A common use of .BLKW is to set
aside a location in the program, as we did here, and have another section of code
produce the number, perhaps from input from a keyboard (which we cannot know
at the time we write the program), and store that value into NUMBER before we
execute the code in Figure 7.1.
7.2.2.4 .STRINGZ
.STRINGZ tells the assembler to initialize a sequence of n+1 memory locations.
The argument is a sequence of n characters inside double quotation marks. The
x
: x
x301C: x0021
x301D: x0000","{'page_number': 157, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '7.2\nAn Assembly Language Program\n237\n7.2.2.2 .FILL\n.FILL tells the assembler to set aside the next location in the program and initial-\nize it with the value of the operand. The value can be either a number or a label.\nIn line 13, the ninth location in the resulting LC-3 program is initialized to the\nvalue x0006.\n7.2.2.3 .BLKW\n.BLKW tells the assembler to set aside some number of sequential memory loca-\ntions (i.e., a BLocK of Words) in the program. The actual number is the operand\nof the .BLKW pseudo-op. In line 12, the pseudo-op instructs the assembler to set\naside one location in memory (and, incidentally, to label it NUMBER).\nThe pseudo-op .BLKW is particularly useful when the actual value of the\noperand is not yet known. In our example we assumed the number in location\nNUMBER was 123. How did it get there? A common use of .BLKW is to set\naside a location in the program, as we did here, and have another section of code\nproduce the number, perhaps from input from a keyboard (which we cannot know\nat the time we write the program), and store that value into NUMBER before we\nexecute the code in Figure 7.1.\n7.2.2.4 .STRINGZ\n.STRINGZ tells the assembler to initialize a sequence of n+1 memory locations.\nThe argument is a sequence of n characters inside double quotation marks. The\nx\n: x\nx301C: x0021\nx301D: x0000'}"
"7.2.2.5 .END
.END tells the assembler it has reached the end of the program and need not
even look at anything after it. That is, any characters that come after .END will
not be processed by the assembler. Note: .END does not stop the program during
execution. In fact, .END does not even exist at the time of execution. It is simply a
delimiterit marks the end of the program. It is a message from the programmer,
telling the assembler where the assembly language program ends.
7.2.3 Example: The Character Count Example of Section 5.5,
Revisited Again!
Now we are ready for a complete example. Lets consider again the problem of
Section 5.5. We wish to write a program that will take a character that is input
from the keyboard and count the number of occurrences of that character in a
le. As before, we rst develop the algorithm by constructing the owchart.
Recall that in Section 6.1, we showed how to decompose the problem system-
atically so as to generate the owchart of Figure 5.16. In fact, the nal step of
that process in Chapter 6 is the owchart of Figure 6.3e, which is essentially
identical to Figure 5.16. Next, we use the owchart to write the actual program.
This time, however, we enjoy the luxury of not worrying about 0s and 1s and
instead write the program in LC-3 assembly language. The program is shown in
Figure 7.2.
A few comments about this program: Three times during this program, assis-
tance in the form of a service call is required of the operating system. In each case,
a TRAP instruction is used. TRAP x23 causes a character to be input from the
keyboard and placed in R0 (line 0D). TRAP x21 causes the ASCII code in R0
to be displayed on the monitor (line 28). TRAP x25 causes the machine to be
halted (line 29). As we said before, we will leave the details of how the TRAP
instruction is carried out until Chapter 9.
The ASCII codes for the decimal digits 0 to 9 (0000 to 1001) are x30 to x39.
The conversion from binary to ASCII is done simply by adding x30 to the binary
value of the decimal digit. Line 2D shows the label ASCII used to identify the
memory location containing x0030. The LD instruction in line 26 uses it to load
x30 into R0, so it can convert the count that is in R2 from a binary value to an
ASCII code. That is done by the ADD instruction in line 27. TRAP x21 in line
28 prints the ASCII code to the monitor.
The le that is to be examined starts at address x4000 (see line 2E). Usually,
this starting address would not be known to the programmer who is writing this
program since we would want the program to work on many les, not just the one
starting at x4000. To accomplish that, line 2E would be replaced with .BLKW
1 and be lled in by some other piece of code that knew the starting address of
the desired le before executing the program of Figure 7.2","{'page_number': 158, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Usually,\nthis starting address would not be known to the programmer who is writing this\nprogram since we would want the program to work on many les, not just the one\nstarting at x4000. To accomplish that, line 2E would be replaced with .BLKW\n1 and be lled in by some other piece of code that knew the starting address of\nthe desired le before executing the program of Figure 7.2. That situation will be\ndiscussed in Section 7.4.'}"
"Usually,
this starting address would not be known to the programmer who is writing this
program since we would want the program to work on many les, not just the one
starting at x4000. To accomplish that, line 2E would be replaced with .BLKW
1 and be lled in by some other piece of code that knew the starting address of
the desired le before executing the program of Figure 7.2. That situation will be
discussed in Section 7.4.","{'page_number': 158, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Usually,\nthis starting address would not be known to the programmer who is writing this\nprogram since we would want the program to work on many les, not just the one\nstarting at x4000. To accomplish that, line 2E would be replaced with .BLKW\n1 and be lled in by some other piece of code that knew the starting address of\nthe desired le before executing the program of Figure 7.2. That situation will be\ndiscussed in Section 7.4.'}"
"7.3 The Assembly Process
7.3.1 Introduction
Before an LC-3 assembly language program can be executed, it must rst be
translated into a machine language program, that is, one in which each instruction
is in the LC-3 ISA. It is the job of the LC-3 assembler to perform that translation.
If you have available an LC-3 assembler, you can cause it to translate your
assembly language program into a machine language program by executing an
appropriate command. In the LC-3 assembler that is generally available via the
web, that command is assemble, and it requires as an argument the lename of
your assembly language program. For example, if the lename is solution1.asm,
then
assemble solution1.asm outle
produces the le outle, which is in the ISA of the LC-3. It is necessary to check
with your instructor for the correct command line to cause the LC-3 assembler to
produce a le of 0s and 1s in the ISA of the LC-3.
7.3.2 A Two-Pass Process
In this section, we will see how the assembler goes through the process of trans-
lating an assembly language program into a machine language program. We will
use as our input to the process the assembly language program of Figure 7.2.
You remember that there is in general a one-to-one correspondence between
instructions in an assembly language program and instructions in the nal
machine language program. We could try to perform this translation in one pass
through the assembly language program. Starting from the top of Figure 7.2, the
assembler discards lines 01 to 09, since they contain only comments. Comments
are strictly for human consumption; they have no bearing on the translation pro-
cess. The assembler then moves on to line 0A. Line 0A is a pseudo-op; it tells
the assembler that the machine language program is to start at location x3000.
The assembler then moves on to line 0B, which it can easily translate into LC-3
machine code. At this point, we have
x3000:
0101010010100000
The LC-3 assembler moves on to translate the next instruction (line 0C). Unfor-
tunately, it is unable to do so since it does not know the meaning of the symbolic
address PTR. At this point the assembler is stuck, and the assembly process fails.
To prevent this from occurring, the assembly process is done in two com-
plete passes (from beginning to .END) through the entire assembly language
program. The objective of the rst pass is to identify the actual binary addresses
corresponding to the symbolic names (or labels). This set of correspondences is
known as the symbol table. In pass 1, we construct the symbol table. In pass 2, we
translate the individual assembly language instructions into their corresponding
machine language instructions.
Thus, when the assembler examines line 0C for the purpose of translating
LD R3,PTR","{'page_number': 159, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '7.3 The Assembly Process\n7.3.1 Introduction\nBefore an LC-3 assembly language program can be executed, it must rst be\ntranslated into a machine language program, that is, one in which each instruction\nis in the LC-3 ISA. It is the job of the LC-3 assembler to perform that translation.\nIf you have available an LC-3 assembler, you can cause it to translate your\nassembly language program into a machine language program by executing an\nappropriate command. In the LC-3 assembler that is generally available via the\nweb, that command is assemble, and it requires as an argument the lename of\nyour assembly language program. For example, if the lename is solution1.asm,\nthen\nassemble solution1.asm outle\nproduces the le outle, which is in the ISA of the LC-3. It is necessary to check\nwith your instructor for the correct command line to cause the LC-3 assembler to\nproduce a le of 0s and 1s in the ISA of the LC-3.\n7.3.2 A Two-Pass Process\nIn this section, we will see how the assembler goes through the process of trans-\nlating an assembly language program into a machine language program. We will\nuse as our input to the process the assembly language program of Figure 7.2.\nYou remember that there is in general a one-to-one correspondence between\ninstructions in an assembly language program and instructions in the nal\nmachine language program. We could try to perform this translation in one pass\nthrough the assembly language program. Starting from the top of Figure 7.2, the\nassembler discards lines 01 to 09, since they contain only comments. Comments\nare strictly for human consumption; they have no bearing on the translation pro-\ncess. The assembler then moves on to line 0A. Line 0A is a pseudo-op; it tells\nthe assembler that the machine language program is to start at location x3000.\nThe assembler then moves on to line 0B, which it can easily translate into LC-3\nmachine code. At this point, we have\nx3000:\n0101010010100000\nThe LC-3 assembler moves on to translate the next instruction (line 0C). Unfor-\ntunately, it is unable to do so since it does not know the meaning of the symbolic\naddress PTR. At this point the assembler is stuck, and the assembly process fails.\nTo prevent this from occurring, the assembly process is done in two com-\nplete passes (from beginning to .END) through the entire assembly language\nprogram. The objective of the rst pass is to identify the actual binary addresses\ncorresponding to the symbolic names (or labels). This set of correspondences is\nknown as the symbol table. In pass 1, we construct the symbol table. In pass 2, we\ntranslate the individual assembly language instructions into their corresponding\nmachine language instructions.\nThus, when the assembler examines line 0C for the purpose of translating\nLD R3,PTR'}"
"during the second pass, it already knows that PTR is the symbolic address of
memory location x3013 (from the rst pass). Thus, it can easily translate line
0C to
x3001:
0010011000010001
The problem of not knowing the 16-bit address corresponding to PTR no longer
exists.
7.3.3 The First Pass: Creating the Symbol Table
For our purposes, the symbol table is simply a correspondence of symbolic names
with their 16-bit memory addresses. We obtain these correspondences by pass-
ing through the assembly language program once, noting which instruction is
assigned to which memory location, and identifying each label with the memory
address of its assigned entry.
Recall that we provide labels in those cases where we have to refer to a loca-
tion, either because it is the target of a branch instruction or because it contains
data that must be loaded or stored. Consequently, if we have not made any pro-
gramming mistakes, and if we identify all the labels, we will have identied all
the symbolic addresses used in the program.
The preceding paragraph assumes that our entire program exists between our
.ORIG and .END pseudo-ops. This is true for the assembly language program
of Figure 7.2. In Section 7.4, we will consider programs that consist of multi-
ple parts, each with its own .ORIG and .END, wherein each part is assembled
separately.
The rst pass starts, after discarding the comments on lines 01 to 09, by
noting (line 0A) that the rst instruction will be assigned to address x3000. We
keep track of the location assigned to each instruction by means of a location
counter (LC). The LC is initialized to the address specied in .ORIG, that is,
x3000.
The assembler examines each instruction in sequence and increments the LC
once for each assembly language instruction. If the instruction examined contains
a label, a symbol table entry is made for that label, specifying the current con-
tents of LC as its address. The rst pass terminates when the .END pseudo-op is
reached.
The rst instruction that has a label is at line 13. Since it is the fth instruction
in the program and since the LC at that point contains x3004, a symbol table entry
is constructed thus:
Symbol
Address
TEST
x3004
The second instruction that has a label is at line 20. At this point, the LC has been
incremented to x300B. Thus, a symbol table entry is constructed, as follows:
Symbol
Address
GETCHAR
x300B","{'page_number': 160, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'during the second pass, it already knows that PTR is the symbolic address of\nmemory location x3013 (from the rst pass). Thus, it can easily translate line\n0C to\nx3001:\n0010011000010001\nThe problem of not knowing the 16-bit address corresponding to PTR no longer\nexists.\n7.3.3 The First Pass: Creating the Symbol Table\nFor our purposes, the symbol table is simply a correspondence of symbolic names\nwith their 16-bit memory addresses. We obtain these correspondences by pass-\ning through the assembly language program once, noting which instruction is\nassigned to which memory location, and identifying each label with the memory\naddress of its assigned entry.\nRecall that we provide labels in those cases where we have to refer to a loca-\ntion, either because it is the target of a branch instruction or because it contains\ndata that must be loaded or stored. Consequently, if we have not made any pro-\ngramming mistakes, and if we identify all the labels, we will have identied all\nthe symbolic addresses used in the program.\nThe preceding paragraph assumes that our entire program exists between our\n.ORIG and .END pseudo-ops. This is true for the assembly language program\nof Figure 7.2. In Section 7.4, we will consider programs that consist of multi-\nple parts, each with its own .ORIG and .END, wherein each part is assembled\nseparately.\nThe rst pass starts, after discarding the comments on lines 01 to 09, by\nnoting (line 0A) that the rst instruction will be assigned to address x3000. We\nkeep track of the location assigned to each instruction by means of a location\ncounter (LC). The LC is initialized to the address specied in .ORIG, that is,\nx3000.\nThe assembler examines each instruction in sequence and increments the LC\nonce for each assembly language instruction. If the instruction examined contains\na label, a symbol table entry is made for that label, specifying the current con-\ntents of LC as its address. The rst pass terminates when the .END pseudo-op is\nreached.\nThe rst instruction that has a label is at line 13. Since it is the fth instruction\nin the program and since the LC at that point contains x3004, a symbol table entry\nis constructed thus:\nSymbol\nAddress\nTEST\nx3004\nThe second instruction that has a label is at line 20. At this point, the LC has been\nincremented to x300B. Thus, a symbol table entry is constructed, as follows:\nSymbol\nAddress\nGETCHAR\nx300B'}"
"At the conclusion of the rst pass, the symbol table has the following entries:
Symbol
Address
TEST
x3004
GETCHAR
x300B
OUTPUT
x300E
ASCII
x3012
PTR
x3013
7.3.4 The Second Pass: Generating the
Machine Language Program
The second pass consists of going through the assembly language program a sec-
ond time, line by line, this time with the help of the symbol table. At each line,
the assembly language instruction is translated into an LC-3 machine language
instruction.
Starting again at the top, the assembler again discards lines 01 through 09
because they contain only comments. Line 0A is the .ORIG pseudo-op, which
the assembler uses to initialize LC to x3000. The assembler moves on to line 0B
and produces the machine language instruction 0101010010100000. Then the
assembler moves on to line 0C.
This time, when the assembler gets to line 0C, it can completely assemble
the instruction since it knows that PTR corresponds to x3013. The instruction is
LD, which has an opcode encoding of 0010. The destination register (DR) is R3,
that is, 011.
The only part of the LD instruction left to do is the PCoset. It is computed as
follows: The assembler knows that PTR is the label for address x3013 and that the
incremented PC is LC+1, in this case x3002. Since PTR (x3013) must be the sum
of the incremented PC (x3002) and the sign-extended PCoset, PCoset must be
x0011. Putting this all together, the assembler sets x3001 to 0010011000010001
and increments the LC to x3002.
Note: In order to use the LD instruction, it is necessary that the source of
the load, in this case the address whose label is PTR, is not more than +256 or
255 memory locations from the LD instruction itself. If the address of PTR
had been greater than LC+1+255 or less than LC+1256, then the oset would
not t in bits [8:0] of the instruction. In such a case, an assembly error would
have occurred, preventing the assembly process from nishing successfully. For-
tunately, PTR is close enough to the LD instruction, so the instruction assembled
correctly.
The second pass continues. At each step, the LC is incremented and the
location specied by LC is assigned the translated LC-3 instruction or, in the
case of .FILL, the value specied. When the second pass encounters the .END
pseudo-op, assembly terminates.
The resulting translated program is shown in Figure 7.3.
That process was, on a good day, merely tedious. Fortunately, you do not have
to do it for a livingthe LC-3 assembler does that. And, since you now know the","{'page_number': 161, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'At the conclusion of the rst pass, the symbol table has the following entries:\nSymbol\nAddress\nTEST\nx3004\nGETCHAR\nx300B\nOUTPUT\nx300E\nASCII\nx3012\nPTR\nx3013\n7.3.4 The Second Pass: Generating the\nMachine Language Program\nThe second pass consists of going through the assembly language program a sec-\nond time, line by line, this time with the help of the symbol table. At each line,\nthe assembly language instruction is translated into an LC-3 machine language\ninstruction.\nStarting again at the top, the assembler again discards lines 01 through 09\nbecause they contain only comments. Line 0A is the .ORIG pseudo-op, which\nthe assembler uses to initialize LC to x3000. The assembler moves on to line 0B\nand produces the machine language instruction 0101010010100000. Then the\nassembler moves on to line 0C.\nThis time, when the assembler gets to line 0C, it can completely assemble\nthe instruction since it knows that PTR corresponds to x3013. The instruction is\nLD, which has an opcode encoding of 0010. The destination register (DR) is R3,\nthat is, 011.\nThe only part of the LD instruction left to do is the PCoset. It is computed as\nfollows: The assembler knows that PTR is the label for address x3013 and that the\nincremented PC is LC+1, in this case x3002. Since PTR (x3013) must be the sum\nof the incremented PC (x3002) and the sign-extended PCoset, PCoset must be\nx0011. Putting this all together, the assembler sets x3001 to 0010011000010001\nand increments the LC to x3002.\nNote: In order to use the LD instruction, it is necessary that the source of\nthe load, in this case the address whose label is PTR, is not more than +256 or\n255 memory locations from the LD instruction itself. If the address of PTR\nhad been greater than LC+1+255 or less than LC+1256, then the oset would\nnot t in bits [8:0] of the instruction. In such a case, an assembly error would\nhave occurred, preventing the assembly process from nishing successfully. For-\ntunately, PTR is close enough to the LD instruction, so the instruction assembled\ncorrectly.\nThe second pass continues. At each step, the LC is incremented and the\nlocation specied by LC is assigned the translated LC-3 instruction or, in the\ncase of .FILL, the value specied. When the second pass encounters the .END\npseudo-op, assembly terminates.\nThe resulting translated program is shown in Figure 7.3.\nThat process was, on a good day, merely tedious. Fortunately, you do not have\nto do it for a livingthe LC-3 assembler does that. And, since you now know the'}"
"Figure 7.3
The machine language program for the assembly language program of
Figure 7.2.
LC-3 assembly language, there is no need to program in machine language. Now
we can write our programs symbolically in LC-3 assembly language and invoke
the LC-3 assembler to create the machine language versions that can execute on
an LC-3 computer.
7.4 Beyond the Assembly of a Single
Assembly Language Program
Our purpose in this chapter has been to take you up one more step from the ISA
of the computer and introduce assembly language. Although it is still quite a
large step from C or C++, assembly language does, in fact, save us a good deal
of pain. We have also shown how a rudimentary two-pass assembler actually
works to translate an assembly language program into the machine language of
the LC-3 ISA.
There are many more aspects to sophisticated assembly language program-
ming that go well beyond an introductory course. However, our reason for
teaching assembly language is not to deal with its sophistication, but rather to
show its innate simplicity. Before we leave this chapter, however, there are a few
additional highlights we should explore.","{'page_number': 162, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure 7.3\nThe machine language program for the assembly language program of\nFigure 7.2.\nLC-3 assembly language, there is no need to program in machine language. Now\nwe can write our programs symbolically in LC-3 assembly language and invoke\nthe LC-3 assembler to create the machine language versions that can execute on\nan LC-3 computer.\n7.4 Beyond the Assembly of a Single\nAssembly Language Program\nOur purpose in this chapter has been to take you up one more step from the ISA\nof the computer and introduce assembly language. Although it is still quite a\nlarge step from C or C++, assembly language does, in fact, save us a good deal\nof pain. We have also shown how a rudimentary two-pass assembler actually\nworks to translate an assembly language program into the machine language of\nthe LC-3 ISA.\nThere are many more aspects to sophisticated assembly language program-\nming that go well beyond an introductory course. However, our reason for\nteaching assembly language is not to deal with its sophistication, but rather to\nshow its innate simplicity. Before we leave this chapter, however, there are a few\nadditional highlights we should explore.'}"
"7.4.1 The Executable Image
When a computer begins execution of a program, the entity being executed is
called an executable image. The executable image is created from modules often
created independently by several dierent programmers. Each module is trans-
lated separately into an object le. We have just gone through the process of
performing that translation ourselves by mimicking the LC-3 assembler. Other
modules, some written in C perhaps, are translated by the C compiler. Some mod-
ules are written by users, and some modules are supplied as library routines by
the operating system. Each object le consists of instructions in the ISA of the
computer being used, along with its associated data. The nal step is to combine
(i.e., link) all the object modules together into one executable image. During exe-
cution of the program, the FETCH, DECODE,  instruction cycle is applied to
instructions in the executable image.
7.4.2 More than One Object File
It is very common to form an executable image from more than one object le.
In fact, in the real world, where most programs invoke libraries provided by the
operating system as well as modules generated by other programmers, it is much
more common to have multiple object les than a single one.
A case in point is our example character count program. The program counts
the number of occurrences of a character in a le. A typical application could
easily have the program as one module and the input data le as another. If this
were the case, then the starting address of the le, shown as x4000 in line 2E of
Figure 7.2, would not be known when the program was written. If we replace line
2E with
PTR
.FILL
STARTofFILE
then the program of Figure 7.2 will not assemble because there will be no symbol
table entry for STARTofFILE. What can we do?
If the LC-3 assembly language, on the other hand, contained the pseudo-op
.EXTERNAL, we could identify STARTofFILE as the symbolic name of an
address that is not known at the time the program of Figure 7.2 is assembled.
This would be done by the following line
.EXTERNAL
STARTofFILE,
which would send a message to the LC-3 assembler that the absence of label
STARTofFILE is not an error in the program. Rather, STARTofFILE is a label
in some other module that will be translated independently. In fact, in our case,
it will be the label of the location of the rst character in the le to be examined
by our character count program.
If the LC-3 assembly language had the pseudo-op .EXTERNAL, and if we
had designated STARTofFILE as .EXTERNAL, the LC-3 assembler would be
able to create a symbol table entry for STARTofFILE, and instead of assigning
it an address, it would mark the symbol as belonging to another module. At link","{'page_number': 163, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '7.4.1 The Executable Image\nWhen a computer begins execution of a program, the entity being executed is\ncalled an executable image. The executable image is created from modules often\ncreated independently by several dierent programmers. Each module is trans-\nlated separately into an object le. We have just gone through the process of\nperforming that translation ourselves by mimicking the LC-3 assembler. Other\nmodules, some written in C perhaps, are translated by the C compiler. Some mod-\nules are written by users, and some modules are supplied as library routines by\nthe operating system. Each object le consists of instructions in the ISA of the\ncomputer being used, along with its associated data. The nal step is to combine\n(i.e., link) all the object modules together into one executable image. During exe-\ncution of the program, the FETCH, DECODE,  instruction cycle is applied to\ninstructions in the executable image.\n7.4.2 More than One Object File\nIt is very common to form an executable image from more than one object le.\nIn fact, in the real world, where most programs invoke libraries provided by the\noperating system as well as modules generated by other programmers, it is much\nmore common to have multiple object les than a single one.\nA case in point is our example character count program. The program counts\nthe number of occurrences of a character in a le. A typical application could\neasily have the program as one module and the input data le as another. If this\nwere the case, then the starting address of the le, shown as x4000 in line 2E of\nFigure 7.2, would not be known when the program was written. If we replace line\n2E with\nPTR\n.FILL\nSTARTofFILE\nthen the program of Figure 7.2 will not assemble because there will be no symbol\ntable entry for STARTofFILE. What can we do?\nIf the LC-3 assembly language, on the other hand, contained the pseudo-op\n.EXTERNAL, we could identify STARTofFILE as the symbolic name of an\naddress that is not known at the time the program of Figure 7.2 is assembled.\nThis would be done by the following line\n.EXTERNAL\nSTARTofFILE,\nwhich would send a message to the LC-3 assembler that the absence of label\nSTARTofFILE is not an error in the program. Rather, STARTofFILE is a label\nin some other module that will be translated independently. In fact, in our case,\nit will be the label of the location of the rst character in the le to be examined\nby our character count program.\nIf the LC-3 assembly language had the pseudo-op .EXTERNAL, and if we\nhad designated STARTofFILE as .EXTERNAL, the LC-3 assembler would be\nable to create a symbol table entry for STARTofFILE, and instead of assigning\nit an address, it would mark the symbol as belonging to another module. At link'}"
"U
p to now we have completely ignored the details of input and output, that is,
how the computer actually gets information from the keyboard (input), and
how the computer actually delivers information to the monitor (output). Instead
we have relied on the TRAP instruction (e.g., TRAP x23 for input and TRAP x21
for output) to accomplish these tasks. The TRAP instruction enables us to tell the
operating system what we need done by means of a trap vector, and we trust the
operating system to do it for us.
The more generic term for our TRAP instruction is system call because the
TRAP instruction is calling on the operating system to do something for us while
allowing us to remain completely clueless as to how it gets done. Now we are
ready to examine how input and output actually work in the LC-3, what happens
when the user program makes a system call by invoking the TRAP instruction,
and how it all works under the control of the operating system.
We will start with the actual physical structures that are required to cause
input and output to occur. But before we do that, it is useful to say a few words
about the operating system and understand a few basic concepts that have not been
important so far but become very important when considering what the operating
system needs to do its job.
You may be familiar with Microsofts various avors of Windows, Apples
MacOS, and Linux. These are all examples of operating systems. They all have
the same goal: to optimize the use of all the resources of the computer system
while making sure that no software does harmful things to any program or data
that it has no right to mess with. To better understand their job, we need to under-
stand the notions of privilege and priority and the layout of the memory address
space (i.e., the regions of memory and the purpose of each).","{'page_number': 164, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'U\np to now we have completely ignored the details of input and output, that is,\nhow the computer actually gets information from the keyboard (input), and\nhow the computer actually delivers information to the monitor (output). Instead\nwe have relied on the TRAP instruction (e.g., TRAP x23 for input and TRAP x21\nfor output) to accomplish these tasks. The TRAP instruction enables us to tell the\noperating system what we need done by means of a trap vector, and we trust the\noperating system to do it for us.\nThe more generic term for our TRAP instruction is system call because the\nTRAP instruction is calling on the operating system to do something for us while\nallowing us to remain completely clueless as to how it gets done. Now we are\nready to examine how input and output actually work in the LC-3, what happens\nwhen the user program makes a system call by invoking the TRAP instruction,\nand how it all works under the control of the operating system.\nWe will start with the actual physical structures that are required to cause\ninput and output to occur. But before we do that, it is useful to say a few words\nabout the operating system and understand a few basic concepts that have not been\nimportant so far but become very important when considering what the operating\nsystem needs to do its job.\nYou may be familiar with Microsofts various avors of Windows, Apples\nMacOS, and Linux. These are all examples of operating systems. They all have\nthe same goal: to optimize the use of all the resources of the computer system\nwhile making sure that no software does harmful things to any program or data\nthat it has no right to mess with. To better understand their job, we need to under-\nstand the notions of privilege and priority and the layout of the memory address\nspace (i.e., the regions of memory and the purpose of each).'}"
"9.1 Privilege, Priority, and the
Memory Address Space
9.1.1 Privilege and Priority
Two very dierent (we often say orthogonal) concepts associated with computer
processing are privilege and priority.
9.1.1.1 Privilege
Privilege is all about the right to do something, such as execute a particular
instruction or access a particular memory location. Not all computer programs
have the right to execute all instructions. For example, if a computer system is
shared among many users and the ISA contains a HALT instruction, we would
not want any random program to execute that HALT instruction and stop the
computer. If we did, we would have some pretty disgruntled users on our hands.
Similarly, some memory locations are only available to the operating system. We
would not want some random program to interfere with the data structures or
code that is part of the operating system, which would in all likelihood cause the
entire system to crash. In order to make sure neither of these two things happens, we
designate every computer program as either privileged or unprivileged. We often
say supervisor privilege to indicate privileged. We say a program is executing in
Supervisor mode to indicate privileged, or User mode to indicate unprivileged. If a
program is executing in Supervisor mode, it can execute all instructions and access
all of memory. If a program is executing in User mode, it cannot. If a program
executing in User mode tries to execute an instruction or access a memory location
that requires being in Supervisor mode, the computer will not allow it.
9.1.1.2 Priority
Priority is all about the urgency of a program to execute. Every program is
assigned a priority, specifying its urgency as compared to all other programs.
This allows programs of greater urgency to interrupt programs of lesser urgency.
For example, programs written by random users may be assigned a priority of 0.
The keyboard may be asigned a priority of 4, and the fact that the computer is
plugged into a source of energy like a wall outlet may be assigned a priority of 6.
If that is the case, a random user program would be interrupted if someone sitting
at a keyboard wanted to execute a program that caused data to be input into the
computer. And that program would be interrupted if someone pulled the power
cord out of the wall outlet, causing the computer to quickly lose its source of
energy. In such an event, we would want the computer to execute some operating
system program that is provided specically to handle that situation.
9.1.1.3 Two Orthogonal Notions
We said privilege and priority are two orthogonal notions, meaning they have
nothing to do with each other. We humans sometimes have a problem with that
as we think of re trucks that have the privilege of ignoring trac lights because","{'page_number': 165, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '9.1 Privilege, Priority, and the\nMemory Address Space\n9.1.1 Privilege and Priority\nTwo very dierent (we often say orthogonal) concepts associated with computer\nprocessing are privilege and priority.\n9.1.1.1 Privilege\nPrivilege is all about the right to do something, such as execute a particular\ninstruction or access a particular memory location. Not all computer programs\nhave the right to execute all instructions. For example, if a computer system is\nshared among many users and the ISA contains a HALT instruction, we would\nnot want any random program to execute that HALT instruction and stop the\ncomputer. If we did, we would have some pretty disgruntled users on our hands.\nSimilarly, some memory locations are only available to the operating system. We\nwould not want some random program to interfere with the data structures or\ncode that is part of the operating system, which would in all likelihood cause the\nentire system to crash. In order to make sure neither of these two things happens, we\ndesignate every computer program as either privileged or unprivileged. We often\nsay supervisor privilege to indicate privileged. We say a program is executing in\nSupervisor mode to indicate privileged, or User mode to indicate unprivileged. If a\nprogram is executing in Supervisor mode, it can execute all instructions and access\nall of memory. If a program is executing in User mode, it cannot. If a program\nexecuting in User mode tries to execute an instruction or access a memory location\nthat requires being in Supervisor mode, the computer will not allow it.\n9.1.1.2 Priority\nPriority is all about the urgency of a program to execute. Every program is\nassigned a priority, specifying its urgency as compared to all other programs.\nThis allows programs of greater urgency to interrupt programs of lesser urgency.\nFor example, programs written by random users may be assigned a priority of 0.\nThe keyboard may be asigned a priority of 4, and the fact that the computer is\nplugged into a source of energy like a wall outlet may be assigned a priority of 6.\nIf that is the case, a random user program would be interrupted if someone sitting\nat a keyboard wanted to execute a program that caused data to be input into the\ncomputer. And that program would be interrupted if someone pulled the power\ncord out of the wall outlet, causing the computer to quickly lose its source of\nenergy. In such an event, we would want the computer to execute some operating\nsystem program that is provided specically to handle that situation.\n9.1.1.3 Two Orthogonal Notions\nWe said privilege and priority are two orthogonal notions, meaning they have\nnothing to do with each other. We humans sometimes have a problem with that\nas we think of re trucks that have the privilege of ignoring trac lights because'}"
"9.1
Privilege, Priority, and the Memory Address Space
315
they must quickly reach the re. In our daily lives, we often are given privileges
because of our greater sense of urgency. Not the case with computer systems.
For example, we can have a user program that is tied to a physics experiment
that needs to interrupt the computer at a specic instance of time to record infor-
mation being generated by the physics experiment. If the user program does not
pre-empt the program running at that instant of time, the data generated by the
experiment may be lost. This is a user program, so it does not have supervisor
privilege. But it does have a greater urgency, so it does have a higher priority.
Another example: The system administrator wants to execute diagnostic pro-
grams that access all memory locations and execute all instructions as part of
some standard preventive maintenance. The diagnostic program needs supervi-
sor privilege to execute all instructions and access all memory locations. But it
has no sense of urgency. Whether this happens at 1 a.m. or 2 a.m. is irrelevant,
compared to the urgency of other programs that need access to the computer
system exactly when they need it. The diagnostic program has privilege but no
priority.
Finally, an example showing that even in human activity one can have priority
but not privilege. Our friend Bob works in the basement of one of those New
York City skyscrapers. He is about to go to the mens room when his manager
tells him to take a message immediately to the vice president on the 88th oor,
and bring back a response. So Bob delays his visit to the mens room and takes
the elevator to the 88th oor. The vice president keeps him waiting, causing Bob
to be concerned he might have an accident. Finally, the vice president gives his
response, and Bob pushes the button to summon the elevator to take him back to
the basement, in pain because he needs to go to the mens room. While waiting for
the elevator, another vice president appears, unlocks the executive mens room,
and enters. Bob is in pain, but he cannot enter the executive mens room. Although
he certainly has the priority, he does not have the privilege!
9.1.1.4 The Processor Status Register (PSR)
Each program executing on the computer has associated with it two very impor-
tant registers. The Program Counter (PC) you are very familiar with. The other
register, the Processor Status Register (PSR), is shown in Figure 9.1. It contains
the privilege and priority assigned to that program.
Bit [15] species the privilege, where PSR[15]=0 means supervisor privi-
lege, and PSR[15]=1 means unprivileged. Bits [10:8] specify the priority level
(PL) of the program. The highest priority level is 7 (PL7), the lowest is PL0.
Figure 9.1
Processor status register (PSR).","{'page_number': 166, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '9.1\nPrivilege, Priority, and the Memory Address Space\n315\nthey must quickly reach the re. In our daily lives, we often are given privileges\nbecause of our greater sense of urgency. Not the case with computer systems.\nFor example, we can have a user program that is tied to a physics experiment\nthat needs to interrupt the computer at a specic instance of time to record infor-\nmation being generated by the physics experiment. If the user program does not\npre-empt the program running at that instant of time, the data generated by the\nexperiment may be lost. This is a user program, so it does not have supervisor\nprivilege. But it does have a greater urgency, so it does have a higher priority.\nAnother example: The system administrator wants to execute diagnostic pro-\ngrams that access all memory locations and execute all instructions as part of\nsome standard preventive maintenance. The diagnostic program needs supervi-\nsor privilege to execute all instructions and access all memory locations. But it\nhas no sense of urgency. Whether this happens at 1 a.m. or 2 a.m. is irrelevant,\ncompared to the urgency of other programs that need access to the computer\nsystem exactly when they need it. The diagnostic program has privilege but no\npriority.\nFinally, an example showing that even in human activity one can have priority\nbut not privilege. Our friend Bob works in the basement of one of those New\nYork City skyscrapers. He is about to go to the mens room when his manager\ntells him to take a message immediately to the vice president on the 88th oor,\nand bring back a response. So Bob delays his visit to the mens room and takes\nthe elevator to the 88th oor. The vice president keeps him waiting, causing Bob\nto be concerned he might have an accident. Finally, the vice president gives his\nresponse, and Bob pushes the button to summon the elevator to take him back to\nthe basement, in pain because he needs to go to the mens room. While waiting for\nthe elevator, another vice president appears, unlocks the executive mens room,\nand enters. Bob is in pain, but he cannot enter the executive mens room. Although\nhe certainly has the priority, he does not have the privilege!\n9.1.1.4 The Processor Status Register (PSR)\nEach program executing on the computer has associated with it two very impor-\ntant registers. The Program Counter (PC) you are very familiar with. The other\nregister, the Processor Status Register (PSR), is shown in Figure 9.1. It contains\nthe privilege and priority assigned to that program.\nBit [15] species the privilege, where PSR[15]=0 means supervisor privi-\nlege, and PSR[15]=1 means unprivileged. Bits [10:8] specify the priority level\n(PL) of the program. The highest priority level is 7 (PL7), the lowest is PL0.\nFigure 9.1\nProcessor status register (PSR).'}"
"9.1.2 Organization of Memory
Figure 9.2 shows the layout of the LC-3 memory.
You know that the LC-3 has a 16-bit address space; ergo, memory locations
from x0000 to xFFFF. Locations x0000 to x2FFF are privileged memory loca-
tions. They contain the various data structures and code of the operating system.
They require supervisor privilege to access. They are referred to as system space.
Locations x3000 to xFDFF are unprivileged memory locations. Supervisor
privilege is not required to access these memory locations. All user programs and
data use this region of memory. The region is often referred to as user space.
Addresses xFE00 to xFFFF do not correspond to memory locations at all.
That is, the last address of a memory location is xFDFF. Addresses xFE00 to
xFFFF are used to identify registers that take part in input and output functions
and some special registers associated with the processor. For example, the PSR
is assigned address xFFFC, and the processors Master Control Register (MCR)
is assigned address xFFFE. The benet of assigning addresses from the memory
address space will be discussed in Section 9.2.1.2. The set of addresses from
xFE00 to xFFFF is usually referred to as the I/O page since most of the addresses
are used for identif in re isters that take art in in ut or out ut functions. Access
Figure 9.2
Regions of memory.","{'page_number': 167, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '9.1.2 Organization of Memory\nFigure 9.2 shows the layout of the LC-3 memory.\nYou know that the LC-3 has a 16-bit address space; ergo, memory locations\nfrom x0000 to xFFFF. Locations x0000 to x2FFF are privileged memory loca-\ntions. They contain the various data structures and code of the operating system.\nThey require supervisor privilege to access. They are referred to as system space.\nLocations x3000 to xFDFF are unprivileged memory locations. Supervisor\nprivilege is not required to access these memory locations. All user programs and\ndata use this region of memory. The region is often referred to as user space.\nAddresses xFE00 to xFFFF do not correspond to memory locations at all.\nThat is, the last address of a memory location is xFDFF. Addresses xFE00 to\nxFFFF are used to identify registers that take part in input and output functions\nand some special registers associated with the processor. For example, the PSR\nis assigned address xFFFC, and the processors Master Control Register (MCR)\nis assigned address xFFFE. The benet of assigning addresses from the memory\naddress space will be discussed in Section 9.2.1.2. The set of addresses from\nxFE00 to xFFFF is usually referred to as the I/O page since most of the addresses\nare used for identif in re isters that take art in in ut or out ut functions. Access\nFigure 9.2\nRegions of memory.'}"
"operating system and requires supervisor privilege to access. The user stack is
controlled by the user program and does not require privilege to access.
Each has a stack pointer, Supervisor Stack Pointer (SSP) and User Stack
Pointer (USP), to indicate the top of the stack. Since a program can only execute
in Supervisor mode or User mode at any one time, only one of the two stacks is
active at any one time. Register 6 is generally used as the stack pointer (SP) for
the active stack. Two registers, Saved SSP and Saved USP, are provided to save
the SP not in use. When privilege changes, for example, from Supervisor mode to
User mode, the SP is stored in Saved SSP, and the SP is loaded from Saved USP.
9.2 Input/Output
Input and output devices (keyboards, monitors, disks, or kiosks at the shopping
mall) all handle input or output data using registers that are tailored to the needs
of each particular input or output device. Even the simplest I/O devices usually
need at least two registers: one to hold the data being transferred between the
device and the computer, and one to indicate status information about the device.
An example of status information is whether the device is available or is it still
busy processing the most recent I/O task.
9.2.1 Some Basic Characteristics of I/O
All I/O activity is controlled by instructions in the computers ISA. Does the ISA
need special instructions for dealing with I/O? Does the I/O device execute at the
same speed as the computer, and if not, what manages the dierence in speeds? Is
the transfer of information between the computer and the I/O device initiated by
a program executing in the computer, or is it initiated by the I/O device? Answers
to these questions form some of the basic characteristics of I/O activity.
9.2.1.1 Memory-Mapped I/O vs. Special I/O Instructions
An instruction that interacts with an input or output device register must identify
the particular input or output device register with which it is interacting. Two
schemes have been used in the past. Some computers use special input and output
instructions. Most computers prefer to use the same data movement instructions
that are used to move data in and out of memory.
The very old PDP-8 (from Digital Equipment Corporation, more than 50
years ago1965) is an example of a computer that used special input and output
instructions. The 12-bit PDP-8 instruction contained a three-bit opcode. If the
opcode was 110, an I/O instruction was indicated. The remaining nine bits of the
PDP-8 instruction identied which I/O device register and what operation was to
be performed.
Most computer designers prefer not to specify an additional set of instructions
for dealing with input and output. They use the same data movement instructions
that are used for loading and storing data between memory and the general pur-
pose registers. For example, a load instruction (LD, LDI, or LDR), in which the","{'page_number': 168, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'operating system and requires supervisor privilege to access. The user stack is\ncontrolled by the user program and does not require privilege to access.\nEach has a stack pointer, Supervisor Stack Pointer (SSP) and User Stack\nPointer (USP), to indicate the top of the stack. Since a program can only execute\nin Supervisor mode or User mode at any one time, only one of the two stacks is\nactive at any one time. Register 6 is generally used as the stack pointer (SP) for\nthe active stack. Two registers, Saved SSP and Saved USP, are provided to save\nthe SP not in use. When privilege changes, for example, from Supervisor mode to\nUser mode, the SP is stored in Saved SSP, and the SP is loaded from Saved USP.\n9.2 Input/Output\nInput and output devices (keyboards, monitors, disks, or kiosks at the shopping\nmall) all handle input or output data using registers that are tailored to the needs\nof each particular input or output device. Even the simplest I/O devices usually\nneed at least two registers: one to hold the data being transferred between the\ndevice and the computer, and one to indicate status information about the device.\nAn example of status information is whether the device is available or is it still\nbusy processing the most recent I/O task.\n9.2.1 Some Basic Characteristics of I/O\nAll I/O activity is controlled by instructions in the computers ISA. Does the ISA\nneed special instructions for dealing with I/O? Does the I/O device execute at the\nsame speed as the computer, and if not, what manages the dierence in speeds? Is\nthe transfer of information between the computer and the I/O device initiated by\na program executing in the computer, or is it initiated by the I/O device? Answers\nto these questions form some of the basic characteristics of I/O activity.\n9.2.1.1 Memory-Mapped I/O vs. Special I/O Instructions\nAn instruction that interacts with an input or output device register must identify\nthe particular input or output device register with which it is interacting. Two\nschemes have been used in the past. Some computers use special input and output\ninstructions. Most computers prefer to use the same data movement instructions\nthat are used to move data in and out of memory.\nThe very old PDP-8 (from Digital Equipment Corporation, more than 50\nyears ago1965) is an example of a computer that used special input and output\ninstructions. The 12-bit PDP-8 instruction contained a three-bit opcode. If the\nopcode was 110, an I/O instruction was indicated. The remaining nine bits of the\nPDP-8 instruction identied which I/O device register and what operation was to\nbe performed.\nMost computer designers prefer not to specify an additional set of instructions\nfor dealing with input and output. They use the same data movement instructions\nthat are used for loading and storing data between memory and the general pur-\npose registers. For example, a load instruction (LD, LDI, or LDR), in which the'}"
"source address is that of an input device register, is an input instruction. Similarly,
a store instruction (ST, STI, or STR) in which the destination address is that of
an output device register is an output instruction.
Since programmers use the same data movement instructions that are used
for memory, every input device register and every output device register must be
uniquely identied in the same way that memory locations are uniquely identied.
Therefore, each device register is assigned an address from the memory address
space of the ISA. That is, the I/O device registers are mapped to a set of addresses
that are allocated to I/O device registers rather than to memory locations. Hence
the name memory-mapped I/O.
The original PDP-11 ISA had a 16-bit address space. All addresses wherein
bits [15:13] = 111 were allocated to I/O device registers. That is, of the 216
addresses, only 57,344 corresponded to memory locations. The remaining 213
were memory-mapped I/O addresses.
The LC-3 uses memory-mapped I/O. As we discussed in Section 9.1.2,
addresses x0000 to xFDFF refer to actual memory locations. Addresses xFE00 to
xFFFF are reserved for input/output device registers. Table A.3 lists the memory-
mapped addresses of the LC-3 device registers that have been assigned so far.
Future uses and future sales of LC-3 microprocessors may require the expansion
of device register address assignments as new and exciting applications emerge!
9.2.1.2 Asynchronous vs. Synchronous
Most I/O is carried out at speeds very much slower than the speed of the processor.
A typist, typing on a keyboard, loads an input device register with one ASCII
code every time he/she types a character. A computer can read the contents of
that device register every time it executes a load instruction, where the operand
address is the memory-mapped address of that input device register.
Many of todays microprocessors execute instructions under the control of a
clock that operates well in excess of 2 GHz. Even for a microprocessor operating
at only 2 GHz, a clock cycle lasts only 0.5 nanoseconds. Suppose a processor
executed one instruction at a time, and it took the processor ten clock cycles to
execute the instruction that reads the input device register and stores its contents.
At that rate, the processor could read the contents of the input device register once
every 5 nanoseconds. Unfortunately, people do not type fast enough to keep this
processor busy full-time reading characters. Question: How fast would a person
have to type to supply input characters to the processor at the maximum rate the
processor can receive them?
We could mitigate this speed disparity by designing hardware that would
accept typed characters at some slower xed rate. For example, we could design
a piece of hardware that accepts one character every 200 million cycles. This
would require a typing speed of 100 words/minute, assuming words on average
consisted of ve letters, which is certainly doable. Unfortunately, it would also
require that the typist work in lockstep with the computers clock","{'page_number': 169, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'For example, we could design\na piece of hardware that accepts one character every 200 million cycles. This\nwould require a typing speed of 100 words/minute, assuming words on average\nconsisted of ve letters, which is certainly doable. Unfortunately, it would also\nrequire that the typist work in lockstep with the computers clock. That is not\nacceptable since the typing speed (even of the same typist) varies from moment\nto moment.\nWhats the point? The point is that I/O devices usually operate at speeds\nvery dierent from that of a microprocessor, and not in lockstep. We call this'}"
"For example, we could design
a piece of hardware that accepts one character every 200 million cycles. This
would require a typing speed of 100 words/minute, assuming words on average
consisted of ve letters, which is certainly doable. Unfortunately, it would also
require that the typist work in lockstep with the computers clock. That is not
acceptable since the typing speed (even of the same typist) varies from moment
to moment.
Whats the point? The point is that I/O devices usually operate at speeds
very dierent from that of a microprocessor, and not in lockstep. We call this","{'page_number': 169, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'For example, we could design\na piece of hardware that accepts one character every 200 million cycles. This\nwould require a typing speed of 100 words/minute, assuming words on average\nconsisted of ve letters, which is certainly doable. Unfortunately, it would also\nrequire that the typist work in lockstep with the computers clock. That is not\nacceptable since the typing speed (even of the same typist) varies from moment\nto moment.\nWhats the point? The point is that I/O devices usually operate at speeds\nvery dierent from that of a microprocessor, and not in lockstep. We call this'}"
"latter characteristic asynchronous. Most interaction between a processor and I/O
is asynchronous. To control processing in an asynchronous world requires some
protocol or handshaking mechanism. So it is with our keyboard and monitor. In
the case of the keyboard, we will need a one-bit status register, called a ag, to
indicate if someone has or has not typed a character. In the case of the monitor,
we will need a one-bit status register to indicate whether or not the most recent
character sent to the monitor has been displayed, and so the monitor can be given
another character to display.
These ags are the simplest form of synchronization. A single ag, called
the ready bit, is enough to synchronize the output of the typist who can type
characters at the rate of 100 words/minute with the input to a processor that can
accept these characters at the rate of 200 million characters/second. Each time
the typist types a character, the ready bit is set to 1. Each time the computer reads
a character, it clears the ready bit. By examining the ready bit before reading
a character, the computer can tell whether it has already read the last character
typed. If the ready bit is clear, no characters have been typed since the last time
the computer read a character, and so no additional read would take place. When
the computer detects that the ready bit is set, it could only have been caused by a
new character being typed, so the computer would know to again read a character.
The single ready bit provides enough handshaking to ensure that the asyn-
chronous transfer of information between the typist and the microprocessor can
be carried out accurately.
If the typist could type at a constant speed, and we did have a piece of hard-
ware that would accept typed characters at precise intervals (e.g., one character
every 200 million cycles), then we would not need the ready bit. The computer
would simply know, after 200 million cycles of doing other stu, that the typist
had typed exactly one more character, and the computer would read that charac-
ter. In this hypothetical situation, the typist would be typing in lockstep with the
processor, and no additional synchronization would be needed. We would say the
computer and typist were operating synchronously. That is, the input activity was
synchronous.
9.2.1.3 Interrupt-Driven vs. Polling
The processor, which is computing, and the typist, who is typing, are two separate
entities. Each is doing its own thing. Still, they need to interact; that is, the data that
is typed has to get into the computer. The issue of interrupt-driven vs. polling is
the issue of who controls the interaction. Does the processor do its own thing until
being interrupted by an announcement from the keyboard, Hey, a key has been
struck. The ASCII code is in the input device register. You need to read it. This is
called interrupt-driven I/O, where the keyboard controls the interaction. Or, does
the processor control the interaction, specifically by interrogating (usually, again
and again) the ready bit until it (the processor) detects that the ready bit is set","{'page_number': 170, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Does the processor do its own thing until\nbeing interrupted by an announcement from the keyboard, Hey, a key has been\nstruck. The ASCII code is in the input device register. You need to read it. This is\ncalled interrupt-driven I/O, where the keyboard controls the interaction. Or, does\nthe processor control the interaction, specifically by interrogating (usually, again\nand again) the ready bit until it (the processor) detects that the ready bit is set. At\nthat point, the processor knows it is time to read the device register. This second\ntype of interaction when the processor is in charge is called polling, since the ready\nbit is polled by the processor, asking if any key has been struck.\nSection 9.2.2.2 describes how polling works. Section9.4 explains interrupt-\ndriven I/O.'}"
"Does the processor do its own thing until
being interrupted by an announcement from the keyboard, Hey, a key has been
struck. The ASCII code is in the input device register. You need to read it. This is
called interrupt-driven I/O, where the keyboard controls the interaction. Or, does
the processor control the interaction, specifically by interrogating (usually, again
and again) the ready bit until it (the processor) detects that the ready bit is set. At
that point, the processor knows it is time to read the device register. This second
type of interaction when the processor is in charge is called polling, since the ready
bit is polled by the processor, asking if any key has been struck.
Section 9.2.2.2 describes how polling works. Section9.4 explains interrupt-
driven I/O.","{'page_number': 170, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Does the processor do its own thing until\nbeing interrupted by an announcement from the keyboard, Hey, a key has been\nstruck. The ASCII code is in the input device register. You need to read it. This is\ncalled interrupt-driven I/O, where the keyboard controls the interaction. Or, does\nthe processor control the interaction, specifically by interrogating (usually, again\nand again) the ready bit until it (the processor) detects that the ready bit is set. At\nthat point, the processor knows it is time to read the device register. This second\ntype of interaction when the processor is in charge is called polling, since the ready\nbit is polled by the processor, asking if any key has been struck.\nSection 9.2.2.2 describes how polling works. Section9.4 explains interrupt-\ndriven I/O.'}"
"9.2.2 Input from the Keyboard
9.2.2.1 Basic Input Registers (KBDR and KBSR)
We have already noted that in order to handle character input from the keyboard,
we need two things: a data register that contains the character to be input and
a synchronization mechanism to let the processor know that input has occurred.
The synchronization mechanism is contained in the status register associated with
the keyboard.
These two registers are called the keyboard data register (KBDR) and the
KBSR
Figure 9.3
Keyboard device registers.
Even though a character needs only 8 bits and the synchronization mecha-
nism needs only 1 bit, it is easier to assign 16 bits (like all memory addresses
in the LC-3) to each. In the case of KBDR, bits [7:0] are used for the data, and
bits [15:8] contain x00. In the case of KBSR, bit [15] contains the synchroniza-
tion mechanism, that is, the ready bit. Figure 9.3 shows the two device registers
needed by the keyboard.
9.2.2.2 The Basic Input Service Routine
KBSR[15] controls the synchronization of the slow keyboard and the fast pro-
cessor. When a key on the keyboard is struck, the ASCII code for that key is
loaded into KBDR[7:0], and the electronic circuits associated with the keyboard
automatically set KBSR[15] to 1. When the LC-3 reads KBDR, the electronic
circuits associated with the keyboard automatically clear KBSR[15], allowing
another key to be struck. If KBSR[15] = 1, the ASCII code corresponding to the
last key struck has not yet been read, and so the keyboard is disabled; that is, no
key can be struck until the last key is read.
If input/output is controlled by the processor (i.e., via polling), then a pro-
gram can repeatedly test KBSR[15] until it notes that the bit is set. At that point,
the processor can load the ASCII code contained in KBDR into one of the LC-3
registers. Since the processor only loads the ASCII code if KBSR[15] is 1, there is
no danger of reading a single typed character multiple times. Furthermore, since
the keyboard is disabled until the previous code is read, there is no danger of the
processor missing characters that were typed. In this way, KBSR[15] provides
the mechanism to guarantee that each key typed will be loaded exactly once.","{'page_number': 171, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '9.2.2 Input from the Keyboard\n9.2.2.1 Basic Input Registers (KBDR and KBSR)\nWe have already noted that in order to handle character input from the keyboard,\nwe need two things: a data register that contains the character to be input and\na synchronization mechanism to let the processor know that input has occurred.\nThe synchronization mechanism is contained in the status register associated with\nthe keyboard.\nThese two registers are called the keyboard data register (KBDR) and the\nKBSR\nFigure 9.3\nKeyboard device registers.\nEven though a character needs only 8 bits and the synchronization mecha-\nnism needs only 1 bit, it is easier to assign 16 bits (like all memory addresses\nin the LC-3) to each. In the case of KBDR, bits [7:0] are used for the data, and\nbits [15:8] contain x00. In the case of KBSR, bit [15] contains the synchroniza-\ntion mechanism, that is, the ready bit. Figure 9.3 shows the two device registers\nneeded by the keyboard.\n9.2.2.2 The Basic Input Service Routine\nKBSR[15] controls the synchronization of the slow keyboard and the fast pro-\ncessor. When a key on the keyboard is struck, the ASCII code for that key is\nloaded into KBDR[7:0], and the electronic circuits associated with the keyboard\nautomatically set KBSR[15] to 1. When the LC-3 reads KBDR, the electronic\ncircuits associated with the keyboard automatically clear KBSR[15], allowing\nanother key to be struck. If KBSR[15] = 1, the ASCII code corresponding to the\nlast key struck has not yet been read, and so the keyboard is disabled; that is, no\nkey can be struck until the last key is read.\nIf input/output is controlled by the processor (i.e., via polling), then a pro-\ngram can repeatedly test KBSR[15] until it notes that the bit is set. At that point,\nthe processor can load the ASCII code contained in KBDR into one of the LC-3\nregisters. Since the processor only loads the ASCII code if KBSR[15] is 1, there is\nno danger of reading a single typed character multiple times. Furthermore, since\nthe keyboard is disabled until the previous code is read, there is no danger of the\nprocessor missing characters that were typed. In this way, KBSR[15] provides\nthe mechanism to guarantee that each key typed will be loaded exactly once.'}"
",
04
BRnzp
NEXT_TASK
; Go to the next task
05
A
.FILL
xFE00
; Address of KBSR
06
B
.FILL
xFE02
; Address of KBDR
As long as KBSR[15] is 0, no key has been struck since the last time the processor
read the data register. Lines 01 and 02 comprise a loop that tests bit [15] of KBSR.
Note the use of the LDI instruction, which loads R1 with the contents of xFE00,
the memory-mapped address of KBSR. If the ready bit, bit [15], is clear, BRzp
will branch to START and another iteration of the loop. When someone strikes a
key, KBDR will be loaded with the ASCII code of that key, and the ready bit of
KBSR will be set. This will cause the branch to fall through, and the instruction
at line 03 will be executed. Again, note the use of the LDI instruction, which
this time loads R0 with the contents of xFE02, the memory-mapped address of
KBDR. The input routine is now done, so the program branches unconditionally
to its NEXT TASK.
Figure 9.4
Memory-mapped input.","{'page_number': 172, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': ',\n04\nBRnzp\nNEXT_TASK\n; Go to the next task\n05\nA\n.FILL\nxFE00\n; Address of KBSR\n06\nB\n.FILL\nxFE02\n; Address of KBDR\nAs long as KBSR[15] is 0, no key has been struck since the last time the processor\nread the data register. Lines 01 and 02 comprise a loop that tests bit [15] of KBSR.\nNote the use of the LDI instruction, which loads R1 with the contents of xFE00,\nthe memory-mapped address of KBSR. If the ready bit, bit [15], is clear, BRzp\nwill branch to START and another iteration of the loop. When someone strikes a\nkey, KBDR will be loaded with the ASCII code of that key, and the ready bit of\nKBSR will be set. This will cause the branch to fall through, and the instruction\nat line 03 will be executed. Again, note the use of the LDI instruction, which\nthis time loads R0 with the contents of xFE02, the memory-mapped address of\nKBDR. The input routine is now done, so the program branches unconditionally\nto its NEXT TASK.\nFigure 9.4\nMemory-mapped input.'}"
"carry out the EXECUTE phase of the load instructions. Essentially three steps
are required:
1. The MAR is loaded with the address of the memory location to be read.
2. Memory is read, resulting in MDR being loaded with the contents at the
specied memory location.
3. The destination register (DR) is loaded with the contents of MDR.
In the case of memory-mapped input, the same steps are carried out, except
instead of MAR being loaded with the address of a memory location, MAR is
loaded with the address of a device register. Instead of the address control logic
enabling memory to read, the address control logic selects the corresponding
device register to provide input to the MDR.
9.2.3 Output to the Monitor
9.2.3.1 Basic Output Registers (DDR and DSR)
Out ut works in a wa ver
similar to in ut with DDR and DSR re lacin the
DSR
Figure 9.5
Monitor device registers.
As is the case with input, even though an output character needs only 8 bits
and the synchronization mechanism needs only one bit, it is easier to assign
16 bits (like all memory addresses in the LC-3) to each output device register.
In the case of DDR, bits [7:0] are used for data, and bits [15:8] contain x00. In
the case of DSR, bit [15] contains the synchronization mechanism, that is, the
ready bit. Figure 9.5 shows the two device registers needed by the monitor.
9.2.3.2 The Basic Output Service Routine
DSR[15] controls the synchronization of the fast processor and the slow monitor
display. When the LC-3 transfers an ASCII code to DDR[7:0] for outputting, the
electronics of the monitor automatically clear DSR[15] as the processing of the
contents of DDR[7:0] begins. When the monitor nishes processing the character
on the screen, it (the monitor) automatically sets DSR[15]. This is a signal to
the processor that it (the processor) can transfer another ASCII code to DDR
for outputting. As long as DSR[15] is clear, the monitor is still processing the
previous character, so the monitor is disabled as far as additional output from the
processor is concerned.","{'page_number': 173, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'carry out the EXECUTE phase of the load instructions. Essentially three steps\nare required:\n1. The MAR is loaded with the address of the memory location to be read.\n2. Memory is read, resulting in MDR being loaded with the contents at the\nspecied memory location.\n3. The destination register (DR) is loaded with the contents of MDR.\nIn the case of memory-mapped input, the same steps are carried out, except\ninstead of MAR being loaded with the address of a memory location, MAR is\nloaded with the address of a device register. Instead of the address control logic\nenabling memory to read, the address control logic selects the corresponding\ndevice register to provide input to the MDR.\n9.2.3 Output to the Monitor\n9.2.3.1 Basic Output Registers (DDR and DSR)\nOut ut works in a wa ver\nsimilar to in ut with DDR and DSR re lacin the\nDSR\nFigure 9.5\nMonitor device registers.\nAs is the case with input, even though an output character needs only 8 bits\nand the synchronization mechanism needs only one bit, it is easier to assign\n16 bits (like all memory addresses in the LC-3) to each output device register.\nIn the case of DDR, bits [7:0] are used for data, and bits [15:8] contain x00. In\nthe case of DSR, bit [15] contains the synchronization mechanism, that is, the\nready bit. Figure 9.5 shows the two device registers needed by the monitor.\n9.2.3.2 The Basic Output Service Routine\nDSR[15] controls the synchronization of the fast processor and the slow monitor\ndisplay. When the LC-3 transfers an ASCII code to DDR[7:0] for outputting, the\nelectronics of the monitor automatically clear DSR[15] as the processing of the\ncontents of DDR[7:0] begins. When the monitor nishes processing the character\non the screen, it (the monitor) automatically sets DSR[15]. This is a signal to\nthe processor that it (the processor) can transfer another ASCII code to DDR\nfor outputting. As long as DSR[15] is clear, the monitor is still processing the\nprevious character, so the monitor is disabled as far as additional output from the\nprocessor is concerned.'}"
"If input/output is controlled by the processor (i.e., via polling), a program can
,
04
BRnzp
NEXT_TASK
05
A
.FILL
xFE04
; Address of DSR
06
B
.FILL
xFE06
; Address of DDR
Like the routine for KBDR and KBSR in Section 9.2.2.2, lines 01 and 02 repeat-
edly poll DSR[15] to see if the monitor electronics is nished with the last
character shipped by the processor. Note the use of LDI and the indirect access
to xFE04, the memory-mapped address of DSR. As long as DSR[15] is clear,
the monitor electronics is still processing this character, and BRzp branches to
START for another iteration of the loop. When the monitor electronics nishes
with the last character shipped by the processor, it automatically sets DSR[15]
to 1, which causes the branch to fall through and the instruction at line 03 to be
executed. Note the use of the STI instruction, which stores R0 into xFE06, the
memory-mapped address of DDR. The write to DDR also clears DSR[15], dis-
abling for the moment DDR from further output. The monitor electronics takes
over and writes the character to the screen. Since the output routine is now done,
the program unconditionally branches (line 04) to its NEXT TASK.
9.2.3.3 Implementation of Memory-Mapped Output
Figure 9.6 shows the additional data path required to implement memory-mapped
output. As we discussed previously with respect to memory-mapped input,
the mechanisms for handling the device registers provide very little additional
complexity to what already exists for handling memory accesses.
In Chapter 5, you became familiar with the process of carrying out the
EXECUTE phase of the store instructions.
1. The MAR is loaded with the address of the memory location to be written.
2. The MDR is loaded with the data to be written to memory.
3. Memory is written, resulting in the contents of MDR being stored in the
specied memory location.
In the case of memory-mapped output, the same steps are carried out, except
instead of MAR being loaded with the address of a memory location, MAR is
loaded with the address of a device register. Instead of the address control logic
enabling memory to write, the address control logic asserts the load enable signal
of DDR.
Memory-mapped output also requires the ability to read output device reg-
isters. You saw in Section 9.2.3.2 that before the DDR could be loaded, the ready","{'page_number': 174, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'If input/output is controlled by the processor (i.e., via polling), a program can\n,\n04\nBRnzp\nNEXT_TASK\n05\nA\n.FILL\nxFE04\n; Address of DSR\n06\nB\n.FILL\nxFE06\n; Address of DDR\nLike the routine for KBDR and KBSR in Section 9.2.2.2, lines 01 and 02 repeat-\nedly poll DSR[15] to see if the monitor electronics is nished with the last\ncharacter shipped by the processor. Note the use of LDI and the indirect access\nto xFE04, the memory-mapped address of DSR. As long as DSR[15] is clear,\nthe monitor electronics is still processing this character, and BRzp branches to\nSTART for another iteration of the loop. When the monitor electronics nishes\nwith the last character shipped by the processor, it automatically sets DSR[15]\nto 1, which causes the branch to fall through and the instruction at line 03 to be\nexecuted. Note the use of the STI instruction, which stores R0 into xFE06, the\nmemory-mapped address of DDR. The write to DDR also clears DSR[15], dis-\nabling for the moment DDR from further output. The monitor electronics takes\nover and writes the character to the screen. Since the output routine is now done,\nthe program unconditionally branches (line 04) to its NEXT TASK.\n9.2.3.3 Implementation of Memory-Mapped Output\nFigure 9.6 shows the additional data path required to implement memory-mapped\noutput. As we discussed previously with respect to memory-mapped input,\nthe mechanisms for handling the device registers provide very little additional\ncomplexity to what already exists for handling memory accesses.\nIn Chapter 5, you became familiar with the process of carrying out the\nEXECUTE phase of the store instructions.\n1. The MAR is loaded with the address of the memory location to be written.\n2. The MDR is loaded with the data to be written to memory.\n3. Memory is written, resulting in the contents of MDR being stored in the\nspecied memory location.\nIn the case of memory-mapped output, the same steps are carried out, except\ninstead of MAR being loaded with the address of a memory location, MAR is\nloaded with the address of a device register. Instead of the address control logic\nenabling memory to write, the address control logic asserts the load enable signal\nof DDR.\nMemory-mapped output also requires the ability to read output device reg-\nisters. You saw in Section 9.2.3.2 that before the DDR could be loaded, the ready'}"
"324
INMUX
Figure 9.6
Memory-mapped output.
bit had to be in state 1, indicating that the previous character had already n-
ished being written to the screen. The LDI and BRzp instructions on lines 01
and 02 perform that test. To do this, the LDI reads the output device register
DSR, and BRzp tests bit [15]. If the MAR is loaded with xFE04 (the memory-
mapped address of the DSR), the address control logic selects DSR as the input
to the MDR, where it is subsequently loaded into R1, and the condition codes
are set.
9.2.3.4 Example: Keyboard Echo
When we type at the keyboard, it is helpful to know exactly what characters we
0B
DDR
.FILL
xFE06
; Address of DDR","{'page_number': 175, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '324\nINMUX\nFigure 9.6\nMemory-mapped output.\nbit had to be in state 1, indicating that the previous character had already n-\nished being written to the screen. The LDI and BRzp instructions on lines 01\nand 02 perform that test. To do this, the LDI reads the output device register\nDSR, and BRzp tests bit [15]. If the MAR is loaded with xFE04 (the memory-\nmapped address of the DSR), the address control logic selects DSR as the input\nto the MDR, where it is subsequently loaded into R1, and the condition codes\nare set.\n9.2.3.4 Example: Keyboard Echo\nWhen we type at the keyboard, it is helpful to know exactly what characters we\n0B\nDDR\n.FILL\nxFE06\n; Address of DDR'}"
"9.2.4 A More Sophisticated Input Routine
In the example of Section 9.2.2.2, the input routine would be a part of a program
being executed by the computer. Presumably, the program requires character input
from the keyboard. But how does the person sitting at the keyboard know when to
type a character? Sitting there, the person may wonder whether or not the program
is actually running, or if perhaps the computer is busy doing something else.
To let the person sitting at the keyboard know that the program is waiting for
input from the keyboard, the computer typically prints a message on the monitor.
Such a message is often referred to as a prompt. The symbols that are displayed
by your operating system (e.g., % or C:) or by your editor (e.g., :) are examples
of prompts.
The program fragment shown in Figure 9.7 obtains keyboard input via
polling as we have shown in Section 9.2.2.2. It also includes a prompt to let the
person sitting at the keyboard know when it is time to type a key. Lets examine
this program fragment.
You are already familiar with lines 13 through 19 and lines 25 through 28,
which correspond to the code in Section 9.2.3.4 for inputting a character via the
keyboard and echoing it on the monitor.
You are also familiar with the need to save and restore registers if those reg-
isters are needed by instructions in the input routine. Lines 01 through 03 save
R1, R2, and R3, lines 1D through 1F restore R1, R2, and R3, and lines 22 through
24 set aside memory locations for those register values.
This leaves lines 05 through 08, 0A through 11, 1A through 1C, 29 and 2A.
These lines serve to alert the person sitting at the keyboard that it is time to type
a character.
Lines 05 through 08 write the ASCII code x0A to the monitor. This is the
ASCII code for a new line. Most ASCII codes correspond to characters that are
visible on the screen. A few, like x0A, are control characters. They cause an action
to occur. Specically, the ASCII code x0A causes the cursor to move to the far
left of the next line on the screen. Thus, the name Newline. Before attempting
to write x0A, however, as is always the case, DSR[15] is tested (line 6) to see
if DDR can accept a character. If DSR[15] is clear, the monitor is busy, and the
loop (lines 06 and 07) is repeated. When DSR[15] is 1, the conditional branch
(line 7) is not taken, and (line 8) x0A is written to DDR for outputting.
Lines 0A through 11 cause the prompt Input a character> to be written
to the screen. The prompt is specified by the .STRINGZ pseudo-op on line 2A
and is stored in 19 memory locations18 ASCII codes, one per memory location,
corresponding to the 18 characters in the prompt, and the terminating sentinel
x0000","{'page_number': 176, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Lines 0A through 11 cause the prompt Input a character> to be written\nto the screen. The prompt is specified by the .STRINGZ pseudo-op on line 2A\nand is stored in 19 memory locations18 ASCII codes, one per memory location,\ncorresponding to the 18 characters in the prompt, and the terminating sentinel\nx0000.\nLine 0C iteratively tests to see if the end of the string has been reached (by\ndetecting x0000), and if not, once DDR is free, line 0F writes the next character\nin the input prompt into DDR. When x0000 is detected, the entire input prompt\nhas been written to the screen, and the program branches to the code that handles\nthe actual keyboard input (starting at line 13).\nAfter the person at the keyboard types a character and it has been echoed\n(lines 13 to 19), the program writes one more new line (lines 1A through 1C)\nbefore branching to its NEXT TASK.'}"
"Lines 0A through 11 cause the prompt Input a character> to be written
to the screen. The prompt is specified by the .STRINGZ pseudo-op on line 2A
and is stored in 19 memory locations18 ASCII codes, one per memory location,
corresponding to the 18 characters in the prompt, and the terminating sentinel
x0000.
Line 0C iteratively tests to see if the end of the string has been reached (by
detecting x0000), and if not, once DDR is free, line 0F writes the next character
in the input prompt into DDR. When x0000 is detected, the entire input prompt
has been written to the screen, and the program branches to the code that handles
the actual keyboard input (starting at line 13).
After the person at the keyboard types a character and it has been echoed
(lines 13 to 19), the program writes one more new line (lines 1A through 1C)
before branching to its NEXT TASK.","{'page_number': 176, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Lines 0A through 11 cause the prompt Input a character> to be written\nto the screen. The prompt is specified by the .STRINGZ pseudo-op on line 2A\nand is stored in 19 memory locations18 ASCII codes, one per memory location,\ncorresponding to the 18 characters in the prompt, and the terminating sentinel\nx0000.\nLine 0C iteratively tests to see if the end of the string has been reached (by\ndetecting x0000), and if not, once DDR is free, line 0F writes the next character\nin the input prompt into DDR. When x0000 is detected, the entire input prompt\nhas been written to the screen, and the program branches to the code that handles\nthe actual keyboard input (starting at line 13).\nAfter the person at the keyboard types a character and it has been echoed\n(lines 13 to 19), the program writes one more new line (lines 1A through 1C)\nbefore branching to its NEXT TASK.'}"
"326
ew
ne .
x
;
co e
or new
ne
2A
Prompt
.STRINGZ ''Input a character>''
Figure 9.7
The more sophisticated input routine.
9.2.5 Implementation of Memory-Mapped I/O, Revisited
We showed in Figures 9.4 and 9.6 partial implementations of the data path to
handle (separately) memory-mapped input and memory-mapped output. We have
also learned that in order to support interrupt-driven I/O, the two status registers
must be writeable as well as readable.","{'page_number': 177, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': ""326\new\nne .\nx\n;\nco e\nor new\nne\n2A\nPrompt\n.STRINGZ ''Input a character>''\nFigure 9.7\nThe more sophisticated input routine.\n9.2.5 Implementation of Memory-Mapped I/O, Revisited\nWe showed in Figures 9.4 and 9.6 partial implementations of the data path to\nhandle (separately) memory-mapped input and memory-mapped output. We have\nalso learned that in order to support interrupt-driven I/O, the two status registers\nmust be writeable as well as readable.""}"
"INMUX
Figure 9.8
Relevant data path implementation of memory-mapped I/O.
Figure 9.8 (also shown as Figure C.3 of Appendix C) shows the data path
necessary to support the full range of features we have discussed for the I/O device
registers. The Address Control Logic Block controls the input or output operation.
Note that there are three inputs to this block. MIO.EN indicates whether a data
movement from/to memory or I/O is to take place this clock cycle. MAR contains
the address of the memory location or the memory-mapped address of an I/O device
register. R.W indicates whether a load or a store is to take place. Depending on the
values of these three inputs, the address control logic does nothing (MIO.EN = 0),
or it provides the control signals to direct the transfer of data between the MDR
and the memory or between the MDR and one of the I/O registers.
If R.W indicates a load, the transfer is from memory or I/O device to the
MDR. The Address Control Logic Block provides the select lines to INMUX to
source the appropriate I/O device register or memory (depending on MAR) and
also enables the memory if MAR contains the address of a memory location.
If R.W indicates a store, the contents of the MDR is written either to memory
or to one of the device registers. The address control logic either enables a write
to memory or asserts the load enable line of the device register specied by the
contents of the MAR.
9.3 Operating System Service
Routines (LC-3 Trap Routines)
9.3.1 Introduction
Recall Figure 9.7 of the previous section. In order for the program to successfully
obtain input from the keyboard, it was necessary for the programmer to know
several things:
1. The hardware data registers for both the monitor and the keyboard: the
monitor so a prompt could be displayed, and the keyboard so the program
would know where to get the input character.","{'page_number': 178, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'INMUX\nFigure 9.8\nRelevant data path implementation of memory-mapped I/O.\nFigure 9.8 (also shown as Figure C.3 of Appendix C) shows the data path\nnecessary to support the full range of features we have discussed for the I/O device\nregisters. The Address Control Logic Block controls the input or output operation.\nNote that there are three inputs to this block. MIO.EN indicates whether a data\nmovement from/to memory or I/O is to take place this clock cycle. MAR contains\nthe address of the memory location or the memory-mapped address of an I/O device\nregister. R.W indicates whether a load or a store is to take place. Depending on the\nvalues of these three inputs, the address control logic does nothing (MIO.EN = 0),\nor it provides the control signals to direct the transfer of data between the MDR\nand the memory or between the MDR and one of the I/O registers.\nIf R.W indicates a load, the transfer is from memory or I/O device to the\nMDR. The Address Control Logic Block provides the select lines to INMUX to\nsource the appropriate I/O device register or memory (depending on MAR) and\nalso enables the memory if MAR contains the address of a memory location.\nIf R.W indicates a store, the contents of the MDR is written either to memory\nor to one of the device registers. The address control logic either enables a write\nto memory or asserts the load enable line of the device register specied by the\ncontents of the MAR.\n9.3 Operating System Service\nRoutines (LC-3 Trap Routines)\n9.3.1 Introduction\nRecall Figure 9.7 of the previous section. In order for the program to successfully\nobtain input from the keyboard, it was necessary for the programmer to know\nseveral things:\n1. The hardware data registers for both the monitor and the keyboard: the\nmonitor so a prompt could be displayed, and the keyboard so the program\nwould know where to get the input character.'}"
"2. The hardware status registers for both the monitor and the keyboard: the
monitor so the program would know when it was OK to display the next
character in the input prompt, and the keyboard so the program would know
when someone had struck a key.
3. The asynchronous nature of keyboard input relative to the executing program.
This is beyond the knowledge of most application programmers. In fact, in the
real world, if application programmers (or user programmers, as they are some-
times called) had to understand I/O at this level, there would be much less I/O
and far fewer programmers in the business.
There is another problem with allowing user programs to perform I/O activity
by directly accessing KBDR and KBSR. I/O activity involves the use of device
registers that are shared by many programs. This means that if a user programmer
were allowed to access the hardware registers, and he/she messed up, it could
create havoc for other user programs. Thus, in general it is ill-advised to give
user programmers access to these registers. That is why the addresses of hardware
registers are part of the privileged memory address space and accessible only to
programs that have supervisor privilege.
The simpler solution, as well as the safer solution to the problem of user
programs requiring I/O, involves the TRAP instruction and the operating system,
which of course has supervisor privilege.
We were rst introduced to the TRAP instruction in Chapter 4 as a way to
get the operating system to halt the computer. In Chapter 5 we saw that a user
program could use the TRAP instruction to get the operating system to do I/O
tasks for it (the user program). In fact a great benet of the TRAP instruction,
which we have already pointed out, is that it allows the user programmer to not
have to know the gory details of I/O discussed earlier in this chapter. In addition,
Figure 9.9
Invoking an OS service routine using the TRAP instruction.","{'page_number': 179, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '2. The hardware status registers for both the monitor and the keyboard: the\nmonitor so the program would know when it was OK to display the next\ncharacter in the input prompt, and the keyboard so the program would know\nwhen someone had struck a key.\n3. The asynchronous nature of keyboard input relative to the executing program.\nThis is beyond the knowledge of most application programmers. In fact, in the\nreal world, if application programmers (or user programmers, as they are some-\ntimes called) had to understand I/O at this level, there would be much less I/O\nand far fewer programmers in the business.\nThere is another problem with allowing user programs to perform I/O activity\nby directly accessing KBDR and KBSR. I/O activity involves the use of device\nregisters that are shared by many programs. This means that if a user programmer\nwere allowed to access the hardware registers, and he/she messed up, it could\ncreate havoc for other user programs. Thus, in general it is ill-advised to give\nuser programmers access to these registers. That is why the addresses of hardware\nregisters are part of the privileged memory address space and accessible only to\nprograms that have supervisor privilege.\nThe simpler solution, as well as the safer solution to the problem of user\nprograms requiring I/O, involves the TRAP instruction and the operating system,\nwhich of course has supervisor privilege.\nWe were rst introduced to the TRAP instruction in Chapter 4 as a way to\nget the operating system to halt the computer. In Chapter 5 we saw that a user\nprogram could use the TRAP instruction to get the operating system to do I/O\ntasks for it (the user program). In fact a great benet of the TRAP instruction,\nwhich we have already pointed out, is that it allows the user programmer to not\nhave to know the gory details of I/O discussed earlier in this chapter. In addition,\nFigure 9.9\nInvoking an OS service routine using the TRAP instruction.'}"
"operating system to perform the task on behalf of the user program. The operating
system takes control of the computer, handles the request specied by the TRAP
instruction, and then returns control back to the user program at location x4001.
As we said at the start of this chapter, we usually refer to the request made by the
user program as a system call or a service call.
9.3.2 The Trap Mechanism
The trap mechanism involves several elements:
1. A set of service routines executed on behalf of user programs by the
operating system. These are part of the operating system and start at
arbitrary addresses in system space. The LC-3 was designed so that up to
256 service routines can be specied. Table A.2 in Appendix A contains the
LC-3s current complete list of operating system service routines.
2. A table of the starting addresses of these 256 service routines. This table
is stored in memory locations x0000 to x00FF. The table is referred to by
various names by various companies. One company calls this table the
System Control Block. Another company calls it the Trap Vector Table.
Figure 9.10 shows the Trap Vector Table of the LC-3, with specic starting
addresses highlighted. Among the starting addresses are the one for the
character output service routine (memory location x0420), which is stored
Figure 9.10
The Trap Vector Table.","{'page_number': 180, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'operating system to perform the task on behalf of the user program. The operating\nsystem takes control of the computer, handles the request specied by the TRAP\ninstruction, and then returns control back to the user program at location x4001.\nAs we said at the start of this chapter, we usually refer to the request made by the\nuser program as a system call or a service call.\n9.3.2 The Trap Mechanism\nThe trap mechanism involves several elements:\n1. A set of service routines executed on behalf of user programs by the\noperating system. These are part of the operating system and start at\narbitrary addresses in system space. The LC-3 was designed so that up to\n256 service routines can be specied. Table A.2 in Appendix A contains the\nLC-3s current complete list of operating system service routines.\n2. A table of the starting addresses of these 256 service routines. This table\nis stored in memory locations x0000 to x00FF. The table is referred to by\nvarious names by various companies. One company calls this table the\nSystem Control Block. Another company calls it the Trap Vector Table.\nFigure 9.10 shows the Trap Vector Table of the LC-3, with specic starting\naddresses highlighted. Among the starting addresses are the one for the\ncharacter output service routine (memory location x0420), which is stored\nFigure 9.10\nThe Trap Vector Table.'}"
"3. The TRAP instruction. When a user program wishes to have the operating
system execute a specic service routine on behalf of the user program, and
then return control to the user program, the user program uses the TRAP
instruction (as we have been doing since Chapter 4).
4. A linkage back to the user program. The service routine must have a
mechanism for returning control to the user program.
9.3.3 The TRAP Instruction
The TRAP instruction causes the service routine to execute by (1) changing the
PC to the starting address of the relevant service routine on the basis of its trap
vector, and (2) providing a way to get back to the program that executed the TRAP
instruction. The way back is referred to as a linkage.
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
1
1
1
1
0
0
0
0
0
0
1
0
0
0
1
1
TRAP
trap vector
The EXECUTE phase of the TRAP instructions instruction cycle does
three things:
1. The PSR and PC are both pushed onto the system stack. Since the PC was
incremented during the FETCH phase of the TRAP instructions instruction
cycle, the return linkage is automatically saved in the PC. When control
returns to the user program, the PC will automatically be pointing to the
instruction following the TRAP instruction.
Note that the program requesting the trap service routine can be running
either in Supervisor mode or in User mode. If in User mode, R6, the stack
pointer, is pointing to the user stack. Before the PSR and PC can be
pushed onto the system stack, the current contents of R6 must be stored
in Saved USP, and the contents of Saved SSP loaded into R6.
2. PSR[15] is set to 0, since the service routine is going to require supervisor
privilege to execute. PSR[10:8] are left unchanged since the priority of the
TRAP routine is the same as the priority of the program that requested it.
3. The 8-bit trap vector is zero-extended to 16 bits to form an address that
corresponds to a location in the Trap Vector Table. For the trap vector x23,
that address is x0023. Memory location x0023 contains x04A0, the starting
address of the TRAP x23 service routine. The PC is loaded with x04A0,
completing the instruction cycle.
Since the PC contains x04A0, processing continues at memory address
x04A0.","{'page_number': 181, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '3. The TRAP instruction. When a user program wishes to have the operating\nsystem execute a specic service routine on behalf of the user program, and\nthen return control to the user program, the user program uses the TRAP\ninstruction (as we have been doing since Chapter 4).\n4. A linkage back to the user program. The service routine must have a\nmechanism for returning control to the user program.\n9.3.3 The TRAP Instruction\nThe TRAP instruction causes the service routine to execute by (1) changing the\nPC to the starting address of the relevant service routine on the basis of its trap\nvector, and (2) providing a way to get back to the program that executed the TRAP\ninstruction. The way back is referred to as a linkage.\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\nTRAP\ntrap vector\nThe EXECUTE phase of the TRAP instructions instruction cycle does\nthree things:\n1. The PSR and PC are both pushed onto the system stack. Since the PC was\nincremented during the FETCH phase of the TRAP instructions instruction\ncycle, the return linkage is automatically saved in the PC. When control\nreturns to the user program, the PC will automatically be pointing to the\ninstruction following the TRAP instruction.\nNote that the program requesting the trap service routine can be running\neither in Supervisor mode or in User mode. If in User mode, R6, the stack\npointer, is pointing to the user stack. Before the PSR and PC can be\npushed onto the system stack, the current contents of R6 must be stored\nin Saved USP, and the contents of Saved SSP loaded into R6.\n2. PSR[15] is set to 0, since the service routine is going to require supervisor\nprivilege to execute. PSR[10:8] are left unchanged since the priority of the\nTRAP routine is the same as the priority of the program that requested it.\n3. The 8-bit trap vector is zero-extended to 16 bits to form an address that\ncorresponds to a location in the Trap Vector Table. For the trap vector x23,\nthat address is x0023. Memory location x0023 contains x04A0, the starting\naddress of the TRAP x23 service routine. The PC is loaded with x04A0,\ncompleting the instruction cycle.\nSince the PC contains x04A0, processing continues at memory address\nx04A0.'}"
"Location x04A0 is the starting address of the operating system service rou-
tine to input a character from the keyboard. We say the trap vector points to
the starting address of the TRAP routine. Thus, TRAP x23 causes the operating
system to start executing the keyboard input service routine.
9.3.4 The RTI Instruction: To Return Control
to the Calling Program
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
RTI
The RTI instruction (opcode = 1000, with no operands) pops the top two
values on the system stack into the PC and PSR. Since the PC contains the address
following the address of the TRAP instruction, control returns to the user program
at the correct address.
Finally, once the PSR has been popped o the system stack, PSR[15] must
be examined to see whether the processor was running in User mode or Super-
visor mode when the TRAP instruction was executed. If in User mode, the stack
pointers need to be adjusted to reect that now back in User mode, the relevant
stack in use is the user stack. This is done by loading the Saved SSP with the
current contents of R6, and loading R6 with the contents of Saved USP.
9.3.5 A Summary of the Trap Service Routine Process
Figure 9.11 shows the LC-3 using the TRAP instruction and the RTI instruction to
implement the example of Figure 9.9. The ow of control goes from (A) within a
user program that needs a character input from the keyboard, to (B) the operating
system service routine that performs that task on behalf of the user program,
back to the user program (C) that presumably uses the information contained in
the input character.
As we know, the computer continually executes its instruction cycle (FETCH,
DECODE, etc.) on sequentially located instructions until the flow of control is
changed by changing the contents of the PC during the EXECUTE phase of the
current instruction. In that way, the next FETCH will be at a redirected address.
The TRAP instruction with trap vector x23 in our user program does exactly
that. Execution of TRAP x23 causes the PSR and incremented PC to be pushed
onto the system stack and the contents of memory location x0023 (which, in this
case, contains x04A0) to be loaded into the PC. The dashed line on Figure 9.11
shows the use of the trap vector x23 to obtain the starting address of the trap
service routine from the Trap Vector Table.
The next instruction cycle starts with the FETCH of the contents of x04A0,
which is the rst instruction of the relevant operating system service routine.
The trap service routine executes to completion, ending with the RTI instruction,","{'page_number': 182, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Location x04A0 is the starting address of the operating system service rou-\ntine to input a character from the keyboard. We say the trap vector points to\nthe starting address of the TRAP routine. Thus, TRAP x23 causes the operating\nsystem to start executing the keyboard input service routine.\n9.3.4 The RTI Instruction: To Return Control\nto the Calling Program\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nRTI\nThe RTI instruction (opcode = 1000, with no operands) pops the top two\nvalues on the system stack into the PC and PSR. Since the PC contains the address\nfollowing the address of the TRAP instruction, control returns to the user program\nat the correct address.\nFinally, once the PSR has been popped o the system stack, PSR[15] must\nbe examined to see whether the processor was running in User mode or Super-\nvisor mode when the TRAP instruction was executed. If in User mode, the stack\npointers need to be adjusted to reect that now back in User mode, the relevant\nstack in use is the user stack. This is done by loading the Saved SSP with the\ncurrent contents of R6, and loading R6 with the contents of Saved USP.\n9.3.5 A Summary of the Trap Service Routine Process\nFigure 9.11 shows the LC-3 using the TRAP instruction and the RTI instruction to\nimplement the example of Figure 9.9. The ow of control goes from (A) within a\nuser program that needs a character input from the keyboard, to (B) the operating\nsystem service routine that performs that task on behalf of the user program,\nback to the user program (C) that presumably uses the information contained in\nthe input character.\nAs we know, the computer continually executes its instruction cycle (FETCH,\nDECODE, etc.) on sequentially located instructions until the flow of control is\nchanged by changing the contents of the PC during the EXECUTE phase of the\ncurrent instruction. In that way, the next FETCH will be at a redirected address.\nThe TRAP instruction with trap vector x23 in our user program does exactly\nthat. Execution of TRAP x23 causes the PSR and incremented PC to be pushed\nonto the system stack and the contents of memory location x0023 (which, in this\ncase, contains x04A0) to be loaded into the PC. The dashed line on Figure 9.11\nshows the use of the trap vector x23 to obtain the starting address of the trap\nservice routine from the Trap Vector Table.\nThe next instruction cycle starts with the FETCH of the contents of x04A0,\nwhich is the rst instruction of the relevant operating system service routine.\nThe trap service routine executes to completion, ending with the RTI instruction,'}"
"332
Figure 9.11
Flow of control from a user program to an OS service routine and back.
which loads the PC and PSR with the top two elements on the system stack, that is,
the PSR and incremented PC that were pushed during execution of the TRAP
instruction. Since the PC was incremented prior to being pushed onto the system
stack, it contains the address of the instruction following the TRAP instruction
in the calling program, and the user program resumes execution by fetching the
instruction following the TRAP instruction.
The following program is provided to illustrate the use of the TRAP instruc-
tion. It can also be used to amuse the average four-year-old!
Example 9.1
Write a game program to do the following: A person is sitting at a keyboard. Each
time the person types a capital letter, the program outputs the lowercase version of
that letter. If the person types a 7, the program terminates.
The following LC-3 assembly language program will do the job.
01
.ORIG x3000
02
LD
R2,TERM
; Load -7
03
LD
R3,ASCII ; Load ASCII difference
04
AGAIN
TRAP
x23
; Request keyboard input
05
ADD
R1,R2,R0 ; Test for terminating
06
BRz
EXIT
; character
07
ADD
R0,R0,R3 ; Change to lowercase
08
TRAP
x21
; Output to the monitor
09
BRnzp AGAIN
; ... and do it again!
0A
TERM
.FILL xFFC9
; FFC9 is negative of ASCII 7
0B
ASCII
.FILL x0020
0C
EXIT
TRAP
x25
; Halt
0D
.END","{'page_number': 183, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '332\nFigure 9.11\nFlow of control from a user program to an OS service routine and back.\nwhich loads the PC and PSR with the top two elements on the system stack, that is,\nthe PSR and incremented PC that were pushed during execution of the TRAP\ninstruction. Since the PC was incremented prior to being pushed onto the system\nstack, it contains the address of the instruction following the TRAP instruction\nin the calling program, and the user program resumes execution by fetching the\ninstruction following the TRAP instruction.\nThe following program is provided to illustrate the use of the TRAP instruc-\ntion. It can also be used to amuse the average four-year-old!\nExample 9.1\nWrite a game program to do the following: A person is sitting at a keyboard. Each\ntime the person types a capital letter, the program outputs the lowercase version of\nthat letter. If the person types a 7, the program terminates.\nThe following LC-3 assembly language program will do the job.\n01\n.ORIG x3000\n02\nLD\nR2,TERM\n; Load -7\n03\nLD\nR3,ASCII ; Load ASCII difference\n04\nAGAIN\nTRAP\nx23\n; Request keyboard input\n05\nADD\nR1,R2,R0 ; Test for terminating\n06\nBRz\nEXIT\n; character\n07\nADD\nR0,R0,R3 ; Change to lowercase\n08\nTRAP\nx21\n; Output to the monitor\n09\nBRnzp AGAIN\n; ... and do it again!\n0A\nTERM\n.FILL xFFC9\n; FFC9 is negative of ASCII 7\n0B\nASCII\n.FILL x0020\n0C\nEXIT\nTRAP\nx25\n; Halt\n0D\n.END'}"
"The program executes as follows: The program rst loads constants xFFC9 and
x0020 into R2 and R3. The constant xFFC9, which is the negative of the ASCII code
for 7, is used to test the character typed at the keyboard to see if the four-year-old wants
to continue playing. The constant x0020 is the zero-extended dierence between the
ASCII code for a capital letter and the ASCII code for that same letters lowercase
representation. For example, the ASCII code for A is x41; the ASCII code for a is
x61. The ASCII codes for Z and z are x5A and x7A, respectively.
Then TRAP x23 is executed, which invokes the keyboard input service routine.
When the service routine is nished, control returns to the application program (at
line 05), and R0 contains the ASCII code of the character typed. The ADD and
BRz instructions test for the terminating character 7. If the character typed is not a
7, the ASCII uppercase/lowercase dierence (x0020) is added to the input ASCII
code, storing the result in R0. Then a TRAP to the monitor output service routine is
called. This causes the lowercase representation of the same letter to be displayed on
the monitor. When control returns to the application program (this time at line 09),
an unconditional BR to AGAIN is executed, and another request for keyboard input
appears.
The correct operation of the program in this example assumes that the per-
son sitting at the keyboard only types capital letters and the value 7. What if the
person types a $? A better solution to Example 9.1 would be a program that tests
the character typed to be sure it really is a capital letter from among the 26 cap-
ital letters in the alphabet or the single digit 7, and if it is not, takes corrective
action.
Question: Augment this program to add the test for bad data. That is, write a
program that will type the lowercase representation of any capital letter typed and
will terminate if anything other than a capital letter is typed. See Exercise 9.20.
9.3.6 Trap Routines for Handling I/O
With the constructs just provided, the input routine described in Figure 9.7 can
be slightly modied to be the input service routine shown in Figure 9.12. Two
changes are needed: (1) We add the appropriate .ORIG and .END pseudo-ops.
.ORIG species the starting address of the input service routinethe address
found at location x0023 in the Trap Vector Table. And (2) we terminate the
input service routine with the RTI instruction rather than the BR NEXT TASK,
as is done on line 20 in Figure 9.7. We use RTI because the service routine is
invoked by TRAP x23. It is not part of the user program, as was the case in
Figure 9.7.
The output routine of Section 9.2.3.2 can be modied in a similar way, as
shown in Figure 9.13. The results are input (Figure 9.12) and output (Figure 9.13)
service routines that can be invoked simply and safely by the TRAP instruction
with the appropriate trap vector","{'page_number': 184, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '7. We use RTI because the service routine is\ninvoked by TRAP x23. It is not part of the user program, as was the case in\nFigure 9.7.\nThe output routine of Section 9.2.3.2 can be modied in a similar way, as\nshown in Figure 9.13. The results are input (Figure 9.12) and output (Figure 9.13)\nservice routines that can be invoked simply and safely by the TRAP instruction\nwith the appropriate trap vector. In the case of input, upon completion of TRAP\nx23, R0 contains the ASCII code of the keyboard character typed. In the case of\noutput, the initiating program must load R0 with the ASCII code of the character\nit wishes displayed on the monitor and then invoke TRAP x21.'}"
"7. We use RTI because the service routine is
invoked by TRAP x23. It is not part of the user program, as was the case in
Figure 9.7.
The output routine of Section 9.2.3.2 can be modied in a similar way, as
shown in Figure 9.13. The results are input (Figure 9.12) and output (Figure 9.13)
service routines that can be invoked simply and safely by the TRAP instruction
with the appropriate trap vector. In the case of input, upon completion of TRAP
x23, R0 contains the ASCII code of the keyboard character typed. In the case of
output, the initiating program must load R0 with the ASCII code of the character
it wishes displayed on the monitor and then invoke TRAP x21.","{'page_number': 184, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '7. We use RTI because the service routine is\ninvoked by TRAP x23. It is not part of the user program, as was the case in\nFigure 9.7.\nThe output routine of Section 9.2.3.2 can be modied in a similar way, as\nshown in Figure 9.13. The results are input (Figure 9.12) and output (Figure 9.13)\nservice routines that can be invoked simply and safely by the TRAP instruction\nwith the appropriate trap vector. In the case of input, upon completion of TRAP\nx23, R0 contains the ASCII code of the keyboard character typed. In the case of\noutput, the initiating program must load R0 with the ASCII code of the character\nit wishes displayed on the monitor and then invoke TRAP x21.'}"
"ave
.
0F
.END
Figure 9.13
Character output service routine.
9.3.7 A Trap Routine for Halting the Computer
Recall from Section 4.5 that the RUN latch is ANDed with the crystal oscillator
to produce the clock that controls the operation of the computer. We noted that if
that one-bit latch was cleared, the output of the AND gate would be 0, stopping
the clock.
Years ago, most ISAs had a HALT instruction for stopping the clock. Given
how infrequently that instruction is executed, it seems wasteful to devote an
o code to it. In man modern com uters, the RUN latch is cleared b a TRAP
,
Figure 9.14
HALT service routine for the LC-3 (Fig. 9.14 continued on next page.)","{'page_number': 185, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'ave\n.\n0F\n.END\nFigure 9.13\nCharacter output service routine.\n9.3.7 A Trap Routine for Halting the Computer\nRecall from Section 4.5 that the RUN latch is ANDed with the crystal oscillator\nto produce the clock that controls the operation of the computer. We noted that if\nthat one-bit latch was cleared, the output of the AND gate would be 0, stopping\nthe clock.\nYears ago, most ISAs had a HALT instruction for stopping the clock. Given\nhow infrequently that instruction is executed, it seems wasteful to devote an\no code to it. In man modern com uters, the RUN latch is cleared b a TRAP\n,\nFigure 9.14\nHALT service routine for the LC-3 (Fig. 9.14 continued on next page.)'}"
"336
.
Figure 9.14
HALT service routine for the LC-3 (continued Fig. 9.14 from
previous page.)
rst (lines 02 and 03), registers R1 and R0 are saved. R1 and R0 are saved
se they are needed by the service routine. Then (lines 07 through 0C),
nner Halting the machine is displayed on the monitor. Finally (lines 10
h 13), the RUN latch (MCR[15]) is cleared by ANDing the MCR with
0111111111111111. That is, MCR[14:0] remains unchanged, but MCR[15] is
cleared. Question: What instruction (or trap service routine) can be used to start
the clock? Hint: This is a trick question! :-)
9.3.8 The Trap Routine for Character Input (One Last Time)
Lets look again at the keyboard input service routine of Figure 9.12. In particular,
lets look at the three-line sequence that occurs at symbolic addresses L1, L2, L3,
and L4:
LABEL
LDI
R3,DSR
BRzp
LABEL
STI
Reg,DDR
Can the JSR/RET mechanism enable us to replace these four occurrences of the
same sequence with a single subroutine? Answer: Yes, almost.
Figure 9.15, our improved keyboard input service routine, contains
JSR
WriteChar
at lines 04, 0A, 10, and 13, and the four-instruction subroutine
WriteChar
LDI
R3,DSR
BRzp
WriteChar
STI
R2,DDR
RET
at lines 1A through 1D. Note the RET instruction (a.k.a. JMP R7) that is needed
to terminate the subroutine.","{'page_number': 186, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '336\n.\nFigure 9.14\nHALT service routine for the LC-3 (continued Fig. 9.14 from\nprevious page.)\nrst (lines 02 and 03), registers R1 and R0 are saved. R1 and R0 are saved\nse they are needed by the service routine. Then (lines 07 through 0C),\nnner Halting the machine is displayed on the monitor. Finally (lines 10\nh 13), the RUN latch (MCR[15]) is cleared by ANDing the MCR with\n0111111111111111. That is, MCR[14:0] remains unchanged, but MCR[15] is\ncleared. Question: What instruction (or trap service routine) can be used to start\nthe clock? Hint: This is a trick question! :-)\n9.3.8 The Trap Routine for Character Input (One Last Time)\nLets look again at the keyboard input service routine of Figure 9.12. In particular,\nlets look at the three-line sequence that occurs at symbolic addresses L1, L2, L3,\nand L4:\nLABEL\nLDI\nR3,DSR\nBRzp\nLABEL\nSTI\nReg,DDR\nCan the JSR/RET mechanism enable us to replace these four occurrences of the\nsame sequence with a single subroutine? Answer: Yes, almost.\nFigure 9.15, our improved keyboard input service routine, contains\nJSR\nWriteChar\nat lines 04, 0A, 10, and 13, and the four-instruction subroutine\nWriteChar\nLDI\nR3,DSR\nBRzp\nWriteChar\nSTI\nR2,DDR\nRET\nat lines 1A through 1D. Note the RET instruction (a.k.a. JMP R7) that is needed\nto terminate the subroutine.'}"
"Note the hedging: almost. In the original sequences starting at L2 and L3,
the STI instruction forwards the contents of R0 (not R2) to the DDR. We can x
that easily enough, as follows: In line 08 of Figure 9.15, we use
LDR
R2,R1,#0
instead of
LDR
R0,R1,#0
This causes each character in the prompt to be loaded into R2. The subroutine
Writechar forwards each character from R2 to the DDR.
In line 0F of Figure 9.15, we insert the instruction
ADD
R2,R0,#0
in order to move the keyboard input (which is in R0) into R2. The subroutine
Writechar forwards it from R2 to the DDR. Note that R0 still contains the key-
board input. Furthermore, since no subsequent instruction in the service routine
loads R0, R0 still contains the keyboard input after control returns to the user
program.
In line 12 of Figure 9.15, we insert the instruction
LD
R2,Newline
in order to move the newline character into R2. The subroutine Writechar
forwards it from R2 to the DDR.
Figure 9.15 is the actual LC-3 trap service routine provided for keyboard
input.
9.3.9 PUTS: Writing a Character String to the Monitor
Before we leave the example of Figure 9.15, note the code on lines 08 through 0C.
This fragment of the service routine is used to write the sequence of characters
Input a character to the monitor. A sequence of characters is often referred to
as a string of characters or a character string. This fragment is also present in
Figure 9.14, with the result that Halting the machine is written to the monitor. In
fact, it is so often the case that a user program needs to write a string of characters
to the monitor that this function is given its own trap service routine in the LC-3
operating system. Thus, if a user program requires a character string to be written
to the monitor, it need only provide (in R0) the starting address of the character
string, and then invoke TRAP x22. In LC-3 assembly language this TRAP is
called PUTS.
PUTS (or TRAP x22) causes control to be passed to the operating system,
and the trap routine shown in Figure 9.16 is executed. Note that PUTS is the code
of lines 08 through 0C of Figure 9.15, with a few minor adjustments.","{'page_number': 187, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Note the hedging: almost. In the original sequences starting at L2 and L3,\nthe STI instruction forwards the contents of R0 (not R2) to the DDR. We can x\nthat easily enough, as follows: In line 08 of Figure 9.15, we use\nLDR\nR2,R1,#0\ninstead of\nLDR\nR0,R1,#0\nThis causes each character in the prompt to be loaded into R2. The subroutine\nWritechar forwards each character from R2 to the DDR.\nIn line 0F of Figure 9.15, we insert the instruction\nADD\nR2,R0,#0\nin order to move the keyboard input (which is in R0) into R2. The subroutine\nWritechar forwards it from R2 to the DDR. Note that R0 still contains the key-\nboard input. Furthermore, since no subsequent instruction in the service routine\nloads R0, R0 still contains the keyboard input after control returns to the user\nprogram.\nIn line 12 of Figure 9.15, we insert the instruction\nLD\nR2,Newline\nin order to move the newline character into R2. The subroutine Writechar\nforwards it from R2 to the DDR.\nFigure 9.15 is the actual LC-3 trap service routine provided for keyboard\ninput.\n9.3.9 PUTS: Writing a Character String to the Monitor\nBefore we leave the example of Figure 9.15, note the code on lines 08 through 0C.\nThis fragment of the service routine is used to write the sequence of characters\nInput a character to the monitor. A sequence of characters is often referred to\nas a string of characters or a character string. This fragment is also present in\nFigure 9.14, with the result that Halting the machine is written to the monitor. In\nfact, it is so often the case that a user program needs to write a string of characters\nto the monitor that this function is given its own trap service routine in the LC-3\noperating system. Thus, if a user program requires a character string to be written\nto the monitor, it need only provide (in R0) the starting address of the character\nstring, and then invoke TRAP x22. In LC-3 assembly language this TRAP is\ncalled PUTS.\nPUTS (or TRAP x22) causes control to be passed to the operating system,\nand the trap routine shown in Figure 9.16 is executed. Note that PUTS is the code\nof lines 08 through 0C of Figure 9.15, with a few minor adjustments.'}"
"1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1F
SaveR3
.FILL
x0000
20
.END
Figure 9.16
The LC-3 PUTS service routine.
9.4 Interrupts and Interrupt-
Driven I/O
In Section 9.2.1.3, we noted that interaction between the processor and an I/O
device can be controlled by the processor (i.e., polling) or it can be controlled by
the I/O device (i.e., interrupt driven). In Sections 9.2.2, 9.2.3, and 9.2.4, we have
studied several examples of polling. In each case, the processor tested the ready
bit of the status register again and again, and when the ready bit was nally 1, the
processor branched to the instruction that did the input or output operation.
We are now ready to study the case where the interaction is controlled by the
I/O device.
9.4.1 What Is Interrupt-Driven I/O?
The essence of interrupt-driven I/O is the notion that an I/O device that may or
may not have anything to do with the program that is running can (1) force the","{'page_number': 188, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1F\nSaveR3\n.FILL\nx0000\n20\n.END\nFigure 9.16\nThe LC-3 PUTS service routine.\n9.4 Interrupts and Interrupt-\nDriven I/O\nIn Section 9.2.1.3, we noted that interaction between the processor and an I/O\ndevice can be controlled by the processor (i.e., polling) or it can be controlled by\nthe I/O device (i.e., interrupt driven). In Sections 9.2.2, 9.2.3, and 9.2.4, we have\nstudied several examples of polling. In each case, the processor tested the ready\nbit of the status register again and again, and when the ready bit was nally 1, the\nprocessor branched to the instruction that did the input or output operation.\nWe are now ready to study the case where the interaction is controlled by the\nI/O device.\n9.4.1 What Is Interrupt-Driven I/O?\nThe essence of interrupt-driven I/O is the notion that an I/O device that may or\nmay not have anything to do with the program that is running can (1) force the'}"
"340
.
.
Figure 9.17
Instruction execution ow for interrupt-driven I/O.
running program to stop, (2) have the processor execute a program that carries out
the needs of the I/O device, and then (3) have the stopped program resume exe-
cution as if nothing had happened. These three stages of the instruction execution
ow are shown in Figure 9.17.
As far as Program A is concerned, the work carried out and the results
computed are no dierent from what would have been the case if the interrupt
had never happened; that is, as if the instruction execution ow had been the
following:
.
.
.
Program A is executing instruction n
Program A is executing instruction n+1
Program A is executing instruction n+2
Program A is executing instruction n+3
Program A is executing instruction n+4
.
.
.
9.4.2 Why Have Interrupt-Driven I/O?
As is undoubtedly clear, polling requires the processor to waste a lot of time spin-
ning its wheels, re-executing again and again the LDI and BR instructions until
the ready bit is set. With interrupt-driven I/O, none of that testing and branching
has to go on. Interrupt-driven I/O allows the processor to spend its time doing","{'page_number': 189, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '340\n.\n.\nFigure 9.17\nInstruction execution ow for interrupt-driven I/O.\nrunning program to stop, (2) have the processor execute a program that carries out\nthe needs of the I/O device, and then (3) have the stopped program resume exe-\ncution as if nothing had happened. These three stages of the instruction execution\now are shown in Figure 9.17.\nAs far as Program A is concerned, the work carried out and the results\ncomputed are no dierent from what would have been the case if the interrupt\nhad never happened; that is, as if the instruction execution ow had been the\nfollowing:\n.\n.\n.\nProgram A is executing instruction n\nProgram A is executing instruction n+1\nProgram A is executing instruction n+2\nProgram A is executing instruction n+3\nProgram A is executing instruction n+4\n.\n.\n.\n9.4.2 Why Have Interrupt-Driven I/O?\nAs is undoubtedly clear, polling requires the processor to waste a lot of time spin-\nning its wheels, re-executing again and again the LDI and BR instructions until\nthe ready bit is set. With interrupt-driven I/O, none of that testing and branching\nhas to go on. Interrupt-driven I/O allows the processor to spend its time doing'}"
"what is hopefully useful work, executing some other program perhaps, until it is
notied that some I/O device needs attention.
Example 9.2
Suppose we are asked to write a program that takes a sequence of 100 characters
typed on a keyboard and processes the information contained in those 100 characters.
Assume the characters are typed at the rate of 80 words/minute, which corresponds
to one character every 0.125 seconds. Assume the processing of the 100-character
sequence takes 12.49999 seconds, and that our program is to perform this process on
1000 consecutive sequences. How long will it take our program to complete the task?
(Why did we pick 12.49999? To make the numbers come out nice, of course.) :-)
We could obtain each character input by polling, as in Section 9.2.2. If we did,
we would waste a lot of time waiting for the next character to be typed. It would
take 100  0.125 or 12.5 seconds to get a 100-character sequence.
On the other hand, if we use interrupt-driven I/O, the processor does not waste
any time re-executing the LDI and BR instructions while waiting for a character to
be typed. Rather, the processor can be busy working on the previous 100-character
sequence that was typed, except for those very small fractions of time when it is inter-
rupted by the I/O device to read the next character typed. Lets say that to read the next
character typed requires executing a ten-instruction program that takes on the aver-
age 0.00000001 seconds to execute each instruction. That means 0.0000001 seconds
for each character typed, or 0.00001 seconds for the entire 100-character sequence.
That is, with interrupt-driven I/O, since the processor is only needed when characters
are actually being read, the time required for each 100-character sequence is 0.00001
seconds, instead of 12.50000 seconds. The remaining 12.49999 of every 12.50000
seconds, the processor is available to do useful work. For example, it can process the
previous 100-character sequence.
The bottom line: With polling, the time to complete the entire task for each
sequence is 24.9999 seconds, 12.5 seconds to obtain the 100 characters + 12.49999
seconds to process them. With interrupt-driven I/O, the time to complete the entire
task for each sequence after the rst is 12.5 seconds, 0.00001 seconds to obtain
the characters + 12.49999 seconds to process them. For 1000 sequences, that is the
dierence between 7 hours and 3 1
2 hours.
9.4.3 Two Parts to the Process
There are two parts to interrupt-driven I/O:
1. the mechanism that enables an I/O device to interrupt the processor, and
2. the mechanism that handles the interrupt request.
9.4.4 Part I: Causing the Interrupt to Occur
Several things must be true for an I/O device to actually interrupt the program
that is running:
1. The I/O device must want service.
2. The device must have the right to request the service.
3. The device request must be more urgent than what the processor is
currently doing.","{'page_number': 190, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'what is hopefully useful work, executing some other program perhaps, until it is\nnotied that some I/O device needs attention.\nExample 9.2\nSuppose we are asked to write a program that takes a sequence of 100 characters\ntyped on a keyboard and processes the information contained in those 100 characters.\nAssume the characters are typed at the rate of 80 words/minute, which corresponds\nto one character every 0.125 seconds. Assume the processing of the 100-character\nsequence takes 12.49999 seconds, and that our program is to perform this process on\n1000 consecutive sequences. How long will it take our program to complete the task?\n(Why did we pick 12.49999? To make the numbers come out nice, of course.) :-)\nWe could obtain each character input by polling, as in Section 9.2.2. If we did,\nwe would waste a lot of time waiting for the next character to be typed. It would\ntake 100  0.125 or 12.5 seconds to get a 100-character sequence.\nOn the other hand, if we use interrupt-driven I/O, the processor does not waste\nany time re-executing the LDI and BR instructions while waiting for a character to\nbe typed. Rather, the processor can be busy working on the previous 100-character\nsequence that was typed, except for those very small fractions of time when it is inter-\nrupted by the I/O device to read the next character typed. Lets say that to read the next\ncharacter typed requires executing a ten-instruction program that takes on the aver-\nage 0.00000001 seconds to execute each instruction. That means 0.0000001 seconds\nfor each character typed, or 0.00001 seconds for the entire 100-character sequence.\nThat is, with interrupt-driven I/O, since the processor is only needed when characters\nare actually being read, the time required for each 100-character sequence is 0.00001\nseconds, instead of 12.50000 seconds. The remaining 12.49999 of every 12.50000\nseconds, the processor is available to do useful work. For example, it can process the\nprevious 100-character sequence.\nThe bottom line: With polling, the time to complete the entire task for each\nsequence is 24.9999 seconds, 12.5 seconds to obtain the 100 characters + 12.49999\nseconds to process them. With interrupt-driven I/O, the time to complete the entire\ntask for each sequence after the rst is 12.5 seconds, 0.00001 seconds to obtain\nthe characters + 12.49999 seconds to process them. For 1000 sequences, that is the\ndierence between 7 hours and 3 1\n2 hours.\n9.4.3 Two Parts to the Process\nThere are two parts to interrupt-driven I/O:\n1. the mechanism that enables an I/O device to interrupt the processor, and\n2. the mechanism that handles the interrupt request.\n9.4.4 Part I: Causing the Interrupt to Occur\nSeveral things must be true for an I/O device to actually interrupt the program\nthat is running:\n1. The I/O device must want service.\n2. The device must have the right to request the service.\n3. The device request must be more urgent than what the processor is\ncurrently doing.'}"
"If all three elements are present, the processor stops executing the program
that is running and takes care of the interrupt.
9.4.4.1 The Interrupt Signal from the Device
For an I/O device to generate an interrupt request, the device must want service,
and it must have the right to request that service.
The Device Must Want Service
We have discussed that already in the study of
polling. It is the ready bit of the KBSR or the DSR. That is, if the I/O device is
the keyboard, it wants service if someone has typed a character. If the I/O device
is the monitor, it wants service (i.e., the next character to output) if the associated
electronic circuits have successfully completed the display of the last character.
In both cases, the I/O device wants service when the corresponding ready bit is
set.
The Device Must Have the Right to Request That Service
This is the interrupt
enable bit, which can be set or cleared by the processor (usually by the operating
system), depending on whether or not the processor wants to give the I/O device
the ri ht to re uest service. In most I/O devices, this interru t enable IE bit is
Interrupt signal to the processor
Figure 9.18
Interrupt enable bits and their use.
If the interrupt enable bit (bit [14]) is clear, it does not matter whether the
ready bit is set; the I/O device will not be able to interrupt the processor because
it (the I/O device) has not been given the right to interrupt the processor. In that
case, the program will have to poll the I/O device to determine if it is ready.
If bit [14] is set, then interrupt-driven I/O is enabled. In that case, as soon as
someone types a key (or as soon as the monitor has nished processing the last
character), bit [15] is set. In this case, the device wants service, and it has been
given the right to request service. The AND gate is asserted, causing an interrupt
request to be generated from the I/O device.
9.4.4.2 The Urgency of the Request
The third element in the list of things that must be true for an I/O device to actually
interrupt the processor is that the request must be more urgent than the program","{'page_number': 191, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'If all three elements are present, the processor stops executing the program\nthat is running and takes care of the interrupt.\n9.4.4.1 The Interrupt Signal from the Device\nFor an I/O device to generate an interrupt request, the device must want service,\nand it must have the right to request that service.\nThe Device Must Want Service\nWe have discussed that already in the study of\npolling. It is the ready bit of the KBSR or the DSR. That is, if the I/O device is\nthe keyboard, it wants service if someone has typed a character. If the I/O device\nis the monitor, it wants service (i.e., the next character to output) if the associated\nelectronic circuits have successfully completed the display of the last character.\nIn both cases, the I/O device wants service when the corresponding ready bit is\nset.\nThe Device Must Have the Right to Request That Service\nThis is the interrupt\nenable bit, which can be set or cleared by the processor (usually by the operating\nsystem), depending on whether or not the processor wants to give the I/O device\nthe ri ht to re uest service. In most I/O devices, this interru t enable IE bit is\nInterrupt signal to the processor\nFigure 9.18\nInterrupt enable bits and their use.\nIf the interrupt enable bit (bit [14]) is clear, it does not matter whether the\nready bit is set; the I/O device will not be able to interrupt the processor because\nit (the I/O device) has not been given the right to interrupt the processor. In that\ncase, the program will have to poll the I/O device to determine if it is ready.\nIf bit [14] is set, then interrupt-driven I/O is enabled. In that case, as soon as\nsomeone types a key (or as soon as the monitor has nished processing the last\ncharacter), bit [15] is set. In this case, the device wants service, and it has been\ngiven the right to request service. The AND gate is asserted, causing an interrupt\nrequest to be generated from the I/O device.\n9.4.4.2 The Urgency of the Request\nThe third element in the list of things that must be true for an I/O device to actually\ninterrupt the processor is that the request must be more urgent than the program'}"
"that is currently executing. Recall from Section 9.1.1.2 that each program runs
at a specied level of urgency called its priority level. To interrupt the running
program, the device must have a higher priority than the program that is currently
running. Actually, there may be many devices that want to interrupt the processor
at a specic time. To succeed, the device must have a higher priority level than
all other demands for use of the processor.
Almost all computers have a set of priority levels that programs can run at.
As we have already noted, the LC-3 has eight priority levels, PL0 to PL7. The
higher the number, the more urgent the program. The PL of a program is usually
the same as the PL (i.e., urgency) of the request to run that program. If a program
is running at one PL, and a higher-level PL request wants the computer, the lower-
priority program suspends processing until the higher-PL program executes and
satises its more urgent request. For example, a computers payroll program may
run overnight, and at PL0. It has all night to nishnot terribly urgent. A program
that corrects for a nuclear plant current surge may run at PL6. We are perfectly
happy to let the payroll wait while the nuclear power correction keeps us from
being blown to bits.
For our I/O device to successfully stop the processor and start an interrupt-
driven I/O request, the priority of the request must be higher than the priority
of the program it wishes to interrupt. For example, we would not normally want
to allow a keyboard interrupt from a professor checking e-mail to interrupt the
nuclear power correction program.
9.4.4.3 The INT Signal
To stop the processor from continuing execution of its currently running program
and service an interrupt request, the INT signal must be asserted. Figure 9.19 shows
what is required to assert the INT signal. Figure 9.19 shows the status registers of
several devices operating at various priority levels (PL). Any device that has bits [14]
and [15] both set asserts its interrupt request signal. The interrupt request signals are
input to a priority encoder, a combinational logic structure that selects the highest
priority request from all those asserted. If the PL of that request is higher than the PL
of the currently executing program, the INT signal is asserted.
9.4.4.4 The Test for INT
Finally, the test to enable the processor to stop and handle the interrupt. Recall
from Chapter 4 that the instruction cycle continually sequences through the
phases of the instruction cycle (FETCH, DECODE, EVALUATE ADDRESS,
FETCH OPERAND, EXECUTE, and STORE RESULT). Each instruction
changes the state of the computer, and that change is completed at the end of
the instruction cycle for that instruction. That is, in the last clock cycle before the
computer returns to the FETCH phase for the next instruction, the computer is
put in the state caused by the complete execution of the current instruction.
Interrupts can happen at any time. They are asynchronous to the synchronous
nite state machine controlling the computer. For example, the interrupt signal
could occur when the instruction cycle is in its FETCH OPERAND phase","{'page_number': 192, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'That is, in the last clock cycle before the\ncomputer returns to the FETCH phase for the next instruction, the computer is\nput in the state caused by the complete execution of the current instruction.\nInterrupts can happen at any time. They are asynchronous to the synchronous\nnite state machine controlling the computer. For example, the interrupt signal\ncould occur when the instruction cycle is in its FETCH OPERAND phase. If\nwe stopped the currently executing program when the instruction cycle was in'}"
"That is, in the last clock cycle before the
computer returns to the FETCH phase for the next instruction, the computer is
put in the state caused by the complete execution of the current instruction.
Interrupts can happen at any time. They are asynchronous to the synchronous
nite state machine controlling the computer. For example, the interrupt signal
could occur when the instruction cycle is in its FETCH OPERAND phase. If
we stopped the currently executing program when the instruction cycle was in","{'page_number': 192, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'That is, in the last clock cycle before the\ncomputer returns to the FETCH phase for the next instruction, the computer is\nput in the state caused by the complete execution of the current instruction.\nInterrupts can happen at any time. They are asynchronous to the synchronous\nnite state machine controlling the computer. For example, the interrupt signal\ncould occur when the instruction cycle is in its FETCH OPERAND phase. If\nwe stopped the currently executing program when the instruction cycle was in'}"
"344
INT
Figure 9.19
Generation of the INT signal.
its FETCH OPERAND phase, we would have to keep track of what part of the
current instruction has executed and what part of the current instruction still has
work to do. It makes much more sense to ignore interrupt signals except when
we are at an instruction boundary; that is, the current instruction has completed,
and the next instruction has not yet started. Doing that means we do not have to
worry about partially executed instructions, since the state of the computer is the
state created by the completion of the current instruction, period!
The additional logic to test for the interrupt signal is to augment the last
state of the instruction cycle for each instruction with a test. Instead of always
going from the last state of one instruction cycle to the rst state of the FETCH
phase of the next instruction, the next state depends on the INT signal. If INT
is not asserted, then it is business as usual, with the control unit returning to the
FETCH phase to start processing the next instruction. If INT is asserted, then the
next state is the rst state of Part II, handling the interrupt request.
9.4.5 Part II: Handling the Interrupt Request
Handling the interrupt request goes through three stages, as shown in Figure 9.17:
1. Initiate the interrupt (three lines numbered 1 in Figure 9.17).
2. Service the interrupt (four lines numbered 2 in Figure 9.17).
3. Return from the interrupt (one line numbered 3 in Figure 9.17).
We will discuss each.","{'page_number': 193, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '344\nINT\nFigure 9.19\nGeneration of the INT signal.\nits FETCH OPERAND phase, we would have to keep track of what part of the\ncurrent instruction has executed and what part of the current instruction still has\nwork to do. It makes much more sense to ignore interrupt signals except when\nwe are at an instruction boundary; that is, the current instruction has completed,\nand the next instruction has not yet started. Doing that means we do not have to\nworry about partially executed instructions, since the state of the computer is the\nstate created by the completion of the current instruction, period!\nThe additional logic to test for the interrupt signal is to augment the last\nstate of the instruction cycle for each instruction with a test. Instead of always\ngoing from the last state of one instruction cycle to the rst state of the FETCH\nphase of the next instruction, the next state depends on the INT signal. If INT\nis not asserted, then it is business as usual, with the control unit returning to the\nFETCH phase to start processing the next instruction. If INT is asserted, then the\nnext state is the rst state of Part II, handling the interrupt request.\n9.4.5 Part II: Handling the Interrupt Request\nHandling the interrupt request goes through three stages, as shown in Figure 9.17:\n1. Initiate the interrupt (three lines numbered 1 in Figure 9.17).\n2. Service the interrupt (four lines numbered 2 in Figure 9.17).\n3. Return from the interrupt (one line numbered 3 in Figure 9.17).\nWe will discuss each.'}"
"9.4.5.1 Initiate the Interrupt
Since the INT signal was asserted, the processor does not return to the rst state
of the FETCH phase of the next instruction cycle, but rather begins a sequence of
actions to initiate the interrupt. The processor must do two things: (1) save the
state of the interrupted program so it can pick up where it left o after the require-
ments of the interrupt have been completed, and (2) load the state of the higher
priority interrupting program so it can start satisfying its request.
Save the State of the Interrupted Program
The state of a program is a snap-
shot of the contents of all the programs resources. It includes the contents of the
memory locations that are part of the program and the contents of all the general
purpose registers. It also includes the PC and PSR.
Recall from Figure 9.1 in Section 9.1.1.4 that a programs PSR species
the privilege level and priority level of that program. PSR[15] indicates whether
the program is running in privileged (Supervisor) or unprivileged (User) mode.
PSR[10:8] species the programs priority level (PL), from PL0 (lowest) to PL7
(highest). Also, PSR[2:0] is used to store the condition codes. PSR[2] is the N
bit, PSR[1] is the Z bit, and PSR[0] is the P bit.
The rst step in initiating the interrupt is to save enough of the state of the
program that is running so that it can continue where it left o after the I/O device
request has been satised. That means, in the case of the LC-3, saving the PC and
the PSR. The PC must be saved since it knows which instruction should be exe-
cuted next when the interrupted program resumes execution. The condition codes
(the N, Z, and P ags) must be saved since they may be needed by a subsequent
conditional branch instruction after the program resumes execution. The priority
level of the interrupted program must be saved because it species the urgency of
the interrupted program with respect to all other programs. When the interrupted
program resumes execution, it is important to know what priority level programs
can interrupt it and which ones cannot. Finally, the privilege level of the program
must be saved since it species what processor resources the interrupted program
can and cannot access.
Although many computers save the contents of the general purpose registers,
we will not since we will assume that the service routine will always save the
contents of any general purpose register that it needs before using it, and then
restore it before returning to the interrupted program. The only state information
the LC-3 saves are the PC and PSR.
The LC-3 saves this state information on the supervisor stack in the same
way the PC and PSR are saved when a TRAP instruction is executed. That is,
before the interrupt service routine starts, if the interrupted program is in User
mode, the User Stack Pointer (USP) is stored in Saved USP, and R6 is loaded with
the Supervisor Stack Pointer (SSP) from Saved SSP","{'page_number': 194, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The LC-3 saves this state information on the supervisor stack in the same\nway the PC and PSR are saved when a TRAP instruction is executed. That is,\nbefore the interrupt service routine starts, if the interrupted program is in User\nmode, the User Stack Pointer (USP) is stored in Saved USP, and R6 is loaded with\nthe Supervisor Stack Pointer (SSP) from Saved SSP. Then the PSR and PC of\nthe interrupted program are pushed onto the supervisor stack, where they remain\nunmolested while the service routine executes.\nLoad the State of the Interrupt Service Routine\nOnce the state of the inter-\nrupted program has been safely saved on the supervisor stack, the second step'}"
"The LC-3 saves this state information on the supervisor stack in the same
way the PC and PSR are saved when a TRAP instruction is executed. That is,
before the interrupt service routine starts, if the interrupted program is in User
mode, the User Stack Pointer (USP) is stored in Saved USP, and R6 is loaded with
the Supervisor Stack Pointer (SSP) from Saved SSP. Then the PSR and PC of
the interrupted program are pushed onto the supervisor stack, where they remain
unmolested while the service routine executes.
Load the State of the Interrupt Service Routine
Once the state of the inter-
rupted program has been safely saved on the supervisor stack, the second step","{'page_number': 194, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The LC-3 saves this state information on the supervisor stack in the same\nway the PC and PSR are saved when a TRAP instruction is executed. That is,\nbefore the interrupt service routine starts, if the interrupted program is in User\nmode, the User Stack Pointer (USP) is stored in Saved USP, and R6 is loaded with\nthe Supervisor Stack Pointer (SSP) from Saved SSP. Then the PSR and PC of\nthe interrupted program are pushed onto the supervisor stack, where they remain\nunmolested while the service routine executes.\nLoad the State of the Interrupt Service Routine\nOnce the state of the inter-\nrupted program has been safely saved on the supervisor stack, the second step'}"
"is to load the PC and PSR of the interrupt service routine. Interrupt service rou-
tines are similar to the trap service routines we have already discussed. They are
program fragments stored in system space. They service interrupt requests.
Most processors use the mechanism of vectored interrupts. You are famil-
iar with this notion from your study of the trap vector contained in the TRAP
instruction. In the case of interrupts, the eight-bit vector is provided by the device
that is requesting the processor be interrupted. That is, the I/O device trans-
mits to the processor an eight-bit interrupt vector along with its interrupt request
signal and its priority level. The interrupt vector corresponding to the highest
priority interrupt request is the one supplied to the processor. It is designated
INTV.
If the interrupt is taken, the processor expands the 8-bit interrupt vector
(INTV) to form a 16-bit address, which is an entry into the Interrupt Vector
Table. You know that the Trap Vector Table consists of memory locations x0000
to x00FF, each containing the starting address of a trap service routine. The Inter-
rupt Vector Table consists of memory locations x0100 to x01FF, each containing
the starting address of an interrupt service routine. The processor loads the PC
with the contents of the location in the Interrupt Vector Table corresponding to
the address formed by expanding the interrupt vector INTV.
For example, the LC-3 keyboard could interrupt the processor every time a
key is pressed by someone sitting at the keyboard. The keyboard interrupt vector
would indicate the location in the interrupt vector table that contains the starting
address of the keyboard interrupt service routine.
The PSR is loaded as follows: Since no instructions in the service routine
have yet executed, PSR[2:0] contains no meaningful information. We arbitrarily
load it initially with 010. Since the interrupt service routine runs in privileged
mode, PSR[15] is set to 0. PSR[10:8] is set to the priority level associated with
the interrupt request.
This completes the initiation phase, and the interrupt service routine is ready
to execute.
9.4.5.2 Service the Interrupt
Since the PC contains the starting address of the interrupt service routine, the
service routine will execute, and the requirements of the I/O device will be
serviced.
9.4.5.3 Return from the Interrupt
The last instruction in every interrupt service routine is RTI, return from trap
or interrupt. When the processor nally accesses the RTI instruction, all the
requirements of the I/O device have been taken care of.
Like the return from a trap routine discussed in Section 9.3.4, execution of
the RTI instruction (opcode = 1000) for an interrupt service routine consists
simply of popping the PC and the PSR from the supervisor stack (where they
have been resting peacefully) and restoring them to their rightful places in the","{'page_number': 195, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'is to load the PC and PSR of the interrupt service routine. Interrupt service rou-\ntines are similar to the trap service routines we have already discussed. They are\nprogram fragments stored in system space. They service interrupt requests.\nMost processors use the mechanism of vectored interrupts. You are famil-\niar with this notion from your study of the trap vector contained in the TRAP\ninstruction. In the case of interrupts, the eight-bit vector is provided by the device\nthat is requesting the processor be interrupted. That is, the I/O device trans-\nmits to the processor an eight-bit interrupt vector along with its interrupt request\nsignal and its priority level. The interrupt vector corresponding to the highest\npriority interrupt request is the one supplied to the processor. It is designated\nINTV.\nIf the interrupt is taken, the processor expands the 8-bit interrupt vector\n(INTV) to form a 16-bit address, which is an entry into the Interrupt Vector\nTable. You know that the Trap Vector Table consists of memory locations x0000\nto x00FF, each containing the starting address of a trap service routine. The Inter-\nrupt Vector Table consists of memory locations x0100 to x01FF, each containing\nthe starting address of an interrupt service routine. The processor loads the PC\nwith the contents of the location in the Interrupt Vector Table corresponding to\nthe address formed by expanding the interrupt vector INTV.\nFor example, the LC-3 keyboard could interrupt the processor every time a\nkey is pressed by someone sitting at the keyboard. The keyboard interrupt vector\nwould indicate the location in the interrupt vector table that contains the starting\naddress of the keyboard interrupt service routine.\nThe PSR is loaded as follows: Since no instructions in the service routine\nhave yet executed, PSR[2:0] contains no meaningful information. We arbitrarily\nload it initially with 010. Since the interrupt service routine runs in privileged\nmode, PSR[15] is set to 0. PSR[10:8] is set to the priority level associated with\nthe interrupt request.\nThis completes the initiation phase, and the interrupt service routine is ready\nto execute.\n9.4.5.2 Service the Interrupt\nSince the PC contains the starting address of the interrupt service routine, the\nservice routine will execute, and the requirements of the I/O device will be\nserviced.\n9.4.5.3 Return from the Interrupt\nThe last instruction in every interrupt service routine is RTI, return from trap\nor interrupt. When the processor nally accesses the RTI instruction, all the\nrequirements of the I/O device have been taken care of.\nLike the return from a trap routine discussed in Section 9.3.4, execution of\nthe RTI instruction (opcode = 1000) for an interrupt service routine consists\nsimply of popping the PC and the PSR from the supervisor stack (where they\nhave been resting peacefully) and restoring them to their rightful places in the'}"
"processor. The condition codes are now restored to what they were when the
program was interrupted, in case they are needed by a subsequent BR instruction
in the interrupted program. PSR[15] and PSR[10:8] now reect the privilege level
and priority level of the about-to-be-resumed program. If the privilege level of
the interrupted program is unprivileged, the stack pointers must be adjusted, that
is, the Supervisor Stack Pointer saved, and the User Stack Pointer loaded into R6.
The PC is restored to the address of the instruction that would have been executed
next if the program had not been interrupted.
With all these things as they were before the interrupt occurred, the program
can resume as if nothing had happened.
9.4.6 An Example
We complete the discussion of interrupt-driven I/O with an example.
Suppose program A is executing when I/O device B, having a PL higher
Figure 9.20
Execution ow for interrupt-driven I/O.
Program A consists of instructions in locations x3000 to x3010 and was in
the middle of executing the ADD instruction at x3006 when device B sent its
interrupt request signal and accompanying interrupt vector xF1, causing INT to
be asserted.
Note that the interrupt service routine for device B is stored in locations
x6200 to x6210; x6210 contains the RTI instruction. Note that the service routine","{'page_number': 196, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'processor. The condition codes are now restored to what they were when the\nprogram was interrupted, in case they are needed by a subsequent BR instruction\nin the interrupted program. PSR[15] and PSR[10:8] now reect the privilege level\nand priority level of the about-to-be-resumed program. If the privilege level of\nthe interrupted program is unprivileged, the stack pointers must be adjusted, that\nis, the Supervisor Stack Pointer saved, and the User Stack Pointer loaded into R6.\nThe PC is restored to the address of the instruction that would have been executed\nnext if the program had not been interrupted.\nWith all these things as they were before the interrupt occurred, the program\ncan resume as if nothing had happened.\n9.4.6 An Example\nWe complete the discussion of interrupt-driven I/O with an example.\nSuppose program A is executing when I/O device B, having a PL higher\nFigure 9.20\nExecution ow for interrupt-driven I/O.\nProgram A consists of instructions in locations x3000 to x3010 and was in\nthe middle of executing the ADD instruction at x3006 when device B sent its\ninterrupt request signal and accompanying interrupt vector xF1, causing INT to\nbe asserted.\nNote that the interrupt service routine for device B is stored in locations\nx6200 to x6210; x6210 contains the RTI instruction. Note that the service routine'}"
"for B was in the middle of executing the AND instruction at x6202 when device
C sent its interrupt request signal and accompanying interrupt vector xF2. Since
the request associated with device C is of a higher priority than that of device B,
INT is again asserted.
Note that the interrupt service routine for device C is stored in locations
x6300 to x6315; x6315 contains the RTI instruction.
Let us examine the order of execution by the processor. Figure 9.21 shows
several snapshots of the contents of the supervisor stack and the PC during the
execution of this example.
The processor executes as follows: Figure 9.21a shows the supervisor stack
and the PC before program A fetches the instruction at x3006. Note that the stack
pointer is shown as Saved SSP, not R6. Since the interrupt has not yet occurred,
Figure 9.21
Snapshots of the contents of the supervisor stack and the PC during
interrupt-driven I/O.","{'page_number': 197, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'for B was in the middle of executing the AND instruction at x6202 when device\nC sent its interrupt request signal and accompanying interrupt vector xF2. Since\nthe request associated with device C is of a higher priority than that of device B,\nINT is again asserted.\nNote that the interrupt service routine for device C is stored in locations\nx6300 to x6315; x6315 contains the RTI instruction.\nLet us examine the order of execution by the processor. Figure 9.21 shows\nseveral snapshots of the contents of the supervisor stack and the PC during the\nexecution of this example.\nThe processor executes as follows: Figure 9.21a shows the supervisor stack\nand the PC before program A fetches the instruction at x3006. Note that the stack\npointer is shown as Saved SSP, not R6. Since the interrupt has not yet occurred,\nFigure 9.21\nSnapshots of the contents of the supervisor stack and the PC during\ninterrupt-driven I/O.'}"
"saved on the supervisor stack, the rst step is to start using the supervisor stack.
This is done by saving R6 in the Saved.UPC register and loading R6 with the
contents of the Saved SSP register. The PSR of program A, which includes the
condition codes produced by the ADD instruction, is pushed on the supervisor
stack. Then the address x3007, the PC for the next instruction to be executed in
program A is pushed on the stack. The interrupt vector associated with device B
is expanded to 16 bits x01F1, and the contents of x01F1 (x6200) is loaded into
the PC. Figure 9.21b shows the stack and PC at this point.
The service routine for device B executes until a higher priority interrupt
is detected at the end of execution of the instruction at x6202. The PSR of the
service routine for B, which includes the condition codes produced by the AND
instruction at x6202, and the address x6203 are pushed on the stack. The interrupt
vector associated with device C is expanded to 16 bits (x01F2), and the contents
of x01F2 (x6300) is loaded into the PC. Figure 9.21c shows the supervisor stack
and PC at this point.
Assume the interrupt service routine for device C executes to completion, n-
ishing with the RTI instruction in x6315. The supervisor stack is popped twice,
restoring the PC to x6203 and the PSR of the service routine for device B, includ-
ing the condition codes produced by the AND instruction in x6202. Figure 9.21d
shows the stack and PC at this point.
The interrupt service routine for device B resumes execution at x6203 and
runs to completion, nishing with the RTI instruction in x6210. The supervisor
stack is popped twice, restoring the PC to x3007 and the PSR of program A,
including the condition codes produced by the ADD instruction in x3006. Finally,
since program A is in User mode, the contents of R6 is stored in Saved SSP and
R6 is loaded with the contents of Saved USP. Figure 9.21e shows the supervisor
stack and PC at this point.
Program A resumes execution with the instruction at x3007.
9.4.7 Not Just I/O Devices
We have discussed the processing of interrupts in the context of I/O devices that
have higher priority than the program that is running and therefore can stop that
program to enable its interrupt service routine to execute.
We must point out that not all interrupts deal with I/O devices. Any event that
has a higher priority and is external to the program that is running can interrupt
the computer. It does so by supplying its INT signal, its INTV vector, and its pri-
ority level. If it is the highest priority event that wishes to interrupt the computer,
it does so in the same way that I/O devices do as described above.
There are many examples of such events that have nothing to do with I/O
devices. For example, a timer interrupt interrupts the program that is running in
order to note the passage of a unit of time","{'page_number': 198, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'It does so by supplying its INT signal, its INTV vector, and its pri-\nority level. If it is the highest priority event that wishes to interrupt the computer,\nit does so in the same way that I/O devices do as described above.\nThere are many examples of such events that have nothing to do with I/O\ndevices. For example, a timer interrupt interrupts the program that is running in\norder to note the passage of a unit of time. The machine check interrupt calls atten-\ntion to the fact that some part of the computer system is not functioning properly.\nThe power failure interrupt noties the computer that, for example, someone has\nyanked the power cord out of its receptacle. Unfortunately, we will have to put\no dealing with all of these until later in your coursework.'}"
"It does so by supplying its INT signal, its INTV vector, and its pri-
ority level. If it is the highest priority event that wishes to interrupt the computer,
it does so in the same way that I/O devices do as described above.
There are many examples of such events that have nothing to do with I/O
devices. For example, a timer interrupt interrupts the program that is running in
order to note the passage of a unit of time. The machine check interrupt calls atten-
tion to the fact that some part of the computer system is not functioning properly.
The power failure interrupt noties the computer that, for example, someone has
yanked the power cord out of its receptacle. Unfortunately, we will have to put
o dealing with all of these until later in your coursework.","{'page_number': 198, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'It does so by supplying its INT signal, its INTV vector, and its pri-\nority level. If it is the highest priority event that wishes to interrupt the computer,\nit does so in the same way that I/O devices do as described above.\nThere are many examples of such events that have nothing to do with I/O\ndevices. For example, a timer interrupt interrupts the program that is running in\norder to note the passage of a unit of time. The machine check interrupt calls atten-\ntion to the fact that some part of the computer system is not functioning properly.\nThe power failure interrupt noties the computer that, for example, someone has\nyanked the power cord out of its receptacle. Unfortunately, we will have to put\no dealing with all of these until later in your coursework.'}"
"9.5 Polling Revisited, Now That
We Know About Interrupts
9.5.1 The Problem
Recall our discussion of polling: We continually test the ready bit in the relevant
status register, and if it is not set, we branch back to again test the ready bit. For
example, suppose we are writing a character string to the monitor, and we are
using polling to determine when the monitor has successfully written the current
character so we can dispatch the next character. We take it for granted that the
three-instruction sequence LDI (to load the ready bit of the DSR), BRzp (to test
it and fall through if the device is ready), and STI (to store the next character in the
DDR) acts as an atomic unit. But what if we had interrupts enabled at the same
time? That is, if an interrupt occurred within that LDI, BRzp, STI sequence (say,
just before the STI instruction), it could easily be the case that the LDI instruction
indicated the DDR was ready, the BRzp instruction did not branch back, but by
the time the interrupt service routine completed so the STI could write to the
DDR, the DDR may no longer be ready. The computer would execute the STI,
but the write would not happen.
A simple, but somewhat contrived example :-), will illustrate the problem.
Suppose you are executing a for loop ten times, where each time the loop body
prints to the monitor a particular character. Polling is used to determine that the
monitor is ready before writing the next character to the DDR. Since the loop
body executes ten times, this should result in the character being printed on the
monitor ten times. Suppose you also have keyboard interrupts enabled, and the
keyboard service routine echoes the character typed.
Suppose the loop body executes as follows: LDI loads the ready bit, BRzp
falls through since the monitor is ready, and STI stores the character in DDR. In
the middle of this sequence, before the STI can execute, someone types a key.
The keyboard interrupt occurs, the character typed is echoed, i.e., written to the
DDR, and the keyboard interrupt service routine completes.
The interrupted loop body then takes over and knows the monitor is ready,
so it executes the STI. ... except the monitor is not ready because it has not com-
pleted the write of the keyboard service routine! The STI of the loop body writes,
but since DDR is not ready, the write does not occur. The nal result: Only nine
characters get written, not ten.
The problem becomes more serious if the string written is in code, and the
missing write prevents the code from being deciphered.
A simple way to handle this would be to disable all interrupts while polling
was going on. But consider the consequences. Suppose the polling was required
for a long time. If we disable interrupts while polling is going on, interrupts would
be disabled for that very long time, unacceptable in an environment where one is
concerned about the time between a higher priority interrupt occurring and the
interrupt getting service.","{'page_number': 199, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '9.5 Polling Revisited, Now That\nWe Know About Interrupts\n9.5.1 The Problem\nRecall our discussion of polling: We continually test the ready bit in the relevant\nstatus register, and if it is not set, we branch back to again test the ready bit. For\nexample, suppose we are writing a character string to the monitor, and we are\nusing polling to determine when the monitor has successfully written the current\ncharacter so we can dispatch the next character. We take it for granted that the\nthree-instruction sequence LDI (to load the ready bit of the DSR), BRzp (to test\nit and fall through if the device is ready), and STI (to store the next character in the\nDDR) acts as an atomic unit. But what if we had interrupts enabled at the same\ntime? That is, if an interrupt occurred within that LDI, BRzp, STI sequence (say,\njust before the STI instruction), it could easily be the case that the LDI instruction\nindicated the DDR was ready, the BRzp instruction did not branch back, but by\nthe time the interrupt service routine completed so the STI could write to the\nDDR, the DDR may no longer be ready. The computer would execute the STI,\nbut the write would not happen.\nA simple, but somewhat contrived example :-), will illustrate the problem.\nSuppose you are executing a for loop ten times, where each time the loop body\nprints to the monitor a particular character. Polling is used to determine that the\nmonitor is ready before writing the next character to the DDR. Since the loop\nbody executes ten times, this should result in the character being printed on the\nmonitor ten times. Suppose you also have keyboard interrupts enabled, and the\nkeyboard service routine echoes the character typed.\nSuppose the loop body executes as follows: LDI loads the ready bit, BRzp\nfalls through since the monitor is ready, and STI stores the character in DDR. In\nthe middle of this sequence, before the STI can execute, someone types a key.\nThe keyboard interrupt occurs, the character typed is echoed, i.e., written to the\nDDR, and the keyboard interrupt service routine completes.\nThe interrupted loop body then takes over and knows the monitor is ready,\nso it executes the STI. ... except the monitor is not ready because it has not com-\npleted the write of the keyboard service routine! The STI of the loop body writes,\nbut since DDR is not ready, the write does not occur. The nal result: Only nine\ncharacters get written, not ten.\nThe problem becomes more serious if the string written is in code, and the\nmissing write prevents the code from being deciphered.\nA simple way to handle this would be to disable all interrupts while polling\nwas going on. But consider the consequences. Suppose the polling was required\nfor a long time. If we disable interrupts while polling is going on, interrupts would\nbe disabled for that very long time, unacceptable in an environment where one is\nconcerned about the time between a higher priority interrupt occurring and the\ninterrupt getting service.'}"
"LC-3 ISA
A.1 Overview
The instruction set architecture (ISA) of the LC-3 is dened as follows:
Memory address space 16 bits, corresponding to 216 locations, each
containing one word (16 bits). Addresses are numbered from 0 (i.e., x0000)
to 65,535 (i.e., xFFFF). Addresses are used to identify memory locations
and memory-mapped I/O device registers. Certain regions of memory are
reserved for special uses, as described in Figure A.1.
xFFFF
Figure A.1
Memory map of the LC-3","{'page_number': 200, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'LC-3 ISA\nA.1 Overview\nThe instruction set architecture (ISA) of the LC-3 is dened as follows:\nMemory address space 16 bits, corresponding to 216 locations, each\ncontaining one word (16 bits). Addresses are numbered from 0 (i.e., x0000)\nto 65,535 (i.e., xFFFF). Addresses are used to identify memory locations\nand memory-mapped I/O device registers. Certain regions of memory are\nreserved for special uses, as described in Figure A.1.\nxFFFF\nFigure A.1\nMemory map of the LC-3'}"
"Table A.1
Device Register Assignments
Address
I/O Register Name
I/O Register Function
xFE00
Keyboard status register (KBSR)
The ready bit (bit [15]) indicates if the keyboard has received a
new character.
xFE02
Keyboard data register (KBDR)
Bits [7:0] contain the last character typed on the keyboard.
xFE04
Display status register (DSR)
The ready bit (bit [15]) indicates if the display device is ready to
receive another character to print on the screen.
xFE06
Display data register (DDR)
A character written in bits [7:0] will be displayed on the screen.
xFFFC
Processor Status Register (PSR)
Contains privilege mode, priority level and condition codes of
the currently executing process.
xFFFE
Machine control register (MCR)
Bit [15] is the clock enable bit. When cleared, instruction
processing stops.
data. Addresses xFE00 to xFFFF specify input and output device registers
and special internal processor registers that are also only accessible if the
process is executing in Supervisor mode (PSR[15]=0). For purposes of
controlling access to these device registers, their addresses are also
considered part of privileged memory.
Memory-mapped I/O Input and output are handled by load/store (LD/ST,
LDI/STI, LDR/STR) instructions using memory addresses from xFE00 to
xFFFF to designate each device register. Table A.1 lists the input and
output device registers and internal processor registers that have been
specied for the LC-3 thus far, along with their corresponding assigned
addresses from the memory address space.
Bit numbering Bits of all quantities are numbered, from right to left,
starting with bit 0. The leftmost bit of the contents of a memory location is
bit 15.
Instructions Instructions are 16 bits wide. Bits [15:12] specify the opcode
(operation to be performed); bits [11:0] provide further information that is
needed to execute the instruction. The specic operation of each LC-3
instruction is described in Section A.2.
Illegal opcode exception Bits [15:12] = 1101 has not been specied. If
an instruction contains 1101 in bits [15:12], an illegal opcode exception
occurs. Section A.3 explains what happens.
Program counter A 16-bit register containing the address of the next
instruction to be processed.
General purpose registers Eight 16-bit registers, numbered from 000 to
111 (R0 to R7).
Condition codes Three 1-bit registers: N (negative), Z (zero), and P
(positive). Load instructions (LD, LDI, and LDR) and operate instructions
(ADD, AND, and NOT) each load a result into one of the eight general
purpose registers. The condition codes are set, based on whether that result,
taken as a 16-bit 2s complement integer, is negative (N = 1; Z, P = 0), zero
(Z = 1; N, P = 0), or positive (P = 1; N, Z = 0)","{'page_number': 201, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Load instructions (LD, LDI, and LDR) and operate instructions\n(ADD, AND, and NOT) each load a result into one of the eight general\npurpose registers. The condition codes are set, based on whether that result,\ntaken as a 16-bit 2s complement integer, is negative (N = 1; Z, P = 0), zero\n(Z = 1; N, P = 0), or positive (P = 1; N, Z = 0). All other LC-3 instructions\nleave the condition codes unchanged.'}"
"Load instructions (LD, LDI, and LDR) and operate instructions
(ADD, AND, and NOT) each load a result into one of the eight general
purpose registers. The condition codes are set, based on whether that result,
taken as a 16-bit 2s complement integer, is negative (N = 1; Z, P = 0), zero
(Z = 1; N, P = 0), or positive (P = 1; N, Z = 0). All other LC-3 instructions
leave the condition codes unchanged.","{'page_number': 201, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Load instructions (LD, LDI, and LDR) and operate instructions\n(ADD, AND, and NOT) each load a result into one of the eight general\npurpose registers. The condition codes are set, based on whether that result,\ntaken as a 16-bit 2s complement integer, is negative (N = 1; Z, P = 0), zero\n(Z = 1; N, P = 0), or positive (P = 1; N, Z = 0). All other LC-3 instructions\nleave the condition codes unchanged.'}"
"Interrupt processing I/O devices have the capability of interrupting the
processor. Section A.3 describes the mechanism.
Priority level The LC-3 supports eight levels of priority. Priority level 7
(PL7) is the highest, PL0 is the lowest. The priority level of the currently
executing process is specied in bits PSR[10:8].
Processor status register (PSR) A 16-bit register, containing status
information about the currently executing process. Seven bits of the PSR
have been dened thus far. PSR[15] species the privilege mode of
the executing process. PSR[10:8] species the priority level of the currently
executing process. PSR[2:0] contains the condition codes. PSR[2] is N,
PSR[1] is Z, and PSR[0] is P.
Supervisor mode The LC-3 species two modes of operation, Supervisor
mode (privileged) and User mode (unprivileged). Interrupt service routines
and trap service routines (i.e., system calls) execute in Supervisor mode.
The privilege mode is specied by PSR[15]. PSR[15]=0 indicates
Supervisor mode; PSR[15]=1 indicates User mode.
Privilege mode exception The RTI instruction executes in Supervisor mode.
If the processor attempts to execute the RTI instruction while in User mode, a
privilege mode exception occurs. Section A.3 explains what happens.
Access Control Violation (ACV) exception An ACV exception occurs if a
process attempts to access a location in privileged memory (either a location in
system space or a device register having an address from xFE00 to xFFFF)
while operating in User mode. Section A.3 explains what happens.
Supervisor stack A region of memory in system space accessible via the
Supervisor Stack Pointer (SSP). When PSR[15]=0, the stack pointer (R6) is
SSP. When the processor is operating in User mode (PSR[15]=1), the SSP
is stored in Saved SSP.
User stack A region of memory in user space accessible via the User Stack
Pointer (USP). When PSR[15]=1, the stack pointer (R6) is USP. When the
processor is operating in Supervisor mode (PSR[15]=0), the USP is stored
in Saved USP.
A.2 The Instruction Set
The LC-3 supports a rich, but lean, instruction set. Each 16-bit instruction consists
of an opcode (bits[15:12]) plus 12 additional bits to specify the other informa-
tion that is needed to carry out that instruction. Figure A.2 summarizes the 15
dierent opcodes in the LC-3 and the specication of the remaining bits of each
instruction. The 16th four-bit opcode is not specied but is reserved for future use.
In the following pages, the instructions will be described in greater detail.
Table A.2 is provided to help you to understand those descriptions","{'page_number': 202, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure A.2 summarizes the 15\ndierent opcodes in the LC-3 and the specication of the remaining bits of each\ninstruction. The 16th four-bit opcode is not specied but is reserved for future use.\nIn the following pages, the instructions will be described in greater detail.\nTable A.2 is provided to help you to understand those descriptions. For each\ninstruction, we show the assembly language representation, the format of the\n16-bit instruction, the operation of the instruction, an English-language descrip-\ntion of its operation, and one or more examples of the instruction. Where relevant,\nadditional notes about the instruction are also provided.'}"
"Figure A.2 summarizes the 15
dierent opcodes in the LC-3 and the specication of the remaining bits of each
instruction. The 16th four-bit opcode is not specied but is reserved for future use.
In the following pages, the instructions will be described in greater detail.
Table A.2 is provided to help you to understand those descriptions. For each
instruction, we show the assembly language representation, the format of the
16-bit instruction, the operation of the instruction, an English-language descrip-
tion of its operation, and one or more examples of the instruction. Where relevant,
additional notes about the instruction are also provided.","{'page_number': 202, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure A.2 summarizes the 15\ndierent opcodes in the LC-3 and the specication of the remaining bits of each\ninstruction. The 16th four-bit opcode is not specied but is reserved for future use.\nIn the following pages, the instructions will be described in greater detail.\nTable A.2 is provided to help you to understand those descriptions. For each\ninstruction, we show the assembly language representation, the format of the\n16-bit instruction, the operation of the instruction, an English-language descrip-\ntion of its operation, and one or more examples of the instruction. Where relevant,\nadditional notes about the instruction are also provided.'}"
"Table A.2
Notational Conventions
Notation
Meaning
xNumber
The number in hexadecimal notation. Example: xF2A1
#Number
The number in decimal notation. Example #793
bNumber
The number in binary. Example b10011
A[l:r]
The eld delimited by bit [l] on the left and bit [r] on the right, of the datum A. For example, if PC
contains 0011001100111111, then PC[15:9] is 0011001. PC[2:2] is 1. If l and r are the same bit
number, we generally write PC[2].
BaseR
Base Register; one of R0..R7, specied by bits [8:6] of the instruction, used in conjunction with a six-bit
ofset to compute Base+ofset addresses (LDR and STR), or alone to identify the target address of a
control instruction (JMP and JSRR).
DR
Destination Register; one of R0..R7, which species the register a result should be written to.
imm5
A ve-bit immediate value (bits [4:0] of an instruction), when used as a literal (immediate) value. Taken
as a ve-bit, 2s complement integer, it is sign-extended to 16 bits before it is used. Range: 16..15.
INTV
An eight-bit value, supplied along with an interrupting event; used to determine the starting address
of an interrupt service routine. The eight bits form an ofset from the starting address of the interrupt
vector table. The corresponding location in the interrupt vector table contains the starting address
of the corresponding interrupt service routine. Range 0..255.
LABEL
An assembly language construct that identies a location symbolically (i.e., by means of a name,
rather than its 16-bit address).
mem[address]
Denotes the contents of memory at the given address.
ofset6
A six-bit signed 2s complement integer (bits [5:0] of an instruction), used with the Base+ofset
addressing mode. Bits [5:0] are sign-extended to 16 bits and then added to the Base Register to
form an address. Range: 32..31.
PC
Program Counter; 16-bit register that contains the memory address of the next instruction to be
fetched. For example, if the instruction at address A is not a control instruction, during its execution,
the PC contains the address A + 1, indicating that the next instruction to be executed is contained in
memory location A + 1.
PCofset9
A nine-bit signed 2s complement integer (bits [8:0] of an instruction), used with the PC+ofset
addressing mode. Bits [8:0] are sign-extended to 16 bits and then added to the incremented PC to
form an address. Range 256..255.
PCofset11
An eleven-bit signed 2s complement integer (bits [10:0] of an instruction), used with the JSR opcode
to compute the target address of a subroutine call","{'page_number': 203, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Bits [8:0] are sign-extended to 16 bits and then added to the incremented PC to\nform an address. Range 256..255.\nPCofset11\nAn eleven-bit signed 2s complement integer (bits [10:0] of an instruction), used with the JSR opcode\nto compute the target address of a subroutine call. Bits [10:0] are sign-extended to 16 bits and then\nadded to the incremented PC to form the target address. Range 1024..1023.\nPSR\nProcessor Status Register. A 16-bit register that contains status information of the process that is\nexecuting. Seven bits of the PSR have been specied. PSR[15] = privilege mode. PSR[10:8] =\nPriority Level. PSR[2:0] contains the condition codes. PSR[2] = N, PSR[1] = Z, PSR[0] = P.\nSaved SSP\nSaved Supervisor Stack Pointer. The processor is executing in either Supervisor mode or User mode.\nIf in User mode, R6, the stack pointer, is the User Stack Pointer (USP). The Supervisor Stack Pointer\n(SSP) is stored in Saved SSP. When the privilege mode changes from User mode to Supervisor\nmode, Saved USP is loaded with R6 and R6 is loaded with Saved SSP.\nSaved USP\nSaved User Stack Pointer. The User Stack Pointer is stored in Saved USP when the processor is\nexecuting in Supervisor mode. See Saved SSP.\nsetcc()\nIndicates that condition codes N, Z, and P are set based on the value of the result written to DR.\nSEXT(A)\nSign-extend A. The most signicant bit of A is replicated as many times as necessary to extend A to\n16 bits. For example, if A = 110000, then SEXT(A) = 1111 1111 1111 0000.\nSP\nThe current stack pointer. R6 is the current stack pointer. There are two stacks, one for each privilege\nmode. SP is SSP if PSR[15] = 0; SP is USP if PSR[15] = 1.\nSR, SR1, SR2\nSource register; one of R0..R7 that species the register from which a source operand is obtained.\nSSP\nThe Supervisor Stack Pointer.\ntrapvect8\nAn eight-bit value (bits [7:0] of an instruction), used with the TRAP opcode to determine the starting\naddress of a trap service routine. Bits [7:0] are taken as an unsigned integer and zero-extended to\n16 bits. This is the address of the memory location containing the starting address of the\ncorresponding service routine. Range 0..255.\nUSP\nThe User Stack Pointer.\nZEXT(A)\nZero-extend A. Zeros are appended to the leftmost bit of A to extend it to 16 bits. For example, if\nA = 110000, then ZEXT(A) = 0000 0000 0011 0000.'}"
"Bits [8:0] are sign-extended to 16 bits and then added to the incremented PC to
form an address. Range 256..255.
PCofset11
An eleven-bit signed 2s complement integer (bits [10:0] of an instruction), used with the JSR opcode
to compute the target address of a subroutine call. Bits [10:0] are sign-extended to 16 bits and then
added to the incremented PC to form the target address. Range 1024..1023.
PSR
Processor Status Register. A 16-bit register that contains status information of the process that is
executing. Seven bits of the PSR have been specied. PSR[15] = privilege mode. PSR[10:8] =
Priority Level. PSR[2:0] contains the condition codes. PSR[2] = N, PSR[1] = Z, PSR[0] = P.
Saved SSP
Saved Supervisor Stack Pointer. The processor is executing in either Supervisor mode or User mode.
If in User mode, R6, the stack pointer, is the User Stack Pointer (USP). The Supervisor Stack Pointer
(SSP) is stored in Saved SSP. When the privilege mode changes from User mode to Supervisor
mode, Saved USP is loaded with R6 and R6 is loaded with Saved SSP.
Saved USP
Saved User Stack Pointer. The User Stack Pointer is stored in Saved USP when the processor is
executing in Supervisor mode. See Saved SSP.
setcc()
Indicates that condition codes N, Z, and P are set based on the value of the result written to DR.
SEXT(A)
Sign-extend A. The most signicant bit of A is replicated as many times as necessary to extend A to
16 bits. For example, if A = 110000, then SEXT(A) = 1111 1111 1111 0000.
SP
The current stack pointer. R6 is the current stack pointer. There are two stacks, one for each privilege
mode. SP is SSP if PSR[15] = 0; SP is USP if PSR[15] = 1.
SR, SR1, SR2
Source register; one of R0..R7 that species the register from which a source operand is obtained.
SSP
The Supervisor Stack Pointer.
trapvect8
An eight-bit value (bits [7:0] of an instruction), used with the TRAP opcode to determine the starting
address of a trap service routine. Bits [7:0] are taken as an unsigned integer and zero-extended to
16 bits. This is the address of the memory location containing the starting address of the
corresponding service routine. Range 0..255.
USP
The User Stack Pointer.
ZEXT(A)
Zero-extend A. Zeros are appended to the leftmost bit of A to extend it to 16 bits. For example, if
A = 110000, then ZEXT(A) = 0000 0000 0011 0000.","{'page_number': 203, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Bits [8:0] are sign-extended to 16 bits and then added to the incremented PC to\nform an address. Range 256..255.\nPCofset11\nAn eleven-bit signed 2s complement integer (bits [10:0] of an instruction), used with the JSR opcode\nto compute the target address of a subroutine call. Bits [10:0] are sign-extended to 16 bits and then\nadded to the incremented PC to form the target address. Range 1024..1023.\nPSR\nProcessor Status Register. A 16-bit register that contains status information of the process that is\nexecuting. Seven bits of the PSR have been specied. PSR[15] = privilege mode. PSR[10:8] =\nPriority Level. PSR[2:0] contains the condition codes. PSR[2] = N, PSR[1] = Z, PSR[0] = P.\nSaved SSP\nSaved Supervisor Stack Pointer. The processor is executing in either Supervisor mode or User mode.\nIf in User mode, R6, the stack pointer, is the User Stack Pointer (USP). The Supervisor Stack Pointer\n(SSP) is stored in Saved SSP. When the privilege mode changes from User mode to Supervisor\nmode, Saved USP is loaded with R6 and R6 is loaded with Saved SSP.\nSaved USP\nSaved User Stack Pointer. The User Stack Pointer is stored in Saved USP when the processor is\nexecuting in Supervisor mode. See Saved SSP.\nsetcc()\nIndicates that condition codes N, Z, and P are set based on the value of the result written to DR.\nSEXT(A)\nSign-extend A. The most signicant bit of A is replicated as many times as necessary to extend A to\n16 bits. For example, if A = 110000, then SEXT(A) = 1111 1111 1111 0000.\nSP\nThe current stack pointer. R6 is the current stack pointer. There are two stacks, one for each privilege\nmode. SP is SSP if PSR[15] = 0; SP is USP if PSR[15] = 1.\nSR, SR1, SR2\nSource register; one of R0..R7 that species the register from which a source operand is obtained.\nSSP\nThe Supervisor Stack Pointer.\ntrapvect8\nAn eight-bit value (bits [7:0] of an instruction), used with the TRAP opcode to determine the starting\naddress of a trap service routine. Bits [7:0] are taken as an unsigned integer and zero-extended to\n16 bits. This is the address of the memory location containing the starting address of the\ncorresponding service routine. Range 0..255.\nUSP\nThe User Stack Pointer.\nZEXT(A)\nZero-extend A. Zeros are appended to the leftmost bit of A to extend it to 16 bits. For example, if\nA = 110000, then ZEXT(A) = 0000 0000 0011 0000.'}"
"ADD
Addition
15
12
11
9
8
6
5
4
0
0001
DR
SR1
1
imm5
Operation
if (bit[5] == 0)
DR = SR1 + SR2;
else
DR = SR1 + SEXT(imm5);
setcc();
Description
If bit [5] is 0, the second source operand is obtained from SR2. If bit [5] is 1, the
second source operand is obtained by sign-extending the imm5 eld to 16 bits.
In both cases, the second source operand is added to the contents of SR1 and the
result stored in DR. The condition codes are set, based on whether the result is
negative, zero, or positive.
Examples
ADD
R2, R3, R4
; R2  R3 + R4
ADD
R2, R3, #7
; R2  R3 + 7","{'page_number': 204, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'ADD\nAddition\n15\n12\n11\n9\n8\n6\n5\n4\n0\n0001\nDR\nSR1\n1\nimm5\nOperation\nif (bit[5] == 0)\nDR = SR1 + SR2;\nelse\nDR = SR1 + SEXT(imm5);\nsetcc();\nDescription\nIf bit [5] is 0, the second source operand is obtained from SR2. If bit [5] is 1, the\nsecond source operand is obtained by sign-extending the imm5 eld to 16 bits.\nIn both cases, the second source operand is added to the contents of SR1 and the\nresult stored in DR. The condition codes are set, based on whether the result is\nnegative, zero, or positive.\nExamples\nADD\nR2, R3, R4\n; R2  R3 + R4\nADD\nR2, R3, #7\n; R2  R3 + 7'}"
"AND
Bit-wise Logical AND
12
11
9
8
6
5
4
0
15
0101
DR
SR1
1
imm5
Operation
if (bit[5] == 0)
DR = SR1 AND SR2;
else
DR = SR1 AND SEXT(imm5);
setcc();
Description
If bit [5] is 0, the second source operand is obtained from SR2. If bit [5] is 1,
the second source operand is obtained by sign-extending the imm5 eld to 16
bits. In either case, the second source operand and the contents of SR1 are bit-
wise ANDed and the result stored in DR. The condition codes are set, based on
whether the binary value produced, taken as a 2s complement integer, is negative,
zero, or positive.
Examples
AND
R2, R3, R4
;R2  R3 AND R4
AND
R2, R3, #7
;R2  R3 AND 7","{'page_number': 205, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'AND\nBit-wise Logical AND\n12\n11\n9\n8\n6\n5\n4\n0\n15\n0101\nDR\nSR1\n1\nimm5\nOperation\nif (bit[5] == 0)\nDR = SR1 AND SR2;\nelse\nDR = SR1 AND SEXT(imm5);\nsetcc();\nDescription\nIf bit [5] is 0, the second source operand is obtained from SR2. If bit [5] is 1,\nthe second source operand is obtained by sign-extending the imm5 eld to 16\nbits. In either case, the second source operand and the contents of SR1 are bit-\nwise ANDed and the result stored in DR. The condition codes are set, based on\nwhether the binary value produced, taken as a 2s complement integer, is negative,\nzero, or positive.\nExamples\nAND\nR2, R3, R4\n;R2  R3 AND R4\nAND\nR2, R3, #7\n;R2  R3 AND 7'}"
"BR
Conditional Branch
Assembler Formats
BRn
LABEL
BRzp
LABEL
z
n
p
0000
PCofset9
Operation
if ((n AND N) OR (z AND Z) OR (p AND P))
PC = PC + SEXT(PCoffset9);
Description
The condition codes specied by bits [11:9] are tested. If bit [11] is 1, N is tested;
if bit [11] is 0, N is not tested. If bit [10] is 1, Z is tested, etc. If any of the condi-
tion codes tested is 1, the program branches to the memory location specied by
adding the sign-extended PCoset9 eld to the incremented PC.
Examples
BRzp
LOOP
; Branch to LOOP if the last result was zero or positive.
BR
NEXT
; Unconditionally branch to NEXT.
The assembly language opcode BR is interpreted the same as BRnzp; that is, always branch to the target
address.
This is the incremented PC.","{'page_number': 206, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'BR\nConditional Branch\nAssembler Formats\nBRn\nLABEL\nBRzp\nLABEL\nz\nn\np\n0000\nPCofset9\nOperation\nif ((n AND N) OR (z AND Z) OR (p AND P))\nPC = PC + SEXT(PCoffset9);\nDescription\nThe condition codes specied by bits [11:9] are tested. If bit [11] is 1, N is tested;\nif bit [11] is 0, N is not tested. If bit [10] is 1, Z is tested, etc. If any of the condi-\ntion codes tested is 1, the program branches to the memory location specied by\nadding the sign-extended PCoset9 eld to the incremented PC.\nExamples\nBRzp\nLOOP\n; Branch to LOOP if the last result was zero or positive.\nBR\nNEXT\n; Unconditionally branch to NEXT.\nThe assembly language opcode BR is interpreted the same as BRnzp; that is, always branch to the target\naddress.\nThis is the incremented PC.'}"
"JMP
RET
Jump
Return from Subroutine
Assembler Formats
000
111
000000
0
5
6
8
9
11
12
15
RET
1100
Operation
PC = BaseR;
Description
The program unconditionally jumps to the location specied by the contents of
the base register. Bits [8:6] identify the base register.
Examples
JMP
R2
; PC  R2
RET
; PC  R7
Note
The RET instruction is a special case of the JMP instruction, normally used in the
return from a subroutine. The PC is loaded with the contents of R7, which con-
tains the linkage back to the instruction following the subroutine call instruction.","{'page_number': 207, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'JMP\nRET\nJump\nReturn from Subroutine\nAssembler Formats\n000\n111\n000000\n0\n5\n6\n8\n9\n11\n12\n15\nRET\n1100\nOperation\nPC = BaseR;\nDescription\nThe program unconditionally jumps to the location specied by the contents of\nthe base register. Bits [8:6] identify the base register.\nExamples\nJMP\nR2\n; PC  R2\nRET\n; PC  R7\nNote\nThe RET instruction is a special case of the JMP instruction, normally used in the\nreturn from a subroutine. The PC is loaded with the contents of R7, which con-\ntains the linkage back to the instruction following the subroutine call instruction.'}"
"JSR
JSRR
Jump to Subroutine
Assembler Formats
00
0
BaseR
000000
0
5
6
8
9
10
11
12
15
JSRR
0100
Operation
TEMP = PC;
if (bit[11] == 0)
PC = BaseR;
else
PC = PC + SEXT(PCoffset11);
R7 = TEMP;
Description
First, the incremented PC is saved in a temporary location. Then the PC is loaded
with the address of the rst instruction of the subroutine, which will cause an
unconditional jump to that address after the current instruction completes execu-
tion. The address of the subroutine is obtained from the base register (if bit [11]
is 0), or the address is computed by sign-extending bits [10:0] and adding this
value to the incremented PC (if bit [11] is 1). Finally, R7 is loaded with the value
stored in the temporary location. This is the linkage back to the calling routine.
Examples
JSR
QUEUE ; Put the address of the instruction following JSR into R7;
; Jump to QUEUE.
JSRR R3
; Put the address of the instruction following JSRR into R7;
; Jump to the address contained in R3.
This is the incremented PC.","{'page_number': 208, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'JSR\nJSRR\nJump to Subroutine\nAssembler Formats\n00\n0\nBaseR\n000000\n0\n5\n6\n8\n9\n10\n11\n12\n15\nJSRR\n0100\nOperation\nTEMP = PC;\nif (bit[11] == 0)\nPC = BaseR;\nelse\nPC = PC + SEXT(PCoffset11);\nR7 = TEMP;\nDescription\nFirst, the incremented PC is saved in a temporary location. Then the PC is loaded\nwith the address of the rst instruction of the subroutine, which will cause an\nunconditional jump to that address after the current instruction completes execu-\ntion. The address of the subroutine is obtained from the base register (if bit [11]\nis 0), or the address is computed by sign-extending bits [10:0] and adding this\nvalue to the incremented PC (if bit [11] is 1). Finally, R7 is loaded with the value\nstored in the temporary location. This is the linkage back to the calling routine.\nExamples\nJSR\nQUEUE ; Put the address of the instruction following JSR into R7;\n; Jump to QUEUE.\nJSRR R3\n; Put the address of the instruction following JSRR into R7;\n; Jump to the address contained in R3.\nThis is the incremented PC.'}"
"LD
Load
PCofset9
0010
DR
15
12
11
9
8
0
Operation
if (computed address is in privileged memory AND PSR[15] == 1)
Initiate ACV exception;
else
DR = mem[PC + SEXT(PCoffset9)];
setcc();
Description
An address is computed by sign-extending bits [8:0] to 16 bits and adding
this value to the incremented PC. If the address is to privileged memory and
PSR[15]=1, initiate ACV exception. If not, the contents of memory at this address
is loaded into DR. The condition codes are set, based on whether the value loaded
is negative, zero, or positive.
Example
LD
R4, VALUE
; R4  mem[VALUE]
This is the incremented PC.","{'page_number': 209, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'LD\nLoad\nPCofset9\n0010\nDR\n15\n12\n11\n9\n8\n0\nOperation\nif (computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nDR = mem[PC + SEXT(PCoffset9)];\nsetcc();\nDescription\nAn address is computed by sign-extending bits [8:0] to 16 bits and adding\nthis value to the incremented PC. If the address is to privileged memory and\nPSR[15]=1, initiate ACV exception. If not, the contents of memory at this address\nis loaded into DR. The condition codes are set, based on whether the value loaded\nis negative, zero, or positive.\nExample\nLD\nR4, VALUE\n; R4  mem[VALUE]\nThis is the incremented PC.'}"
"LDI
Load Indirect
PCofset9
1010
DR
15
12
11
9
8
0
Operation
if (either computed address is in privileged memory AND PSR[15] == 1)
Initiate ACV exception;
else
DR = mem[mem[PC + SEXT(PCoffset9)]];
setcc();
Description
An address is computed by sign-extending bits [8:0] to 16 bits and adding this
value to the incremented PC. What is stored in memory at this address is the
address of the data to be loaded into DR. If either address is to privileged mem-
ory and PSR[15]=1, initiate ACV exception. If not, the data is loaded and the
condition codes are set, based on whether the value loaded is negative, zero, or
positive.
Example
LDI
R4, ONEMORE
; R4  mem[mem[ONEMORE]]
This is the incremented PC.","{'page_number': 210, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'LDI\nLoad Indirect\nPCofset9\n1010\nDR\n15\n12\n11\n9\n8\n0\nOperation\nif (either computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nDR = mem[mem[PC + SEXT(PCoffset9)]];\nsetcc();\nDescription\nAn address is computed by sign-extending bits [8:0] to 16 bits and adding this\nvalue to the incremented PC. What is stored in memory at this address is the\naddress of the data to be loaded into DR. If either address is to privileged mem-\nory and PSR[15]=1, initiate ACV exception. If not, the data is loaded and the\ncondition codes are set, based on whether the value loaded is negative, zero, or\npositive.\nExample\nLDI\nR4, ONEMORE\n; R4  mem[mem[ONEMORE]]\nThis is the incremented PC.'}"
"LDR
Load Base+oset
15
12
11
9
8
6
5
0
BaseR
DR
0110
ofset6
Operation
If (computed address is in privileged memory AND PSR[15] == 1)
Initiate ACV exception;
else
DR = mem[BaseR + SEXT(offset6)];
setcc();
Description
An address is computed by sign-extending bits [5:0] to 16 bits and adding this
value to the contents of the register specied by bits [8:6]. If the computed address
is to privileged memory and PSR[15]=1, initiate ACV exception. If not, the con-
tents of memory at this address is loaded into DR. The condition codes are set,
based on whether the value loaded is negative, zero, or positive.
Example
LDR
R4, R2, #5
; R4  mem[R2  5]","{'page_number': 211, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'LDR\nLoad Base+oset\n15\n12\n11\n9\n8\n6\n5\n0\nBaseR\nDR\n0110\nofset6\nOperation\nIf (computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nDR = mem[BaseR + SEXT(offset6)];\nsetcc();\nDescription\nAn address is computed by sign-extending bits [5:0] to 16 bits and adding this\nvalue to the contents of the register specied by bits [8:6]. If the computed address\nis to privileged memory and PSR[15]=1, initiate ACV exception. If not, the con-\ntents of memory at this address is loaded into DR. The condition codes are set,\nbased on whether the value loaded is negative, zero, or positive.\nExample\nLDR\nR4, R2, #5\n; R4  mem[R2  5]'}"
"LEA
Load Eective Address
15
12
11
9
8
0
DR
1110
PCofset9
Operation
DR = PC + SEXT(PCoffset9);
Description
An address is computed by sign-extending bits [8:0] to 16 bits and adding this
value to the incremented PC. This address is loaded into DR.
Example
LEA
R4, TARGET
; R4  address of TARGET.
This is the incremented PC.
The LEA instruction computes an address but does NOT read memory. Instead, the address itself is
loaded into DR.","{'page_number': 212, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'LEA\nLoad Eective Address\n15\n12\n11\n9\n8\n0\nDR\n1110\nPCofset9\nOperation\nDR = PC + SEXT(PCoffset9);\nDescription\nAn address is computed by sign-extending bits [8:0] to 16 bits and adding this\nvalue to the incremented PC. This address is loaded into DR.\nExample\nLEA\nR4, TARGET\n; R4  address of TARGET.\nThis is the incremented PC.\nThe LEA instruction computes an address but does NOT read memory. Instead, the address itself is\nloaded into DR.'}"
"NOT
Bit-Wise Complement
11111
15
12
11
9
8
6
5
4
3
2
0
DR
1001
SR
1
Operation
DR = NOT(SR);
setcc();
Description
The bit-wise complement of the contents of SR is stored in DR. The condi-
tion codes are set, based on whether the binary value produced, taken as a 2s
complement integer, is negative, zero, or positive.
Example
NOT
R4, R2
; R4  NOT(R2)","{'page_number': 213, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'NOT\nBit-Wise Complement\n11111\n15\n12\n11\n9\n8\n6\n5\n4\n3\n2\n0\nDR\n1001\nSR\n1\nOperation\nDR = NOT(SR);\nsetcc();\nDescription\nThe bit-wise complement of the contents of SR is stored in DR. The condi-\ntion codes are set, based on whether the binary value produced, taken as a 2s\ncomplement integer, is negative, zero, or positive.\nExample\nNOT\nR4, R2\n; R4  NOT(R2)'}"
"RET
Return from Subroutine
000
111
000000
5
11
1
15
1100
Operation
PC = R7;
Description
The PC is loaded with the value in R7. Its normal use is to cause a return from a
previous JSR(R) instruction.
Example
RET
; PC  R7
The RET instruction is a specic encoding of the JMP instruction. See also JMP.","{'page_number': 214, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'RET\nReturn from Subroutine\n000\n111\n000000\n5\n11\n1\n15\n1100\nOperation\nPC = R7;\nDescription\nThe PC is loaded with the value in R7. Its normal use is to cause a return from a\nprevious JSR(R) instruction.\nExample\nRET\n; PC  R7\nThe RET instruction is a specic encoding of the JMP instruction. See also JMP.'}"
"RTI
terrupt
15
12
11
0
000000000000
1000
Operation
if (PSR[15] == 1)
Initiate a privilege mode exception;
else
PC = mem[R6]; R6 is the SSP, PC is restored
R6 = R6+1;
TEMP = mem[R6];
R6 = R6+1; system stack completes POP before saved PSR is restored
PSR = TEMP; PSR is restored
if (PSR[15] == 1)
Saved SSP=R6 and R6=Saved USP;
Description
If the processor is running in User mode, a privilege mode exception occurs. If
in Supervisor mode, the top two elements on the system stack are popped and
loaded into PC, PSR. After PSR is restored, if the processor is running in User
mode, the SSP is saved in Saved SSP, and R6 is loaded with Saved USP.
Example
RTI
; PC, PSR  top two values popped o stack.
Note
RTI is the last instruction in both interrupt and trap service routines and returns
control to the program that was running. In both cases, the relevant service routine
is initiated by first pushing the PSR and PC of the program that is running onto the
system stack. Then the starting address of the appropriate service routine is loaded
into the PC, and the service routine executes with supervisor privilege. The last
instruction in the service routine is RTI, which returns control to the interrupted
program by popping two values off the supervisor stack to restore the PC and PSR.
In the case of an interrupt, the PC is restored to the address of the instruction that was
about to be processed when the interrupt was initiated. In the case of an exception,
the PC is restored to either the address of the instruction that caused the exception or
the address of the following instruction, depending on whether the instruction that
caused the exception is to be re-executed. In the case of a TRAP service routine,
the PC is restored to the instruction following the TRAP instruction in the calling
routine. In the case of an interrupt or TRAP, the PSR is restored to the value it had
when the interrupt was initiated. In the case of an exception, the PSR is restored to
the value it had when the exception occurred or to some modified value, depending
on the exception. See also Section A.3.","{'page_number': 215, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'RTI\nterrupt\n15\n12\n11\n0\n000000000000\n1000\nOperation\nif (PSR[15] == 1)\nInitiate a privilege mode exception;\nelse\nPC = mem[R6]; R6 is the SSP, PC is restored\nR6 = R6+1;\nTEMP = mem[R6];\nR6 = R6+1; system stack completes POP before saved PSR is restored\nPSR = TEMP; PSR is restored\nif (PSR[15] == 1)\nSaved SSP=R6 and R6=Saved USP;\nDescription\nIf the processor is running in User mode, a privilege mode exception occurs. If\nin Supervisor mode, the top two elements on the system stack are popped and\nloaded into PC, PSR. After PSR is restored, if the processor is running in User\nmode, the SSP is saved in Saved SSP, and R6 is loaded with Saved USP.\nExample\nRTI\n; PC, PSR  top two values popped o stack.\nNote\nRTI is the last instruction in both interrupt and trap service routines and returns\ncontrol to the program that was running. In both cases, the relevant service routine\nis initiated by first pushing the PSR and PC of the program that is running onto the\nsystem stack. Then the starting address of the appropriate service routine is loaded\ninto the PC, and the service routine executes with supervisor privilege. The last\ninstruction in the service routine is RTI, which returns control to the interrupted\nprogram by popping two values off the supervisor stack to restore the PC and PSR.\nIn the case of an interrupt, the PC is restored to the address of the instruction that was\nabout to be processed when the interrupt was initiated. In the case of an exception,\nthe PC is restored to either the address of the instruction that caused the exception or\nthe address of the following instruction, depending on whether the instruction that\ncaused the exception is to be re-executed. In the case of a TRAP service routine,\nthe PC is restored to the instruction following the TRAP instruction in the calling\nroutine. In the case of an interrupt or TRAP, the PSR is restored to the value it had\nwhen the interrupt was initiated. In the case of an exception, the PSR is restored to\nthe value it had when the exception occurred or to some modified value, depending\non the exception. See also Section A.3.'}"
"ST
Store
PCofset9
0011
SR
15
12
11
9
8
0
Operation
if (computed address is in privileged memory AND PSR[15] == 1)
Initiate ACV exception;
else
mem[PC + SEXT(PCoffset9)] = SR;
Description
If the computed address is to privileged memory and PSR[15]=1, initiate ACV
exception. If not, the contents of the register specied by SR is stored in the
memory location whose address is computed by sign-extending bits [8:0] to 16
bits and adding this value to the incremented PC.
Example
ST
R4, HERE
; mem[HERE]  R4
This is the incremented PC.","{'page_number': 216, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'ST\nStore\nPCofset9\n0011\nSR\n15\n12\n11\n9\n8\n0\nOperation\nif (computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nmem[PC + SEXT(PCoffset9)] = SR;\nDescription\nIf the computed address is to privileged memory and PSR[15]=1, initiate ACV\nexception. If not, the contents of the register specied by SR is stored in the\nmemory location whose address is computed by sign-extending bits [8:0] to 16\nbits and adding this value to the incremented PC.\nExample\nST\nR4, HERE\n; mem[HERE]  R4\nThis is the incremented PC.'}"
"STI
Store Indirect
PCofset9
1011
SR
15
12
11
9
8
0
Operation
if (either computed address is in privileged memory AND PSR[15] == 1)
Initiate ACV exception;
else
mem[mem[PC + SEXT(PCoffset9)]] = SR;
Description
If either computed address is to privileged memory and PSR[15]=1, initiate
ACV exception. If not, the contents of the register specied by SR is stored
in the memory location whose address is obtained as follows: Bits [8:0] are sign-
extended to 16 bits and added to the incremented PC. What is in memory at this
address is the address of the location to which the data in SR is stored.
Example
STI
R4, NOT HERE
; mem[mem[NOT HERE]]  R4
This is the incremented PC.","{'page_number': 217, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'STI\nStore Indirect\nPCofset9\n1011\nSR\n15\n12\n11\n9\n8\n0\nOperation\nif (either computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nmem[mem[PC + SEXT(PCoffset9)]] = SR;\nDescription\nIf either computed address is to privileged memory and PSR[15]=1, initiate\nACV exception. If not, the contents of the register specied by SR is stored\nin the memory location whose address is obtained as follows: Bits [8:0] are sign-\nextended to 16 bits and added to the incremented PC. What is in memory at this\naddress is the address of the location to which the data in SR is stored.\nExample\nSTI\nR4, NOT HERE\n; mem[mem[NOT HERE]]  R4\nThis is the incremented PC.'}"
"STR
Store Base+oset
15
12
11
9
8
6
5
0
BaseR
SR
0111
ofset6
Operation
if (computed address is in privileged memory AND PSR[15] == 1)
Initiate ACV exception;
else
mem[BaseR + SEXT(offset6)] = SR;
Description
If the computed address is to privileged memory and PSR[15]=1, initiate ACV
exception. If not, the contents of the register specied by SR is stored in the
memory location whose address is computed by sign-extending bits [5:0] to 16
bits and adding this value to the contents of the register specied by bits [8:6].
Example
STR
R4, R2, #5
; mem[R2+5]  R4","{'page_number': 218, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'STR\nStore Base+oset\n15\n12\n11\n9\n8\n6\n5\n0\nBaseR\nSR\n0111\nofset6\nOperation\nif (computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nmem[BaseR + SEXT(offset6)] = SR;\nDescription\nIf the computed address is to privileged memory and PSR[15]=1, initiate ACV\nexception. If not, the contents of the register specied by SR is stored in the\nmemory location whose address is computed by sign-extending bits [5:0] to 16\nbits and adding this value to the contents of the register specied by bits [8:6].\nExample\nSTR\nR4, R2, #5\n; mem[R2+5]  R4'}"
"TRAP
System Call
As
En
0
7
8
11
12
15
1111
0000
trapvect8
Operation
TEMP=PSR;
if (PSR[15] == 1)
Saved USP=R6 and R6=Saved SSP;
PSR[15]=0;
Push TEMP,PC on the system stack
PC = mem[ZEXT(trapvect8)];
Description
If the the program is executing in User mode, the User Stack Pointer must be
saved and the System Stack Pointer loaded. Then the PSR and PC are pushed
on the system stack. (This enables a return to the instruction physically follow-
ing the TRAP instruction in the original program after the last instruction in the
service routine (RTI) has completed execution.) Then the PC is loaded with the
starting address of the system call specied by trapvector8. The starting address
is contained in the memory location whose address is obtained by zero-extending
trapvector8 to 16 bits.
Example
TRAP
x23
; Directs the operating system to execute the IN system call.
; The starting address of this system call is contained in
; memory location x0023.
Note:
Memory locations x0000 through x00FF, 256 in all, are available to contain
starting addresses for system calls specied by their corresponding trap vectors.
This region of memory is called the Trap Vector Table. Table A.3 describes the
functions performed by the service routines corresponding to trap vectors x20
to x25.
This is the incremented PC.","{'page_number': 219, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'TRAP\nSystem Call\nAs\nEn\n0\n7\n8\n11\n12\n15\n1111\n0000\ntrapvect8\nOperation\nTEMP=PSR;\nif (PSR[15] == 1)\nSaved USP=R6 and R6=Saved SSP;\nPSR[15]=0;\nPush TEMP,PC on the system stack\nPC = mem[ZEXT(trapvect8)];\nDescription\nIf the the program is executing in User mode, the User Stack Pointer must be\nsaved and the System Stack Pointer loaded. Then the PSR and PC are pushed\non the system stack. (This enables a return to the instruction physically follow-\ning the TRAP instruction in the original program after the last instruction in the\nservice routine (RTI) has completed execution.) Then the PC is loaded with the\nstarting address of the system call specied by trapvector8. The starting address\nis contained in the memory location whose address is obtained by zero-extending\ntrapvector8 to 16 bits.\nExample\nTRAP\nx23\n; Directs the operating system to execute the IN system call.\n; The starting address of this system call is contained in\n; memory location x0023.\nNote:\nMemory locations x0000 through x00FF, 256 in all, are available to contain\nstarting addresses for system calls specied by their corresponding trap vectors.\nThis region of memory is called the Trap Vector Table. Table A.3 describes the\nfunctions performed by the service routines corresponding to trap vectors x20\nto x25.\nThis is the incremented PC.'}"
"Unused Opcode
1101
Operation
Initiate an illegal opcode exception.
Description
If an illegal opcode is encountered, an illegal opcode exception occurs.
Note:
The opcode 1101 has been reserved for future use. It is currently not dened. If
the instruction currently executing has bits [15:12] = 1101, an illegal opcode
exception occurs. Section A.3 describes what happens.","{'page_number': 220, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Unused Opcode\n1101\nOperation\nInitiate an illegal opcode exception.\nDescription\nIf an illegal opcode is encountered, an illegal opcode exception occurs.\nNote:\nThe opcode 1101 has been reserved for future use. It is currently not dened. If\nthe instruction currently executing has bits [15:12] = 1101, an illegal opcode\nexception occurs. Section A.3 describes what happens.'}"
"Table A.3
Trap Service Routines
Trap Vector
Assembler Name
Description
x20
GETC
Read a single character from the keyboard. The character is not echoed onto the
console. Its ASCII code is copied into R0. The high eight bits of R0 are cleared.
x21
OUT
Write a character in R0[7:0] to the console display.
x22
PUTS
Write a string of ASCII characters to the console display. The characters are
contained in consecutive memory locations, one character per memory location,
starting with the address specied in R0. Writing terminates with the occurrence of
x0000 in a memory location.
x23
IN
Print a prompt on the screen and read a single character from the keyboard. The
character is echoed onto the console monitor, and its ASCII code is copied into
R0. The high eight bits of R0 are cleared.
x24
PUTSP
Write a string of ASCII characters to the console. The characters are contained in
consecutive memory locations, two characters per memory location, starting with
the address specied in R0. The ASCII code contained in bits [7:0] of a memory
location is written to the console rst. Then the ASCII code contained in bits [15:8]
of that memory location is written to the console. (A character string consisting of
an odd number of characters to be written will have x00 in bits [15:8] of the
memory location containing the last character to be written.) Writing terminates
with the occurrence of x0000 in a memory location.
x25
HALT
Halt execution and print a message on the console.
A.3 Interrupt and Exception
Processing
As has been discussed in detail in Chapter 9, events external to the program that
is running can interrupt the processor. A common example of an external event
is interrupt-driven I/O. It is also the case that the processor can be interrupted
by exceptional events that occur while the program is running that are caused by
the program itself. An example of such an internal event is the presence of an
unused opcode in the computer program that is running.
Associated with each event that can interrupt the processor is an eight-bit
vector that provides an entry point into a 256-entry interrupt vector table. The
starting address of the interrupt vector table is x0100. That is, the interrupt vector
table occupies memory locations x0100 to x01FF. Each entry in the interrupt
vector table contains the starting address of the service routine that handles the
needs of the corresponding event. These service routines execute in Supervisor
mode.
Half (128) of these entries, locations x0100 to x017F, provide the starting
addresses of routines that service events caused by the running program itself.
These routines are called exception service routines because they handle excep-
tional events, that is, events that prevent the program from executing normally.
The other half of the entries, locations x0180 to x01FF, provide the starting
addresses of routines that service events that are external to the program that
is running, such as requests from I/O devices. These routines are called interrupt
service routines.","{'page_number': 221, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Table A.3\nTrap Service Routines\nTrap Vector\nAssembler Name\nDescription\nx20\nGETC\nRead a single character from the keyboard. The character is not echoed onto the\nconsole. Its ASCII code is copied into R0. The high eight bits of R0 are cleared.\nx21\nOUT\nWrite a character in R0[7:0] to the console display.\nx22\nPUTS\nWrite a string of ASCII characters to the console display. The characters are\ncontained in consecutive memory locations, one character per memory location,\nstarting with the address specied in R0. Writing terminates with the occurrence of\nx0000 in a memory location.\nx23\nIN\nPrint a prompt on the screen and read a single character from the keyboard. The\ncharacter is echoed onto the console monitor, and its ASCII code is copied into\nR0. The high eight bits of R0 are cleared.\nx24\nPUTSP\nWrite a string of ASCII characters to the console. The characters are contained in\nconsecutive memory locations, two characters per memory location, starting with\nthe address specied in R0. The ASCII code contained in bits [7:0] of a memory\nlocation is written to the console rst. Then the ASCII code contained in bits [15:8]\nof that memory location is written to the console. (A character string consisting of\nan odd number of characters to be written will have x00 in bits [15:8] of the\nmemory location containing the last character to be written.) Writing terminates\nwith the occurrence of x0000 in a memory location.\nx25\nHALT\nHalt execution and print a message on the console.\nA.3 Interrupt and Exception\nProcessing\nAs has been discussed in detail in Chapter 9, events external to the program that\nis running can interrupt the processor. A common example of an external event\nis interrupt-driven I/O. It is also the case that the processor can be interrupted\nby exceptional events that occur while the program is running that are caused by\nthe program itself. An example of such an internal event is the presence of an\nunused opcode in the computer program that is running.\nAssociated with each event that can interrupt the processor is an eight-bit\nvector that provides an entry point into a 256-entry interrupt vector table. The\nstarting address of the interrupt vector table is x0100. That is, the interrupt vector\ntable occupies memory locations x0100 to x01FF. Each entry in the interrupt\nvector table contains the starting address of the service routine that handles the\nneeds of the corresponding event. These service routines execute in Supervisor\nmode.\nHalf (128) of these entries, locations x0100 to x017F, provide the starting\naddresses of routines that service events caused by the running program itself.\nThese routines are called exception service routines because they handle excep-\ntional events, that is, events that prevent the program from executing normally.\nThe other half of the entries, locations x0180 to x01FF, provide the starting\naddresses of routines that service events that are external to the program that\nis running, such as requests from I/O devices. These routines are called interrupt\nservice routines.'}"
"A.3.1 Interrupts
At this time, an LC-3 computer system provides only one I/O device that can
interrupt the processor. That device is the keyboard. It interrupts at priority level
PL4 and supplies the interrupt vector x80.
An I/O device can interrupt the processor if it wants service, if its interrupt
enable (IE) bit is set, and if the priority of its request is greater than the priority of
any other event that wants to interrupt and greater than the priority of the program
that is running.
Assume a program is running at a priority level less than 4, and someone
strikes a key on the keyboard. If the IE bit of the KBSR is 1, the currently execut-
ing program is interrupted at the end of the current instruction cycle. The interrupt
service routine is initiated as follows:
1. The PSR of the interrupted process is saved in TEMP.
2. The processor sets the privilege mode to Supervisor mode (PSR[15]=0).
3. The processor sets the priority level to PL4, the priority level of the
interrupting device (PSR[10:8]=100).
4. If the interrupted process is in User mode, R6 is saved in Saved USP and
R6 is loaded with the Supervisor Stack Pointer (SSP).
5. TEMP and the PC of the interrupted process are pushed onto the supervisor
stack.
6. The keyboard supplies its eight-bit interrupt vector, in this case x80.
7. The processor expands that vector to x0180, the corresponding 16-bit
address in the interrupt vector table.
8. The PC is loaded with the contents of memory location x0180, the address
of the rst instruction in the keyboard interrupt service routine.
The processor then begins execution of the interrupt service routine.
The last instruction executed in an interrupt service routine is RTI. The top
two elements of the supervisor stack are popped and loaded into the PC and PSR
registers. R6 is loaded with the appropriate stack pointer, depending on the new value
of PSR[15]. Processing then continues where the interrupted program left off.
A.3.2 Exceptions
At this time, the LC-3 ISA species three exception conditions: privilege mode
violation, illegal opcode, and access control violation (ACV). The privilege mode
violation occurs if the processor attempts to execute the RTI instruction while
running in User mode. The illegal opcode exception occurs if the processor
attempts to execute an instruction having the unused opcode (bits [15:12] =
1101). The ACV exception occurs if the processor attempts to access privileged
memory (i.e., a memory location in system space or a device register having an
address from xFE00 to xFFFF while running in User mode).
Exceptions are handled as soon as they are detected. They are initiated very
much like interrupts are initiated, that is:
1. The PSR of the process causing the exception is saved in TEMP.","{'page_number': 222, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'A.3.1 Interrupts\nAt this time, an LC-3 computer system provides only one I/O device that can\ninterrupt the processor. That device is the keyboard. It interrupts at priority level\nPL4 and supplies the interrupt vector x80.\nAn I/O device can interrupt the processor if it wants service, if its interrupt\nenable (IE) bit is set, and if the priority of its request is greater than the priority of\nany other event that wants to interrupt and greater than the priority of the program\nthat is running.\nAssume a program is running at a priority level less than 4, and someone\nstrikes a key on the keyboard. If the IE bit of the KBSR is 1, the currently execut-\ning program is interrupted at the end of the current instruction cycle. The interrupt\nservice routine is initiated as follows:\n1. The PSR of the interrupted process is saved in TEMP.\n2. The processor sets the privilege mode to Supervisor mode (PSR[15]=0).\n3. The processor sets the priority level to PL4, the priority level of the\ninterrupting device (PSR[10:8]=100).\n4. If the interrupted process is in User mode, R6 is saved in Saved USP and\nR6 is loaded with the Supervisor Stack Pointer (SSP).\n5. TEMP and the PC of the interrupted process are pushed onto the supervisor\nstack.\n6. The keyboard supplies its eight-bit interrupt vector, in this case x80.\n7. The processor expands that vector to x0180, the corresponding 16-bit\naddress in the interrupt vector table.\n8. The PC is loaded with the contents of memory location x0180, the address\nof the rst instruction in the keyboard interrupt service routine.\nThe processor then begins execution of the interrupt service routine.\nThe last instruction executed in an interrupt service routine is RTI. The top\ntwo elements of the supervisor stack are popped and loaded into the PC and PSR\nregisters. R6 is loaded with the appropriate stack pointer, depending on the new value\nof PSR[15]. Processing then continues where the interrupted program left off.\nA.3.2 Exceptions\nAt this time, the LC-3 ISA species three exception conditions: privilege mode\nviolation, illegal opcode, and access control violation (ACV). The privilege mode\nviolation occurs if the processor attempts to execute the RTI instruction while\nrunning in User mode. The illegal opcode exception occurs if the processor\nattempts to execute an instruction having the unused opcode (bits [15:12] =\n1101). The ACV exception occurs if the processor attempts to access privileged\nmemory (i.e., a memory location in system space or a device register having an\naddress from xFE00 to xFFFF while running in User mode).\nExceptions are handled as soon as they are detected. They are initiated very\nmuch like interrupts are initiated, that is:\n1. The PSR of the process causing the exception is saved in TEMP.'}"
"2. The processor sets the privilege mode to Supervisor mode (PSR[15]=0).
3. If the process causing the exception is in User mode, R6 is saved in
Saved USP and R6 is loaded with the SSP.
4. TEMP and the PC of the process causing the exception are pushed onto the
supervisor stack.
5. The exception supplies its eight-bit vector. In the case of the privilege mode
violation, that vector is x00. In the case of the illegal opcode, that vector is
x01. In the case of the ACV exception, that vector is x02.
6. The processor expands that vector to x0100, x0101, or x0102, the
corresponding 16-bit address in the interrupt vector table.
7. The PC is loaded with the contents of memory location x0100, x0101, or
x0102, the address of the rst instruction in the corresponding exception
service routine.
The processor then begins execution of the exception service routine.
The details of the exception service routine depend on the exception and the
way in which the operating system wishes to handle that exception.
In many cases, the exception service routine can correct any problem caused
by the exceptional event and then continue processing the original program. In
those cases, the last instruction in the exception service routine is RTI, which pops
the top two elements from the supervisor stack and loads them into the PC and
PSR registers. The program then resumes execution with the problem corrected.
In some cases, the cause of the exceptional event is suciently catastrophic
that the exception service routine removes the program from further processing.
Another dierence between the handling of interrupts and the handling of
exceptions is the priority level of the processor during the execution of the service
routine. In the case of exceptions, we normally do not change the priority level
when we service the exception. The priority level of a program is the urgency
with which it needs to be executed. In the case of the exceptions specied by the
LC-3 ISA, the urgency of a program is not changed by the fact that a privilege
mode violation occurred or there was an illegal opcode in the program or the
program attempted to access privileged memory while it was in User mode.","{'page_number': 223, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '2. The processor sets the privilege mode to Supervisor mode (PSR[15]=0).\n3. If the process causing the exception is in User mode, R6 is saved in\nSaved USP and R6 is loaded with the SSP.\n4. TEMP and the PC of the process causing the exception are pushed onto the\nsupervisor stack.\n5. The exception supplies its eight-bit vector. In the case of the privilege mode\nviolation, that vector is x00. In the case of the illegal opcode, that vector is\nx01. In the case of the ACV exception, that vector is x02.\n6. The processor expands that vector to x0100, x0101, or x0102, the\ncorresponding 16-bit address in the interrupt vector table.\n7. The PC is loaded with the contents of memory location x0100, x0101, or\nx0102, the address of the rst instruction in the corresponding exception\nservice routine.\nThe processor then begins execution of the exception service routine.\nThe details of the exception service routine depend on the exception and the\nway in which the operating system wishes to handle that exception.\nIn many cases, the exception service routine can correct any problem caused\nby the exceptional event and then continue processing the original program. In\nthose cases, the last instruction in the exception service routine is RTI, which pops\nthe top two elements from the supervisor stack and loads them into the PC and\nPSR registers. The program then resumes execution with the problem corrected.\nIn some cases, the cause of the exceptional event is suciently catastrophic\nthat the exception service routine removes the program from further processing.\nAnother dierence between the handling of interrupts and the handling of\nexceptions is the priority level of the processor during the execution of the service\nroutine. In the case of exceptions, we normally do not change the priority level\nwhen we service the exception. The priority level of a program is the urgency\nwith which it needs to be executed. In the case of the exceptions specied by the\nLC-3 ISA, the urgency of a program is not changed by the fact that a privilege\nmode violation occurred or there was an illegal opcode in the program or the\nprogram attempted to access privileged memory while it was in User mode.'}"
"LC-3 to x86
A
s you know, the ISA of the LC-3 explicitly species the interface between
what the LC-3 machine language programmer or LC-3 compilers produce
and what a microarchitecture of the LC-3 can accept and process. Among those
things specied are the address space and addressability of memory, the number
and size of the registers, the format of the instructions, the opcodes, the data types
that are the encodings used to represent information, and the addressing modes
that are available for determining the location of an operand.
The ISA of the microprocessor in your PC also species an interface between
the compilers and the microarchitecture. However, in the case of the PC, the ISA
is not the LC-3. Rather it is the x86. Intel introduced the rst member of this ISA
in 1979. It was called the 8086, and the normal size of the addresses and data
elements it processed was 16 bits, the same size as the LC-3. Today, the typical
size of addresses and data is 64 bits. With special vector extensions, instructions
can operate on vectors that can be of size 128, 256, and 512 bits. Because there
are a lot of old programs and data expressed in 32 bits, the x86 is able to process
instructions in what we call 64-bit mode or 32-bit mode. That is, in 32-bit mode,
the x86 restricts itself to a 32-bit address space and 32-bit elements.
From the 8086 to the present time, Intel has continued implementations
of the x86 ISA, among them the 386 (in 1985), 486 (in 1989), Pentium (in
1992), Pentium Pro (in 1995), Pentium II (in 1997), Pentium IV (in 2001), 1st
Generation Core i7-9xx Series, codename Nehalem (in 2008), 4th Generation
Core i7-4xxx Series, codename Haswell (in 2013), and 8th Generation Core
i7-8086K, codename: Coee Lake (in 2018).
The ISA of the x86 is much more complicated than that of the LC-3. There
are more opcodes, more data types, more addressing modes, a more complicated
memory structure, and a more complicated encoding of instructions into 0s and
1s. However, fundamentally, they have the same basic ingredients.
You have spent a good deal of time understanding computing within the con-
text of the LC-3. Some may feel that it would be good to learn about a real ISA.
One way to do that would be to have some company such as Intel mass-produce
LC-3 microprocessors, some other company like Dell put them in their PCs, and a
third company such as Microsoft compile Windows NT into the ISA of the LC-3.
An easier way to introduce you to a real ISA is by way of this appendix.","{'page_number': 224, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'LC-3 to x86\nA\ns you know, the ISA of the LC-3 explicitly species the interface between\nwhat the LC-3 machine language programmer or LC-3 compilers produce\nand what a microarchitecture of the LC-3 can accept and process. Among those\nthings specied are the address space and addressability of memory, the number\nand size of the registers, the format of the instructions, the opcodes, the data types\nthat are the encodings used to represent information, and the addressing modes\nthat are available for determining the location of an operand.\nThe ISA of the microprocessor in your PC also species an interface between\nthe compilers and the microarchitecture. However, in the case of the PC, the ISA\nis not the LC-3. Rather it is the x86. Intel introduced the rst member of this ISA\nin 1979. It was called the 8086, and the normal size of the addresses and data\nelements it processed was 16 bits, the same size as the LC-3. Today, the typical\nsize of addresses and data is 64 bits. With special vector extensions, instructions\ncan operate on vectors that can be of size 128, 256, and 512 bits. Because there\nare a lot of old programs and data expressed in 32 bits, the x86 is able to process\ninstructions in what we call 64-bit mode or 32-bit mode. That is, in 32-bit mode,\nthe x86 restricts itself to a 32-bit address space and 32-bit elements.\nFrom the 8086 to the present time, Intel has continued implementations\nof the x86 ISA, among them the 386 (in 1985), 486 (in 1989), Pentium (in\n1992), Pentium Pro (in 1995), Pentium II (in 1997), Pentium IV (in 2001), 1st\nGeneration Core i7-9xx Series, codename Nehalem (in 2008), 4th Generation\nCore i7-4xxx Series, codename Haswell (in 2013), and 8th Generation Core\ni7-8086K, codename: Coee Lake (in 2018).\nThe ISA of the x86 is much more complicated than that of the LC-3. There\nare more opcodes, more data types, more addressing modes, a more complicated\nmemory structure, and a more complicated encoding of instructions into 0s and\n1s. However, fundamentally, they have the same basic ingredients.\nYou have spent a good deal of time understanding computing within the con-\ntext of the LC-3. Some may feel that it would be good to learn about a real ISA.\nOne way to do that would be to have some company such as Intel mass-produce\nLC-3 microprocessors, some other company like Dell put them in their PCs, and a\nthird company such as Microsoft compile Windows NT into the ISA of the LC-3.\nAn easier way to introduce you to a real ISA is by way of this appendix.'}"
"We present here elements of the x86, a very complicated ISA. We do so in
spite of its complexity because it is one of the most pervasive of all ISAs available
in the marketplace.
We make no attempt to provide a complete specication of the x86 ISA.
That would require a whole book by itself, and to appreciate it, a deeper under-
standing of operating systems, compilers, and computer systems than we think
is reasonable at this point in your education. If one wants a complete treat-
ment, we recommend the Intel Architecture Software Developers Manual. In this
appendix, we restrict ourselves to some of the characteristics that are relevant to
application programs. Our intent is to give you a sense of the richness of the x86
ISA. We introduce these characteristics within the context of the LC-3 ISA, which
at this point you are very familiar with.
B.1 LC-3 Features and
Corresponding x86 Features
B.1.1 Instruction Set
An instruction set is made up of instructions, each of which has an opcode and
zero or more operands. The number of operands depends on how many are needed
by the corresponding opcode. Each operand is a data element and is encoded
according to its data type. The location of an operand is determined by evaluating
its addressing mode.
The LC-3 instruction set contains one data type, 15 opcodes, and three
addressing modes: PC-relative (LD, ST), indirect (LDI, STI), and register-plus-
oset (LDR, STR). The x86 instruction set has more than a dozen data types,
more than a thousand opcodes, and more than two dozen addressing modes
(depending on how you count).
B.1.1.1 Data Types
Recall that a data type is a representation of information such that the ISA pro-
vides opcodes that operate on information that is encoded in that representation.
The LC-3 supports only one data type, 16-bit 2s-complement integers. This
is not enough for ecient processing in the real world. Scientic applications
need numbers that are represented by the oating point data type. Multimedia
applications require information that is represented by a dierent data type. Com-
mercial applications written years ago, but still active today, require an additional
data type, referred to as packed decimal. Some applications require a greater
range of values and a greater precision of each value than other applications.
As a result of all these requirements, the x86 is designed with instructions
that operate on (for example) 8-bit integers, 16-bit integers, and 32-bit integers,
32-bit and 64-bit oating point numbers, 64-bit, 128-bit, 256-bit, and 512-bit-
multimedia values. Figure B.1 shows some of the data types present in the
x86 ISA.","{'page_number': 225, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'We present here elements of the x86, a very complicated ISA. We do so in\nspite of its complexity because it is one of the most pervasive of all ISAs available\nin the marketplace.\nWe make no attempt to provide a complete specication of the x86 ISA.\nThat would require a whole book by itself, and to appreciate it, a deeper under-\nstanding of operating systems, compilers, and computer systems than we think\nis reasonable at this point in your education. If one wants a complete treat-\nment, we recommend the Intel Architecture Software Developers Manual. In this\nappendix, we restrict ourselves to some of the characteristics that are relevant to\napplication programs. Our intent is to give you a sense of the richness of the x86\nISA. We introduce these characteristics within the context of the LC-3 ISA, which\nat this point you are very familiar with.\nB.1 LC-3 Features and\nCorresponding x86 Features\nB.1.1 Instruction Set\nAn instruction set is made up of instructions, each of which has an opcode and\nzero or more operands. The number of operands depends on how many are needed\nby the corresponding opcode. Each operand is a data element and is encoded\naccording to its data type. The location of an operand is determined by evaluating\nits addressing mode.\nThe LC-3 instruction set contains one data type, 15 opcodes, and three\naddressing modes: PC-relative (LD, ST), indirect (LDI, STI), and register-plus-\noset (LDR, STR). The x86 instruction set has more than a dozen data types,\nmore than a thousand opcodes, and more than two dozen addressing modes\n(depending on how you count).\nB.1.1.1 Data Types\nRecall that a data type is a representation of information such that the ISA pro-\nvides opcodes that operate on information that is encoded in that representation.\nThe LC-3 supports only one data type, 16-bit 2s-complement integers. This\nis not enough for ecient processing in the real world. Scientic applications\nneed numbers that are represented by the oating point data type. Multimedia\napplications require information that is represented by a dierent data type. Com-\nmercial applications written years ago, but still active today, require an additional\ndata type, referred to as packed decimal. Some applications require a greater\nrange of values and a greater precision of each value than other applications.\nAs a result of all these requirements, the x86 is designed with instructions\nthat operate on (for example) 8-bit integers, 16-bit integers, and 32-bit integers,\n32-bit and 64-bit oating point numbers, 64-bit, 128-bit, 256-bit, and 512-bit-\nmultimedia values. Figure B.1 shows some of the data types present in the\nx86 ISA.'}"
"B.1.1.2 Opcodes
The LC-3 comprises 15 opcodes; the x86 instruction set comprises more than
a thousand. Recall that the three basic instruction types are operates, data
movement, and control. Operates process information, data movement opcodes
move information from one place to another (including input and output), and
control opcodes change the ow of the instruction stream.
In addition, we should add a fourth category to handle functions that must
be performed in the real world because a user program runs in the context of an
operating system that is controlling a computer system, rather than in isolation.
These instructions deal with computer security, system management, hardware
performance monitoring, and various other issues that are beyond what the typical
application program pays attention to. We will ignore those instructions in this
appendix, but please realize that they do exist, and you will see them as your
studies progress. Here we will concentrate on the three basic instruction types:
operates, data movement, and control.
Operates
The LC-3 has three operate instructions: ADD, AND, and NOT. The
ADD opcode is the only LC-3 opcode that performs arithmetic. If one wants to
subtract, one obtains the negative of an operand and then adds. If one wants
to multiply, one can write a program with a loop to ADD a number some specied
number of times. However, this is too time-consuming for a real microprocessor.
So the x86 has separate SUB and MUL, as well as DIV, INC (increment), DEC
(decrement), and ADC (add with carry), to name a few.
A useful feature of an ISA is to extend the size of the integers on which it can
operate. To do this, one writes a program to operate on such long integers. The
ADC opcode, which adds two operands plus the carry from the previous add, is
a very useful opcode for extending the size of integers.
In addition, the x86 has, for each data type, its own set of opcodes to operate
on that data type. For example, multimedia instructions (collectively called the
MMX instructions) often require saturating arithmetic, which is very dierent
from the arithmetic we are used to. PADDS is an opcode that adds two operands
with saturating arithmetic.
Saturating arithmetic can be explained as follows: Suppose we represent the
degree of grayness of an element in a gure with a digit from 0 to 9, where 0 is
white and 9 is black. Suppose we want to add some darkness to an existing value
of grayness of that gure. An element could start out with a grayness value of
7, and we might wish to add a 5 worth of darkness to it. In normal arithmetic,
7 + 5 is 2 (with a carry), which is lighter than either 7 or 5. Something is wrong!
With saturating arithmetic, when we reach 9, we stay therewe do not generate
a carry. So, for example, 7 + 5 = 9 and 9 + n = 9. Saturating arithmetic is a
dierent kind of arithmetic, and the x86 has opcodes (MMX instructions) that
perform this type of arithmetic","{'page_number': 226, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In normal arithmetic,\n7 + 5 is 2 (with a carry), which is lighter than either 7 or 5. Something is wrong!\nWith saturating arithmetic, when we reach 9, we stay therewe do not generate\na carry. So, for example, 7 + 5 = 9 and 9 + n = 9. Saturating arithmetic is a\ndierent kind of arithmetic, and the x86 has opcodes (MMX instructions) that\nperform this type of arithmetic.\nScientic applications require opcodes that operate on values represented\nin the oating point data type. FADD, FMUL, FSIN, FSQRT are examples of\noating point opcodes in the x86 ISA.\nThe AND and NOT opcodes are the only LC-3 opcodes that perform logical\nfunctions. One can construct any logical expression using these two opcodes.'}"
"In normal arithmetic,
7 + 5 is 2 (with a carry), which is lighter than either 7 or 5. Something is wrong!
With saturating arithmetic, when we reach 9, we stay therewe do not generate
a carry. So, for example, 7 + 5 = 9 and 9 + n = 9. Saturating arithmetic is a
dierent kind of arithmetic, and the x86 has opcodes (MMX instructions) that
perform this type of arithmetic.
Scientic applications require opcodes that operate on values represented
in the oating point data type. FADD, FMUL, FSIN, FSQRT are examples of
oating point opcodes in the x86 ISA.
The AND and NOT opcodes are the only LC-3 opcodes that perform logical
functions. One can construct any logical expression using these two opcodes.","{'page_number': 226, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In normal arithmetic,\n7 + 5 is 2 (with a carry), which is lighter than either 7 or 5. Something is wrong!\nWith saturating arithmetic, when we reach 9, we stay therewe do not generate\na carry. So, for example, 7 + 5 = 9 and 9 + n = 9. Saturating arithmetic is a\ndierent kind of arithmetic, and the x86 has opcodes (MMX instructions) that\nperform this type of arithmetic.\nScientic applications require opcodes that operate on values represented\nin the oating point data type. FADD, FMUL, FSIN, FSQRT are examples of\noating point opcodes in the x86 ISA.\nThe AND and NOT opcodes are the only LC-3 opcodes that perform logical\nfunctions. One can construct any logical expression using these two opcodes.'}"
"Table B.1
Operate Instructions, x86 ISA
Instruction
Explanation
ADC x, y
x, y, and the carry retained from the last relevant operation (in CF) are added and
the result stored in x.
MUL x
The value in EAX is multiplied by x, and the result is stored in the 64-bit register
formed by EDX, EAX.
SAR x
x is right shifted (arithmetic shift) n bits, and the result is stored in x. The value of n
can be 1, an immediate operand, or the count in the CL register.
XOR x, y
A bit-wise exclusive-OR is performed on x, y and the result is stored in x.
DAA
After adding two packed decimal numbers, AL contains two BCD values, which
may be incorrect due to propagation of the carry bit after 15, rather than after 9.
DAA corrects the two BCD digits in AL.
FSIN
The top of the stack (call it x) is popped. The sin(x) is computed and pushed onto
the stack.
FADD
The top two elements on the stack are popped, added, and their result pushed
onto the stack.
PANDN x, y
A bit-wise AND-NOT operation is performed on MMX values x, y, and the result is
stored in x.
PADDS x, y
Saturating addition is performed on packed MMX values x, y, and the result is
stored in x.
However, as is the case with arithmetic, this also is too time-consuming. The x86
has in addition separate OR, XOR, AND-NOT, and separate logical operators for
dierent data types.
Furthermore, the x86 has a number of other operate instructions that set and
clear registers, convert a value from one data type to another, shift or rotate the
bits of a data element, and so on. Table B.1 lists some of the operate opcodes in
the x86 instruction set.
Data Movement
The LC-3 has six data movement opcodes: LD, LDI, ST,
STI, LDR, and STR. They all copy information between memory (and memory-
mapped device registers) and the eight general purpose registers, R0 to R7.
Although the x86 does not have LDI or STI opcodes, it does have the other
four, and in addition to these, many other data movement opcodes. XCHG can
swap the contents of two locations. PUSHA pushes all eight general purpose
registers onto the stack. IN and OUT move data between input and output ports
and the processor. CMOVcc copies a value from one location to another only if a
previously computed condition is true. Table B.2 lists some of the data movement
opcodes in the x86 instruction set.
Control
The LC-3 has ve control opcodes: BR, JSR/JSRR, JMP, RTI, and
TRAP. x86 has all these and more. Table B.3 lists some of the control opcodes in
the x86 instruction set.
B.1.1.3 Two Address vs. Three Address
The LC-3 is a three-address ISA","{'page_number': 227, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Table B.2 lists some of the data movement\nopcodes in the x86 instruction set.\nControl\nThe LC-3 has ve control opcodes: BR, JSR/JSRR, JMP, RTI, and\nTRAP. x86 has all these and more. Table B.3 lists some of the control opcodes in\nthe x86 instruction set.\nB.1.1.3 Two Address vs. Three Address\nThe LC-3 is a three-address ISA. This description reects the number of operands\nexplicitly specied by the ADD instruction. An add operation requires two\nsource operands (the numbers to be added) and one destination operand to store'}"
"Table B.2 lists some of the data movement
opcodes in the x86 instruction set.
Control
The LC-3 has ve control opcodes: BR, JSR/JSRR, JMP, RTI, and
TRAP. x86 has all these and more. Table B.3 lists some of the control opcodes in
the x86 instruction set.
B.1.1.3 Two Address vs. Three Address
The LC-3 is a three-address ISA. This description reects the number of operands
explicitly specied by the ADD instruction. An add operation requires two
source operands (the numbers to be added) and one destination operand to store","{'page_number': 227, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Table B.2 lists some of the data movement\nopcodes in the x86 instruction set.\nControl\nThe LC-3 has ve control opcodes: BR, JSR/JSRR, JMP, RTI, and\nTRAP. x86 has all these and more. Table B.3 lists some of the control opcodes in\nthe x86 instruction set.\nB.1.1.3 Two Address vs. Three Address\nThe LC-3 is a three-address ISA. This description reects the number of operands\nexplicitly specied by the ADD instruction. An add operation requires two\nsource operands (the numbers to be added) and one destination operand to store'}"
"Table B.2
Data Movement Instructions, x86 ISA
Instruction
Explanation
MOV x, y
The value stored in y is copied into x.
XCHG x, y
The values stored in x and y are swapped.
PUSHA
All the registers are pushed onto the top of the stack.
PUSH
Push a register onto the top of the stack.
POP
Pop a register from the top of the stack.
MOVS
The element in the DS segment pointed to by ESI is copied into the location in
the ES segment pointed to by EDI. After the copy has been performed, ESI and
EDI are both incremented.
REP MOVS
Perform the MOVS. Then decrement ECX. Repeat this instruction until ECX = 0.
(This allows a string to be copied in a single instruction, after initializing ECX.)
LODS
The element in the DS segment pointed to by ESI is loaded into EAX, and ESI is
incremented or decremented, according to the value of the DF ag.
INS
Data from the I/O port specied by the DX register is loaded into the EAX
register (or AX or AL, if the size of the data is 16 bits or 8 bits, respectively).
CMOVZ x, y
If ZF = 1, the value stored in y is copied into x. If ZF = 0, the instruction acts like
a no-op.
LEA x, y
The address y is stored in x. This is very much like the LC-3 instruction of the
same name.
Table B.3
Control Instructions, x86 ISA
Instruction
Explanation
Jcond x
Branch based on the condition specied by cond. If cond is true, the IP is loaded
with x.
JMP x
IP is loaded with the address x. This is very much like the LC-3 instruction of the
same name.
CALL x
The IP is pushed onto the stack, and a new IP is loaded with x.
RET
The stack is popped, and the value popped is loaded into IP.
LOOP x
ECX is decremented. If ECX is not 0 and ZF = 1, the IP is loaded with x.
INT n
The value n is an index into a table of descriptors that specify operating system
service routines. The end result of this instruction is that IP is loaded with the
starting result of the corresponding service routine. This is very much like the
TRAP instruction in the LC-3.
the result. In the LC-3, all three must be specied explicitly, hence the name
three-address ISA.
Even if the same location is to be used both for one of the sources and for the
destination, the three addresses are all specied. For example, the LC-3
ADD R1,R1,R2
identies R1 as both a source and the destination.
The x86 is mostly (except for special instructions dened as SSE or AVX
instructions) a two-address ISA. Since the add operation needs three operands, the
location of one of the sources must also be used to store the result","{'page_number': 228, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'For example, the LC-3\nADD R1,R1,R2\nidenties R1 as both a source and the destination.\nThe x86 is mostly (except for special instructions dened as SSE or AVX\ninstructions) a two-address ISA. Since the add operation needs three operands, the\nlocation of one of the sources must also be used to store the result. For example,\nthe corresponding 16-bit ADD instruction in the x86 ISA would be\nADD AX,BX\nwhere AX and BX are names of two of the x86s eight 16-bit general purpose\nregisters. AX and BX are the sources, and AX is the destination.'}"
"For example, the LC-3
ADD R1,R1,R2
identies R1 as both a source and the destination.
The x86 is mostly (except for special instructions dened as SSE or AVX
instructions) a two-address ISA. Since the add operation needs three operands, the
location of one of the sources must also be used to store the result. For example,
the corresponding 16-bit ADD instruction in the x86 ISA would be
ADD AX,BX
where AX and BX are names of two of the x86s eight 16-bit general purpose
registers. AX and BX are the sources, and AX is the destination.","{'page_number': 228, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'For example, the LC-3\nADD R1,R1,R2\nidenties R1 as both a source and the destination.\nThe x86 is mostly (except for special instructions dened as SSE or AVX\ninstructions) a two-address ISA. Since the add operation needs three operands, the\nlocation of one of the sources must also be used to store the result. For example,\nthe corresponding 16-bit ADD instruction in the x86 ISA would be\nADD AX,BX\nwhere AX and BX are names of two of the x86s eight 16-bit general purpose\nregisters. AX and BX are the sources, and AX is the destination.'}"
"Since the result of the operate is stored in the location that originally con-
tained one of the sources, that source operand is no longer available after that
instruction is executed. If that source operand is needed later, it must be saved
before the operate instruction is executed.
B.1.1.4 Memory Operands
A major dierence between the LC-3 instruction set and the x86 instruction set
is the restriction on where operate instructions can get their operands. An LC-3
operate instruction must obtain its source operands from registers and write the
result to a destination register. An x86 instruction, on the other hand, can obtain
one of its sources from memory and/or write its result to memory. In other words,
the x86 can read a value from memory, operate on that value, and store the result
in memory all in a single instruction. The LC-3 cannot.
The LC-3 program requires a separate load instruction to read the value from
memory before operating on it, and a separate store instruction to write the result
in memory after the operate instruction. An ISA, like the LC-3, that has this
restriction is called a load-store ISA. The x86 is not a load-store ISA.
B.1.2 Memory
The LC-3 memory consists of 216 locations, each containing 16 bits of informa-
tion. We say the LC-3 has a 16-bit address space, since one can uniquely address
its 216 locations with 16 bits of address. We say the LC-3 has an addressability
of 16 bits, since each memory location contains 16 bits of information.
The x86 memory has a 64-bit address space and an addressability of eight
bits. Since one byte contains eight bits, we say the x86 memory is byte address-
able. Since each location contains only eight bits, four contiguous locations in
memory are needed to store a 32-bit data element, say locations X, X+1, X+2,
and X+3. We designate X as the address of the 32-bit data element. In actuality,
X only contains bits [7:0], X+1 contains bits [15:8], X+2 contains bits [23:16],
and X+3 contains bits [31:24] of the 32-bit value.
One can determine an LC-3 memory location by simply obtaining its address
from the instruction, using one of the three addressing modes available in the
instruction set. An x86 instruction has available to it more than two dozen
addressing modes that it can use to specify the memory address of an operand.
We examine the addressing modes of an x86 instruction in Section B.2.
In addition to the larger number of addressing modes, the x86 contains a
mechanism called segmentation that provides a measure of protection against
unwanted accesses to particular memory addresses. The address produced by an
instructions addressing mode, rather than being an address in its own right, is
used as an address within a segment of memory. Access to that memory location
must take into account the segment register that controls access to that segment.
The details of how the protection mechanism works will have to wait for later in
your studies.
However, Figure B","{'page_number': 229, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The address produced by an\ninstructions addressing mode, rather than being an address in its own right, is\nused as an address within a segment of memory. Access to that memory location\nmust take into account the segment register that controls access to that segment.\nThe details of how the protection mechanism works will have to wait for later in\nyour studies.\nHowever, Figure B.2 does show how an address is calculated for the\nregister+oset addressing mode, both for the LC-3 and for the x86, with'}"
"The address produced by an
instructions addressing mode, rather than being an address in its own right, is
used as an address within a segment of memory. Access to that memory location
must take into account the segment register that controls access to that segment.
The details of how the protection mechanism works will have to wait for later in
your studies.
However, Figure B.2 does show how an address is calculated for the
register+oset addressing mode, both for the LC-3 and for the x86, with","{'page_number': 229, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The address produced by an\ninstructions addressing mode, rather than being an address in its own right, is\nused as an address within a segment of memory. Access to that memory location\nmust take into account the segment register that controls access to that segment.\nThe details of how the protection mechanism works will have to wait for later in\nyour studies.\nHowever, Figure B.2 does show how an address is calculated for the\nregister+oset addressing mode, both for the LC-3 and for the x86, with'}"
"segmentation. In both cases, the opcode is to move data from memory to a gen-
eral purpose register. The LC-3 uses the LDR instruction. The x86 uses the
MOV instruction. In the case of the x86, the address calculated is in the DS
segment, which is accessed via the DS register. That access is done through a 16-
bit selector, which indexes into a segment descriptor table, yielding the segment
descriptor for that segment. The segment descriptor contains a segment base reg-
ister, a segment limit register, and protection information. The memory address
obtained from the addressing mode of the instruction is added to the segment
base register to provide the actual memory address, as shown in Figure B.2.
B.1.3 Internal State
The internal state of the LC-3 consists of eight 16-bit general purpose registers,
R0 to R7, a 16-bit PC, and a 16-bit PSR that species the privilege mode, priority,
and three 1-bit condition codes (N, Z, and P). The user-visible internal state of
the x86 consists of 64-bit application-visible registers, a 64-bit Instruction pointer
(RIP), a 64-bit RFLAGS register, and the 16-bit segment registers.
B.1.3.1 Application-Visible Registers
Figure B.3 shows some of the application-visible registers in the x86 ISA.
In 64-bit mode, the x86 has 16 general purpose registers: RAX, RBX, RCX,
RDX, RSP, RBP, RCI, RDI, and R8 through R15. Each register contains 64 bits
reecting the normal size of operands. In 32-bit mode, there are eight general
purpose registers: EAX, EBX, ECX, EDX, ESP, EBP, ECI, and EDI, which use
bits [31:0] of the corresponding 64-bit registers. Also, since some x86 opcodes
process 16-bit and 8-bit operands, x86 also species 16-bit registers AX, BX,
...DI by using bits [15:0] of the 64-bit registers, and 8 bit registers AL, BL,
CL, and DL using bits [7:0] and AH, BH, CH, and DH, using bits [15:8] of
the corresponding 64-bit registers. The x86 also provides 128-bit, 256-bit, and
512-bit SIMD registers for operands needed by SSE and AVX operations. They
are, respectively, XMM0 to XMM31 for 128 bits, YMM0 to YMM31 for 256
bits, and ZMM0 to ZMM31 for 512 bits.
B.1.3.2 System Registers
The LC-3 has two system-level registersthe PC and the PSR. The user-visible
x86 has these and more. Figure B.4 shows some of the user-visible system
registers in the x86 ISA.
Instruction Pointer (RIP)
The x86 has the equivalent of the LC-3s 16-bit pro-
gram counter. The x86 calls it an instruction pointer (RIP)","{'page_number': 230, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'B.1.3.2 System Registers\nThe LC-3 has two system-level registersthe PC and the PSR. The user-visible\nx86 has these and more. Figure B.4 shows some of the user-visible system\nregisters in the x86 ISA.\nInstruction Pointer (RIP)\nThe x86 has the equivalent of the LC-3s 16-bit pro-\ngram counter. The x86 calls it an instruction pointer (RIP). Since the address\nspace of the x86 is 64 bits, the RIP is a 64-bit register. In 32-bit mode, since\nthe address space is only 32 bits, the instruction pointer (EIP) uses bits [31:0] of\nthe RIP.\nRFLAGS Register\nCorresponding to the LC-3s N, Z, and P condition codes,\nthe x86 has a one-bit SF (sign ag) register and a one-bit ZF (zero ag) register.'}"
"B.1.3.2 System Registers
The LC-3 has two system-level registersthe PC and the PSR. The user-visible
x86 has these and more. Figure B.4 shows some of the user-visible system
registers in the x86 ISA.
Instruction Pointer (RIP)
The x86 has the equivalent of the LC-3s 16-bit pro-
gram counter. The x86 calls it an instruction pointer (RIP). Since the address
space of the x86 is 64 bits, the RIP is a 64-bit register. In 32-bit mode, since
the address space is only 32 bits, the instruction pointer (EIP) uses bits [31:0] of
the RIP.
RFLAGS Register
Corresponding to the LC-3s N, Z, and P condition codes,
the x86 has a one-bit SF (sign ag) register and a one-bit ZF (zero ag) register.","{'page_number': 230, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'B.1.3.2 System Registers\nThe LC-3 has two system-level registersthe PC and the PSR. The user-visible\nx86 has these and more. Figure B.4 shows some of the user-visible system\nregisters in the x86 ISA.\nInstruction Pointer (RIP)\nThe x86 has the equivalent of the LC-3s 16-bit pro-\ngram counter. The x86 calls it an instruction pointer (RIP). Since the address\nspace of the x86 is 64 bits, the RIP is a 64-bit register. In 32-bit mode, since\nthe address space is only 32 bits, the instruction pointer (EIP) uses bits [31:0] of\nthe RIP.\nRFLAGS Register\nCorresponding to the LC-3s N, Z, and P condition codes,\nthe x86 has a one-bit SF (sign ag) register and a one-bit ZF (zero ag) register.'}"
"Figure B.4
x86 system registers.
The CF ag stores the carry produced by the last relevant operation that
generated a carry. As we said earlier, together with the ADC instruction, CF facil-
itates the generation of procedures, which allows the software to deal with larger
integers than the ISA supports.
The OF ag stores an overow condition if the last relevant operate generated
a value too large to store in the available number of bits. Recall the discussion of
overow in Section 2.5.3.
The DF ag indicates the direction in which string operations are to process
strings. If DF = 0, the string is processed from the high-address byte down (i.e.,
the pointer keeping track of the element in the string to be processed next is decre-
mented). If DF = 1, the string is processed from the low-address byte up (i.e., the
string pointer is incremented).
Two ags not usually considered as part of the application state are the IF
(interrupt) ag and the TF (trap) ag. Both correspond to functions with which
you are familiar.
IF is very similar to the IE (interrupt enable) bit in the KBSR and DSR,
discussed in Section 9.4.4.1. If IF = 1, the processor can recognize external","{'page_number': 231, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Figure B.4\nx86 system registers.\nThe CF ag stores the carry produced by the last relevant operation that\ngenerated a carry. As we said earlier, together with the ADC instruction, CF facil-\nitates the generation of procedures, which allows the software to deal with larger\nintegers than the ISA supports.\nThe OF ag stores an overow condition if the last relevant operate generated\na value too large to store in the available number of bits. Recall the discussion of\noverow in Section 2.5.3.\nThe DF ag indicates the direction in which string operations are to process\nstrings. If DF = 0, the string is processed from the high-address byte down (i.e.,\nthe pointer keeping track of the element in the string to be processed next is decre-\nmented). If DF = 1, the string is processed from the low-address byte up (i.e., the\nstring pointer is incremented).\nTwo ags not usually considered as part of the application state are the IF\n(interrupt) ag and the TF (trap) ag. Both correspond to functions with which\nyou are familiar.\nIF is very similar to the IE (interrupt enable) bit in the KBSR and DSR,\ndiscussed in Section 9.4.4.1. If IF = 1, the processor can recognize external'}"
"interrupts (like keyboard input, for example). If IF = 0, these external inter-
rupts have no eect on the process that is executing. We say the interrupts are
disabled.
TF is very similar to single-step mode in the LC-3 simulator, only in this case
it is part of the ISA. If TF = 1, the processor halts after every instruction so the
state of the system can be examined. If TF = 0, the processor ignores the trap and
processes the next instruction.
Segment Registers
When operating in its preferred operating mode (called pro-
tected mode), the address calculated by the instruction is really an oset from the
starting address of a segment, which is specied by some segment base regis-
ter. These segment base registers are part of their corresponding data segment
descriptors, which are contained in the segment descriptor table. At each instant
of time, six of these segments are active. They are called, respectively, the code
segment (CS), stack segment (SS), and four data segments (DS, ES, FS, and
GS). The six active segments are accessed via their corresponding segment reg-
isters shown in Figure B.4, which contain pointers to their respective segment
descriptors.
B.2 The Format and Specication of
x86 Instructions
The LC-3 instruction is a 16-bit instruction. Bits [15:12] always contain the
opcode; the remaining 12 bits of each instruction are used to support the needs
of that opcode.
The length of an x86 instruction is not xed. It consists of from 1 to
16 bytes, depending on the needs of that instruction. A lot of information
can be packed into one x86 instruction. Figure B.5 shows the format of an
x86 instruction.
The two key parts of an x86 instruction are the opcode and, where neces-
Figure B.5
Format of the x86 instruction.","{'page_number': 232, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'interrupts (like keyboard input, for example). If IF = 0, these external inter-\nrupts have no eect on the process that is executing. We say the interrupts are\ndisabled.\nTF is very similar to single-step mode in the LC-3 simulator, only in this case\nit is part of the ISA. If TF = 1, the processor halts after every instruction so the\nstate of the system can be examined. If TF = 0, the processor ignores the trap and\nprocesses the next instruction.\nSegment Registers\nWhen operating in its preferred operating mode (called pro-\ntected mode), the address calculated by the instruction is really an oset from the\nstarting address of a segment, which is specied by some segment base regis-\nter. These segment base registers are part of their corresponding data segment\ndescriptors, which are contained in the segment descriptor table. At each instant\nof time, six of these segments are active. They are called, respectively, the code\nsegment (CS), stack segment (SS), and four data segments (DS, ES, FS, and\nGS). The six active segments are accessed via their corresponding segment reg-\nisters shown in Figure B.4, which contain pointers to their respective segment\ndescriptors.\nB.2 The Format and Specication of\nx86 Instructions\nThe LC-3 instruction is a 16-bit instruction. Bits [15:12] always contain the\nopcode; the remaining 12 bits of each instruction are used to support the needs\nof that opcode.\nThe length of an x86 instruction is not xed. It consists of from 1 to\n16 bytes, depending on the needs of that instruction. A lot of information\ncan be packed into one x86 instruction. Figure B.5 shows the format of an\nx86 instruction.\nThe two key parts of an x86 instruction are the opcode and, where neces-\nFigure B.5\nFormat of the x86 instruction.'}"
"the use of registers, a one-, two-, or four-byte displacement, and additional register
information contained in an optional SIB byte.
Some opcodes specify an immediate operand and also specify the number
of bytes of the instruction that is used to store that immediate information. The
immediate value (when one is specied) is the last element of the instruction.
Finally, instructions assume certain default information with respect to the
semantics of an instruction, such as address size, operand size, segment to be
used, and so forth. The instruction can change this default information by means
of one or more prexes, which are located at the beginning of the instruction.
Each part of an x86 instruction is discussed in more detail in Sections B.2.1
through B.2.6.
B.2.1 Prex
Prexes provide additional information that is used to process the instruction.
There are four classes of prex information, and each instruction can have from
zero to four prexes, depending on its needs. Fundamentally, a prex overrides
the usual interpretation of the instruction.
The four classes of prexes are lock and repeat, segment override, operand
override, and address override. Table B.4 describes the four types of prexes.
B.2.2 Opcode
The opcode byte (or bytessome opcodes are represented by two bytes) spec-
ies a large amount of information about the needs of that instruction. The
Table B.4
Prexes, x86 ISA
Repeat/Lock
xF0 (LOCK)
This prex guarantees that the instruction will have exclusive use
of all shared memory until the instruction completes execution.
xF2, xF3
(REP/REPE/REPNE)
This prex allows the instruction (a string instruction) to be
repeated some specied number of times. The iteration count
is specied by ECX. The instruction is also terminated on the
occurrence of a specied value of ZF.
Segment override
x2E(CS), x36(SS),
x3E(DS), x26(ES),
x64(FS), x65(GS)
This prex causes the memory access to use the specied
segment, instead of the default segment expected for that
instruction.
Operand size override
x66
This prex changes the size of data expected for this instruction.
That is, instructions expecting 32-bit data elements use 16-bit
data elements. And instructions expecting 16-bit data elements
use 32-bit data elements.
Address size override
x67
This prex changes the size of operand addresses expected for
this instruction. That is, instructions expecting a 32-bit address
use 16-bit addresses. And instructions expecting 16-bit
addresses use 32-bit addresses.","{'page_number': 233, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'the use of registers, a one-, two-, or four-byte displacement, and additional register\ninformation contained in an optional SIB byte.\nSome opcodes specify an immediate operand and also specify the number\nof bytes of the instruction that is used to store that immediate information. The\nimmediate value (when one is specied) is the last element of the instruction.\nFinally, instructions assume certain default information with respect to the\nsemantics of an instruction, such as address size, operand size, segment to be\nused, and so forth. The instruction can change this default information by means\nof one or more prexes, which are located at the beginning of the instruction.\nEach part of an x86 instruction is discussed in more detail in Sections B.2.1\nthrough B.2.6.\nB.2.1 Prex\nPrexes provide additional information that is used to process the instruction.\nThere are four classes of prex information, and each instruction can have from\nzero to four prexes, depending on its needs. Fundamentally, a prex overrides\nthe usual interpretation of the instruction.\nThe four classes of prexes are lock and repeat, segment override, operand\noverride, and address override. Table B.4 describes the four types of prexes.\nB.2.2 Opcode\nThe opcode byte (or bytessome opcodes are represented by two bytes) spec-\nies a large amount of information about the needs of that instruction. The\nTable B.4\nPrexes, x86 ISA\nRepeat/Lock\nxF0 (LOCK)\nThis prex guarantees that the instruction will have exclusive use\nof all shared memory until the instruction completes execution.\nxF2, xF3\n(REP/REPE/REPNE)\nThis prex allows the instruction (a string instruction) to be\nrepeated some specied number of times. The iteration count\nis specied by ECX. The instruction is also terminated on the\noccurrence of a specied value of ZF.\nSegment override\nx2E(CS), x36(SS),\nx3E(DS), x26(ES),\nx64(FS), x65(GS)\nThis prex causes the memory access to use the specied\nsegment, instead of the default segment expected for that\ninstruction.\nOperand size override\nx66\nThis prex changes the size of data expected for this instruction.\nThat is, instructions expecting 32-bit data elements use 16-bit\ndata elements. And instructions expecting 16-bit data elements\nuse 32-bit data elements.\nAddress size override\nx67\nThis prex changes the size of operand addresses expected for\nthis instruction. That is, instructions expecting a 32-bit address\nuse 16-bit addresses. And instructions expecting 16-bit\naddresses use 32-bit addresses.'}"
"opcode byte (or bytes) species, among other things, the operation to be per-
formed, whether the operands are to be obtained from memory or from reg-
isters, the size of the operands, whether or not one of the source operands is
an immediate value in the instruction, and if so, the size of that immediate
operand.
Some opcodes are formed by combining the opcode byte with bits [5:3]
of the ModR/M byte, if those bits are not needed to provide addressing mode
information. The ModR/M byte is described in Section B.2.3.
B.2.3 ModR/M Byte
The ModR/M byte, shown in Figure B.5, provides addressing mode information
for two operands, when necessary, or for one operand, if that is all that is needed.
If two operands are needed, one may be in memory, the other in a register, or both
may be in registers. If one operand is needed, it can be either in a register or in
memory. The ModR/M byte supports all cases.
The ModR/M byte is essentially partitioned into two parts. The rst part
consists of bits [7:6] and bits [2:0]. The second part consists of bits [5:3].
If bits [7:6] = 00, 01, or 10, the rst part species the addressing mode
of a memory operand, and the combined ve bits ([7:6],[2:0]) identify which
addressing mode. If bits [7:6] = 11, there is no memory operand, and bits [2:0]
specify a register operand.
Bits [5:3] specify the register number of the other operand, if the opcode
requires two operands. If the opcode only requires one operand, bits [5:3] are
available as a subopcode to dierentiate among eight opcodes that have the same
,
.
,
[5:3]) are part of the opcode.","{'page_number': 234, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'opcode byte (or bytes) species, among other things, the operation to be per-\nformed, whether the operands are to be obtained from memory or from reg-\nisters, the size of the operands, whether or not one of the source operands is\nan immediate value in the instruction, and if so, the size of that immediate\noperand.\nSome opcodes are formed by combining the opcode byte with bits [5:3]\nof the ModR/M byte, if those bits are not needed to provide addressing mode\ninformation. The ModR/M byte is described in Section B.2.3.\nB.2.3 ModR/M Byte\nThe ModR/M byte, shown in Figure B.5, provides addressing mode information\nfor two operands, when necessary, or for one operand, if that is all that is needed.\nIf two operands are needed, one may be in memory, the other in a register, or both\nmay be in registers. If one operand is needed, it can be either in a register or in\nmemory. The ModR/M byte supports all cases.\nThe ModR/M byte is essentially partitioned into two parts. The rst part\nconsists of bits [7:6] and bits [2:0]. The second part consists of bits [5:3].\nIf bits [7:6] = 00, 01, or 10, the rst part species the addressing mode\nof a memory operand, and the combined ve bits ([7:6],[2:0]) identify which\naddressing mode. If bits [7:6] = 11, there is no memory operand, and bits [2:0]\nspecify a register operand.\nBits [5:3] specify the register number of the other operand, if the opcode\nrequires two operands. If the opcode only requires one operand, bits [5:3] are\navailable as a subopcode to dierentiate among eight opcodes that have the same\n,\n.\n,\n[5:3]) are part of the opcode.'}"
".
then added to whatever is specied by the
ModR/M byte.
B.2.4 SIB Byte
If the opcode species that an operand is to be obtained from memory, the Mod-
R/M byte species the addressing mode, that is, the information that is needed
to calculate the address of that operand. Some addressing modes require more
information than can be specied by the ModR/M byte alone. Those operand
speciers (see the third entry in Table B.5) specify the inclusion of an SIB byte
in the instruction. The SIB byte (for scaled-index-base), shown in Figure B.5, pro-
vides scaling information and identies which register is to be used as an index
register and/or which register is to be used as a base register. Taken together, the
SIB byte computes scale  index + base, where base and/or index can be zero,
and scale can be 1. Table B.6 lists some of the interpretations of the SIB byte.
B.2.5 Displacement
If the ModR/M byte species that the address calculation requires a displacement,
the displacement (one, two, or four bytes) is contained in the instruction. The
opcode and/or ModR/M byte species the size of the displacement.
Figure B.6 shows the addressing mode calculation for the source operand if
the instruction is as shown. The prex x26 overrides the segment register and
species using the ES segment. The ModR/M and SIB bytes specify that a four-
byte displacement is to be added to the base register ECX + the index register
EBX after its contents is multiplied by 4.
B.2.6 Immediate
Recall that the LC-3 allowed small immediate values to be present in the instruc-
tion, by setting inst[5:5] to 1. The x86 also permits immediate values in the
instruction. As stated previously, if the opcode species that a source operand
is an immediate value in the instruction, it also species the number of bytes
of the instruction used to represent the operand. That is, an immediate can be","{'page_number': 235, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '.\nthen added to whatever is specied by the\nModR/M byte.\nB.2.4 SIB Byte\nIf the opcode species that an operand is to be obtained from memory, the Mod-\nR/M byte species the addressing mode, that is, the information that is needed\nto calculate the address of that operand. Some addressing modes require more\ninformation than can be specied by the ModR/M byte alone. Those operand\nspeciers (see the third entry in Table B.5) specify the inclusion of an SIB byte\nin the instruction. The SIB byte (for scaled-index-base), shown in Figure B.5, pro-\nvides scaling information and identies which register is to be used as an index\nregister and/or which register is to be used as a base register. Taken together, the\nSIB byte computes scale  index + base, where base and/or index can be zero,\nand scale can be 1. Table B.6 lists some of the interpretations of the SIB byte.\nB.2.5 Displacement\nIf the ModR/M byte species that the address calculation requires a displacement,\nthe displacement (one, two, or four bytes) is contained in the instruction. The\nopcode and/or ModR/M byte species the size of the displacement.\nFigure B.6 shows the addressing mode calculation for the source operand if\nthe instruction is as shown. The prex x26 overrides the segment register and\nspecies using the ES segment. The ModR/M and SIB bytes specify that a four-\nbyte displacement is to be added to the base register ECX + the index register\nEBX after its contents is multiplied by 4.\nB.2.6 Immediate\nRecall that the LC-3 allowed small immediate values to be present in the instruc-\ntion, by setting inst[5:5] to 1. The x86 also permits immediate values in the\ninstruction. As stated previously, if the opcode species that a source operand\nis an immediate value in the instruction, it also species the number of bytes\nof the instruction used to represent the operand. That is, an immediate can be'}"
"694
Address
Figure B.6
Addressing mode calculation for Base+ScaledIndes+disp32.
represented in the instruction with one, two, or four bytes. Since the opcode
also species the size of the operand, immediate values that can be stored in
fewer bytes than the operand size are rst sign-extended to their full size before
Figure B.7
Example x86 instruction in 32-bit mode: ADD EAX, $5.","{'page_number': 236, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '694\nAddress\nFigure B.6\nAddressing mode calculation for Base+ScaledIndes+disp32.\nrepresented in the instruction with one, two, or four bytes. Since the opcode\nalso species the size of the operand, immediate values that can be stored in\nfewer bytes than the operand size are rst sign-extended to their full size before\nFigure B.7\nExample x86 instruction in 32-bit mode: ADD EAX, $5.'}"
"B.3 An Example
We conclude this appendix with an example. The problem is one we have dealt
with extensively in Chapter 14. Given an input character string consisting of text,
numbers, and punctuation, write a C program to convert all the lowercase letters
to uppercase. Figure B.8 shows a C program that solves this problem. Figure B.9
shows the annotated LC-3 assembly language code that a C compiler would gen-
erate. Figure B.10 shows the corresponding annotated x86 assembly language
code, assuming we are operating the x86 in 32-bit mode. For readability, we show
assembly language representations of the LC-3 and x86 programs rather than the
machine code.
#include <stdio.h>
void UpcaseString(char inputString[]);
int main (void)
{
char string[8];
scanf(""%s"", string);
UpcaseString(string);
}
void UpcaseString(char inputString[])
{
int i = 0;
while(inputString[i]) {
if (('a' <= inputString[i]) && (inputString[i] <= 'z'))
inputString[i] = inputString[i] - ('a' - 'A');
i++;
}
}
Figure B.8
C source code for the upper-/lowercase program.","{'page_number': 237, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'B.3 An Example\nWe conclude this appendix with an example. The problem is one we have dealt\nwith extensively in Chapter 14. Given an input character string consisting of text,\nnumbers, and punctuation, write a C program to convert all the lowercase letters\nto uppercase. Figure B.8 shows a C program that solves this problem. Figure B.9\nshows the annotated LC-3 assembly language code that a C compiler would gen-\nerate. Figure B.10 shows the corresponding annotated x86 assembly language\ncode, assuming we are operating the x86 in 32-bit mode. For readability, we show\nassembly language representations of the LC-3 and x86 programs rather than the\nmachine code.\n#include <stdio.h>\nvoid UpcaseString(char inputString[]);\nint main (void)\n{\nchar string[8];\nscanf(""%s"", string);\nUpcaseString(string);\n}\nvoid UpcaseString(char inputString[])\n{\nint i = 0;\nwhile(inputString[i]) {\nif ((\'a\' <= inputString[i]) && (inputString[i] <= \'z\'))\ninputString[i] = inputString[i] - (\'a\' - \'A\');\ni++;\n}\n}\nFigure B.8\nC source code for the upper-/lowercase program.'}"
"Microarchitecture
e LC-3
W
e have seen in Chapters 4 and 5 the several stages of the instruction cycle
that must occur in order for the computer to process each instruction. If a
microarchitecture is to implement an ISA, it must be able to carry out this instruc-
tion cycle for every instruction in the ISA. This appendix illustrates one example
of a microarchitecture that can do that for the LC-3 ISA. Many of the details of
the microarchitecture and the reasons for each design decision are well beyond
the scope of an introductory course. However, for those who want to understand
how a microarchitecture can carry out the requirements of each instruction of the
LC-3 ISA, this appendix is provided.
C.1 Overview
Figure C.1 shows the two main components of a microarchitecture: the data path,
which contains all the components that actually process the instructions, and the
control, which contains all the components that generate the set of control signals
that are needed to control the processing at each instant of time.
We say, at each instant of time, but we really mean during each clock cycle.
That is, time is divided into clock cycles. The cycle time of a microprocessor is the
duration of a clock cycle. A common cycle time for a microprocessor today is 0.33
nanoseconds, which corresponds to 3 billion clock cycles each second. We say
that such a microprocessor is operating at a frequency of 3 gigahertz, or 3 GHz.
At each instant of timeor, rather, during each clock cyclethe 52 control
signals (as shown in Figure C.1) control both the processing in the data path and
the generation of the control signals for the next clock cycle. Processing in the
data path is controlled by 42 bits, and the generation of the control signals for the
next clock cycle is controlled by 10 bits.
Note that the hardware that determines which control signals are needed each
clock cycle does not operate in a vacuum. On the contrary, the control signals
needed in the next clock cycle depend on the following:
1. The control signals that are present during the current clock cycle.
2. The LC-3 instruction that is being executed.","{'page_number': 238, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Microarchitecture\ne LC-3\nW\ne have seen in Chapters 4 and 5 the several stages of the instruction cycle\nthat must occur in order for the computer to process each instruction. If a\nmicroarchitecture is to implement an ISA, it must be able to carry out this instruc-\ntion cycle for every instruction in the ISA. This appendix illustrates one example\nof a microarchitecture that can do that for the LC-3 ISA. Many of the details of\nthe microarchitecture and the reasons for each design decision are well beyond\nthe scope of an introductory course. However, for those who want to understand\nhow a microarchitecture can carry out the requirements of each instruction of the\nLC-3 ISA, this appendix is provided.\nC.1 Overview\nFigure C.1 shows the two main components of a microarchitecture: the data path,\nwhich contains all the components that actually process the instructions, and the\ncontrol, which contains all the components that generate the set of control signals\nthat are needed to control the processing at each instant of time.\nWe say, at each instant of time, but we really mean during each clock cycle.\nThat is, time is divided into clock cycles. The cycle time of a microprocessor is the\nduration of a clock cycle. A common cycle time for a microprocessor today is 0.33\nnanoseconds, which corresponds to 3 billion clock cycles each second. We say\nthat such a microprocessor is operating at a frequency of 3 gigahertz, or 3 GHz.\nAt each instant of timeor, rather, during each clock cyclethe 52 control\nsignals (as shown in Figure C.1) control both the processing in the data path and\nthe generation of the control signals for the next clock cycle. Processing in the\ndata path is controlled by 42 bits, and the generation of the control signals for the\nnext clock cycle is controlled by 10 bits.\nNote that the hardware that determines which control signals are needed each\nclock cycle does not operate in a vacuum. On the contrary, the control signals\nneeded in the next clock cycle depend on the following:\n1. The control signals that are present during the current clock cycle.\n2. The LC-3 instruction that is being executed.'}"
"42
40
Control
10
Control Signals
(J, COND, IRD)
52
Memory, I/O
Addr
16
Inst.
Data,
16
16
Data
Data Path
2
R
INT
IR[15:11]
BEN
PSR[15]
ACV
Figure C.1
Microarchitecture of the LC-3, major components.
3. The privilege mode of the program that is executing, and whether the
processor has the right to access a particular memory location.
4. If that LC-3 instruction is a BR, whether the conditions for the branch have
been met (i.e., the state of the relevant condition codes).
5. Whether or not an external device is requesting that the processor be
interrupted.
6. If a memory operation is in progress, whether it is completing during this cycle.
Figure C.1 identies the specic information in our implementation of the
LC-3 that corresponds to these six items. They are, respectively:
1. J[5:0], COND[2:0], and IRDten bits of control signals provided by the
current clock cycle.
2. inst[15:12], which identies the opcode, and inst[11:11], which
dierentiates JSR from JSRR (i.e., the addressing mode for the target of the
subroutine call).
3. PSR[15], bit [15] of the Processor Status Register, which indicates whether
the current program is executing with supervisor or user privileges, and
ACV, a signal that informs the processor that a process operating in User","{'page_number': 239, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '42\n40\nControl\n10\nControl Signals\n(J, COND, IRD)\n52\nMemory, I/O\nAddr\n16\nInst.\nData,\n16\n16\nData\nData Path\n2\nR\nINT\nIR[15:11]\nBEN\nPSR[15]\nACV\nFigure C.1\nMicroarchitecture of the LC-3, major components.\n3. The privilege mode of the program that is executing, and whether the\nprocessor has the right to access a particular memory location.\n4. If that LC-3 instruction is a BR, whether the conditions for the branch have\nbeen met (i.e., the state of the relevant condition codes).\n5. Whether or not an external device is requesting that the processor be\ninterrupted.\n6. If a memory operation is in progress, whether it is completing during this cycle.\nFigure C.1 identies the specic information in our implementation of the\nLC-3 that corresponds to these six items. They are, respectively:\n1. J[5:0], COND[2:0], and IRDten bits of control signals provided by the\ncurrent clock cycle.\n2. inst[15:12], which identies the opcode, and inst[11:11], which\ndierentiates JSR from JSRR (i.e., the addressing mode for the target of the\nsubroutine call).\n3. PSR[15], bit [15] of the Processor Status Register, which indicates whether\nthe current program is executing with supervisor or user privileges, and\nACV, a signal that informs the processor that a process operating in User'}"
"mode is trying to access a location in privileged memory. ACV stands for
Access Control Violation. When asserted, it denies the process access to the
privileged memory location.
4. BEN to indicate whether or not a BR should be taken.
5. INT to indicate that some external device of higher priority than the
executing process requests service.
6. R to indicate the end of a memory operation.
C.2 The State Machine
The behavior of the LC-3 microarchitecture during a given clock cycle is com-
pletely determined by the 52 control signals, combined with ten bits of addi-
tional information (inst[15:11], PSR[15], ACV, BEN, INT, and R), as shown in
Figure C.1. We have said that during each clock cycle, 42 of these control signals
determine the processing of information in the data path and the other ten control
signals combine with the ten bits of additional information to determine which
set of control signals will be required in the next clock cycle.
We say that these 52 control signals specify the state of the control struc-
ture of the LC-3 microarchitecture. We can completely describe the behavior of
the LC-3 microarchitecture by means of a directed graph that consists of nodes
(one corresponding to each state) and arcs (showing the ow from each state to
the one[s] it goes to next). We call such a graph a state machine.
Figure C.2, combined with Figure C.7, is the state machine for our implemen-
tation of the LC-3. The state machine describes what happens during each clock
cycle in which the computer is running. Each state is active for exactly one clock
cycle before control passes to the next state. The state machine shows the step-
by-step (clock cyclebyclock cycle) process that each instruction goes through
from the start of its FETCH phase to the end of its instruction cycle, as described
in Section 4.3.2. Each node in the state machine corresponds to the activity that
the processor carries out during a single clock cycle. The actual processing that
is performed in the data path is contained inside the node. The step-by-step ow
is conveyed by the arcs that take the processor from one state to the next.
Lets start our study of Figure C.2 by examining the FETCH phase of the
instruction cycle. As you know, every instruction goes through the same FETCH
phase in its instruction cycle. Recall from Chapter 4 that the FETCH phase starts
with a memory access to read the instruction at the address specied by the PC.
Note that in the state numbered 18, the MAR is loaded with the address contained
in PC, and the PC is incremented in preparation for the FETCH of the next LC-3
instruction after the current instruction nishes its instruction cycle. If the content
of MAR species privileged memory, and PSR[15] = 1, indicating User mode,
the access of the instruction will not be allowed. That would be an access control
violation, so ACV is set. Finally, if there is no interrupt request present (INT = 0),
the ow passes to the state numbered 33. We will describe in Section C","{'page_number': 240, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'If the content\nof MAR species privileged memory, and PSR[15] = 1, indicating User mode,\nthe access of the instruction will not be allowed. That would be an access control\nviolation, so ACV is set. Finally, if there is no interrupt request present (INT = 0),\nthe ow passes to the state numbered 33. We will describe in Section C.7 the ow\nof control if INT = 1, that is, if an external device is requesting an interrupt.'}"
"If the content
of MAR species privileged memory, and PSR[15] = 1, indicating User mode,
the access of the instruction will not be allowed. That would be an access control
violation, so ACV is set. Finally, if there is no interrupt request present (INT = 0),
the ow passes to the state numbered 33. We will describe in Section C.7 the ow
of control if INT = 1, that is, if an external device is requesting an interrupt.","{'page_number': 240, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'If the content\nof MAR species privileged memory, and PSR[15] = 1, indicating User mode,\nthe access of the instruction will not be allowed. That would be an access control\nviolation, so ACV is set. Finally, if there is no interrupt request present (INT = 0),\nthe ow passes to the state numbered 33. We will describe in Section C.7 the ow\nof control if INT = 1, that is, if an external device is requesting an interrupt.'}"
"Before we get into what happens during the clock cycle when the proces-
sor is in the state numbered 33, we should explain the numbering systemthat
is, why are states numbered 18 and 33. Recall, from our discussion of nite
state machines in Chapter 3, that each state must be uniquely specied and that
this unique specication is accomplished by means of state variables. Our state
machine that implements the LC-3 ISA requires 59 distinct states to implement
the entire behavior of the LC-3. Figure C.2 shows 31 of them plus pointers to
seven others (states 8, 13, 15, 48, 49, 57, and 60). Figure C.7 shows the other
28 states (including the seven that are pointed to in Figure C.2). We will visit all
of them as we go through this appendix. Since k logical variables can uniquely
identify 2k items, six state variables are needed to uniquely specify 59 states. The
number next to each node in Figure C.2 and Figure C.7 is the decimal equivalent
of the values (0 or 1) of the six state variables for the corresponding state. Thus,
for example, the state numbered 18 has state variable values 010010.
Now, back to what happens after the clock cycle in which the activity of state
18 has nished. As we said, if no external device is requesting an interrupt, the
ow passes to state 33 (i.e., 100001). From state 33, control passes to state 60
if the processor is trying to access privileged memory while in User mode, or to
state 28, if the memory access is allowed, that is, if there is no ACV violation.
We will discuss what happens if there is an ACV violation in Section C.7.
Instate28,sincetheMARcontainstheaddressoftheinstructiontobeprocessed,
this instruction is read from memory and loaded into the MDR. Since this memory
access can take multiple cycles, this state continues to execute until a ready signal
from the memory (R) is asserted, indicating that the memory access has completed.
Thus, the MDR contains the valid contents of the memory location specified by
MAR. The state machine then moves on to state 30, where the instruction is loaded
into the instruction register (IR), completing the fetch phase of the instruction cycle.
The state machine then moves to state 32, where DECODE takes place. Note
that there are 13 arcs emanating from state 32, each one corresponding to bits [15:12]
of the LC-3 instruction. These are the opcode bits that direct the state machine to
one of 16 paths to carry out the instruction cycle of the particular instruction that has
just been fetched. Note that the arc from the last state of each instruction cycle (i.e.,
the state that completes the processing of that LC-3 instruction) takes us to state 18
(to begin the instruction cycle of the next LC-3 instruction).
C","{'page_number': 241, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'These are the opcode bits that direct the state machine to\none of 16 paths to carry out the instruction cycle of the particular instruction that has\njust been fetched. Note that the arc from the last state of each instruction cycle (i.e.,\nthe state that completes the processing of that LC-3 instruction) takes us to state 18\n(to begin the instruction cycle of the next LC-3 instruction).\nC.3 The Data Path\nThe data path consists of all components that actually process the information\nduring each clock cyclethe functional units that operate on the information, the\nregisters that store information at the end of one cycle so it will be available for\nfurther use in subsequent cycles, and the buses and wires that carry information\nfrom one point to another in the data path. Figure C.3, an expanded version of\nwhat you have already encountered in Figure 5.18, illustrates the data path of our\nmicroarchitecture of the LC-3.\nNote the control signals that are associated with each component in the data\npath. For example, ALUK, consisting of two control signals, is associated with'}"
"These are the opcode bits that direct the state machine to
one of 16 paths to carry out the instruction cycle of the particular instruction that has
just been fetched. Note that the arc from the last state of each instruction cycle (i.e.,
the state that completes the processing of that LC-3 instruction) takes us to state 18
(to begin the instruction cycle of the next LC-3 instruction).
C.3 The Data Path
The data path consists of all components that actually process the information
during each clock cyclethe functional units that operate on the information, the
registers that store information at the end of one cycle so it will be available for
further use in subsequent cycles, and the buses and wires that carry information
from one point to another in the data path. Figure C.3, an expanded version of
what you have already encountered in Figure 5.18, illustrates the data path of our
microarchitecture of the LC-3.
Note the control signals that are associated with each component in the data
path. For example, ALUK, consisting of two control signals, is associated with","{'page_number': 241, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'These are the opcode bits that direct the state machine to\none of 16 paths to carry out the instruction cycle of the particular instruction that has\njust been fetched. Note that the arc from the last state of each instruction cycle (i.e.,\nthe state that completes the processing of that LC-3 instruction) takes us to state 18\n(to begin the instruction cycle of the next LC-3 instruction).\nC.3 The Data Path\nThe data path consists of all components that actually process the information\nduring each clock cyclethe functional units that operate on the information, the\nregisters that store information at the end of one cycle so it will be available for\nfurther use in subsequent cycles, and the buses and wires that carry information\nfrom one point to another in the data path. Figure C.3, an expanded version of\nwhat you have already encountered in Figure 5.18, illustrates the data path of our\nmicroarchitecture of the LC-3.\nNote the control signals that are associated with each component in the data\npath. For example, ALUK, consisting of two control signals, is associated with'}"
"the ALU. These control signals determine how that component (the ALU) will
be used each cycle. Table C.1 lists the set of 42 control signals that control the
elements of the data path and the set of values that each control signal can have.
(Actually, for readability, we provide a symbolic name for each value, rather than
the binary value.) For example, since ALUK consists of two bits, it can have one
of four values. Which value it has during any particular clock cycle depends on
whether the ALU is required to ADD, AND, NOT, or simply pass one of its inputs
to the output during that clock cycle. PCMUX also consists of two control signals
and species which input to the MUX is required during a given clock cycle.
LD.PC is a single-bit control signal and is a 0 (NO) or a 1 (YES), depending on
whether or not the PC is to be loaded during the given clock cycle.
During each clock cycle, corresponding to the current state in the state
machine, the 42 bits of control direct the processing of all components in the data
path that are required during that clock cycle. As we have said, the processing
that takes place in the data path during that clock cycle is specied inside the
node representing the state.
C.4 The Control Structure
The control structure of a microarchitecture is specied by its state machine. As
described earlier, the state machine (Figure C.2 and Figure C.7) determines which
control signals are needed each clock cycle to process information in the data path
and which control signals are needed each clock cycle to direct the ow of control
from the currently active state to its successor state.
Figure C.4 shows a block diagram of the control structure of our implemen-
tation of the LC-3. Many implementations are possible, and the design consider-
ations that must be studied to determine which of many possible implementations
should be used is the subject of a full course in computer architecture.
We have chosen here a straightforward microprogrammed implementation.
Each state of the control structure requires 42 bits to control the processing in the
data path and 10 bits to help determine which state comes next. These 52 bits are
collectively known as a microinstruction. Each microinstruction (i.e., each state
of the state machine) is stored in one 52-bit location of a special memory called
the control store. There are 59 distinct states. Since each state corresponds to one
microinstruction in the control store, the control store for our microprogrammed
implementation requires six bits to specify the address of each microinstruction.
Those six bits correspond to the state number associated with each state in the
state machine. For example, the microinstruction associated with state 18 is the
set of 52 control signals stored in address 18 of the control store.
Table C.2 lists the function of the ten bits of control information that help
determine which state comes next. Figure C.5 shows the logic of the micro-
sequencer. The purpose of the microsequencer is to determine the address in the
control store that corresponds to the next state, that is, the location where the
52 bits of control information for the next state are stored.","{'page_number': 242, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'the ALU. These control signals determine how that component (the ALU) will\nbe used each cycle. Table C.1 lists the set of 42 control signals that control the\nelements of the data path and the set of values that each control signal can have.\n(Actually, for readability, we provide a symbolic name for each value, rather than\nthe binary value.) For example, since ALUK consists of two bits, it can have one\nof four values. Which value it has during any particular clock cycle depends on\nwhether the ALU is required to ADD, AND, NOT, or simply pass one of its inputs\nto the output during that clock cycle. PCMUX also consists of two control signals\nand species which input to the MUX is required during a given clock cycle.\nLD.PC is a single-bit control signal and is a 0 (NO) or a 1 (YES), depending on\nwhether or not the PC is to be loaded during the given clock cycle.\nDuring each clock cycle, corresponding to the current state in the state\nmachine, the 42 bits of control direct the processing of all components in the data\npath that are required during that clock cycle. As we have said, the processing\nthat takes place in the data path during that clock cycle is specied inside the\nnode representing the state.\nC.4 The Control Structure\nThe control structure of a microarchitecture is specied by its state machine. As\ndescribed earlier, the state machine (Figure C.2 and Figure C.7) determines which\ncontrol signals are needed each clock cycle to process information in the data path\nand which control signals are needed each clock cycle to direct the ow of control\nfrom the currently active state to its successor state.\nFigure C.4 shows a block diagram of the control structure of our implemen-\ntation of the LC-3. Many implementations are possible, and the design consider-\nations that must be studied to determine which of many possible implementations\nshould be used is the subject of a full course in computer architecture.\nWe have chosen here a straightforward microprogrammed implementation.\nEach state of the control structure requires 42 bits to control the processing in the\ndata path and 10 bits to help determine which state comes next. These 52 bits are\ncollectively known as a microinstruction. Each microinstruction (i.e., each state\nof the state machine) is stored in one 52-bit location of a special memory called\nthe control store. There are 59 distinct states. Since each state corresponds to one\nmicroinstruction in the control store, the control store for our microprogrammed\nimplementation requires six bits to specify the address of each microinstruction.\nThose six bits correspond to the state number associated with each state in the\nstate machine. For example, the microinstruction associated with state 18 is the\nset of 52 control signals stored in address 18 of the control store.\nTable C.2 lists the function of the ten bits of control information that help\ndetermine which state comes next. Figure C.5 shows the logic of the micro-\nsequencer. The purpose of the microsequencer is to determine the address in the\ncontrol store that corresponds to the next state, that is, the location where the\n52 bits of control information for the next state are stored.'}"
"708
Address of Next State
Figure C.5
The microsequencer of the LC-3.
As we said, state 32 of the state machine (Figure C.2) performs the DECODE
phase of the instruction cycle. It has 16 next states, depending on the LC-3
instruction being executed during the current instruction cycle. If the IRD con-
trol signal in the microinstruction corresponding to state 32 is 1, the output MUX
of the microsequencer (Figure C.5) will take its source from the six bits formed
by 00 concatenated with the four opcode bits IR[15:12]. Since IR[15:12] speci-
es the opcode of the current LC-3 instruction being processed, the next address
of the control store will be one of 16 addresses, corresponding to the 15 opcodes
plus the one unused opcode, IR[15:12] = 1101. That is, each of the 16 next states
after state 32 is the rst state to be carried out after the instruction has been
decoded in state 32. For example, if the instruction being processed is ADD, the
address of the next state is state 1, whose microinstruction is stored at location
000001. Recall that IR[15:12] for ADD is 0001.
If, somehow, the instruction inadvertently contained IR[15:12] = 1101,
the unused opcode, the microarchitecture would execute a sequence of
microinstructions, starting at state 13. These microinstructions would respond to
the fact that an instruction with an illegal opcode had been fetched. Section C.7.3
describes what happens in that case.","{'page_number': 243, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '708\nAddress of Next State\nFigure C.5\nThe microsequencer of the LC-3.\nAs we said, state 32 of the state machine (Figure C.2) performs the DECODE\nphase of the instruction cycle. It has 16 next states, depending on the LC-3\ninstruction being executed during the current instruction cycle. If the IRD con-\ntrol signal in the microinstruction corresponding to state 32 is 1, the output MUX\nof the microsequencer (Figure C.5) will take its source from the six bits formed\nby 00 concatenated with the four opcode bits IR[15:12]. Since IR[15:12] speci-\nes the opcode of the current LC-3 instruction being processed, the next address\nof the control store will be one of 16 addresses, corresponding to the 15 opcodes\nplus the one unused opcode, IR[15:12] = 1101. That is, each of the 16 next states\nafter state 32 is the rst state to be carried out after the instruction has been\ndecoded in state 32. For example, if the instruction being processed is ADD, the\naddress of the next state is state 1, whose microinstruction is stored at location\n000001. Recall that IR[15:12] for ADD is 0001.\nIf, somehow, the instruction inadvertently contained IR[15:12] = 1101,\nthe unused opcode, the microarchitecture would execute a sequence of\nmicroinstructions, starting at state 13. These microinstructions would respond to\nthe fact that an instruction with an illegal opcode had been fetched. Section C.7.3\ndescribes what happens in that case.'}"
"(d)
Figure C.6
Additional logic required to provide control signals.
Several signals necessary to control the data path and the microsequencer
are not among those listed in Tables C.1 and C.2. They are DR, SR1, BEN, INT,
ACV, and R. Figure C.6 shows the additional logic needed to generate DR, SR1,
BEN, and ACV.
The INT signal is supplied by some event external to the normal instruction
processing, indicating that normal instruction processing should be interrupted
and this external event dealt with. The interrupt mechanism was described in
Chapter 9. The corresponding ow of control within the microarchitecture is
described in Section C.7.
The remaining signal, R, is a signal generated by the memory in order to
allow the LC-3 to operate correctly with a memory that takes multiple clock
cycles to read or store a value.
Suppose it takes memory ve cycles to read a value. That is, once MAR
contains the address to be read and the microinstruction asserts READ, it will take
ve cycles before the contents of the specied location in memory is available to
be loaded into MDR. (Note that the microinstruction asserts READ by means of
two control signals: MIO.EN/YES and R.W/RD; see Figure C.3.)","{'page_number': 244, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '(d)\nFigure C.6\nAdditional logic required to provide control signals.\nSeveral signals necessary to control the data path and the microsequencer\nare not among those listed in Tables C.1 and C.2. They are DR, SR1, BEN, INT,\nACV, and R. Figure C.6 shows the additional logic needed to generate DR, SR1,\nBEN, and ACV.\nThe INT signal is supplied by some event external to the normal instruction\nprocessing, indicating that normal instruction processing should be interrupted\nand this external event dealt with. The interrupt mechanism was described in\nChapter 9. The corresponding ow of control within the microarchitecture is\ndescribed in Section C.7.\nThe remaining signal, R, is a signal generated by the memory in order to\nallow the LC-3 to operate correctly with a memory that takes multiple clock\ncycles to read or store a value.\nSuppose it takes memory ve cycles to read a value. That is, once MAR\ncontains the address to be read and the microinstruction asserts READ, it will take\nve cycles before the contents of the specied location in memory is available to\nbe loaded into MDR. (Note that the microinstruction asserts READ by means of\ntwo control signals: MIO.EN/YES and R.W/RD; see Figure C.3.)'}"
"Recall our discussion in Section C.2 of the function of state 28, which
accesses an instruction from memory during the FETCH phase of each instruc-
tion cycle. If the memory takes ve cycles to read a value, for the LC-3 to operate
correctly, state 28 must execute ve times before moving on to state 30. That is,
until MDR contains valid data from the memory location specied by the con-
tents of MAR, we want state 28 to continue to re-execute. After ve clock cycles,
the memory has completed the read, resulting in valid data in MDR, so the pro-
cessor can move on to state 30. What if the microarchitecture did not wait for the
memory to complete the read operation before moving on to state 30? Since the
contents of MDR would still be garbage, the microarchitecture would put garbage
into the IR in state 30.
The ready signal (R) enables the memory read to execute correctly. Since the
memory knows it needs ve clock cycles to complete the read, it asserts a ready
signal (R) throughout the fth clock cycle. Figure C.2 shows that the next state
is 28 (i.e., 011100) if the memory read will not complete in the current clock
cycle and state 30 (i.e., 011110) if it will. As we have seen, it is the job of the
microsequencer (Figure C.5) to produce the next state address.
The ten microsequencer control signals for state 28 are:
IRD/0
; NO
COND/001
; Memory Ready
J/011100
With these control signals, what next state address is generated by the microse-
quencer? For each of the rst four executions of state 28, since R = 0, the next
state address is 011100. This causes state 28 to be executed again in the next clock
cycle. In the fth clock cycle, since R = 1, the next state address is 011110, and
the LC-3 moves on to state 30. Note that in order for the ready signal (R) from
memory to be part of the next state address, COND had to be set to 001, which
allowed R to pass through its four-input AND gate.
C.5 The TRAP Instruction
As we have said, each LC-3 instruction follows its own path from state 32 to
its nal state in its instruction cycle, after which it returns to state 18 to start
processing the next instruction. As an example, we will follow the instruction
cycle of the TRAP instruction, shown in Figure C.7.
Recall that the TRAP instruction pushes the PSR and PC onto the system
stack, loads the PC with the starting address of the trap service routine, and
executes the service routine from privileged memory.
From state 32, the next state after DECODE is state 15, consistent with the
TRAP instruction opcode 1111","{'page_number': 245, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'As an example, we will follow the instruction\ncycle of the TRAP instruction, shown in Figure C.7.\nRecall that the TRAP instruction pushes the PSR and PC onto the system\nstack, loads the PC with the starting address of the trap service routine, and\nexecutes the service routine from privileged memory.\nFrom state 32, the next state after DECODE is state 15, consistent with the\nTRAP instruction opcode 1111. In state 15, the Table register, which will be\nused to form MAR[15:8] of the trap vector table entry, is loaded with x00, the\nPC is incremented (we will see why momentarily), and the MDR is loaded with\nthe PSR in preparation for pushing it onto the system stack. Control passes to\nstate 47.'}"
"As an example, we will follow the instruction
cycle of the TRAP instruction, shown in Figure C.7.
Recall that the TRAP instruction pushes the PSR and PC onto the system
stack, loads the PC with the starting address of the trap service routine, and
executes the service routine from privileged memory.
From state 32, the next state after DECODE is state 15, consistent with the
TRAP instruction opcode 1111. In state 15, the Table register, which will be
used to form MAR[15:8] of the trap vector table entry, is loaded with x00, the
PC is incremented (we will see why momentarily), and the MDR is loaded with
the PSR in preparation for pushing it onto the system stack. Control passes to
state 47.","{'page_number': 245, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'As an example, we will follow the instruction\ncycle of the TRAP instruction, shown in Figure C.7.\nRecall that the TRAP instruction pushes the PSR and PC onto the system\nstack, loads the PC with the starting address of the trap service routine, and\nexecutes the service routine from privileged memory.\nFrom state 32, the next state after DECODE is state 15, consistent with the\nTRAP instruction opcode 1111. In state 15, the Table register, which will be\nused to form MAR[15:8] of the trap vector table entry, is loaded with x00, the\nPC is incremented (we will see why momentarily), and the MDR is loaded with\nthe PSR in preparation for pushing it onto the system stack. Control passes to\nstate 47.'}"
"In state 47, the trap vector (IR[7:0]) is loaded into the eight-bit register
Vector, PSR[15] is set to Supervisor mode since the trap service routine exe-
cutes in privileged memory, and the state machine branches to state 37 or 45,
depending on whether the program that executed the TRAP instruction was in
User mode or Supervisor mode. If in User mode, state 45 saves the User Stack
Pointer in Saved USP, loads the stack pointer from Saved SSP, and continues on
to state 37, where the processor starts pushing PSR and PC onto the stack. If the
program executing the TRAP instruction is already in Privileged mode, state 45
is not necessary.
In states 37 and 41, the PSR is pushed onto the system stack. In states 43, 46,
and 52, the PC is pushed onto the system stack. Note that in state 43, the PC is
decremented before being pushed onto the stack. This is necessary in the case of
dealing with interrupts and exceptions, which will be explained in Section C.7.
This is not necessary for processing the TRAP instruction, which is why PC is
incremented in state 15.
The only thing remaining is to load PC with the starting address of the trap
service routine. This is done by loading MAR with the address of the proper entry
in the trap vector table, obtained by concatenating Table and Vector (in state 54),
loading the starting address from memory into MDR (in state 53), and loading
the PC (in state 55). This completes the execution of the TRAP instruction, and
control returns to state 18 to begin processing the next instruction  in this case,
the rst instruction of the trap service routine.
The last instruction in every trap service routine is RTI (return from trap or
interrupt). From DECODE in state 32, the next state of RTI is state 8, consistent
with its eight-bit opcode 1000. In states 8, 36, and 38, the PC is popped o the
system stack and loaded into PC. In states 39, 40, 42, and 34, the PSR is popped
o the system stack and loaded into PSR. This returns the PC and PSR to the
values it had before the trap service routine was executed. Finally, if the program
that invoked the TRAP instruction was in User mode, PSR[15] must be returned
to 1, the Supervisor Stack Pointer saved, and the User Stack Pointer loaded into
SP. This is done in state 59, completing the instruction cycle for RTI.
C.6 Memory-Mapped I/O
As you know from Chapter 9, the LC-3 ISA performs input and output via
memory-mapped I/O, that is, with the same data movement instructions that it
uses to read from and write to memory. The LC-3 does this by assigning an
address to each device register. Input is accomplished by a load instruction whose
eective address is the address of an input device register. Output is accomplished
by a store instruction whose eective address is the address of an output device
register. For example, in state 25 of Figure C","{'page_number': 246, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The LC-3 does this by assigning an\naddress to each device register. Input is accomplished by a load instruction whose\neective address is the address of an input device register. Output is accomplished\nby a store instruction whose eective address is the address of an output device\nregister. For example, in state 25 of Figure C.2, if the address in MAR is xFE02,\nMDR is supplied by the KBDR, and the data input will be the last keyboard\ncharacter typed. On the other hand, if the address in MAR is a legitimate memory\naddress, MDR is supplied by the memory.'}"
"The LC-3 does this by assigning an
address to each device register. Input is accomplished by a load instruction whose
eective address is the address of an input device register. Output is accomplished
by a store instruction whose eective address is the address of an output device
register. For example, in state 25 of Figure C.2, if the address in MAR is xFE02,
MDR is supplied by the KBDR, and the data input will be the last keyboard
character typed. On the other hand, if the address in MAR is a legitimate memory
address, MDR is supplied by the memory.","{'page_number': 246, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The LC-3 does this by assigning an\naddress to each device register. Input is accomplished by a load instruction whose\neective address is the address of an input device register. Output is accomplished\nby a store instruction whose eective address is the address of an output device\nregister. For example, in state 25 of Figure C.2, if the address in MAR is xFE02,\nMDR is supplied by the KBDR, and the data input will be the last keyboard\ncharacter typed. On the other hand, if the address in MAR is a legitimate memory\naddress, MDR is supplied by the memory.'}"
"712
other
0
W
0
x
0
0
0
other
1
R
1
mem
0
0
0
other
1
W
1
x
0
0
0
The state machine of Figure C.2 does not have to be altered to accommo-
date memory-mapped I/O. However, something has to determine when memory
should be accessed and when I/O device registers should be accessed. This is the
job of the address control logic (ADDR.CTL.LOGIC) shown in Figure C.3.
Table C.3 is a truth table for the address control logic, showing what con-
trol signals are generated, based on (1) the contents of MAR, (2) whether or not
memory or I/O is accessed this cycle (MIO.EN/NO, YES), and (3) whether a load
(R.W/Read) or store (R.W/Write) is requested. Note that, for a memory-mapped
load, data can be supplied to MDR from one of four sources: memory, KBDR,
KBSR, or DSR. The address control logic provides the appropriate select signals
to the INMUX. For a memory-mapped store, the data supplied by MDR can be
written to memory, KBSR, DDR, or DSR. The address control logic supplies the
appropriate enable signal to the corresponding structure.
C.7 Interrupt and Exception Control
The nal piece of the state machine needed to complete the LC-3 story are those
states that control the initiation of an interrupt, those states that control the return
from an interrupt (the RTI instruction), and those states that control the initiation
of one of the three exceptions specied by the ISA.
Interrupts and exceptions are very similar. Both stop the program that is cur-
rently executing. Both push the PSR and PC of the interrupted program onto the
system stack, obtain the starting address of the interrupt or exception service rou-
tine from the interrupt vector table, and load that starting address into the Program
Counter. The main dierence between interrupts and exceptions is the nature of","{'page_number': 247, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': '712\nother\n0\nW\n0\nx\n0\n0\n0\nother\n1\nR\n1\nmem\n0\n0\n0\nother\n1\nW\n1\nx\n0\n0\n0\nThe state machine of Figure C.2 does not have to be altered to accommo-\ndate memory-mapped I/O. However, something has to determine when memory\nshould be accessed and when I/O device registers should be accessed. This is the\njob of the address control logic (ADDR.CTL.LOGIC) shown in Figure C.3.\nTable C.3 is a truth table for the address control logic, showing what con-\ntrol signals are generated, based on (1) the contents of MAR, (2) whether or not\nmemory or I/O is accessed this cycle (MIO.EN/NO, YES), and (3) whether a load\n(R.W/Read) or store (R.W/Write) is requested. Note that, for a memory-mapped\nload, data can be supplied to MDR from one of four sources: memory, KBDR,\nKBSR, or DSR. The address control logic provides the appropriate select signals\nto the INMUX. For a memory-mapped store, the data supplied by MDR can be\nwritten to memory, KBSR, DDR, or DSR. The address control logic supplies the\nappropriate enable signal to the corresponding structure.\nC.7 Interrupt and Exception Control\nThe nal piece of the state machine needed to complete the LC-3 story are those\nstates that control the initiation of an interrupt, those states that control the return\nfrom an interrupt (the RTI instruction), and those states that control the initiation\nof one of the three exceptions specied by the ISA.\nInterrupts and exceptions are very similar. Both stop the program that is cur-\nrently executing. Both push the PSR and PC of the interrupted program onto the\nsystem stack, obtain the starting address of the interrupt or exception service rou-\ntine from the interrupt vector table, and load that starting address into the Program\nCounter. The main dierence between interrupts and exceptions is the nature of'}"
"an interrupt by asserting its interrupt request signal. Recall from Chapter 9 that
if the priority level of the device asserting its interrupt request signal is higher than
both the priority level of the currently executing program and any other external
interrupt request asserted at the same time, INT is asserted and INTV is loaded
with the interrupt vector corresponding to that external event. The microproces-
sor responds to INT by initiating the interrupt. That is, the processor puts itself
into Supervisor mode if it isnt in Supervisor mode, pushes the PSR and PC of
the interrupted process onto the supervisor stack, and loads the PC with the start-
ing address of the interrupt service routine. The PSR contains the privilege mode
PSR[15], priority level PSR[10:8], and condition codes PSR[2:0] of a program.
It is important that when the processor resumes execution of the interrupted pro-
gram, the privilege mode, priority level, and condition codes are restored to what
they were when the interrupt occurred.
The microarchitecture of the LC-3 initiates an interrupt as follows: Recall
from Figure C.2 that in state 18, while MAR is loaded with the contents of PC
and PC is incremented, INT is tested.
State 18 is the only state in which the processor checks for interrupts. The
reason for only testing in state 18 is straightforward: Once an LC-3 instruction
starts processing, it is easier to let it nish its complete instruction cycle (FETCH,
DECODE, etc.) than to interrupt it in the middle and have to keep track of how far
along it was when the external device requested an interrupt (i.e., asserted INT).
If INT is only tested in state 18, the current instruction cycle can be aborted early
(even before the instruction has been fetched), and control directed to initiating
the interrupt.
The test is enabled by the control signals that make up COND5, which are
101 only in state 18, allowing the value of INT to pass through its four-input AND
gate, shown in Figure C.5, to contribute to the address of the next state. Since the
COND signals are not 101 in any other state, INT has no eect in any other state.
In state 18, the ten microsequencer control bits are as follows:
IRD/0
; NO
COND/101
; Test for interrupts
J/100001
If INT = 1, a 1 is produced at the output of the AND gate, which in turn
makes the next state address not 100001, corresponding to state 33, but rather
110001, corresponding to state 49. This starts the initiation of the interrupt (see
Figure C.7).
Several functions are performed in state 49. The PSR, which contains the
privilege mode, priority level, and condition codes of the interrupted program,
are loaded into MDR, in preparation for pushing it onto the supervisor stack.
PSR[15] is cleared, reecting the change to Supervisor mode, since all inter-
rupt service routines execute in Supervisor mode. The three-bit priority level
and eight-bit interrupt vector (INTV) provided by the interrupting device are
recorded","{'page_number': 248, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The PSR, which contains the\nprivilege mode, priority level, and condition codes of the interrupted program,\nare loaded into MDR, in preparation for pushing it onto the supervisor stack.\nPSR[15] is cleared, reecting the change to Supervisor mode, since all inter-\nrupt service routines execute in Supervisor mode. The three-bit priority level\nand eight-bit interrupt vector (INTV) provided by the interrupting device are\nrecorded. PSR[10:8] is loaded with the priority level of the interrupting device.\nThe internal register Vector is loaded with INTV and the eight-bit register Table\nis loaded with x01 in preparation for accessing the interrupt vector table to obtain\nthe starting address of the interrupt service routine. Finally, the processor tests'}"
"The PSR, which contains the
privilege mode, priority level, and condition codes of the interrupted program,
are loaded into MDR, in preparation for pushing it onto the supervisor stack.
PSR[15] is cleared, reecting the change to Supervisor mode, since all inter-
rupt service routines execute in Supervisor mode. The three-bit priority level
and eight-bit interrupt vector (INTV) provided by the interrupting device are
recorded. PSR[10:8] is loaded with the priority level of the interrupting device.
The internal register Vector is loaded with INTV and the eight-bit register Table
is loaded with x01 in preparation for accessing the interrupt vector table to obtain
the starting address of the interrupt service routine. Finally, the processor tests","{'page_number': 248, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The PSR, which contains the\nprivilege mode, priority level, and condition codes of the interrupted program,\nare loaded into MDR, in preparation for pushing it onto the supervisor stack.\nPSR[15] is cleared, reecting the change to Supervisor mode, since all inter-\nrupt service routines execute in Supervisor mode. The three-bit priority level\nand eight-bit interrupt vector (INTV) provided by the interrupting device are\nrecorded. PSR[10:8] is loaded with the priority level of the interrupting device.\nThe internal register Vector is loaded with INTV and the eight-bit register Table\nis loaded with x01 in preparation for accessing the interrupt vector table to obtain\nthe starting address of the interrupt service routine. Finally, the processor tests'}"
"the old PSR[15] to determine whether the stack pointers must be adjusted before
pushing PSR and PC.
If the old PSR[15] = 0, the processor is already operating in Supervisor mode.
R6 is the Supervisor Stack Pointer (SSP), so the processor proceeds immediately
to states 37 and 41 to push the PSR of the interrupted program onto the super-
visor stack. If PSR[15] = 1, the interrupted program was in User mode. In that
case, the User Stack Pointer (USP) must be saved in Saved USP and R6 must be
loaded with the contents of Saved SSP before moving to state 37. This is done in
state 45.
The control ow from state 49 to either 37 or 45 is enabled by the ten
microsequencer control bits, as follows:
IRD/0
; NO
COND/100
; Test PSR[15], privilege mode
J/100101
If PSR[15] = 0, control goes to state 37 (100101); if PSR[15] = 1, control
goes to state 45 (101101).
In state 37, R6 (the SSP) is decremented (preparing for the push), and MAR
is loaded with the address of the new top of the stack.
In state 41, the memory is enabled to WRITE (MIO.EN/YES, R.W/WR).
When the write completes, signaled by R = 1, PSR has been pushed onto the
supervisor stack, and the ow moves on to state 43.
In state 43, the PC is loaded into MDR. Note that state 43 says MDR is loaded
with PC-1. Recall that in state 18, at the beginning of the instruction cycle for the
interrupted instruction, PC was incremented. Loading MDR with PC-1 adjusts
PC to the correct address of the interrupted program.
In states 46 and 52, the same sequence as in states 37 and 41 occurs, only
this time the PC of the interrupted program is pushed onto the supervisor stack.
The nal task to complete the initiation of the interrupt is to load the PC
with the starting address of the interrupt service routine. This is carried out by
states 54, 53, and 55. It is accomplished in a manner similar to the loading of
the PC with the starting address of a TRAP service routine. The event causing
the INT request supplies the eight-bit interrupt vector INTV associated with the
interrupt, similar to the eight-bit trap vector contained in the TRAP instruction.
This interrupt vector is stored in the eight-bit register INTV, shown on the data
path in Figure C.8.
The interrupt vector table occupies memory locations x0100 to x01FF. In
state 54, the interrupt vector that was loaded into Vector in state 49 is combined
with the base address of the interrupt vector table (x0100) and loaded into MAR.
In state 53, memory is READ. When R = 1, the read has completed, and MDR
contains the starting address of the interrupt service routine. In state 55, the PC
is loaded with that starting address, completing the initiation of the interrupt","{'page_number': 249, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The interrupt vector table occupies memory locations x0100 to x01FF. In\nstate 54, the interrupt vector that was loaded into Vector in state 49 is combined\nwith the base address of the interrupt vector table (x0100) and loaded into MAR.\nIn state 53, memory is READ. When R = 1, the read has completed, and MDR\ncontains the starting address of the interrupt service routine. In state 55, the PC\nis loaded with that starting address, completing the initiation of the interrupt.\nIt is important to emphasize that the LC-3 supports two stacks, one for each\nprivilege mode, and two stack pointers (USP and SSP), one for each stack. R6 is\nthe stack pointer and is loaded from the Saved SSP when privilege changes from\nUser mode to Supervisor mode, and from Saved USP when privilege changes'}"
"The interrupt vector table occupies memory locations x0100 to x01FF. In
state 54, the interrupt vector that was loaded into Vector in state 49 is combined
with the base address of the interrupt vector table (x0100) and loaded into MAR.
In state 53, memory is READ. When R = 1, the read has completed, and MDR
contains the starting address of the interrupt service routine. In state 55, the PC
is loaded with that starting address, completing the initiation of the interrupt.
It is important to emphasize that the LC-3 supports two stacks, one for each
privilege mode, and two stack pointers (USP and SSP), one for each stack. R6 is
the stack pointer and is loaded from the Saved SSP when privilege changes from
User mode to Supervisor mode, and from Saved USP when privilege changes","{'page_number': 249, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'The interrupt vector table occupies memory locations x0100 to x01FF. In\nstate 54, the interrupt vector that was loaded into Vector in state 49 is combined\nwith the base address of the interrupt vector table (x0100) and loaded into MAR.\nIn state 53, memory is READ. When R = 1, the read has completed, and MDR\ncontains the starting address of the interrupt service routine. In state 55, the PC\nis loaded with that starting address, completing the initiation of the interrupt.\nIt is important to emphasize that the LC-3 supports two stacks, one for each\nprivilege mode, and two stack pointers (USP and SSP), one for each stack. R6 is\nthe stack pointer and is loaded from the Saved SSP when privilege changes from\nUser mode to Supervisor mode, and from Saved USP when privilege changes'}"
"from Supervisor mode to User mode. When the privilege mode changes, the cur-
rent value in R6 must be stored in the appropriate Saved stack pointer in order
to be available the next time the privilege mode changes back.
C.7.2 Returning from an Interrupt or Trap Service Routine, RTI
Interrupt service routines, like trap service routines already described, end with
the execution of the RTI instruction. The job of the RTI instruction is to restore
the computer to the state it was in before the interrupt or trap service routine was
executed. This means restoring the PSR (i.e., the privilege mode, priority level,
and the values of the condition codes N, Z, P) and restoring the PC. These values
were pushed onto the stack during the initiation of the interrupt or execution of
the TRAP instruction. They must, therefore, be popped o the stack in the reverse
order.
The rst state after DECODE is state 8. Here we load the MAR with the
address of the top of the supervisor stack, which contains the last thing pushed
(that has not been subsequently popped)the state of the PC when the interrupt
was initiated. At the same time, we test PSR[15] since RTI is a privileged instruc-
tion and can only execute in Supervisor mode. If PSR[15] = 0, we can continue
to carry out the requirements of RTI.
States 36 and 38 restore PC to the value it had when the interrupt was initi-
ated. In state 36, the memory is read. When the read is completed, MDR contains
the address of the instruction that was to be processed next when the interrupt
occurred. State 38 loads that address into the PC.
States 39, 40, 42, and 34 restore the privilege mode, priority level, and con-
dition codes (N, Z, P) to their original values. In state 39, the Supervisor Stack
Pointer is incremented so that it points to the top of the stack after the PC was
popped. The MAR is loaded with the address of the new top of the stack. State
40 initiates the memory READ; when the READ is completed, MDR contains
the interrupted PSR. State 42 loads the PSR from MDR, and state 34 increments
the stack pointer.
The only thing left is to check the privilege mode of the interrupted pro-
gram to see whether the stack pointers have to be switched. In state 34, the
microsequencer control bits are as follows:
IRD/0
; NO
COND/100
; Test PSR[15], privilege mode
J/110011
If PSR[15] = 0, control ows to state 51 (110011) to do nothing for one cycle.
If PSR[15] = 1, control ows to state 59, where R6 is saved in Saved SSP and
R6 is loaded from Saved USP. In both cases, control returns to state 18 to begin
processing the next instruction.
C.7.3 Initiating an Exception
The LC-3 identies three cases where processing is not allowed to continue nor-
mally due to something going awry in the executing program. We refer to these
cases as exceptions","{'page_number': 250, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In both cases, control returns to state 18 to begin\nprocessing the next instruction.\nC.7.3 Initiating an Exception\nThe LC-3 identies three cases where processing is not allowed to continue nor-\nmally due to something going awry in the executing program. We refer to these\ncases as exceptions. They are initiated in the same way interrupts are initiated,'}"
"In both cases, control returns to state 18 to begin
processing the next instruction.
C.7.3 Initiating an Exception
The LC-3 identies three cases where processing is not allowed to continue nor-
mally due to something going awry in the executing program. We refer to these
cases as exceptions. They are initiated in the same way interrupts are initiated,","{'page_number': 250, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'In both cases, control returns to state 18 to begin\nprocessing the next instruction.\nC.7.3 Initiating an Exception\nThe LC-3 identies three cases where processing is not allowed to continue nor-\nmally due to something going awry in the executing program. We refer to these\ncases as exceptions. They are initiated in the same way interrupts are initiated,'}"
"by pushing the PSR and PC onto the system stack, obtaining the starting address
of the exception service routine from the interrupt vector table, and loading that
address into the PC to initiate the exception service routine.
The three exceptions identied in the LC-3 are (1) a privileged mode excep-
tion caused by the program attempting to execute the RTI instruction while in
User mode, (2) the illegal opcode exception caused by the program trying to exe-
cute an instruction whose opcode is 1101, and (3) an access control violation
(ACV) exception caused by the program trying to access a privileged memory
location while in User mode.
C.7.3.1 Privilege Mode Exception
If the processor is in User mode (PSR[15] = 1) and is attempting to execute
RTI, a privilege mode exception occurs. The processor pushes the PSR and the
address of the RTI instruction onto the supervisor stack and loads the PC with
the starting address of the service routine that handles privilege mode violations.
Figure C.7 shows the ow, starting with a branch from state 8 to state 44 if
PSR[15] = 1.
In state 44, the eight-bit Table register is loaded with x01, indicating the
address of an entry in the interrupt vector table, and the eight-bit Vector register
is loaded with x00, indicating the rst entry in the interrupt vector table. The con-
tents of x0100 is the starting address of the service routine that handles privilege
mode exceptions. The MDR is loaded with the PSR of the program that caused
the exception in preparation for pushing it onto the system stack. Finally, PSR[15]
is set to 0, since the service routine will execute with supervisor privileges. Then
the processor moves to state 45, where it follows the same ow as the initiation
of interrupts.
The main dierence between this ow and that for the initiation of interrupts
is in state 54, where MAR is loaded with x01Vector. In the case of interrupts,
Vector is loaded in state 49 with INTV, which is supplied by the interrupting
device. In the case of the privilege mode violation, Vector is loaded in state
44 with x00.
There are two additional functions performed in state 49 that are not per-
formed in state 44. First, the priority level is changed, based on the priority of
the interrupting device. We do not change the priority in handling a privilege
mode violation. The service routine executes at the same priority as the program
that caused the violation. Second, a test to determine the privilege mode is per-
formed for an interrupt. This is unnecessary for a privilege mode violation since
the processor already knows it is executing in User mode.
C.7.3.2 Illegal Opcode Exception
Although it would be a rare situation, it is possible, we suppose, that a pro-
grammer writing a program in machine language could mistakenly include an
instruction having opcode = 1101. Since there is no such opcode in the LC-3 ISA,
the computer cannot process that instruction. State 32 performs the DECODE,
and the next state is state 13.","{'page_number': 251, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'by pushing the PSR and PC onto the system stack, obtaining the starting address\nof the exception service routine from the interrupt vector table, and loading that\naddress into the PC to initiate the exception service routine.\nThe three exceptions identied in the LC-3 are (1) a privileged mode excep-\ntion caused by the program attempting to execute the RTI instruction while in\nUser mode, (2) the illegal opcode exception caused by the program trying to exe-\ncute an instruction whose opcode is 1101, and (3) an access control violation\n(ACV) exception caused by the program trying to access a privileged memory\nlocation while in User mode.\nC.7.3.1 Privilege Mode Exception\nIf the processor is in User mode (PSR[15] = 1) and is attempting to execute\nRTI, a privilege mode exception occurs. The processor pushes the PSR and the\naddress of the RTI instruction onto the supervisor stack and loads the PC with\nthe starting address of the service routine that handles privilege mode violations.\nFigure C.7 shows the ow, starting with a branch from state 8 to state 44 if\nPSR[15] = 1.\nIn state 44, the eight-bit Table register is loaded with x01, indicating the\naddress of an entry in the interrupt vector table, and the eight-bit Vector register\nis loaded with x00, indicating the rst entry in the interrupt vector table. The con-\ntents of x0100 is the starting address of the service routine that handles privilege\nmode exceptions. The MDR is loaded with the PSR of the program that caused\nthe exception in preparation for pushing it onto the system stack. Finally, PSR[15]\nis set to 0, since the service routine will execute with supervisor privileges. Then\nthe processor moves to state 45, where it follows the same ow as the initiation\nof interrupts.\nThe main dierence between this ow and that for the initiation of interrupts\nis in state 54, where MAR is loaded with x01Vector. In the case of interrupts,\nVector is loaded in state 49 with INTV, which is supplied by the interrupting\ndevice. In the case of the privilege mode violation, Vector is loaded in state\n44 with x00.\nThere are two additional functions performed in state 49 that are not per-\nformed in state 44. First, the priority level is changed, based on the priority of\nthe interrupting device. We do not change the priority in handling a privilege\nmode violation. The service routine executes at the same priority as the program\nthat caused the violation. Second, a test to determine the privilege mode is per-\nformed for an interrupt. This is unnecessary for a privilege mode violation since\nthe processor already knows it is executing in User mode.\nC.7.3.2 Illegal Opcode Exception\nAlthough it would be a rare situation, it is possible, we suppose, that a pro-\ngrammer writing a program in machine language could mistakenly include an\ninstruction having opcode = 1101. Since there is no such opcode in the LC-3 ISA,\nthe computer cannot process that instruction. State 32 performs the DECODE,\nand the next state is state 13.'}"
"The action the processor takes is very similar to that of a privilege mode
exception. The PSR and PC of the program are pushed onto the supervisor stack,
and the PC is loaded with the starting address of the Illegal Opcode exception
service routine.
State 13 is very similar to state 44, which starts the initiation of a privilege
mode exception. There are two dierences: (1) Vector is loaded with x01, since
the starting address of the service routine for the illegal opcode exception is in
x0101. (2) In the case of the privilege mode exception, we know the program is
in User mode when the processor attempts to execute the RTI instruction. In the
case of an illegal opcode, the processor can be in either mode, so from state 13
the processor goes to state 37 or state 45, depending on whether the program is
executing in Supervisor mode or User mode when the illegal opcode instruction
is encountered.
Like state 44, the priority of the running program is not changed, since the
urgency of handling the exception is the same as the urgency of executing the
program that contains it. Like state 49, state 13 tests the privilege mode of the
program that contains the illegal opcode, since if the currently executing pro-
gram is in User mode, the stack pointers need to be switched as described in
Section C.7.1. Like state 49, the processor then microbranches either to state 37
if the stack pointer is already pointing to the supervisor stack, or to state 45 if the
stack pointers have to be switched. From there, the initiating sequence continues
in states 37, 41, 43, etc., identical to what happens when an interrupt is initiated
(Section C.7.1) or a privilege mode exception is initiated (Section C.7.3.1). The
PSR and PC are pushed onto the supervisor stack and the starting address of the
service routine is loaded into the PC, completing the initiation of the exception.
C.7.3.3 Access Control Violation (ACV) Exception
An Access Control Violation (ACV) exception occurs if the processor attempts
to access privileged memory while operating in User mode. The state machine
checks for this in every case where the processor accesses memory, that is, in
states 17, 19, 23, 33, and 35. If an ACV violation occurs, the next state is respec-
tively states 56, 61, 48, 60, or 57 (see Figure C.2). In all ve states, the processor
loads Table with x01, Vector with x02, MDR with the PSR, sets PSR[15] to 0,
exactly like state 44, with one exception. Vector is set to x02 since the starting
address of the ACV exception service routine is in memory location x0102. Pro-
cessing continues exactly like in state 44, moving rst to state 45 to switch to the
system stack, and then pushing PSR and PC onto the stack and loading the PC
with the starting address of the service routine.
C.8 Control Store
Figure C.9 completes our microprogrammed implementation of the LC-3. It
shows the contents of each location of the control store, corresponding to the
52 control signals required by each state of the state machine","{'page_number': 252, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Pro-\ncessing continues exactly like in state 44, moving rst to state 45 to switch to the\nsystem stack, and then pushing PSR and PC onto the stack and loading the PC\nwith the starting address of the service routine.\nC.8 Control Store\nFigure C.9 completes our microprogrammed implementation of the LC-3. It\nshows the contents of each location of the control store, corresponding to the\n52 control signals required by each state of the state machine. We have left the\nexact entries blank to allow you, the reader, the joy of lling in the required signals\nyourself. The solution is available from your instructor.'}"
"Pro-
cessing continues exactly like in state 44, moving rst to state 45 to switch to the
system stack, and then pushing PSR and PC onto the stack and loading the PC
with the starting address of the service routine.
C.8 Control Store
Figure C.9 completes our microprogrammed implementation of the LC-3. It
shows the contents of each location of the control store, corresponding to the
52 control signals required by each state of the state machine. We have left the
exact entries blank to allow you, the reader, the joy of lling in the required signals
yourself. The solution is available from your instructor.","{'page_number': 252, 'textbook_name': 'Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems', 'text': 'Pro-\ncessing continues exactly like in state 44, moving rst to state 45 to switch to the\nsystem stack, and then pushing PSR and PC onto the stack and loading the PC\nwith the starting address of the service routine.\nC.8 Control Store\nFigure C.9 completes our microprogrammed implementation of the LC-3. It\nshows the contents of each location of the control store, corresponding to the\n52 control signals required by each state of the state machine. We have left the\nexact entries blank to allow you, the reader, the joy of lling in the required signals\nyourself. The solution is available from your instructor.'}"
