[
  {
    "text": "Why the Book Happened\nThis textbook evolved from EECS 100, the rst computing course for computer\nscience, computer engineering, and electrical engineering majors at the Univer-\nsity of Michigan, Ann Arbor, that Kevin Compton and the rst author introduced\nfor the rst time in the fall term, 1995.\nEECS 100 happened at Michigan because Computer Science and Engi-\nneering faculty had been dissatised for many years with the lack of student\ncomprehension of some very basic concepts. For example, students had a lot\nof trouble with pointer variables. Recursion seemed to be magic, beyond\nunderstanding.\nWe decided in 1993 that the conventional wisdom of starting with a high-\nlevel programming language, which was the way we (and most universities) were\n",
    "page_number": 0,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "doing it, had its shortcomings. We decided that the reason students were not get-\nting it was that they were forced to memorize technical details when they did not\nunderstand the basic underpinnings.\nOur result was the bottom-up approach taken in this book, where we contin-\nually build on what the student already knows, only memorizing when absolutely\nnecessary. We did not endorse then and we do not endorse now the popular\ninformation hiding approach when it comes to learning. Information hiding is a\nuseful productivity enhancement technique after one understands what is going on.\nBut until one gets to that point, we insist that information hiding gets in the way of\nunderstanding. Thus, we continually build on what has gone before so that nothing\nis magic and everything can be tied to the foundation that has already been laid.\nWe should point out that we do not disagree with the notion of top-down\ndesign. On the contrary, we believe strongly that top-down design is correct\ndesign. But there is a clear dierence between how one approaches a design prob-\nlem (after one understands the underlying building blocks) and what it takes to get\nto the point where one does understand the building blocks. In short, we believe\nin top-down design, but bottom-up learning for understanding.\nMajor Changes in the Third Edition\nThe LC-3\nA hallmark of our book continues to be the LC-3 ISA, which is small enough to\nbe described in a few pages and hopefully mastered in a very short time, yet rich\nenough to convey the essence of what an ISA provides. It is the LC 3 because\nit took us three tries to get it right. Four tries, actually, but the two changes in the\nLC-3 ISA since the second edition (i.e., changes to the LEA instruction and to the\nTRAP instruction) are so minor that we decided not to call the slightly modied\nISA the LC-4.\nThe LEA instruction no longer sets condition codes. It used to set condition\ncodes on the mistaken belief that since LEA stands for Load Eective Address,\nit should set condition codes like LD, LDI, and LDR do. We recognize now that\nthis reason was silly. LD, LDI, and LDR load a register from memory, and so\nthe condition codes provide useful information  whether the value loaded is\nnegative, zero, or positive. LEA loads an address into a register, and for that, the\ncondition codes do not really provide any value. Legacy code written before this\nchange should still run correctly.\nThe TRAP instruction no longer stores the linkage back to the calling pro-\ngram in R7. Instead, the PC and PSR are pushed onto the system stack and popped\nby the RTI instruction (renamed Return from Trap or Interrupt) as the last instruc-\ntion in a trap routine. Trap routines now execute in privileged memory (x0000 to\nx2FFF). This change allows trap routines to be re-entrant. It does not aect old\ncode provided the starting address of the trap service routines, obtained from the\nTrap Vector Table, is in privileged memory and the terminating instruction of\neach trap service routine is changed from RET to RTI.\nAs before, Appendix A species the LC-3 completely.\n",
    "page_number": 1,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "The Addition of C++\nWeve had an ongoing debate about how to extend our approach and textbook\nto C++. One of the concerns about C++ is that many of its language features\nare too far abstracted from the underlying layers to make for an easy t to our\napproach. Another concern is that C++ is such a vast language that any adequate\ncoverage would require an additional thousand pages. We also didnt want to drop\nC completely, as it serves as a de facto development language for systems and\nhardware-oriented projects.\nWe adopted an approach where we cover the common core of C and C++\nfrom Chapters 11 through 19. This common core is similar to what was covered\nin the second edition, with some minor updates. Chapter 20 serves as a transition,\nwhich we aspired to make very smooth, to the core concepts of C++. With this\napproach, we get to explore the evolution between C and C++, which serves as\na key learning opportunity on what changes were essential to boost programmer\nproductivity.\nIn particular, we focus on classes in C++ as an evolution from structures in\nC. We discuss classes as a compiler construct, how method calls are made, and\nthe notion of constructors. We touch upon inheritance, too, but leave the details\nfor subsequent treatment in follow-on courses.\nAn important element of C++ is the introduction of container classes in the\nStandard Template Library, which is a heavily utilized part of the C++ language.\nThis provides an opportunity to dive deep into the vector class, which serves as\na continuation of a running example in the second half around the support for\nvariable-sized arrays in high-level languages, or in particular, Cs lack of support\nfor them.\nOther Important Updates\nAlthough no chapter in the book has remained untouched, some chapters have\nbeen changed more than others. In Chapter 2, we expanded the coverage of the\noating point data type and the conversion of fractions between decimal and\nbinary numbers in response to several instructors who wanted them. We moved\nDeMorgans Laws from Chapter 3 to Chapter 2 because the concept is really about\nAND and OR functions and not about digital logic implementation. In Chap-\nter 3, we completely overhauled the description of state, latches, ip-ops, nite\nstate machines, and our example of a danger sign. We felt the explanations in the\nsecond edition were not as clear as they needed to be, and the concepts are too\nimportant to not get right. We revised Chapter 4 to better introduce the LC-3,\nincluding a dierent set of instructions, leading to our rst complete example of\na computer program.\nOur organization of Chapters 8, 9, and 10 was completely overhauled in order\nto present essentially the same material in a more understandable way. Although\nmost of our treatment of data structures waits until we have introduced C in the\nsecond half of the book, we felt it was important to introduce stacks, queues,\nand character strings as soon as the students have moved out of programming in\nmachine language so they can write programs dealing with these data structures\n",
    "page_number": 2,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "and see how these structures are actually organized in memory. We moved our dis-\ncussion of subroutines up to Chapter 8 because of their importance in constructing\nricher programs.\nWe also introduced recursion in Chapter 8, although its main treatment is still\nleft for the second half of the book. Both the expressive power of recursion and\nits misuse are so common in undergraduate curricula that we felt dealing with\nit twice, rst while they are engrossed in the bowels of assembly language and\nagain after moving up to the richness of C, was worthwhile.\nChapter 9 now covers all aspects of I/O in one place, including polling and\ninterrupt-driven I/O. Although the concept of privilege is present in the second\nedition, we have put greater emphasis on it in the third edition. Our coverage\nof system calls (the trap routines invoked by the TRAP instruction) appears in\nChapter 9. All of the above reduce Chapter 10 to simply a comprehensive example\nthat pulls together a lot of the rst half of the book: the simulation of a calculator.\nDoing so requires 12 subroutines that are laid out in complete detail. Two con-\ncepts that are needed to make this happen are stack arithmetic and ASCII/binary\nconversion, so they are included in Chapter 10.\nWe reworked all the examples in Chapters 11 through 19 to use the latest\nANSI Standard C or C18. We also added more coding examples to further empha-\nsize points and to provide clarity on complex topics such as pointers, arrays,\nrecursion, and pointers to pointers in C. In Chapter 16, we added additional\nsections on variable-sized arrays in C, and on multidimensional arrays.\nChapter Organization\nThe book breaks down into two major segments, (a) the underlying structure\nof a computer, as manifested in the LC-3; and (b) programming in a high-level\nlanguage, in our case C and C++.\nThe LC-3\nWe start with the underpinnings that are needed to understand the workings of a\nreal computer. Chapter 2 introduces the bit and arithmetic and logical operations\non bits. Then we begin to build the structure needed to understand the LC-3.\nChapter 3 takes the student from an MOS transistor, step by step, to a real\nmemory and a nite state machine.\nOur real memory consists of four words of three bits each, rather than\n16 gigabytes, which is common in most laptops today. Its description ts on a\nsingle page (Figure 3.20), making it easy for a student to grasp. By the time stu-\ndents get there, they have been exposed to all the elements needed to construct the\nmemory. The nite state machine is needed to understand how a computer pro-\ncesses instructions, starting in Chapter 4. Chapter 4 introduces the von Neumann\nexecution model and enough LC-3 instructions to allow an LC-3 program to be\nwritten. Chapter 5 introduces most of the rest of the LC-3 ISA.\n",
    "page_number": 3,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "The LC-3 is a 16-bit architecture that includes physical I/O via keyboard\nand monitor, TRAPs to the operating system for handling service calls, con-\nditional branches on (N, Z, and P) condition codes, a subroutine call/return\nmechanism, a minimal set of operate instructions (ADD, AND, and NOT), and\nvarious addressing modes for loads and stores (direct, indirect, Base+oset).\nChapter 6 is devoted to programming methodology (stepwise renement)\nand debugging, and Chapter 7 is an introduction to assembly language program-\nming. We have developed a simulator and an assembler for the LC-3 that runs on\nWindows, Linux, and Mac0S platforms. It can be downloaded from the web at\nno charge.\nStudents use the simulator to test and debug programs written in LC-3\nmachine language and in LC-3 assembly language. The simulator allows online\ndebugging (deposit, examine, single-step, set breakpoint, and so on). The sim-\nulator can be used for simple LC-3 machine language and assembly language\nprogramming assignments, which are essential for students to master the concepts\npresented throughout the rst ten chapters.\nAssembly language is taught, but not to train expert assembly language pro-\ngrammers. Indeed, if the purpose was to train assembly language programmers,\nthe material would be presented in an upper-level course, not in an introductory\ncourse for freshmen. Rather, the material is presented in Chapter 7 because it\nis consistent with the paradigm of the book. In our bottom-up approach, by the\ntime the student reaches Chapter 7, he/she can handle the process of transform-\ning assembly language programs to sequences of 0s and 1s. We go through the\nprocess of assembly step by step for a very simple LC-3 Assembler. By hand\nassembling, the student (at a very small additional cost in time) reinforces the\nimportant fundamental concept of translation.\nIt is also the case that assembly language provides a user-friendly notation\nto describe machine instructions, something that is particularly useful for writing\nprograms in Chapters 8, 9, and 10, and for providing many of the explanations in\nthe second half of the book. Starting in Chapter 11, when we teach the semantics\nof C statements, it is far easier for the reader to deal with ADD R1, R2, R3 than\nto have to struggle with 0001001010000011.\nChapter 8 introduces three important data structures: the stack, the queue,\nand the character string, and shows how they are stored in memory. The sub-\nroutine call/return mechanism of the LC-3 is presented because of its usefulness\nboth in manipulating these data structures and in writing programs. We also intro-\nduce recursion, a powerful construct that we revisit much more thoroughly in the\nsecond half of the book (in Chapter 17), after the student has acquired a much\nstronger capability in high-level language programming. We introduce recursion\nhere to show by means of a few examples the execution-time tradeos incurred\nwith recursion as a rst step in understanding when its use makes sense and when\nit doesnt.\nChapter 9 deals with input/output and some basic interaction between the\nprocessor and the operating system. We introduce the notions of priority and\nprivilege, which are central to a systems environment. Our treatment of I/O is\nall physical, using keyboard data and status registers for input and display data\nand status registers for output. We describe both interrupt-driven I/O and I/O\n",
    "page_number": 4,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "under program control. Both are supported by our LC-3 simulator so the student\ncan write interrupt drivers. Finally, we show the actual LC-3 code of the trap ser-\nvice routines that the student has invoked with the TRAP instruction starting in\nChapter 4. To handle interrupt-driven I/O and trap service routines, we complete\nthe description of the LC-3 ISA with details of the operation of the Return from\nTrap or Interrupt (RTI) and TRAP instructions.\nThe rst half of the book concludes with Chapter 10, a comprehensive exam-\nple of a simple calculator that pulls together a lot of what the students have learned\nin Chapters 1 through 9.\nProgramming in C and C++\nBy the time the student gets to the second part of the textbook, he/she has an\nunderstanding of the layers below. In our coverage of programming in C and\nC++, we leverage this foundation by showing the resulting LC-3 code generated\nby a compiler with each new concept in C/C++.\nWe start with the C language because it provides the common, essential\ncore with C++. The C programming language ts very nicely with our bottom-\nup approach. Its low-level nature allows students to see clearly the connection\nbetween software and the underlying hardware. In this book, we focus on basic\nconcepts such as control structures, functions, and arrays. Once basic program-\nming concepts are mastered, it is a short step for students to learn more advanced\nconcepts such as objects and abstraction in C++.\nEach time a new high-level construct is introduced, the student is shown\nthe LC-3 code that a compiler would produce. We cover the basic constructs of\nC (variables, operators, control, and functions), pointers, arrays, recursion, I/O,\ncomplex data structures, and dynamic allocation. With C++, we cover some basic\nimprovements over C, classes, and containers.\nChapter 11 is a gentle introduction to high-level programming languages. At\nthis point, students have dealt heavily with assembly language and can understand\nthe motivation behind what high-level programming languages provide. Chapter\n11 also contains a simple C program, which we use to kick-start the process of\nlearning C.\nChapter 12 deals with values, variables, constants, and operators. Chapter 13\nintroduces C control structures. We provide many complete program examples\nto give students a sample of how each of these concepts is used in practice. LC-3\ncode is used to demonstrate how each C construct aects the machine at the lower\nlevels.\nChapter 14 introduces functions in C. Students are not merely exposed to the\nsyntax of functions. Rather they learn how functions are actually executed, with\nargument-passing using a run-time stack. A number of examples are provided.\nIn Chapter 15, students are exposed to techniques for testing their code, along\nwith debugging high-level source code. The ideas of white-box and black-box\ntesting are discussed.\nChapter 16 teaches pointers and arrays, relying heavily on the students\nunderstanding of how memory is organized. We discuss Cs notions of xed size\nand variable-length arrays, along with multidimensional array allocation.\n",
    "page_number": 5,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Chapter 17 teaches recursion, using the students newly gained knowledge of\nfunctions, stack frames, and the run-time stack. Chapter 18 introduces the details\nof I/O functions in C, in particular, streams, variable length argument lists, and\nhow C I/O is aected by the various format specications. This chapter relies on\nthe students earlier exposure to physical I/O in Chapter 8. Chapter 19 discusses\nstructures in C, dynamic memory allocation, and linked lists.\nChapter 20 provides a jump-start on C++ programming by discussing its\nroots in C and introducing the idea of classes as a natural evolution from struc-\ntures. We also cover the idea of containers in the standard template library, to\nenable students to quickly jump into productive programming with C++.\nAlong the way, we have tried to emphasize good programming style and cod-\ning methodology by means of examples. Novice programmers probably learn at\nleast as much from the programming examples they read as from the rules they\nare forced to study. Insights that accompany these examples are highlighted by\nmeans of lightbulb icons that are included in the margins.\nWe have found that the concept of pointer variables (Chapter 16) is not at all\na problem. By the time students encounter it, they have a good understanding of\nwhat memory is all about, since they have analyzed the logic design of a small\nmemory (Chapter 3). They know the dierence, for example, between a memory\nlocations address and the data stored there.\nRecursion ceases to be magic since, by the time a student gets to that point\n(Chapter 17), he/she has already encountered all the underpinnings. Students\nunderstand how stacks work at the machine level (Chapter 8), and they understand\nthe call/return mechanism from their LC-3 machine language programming expe-\nrience, and the need for linkages between a called program and the return to the\ncaller (Chapter 8). From this foundation, it is not a large step to explain functions\nby introducing run-time stack frames (Chapter 14), with a lot of the mystery about\nargument passing, dynamic declarations, and so on, going away. Since a function\ncan call a function, it is one additional small step (certainly no magic involved)\nfor a function to call itself.\nThe Simulator/Debugger\nThe importance of the Simulator/Debugger for testing the programs a student\nwrites cannot be overemphasized. We believe strongly that there is no substi-\ntute for hands-on practice testing ones knowledge. It is incredibly fullling\nto a students education to write a program that does not work, testing it to\nnd out why it does not work, xing the bugs himself/herself, and then see-\ning the program run correctly. To that end, the Simulator/Debugger has been\ncompletely rewritten. It runs on Windows, Linux, and MacOS while present-\ning the same user interface (GUI) regardless of which platform the student is\nusing. We have improved our incorporation of interrupt-driven I/O into the Sim-\nulators functionality so students can easily write interrupt drivers and invoke\nthem by interrupting a lower priority executing program. ...in their rst course in\ncomputing!\n",
    "page_number": 6,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "education, but we feel they are better suited to a later course in computer\narchitecture and design. This book is not intended for that purpose.\nWhy LC-3, and Not ARM or RISCV?\nWe have been asked why we invented the LC-3 ISA, rather than going with ARM,\nwhich seems to be the ISA of choice for most mobile devices, or RISCV, which\nhas attracted substantial interest over the last few years.\nThere are many reasons. First, we knew that the ISA we selected would\nbe the students rst ISA, not his/her last ISA. Between the freshman year and\ngraduation, the student is likely to encounter several ISAs, most of which are in\ncommercial products: ARM, RISCV, x86, and POWER, to name a few.\nBut all the commercial ISAs have details that have no place in an introductory\ncourse but still have to be understood for the student to use them eectively. We\ncould, of course, have subset an existing ISA, but that always ends up in questions\nof what to take out and what to leave in with a result that is not as clean as one\nwould think at rst blush. Certainly not as clean as what one can get when starting\nfrom scratch. It also creates an issue whenever the student uses an instruction in\nan exam or on an assignment that is not in the subset. Not very clean from a\npedagogical sense.\nWe wanted an ISA that was clean with no special cases to deal with, with as\nfew opcodes as necessary so the student could spend almost all his/her time on\nthe fundamental concepts in the course and very little time on the nuances of the\ninstruction set. The formats of all instructions in the LC-3 t on a single page.\nAppendix A provides all the details (i.e., the complete data sheet) of the entire\nLC-3 ISA in 25 pages.\nWe also wanted an instruction set that in addition to containing only a few\ninstructions was very rich in the breadth of what it embraced. So, we came up\nwith the LC-3, an instruction set with only 15 four-bit opcodes, a small enough\nnumber that students can absorb the ISA without even trying. For arithmetic, we\nhave only ADD instead of ADD, SUB, MUL, and DIV. For logical operations,\nwe have AND and NOT, foregoing OR, XOR, etc. We have no shift or rotate\ninstructions. In all these cases, the missing opcodes can be implemented with\nprocedures using the few opcodes that the LC-3 provides. We have loads and\nstores with three dierent addressing modes, each addressing mode useful for a\ndierent purpose. We have conditional branches, subroutine calls, return from\ntrap or interrupt, and system calls (the TRAP instruction).\nIn fact, this sparse set of opcodes is a feature! It drives home the need for\ncreating more complex functionality out of simple operations, and the need for\nabstraction, both of which are core concepts in the book.\nMost importantly, we have found from discussions with hundreds of students\nthat starting with the LC-3 does not put them at a disadvantage in later courses.\nOn the contrary: For example, at one campus students were introduced to ARM in\nthe follow-on course, while at another campus, students were introduced to x86.\n",
    "page_number": 7,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "In both cases, students appreciated starting with the LC-3, and their subsequent\nintroduction to ARM or x86 was much easier as a result of their rst learning the\nfundamental concepts with the LC-3.\nA Few Observations\nHaving now taught the course more than 20 times between us, we note the\nfollowing:\nUnderstanding, Not Memorizing\nSince the course builds from the bottom up, we have found that less memorization\nof seemingly arbitrary rules is required than in traditional programming courses.\nStudents understand that the rules make sense since by the time a topic is taught,\nthey have an awareness of how that topic is implemented at the levels below\nit. This approach is good preparation for later courses in design, where under-\nstanding of and insights gained from fundamental underpinnings are essential to\nmaking the required design tradeos.\nThe Student Debugs the Students Program\nWe hear complaints from industry all the time about CS graduates not being able\nto program. Part of the problem is the helpful teaching assistant, who contributes\nfar too much of the intellectual content of the students program so the student\nnever has to really master the art. Our approach is to push the student to do the\njob without the teaching assistant (TA). Part of this comes from the bottom-up\napproach, where memorizing is minimized and the student builds on what he/she\nalready knows. Part of this is the simulator, which the student uses from the day\nhe/she writes his/her rst program. The student is taught debugging from his/her\nrst program and is required from the very beginning to use the debugging tools\nof the simulator to get his/her programs to work. The combination of the simulator\nand the order in which the subject material is taught results in students actually\ndebugging their own programs instead of taking their programs to the TA for\nhelp ... with the too-frequent result that the TAs end up writing the programs for\nthe students.\nPreparation for the Future: Cutting Through Protective Layers\nProfessionals who use computers in systems today but remain ignorant of what\nis going on underneath are likely to discover the hard way that the eectiveness\nof their solutions is impacted adversely by things other than the actual programs\nthey write. This is true for the sophisticated computer programmer as well as the\nsophisticated engineer.\nSerious programmers will write more ecient code if they understand what\nis going on beyond the statements in their high-level language. Engineers, and not\njust computer engineers, are having to interact with their computer systems today\n",
    "page_number": 8,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "come Aboard\n1.1 What We Will Try to Do\nWelcome to From Bits and Gates to C and Beyond. Our intent is to introduce\nyou over the next xxx pages to the world of computing. As we do so, we have\none objective above all others: to show you very clearly that there is no magic to\ncomputing. The computer is a deterministic systemevery time we hit it over the\nhead in the same way and in the same place (provided, of course, it was in the same\nstarting condition), we get the same response. The computer is not an electronic\ngenius; on the contrary, if anything, it is an electronic idiot, doing exactly what\nwe tell it to do. It has no mind of its own.\nWhat appears to be a very complex organism is really just a very large, sys-\ntematically interconnected collection of very simple parts. Our job throughout\nthis book is to introduce you to those very simple parts and, step-by-step, build the\ninterconnected structure that you know by the name computer. Like a house, we\nwill start at the bottom, construct the foundation rst, and then go on to add layer\nafter layer, as we get closer and closer to what most people know as a full-blown\ncomputer. Each time we add a layer, we will explain what we are doing, tying the\nnew ideas to the underlying fabric. Our goal is that when we are done, you will be\nable to write programs in a computer language such as C using the sophisticated\nfeatures of that language and to understand what is going on underneath, inside\nthe computer.\n1.2 How We Will Get There\nWe will start (in Chapter 2) by rst showing that any information processed by\nthe computer is represented by a sequence of 0s and 1s. That is, we will encode\nall information as sequences of 0s and 1s. For example, one encoding of the letter\na that is commonly used is the sequence 01100001. One encoding of the decimal\nnumber 35 is the sequence 00100011. We will see how to perform operations on\nsuch encoded information.\n",
    "page_number": 9,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Once we are comfortable with information represented as codes made up of\n0s and 1s and operations (addition, for example) being performed on these repre-\nsentations, we will begin the process of showing how a computer works. Starting\nin Chapter 3, we will note that the computer is a piece of electronic equipment\nand, as such, consists of electronic parts operated by voltages and interconnected\nby wires. Every wire in the computer, at every moment in time, is at either a high\nvoltage or a low voltage. For our representation of 0s and 1s, we do not specify\nexactly how high. We only care whether there is or is not a large enough voltage\nrelative to 0 volts to identify it as a 1. That is, the absence or presence of a rea-\nsonable voltage relative to 0 volts is what determines whether it represents the\nvalue 0 or the value 1.\nIn Chapter 3, we will see how the transistors that make up todays micro-\nprocessor (the heart of the modern computer) work. We will further see how\nthose transistors are combined into larger structures that perform operations,\nsuch as addition, and into structures that allow us to save information for later\nuse. In Chapter 4, we will combine these larger structures into the von Neumann\nmachine, a basic model that describes how a computer works. We will also begin\nto study a simple computer, the LC-3. We will continue our study of the LC-3 in\nChapter 5. LC-3 stands for Little Computer 3. We actually started with LC-1 but\nneeded two more shots at it before (we think) we got it right! The LC-3 has all\nthe important characteristics of the microprocessors that you may have already\nheard of, for example, the Intel 8088, which was used in the rst IBM PCs back\nin 1981. Or the Motorola 68000, which was used in the Macintosh, vintage 1984.\nOr the Pentium IV, one of the high-performance microprocessors of choice for\nthe PC in the year 2003. Or todays laptop and desktop microprocessors, the Intel\nCore processors  I3, I5, and I7. Or even the ARM microprocessors that are used\nin most smartphones today. That is, the LC-3 has all the important characteristics\nof these real microprocessors without being so complicated that it gets in the\nway of your understanding.\nOnce we understand how the LC-3 works, the next step is to program it, rst\nin its own language (Chapter 5 and Chapter 6), and then in a language called\nassembly language that is a little bit easier for humans to work with (Chap-\nter 7). Chapter 8 introduces representations of information more complex than a\nsimple number  stacks, queues, and character strings, and shows how to imple-\nment them. Chapter 9 deals with the problem of getting information into (input)\nand out of (output) the LC-3. Chapter 9 also deals with services provided to a\ncomputer user by the operating system. We conclude the rst half of the book\n(Chapter 10) with an extensive example, the simulation of a calculator, an app on\nmost smartphones today.\nIn the second half of the book (Chapters 1120), we turn our attention\nto high-level programming concepts, which we introduce via the C and C++\nprogramming languages. High-level languages enable programmers to more\neectively develop complex software by abstracting away the details of the under-\nlying hardware. C and C++ in particular oer a rich set of programmer-friendly\nconstructs, but they are close enough to the hardware that we can examine\nhow code is transformed to execute on the layers below. Our goal is to enable\nyou to write short, simple programs using the core parts of these programming\n",
    "page_number": 10,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "languages, all the while being able to comprehend the transformations required\nfor your code to execute on the underlying hardware.\nWell start with basic topics in C such as variables and operators (Chapter 12),\ncontrol structures (Chapter 13), and functions (Chapter 14). Well see that these are\nstraightforward extensions of concepts introduced in the first half of the textbook.\nWe then move on to programming concepts in Chapters 1519 that will enable\nus to create more powerful pieces of code: Testing and Debugging (Chapter 15),\nPointers and Arrays in C (Chapter 16), Recursion (Chapter 17), Input and Output in\nC (Chapter 18), and Data Structures in C (Chapter 19).\nChapters 20 is devoted to C++, which we present as an evolution of the\nC programming language. Because the C++ language was initially dened as\na superset of C, many of the concepts covered in Chapters 1119 directly map\nonto the C++ language. We will introduce some of the core notions in C++ that\nhave helped establish C++ as one of the most popular languages for developing\nreal-world software. Chapter 20 is our Introduction to C++.\nIn almost all cases, we try to tie high-level C and C++ constructs to the\nunderlying LC-3 so that you will understand what you demand of the computer\nwhen you use a particular construct in a C or C++ program.\n1.3 Two Recurring Themes\nTwo themes permeate this book that we as professors previously took for granted,\nassuming that everyone recognized their value and regularly emphasized them\nto students of engineering and computer science. However, it has become clear\nto us that from the git-go, we need to make these points explicit. So, we state\nthem here up front. The two themes are (a) the notion of abstraction and (b) the\nimportance of not separating in your mind the notions of hardware and software.\nTheir value to your development as an eective engineer or computer scien-\ntist goes well beyond your understanding of how a computer works and how to\nprogram it.\nThe notion of abstraction is central to all that you will learn and expect to\nuse in practicing your craft, whether it be in mathematics, physics, any aspect of\nengineering, or business. It is hard to think of any body of knowledge where the\nnotion of abstraction is not critical.\nThe misguided hardware/software separation is directly related to your\ncontinuing study of computers and your work with them.\nWe will discuss each in turn.\n1.3.1 The Notion of Abstraction\nThe use of abstraction is all around us. When we get in a taxi and tell the driver,\nTake me to the airport, we are using abstraction. If we had to, we could probably\ndirect the driver each step of the way: Go down this street ten blocks, and make\na left turn. And, when the driver got there, Now take this street ve blocks and\nmake a right turn. And on and on. You know the details, but it is a lot quicker to\njust tell the driver to take you to the airport.\n",
    "page_number": 11,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Even the statement Go down this street ten blocks  can be broken down\nfurther with instructions on using the accelerator, the steering wheel, watching\nout for other vehicles, pedestrians, etc.\nAbstraction is a technique for establishing a simpler way for a person to inter-\nact with a system, removing the details that are unnecessary for the person to\ninteract eectively with that system. Our ability to abstract is very much a pro-\nductivity enhancer. It allows us to deal with a situation at a higher level, focusing\non the essential aspects, while keeping the component ideas in the background.\nIt allows us to be more ecient in our use of time and brain activity. It allows us\nto not get bogged down in the detail when everything about the detail is working\njust ne.\nThere is an underlying assumption to this, however: when everything about\nthe detail is just ne. What if everything about the detail is not just ne? Then,\nto be successful, our ability to abstract must be combined with our ability to\nun-abstract. Some people use the word deconstructthe ability to go from the\nabstraction back to its component parts.\nTwo stories come to mind.\nThe rst involves a trip through Arizona the rst author made a long time\nago in the hottest part of the summer. At the time he was living in Palo Alto,\nCalifornia, where the temperature tends to be mild almost always. He knew\nenough to take the car to a mechanic before making the trip and tell him to check\nthe cooling system. That was the abstraction: cooling system. What he had not\nmastered was that the capability of a cooling system for Palo Alto, California,\nis not the same as the capability of a cooling system for the summer deserts of\nArizona. The result: two days in Deer Lodge, Arizona (population 3), waiting for\na head gasket to be shipped in.\nThe second story (perhaps apocryphal) is supposed to have happened during\nthe infancy of electric power generation. General Electric Co. was having trouble\nwith one of its huge electric power generators and did not know what to do. On\nthe front of the generator were lots of dials containing lots of information, and\nlots of screws that could be rotated clockwise or counterclockwise as the operator\nwished. Something on the other side of the wall of dials and screws was malfunc-\ntioning and no one knew what to do. As the story goes, they called in one of the\nearly giants in the electric power industry. He looked at the dials and listened to\nthe noises for a minute, then took a small screwdriver from his pocket and rotated\none screw 35 degrees counterclockwise. The problem immediately went away. He\nsubmitted a bill for $1000 (a lot of money in those days) without any elaboration.\nThe controller found the bill for two minutes work a little unsettling and asked\nfor further clarication. Back came the new bill:\nTurning a screw 35 degrees counterclockwise:\n$\n0.75\nKnowing which screw to turn and by how much:\n999.25\nIn both stories the message is the same. It is more ecient to think of entities\nas abstractions. One does not want to get bogged down in details unnecessarily.\nAnd as long as nothing untoward happens, we are OK. If there had been no trip\nto Arizona, the abstraction cooling system would have been sucient. If the\n",
    "page_number": 12,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "electric power generator never malfunctioned, there would have been no need for\nthe power engineering gurus deeper understanding.\nAs we will see, modern computers are composed of transistors. These tran-\nsistors are combined to form logic gatesan abstraction that lets us think in\nterms of 0s and 1s instead of the varying voltages on the transistors. A logic cir-\ncuit is a further abstraction of a combination of gates. When one designs a logic\ncircuit out of gates, it is much more ecient to not have to think about the inter-\nnals of each gate. To do so would slow down the process of designing the logic\ncircuit. One wants to think of the gate as a component. But if there is a problem\nwith getting the logic circuit to work, it is often helpful to look at the internal\nstructure of the gate and see if something about its functioning is causing the\nproblem.\nWhen one designs a sophisticated computer application program, whether it\nbe a new spreadsheet program, word processing system, or computer game, one\nwants to think of each of the components one is using as an abstraction. If one\nspent time thinking about the details of each component when it was not neces-\nsary, the distraction could easily prevent the total job from ever getting nished.\nBut when there is a problem putting the components together, it is often useful to\nexamine carefully the details of each component in order to uncover the problem.\nThe ability to abstract is the most important skill. In our view, one should\ntry to keep the level of abstraction as high as possible, consistent with getting\neverything to work eectively. Our approach in this book is to continually raise\nthe level of abstraction. We describe logic gates in terms of transistors. Once we\nunderstand the abstraction of gates, we no longer think in terms of transistors.\nThen we build larger structures out of gates. Once we understand these larger\nabstractions, we no longer think in terms of gates.\nThe Bottom Line\nAbstractions allow us to be much more ecient in dealing\nwith all kinds of situations. It is also true that one can be eective without under-\nstanding what is below the abstraction as long as everything behaves nicely. So,\none should not pooh-pooh the notion of abstraction. On the contrary, one should\ncelebrate it since it allows us to be more ecient.\nIn fact, if we never have to combine a component with anything else into a\nlarger system, and if nothing can go wrong with the component, then it is perfectly\nne to understand this component only at the level of its abstraction.\nBut if we have to combine multiple components into a larger system, we\nshould be careful not to allow their abstractions to be the deepest level of our\nunderstanding. If we dont know the components below the level of their abstrac-\ntions, then we are at the mercy of them working together without our intervention.\nIf they dont work together, and we are unable to go below the level of abstraction,\nwe are stuck. And that is the state we should take care not to nd ourselves in.\n1.3.2 Hardware vs. Software\nMany computer scientists and engineers refer to themselves as hardware people\nor software people. By hardware, they generally mean the physical computer and\n",
    "page_number": 13,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "all the specications associated with it. By software, they generally mean the pro-\ngrams, whether operating systems like Android, ChromeOS, Linux, or Windows,\nor database systems like Access, MongoDB, Oracle, or DB-terric, or applica-\ntion programs like Facebook, Chrome, Excel, or Word. The implication is that\nthe person knows a whole lot about one of these two things and precious little\nabout the other. Usually, there is the further implication that it is OK to be an\nexpert at one of these (hardware OR software) and clueless about the other. It is\nas if there were a big wall between the hardware (the computer and how it actu-\nally works) and the software (the programs that direct the computer to do their\nbidding), and that one should be content to remain on one side of that wall or\nthe other.\nThe power of abstraction allows us to usually operate at a level where we\ndo not have to think about the underlying layers all the time. This is a good thing.\nIt enables us to be more productive. But if we are clueless about the underlying\nlayers, then we are not able to take advantage of the nuances of those underlying\nlayers when it is very important to be able to.\nThat is not to say that you must work at the lower level of abstraction and not\ntake advantage of the productivity enhancements that the abstractions provide.\nOn the contrary, you are encouraged to work at the highest level of abstraction\navailable to you. But in doing so, if you are able to, at the same time, keep in\nmind the underlying levels, you will nd yourself able to do a much better job.\nAs you approach your study and practice of computing, we urge you to take\nthe approach that hardware and software are names for components of two parts\nof a computing system that work best when they are designed by people who take\ninto account the capabilities and limitations of both.\nMicroprocessor designers who understand the needs of the programs that\nwill execute on the microprocessor they are designing can design much more\neective microprocessors than those who dont. For example, Intel, AMD, ARM,\nand other major producers of microprocessors recognized a few years ago that a\nlarge fraction of future programs would contain video clips as part of e-mail,\nvideo games, and full-length movies. They recognized that it would be impor-\ntant for such programs to execute eciently. The result: most microprocessors\ntoday contain special hardware capability to process those video clips. Intel\ndened additional instructions, initially called their MMX instruction set, and\ndeveloped special hardware for it. Motorola, IBM, and Apple did essentially\nthe same thing, resulting in the AltiVec instruction set and special hardware to\nsupport it.\nA similar story can be told about software designers. The designer of a large\ncomputer program who understands the capabilities and limitations of the hard-\nware that will carry out the tasks of that program can design the program so it\nexecutes more eciently than the designer who does not understand the nature of\nthe hardware. One important task that almost all large software systems need to\ncarry out is called sorting, where a number of items have to be arranged in some\norder. The words in a dictionary are arranged in alphabetical order. Students in\na class are often graded based on a numerical order, according to their scores\non the nal exam. There is a large number of fundamentally dierent programs\none can write to arrange a collection of items in order. Donald Knuth, one of the\n",
    "page_number": 14,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "top computer scientists in the world, devoted 391 pages to the task in The Art\nof Computer Programming, vol. 3. Which sorting program works best is often\nvery dependent on how much the software designer is aware of the underlying\ncharacteristics of the hardware.\nThe Bottom Line\nWe believe that whether your inclinations are in the direction\nof a computer hardware career or a computer software career, you will be much\nmore capable if you master both. This book is about getting you started on the\npath to mastering both hardware and software. Although we sometimes ignore\nmaking the point explicitly when we are in the trenches of working through a\nconcept, it really is the case that each sheds light on the other.\nWhen you study data types, a software concept, in C (Chapter 12), you will\nunderstand how the nite word length of the computer, a hardware concept,\naects our notion of data types.\nWhen you study functions in C (Chapter 14), you will be able to tie the rules\nof calling a function with the hardware implementation that makes those rules\nnecessary.\nWhen you study recursion, a powerful algorithmic device (initially in\nChapter 8 and more extensively in Chapter 17), you will be able to tie it to the\nhardware. If you take the time to do that, you will better understand when the\nadditional time to execute a procedure recursively is worth it.\nWhen you study pointer variables in C (in Chapter 16), your knowledge of\ncomputer memory will provide a deeper understanding of what pointers pro-\nvide, and very importantly, when they should be used and when they should be\navoided.\nWhen you study data structures in C (in Chapter 19), your knowledge of com-\nputer memory will help you better understand what must be done to manipulate\nthe actual structures in memory eciently.\nWe realize that most of the terms in the preceding ve short paragraphs may\nnot be familiar to you yet. That is OK; you can reread this page at the end of the\nsemester. What is important to know right now is that there are important topics\nin the software that are very deeply interwoven with topics in the hardware. Our\ncontention is that mastering either is easier if you pay attention to both.\nMost importantly, most computing problems yield better solutions when the\nproblem solver has the capability of both at his or her disposal.\n1.4 A Computer System\nWe have used the word computer more than two dozen times in the preceding\npages, and although we did not say so explicitly, we used it to mean a system\nconsisting of the software (i.e., computer programs) that directs and species the\nprocessing of information and the hardware that performs the actual processing\nof information in response to what the software asks the hardware to do. When\nwe say performing the actual processing, we mean doing the actual additions,\nmultiplications, and so forth in the hardware that are necessary to get the job\n",
    "page_number": 15,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "done. A more precise term for this hardware is a central processing unit (CPU),\nor simply a processor or microprocessor. This textbook is primarily about the\nprocessor and the programs that are executed by the processor.\n1.4.1 A (Very) Little History for a (Lot) Better Perspective\nBefore we get into the detail of how the processor and the software associated\nwith it work, we should take a moment and note the enormous and unparalleled\nleaps of performance that the computing industry has made in the relatively short\ntime computers have been around. After all, it wasnt until the 1940s that the\nfirst computers showed their faces. One of the first computers was the ENIAC\n(the Electronic Numerical Integrator and Calculator), a general purpose electronic\ncomputer that could be reprogrammed for different tasks. It was designed and built\nin 19431945 attheUniversityofPennsylvaniabyPresperEckertandhiscolleagues.\nIt contained more than 17,000 vacuum tubes. It was approximately 8 feet high, more\nthan 100 feet wide, and about 3 feet deep (about 300 square feet of floor space). It\nweighed 30 tons and required 140 kW to operate. Figure 1.1 shows three operators\nprogramming the ENIAC by plugging and unplugging cables and switches.\nAbout 40 years and many computer companies and computers later, in the\nearly 1980s, the Burroughs A series was born. One of the dozen or so 18-inch\noards that comprise that machine is shown in Figure 1.2. Each board contained\nFigure 1.1\nThe ENIAC, designed and built at University of Pennsylvania, 194345.\ncHistorical/Getty Images\n",
    "page_number": 16,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure 1.2\nA processor board, vintage 1980s. Courtesy of Emilio Salguerio\nFast forward another 30 or so years and we nd many of todays computers on\ndesktops (Figure 1.3), in laptops (Figure 1.4), and most recently in smartphones\n(Figure 1.5). Their relative weights and energy requirements have decreased\nenormousl\nand the s eed at which the\nrocess information has also increased\neno\nhow\ncom\nFigu\nA de\ncJo\nFutu\nShutterstock\n",
    "page_number": 17,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "10\nc\nFigure 1.6\nA microprocessor. cPeter Gudella/Shutterstock\nThe integrated circuit packages that comprise modern digital computers have\nalso seen phenomenal improvement. An example of one of todays microproces-\nsors is shown in Figure 1.6. The rst microprocessor, the Intel 4004 in 1971,\ncontained 2300 transistors and operated at 106 KHz. By 1992, those numbers\nhad jumped to 3.1 million transistors at a frequency of 66 MHz on the Intel\nPentium microprocessor, an increase in both parameters of a factor of about 1000.\nTodays microprocessors contain upwards of ve billion transistors and can oper-\nate at upwards of 4 GHz, another increase in both parameters of about a factor\nof 1000.\nThis factor of one million since 1971 in both the number of transistors and\nthe frequency that the microprocessor operates at has had very important impli-\ncations. The fact that each operation can be performed in one millionth of the\ntime it took in 1971 means the microprocessor can do one million things today\nin the time it took to do one thing in 1971. The fact that there are more than a\nmillion times as many transistors on a chip means we can do a lot more things at\nthe same time today than we could in 1971.\nThe result of all this is we have today computers that seem able to understand\nthe languages people speak  English, Spanish, Chinese, for example. We have\ncomputers that seem able to recognize faces. Many see this as the magic of arti-\ncial intelligence. We will see as we get into the details of how a computer works\nthat much of what appears to be magic is really due to how blazingly fast very\nsimple mindless operations (many at the same time) can be carried out.\n1.4.2 The Parts of a Computer System\nWhen most people use the word computer, they usually mean more than just\nthe processor (i.e., CPU) that is in charge of doing what the software directs.\n",
    "page_number": 18,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "They usually mean the collection of parts that in combination form their computer\nsystem. Today that computer system is often a laptop (see Figure 1.4), augmented\nwith many additional devices.\nA computer system generally includes, in addition to the processor, a key-\nboard for typing commands, a mouse or keypad or joystick for positioning on\nmenu entries, a monitor for displaying information that the computer system has\nproduced, memory for temporarily storing information, disks and USB memory\nsticks of one sort or another for storing information for a very long time, even after\nthe computer has been turned o, connections to other devices such as a printer\nfor obtaining paper copies of that information, and the collection of programs\n(the software) that the user wishes to execute.\nAll these items help computer users to do their jobs. Without a printer, for\nexample, the user would have to copy by hand what is displayed on the monitor.\nWithout a mouse, keypad, or joystick, the user would have to type each command,\nrather than simply position the mouse, keypad, or joystick.\nSo, as we begin our journey, which focuses on the CPU that occupies a small\nfraction of 1 square inch of silicon and the software that makes the CPU do our\nbidding, we note that the computer systems we use contain a lot of additional\ncomponents.\n1.5 Two Very Important Ideas\nBefore we leave this rst chapter, there are two very important ideas that we\nwould like you to understand, ideas that are at the core of what computing is all\nabout.\nIdea 1:\nAll computers (the biggest and the smallest, the fastest and the\nslowest, the most expensive and the cheapest) are capable of comput-\ning exactly the same things if they are given enough time and enough\nmemory. That is, anything a fast computer can do, a slow computer can\ndo also. The slow computer just does it more slowly. A more expensive\ncomputer cannot gure out something that a cheaper computer is unable\nto gure out as long as the cheaper computer can access enough mem-\nory. (You may have to go to the store to buy more memory whenever it\nruns out of memory in order to keep increasing memory.) All computers\ncan do exactly the same things. Some computers can do things faster,\nbut none can do more than any other.\nIdea 2:\nWe describe our problems in English or some other language\nspoken by people. Yet the problems are solved by electrons running\naround inside the computer. It is necessary to transform our problem\nfrom the language of humans to the voltages that inuence the ow of\nelectrons. This transformation is really a sequence of systematic trans-\nformations, developed and improved over the last 70 years, which\ncombine to give the computer the ability to carry out what appear to\n",
    "page_number": 19,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "be some very complicated tasks. In reality, these tasks are simple and\nstraightforward.\nThe rest of this chapter is devoted to discussing these two ideas.\n1.6 Computers as Universal\nComputational Devices\nIt may seem strange that an introductory textbook begins by describing how\ncomputers work. After all, mechanical engineering students begin by studying\nphysics, not how car engines work. Chemical engineering students begin by\nstudying chemistry, not oil reneries. Why should computing students begin by\nstudying computers?\nThe answer is that computers are dierent. To learn the fundamental prin-\nciples of computing, you must study computers or machines that can do what\ncomputers can do. The reason for this has to do with the notion that computers\nare universal computational devices. Lets see what that means.\nBefore modern computers, there were many kinds of calculating machines.\nSome were analog machinesmachines that produced an answer by measuring\nsome physical quantity such as distance or voltage. For example, a slide rule is\nan analog machine that multiplies numbers by sliding one logarithmically graded\nruler next to another. The user can read a logarithmic distance on the sec-\nond ruler. Some early analog adding machines worked by dropping weights on a\nscale. The diculty with analog machines is that it is very hard to increase their\naccuracy.\nThis is why digital machinesmachines that perform computations by\nmanipulating a xed nite set of digits or letterscame to dominate comput-\ning. You are familiar with the distinction between analog and digital watches. An\nanalog watch has hour and minute hands, and perhaps a second hand. It gives\nthe time by the positions of its hands, which are really angular measures. Digital\nwatches give the time in digits. You can increase accuracy just by adding more\ndigits. For example, if it is important for you to measure time in hundredths of\na second, you can buy a watch that gives a reading like 10:35.16 rather than just\n10:35. How would you get an analog watch that would give you an accurate read-\ning to one one-hundredth of a second? You could do it, but it would take a mighty\nlong second hand! When we talk about computers in this book, we will always\nmean digital machines.\nBefore modern digital computers, the most common digital machines in the\nWest were adding machines. In other parts of the world another digital machine,\nthe abacus, was common. Digital adding machines were mechanical or elec-\ntromechanical devices that could perform a specic kind of computation: adding\nintegers. There were also digital machines that could multiply integers. There\nwere digital machines that could put a stack of cards with punched names in\nalphabetical order. The main limitation of all these machines is that they could\ndo only one specic kind of computation. If you owned only an adding machine\n",
    "page_number": 20,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "and wanted to multiply two integers, you had some pencil-and-paper work\nto do.\nThis is why computers are dierent. You can tell a computer how to add num-\nbers. You can tell it how to multiply. You can tell it how to alphabetize a list or\nperform any computation you like. When you think of a new kind of computation,\nyou do not have to buy or design a new computer. You just give the old computer\na new set of instructions (a program) to carry out the new computation. This is\nwhy we say the computer is a universal computational device. Computer scien-\ntists believe that anything that can be computed, can be computed by a computer\nprovided it has enough time and enough memory. When we study computers, we\nstudy the fundamentals of all computing. We learn what computation is and what\ncan be computed.\nThe idea of a universal computational device is due to Alan Turing. Turing\nproposed in 1937 that all computations could be carried out by a particular kind of\nmachine, which is now called a Turing machine. He gave a mathematical descrip-\ntion of this kind of machine, but did not actually build one. Digital computers\nwere not operating until several years later. Turing was more interested in solv-\ning a philosophical problem: dening computation. He began by looking at the\nkinds of actions that people perform when they compute; these include making\nmarks on paper, writing symbols according to certain rules when other symbols\nare present, and so on. He abstracted these actions and specied a mechanism that\ncould carry them out. He gave some examples of the kinds of things that these\nmachines could do. One Turing machine could add two integers; another could\nmultiply two integers.\nFigure 1.7 shows what we call black box models of Turing machines that\nadd and multiply. In each case, the operation to be performed is described in\nthe box. The data elements on which to operate are shown as inputs to the box.\nThe r\nprovi\nthere\na, b\n(Turing machine\nthat adds)\n(Turing machine\nthat multiplies)\nFigure 1.7\nBlack box models of Turing machines.\nTuring proposed that every computation can be performed by some Turing\nmachine. We call this Turings thesis. Although Turings thesis has never been\nproved, there does exist a lot of evidence to suggest it is true. We know, for exam-\nple, that various enhancements one can make to Turing machines do not result in\nmachines that can compute more.\nPerhaps the best argument to support Turings thesis was provided by Turing\nhimself in his original paper. He said that one way to try to construct a machine\nmore powerful than any particular Turing machine was to make a machine U\nthat could simulate all Turing machines. You would simply describe to U the\n",
    "page_number": 21,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "particular Turing machine you wanted it to simulate, say a machine to add two\nintegers, give U the input data, and U would compute the appropriate output,\nin this case the sum of the inputs. Turing then showed that there was, in fact,\na Turing machine that could do this, so even this attempt to nd something that\ncould not be computed by Turing machines failed.\ne, f, g\nv\nTuring machine)\nFigure 1.8\nBlack box model of a universal Turing machine.\nIn specifying U, Turing had provided us with a deep insight: He had given us\nthe rst description of what computers do. In fact, both a computer (with as much\nmemory as it wants) and a universal Turing machine can compute exactly the\nsame things. In both cases, you give the machine a description of a computation\nand the data it needs, and the machine computes the appropriate answer. Comput-\ners and universal Turing machines can compute anything that can be computed\nbecause they are programmable.\nThis is the reason that a big or expensive computer cannot do more than a\nsmall, cheap computer. More money may buy you a faster computer, a monitor\nwith higher resolution, or a nice sound system. But if you have a small, cheap\ncomputer, you already have a universal computational device.\n1.7 How Do We Get the Electrons to\nDo the Work?\nFigure 1.9 shows the process we must go through to get the electrons (which\nactually do the work) to do our bidding. We call the steps of this process the\nLevels of Transformation. As we will see, at each level we have choices. If we\nignore any of the levels, our ability to make the best use of our computing system\ncan be very adversely aected.\n1.7.1 The Statement of the Problem\nWe describe the problems we wish to solve in a natural language. Natural lan-\nguages are languages that people speak, like English, French, Japanese, Italian,\nand so on. They have evolved over centuries in accordance with their usage.\nThey are fraught with a lot of things unacceptable for providing instructions to a\n",
    "page_number": 22,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "15\nDevices\nFigure 1.9\nLevels of transformation.\ncomputer. Most important of these unacceptable attributes is ambiguity. Natural\nlanguage is lled with ambiguity. To infer the meaning of a sentence, a listener is\noften helped by the tone of voice of the speaker, or at the very least, the context\nof the sentence.\nAn example of ambiguity in English is the sentence, Time ies like an\narrow. At least three interpretations are possible, depending on whether (1) one is\nnoticing how fast time passes, (2) one is at a track meet for insects, or (3) one is\nwriting a letter to the Dear Abby of Insectville. In the rst case, a simile; one\nis comparing the speed of time passing to the speed of an arrow that has been\nreleased. In the second case, one is telling the timekeeper to do his/her job much\nlike an arrow would. In the third case, one is relating that a particular group of\nies (time ies, as opposed to fruit ies) are all in love with the same arrow.\nSuch ambiguity would be unacceptable in instructions provided to a com-\nputer. The computer, electronic idiot that it is, can only do as it is told. To tell it to\ndo something where there are multiple interpretations would cause the computer\nto not know which interpretation to follow.\n",
    "page_number": 23,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "1.7.2 The Algorithm\nThe rst step in the sequence of transformations is to transform the natural lan-\nguage description of the problem to an algorithm, and in so doing, get rid of the\nobjectionable characteristics of the natural language. An algorithm is a step-by-\nstep procedure that is guaranteed to terminate, such that each step is precisely\nstated and can be carried out by the computer. There are terms to describe each\nof these properties.\nWe use the term deniteness to describe the notion that each step is precisely\nstated. A recipe for excellent pancakes that instructs the preparer to stir until\nlumpy lacks deniteness, since the notion of lumpiness is not precise.\nWe use the term eective computability to describe the notion that each step\ncan be carried out by a computer. A procedure that instructs the computer to take\nthe largest prime number lacks eective computability, since there is no largest\nprime number.\nWe use the term niteness to describe the notion that the procedure termi-\nnates.\nFor every problem there are usually many dierent algorithms for solving\nthat problem. One algorithm may require the fewest steps. Another algorithm\nmay allow some steps to be performed concurrently. A computer that allows\nmore than one thing to be done at a time can often solve the problem in less\ntime, even though it is likely that the total number of steps to be performed has\nincreased.\n1.7.3 The Program\nThe next step is to transform the algorithm into a computer program in one of the\nprogramming languages that are available. Programming languages are mechan-\nical languages. That is, unlike natural languages, mechanical languages did not\nevolve through human discourse. Rather, they were invented for use in specify-\ning a sequence of instructions to a computer. Therefore, mechanical languages do\nnot suer from failings such as ambiguity that would make them unacceptable for\nspecifying a computer program.\nThere are more than 1000 programming languages. Some have been designed\nfor use with particular applications, such as Fortran for solving scientic calcula-\ntions and COBOL for solving business data-processing problems. In the second\nhalf of this book, we will use C and C++, languages that were designed for\nmanipulating low-level hardware structures.\nOther languages are useful for still other purposes. Prolog is the language of\nchoice for many applications that require the design of an expert system. LISP\nwas for years the language of choice of a substantial number of people working\non problems dealing with articial intelligence. Pascal is a language invented as\na vehicle for teaching beginning students how to program.\nThere are two kinds of programming languages, high-level languages and\nlow-level languages. High-level languages are at a distance (a high level) from\nthe underlying computer. At their best, they are independent of the computer on\nwhich the programs will execute. We say the language is machine independent.\n",
    "page_number": 24,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "All the languages mentioned thus far are high-level languages. Low-level lan-\nguages are tied to the computer on which the programs will execute. There is\ngenerally one such low-level language for each computer. That language is called\nthe assembly language for that computer.\n1.7.4 The ISA\nThe next step is to translate the program into the instruction set of the particular\ncomputer that will be used to carry out the work of the program. The instruction\nset architecture (ISA) is the complete specication of the interface between pro-\ngrams that have been written and the underlying computer hardware that must\ncarry out the work of those programs.\nAn analogy that may be helpful in understanding the concept of an ISA is\nprovided by the automobile. Corresponding to a computer program, represented\nas a sequence of 0s and 1s in the case of the computer, is the human sitting in the\ndrivers seat of a car. Corresponding to the microprocessor hardware is the car\nitself. The ISA of the automobile is the specication of everything the human\nneeds to know to tell the automobile what to do, and everything the automobile\nneeds to know to carry out the tasks specied by the human driver. For example,\none element of the automobiles ISA is the pedal on the oor known as the\nbrake, and its function. The human knows that if he/she steps on the brake, the\ncar will stop. The automobile knows that if it feels pressure from the human on\nthat pedal, the hardware of the automobile must engage those elements necessary\nto stop the car. The full ISA of the car includes the specication of the other\npedals, the steering wheel, the ignition key, the gears, windshield wipers, etc. For\neach, the ISA species (a) what the human has to do to tell the automobile what\nhe/she wants done, and (b) correspondingly, what the automobile will interpret\nthose actions to mean so it (the automobile) can carry out the specied task.\nThe ISA of a computer serves the same purpose as the ISA of an auto-\nmobile, except instead of the driver and the car, the ISA of a computer species\nthe interface between the computer program directing the computer hardware\nand the hardware carrying out those directions. For example, consider the set of\ninstructions that the computer can carry outthat is, what operations the com-\nputer can perform and where to get the data needed to perform those operations.\nThe term opcode is used to describe the operation. The term operand is used to\ndescribe individual data values. The ISA species the acceptable representations\nfor operands. They are called data types. A data type is a representation of an\noperand such that the computer can perform operations on that representation.\nThe ISA species the mechanisms that the computer can use to gure out where\nthe operands are located. These mechanisms are called addressing modes.\nThe number of opcodes, data types, and addressing modes specied by an\nISA vary among dierent ISAs. Some ISAs have as few as a half dozen opcodes,\nwhereas others have as many as several hundred. Some ISAs have only one data\ntype, while others have more than a dozen. Some ISAs have one or two addressing\nmodes, whereas others have more than 20. The x86, the ISA used in the PC, has\nmore than 200 opcodes, more than a dozen data types, and more than two dozen\naddressing modes.\n",
    "page_number": 25,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "The ISA also species the number of unique locations that comprise the com-\nputers memory and the number of individual 0s and 1s that are contained in each\nlocation.\nMany ISAs are in use today. The most widely known example is the x86,\nintroduced by Intel Corporation in 1979 and currently also manufactured by\nAMD and other companies. Other ISAs and the companies responsible for them\ninclude ARM and THUMB (ARM), POWER and z/Architecture (IBM), and\nSPARC (Oracle).\nThe translation from a high-level language (such as C) to the ISA of the\ncomputer on which the program will execute (such as x86) is usually done by\na translating program called a compiler. To translate from a program written in\nC to the x86 ISA, one would need a C to x86 compiler. For each high-level lan-\nguage and each desired target ISA, one must provide a corresponding compiler.\nThe translation from the unique assembly language of a computer to its ISA\nis done by an assembler.\n1.7.5 The Microarchitecture\nThe next step is the implementation of the ISA, referred to as its microarchitec-\nture. The automobile analogy that we used in our discussion of the ISA is also\nuseful in showing the relationship between an ISA and a microarchitecture that\nimplements that ISA. The automobiles ISA describes what the driver needs to\nknow as he/she sits inside the automobile to make the automobile carry out the\ndrivers wishes. All automobiles have the same ISA. If there are three pedals on\nthe oor, it does not matter what manufacturer produced the car, the middle one\nis always the brake. The one on the right is always the accelerator, and the more\nit is depressed, the faster the car will move. Because there is only one ISA for\nautomobiles, one does not need one drivers license for Buicks and a dierent\ndrivers license for Hondas.\nThe microarchitecture (or implementation) of the automobiles ISA, on\nthe other hand, is about what goes on underneath the hood. Here all automo-\nbile makes and models can be dierent, depending on what cost/performance\ntradeos the automobile designer made before the car was manufactured. Some\nautomobiles come with disc brakes, others (in the past, at least) with drums.\nSome automobiles have eight cylinders, others run on six cylinders, and still oth-\ners have only four. Some are turbocharged, some are not. Some automobiles can\ntravel 60 miles on one gallon of gasoline, others are lucky to travel from one gas\nstation to the next without running out of gas. Some automobiles cost 6000 US\ndollars, others cost 200,000 US dollars. In each case, the microarchitecture\nof the specic automobile is a result of the automobile designers decisions\nregarding the tradeos of cost and performance. The fact that the micro-\narchitecture of every model or make is dierent is a good reason to take ones\nHonda, when it is malfunctioning, to a Honda repair person, and not to a Buick\nrepair person.\nIn the previous section, we identied ISAs of several computer manufactur-\ners, including the x86 (Intel), the PowerPC (IBM and Motorola), and THUMB\n(ARM). Each has been implemented by many dierent microarchitectures. For\n",
    "page_number": 26,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "example, the x86s original implementation in 1979 was the 8086, followed by the\n80286, 80386, and 80486 in the 1980s. More recently, in 2001, Intel introduced\nthe Pentium IV microprocessor. Even more recently, in 2015, Intel introduced\nSkylake. Each of these x86 microprocessors has its own microarchitecture.\nThe story is the same for the PowerPC ISA, with more than a dozen dierent\nmicroprocessors, each having its own microarchitecture.\nEach microarchitecture is an opportunity for computer designers to make dif-\nferent tradeos between the cost of the microprocessor, the performance that the\nmicroprocessor will provide, and the energy that is required to power the micro-\nprocessor. Computer design is always an exercise in tradeos, as the designer\nopts for higher (or lower) performance, more (or less) energy required, at greater\n(or lesser) cost.\n1.7.6 The Logic Circuit\nThe next step is to implement each element of the microarchitecture out of simple\nlogic circuits. Here also there are choices, as the logic designer decides how to\nbest make the tradeos between cost and performance. So, for example, even for\nan operation as simple as addition, there are several choices of logic circuits to\nperform the operation at diering speeds and corresponding costs.\n1.7.7 The Devices\nFinally, each basic logic circuit is implemented in accordance with the require-\nments of the particular device technology used. So, CMOS circuits are dierent\nfrom NMOS circuits, which are dierent, in turn, from gallium arsenide\ncircuits.\nThe Bottom Line\nIn summary, from the natural language description of a prob-\nlem to the electrons that actually solve the problem by moving from one voltage\npotential to another, many transformations need to be performed. If we could\nspeak electron, or if the electrons could understand English, perhaps we could just\nwalk up to the computer and get the electrons to do our bidding. Since we cant\nspeak electron and they cant speak English, the best we can do is this systematic\nsequence of transformations. At each level of transformation, there are choices as\nto how to proceed. Our handling of those choices determines the resulting cost\nand performance of our computer.\nIn this book, we describe each of these transformations. We show how tran-\nsistors combine to form logic circuits, how logic circuits combine to form the\nmicroarchitecture, and how the microarchitecture implements a particular ISA.\nIn our case, the ISA is the LC-3. We complete the process by going from the\nEnglish-language description of a problem to a C or C++ program that solves the\nproblem, and we show how that C or C++ program is translated (i.e., compiled)\nto the ISA of the LC-3.\nWe hope you enjoy the ride.\n",
    "page_number": 27,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": ", Data Types, and\nrations\n2.1 Bits and Data Types\n2.1.1 The Bit as the Unit of Information\nWe noted in Chapter 1 that the computer was organized as a system with several\nlevels of transformation. A problem stated in a natural language such as English\nis actually solved by the electrons moving around inside the components of the\ncomputer.\nInside the computer, millions of very tiny, very fast devices control the move-\nment of those electrons. These devices react to the presence or absence of voltages\nin electronic circuits. They could react to the actual values of the voltages, rather\nthan simply to the presence or absence of voltages. However, this would make\nthe control and detection circuits more complex than they need to be. It is much\neasier to detect simply whether or not a voltage exists at a point in a circuit than\nit is to measure exactly what that voltage is.\nTo understand this, consider any wall outlet in your home. You could measure\nthe exact voltage it is carrying, whether 120 volts or 115 volts, or 118.6 volts,\nfor example. However, the detection circuitry to determine only whether there is\na voltage or whether there is no voltage is much simpler. Your nger casually\ninserted into the wall socket, for example, will suce.\nWe symbolically represent the presence of a voltage as 1 and the absence\nof a voltage as 0. We refer to each 0 and each 1 as a bit, which is a shortened\nform of binary digit. Recall the digits you have been using since you were a\nchild0, 1, 2, 3,  , 9. There are ten of them, and they are referred to as decimal\ndigits. In the case of binary digits, there are two of them, 0 and 1.\nTo be perfectly precise, it is not really the case that the computer dieren-\ntiates the absolute absence of a voltage (i.e., 0) from the absolute presence of\na voltage (i.e., 1). Actually, the electronic circuits in the computer dierentiate\nvoltages close to 0 from voltages far from 0. So, for example, if the computer\nexpects either a voltage of 1.2 volts or a voltage of 0 volts (1.2 volts signifying\n1 and 0 volts signifying 0), then a voltage of 1.0 volts will be taken as a 1 and\n0.2 volts will be taken as a 0.\n",
    "page_number": 28,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "honewire,onecandifferentiateonlytwothings.Oneofthemcanbeassigned\ne 0, the other can be assigned the value 1. But to get useful work done by\nputer, it is necessary to be able to differentiate a large number of distinct\nand to assign each of them a unique representation. We can accomplish\nombining many wires, that is, many bits. For example, if we use eight bits\n(corresponding to the voltage present on each of eight wires), we can represent one\nparticularvalueas01001110,andanothervalueas11100111.Infact,ifwearelimited\ntoeightbits,wecandifferentiateatmostonly256(i.e., 28) differentthings.Ingeneral,\nwith k bits, we can distinguish at most 2k distinct items. Each pattern of these k bits\nis a code; that is, it corresponds to a particular item (or value).\n2.1.2 Data Types\nThere are many ways to represent the same value. For example, the number ve\ncan be written as a 5. This is the standard decimal notation that you are used to.\nThe value ve can also be represented by someone holding up one hand, with all\nngers and thumb extended. The person is saying, The number I wish to com-\nmunicate can be determined by counting the number of ngers I am showing. A\nwritten version of that scheme would be the value 11111. This notation has a name\nalsounary. The Romans had yet another notation for vethe character V. We\nmomentarily that a fourth notation for ve is the binary representation\n01.\ns not enough simply to represent values; we must be able to operate on\nalues. We say a particular representation is a data type if there are oper-\nn the computer that can operate on information that is encoded in that\nntation. Each instruction set architecture (ISA) has its own set of data types\nown set of instructions that can operate on those data types. In this book,\nmainly use two data types: 2s complement integers for representing posi-\ntive and negative integers that we wish to perform arithmetic on, and ASCII codes\nfor representing characters that we wish to input to a computer via the keyboard or\noutput from the computer via a monitor. Both data types will be explained shortly.\nThere are other representations of information that could be used, and indeed\nthat are present in most computers. Recall the scientic notation from high\nschool chemistry where you were admonished to represent the decimal num-\nber 621 as 6.21102. There are computers that represent numbers in that form,\nand they provide operations that can operate on numbers so represented. That\ndata type is usually called oating point. We will examine its representation in\nSection 2.7.1.\n2.2 Integer Data Types\n2.2.1 Unsigned Integers\nThe rst representation of information, or data type, that we shall look at is the\nunsigned integer. As its name suggests, an unsigned integer has no sign (plus or\nminus) associated with it. An unsigned integer just has a magnitude. Unsigned\n",
    "page_number": 29,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "integers have many uses in a computer. If we wish to perform a task some spe-\ncic number of times, unsigned integers enable us to keep track of this number\neasily by simply counting how many times we have performed the task. Unsigned\nintegers also provide a means for identifying dierent memory locations in the\ncomputer in the same way that house numbers dierentiate 129 Main Street from\n131 Main Street. I dont recall ever seeing a house number with a minus sign in\nfront of it.\nWe can represent unsigned integers as strings of binary digits. To do this, we\nuse a positional notation much like the decimal system that you have been using\nsince you were three years old.\nYou are familiar with the decimal number 329, which also uses positional\nnotation. The 3 is worth much more than the 9, even though the absolute value of\n3 standing alone is only worth 1/3 the value of 9 standing alone. This is because,\nas you know, the 3 stands for 300 (3102) due to its position in the decimal string\n329, while the 9 stands for 9  100.\nInstead of using decimal digits, we can represent unsigned integers using just\nthe binary digits 0 and 1. Here the base is 2, rather than 10. So, for example, if\nwe have ve bits (binary digits) available to represent our values, the number 5,\nwhich we mentioned earlier, is represented as 00101, corresponding to\n0  24 + 0  23 + 1  22 + 0  21 + 1  20\nWith k bits, we can represent in this positional notation exactly 2k integers, rang-\ning from 0 to 2k  1. Figure 2.1 shows the ve-bit representations for the integers\nfrom 0 to 31.\n2.2.2 Signed Integers\nTo do useful arithmetic, however, it is often (although not always) necessary to\nbe able to deal with negative quantities as well as positive. We could take our 2k\ndistinct patterns of k bits and separate them in half, half for positive integers and\nhalf for negative integers. In this way, with ve-bit codes, instead of representing\nintegers from 0 to +31, we could choose to represent positive integers from +1\nto +15 and negative integers from 1 to 15. There are 30 such integers. Since\n25 is 32, we still have two 5-bit codes unassigned. One of them, 00000, we would\npresumably assign to the value 0, giving us the full range of integer values from\n15 to +15. That leaves one 5-bit code left over, and there are dierent ways to\nassign this code, as we will see momentarily.\nWe are still left with the problem of determining what codes to assign to what\nvalues. That is, we have 32 codes, but which value should go with which code?\nPositive integers are represented in the straightforward positional scheme.\nSince there are k bits, and we wish to use exactly half of the 2k codes to represent\nthe integers from 0 to 2k1  1, all positive integers will have a leading 0 in their\nrepresentation. In our example of Figure 2.1 (with k = 5), the largest positive\ninteger +15 is represented as 01111.\nNote that in all three signed data types shown in Figure 2.1 , the represen-\ntation for 0 and all the positive integers start with a leading 0. What about the\nrepresentations for the negative integers (in our ve-bit example, 1 to 15)?\n",
    "page_number": 30,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "11111\n31\n15\n0\n1\nFigure 2.1\nFour representations of integers.\nThe rst thought that usually comes to mind is: If a leading 0 signies a positive\ninteger, how about letting a leading 1 signify a negative integer? The result is\nthe signed-magnitude data type shown in Figure 2.1. A second thought (which\nwas actually used on some early computers such as the Control Data Corpora-\ntion 6600) was the following: Let a negative number be represented by taking the\nrepresentation of the positive number having the same magnitude, and ipping\nall the bits. That is, if the original representation had a 0, replace it with a 1; if\nit originally had a 1, replace it with a 0. For example, since +5 is represented as\n00101, we designate 5 as 11010. This data type is referred to in the computer\nengineering community as 1s complement and is also shown in Figure 2.1.\n",
    "page_number": 31,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "At this point, you might think that a computer designer could assign any bit\npattern to represent any integer he or she wants. And you would be right! Unfor-\ntunately, that could complicate matters when we try to build an electronic circuit\nto add two integers. In fact, the signed-magnitude and 1s complement data types\nboth require unnecessarily cumbersome hardware to do addition. Because com-\nputer designers knew what it would take to design a circuit to add two integers,\nthey chose representations that simplied the circuit. The result is the 2s comple-\nment data type, also shown in Figure 2.1. It is used on just about every computer\nmanufactured today.\n2.3 2s Complement Integers\nWe see in Figure 2.1 the representations of the integers from 16 to +15 for the\n2s complement data type. Why were those representations chosen?\nThe positive integers, we saw, are represented in the straightforward posi-\ntional scheme. With ve bits, we use exactly half of the 25 codes to represent 0\nand the positive integers from 1 to 24  1.\nThe choice of representations for the negative integers was based, as we said\npreviously, on the wish to keep the logic circuits as simple as possible. Almost\nall computers use the same basic mechanism to perform addition. It is called an\narithmetic and logic unit, usually known by its acronym ALU. We will get into\nthe actual structure of the ALU in Chapters 3 and 4. What is relevant right now\nis that an ALU has two inputs and one output. It performs addition by adding the\nbinary bit patterns at its inputs, producing a bit pattern at its output that is the\nsum of the two input bit patterns.\nFor example, if the ALU processe\nwere 00110 and 00101, the result (o\naddition is as follows:\n00110\n00101\n01011\nThe addition of two binary strings is performed in the same way the addi-\ntion of two decimal strings is performed, from right to left, column by column.\nIf the addition in a column generates a carry, the carry is added to the column\nimmediately to its left.\nWhat is particularly relevant is that the binary ALU does not know (and does\nnot care) what the two patterns it is adding represent. It simply adds the two binary\npatterns. Since the binary ALU only ADDs and does not CARE, it would be nice\nif our assignment of codes to the integers resulted in the ALU producing correct\nresults when it added two integers.\nFor starters, it would be nice if, when the ALU adds the representation for an\narbitrary integer to the representation of the integer having the same magnitude\nbut opposite sign, the sum would be 0. That is, if the inputs to the ALU are the\nrepresentations of non-zero integers A and A, the output of the ALU should\nbe 00000.\n",
    "page_number": 32,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "To accomplish that, the 2s complement data type species the representation\nfor each negative integer so that when the ALU adds it to the representation of\nthe positive integer of the same magnitude, the result will be the representation\nfor 0. For example, since 00101 is the representation of +5, 11011 is chosen as\nthe representation for 5.\nMoreover, and actually more importantly, as we sequence through represen-\ntations of 15 to +15, the ALU is adding 00001 to each successive representation.\nWe can express this mathematically as:\nREPRESENTATION(value + 1) =\nREPRESENTATION(value) + REPRESENTATION(1).\nThis is sucient to guarantee (as long as we do not get a result larger than\n+15 or smaller than 15) that the binary ALU will perform addition correctly.\nNote in particular the representations for 1 and 0, that is, 11111 and 00000.\nWhen we add 00001 to the representation for 1, we do get 00000, but we also\ngenerate a carry. That carry, however, does not inuence the result. That is, the\ncorrect result of adding 00001 to the representation for 1 is 0, not 100000.\nTherefore, the carry is ignored. In fact, because the carry obtained by adding\n00001 to 11111 is ignored, the carry can always be ignored when dealing with\n2s complement arithmetic.\nNote: If we know the representation for A, a shortcut for guring out the\nrepresentation for A(A  0) is as follows: Flip all the bits of A (the ocial term\nfor ip is complement), and add 1 to the complement of A. The sum of A and\nExa\n10011\n00000\nYou may have noticed that the addition of 01101 and 10011, in addition to\nproducing 00000, also produces a carry out of the ve-bit ALU. That is, the binary\naddition of 01101 and 10011 is really 100000. However, as we saw previously,\nthis carry out can be ignored in the case of the 2s complement data type.\nAt this point, we have identied in our ve-bit scheme 15 positive inte-\ngers. We have constructed 15 negative integers. We also have a representation\n",
    "page_number": 33,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "for 0. With k = 5, we can uniquely identify 32 distinct quantities, and we have\naccounted for only 31 (15+15+1). The remaining representation is 10000. What\nvalue shall we assign to it?\nWe note that 1 is 11111, 2 is 11110, 3 is 11101, and so on. If we continue\nthis, we note that 15 is 10001. Note that, as in the case of the positive represen-\ntations, as we sequence backwards from representations of 1 to 15, the ALU\nis subtracting 00001 from each successive representation. Thus, it is convenient\nto assign to 10000 the value 16; that is the value one gets by subtracting 00001\nfrom 10001 (the representation for 15).\nIn Chapter 5 we will specify a computer that we aectionately have named\nthe LC-3 (for Little Computer 3). The LC-3 operates on 16-bit values. Therefore,\nthe 2s complement integers that can be represented in the LC-3 are the integers\nfrom 32,768 to +32,767.\n2.4 Conversion Between Binary and\nDecimal\nIt is often useful to convert numbers between the 2s complement data type\nthe computer likes and the decimal representation that you have used all\nyour life.\n2.4.1 Binary to Decimal Conversion\nWe convert a 2s complement representation of an integer to a decimal represen-\ntation as follows: For purposes of illustration, we will assume our number can be\nrepresented in eight bits, corresponding to decimal integer values from 128 to\n+127.\nRecall that an eight-bit 2s complement integer takes the form\nb7 b6 b5 b4 b3 b2 b1 b0\nwhere each of the bits bi is either 0 or 1.\n1. Examine the leading bit b7. If it is a 0, the integer is positive, and we can\nbegin evaluating its magnitude. If it is a 1, the integer is negative. In that\ncase, we need to rst obtain the 2s complement representation of the\npositive number having the same magnitude. We do this by ipping all the\nbits and adding 1.\n2. The magnitude is simply\nb6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20\nIn either case, we obtain the decimal magnitude by simply adding the\npowers of 2 that have coecients of 1.\n3. Finally, if the original number is negative, we ax a minus sign in front.\nDone!\n",
    "page_number": 34,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Exa\n.\n3. The decimal integer value corresponding to 11000111 is 57.\n2.4.2 Decimal to Binary Conversion\nConverting from decimal to 2s complement is a little more complicated. The\ncrux of the method is to note that a positive binary number is odd if the rightmost\ndigit is 1 and even if the rightmost digit is 0.\nConsider again our generic eight-bit representation:\nb7  27 + b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20\nWe can illustrate the conversion best by rst working through an example.\nSuppose we wish to convert the value +105 to a 2s complement binary code.\nWe note that +105 is positive. We rst nd values for bi, representing the mag-\nnitude 105. Since the value is positive, we will then obtain the 2s complement\nresult by simply appending b7, which we know is 0.\nOur rst step is to nd values for bi that satisfy the following:\n105 = b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20\nSince 105 is odd, we know that b0 is 1. We subtract 1 from both sides of the\nequation, yielding\n104 = b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21\nWe next divide both sides of the equation by 2, yielding\n52 = b6  25 + b5  24 + b4  23 + b3  22 + b2  21 + b1  20\nSince 52 is even, b1, the only coecient not multiplied by a power of 2, must be\nequal to 0.\nWe iterate this process, each time subtracting the rightmost digit from both\nsides of the equation, then dividing both sides by 2, and nally noting whether\nthe new decimal number on the left side is odd or even. Continuing where we left\no, with\n52 = b6  25 + j5  24 + b4  23 + b3  22 + b2  21\nthe process produces, in turn:\n26 = b6  24 + b5  23 + b4  22 + b3  21 + b2  20\n",
    "page_number": 35,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Therefore, b2 = 0.\n13 = b6  23 + b5  22 + b4  21 + b3  20\nTherefore, b3 = 1.\n6 = b6  22 + b5  21 + b4  20\nTherefore, b4 = 0.\n3 = b6  21 + b5  20\nTherefore, b5 = 1.\n1 = b6  20\nTherefore, b6 = 1, and we are done. The binary representation is 01101001.\nLets summarize the process. If we are given a decimal integer value N, we\nconstruct the 2s complement representation as follows:\n1. We rst obtain the binary representation of the magnitude of N by forming\nthe equation\nN = b6  26 + b5  25 + b4  24 + b3  23 + b2  22 + b1  21 + b0  20\nand repeating the following, until the left side of the equation is 0:\na. If N is odd, the rightmost bit is 1. If N is even, the rightmost bit is 0.\nb. Subtract 1 or 0 (according to whether N is odd or even) from N, remove\nthe least signicant term from the right side, and divide both sides of\nthe equation by 2.\nEach iteration produces the value of one coecient bi.\n2. If the original decimal number is positive, append a leading 0 sign bit, and\nyou are done.\n3. If the original decimal number is negative, append a leading 0 and then\nform the negative of this 2s complement representation, and then you\nare done.\n2.4.3 Extending Conversion to Numbers with Fractional Parts\nWhat if the number we wish to convert is not an integer, but instead has a\nfractional part. How do we handle that wrinkle?\nBinary to decimal\nThe binary to decimal case is straightforward. In a positional\nnotation system, the number\n0.b1b2b3b4\nshows four bits to the right of the binary point, representing (when the cor-\nresponding bi = 1) the values 0.5, 0.25, 0.125, and 0.0625. To complete the\n",
    "page_number": 36,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "conversion to decimal, we simply add those values where the corresponding bi =\n1. For example, if the fractional part of the binary representation is\n. 1 0 1 1\nwe would add 0.5 plus 0.125 plus 0.0625, or 0.6875.\nDecimal to binary\nThe decimal to binary case requires a little more work. Sup-\npose we wanted to convert 0.421 to binary. As we did for integer conversion, we\nrst form the equation\n0.421 = b1  21 + b2  22 + b3  23 + b4  24 + ...\nIn the case of converting a decimal integer value to binary, we divided by 2 and\nassigned a 1 or 0 to the coecient of 20 depending on whether the number on the\nleft of the equal sign is odd or even. Here (i.e., in the case of converting a decimal\nfraction to binary), we multiply both sides of the equation by 2 and assign a 1 or\na 0 to the coecient of 20 depending on whether the left side of the equation is\ngreater than or equal to 1 or whether the left side is less than 1. Do you see why?\nSince\n0.842 = b1  20 + b2  21 + b3  22 + b4  23 + ...\nwe assign b1 = 0. Continuing,\n1.684 = b2  20 + b3  21 + b4  22 + ...\nso we assign b2 = 1 and subtract 1 from both sides of the equation, yielding\n0.684 = b3  21 + b4  22 + ...\nMultiplying by 2, we get\n1.368 = b3  20 + b4  21 + ...\nso we assign b3 = 1 and subtract 1 from both sides of the equation, yielding\n0.368 = b4  20 + ...\nwhich assigns 0 to b4. We can continue this process indenitely, until we are\nsimply too tired to go on, or until the left side = 0, in which case all bits to\nthe right of where we stop are 0s. In our case, stopping with four bits, we have\nconverted 0.421 decimal to 0.0110 in binary.\n2.5 Operations on Bits\nPart I: Arithmetic\n2.5.1 Addition and Subtraction\nArithmetic on 2s complement numbers is very much like the arithmetic on\ndecimal numbers that you have been doing for a long time.\n",
    "page_number": 37,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Addition still proceeds from right to left, one digit at a time. At each point,\nwe generate a sum digit and a carry. Instead of generating a carry after 9 (since\n9 is the largest decimal digit), we generate a carry after 1 (since 1 is the largest\nbinary digit).\nExample 2.3\nUsing our ve-bit notation, what is 11 + 3?\nThe decimal value 11 is represented as 01011\nThe decimal value 3 is represented as\n00011\nThe sum, which is the value 14, is\n01110\nSubtraction is simply addition, preceded by determining the negative of the\nnumber to be subtracted. That is, A  B is simply A + (B).\nExample 2.4\nWhat is 14  9?\nThe decimal value 14 is represented as\n01110\nThe decimal value 9 is represented as\n01001\nFirst we form the negative, that is, -9: 10111\nAdding 14 to -9, we get\n01110\n10111\nwhich results in the value 5.\n00101\nNote again that the carry out is ignored.\nExam\nWhat happens when we add a number to itself (e.g., x + x)?\nLets assume for this example eight-bit codes, which would allow us to represent\nintegers from 128 to 127. Consider a value for x, the integer 59, represented as\n00111011. If we add 59 to itself, we get the code 01110110. Note that the bits have\nall shifted to the left by one position. Is that a curiosity, or will that happen all the\ntime as long as the sum x + x is not too large to represent with the available number\nof bits?\nUsing our positional notation, the number 59 is\n0  26 + 1  25 + 1  24 + 1  23 + 0  22 + 1  21 + 1  20\nThe sum 59 + 59 is 2  59, which, in our representation, is\n2  (0  26 + 1  25 + 1  24 + 1  23 + 0  22 + 1  21 + 1  20)\nBut that is nothing more than\n0  27 + 1  26 + 1  25 + 1  24 + 0  23 + 1  22 + 1  21\nwhich shifts each digit one position to the left. Thus, adding a number to itself\n(provided there are enough bits to represent the result) is equivalent to shifting the\nrepresentation one bit position to the left.\n",
    "page_number": 38,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "2.5.2 Sign-Extension\nIt is often useful to represent a small number with fewer bits. For example, rather\nthan represent the value 5 as 0000000000000101, there are times when it makes\nnse to use only six bits to represent the value 5: 000101. There is little\nn, since we are all used to adding leading zeros without aecting the\na number. A check for $456.78 and a check for $0000456.78 are checks\nhe same value.\nat about negative representations? We obtained the negative representa-\nits positive counterpart by complementing the positive representation\ning 1. Thus, the representation for 5, given that 5 is represented as\n, is 111011. If 5 is represented as 0000000000000101, then the represen-\ntation for 5 is 1111111111111011. In the same way that leading 0s do not aect\nthe value of a positive number, leading 1s do not aect the value of a negative\nnumber.\nIn order to add representations of dierent lengths, it is rst necessary to\nrepresent them with the same number of bits. For example, suppose we wish to\nadd the number 13 to 5, where 13 is represented as 0000000000001101 and 5\nis represented as 111011. If we do not represent the two values with the same\nnumber of bits, we have\n0000000000001101\n+ 111011\nWhen we attempt to perform the addition, what shall we do with the missing bits\nin the representation for 5? If we take the absence of a bit to be a 0, then we are\nno longer adding 5 to 13. On the contrary, if we take the absence of bits to be 0s,\nwe have changed the 5 to the number represented as 0000000000111011, that\nis, +59. Not surprisingly, then, our result turns out to be the representation for 72.\nHowever, if we understand that a 6-bit 5 and a 16-bit 5 dier only in the\nnumber of meaningless leading 1s, then we rst extend the value of 5 to 16 bits\nbefore we perform the addition. Thus, we have\n0000000000001101\n+ 1111111111111011\n0000000000001000\nresult is +8, as we should expect.\ne value of a positive number does not change if we extend the sign bit\nany bit positions to the left as desired. Similarly, the value of a negative\nr does not change by extending the sign bit 1 as many bit positions to the\ndesired. Since in both cases it is the sign bit that is extended, we refer\nto the operation as Sign-EXTension, often abbreviated SEXT. Sign-extension is\nperformed in order to be able to operate on representations of dierent lengths.\nIt does not aect the values of the numbers being represented.\n2.5.3 Overow\nUp to now, we have always insisted that the sum of two integers be small enough\nto be represented by the available bits. What happens if such is not the case?\n",
    "page_number": 39,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "You are undoubtedly familiar with the odometer on the front dashboard of\nyour automobile. It keeps track of how many miles your car has been drivenbut\nonly up to a point. In the old days, when the odometer registered 99992 and you\ndrove it 100 miles, its new reading became 00092. A brand new car! The problem,\nas you know, is that the largest value the odometer could store was 99999, so the\nvalue 100092 showed up as 00092. The carry out of the ten-thousands digit was\nlost. (Of course, if you grew up in Boston, the carry out was not lost at allit\nwas in full display in the rusted chrome all over the car.)\nWe say the odometer overowed. Representing 100092 as 00092 is unaccept-\nable. As more and more cars lasted more than 100,000 miles, car makers felt the\npressure to add a digit to the odometer. Today, practically all cars overow at\n1,000,000 miles, rather than 100,000 miles.\nThe odometer provides an example of unsigned arithmetic. The miles you\nadd are always positive miles. The odometer reads 000129 and you drive 50 miles.\nThe odometer now reads 000179. Overow is a carry out of the leading digit.\nIn the case of signed arithmetic, or more particularly, 2s complement\narithmetic, overow is a little more subtle.\nLets return to our ve-bit 2s complement data type, which allowed us to\nrepresent integers from 16 to +15. Suppose we wish to add +9 and +11. Our\narithmetic takes the following form:\n01001\n01011\n10100\nNote that the sum is larger than +15, and therefore too large to represent with\nour 2s complement scheme. The fact that the number is too large means that the\nnumber is larger than 01111, the largest positive number we can represent with\na ve-bit 2s complement data type. Note that because our positive result was\nlarger than +15, it generated a carry into the leading bit position. But this bit\nposition is used to indicate the sign of a value. Thus, detecting that the result is\ntoo large is an easy matter. Since we are adding two positive numbers, the result\nmust be positive. Since the ALU has produced a negative result, something must\nbe wrong. The thing that is wrong is that the sum of the two positive numbers\nis too large to be represented with the available bits. We say that the result has\noverowed the capacity of the representation.\nSuppose instead, we had started with negative numbers, for example, 12\nand 6. In this case, our arithmetic takes the following form:\n10100\n11010\n01110\nHere, too, the result has overowed the capacity of the machine, since 12 + 6\nequals 18, which is more negative than 16, the negative number with the\nlargest allowable magnitude. The ALU obliges by producing a positive result.\nAgain, this is easy to detect since the sum of two negative numbers cannot be\npositive.\n",
    "page_number": 40,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Note that the sum of a negative number and a positive number never presents\na problem. Why is that? See Exercise 2.25.\n2.6 Operations on Bits\nPart II: Logical Operations\nWe have seen that it is possible to perform arithmetic (e.g., add, subtract) on val-\nues represented as binary patterns. Another class of operations useful to perform\non binary patterns is the set of logical operations.\n2.6.1 A Logical Variable\nLogical operations operate on logical variables. A logical variable can have one\nof two values, 0 or 1. The name logical is a historical one; it comes from the fact\nthat the two values 0 and 1 can represent the two logical values false and true,\nbut the use of logical operations has traveled far from this original meaning.\nThere are several basic logic functions, and most ALUs perform all of them.\n2.6.2 The AND Function\nAND is a binary logical function. This means it requires two pieces of input data.\nSaid another way, AND requires two source operands. Each source is a logical\nvariable, taking the value 0 or 1. The output of AND is 1 only if both sources have\nthe value 1. Otherwise, the output is 0. We can think of the AND operation as the\nALL operation; that is, the output is 1 only if ALL two inputs are 1. Otherwise,\nthe output is 0.\nA convenient mechanism for representing the behavior of a logical operation\nis the truth table. A truth table consists of n + 1 columns and 2n rows. The rst\nn columns correspond to the n source operands. Since each source operand is a\nlogical variable and can have one of two values, there are 2n unique values that\nthese source operands can have. Each such set of values (sometimes called an\ninput combination) is represented as one row of the truth table. The nal column\nin the truth table shows the output for each input combination.\nIn the case of a two-input AND function, the truth table has two columns for\nsource operands, and four (22) rows for unique input combinations.\nA\nB\nAND\n0\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n1\nWe can apply the logical operation AND to two bit patterns of m bits each. This\ninvolves applying the operation individually and independently to each pair of\n",
    "page_number": 41,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "bits in the two source operands. For example, if a and b in Example 2.6 are 16-\nbit patterns, then c is the AND of a and b. This operation is often called a bit-\nwise AND because the operation is applied to each pair of bits individually and\nindependently.\nExample 2.6\nIf c is the AND of a and b, where a=0011101001101001 and b=0101100100100001,\nwhat is c?\nWe form the AND of a and b by bit-wise ANDing the two values.\nThat means individually ANDing each pair of bits ai and bi to form ci. For\nexample, since a0=1 and b0=1, c0 is the AND of a0 and b0, which is 1.\nSince a6=1 and b6=0, c6 is the AND of a6 and b6, which is 0.\nThe complete solution for c is\na: 0011101001101001\nb: 0101100100100001\nc: 0001100000100001\nExam\nSuppose we have an eight-bit patternlets call it Ain which the rightmost two\nbits have particular signicance. The computer could be asked to do one of four tasks\ndepending on the value stored in the two rightmost bits of A. Can we isolate those\ntwo bits?\nYes, we can, using a bit mask. A bit mask is a binary pattern that enables the bits\nof A to be separated into two partsgenerally the part you care about and the part\nyou wish to ignore. In this case, the bit mask 00000011 ANDed with A produces 0 in\nbit positions 7 through 2, and the original values of bits 1 and 0 of A in bit positions\n1 and 0. The bit mask is said to mask out the values in bit positions 7 through 2.\nIf A is 01010110, the AND of A and the bit mask 00000011 is 00000010. If A is\n11111100, the AND of A and the bit mask 00000011 is 00000000.\nThat is, the result of ANDing any eight-bit pattern with the mask 00000011 is\none of the four patterns: 00000000, 00000001, 00000010, or 00000011. The result\nof ANDing with the mask is to highlight the two bits that are relevant.\n2.6.3 The OR Function\nOR is also a binary logical function. It requires two source operands, both of\nwhich are logical variables. The output of OR is 1 if any source has the value 1.\nOnly if both sources are 0 is the output 0. We can think of the OR operation as\nthe ANY operation; that is, the output is 1 if ANY of the two inputs are 1.\nThe truth table for a two-input OR function is\nA\nB\nOR\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n1\n1\nIn the same way that we applied the logical operation AND to two m-bit patterns,\nwe can apply the OR operation bit-wise to two m-bit patterns.\n",
    "page_number": 42,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Example 2.8\nIf c is the OR of a and b, where a=0011101001101001 and b=0101100100100001,\nas before, what is c?\nWe form the OR of a and b by bit-wise ORing the two values. That means\nindividually ORing each pair of bits ai and bi to form ci. For example, since a0=1\nand b0=1, c0 is the OR of a0 and b0, which is 1. Since a6=1 and b6=0, c6 is the OR\nof a6 and b6, which is also 1.\nThe complete solution for c is\na: 0011101001101001\nb: 0101100100100001\nc: 0111101101101001\nSometimes this OR operation is referred to as the inclusive-OR in order to distinguish\nit from the exclusive-OR function, which we will discuss momentarily.\n2.6.4 The NOT Function\nNOT is a unary logical function. This means it operates on only one source\noperand. It is also known as the complement operation. The output is formed by\ncomplementing the input. We sometimes say the output is formed by inverting\nthe input. A 1 input results in a 0 output. A 0 input results in a 1 output.\nThe truth table for the NOT function is\nA\nNOT\n0\n1\n1\n0\nIn the same way that we applied the logical operation AND and OR to two m-bit\npatterns, we can apply the NOT operation bit-wise to one m-bit pattern. If a is as\nbefore, then c is the NOT of a.\na: 0011101001101001\nc: 1100010110010110\n2.6.5 The Exclusive-OR Function\nExclusive-OR, often abbreviated XOR, is a binary logical function. It, too,\nrequires two source operands, both of which are logical variables. The output\nof XOR is 1 if one (but not both) of the two sources is 1. The output of XOR is 0\nif both sources are 1 or if neither source is 1. In other words, the output of XOR is\n1 if the two sources are dierent. The output is 0 if the two sources are the same.\nThe truth table for the XOR function is\nA\nB\nXOR\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0\n",
    "page_number": 43,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "In the same way that we applied the logical operation AND to two m-bit patterns,\nwe can apply the XOR operation bit-wise to two m-bit patterns.\nExample 2.9\nIf a and b are 16-bit patterns as before, then c (shown here) is the XOR of a and b.\na: 0011101001101001\nb: 0101100100100001\nc: 0110001101001000\nNote the distinction between the truth table for XOR shown here and the truth table\nfor OR shown earlier. In the case of exclusive-OR, if both source operands are 1, the\noutput is 0. That is, the output is 1 if the rst operand is 1 but the second operand is\nnot 1 or if the second operand is 1 but the rst operand is not 1. The term exclusive\nis used because the output is 1 if only one of the two sources is 1. The OR function,\non the other hand, produces an output 1 if only one of the two sources is 1, or if both\nsources are 1. Ergo, the name inclusive-OR.\nExample 2.10\nSuppose we wish to know if two patterns are identical. Since the XOR function pro-\nduces a 0 only if the corresponding pair of bits is identical, two patterns are identical\nif the output of the XOR is all 0s.\n2.6.6 DeMorgans Laws\nThere are two well-known relationships between AND functions and OR func-\nt\n\n1\n0\n1\n0\n0\n0\n0\n1\n1\n1\n0\n1\nFigure 2.2\nDeMorgans Law.\n",
    "page_number": 44,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "the\nB =\nfun\nrepr\nA AND B = A OR B\nWe can also state this behavior in English:\nIt is not the case that both A and B are false is equivalent to saying At least\none of A and B is true.\nThis equivalence is known as one of two DeMorgans Laws. Question: Is there\na similar result if one inverts both inputs to an OR function, and then inverts the\noutput?\n2.6.7 The Bit Vector\nWe have discussed the AND, OR, and NOT functions performed on m-bit pat-\nterns, where each of the m bits is a logical value (0 or 1) and the operations are per-\nformed bit-wise (i.e., individually and independently). We have also discussed the\nuse of an m-bit bit mask, where our choice of 0 or 1 for each bit allows us to isolate\nthe bits we are interested in focusing on and ignore the bits that dont matter.\nAn m-bit pattern where each bit has a logical value (0 or 1) independent of\nthe other bits is called a bit vector. It is a convenient mechanism for identifying\na property such that some of the bits identify the presence of the property and\nother bits identify the absence of the property.\nThere are many uses for bit vectors. The most common use is a bit mask,\nas we saw in Example 2.7. In that example, we had an eight-bit value, and we\nwanted to focus on bit 1 and bit 0 of that value. We did not care about the other\nbits. Performing the AND of that value with the bit mask 00000011 caused bit 7\nthrough bit 2 to be ignored, resulting in the AND function producing 00000000,\n00000001, 00000010, or 00000011, depending on the values of bit 1 and bit 0.\nThe bit mask is a bit vector, where the property of each of the bits is whether or\nnot we care about that bit. In Example 2.7, we only cared about bit 1 and bit 0.\nAnother use of a bit mask could involve a 16-bit 2s complement integer.\nSuppose the only thing we cared about was whether the integer was odd or even\nand whether it was positive or negative. The bit vector 1000000000000001 has a 1 in\nbit 15 that is used to identify a number as positive or negative, and a 1 in bit 0 that is\nused to identify if the integer is odd or even. If we perform the AND of this bit vector\nwith a 16-bit 2s complement integer, we would get one of four results, depending\non whether the integer was positive or negative and odd or even:\n0000000000000000\n0000000000000001\n1000000000000000\n1000000000000001\nAnother common use of bit vectors involves managing a complex system\nmade up of several units, each of which is individually and independently either\n",
    "page_number": 45,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "busy or available. The system could be a manufacturing plant where each unit is\na particular machine. Or the system could be a taxicab network where each unit\nis a particular taxicab. In both cases, it is important to identify which units are\nbusy and which are available so that work can be properly assigned.\nSay we have m such units. We can keep track of these m units with a bit\nvector, where a bit is 1 if the unit is free and 0 if the unit is busy.\nExample 2.11\nSuppose we have eight machines that we want to monitor with respect to their avail-\nability. We can keep track of them with an eight-bit BUSYNESS bit vector, where a\nbit is 1 if the unit is free and 0 if the unit is busy. The bits are labeled, from right to\nleft, from 0 to 7.\nThe BUSYNESS bit vector 11000010 corresponds to the situation where only\nunits 7, 6, and 1 are free and therefore available for work assignment.\nSuppose work is assigned to unit 7. We update our BUSYNESS bit vector by per-\nforming the logical AND, where our two sources are the current bit vector 11000010\nand the bit mask 01111111. The purpose of the bit mask is to clear bit 7 of the\nBUSYNESS bit vector, while leaving alone the values corresponding to all the other\nunits. The result is the bit vector 01000010, indicating that unit 7 is now busy.\nSuppose unit 5 nishes its task and becomes idle. We can update the\nBUSYNESS bit vector by performing the logical OR of it with the bit mask\n00100000. The result is 01100010, indicating that unit 5 is now available.\n2.7 Other Representations\nThere are many other representations of information that are used in computers.\nTwo that are among the most useful are the oating point data type and ASCII\ncodes. We will describe both in this section. We will also describe a notation\ncalled hexadecimal that, although not a data type, is convenient for humans to\nuse when dealing with long strings of 0s and 1s.\n2.7.1 Floating Point Data Type (Greater Range, Less Precision)\nMost of the arithmetic in this book uses integer values. The LC-3 computer, which\nyou will start studying in Chapter 4, uses the 16-bit, 2s complement integer data\ntype. That data type provides one bit to identify whether the number is positive\nor negative and 15 bits to represent the magnitude of the value. With 16 bits used\nin this way, we can express integer values between 32,768 and +32,767, that\nis, between 215 and +215  1. We say the precision of our value is 15 bits, and\nthe range is 216. As you learned in high school chemistry class, sometimes we\nneed to express much larger numbers, but we do not require so many digits of\nprecision. In fact, recall the value 6.0221023, which you may have been required\nto memorize back then. The range needed to express the value 1023 is far greater\nthan the largest value 2151 that is available with 16-bit 2s complement integers.\nOn the other hand, the 15 bits of precision available with 16-bit 2s complement\nintegers are overkill. We need only enough bits to express four signicant decimal\ndigits (6022).\n",
    "page_number": 46,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "So we have a problem. We have more bits than we need for precision. But we\ndont have enough bits to represent the range.\nThe oating point data type solves the problem. Instead of using all the bits\nto represent the precision of a value, the oating point data type allocates some of\nthe bits to the range of values (i.e., how big or how small) that can be expressed.\nThe rest of the bits (except for the sign bit) are used for precision.\nMost ISAs today specify more than one oating point data type. One of them,\nusually called oat, consists of 32 bits, allocated as follows:\nI\nFigure 2.3\nThe 32-bit oating point data type.\n2.7.1.1 Normalized Form\nLike Avogadros number that you learned years ago, the oating point data type\nrepresents numbers expressed in scientic notation, and mostly in normalized\nform:\nN = (1)S  1. fraction  2exponent127, 1  exponent  254\nwhere S, fraction, and exponent are the binary numbers in the elds of Figure 2.3.\nWe say mostly in normalized form because (as noted in the equation) the data\ntype represents a oating point number in normalized form only if the eight-bit\nexponent is restricted to the 254 unsigned integer values, 1 (00000001) through\n254 (11111110).\nAs you know, with eight bits, one can represent 256 values uniquely. For the\nother two integer values 0 (00000000) and 255 (11111111), the oating point\ndata type does not represent normalized numbers. We will explain what it does\nrepresent in Section 2.7.1.2 and Section 2.7.1.3.\nRecall again Avogadros number: (a) an implied + sign (often left out when\nthe value is positive), (b) four decimal digits 6.022 in normalized form (one non-\nzero decimal digit 6 before the decimal point) times (c) the radix 10 raised to\nthe power 23. The computers 32-bit oating point data type, on the other hand,\nconsists of (a) a sign bit (positive or negative), (b) 24 binary digits in normalized\nform (one non-zero binary digit to the left of the binary point) times (c) the radix\n2 raised to an exponent expressed in eight bits.\nWe determine the value of the 32-bit oating point representation shown in\nFigure 2.3 by examining its three parts.\n",
    "page_number": 47,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "The sign bit S is just a single binary digit, 0 for positive numbers, 1 for neg-\native numbers. The formula contains the factor 1S, which evaluates to +1 if\nS = 0, and 1 if S = 1.\nThe 23 fraction bits form the 24-bit quantity 1.fraction, where normalized\nform demands exactly one non-zero binary digit to the left of the binary point.\nSince there exists only one non-zero binary digit (i.e., the value 1), it is unneces-\nsary to explicitly store that bit in our 32-bit oating point format. In fact that is\nhow we get 24 bits of precision, the 1 to the left of the binary point that is always\npresent in normalized numbers and so is unnecessary to store, and the 23 bits of\nfraction that are actually part of the 32-bit data type.\nThe eight exponent bits are encoded in what we call an excess code, named\nfor the notion that one can get the *real* exponent by treating the code as\nan unsigned integer and subtracting the excess (sometimes called the bias). In\nthe case of the IEEE Floating Point that almost everyone uses, that excess (or\nbias) is 127 for 32-bit oating point numbers. Thus, an exponent eld contain-\ning 10000110 corresponds to the exponent +7 (since 10000110 represents the\nunsigned integer 134, from which we subtract 127, yielding +7). An exponent\neld containing 00000111 corresponds to the exponent 120 (since 00000111\nrepresents the unsigned integer 7, from which we subtract 127, yielding 120).\nThe exponent eld gives us numbers as large as 2+127 for an exponent eld con-\ntaining 254 (11111110) and as small as 2126 for an exponent eld containing\n1 (00000001).\nExample 2.12\nWhat does the oating point data type\n00111101100000000000000000000000\nrepresent?\nThe leading bit is a 0. This signies a positive number. The next eight bits\nrepresent the unsigned number 123. If we subtract 127, we get the actual expo-\nnent 4. The last 23 bits are all 0. Therefore, the number being represented is\n+1.000000000000000000000000  24, which is 1\n16.\nExample 2.13\nHow is the number 6 5\n8 represented in the oating point data type?\nFirst, we express 6 5\n8 as a binary number: 110.101.\n(1  22 + 1  21 + 0  20 + 1  21 + 0  22 + 1  23)\nThen we normalize the value, yielding 1.10101  22.\nThe sign bit is 1, reecting the fact that 6 5\n8 is a negative number. The exponent\neld contains 10000001, the unsigned number 129, reecting the fact that the real\nexponent is +2 (129127 = +2). The fraction is the 23 bits of precision, after remov-\ning the leading 1. That is, the fraction is 10101000000000000000000. The result is\nthe number 6 5\n8, expressed as a oating point number:\n1 10000001 10101000000000000000000\n",
    "page_number": 48,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Example 2.14\nThe following three examples provide further illustrations of the interpretation of\nthe 32-bit oating point data type according to the rules of the IEEE standard.\n0 10000011 00101000000000000000000 is 1.00101  24 = 18.5\nThe exponent eld contains the unsigned number 131. Since 131  127 is 4, the\nexponent is +4. Combining a 1 to the left of the binary point with the fraction eld\nto the right of the binary point yields 1.00101. If we move the binary point four\npositions to the right, we get 10010.1, which is 18.5.\n1 10000010 00101000000000000000000 is 1  1.00101  23 = 9.25\nThe sign bit is 1, signifying a negative number. The exponent is 130, signifying an\nexponent of 130  127, or +3. Combining a 1 to the left of the binary point with\nthe fraction eld to the right of the binary point yields 1.00101. Moving the binary\npoint three positions to the right, we get 1001.01, which is 9.25.\n0 11111110 11111111111111111111111 is 2128\nThe sign is +. The exponent is 254  127, or +127. Combining a 1 to the left\nof the binary point with the fraction eld to the right of the binary point yields\n1.11111111  1, which is approximately 2. Therefore, the result is approximately\n2128.\n2.7.1.2 Innities\nWe noted above that the oating point data type represented numbers expressed\nin scientic notation in normalized form provided the exponent eld does not\ncontain 00000000 or 11111111.\nIf the exponent eld contains 11111111, we use the oating point data type to\nrepresent various things, among them the notion of innity. Innity is represented\nby the exponent eld containing all 1s and the fraction eld containing all 0s. We\nrepresent positive innity if the sign bit is 0 and negative innity if the sign bit is 1.\n2.7.1.3 Subnormal Numbers\nThe smallest number that can be represented in normalized form is\nN = 1.00000000000000000000000  2126\nWhat about numbers smaller than 2126 but larger than 0? We call such num-\nbers subnormal numbers because they cannot be represented in normalized form.\nThe largest subnormal number is\nN = 0.11111111111111111111111  2126\nThe smallest subnormal number is\nN = 0.00000000000000000000001  2126, i.e., 223  2126 which is 2149.\nNote that the largest subnormal number is 2126 minus 2149. Do you see why\nthat is the case?\n",
    "page_number": 49,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Subnormal numbers are numbers of the form\nN = (1)S  0. fraction  2126\nWe represent them with an exponent eld of 00000000. The fraction eld is\nrepresented in the same way as with normalized numbers. That is, if the expo-\nnent eld contains 00000000, the exponent is 126, and the signicant digits are\nobtained by starting with a leading 0, followed by a binary point, followed by the\n23 bits of the fraction eld.\nExample 2.15\nWhat number corresponds to the following oating point representation?\n0 00000000 00001000000000000000000\nAnswer: The leading 0 means the number is positive. The next eight bits, a zero\nexponent, means the exponent is 126, and the bit to the left of the binary point is 0.\nThe last 23 bits form the number 0.00001000000000000000000, which equals 25.\nThus, the number represented is 25  2126, which is 2131.\nIncluding subnormal numbers allows very, very tiny numbers to be repre-\nsented.\nA detailed understanding of IEEE Floating Point Arithmetic is well beyond\nwhat should be expected in this rst course. Our purpose in including this sec-\ntion in the textbook is to at least let you know that there is, in addition to 2s\ncomplement integers, another very important data type available in almost all\nISAs, which is called oating point; it allows very large and very tiny numbers to\nbe expressed at the cost of reducing the number of binary digits of precision.\n2.7.2 ASCII Codes\nAnother representation of information is the standard code that almost all com-\nputer equipment manufacturers have agreed to use for transferring characters\nbetween the main computer processing unit and the input and output devices.\nThat code is an eight-bit code referred to as ASCII. ASCII stands for Ameri-\ncan Standard Code for Information Interchange. It (ASCII) greatly simplies the\ninterface between a keyboard manufactured by one company, a computer made\nby another company, and a monitor made by a third company.\nEach key on the keyboard is identied by its unique ASCII code. So, for\nexample, the digit 3 is represented as 00110011, the digit 2 is 00110010, the\nlowercase e is 01100101, and the ENTER key is 00001101. The entire set of\neight-bit ASCII codes is listed in Figure E.2 of Appendix E. When you type a key\non the keyboard, the corresponding eight-bit code is stored and made available to\nthe computer. Where it is stored and how it gets into the computer are discussed\nin Chapter 9.\nMost keys are associated with more than one code. For example, the ASCII\ncode for the letter E is 01000101, and the ASCII code for the letter e is 01100101.\n",
    "page_number": 50,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Both are associated with the same key, although in one case the Shift key is also\ndepressed while in the other case, it is not.\nIn order to display a particular character on the monitor, the computer must\ntransfer the ASCII code for that character to the electronics associated with the\nmonitor. That, too, is discussed in Chapter 9.\n2.7.3 Hexadecimal Notation\nWe have seen that information can be represented as 2s complement integers,\nas bit vectors, in oating point format, or as an ASCII code. There are other\nrepresentations also, but we will leave them for another book. However, before\nwe leave this topic, we would like to introduce you to a representation that is used\nmore as a convenience for humans than as a data type to support operations being\nperformed by the computer. This is the hexadecimal notation. As we will see, it\nevolves nicely from the positional binary notation and is useful for dealing with\nlong strings of binary digits without making errors.\nIt will be particularly useful in dealing with the LC-3 where 16-bit binary\nstrings will be encountered often.\nAn example of such a binary string is\n0011110101101110\nLets try an experiment. Cover the preceding 16-bit binary string of 0s and 1s\nwith one hand, and try to write it down from memory. How did you do? Hex-\nadecimal notation is about being able to do this without making mistakes. We\nshall see how.\nIn general, a 16-bit binary string takes the form\na15 a14 a13 a12 a11 a10 a9 a8 a7 a6 a5 a4 a3 a2 a1 a0\nwhere each of the bits ai is either 0 or 1.\nIf we think of this binary string as an unsigned integer, its value can be\ncomputed as\na15  215 + a14  214 + a13  213 + a12  212 + a11  211 + a10  210\n+ a9  29 + a8  28 + a7  27 + a6  26 + a5  25 + a4  24 + a3  23\n+ a2  22 + a1  21 + a0  20\nWe can factor 212 from the rst four terms, 28 from the second four terms, 24\nfrom the third set of four terms, and 20 from the last four terms, yielding\n212(a15  23 + a14  22 + a13  21 + a12  20)\n+ 28(a11  23 + a10  22 + a9  21 + a8  20)\n+ 24(a7  23 + a6  22 + a5  21 + a4  20)\n+ 20(a3  23 + a2  22 + a1  21 + a0  20)\nNote that the largest value inside a set of parentheses is 15, which would be the\ncase if each of the four bits is 1. If we replace what is inside each square bracket\n",
    "page_number": 51,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "with a symbol representing its value (from 0 to 15), and we replace 212 with its\nequivalent 163, 28 with 162, 24 with 161, and 20 with 160, we have\nh3  163 + h2  162 + h1  161 + h0  160\nwhere h3, for example, is a symbol representing\na15  23 + a14  22 + a13  21 + a12  20\nSince the symbols must represent values from 0 to 15, we assign symbols to these\nvalues as follows: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F. That is, we represent\n0000 with the symbol 0, 0001 with the symbol 1,  1001 with 9, 1010 with A,\n1011 with B,  1111 with F. The resulting notation is called hexadecimal, or\nbase 16.\nSo, for example, if the hex digits E92F represent a 16-bit 2s complement\ninteger, is the value of that integer positive or negative? How do you know?\nNow, then, what is this hexadecimal representation good for, anyway? It\nseems like just another way to represent a number without adding any benet.\nLets return to the exercise where you tried to write from memory the string\n0011110101101110\nIf we had rst broken the string at four-bit boundaries\n0011\n1101\n0110\n1110\nand then converted each four-bit string to its equivalent hex digit\n3\nD\n6\nE\nit would have been no problem to jot down (with the string covered) 3D6E.\nIn summary, although hexadecimal notation can be used to perform base-\n16 arithmetic, it is mainly used as a convenience for humans. It can be used to\nrepresent binary strings that are integers or oating point numbers or sequences\nof ASCII codes, or bit vectors. It simply reduces the number of digits by a factor\nof\nres\n2.1\n2.\n2.\nrequiring additional bits for each students unique bit pattern?\n",
    "page_number": 52,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "ital Logic Structures\nI\nn Chapter 1, we stated that computers were built from very large numbers\nof very simple structures. For example, Intels Broadwell-E5 microproces-\nsor, introduced in 2016, contained more than seven billion transistors. Similarly,\nIBMs Power9 microprocessor, introduced in 2017, contained eight billion tran-\nsistors. In this chapter, we will explain how the MOS transistor works (as a logic\nelement), show how these transistors are connected to form logic gates, and then\nshow how logic gates are interconnected to form larger units that are needed to\nconstruct a computer. In Chapter 4, we will connect those larger units and form\na computer.\nBut rst, the transistor.\n3.1 The Transistor\nMost computers today or rather most microprocessors (which form the core of the\ncomputer) are constructed out of MOS transistors. MOS stands for metal-oxide\nsemiconductor. The electrical properties of metal-oxide semiconductors are well\nbeyond the scope of what we want to understand in this course. They are below\nour lowest level of abstraction, which means that if somehow transistors start\nmisbehaving, we are at their mercy. However, it is unlikely in this course that we\nwill have any problems from the transistors.\nStill, it is useful to know that there are two types of MOS transistors: P-type\nand N-type. They both operate logically, very similar to the way wall switches\nwork.\nFigure 3.1 shows the most basic of electrical circuits. It consists of (1) a\npower supply (in this case, the 120 volts that come into your house if you live in\nthe United States, or the 220 volts if you live in most of the rest of the world),\n(2) a wall switch, and (3) a lamp (plugged into an outlet in the wall). In order for\nthe lamp to glow, electrons must ow; in order for electrons to ow, there must be\na closed circuit from the power supply to the lamp and back to the power supply.\n",
    "page_number": 53,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure 3.1\nA simple electric circuit showing the use of a wall switch.\nThe lamp can be turned on and o by simply manipulating the wall switch to\nmake or break the closed circuit.\nInstead of the wall switch, we could use an N-type or a P-type MOS transistor\nto make or break the closed circuit. Figure 3.2 shows a schematic rendering of\nan N-type transistor (a) by itself, and (b) in a circuit. Note (Figure 3.2a) that\nthe transistor has three terminals. They are called the gate, the source, and the\ndrain. The reasons for the names source and drain are not of interest to us in this\ncourse. What is of interest is the fact that if the gate of the N-type transistor is\nsupplied with 1.2 volts, the connection from source to drain acts like a piece of\nwire. We say (in the language of electricity) that we have a short circuit between\nthe source and drain. If the gate of the N-type transistor is supplied with 0 volts,\nthe connection between the source and drain is broken. We say that between the\nsource and drain we have an open circuit.\nFigure 3.2 shows the N-type transistor in a circuit with a battery and a bulb.\nWhen the gate is supplied with 1.2 volts, the transistor acts like a piece of wire,\ncompleting the circuit and causing the bulb to glow. When the gate is supplied\nFigure 3.2\nThe N-type MOS transistor.\n",
    "page_number": 54,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure 3.2c is a shorthand notation for describing the circuit of Figure 3.2b.\nRather than always showing the power supply and the complete circuit, electri-\ncal engineers usually show only the terminals of the power supply. The fact that\nthe power supply itself provides the completion of the completed circuit is well\nunderstood, and so is not usually shown.\nThe P-type transistor works in exactly the opposite fashion from the N-type\ntransistor. Figure 3.3 shows the schematic representation of a P-type transistor.\nWhen the gate is supplied with 0 volts, the P-type transistor acts (more or less)\nlike a piece of wire, closing the circuit. When the gate is supplied with 1.2 volts,\nthe P-type transistor acts like an open circuit. Because the P-type and N-type\nt\ns\nDrain\nFigure 3.3\nA P-type MOS transistor.\n3.2 Logic Gates\nOne step up from the transistor is the logic gate. That is, we construct basic logic\nstructures out of individual MOS transistors. In Chapter 2, we studied the behav-\nior of the AND, the OR, and the NOT functions. In this chapter, we construct\ntransistor circuits that implement each of these functions. The corresponding\ncircuits are called AND, OR, and NOT gates.\n3.2.1 The NOT Gate (Inverter)\nFigure 3.4 shows the simplest logic structure that exists in a computer. It is con-\nstructed from two MOS transistors, one P-type and one N-type. Figure 3.4a is the\nschematic representation of that circuit. Figure 3.4b shows the behavior of the\ncircuit if the input is supplied with 0 volts. Note that the P-type transistor acts\nlike a short circuit and the N-type transistor acts like an open circuit. The output\nis, therefore, connected to 1.2 volts. On the other hand, if the input is supplied\nwith 1.2 volts, the P-type transistor acts like an open circuit, but the N-type tran-\nsistor acts like a short circuit. The output in this case is connected to ground (i.e.,\n0 volts). The complete behavior of the circuit can be described by means of a\ntable, as shown in Figure 3.4c. If we replace 0 volts with the symbol 0 and 1.2\n",
    "page_number": 55,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": " \n1.2 volts\n.  \n0 volts\n      \n1      0\nFigure 3.4\nA CMOS inverter.\nvolts with the symbol 1, we have the truth table (Figure 3.4d) for the complement\nor NOT function, which we studied in Chapter 2.\nIn other words, we have just shown how to construct an electronic circuit that\nimplements the NOT logic function discussed in Chapter 2. We call this circuit\na NOT gate, or an inverter.\n3.2.2 OR and NOR Gates\nFigure 3.5 illustrates a NOR gate. Figure 3.5a is a schematic of a circuit that\nimplements a NOR gate. It contains two P-type and two N-type transistors.\nFigure 3.5b shows the behavior of the circuit if A is supplied with 0 volts\nand B is supplied with 1.2 volts. In this case, the lower of the two P-type transis-\ntors produces an open circuit, and the output C is disconnected from the 1.2-volt\npower supply. However, the leftmost N-type transistor acts like a piece of wire,\nconnecting the output C to 0 volts.\nNote that if both A and B are supplied with 0 volts, the two P-type transistors\nconduct, and the output C is connected to 1.2 volts. Note further that there is no\nambiguity here, since both N-type transistors act as open circuits, and so C is\ndisconnected from ground.\nIf either A or B is supplied with 1.2 volts, the corresponding P-type transistor\nresults in an open circuit. That is sucient to break the connection from C to\nthe 1.2-volt source. However, 1.2 volts supplied to the gate of one of the N-type\n",
    "page_number": 56,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "1\n1\n0\n1.2 volts\n0 volts\n1.2 volts\nA\nB\nFigure 3.5\nThe NOR gate.\ntransistors is sucient to cause that transistor to conduct, resulting in C being\nconnected to ground (i.e., 0 volts).\nFigure 3.5c summarizes the complete behavior of the circuit of Figure 3.5a.\nIt shows the behavior of the circuit for each of the four pairs of voltages that can\nbe supplied to A and B. That is,\nA = 0 volts,\nB = 0 volts\nA = 0 volts,\nB = 1.2 volts\nA = 1.2 volts,\nB = 0 volts\nA = 1.2 volts,\nB = 1.2 volts\nIf we replace the voltages with their logical equivalents, we have the truth\ntable of Figure 3.5d. Note that the output C is exactly the opposite of the logical\nOR function that we studied in Chapter 2. In fact, it is the NOT-OR function,\nmore typically abbreviated as NOR. We refer to the circuit that implements the\nNOR function as a NOR gate.\nIf we augment the circuit of Figure 3.5a by adding an inverter at its output, as\nshown in Figure 3.6a, we have at the output D the logical function OR. Figure 3.6a\nis the circuit for an OR gate. Figure 3.6b describes the behavior of this circuit if\nthe input variable A is set to 0 and the input variable B is set to 1. Figure 3.6c\nshows the circuits truth table.\n",
    "page_number": 57,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "64\nA\nB\n1    0      0     1\n1    1      0     1\nFigure 3.6\nThe OR gate.\n3.2.3 Why We Cant Simply Connect P-Type to Ground\nSome bright students have looked at our implementation of the OR gate (a NOR-\ngate followed by an inverter) and asked the question, why cant we simply connect\nthe transistors as shown in Figure 3.7a?\nLogically, it looks very tempting. Four transistors instead of six. Unfortu-\nnately, the electrical properties of transistors make this problematic. When we\nconnect a P-type transistor to 1.2 volts or an N-type transistor to ground, there is\nno voltage across the transistor, resulting in outputs as shown in Figure 3.5, for\nexample, of 0 volts or 1.2 volts, depending on the input voltages to A and B. How-\never, when we connect a P-type transistor to ground or an N-type transistor to 1.2\nvolts, because of the electrical characteristics of the transistors, we get what is\nusually referred to as a transmission voltage of approximately 0.5 volts across the\ntransistor. This results in the output of the transistor circuit of Figure 3.7 being\n0.5 volts + 0.5 volts, or 1.0 volt if A and B are both 0, and 0.7 volts (1.2 volts\nminus 0.5 volts) otherwise. Figure 3.7b shows the actual voltages in the resulting\ntruth table, rather than 0s and 1s. That is, even though the transistor circuit looks\nlike it would work, the transmission voltages across the transistors would yield\nan output voltage of 1 volt for a logical 0 and 0.7 volts for a logical 1. Not what\nwe would like for an OR gate!\n",
    "page_number": 58,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure 3.7\nAn OR gate (not really!).\n3.2.4 AND and NAND Gates\nFigure 3.8 shows an AND gate. Note that if either A or B is supplied with 0 volts,\nthere is a direct connection from C to the 1.2-volt power supply. The fact that C\nFigure 3.8\nThe AND gate.\n",
    "page_number": 59,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "(e) NOR gate \n(d) NAND gate \nFigure 3.9\nBasic logic gates.\nAgain, we note that there is no ambiguity. The fact that at least one of the two\ninputs A or B is supplied with 0 volts means that at least one of the two N-type\ntransistors whose gates are connected to A or B is open, and that consequently, C\nis disconnected from ground. Furthermore, the fact that C is at 1.2 volts means\nthe P-type transistor whose gate is connected to C is open-circuited. Therefore,\nD is not connected to 1.2 volts.\nOn the other hand, if both A and B are supplied with 1.2 volts, then both\nof their corresponding P-type transistors are open. However, their corresponding\nN-type transistors act like pieces of wire, providing a direct connection from C\nto ground. Because C is at ground, the rightmost P-type transistor acts like a\nclosed circuit, forcing D to 1.2 volts.\nFigure 3.8b is a truth table that summarizes the behavior of the circuit of\nFigure 3.8a. Note that the circuit is an AND gate. The circuit shown within the\ndashed lines (i.e., having output C) is a NOT-AND gate, which we generally\nabbreviate as NAND.\nThe gates just discussed are very common in digital logic circuits and in\ndigital computers. There are billions of inverters (NOT gates) in Intels Skylake\nmicroprocessor. As a convenience, we can represent each of these gates by stan-\ndard symbols, as shown in Figure 3.9. The bubble shown in the inverter, NAND,\nand NOR gates signies the complement (i.e., NOT) function. From now on, we\nwill not draw circuits showing the individual transistors. Instead, we will raise\nour level of abstraction and use the symbols shown in Figure 3.9.\n3.2.5 Gates with More Than Two Inputs\nBefore we leave the topic of logic gates, we should note that the notion of AND,\nOR, NAND, and NOR gates extends to larger numbers of inputs. One could build\na three-input AND gate or a four-input OR gate, for example. An n-input AND\ngate has an output value of 1 only if ALL n input variables have values of 1. If\nany of the n inputs has a value of 0, the output of the n-input AND gate is 0. An\nn-input OR gate has an output value of 1 if ANY of the n input variables has a\nvalue of 1. That is, an n-input OR gate has an output value of 0 only if ALL n\ninput variables have values of 0.\n",
    "page_number": 60,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "                   \n1      1      0       0\n1      1      1       1\nFigure 3.10\nA three-input AND gate.\nFigure 3.10 illustrates a three-input AND gate. Figure 3.10a shows its truth\ntable. Figure 3.10b shows the symbol for a three-input AND gate.\nQuestion: Can you draw a transistor-level circuit for a three-input AND gate?\nHow about a four-input AND gate? How about a four-input OR gate?\n3.3 Combinational Logic Circuits\nNow that we understand the workings of the basic logic gates, the next step\nis to build some of the logic structures that are important components of the\nmicroarchitecture of a computer.\nThere are fundamentally two kinds of logic structures, those that include the\nstorage of information and those that do not. In Sections 3.4, 3.5, and 3.6, we\nwill deal with structures that store information. In this section, we will deal with\nstructures that do not store information. These structures are sometimes referred\nto as decision elements. Usually, they are referred to as combinational logic struc-\ntures because their outputs are strictly dependent on the combination of input\nvalues that are being applied to the structure right now. Their outputs are not at\nall dependent on any past history of information that is stored internally, since no\ninformation can be stored internally in a combinational logic circuit.\nWe will next examine three useful combinational logic circuits: a decoder, a\nmux, and a one-bit adder.\n3.3.1 Decoder\nFigure 3.11 shows a logic gate implementation of a two-input decoder. A decoder\nhas the property that exactly one of its outputs is 1 and all the rest are 0s. The one\noutput that is logically 1 is the output corresponding to the input pattern that it is\nexpected to detect. In general, decoders have n inputs and 2n outputs. We say the\noutput line that detects the input pattern is asserted. That is, that output line has\nthe value 1, rather than 0 as is the case for all the other output lines. In Figure 3.11,\nnote that for each of the four possible combinations of inputs A and B, exactly one\noutput has the value 1 at any one time. In Figure 3.11b, the input to the decoder\nis 10, resulting in the third output line being asserted.\n",
    "page_number": 61,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "A\nB\n1,  if   A, B  is  11\n0\nFigure 3.11\nA two-input decoder.\nThe decoder is useful in determining how to interpret a bit pattern. We will\nsee in Chapter 5 that the work to be carried out by each instruction in the LC-\n3 computer is determined by a four-bit pattern that is the part of the instruction\ncalled the opcode, A 4-to-16 decoder is a simple combinational logic structure\nfor identifying what work is to be performed by each instruction.\n3.3.2 Mux\nFigure 3.12a shows a logic gate implementation of a two-input multiplexer, more\nFigure 3.12\nA 2-to-1 mux.\n",
    "page_number": 62,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "OUT\nFigure 3.13\nA four-input mux.\nThe mux of Figure 3.12 works as follows: Suppose S = 0, as shown in\nFigure 3.12b. Since the output of an AND gate is 0 unless all inputs are 1, the out-\nput of the rightmost AND gate is 0. Also, the output of the leftmost AND gate is\nwhatever the input A is. That is, if A = 0, then the output of the leftmost AND gate\nis 0, and if A = 1, then the output of the leftmost AND gate is 1. Since the output\nof the rightmost AND gate is 0, it has no eect on the OR gate. Consequently,\nthe output at C is exactly the same as the output of the leftmost AND gate. The\nnet result of all this is that if S = 0, the output C is identical to the input A.\nOn the other hand, if S = 1, it is B that is ANDed with 1, resulting in the\noutput of the OR gate having the value of B.\nIn summary, the output C is always connected to either the input A or the\ninput Bwhich one depends on the value of the select line S. We say S selects the\nsource of the mux (either A or B) to be routed through to the output C. Figure 3.12c\nshows the standard representation for a mux.\nIn general, a mux consists of 2n inputs and n select lines. Figure 3.13a\nshows a gate-level description of a four-input mux. It requires two select lines.\nFigure 3.13b shows the standard representation for a four-input mux.\nQuestion: Can you construct the gate-level representation for an eight-input\nmux? How many select lines must you have?\n3.3.3 A One-Bit Adder (a.k.a. a Full Adder)\nRecall in Chapter 2, we discussed binary addition. A simple algorithm for binary\naddition is to proceed as you have always done in the case of decimal addition,\nfrom right to left, one column at a time, adding the two digits from the two values\nplus the carry in, and generating a sum digit and a carry to the next column. The\nonly dierence here (with binary addition) is you get a carry after 1, rather than\nafter 9.\nFigure 3.14 is a truth table that describes the result of binary addition on one\ncolumn of bits within two n-bit operands. At each column, there are three values\n",
    "page_number": 63,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "1\n1\n1\n1\n1\nFigure 3.14\nThe truth table for a one-bit adder.\nthat must be added: one bit from each of the two operands A and B and the carry\nfrom the previous column. We designate these three bits as Ai, Bi, and Ci. There\nare two results, the sum bit (Si) and the carry over to the next column, Ci+1. Note\nthat if only one of the three bits equals 1, we get a sum of 1, and no carry (i.e.,\nCi+1 = 0). If two of the three bits equal 1, we get a sum of 0, and a carry of 1. If\nall three bits equal 1, the sum is 3, which in binary corresponds to a sum of 1 and\na carry of 1.\nFigure 3.15 shows a logic gate implementation of a one-bit adder. Note that\neach AND gate in Figure 3.15 produces an output 1 for exactly one of the eight\ninput combinations of Ai, Bi, and Ci. The output of the OR gate for Ci+1 must be 1\nin e\npro\nFigure 3.15\nGate-level description of a one-bit adder.\n",
    "page_number": 64,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "S0\nS1\nS2\nS3\nFigure 3.16\nA circuit for adding two 4-bit binary numbers.\nare the outputs of the AND gates corresponding to those input combinations.\nSimilarly, the inputs to the OR gate that generates Si are the outputs of the AND\ngates corresponding to the input combinations that require an output 1 for Si in\nthe truth table of Figure 3.14.\nNote that since the input combination 000 does not result in an output 1 for\neither Ci+1 or Si, its corresponding AND gate is not an input to either of the two\nOR gates.\nFigure 3.16 shows a circuit for adding two 4-bit binary numbers, using four\nof the one-bit adder circuits of Figure 3.15. Note that the carry out of column i is\nan input to the addition performed in column i + 1.\nIf we wish to implement a logic circuit that adds two 16-bit numbers, we can\ndo so with a circuit of 16 one-bit adders.\nWe should point out that historically the logic circuit of Figure 3.15 that\nprovides three inputs (Ai, Bi, and Ci) and two outputs (the sum bit Si and the\ncarry over to the next column Ci+1) has generally been referred to as a full adder\nto dierentiate it from another structure, which is called a half adder. The dis-\ntinction between the two is the carry bit. Note that the carry into the rightmost\ncolumn in Figure 3.16 is 0. That is, in the rightmost circuit, S0 and C1 depend only\non two inputs, A0 and B0. Since that circuit depends on only two inputs, it has been\nreferred to as a half adder. Since the other circuits depend on all three inputs, they\nare referred to as full adders. We prefer the term one-bit adder as a simpler term\nfor describing what is happening in each column.\n3.3.4 The Programmable Logic Array (PLA)\nFigure 3.17 illustrates a very common building block for implementing any col-\nlection of logic functions one wishes to implement. The building block is called\na programmable logic array (PLA). It consists of an array of AND gates (called\nan AND array) followed by an array of OR gates (called an OR array). The num-\nber of AND gates corresponds to the number of input combinations (rows) in\nthe truth table. For n-input logic functions, we need a PLA with 2n n-input AND\n",
    "page_number": 65,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "72\nFigure 3.17\nA programmable logic array.\ngates. In Figure 3.17, we have 23 three-input AND gates, corresponding to three\nlogical input variables. The number of OR gates corresponds to the number of\nlogic functions we wish to implement, that is, the number of output columns in\nthe truth table. The implementation algorithm is simply to connect the output of\nan AND gate to the input of an OR gate if the corresponding row of the truth table\nproduces an output 1 for that output column. Hence the notion of programmable.\nThat is, we say we program the connections from AND gate outputs to OR gate\ninputs to implement our desired logic functions.\nFigure 3.15 shows seven AND gates connected to two OR gates since our\nrequirement was to implement two functions (sum and carry) of three input vari-\nables. Figure 3.17 shows a PLA that can implement any four functions of three\nvariables by appropriately connecting AND gate outputs to OR gate inputs. That\nis, any function of three variables can be implemented by connecting the outputs\nof all AND gates corresponding to input combinations for which the output is 1\nto inputs of one of the OR gates. Thus, we could implement the one-bit adder by\nprogramming the two OR gates in Figure 3.17 whose outputs are W and X by\nconnecting or not connecting the outputs of the AND gates to the inputs of those\ntwo OR gates as specied by the two output columns of Figure 3.14.\n3.3.5 Logical Completeness\nBefore we leave the topic of combinational logic circuits, it is worth noting an\nimportant property of building blocks for logic circuits: logical completeness. We\nshowed in Section 3.3.4 that any logic function we wished to implement could\nbe accomplished with a PLA. We saw that the PLA consists of only AND gates,\nOR gates, and inverters. That means that any logic function can be implemented,\nprovided that enough AND, OR, and NOT gates are available. We say that the set\nof gates {AND, OR, NOT} is logically complete because we can build a circuit\nto carry out the specication of any truth table we wish without using any other\n",
    "page_number": 66,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "kind of gate. That is, the set of gates {AND, OR, and NOT} is logically complete\nbecause a barrel of AND gates, a barrel of OR gates, and a barrel of NOT gates are\nsucient to build a logic circuit that carries out the specication of any desired\ntruth table. The barrels may have to be big, but the point is, we do not need any\nother kind of gate to do the job.\nQuestion: Is there any single two-input logic gate that is logically complete?\nFor example, is the NAND gate logically complete? Hint: Can I implement a\nNOT gate with a NAND gate? If yes, can I then implement an AND gate using a\nNAND gate followed by a NOT gate? If yes, can I implement an OR gate using\njust AND gates and NOT gates?\nIf all of the above is true, then the NAND gate is logically complete, and I\ncan implement any desired logic function as described by its truth table with a\nbarrel of NAND gates.\n3.4 Basic Storage Elements\nRecall our statement at the beginning of Section 3.3 that there are two kinds of\nlogic structures, those that involve the storage of information and those that do\nnot. We have discussed three examples of those that do not: the decoder, the mux,\nand the full adder. Now we are ready to discuss logic structures that do include\nthe storage of information.\n3.4.1 The R-S Latch\nA simple example of a storage element is the R-S latch. It can store one bit of\ninformation, a 0 or a 1. The R-S latch can be implemented in many ways, the\nsimplest being the one shown in Figure 3.18. Two 2-input NAND gates are con-\nnected such that the output of each is connected to one of the inputs of the other.\nThe remaining inputs S and R are normally held at a logic level 1.\nThe R-S latch\nets its name from the old desi nations for settin\nthe latch\nR\nb\nFigure 3.18\nAn R-S latch.\nThe Quiescent State\nWe describe the quiescent (or quiet) state of a latch as\nthe state when the latch is storing a value, either 0 or 1, and nothing is trying to\n",
    "page_number": 67,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "change that value. This is the case when inputs S and R both have the logic value\n1. In Figure 3.18 the letter a designates the value that is currently stored in the\nlatch, which we also refer to as the output of the latch.\nConsider rst the case where the value stored and therefore the output a is\n1. Since that means the value A is 1 (and since we know the input R is 1 because\nwe are in the quiescent state), the NAND gates output b must be 0. That, in turn,\nmeans B must be 0, which results in the output a equal to 1. As long as the inputs\nS and R remain 1, the state of the circuit will not change. That is, the R-S latch\nwill continue to store the value 1 (the value of the output a).\nIf, on the other hand, we assume the output a is 0, then A must be 0, and\nthe out ut b must be 1. That, in turn, results in B equal to 1, and combined with\nt S equal to 1 (again due to quiescence), results in the output a equal to\nn, as long as the inputs S and R remain 1, the state of the circuit will not\n. In this case, we say the R-S latch stores the value 0.\nthe Latch to a 1 or a 0\nThe latch can be set to 1 by momentarily setting\nprovided we keep the value of R at 1. Similarly, the latch can be set to 0\nentarily setting R to 0, provided we keep the value of S at 1. In order for\nthe R-S latch to work properly, both S and R must never be allowed to be set to 0\nat the same time.\nWe use the term set to denote setting a variable to 0 or 1, as in set to 0 or\nset to 1. In addition, we often use the term clear to denote the act of setting a\nvariable to 0.\nIf we set S to 0 for a very brief period of time, this causes a to equal 1, which\nin turn causes A to equal 1. Since R is also 1, the output at b must be 0. This causes\nB to be 0, which in turn makes a equal to 1. If, after that very brief period of time,\nwe now return S to 1, it does not aect a. Why? Answer: Since B is also 0, and\nsince only one input 0 to a NAND gate is enough to guarantee that the output of\nthe NAND gate is 1, the latch will continue to store a 1 long after S returns to 1.\nIn the same way, we can clear the latch (set the latch to 0) by setting R to 0\nfor a very short period of time.\nWe should point out that if both S and R were allowed to be set to 0 at the same\ntime, the outputs a and b would both be 1, and the nal state of the latch would\ndepend on the electrical properties of the transistors making up the gates and\nnot on the logic being performed. How the electrical properties of the transistors\nwould determine the nal state in this case is a subject we will have to leave for\na later semester. :-(\nFinally, we should note that when a digital circuit is powered on, the latch\ncan be in either of its two states, 0 or 1. It does not matter which state since we\nnever use that information until after we have set it to 1 or 0.\n3.4.2 The Gated D Latch\nTo be useful, it is necessary to control when a latch is set and when it is cleared.\nA simple way to accomplish this is with the gated latch.\nFigure 3.19 shows a logic circuit that implements a gated D latch. It consists\nof the R-S latch of Figure 3.18, plus two additional NAND gates that allow the\n",
    "page_number": 68,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "R\nFigure 3.19\nA gated D latch.\nlatch to be set to the value of D, but only when WE is asserted (i.e., when WE\nequals 1). WE stands for write enable. When WE is not asserted (i.e., when WE\nequals 0), the outputs S and R are both equal to 1. Since S and R are inputs to the\nR-S latch, if they are kept at 1, the value stored in the latch remains unchanged,\nas we explained in Section 3.4.1. When WE is momentarily set to 1, exactly one\nof the outputs S or R is set to 0, depending on the value of D. If D equals 1, then S\nis set to 0. If D equals 0, then both inputs to the lower NAND gate are 1, resulting\nin R being set to 0. As we saw earlier, if S is set to 0, the R-S latch is set to 1. If R\nis set to 0, the R-S latch is set to 0. Thus, the R-S latch is set to 1 or 0 according\nto whether D is 1 or 0. When WE returns to 0, S and R return to 1, and the value\nstored in the R-S latch persists.\n3.5 The Concept of Memory\nWe now have all the tools we need to describe one of the most important struc-\ntures in the electronic digital computer, its memory. We will see in Chapter 4\nhow memory ts into the basic scheme of computer processing, and you will see\nthroughout the rest of the book and indeed the rest of your work with computers\nhow important the concept of memory is to computing.\nMemory is made up of a (usually large) number of locations, each uniquely\nidentiable and each having the ability to store a value. We refer to the unique\nidentier associated with each memory location as its address. We refer to the\nnumber of bits of information stored in each location as its addressability.\nFor example, an advertisement for a laptop computer might say, This com-\nputer comes with 2 gigabytes of memory. Actually, most ads generally use\nthe abbreviation 2 GB (or, often: 2 Gig). This statement means, as we will\nexplain momentarily, that the laptop includes two billion memory locations, each\ncontaining one byte of information.\n3.5.1 Address Space\nWe refer to the total number of uniquely identiable locations as the memorys\naddress space. A 2 GB memory, for example, refers to a memory that consists of\ntwo billion uniquely identiable memory locations.\n",
    "page_number": 69,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Actually, the number two billion is only an approximation, due to the way we\nspecify memory locations. Since everything else in the computer is represented\nby sequences of 0s and 1s, it should not be surprising that memory locations are\nidentied by binary addresses as well. With n bits of address, we can uniquely\nidentify 2n locations. Ten bits provide 1024 locations, which is approximately\n1000. If we have 20 bits to represent each address, we have 220 uniquely identi-\nable locations, which is approximately one million. With 30 bits, we have 230\nlocations, which is approximately one billion. In the same way we use the prexes\nkilo to represent 210 (approximately 1000) and mega to represent 220 (approx-\nimately one million), we use the prex giga to represent 230 (approximately\none billion). Thus, 2 giga really corresponds to the number of uniquely iden-\ntiable locations that can be specied with 31 address bits. We say the address\nspace is 231, which is exactly 2,147,483,648 locations, rather than 2,000,000,000,\nalthough we colloquially refer to it as two billion.\n3.5.2 Addressability\nThe number of bits stored in each memory location is the memorys addressabil-\nity. A 2-gigabyte memory (written 2GB) is a memory consisting of 2,147,483,648\nmemory locations, each containing one byte (i.e., eight bits) of storage. Most\nmemories are byte-addressable. The reason is historical; most computers got their\nstart processing data, and one character stroke on the keyboard corresponds to one\n8-bit ASCII code, as we learned in Chapter 2. If the memory is byte-addressable,\nthen each ASCII character occupies one location in memory. Uniquely identi-\nfying each byte of memory allows individual bytes of stored information to be\nchanged easily.\nMany computers that have been designed specically to perform large scien-\ntic calculations are 64-bit addressable. This is due to the fact that numbers used\nin scientic calculations are often represented as 64-bit oating-point quantities.\nRecall that we discussed the oating-point data type in Chapter 2. Since scientic\ncalculations are likely to use numbers that require 64 bits to represent them, it is\nreasonable to design a memory for such a computer that stores one such number\nin each uniquely identiable memory location.\n3.5.3 A 22-by-3-Bit Memory\nFigure 3.20 illustrates a memory of size 22 by 3 bits. That is, the memory\nhas an address space of four locations and an addressability of three bits.\nA memory of size 22 requires two bits to specify the address. We describe\nthe two-bit address as A[1:0]. A memory of addressability three stores three\nbits of information in each memory location. We describe the three bits\nof data as D[2:0]. In both cases, our notation A[high:low] and D[high:low]\nreects the fact that we have numbered the bits of address and data from\nright to left, in order, starting with the rightmost bit, which is numbered 0.\nThe notation [high:low] means a sequence of high  low + 1 bits such that\nhigh is the bit number of the leftmost (or high) bit number in the sequence\nand low is the bit number of the rightmost (or low) bit number in the sequence.\n",
    "page_number": 70,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "D [1]\nD[2]\nD[0]\nFigure 3.20\nA 22-by-3-bit memory.\nAccesses of memory require decoding the address bits. Note that the address\ndecoder takes as input the address bits A[1:0] and asserts exactly one of its four\noutputs, corresponding to the word line being addressed. In Figure 3.20, each row\nof the memory corresponds to a unique three-bit word, thus the term word line.\nMemory can be read by applying the address A[1:0], which asserts the word line\nto be read. Note that each bit of the memory is ANDed with its word line and then\nORed with the corresponding bits of the other words. Since only one word line\ncan be asserted at a time, this is eectively a mux with the output of the decoder\nproviding the select function to each bit line. Thus, the appropriate word is read\nat D[2:0].\nFigure 3.21 shows the process of reading location 3. The code for 3 is 11.\nThe address A[1:0]=11 is decoded, and the bottom word line is asserted. Note\nthat the three other decoder outputs are not asserted. That is, they have the value\n0. The value stored in location 3 is 101. These three bits are each ANDed with\ntheir word line producing the bits 101, which are supplied to the three output\nOR gates. Note that all other inputs to the OR gates are 0, since they have been\nproduced by ANDing with their unasserted word lines. The result is that D[2:0]\n= 101. That is, the value stored in location 3 is output by the OR gates. Memory\n",
    "page_number": 71,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "78\nD[1]\nD[2]\nD[0]\n  \n1\n0\n1\nFigure 3.21\nReading location 3 in our 22-by-3-bit memory.\ncan be written in a similar fashion. The address specied by A[1:0] is presented to\nthe address decoder, resulting in the correct word line being asserted. With write\nenable (WE) also asserted, the three bits D[2:0] can be written into the three gated\nlatches corresponding to that word line.\n3.6 Sequential Logic Circuits\nIn Section 3.3, we discussed digital logic structures that process information\n(decision structures, we call them) wherein the outputs depend solely on the val-\nues that are present on the inputs now. Examples are muxes, decoders, and full\nadders. We call these structures combinational logic circuits. In these circuits,\nthere is no sense of the past. Indeed, there is no capability for storing any infor-\nmation about anything that happened before the present time. In Sections 3.4\nand 3.5, we described structures that do store informationin Section 3.4, some\nbasic storage elements, and in Section 3.5, a simple 22-by-3-bit memory.\n",
    "page_number": 72,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure 3.22\nSequential logic circuit block diagram.\nIn this section, we discuss digital logic structures that can both process infor-\nmation (i.e., make decisions) and store information. That is, these structures base\ntheir decisions not only on the input values now present, but also (and this is\nvery important) on what has happened before. These structures are usually called\nsequential logic circuits. They are distinguishable from combinational logic cir-\ncuits because, unlike combinational logic circuits, they contain storage elements\nthat allow them to keep track of prior history information. Figure 3.22 shows a\nblock diagram of a sequential logic circuit. Note the storage elements. Note also\nthat the output can be dependent on both the inputs now and the values stored in\nthe storage elements. The values stored in the storage elements reect the history\nof what has happened before.\nSequential logic circuits are used to implement a very important class of\nmechanisms called nite state machines. We use nite state machines in essen-\ntially all branches of engineering. For example, they are used as controllers of\nelectrical systems, mechanical systems, and aeronautical systems. A trac light\ncontroller that sets the trac light to red, yellow, or green depends on the light\nthat is currently on (history information) and input information from sensors such\nas trip wires on the road, a timer keeping track of how long the current light has\nbeen on, and perhaps optical devices that are monitoring trac.\nWe will see in Chapter 4 when we introduce the von Neumann model of a\ncomputer that a nite state machine is at the heart of the computer. It controls the\nprocessing of information by the computer.\n3.6.1 A Simple Example: The Combination Lock\nA simple example shows the dierence between combinational logic structures\nand sequential logic structures. Suppose one wishes to secure a bicycle with a\nlock, but does not want to carry a key. A common solution is the combination\nlock. The person memorizes a combination and uses it to open the lock. Two\ncommon types of locks are shown in Figure 3.23.\nIn Figure 3.23a, the lock consists of a dial, with the numbers from 0 to 30\nequally spaced around its circumference. To open the lock, one needs to know\nthe combination. One such combination could be: R13-L22-R3. If this were\nthe case, one would open the lock by turning the dial two complete turns to the\nright (clockwise), and then continuing until the dial points to 13, followed by one\n",
    "page_number": 73,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "80\n(a)\n(b)\nFigure 3.23\nCombination locks.\ncomplete turn to the left (counterclockwise), and then continuing until the dial\npoints to 22, followed by turning the dial again to the right (clockwise) until it\npoints to 3. At that point, the lock opens. What is important here is the sequence\nof the turns. The lock will not open, for example if one performed two turns to the\nright, and then stopped on 22 (instead of 13), followed by one complete turn to\nthe left, ending on 13, followed by one turn to the right, ending on 3. That is, even\nthough the nal position of the dial is 3, and even though R22-L13-R3 uses the\nsame three numbers as the combination R13-L22-R3, the lock would not open.\nWhy? Because the lock stores the previous rotations and makes its decision (open\nor dont open) on the basis of the the history of the past operations, that is, on the\ncorrect sequence being performed.\nAnother type of lock is shown in Figure 3.23b. The mechanism consists of\n(usually) four wheels, each containing the digits 0 through 9. When the digits\nare lined up properly, the lock will open. In this case, the combination is the set\nof four digits. Whether or not this lock opens is totally independent of the past\nrotations of the four wheels. The lock does not care at all about past rotations.\nThe only thing important is the current value of each of the four wheels. This is\na simple example of a combinational structure.\nIt is curious that in our everyday speech, both mechanisms are referred to\nas combination locks. In fact, only the lock of Figure 3.23b is a combinational\nlock. The lock of Figure 3.23a would be better called a sequential lock!\n3.6.2 The Concept of State\nFor the mechanism of Figure 3.23a to work properly, it has to keep track of the\nsequence of rotations leading up to the opening of the lock. In particular, it has\nto dierentiate the correct sequence R13-L22-R3 from all other sequences. For\nexample, R22-L13-R3 must not be allowed to open the lock. Likewise, R10-L22-\nR3 must also not be allowed to open the lock.\nFor the lock of Figure 3.23a to work, it must identify several relevant\nsituations, as follows:\nA. The lock is not open, and NO relevant operations have been\nperformed.\nB. The lock is not open, but the user has just completed the\nR13 operation.\n",
    "page_number": 74,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "C. The lock is not open, but the user has just completed R13,\nfollowed by L22.\nD. The lock is open, since the user has just completed R13,\nfollowed by L22, followed by R3.\nWe have labeled these four situations A, B, C, and D. We refer to each of these\nsituations as the state of the lock.\nThe notion of state is a very important concept in computer engineering, and\nactually, in just about all branches of engineering. The state of a mechanism\nmore generally, the state of a systemis a snapshot of that system in which all\nrelevant items are explicitly expressed.\nThat is: The state of a system is a snapshot of all the relevant elements of the\nsystem at the moment the snapshot is taken.\nIn the case of the lock of Figure 3.23a, there are four states A, B, C, and D.\nEither the lock is open (State D), or if it is not open, we have already performed\neither zero (State A), one (State B), or two (State C) correct operations. This is\nthe sum total of all possible states that can exist.\nQuestion: Why are there exactly four states needed to describe the combina-\ntion lock of Figure 3.23a? Can you think of a snapshot of the combination lock\nafter an operation (Rn or Ln) that requires a fth state because it is not covered\nby one of the four states A, B, C, or D?\nThere are many examples of systems that you are familiar with that can be\neasily described by means of states.\nThe state of a game of basketball can be described by the scoreboard in the\nbasketball arena. Figure 3.24 shows the state of the basketball game as Texas 73,\nOklahoma 68, 7 minutes and 38 seconds left in the second half, 14 seconds left on\nthe s\nfouls\nball\nFigure 3.24\nAn example of a state.\n",
    "page_number": 75,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "82\n(a)\n(b)\n(c)\nFigure 3.25\nThree states in a tic-tac-toe machine.\na two-point shot, the new state would be described by the updated scoreboard.\nThat is, the score would then be Texas 75, Oklahoma 68, the time remaining in\nthe game would be 7 minutes and 26 seconds, the shot clock would be back to 25\nseconds, and Oklahoma would have the ball.\nThe game of tic-tac-toe can also be described in accordance with the notion\nof state. Recall that the game is played by two people (or, in our case, a person\nand the computer). The state is a snapshot of the game in progress each time the\ncomputer asks the person to make a move. The game is played as follows: There\nare nine locations on the diagram. The person and then the computer take turns\nplacing an X (the person) and an O (the computer) in an empty location. The\nperson goes rst. The winner is the rst to place three symbols (three Xs for the\nperson, three Os for the computer) in a straight line, either vertically, horizontally,\nor diagonally.\nThe initial state, before either the person or the computer has had a turn, is\nshown in Figure 3.25a. Figure 3.25b shows a possible state of the game when the\nperson is prompted for a second move, if he/she put an X in the upper left corner\nas his/her rst move, and the computer followed with an O in the middle square\nas its rst move. Figure 3.25c shows a possible state of the game when the person\nis prompted for a third move if he/she put an X in the upper right corner on the\nsecond move, and the computer followed by putting its second O in the upper\nmiddle location.\nOne nal example: a very old soft drink machine, when drinks sold for\n15 cents, and the machine would only take nickels (5 cents) and dimes (10 cents)\nand not be able to give change.\nThe state of the machine can be described as the amount of money inserted,\nand whether the machine is open (so one can remove a bottle). There are only\nthree possible states:\nA. The lock is open, so a bottle can be (or has been!) removed.\nB. The lock is not open, but 5 cents has been inserted.\nC. The lock is not open, but 10 cents has been inserted.\n3.6.3 The Finite State Machine and Its State Diagram\nWe have seen that a state is a snapshot of all relevant parts of a system at a\nparticular point in time. At other times, that system can be in other states. We\nhave described four systems: a combination lock, a basketball game, a tic-tac-\ntoe machine, and a very old soft drink machine when a bottle of cola cost only\n",
    "page_number": 76,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "15 cents. The behavior of each of these systems can be specied by a nite state\nmachine, and represented as a state diagram.\nA nite state machine consists of ve elements:\n1. a nite number of states\n2. a nite number of external inputs\n3. a nite number of external outputs\n4. an explicit specication of all state transitions\n5. an explicit specication of what determines each external\noutput value.\nThe set of states represents all possible situations (or snapshots) that the sys-\ntem can be in. Each state transition describes what it takes to get from one state\nto another.\nLets examine the nite state machines for these four systems.\nThe Combination Lock\nA state diagram is a convenient representation of a\nnite state machine. Figure 3.26 is a state diagram for the combination lock.\nRecall, we identied four states A, B, C, and D. Which state we are in depends\non the progress we have made in getting from a random initial state to the lock\nbeing open. In the state diagram of Figure 3.26, each circle corresponds to one\nof the four states, A, B, C, or D.\nThe external inputs are R13, L22, R3, and R-other-than-13, L-other-than-22,\nand R-other-than-3.\nThe external output is either the lock is open or the lock is not open. (One\nlogical variable will suce to describe that!) As shown in the state diagram, in\nstates A, B, and C, the combination lock is locked. In state D, the combination\nlo\nth\nFigure 3.26\nState diagram of the combination lock of Figure 3.23a.\n",
    "page_number": 77,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "on each arc species which state the system is coming from and which state it is\ngoing to. We refer to the state the system is coming from as the current state,\nand the state it is going to as the next state. The combination lock has eight state\ntransitions. Associated with each transition is the input that causes the transition\nfrom the current state to the next state. For example, R13 causes the transition\nfrom state A to state B.\nA couple of things are worth noting. First, it is usually the case that from a\ncurrent state there are multiple transitions to next states. The state transition that\noccurs depends on both the current state and the value of the external input. For\nexample, if the combination lock is in state B, and the input is L22, the next state\nis state C. If the current state is state B and the input is anything other than L22,\nthe next state is state A. In short, the next state is determined by the combination\nof the current state and the current external input.\nThe output values of a system can also be determined by the combination of\nthe current state and the value of the current external input. However, as is the case\nfor the combination lock, where states A, B, and C specify the lock is locked,\nand state D species the lock is unlocked, the output can also be determined\nsolely by the current state of the system. In all the systems we will study in this\nbook, the output values will be specied solely by the current state of the system.\nA Very Old Soft Drink Machine\nFigure 3.27 is the state diagram for the soft\ndrink machine.\nThe soft drink machine has only three states: 5 cents has been inserted,\n10 cents has been inserted, and at least 15 cents has been inserted. Transitions\nare caused by the insertion (the input) of a nickel or a dime. The output is asso-\nFigure 3.27\nState diagram of the soft drink machine.\nA Basketball Game\nWe could similarly draw a state diagram for the basketball\ngame we described earlier, where each state would be one possible conguration\nof the scoreboard. A transition would occur if either the referee blew a whistle or\nthe other team got the ball. We showed earlier the transition that would be caused\nby Texas scoring a two-point shot. Clearly, the number of states in the nite state\nmachine describing a basketball game would be huge.\n",
    "page_number": 78,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Also clearly, the number of legitimate transitions from one state to another is\nsmall, compared to the number of arcs one could draw connecting arbitrary pairs\nof states. For example, there is no arc from a score of Texas 68, Oklahoma 67 to\nTexas 75, Oklahoma 91, since no single input can cause that transition. The input\nis the activity that occurred on the basketball court since the last transition. Some\ninput values are: Texas scored two points, Oklahoma scored three points, Texas\nstole the ball, Oklahoma successfully rebounded a Texas shot, and so forth.\nThe output is the nal result of the game. The output has three values: Game\nstill in progress, Texas wins, Oklahoma wins.\nQuestion: Can one have an arc from a state where the score is Texas 30,\nOklahoma 28 to a state where the score is tied, Texas 30, Oklahoma 30? Is it\npossible to have two states, one where Texas is ahead 30-28 and the other where\nthe score is tied 30-30, but no arc between the two?\nA Tic-Tac-Toe Machine\nWe could also draw a state diagram for a tic-tac-toe\nmachine, in our case when a person is playing against a computer. Each state is a\nrepresentation of the position of the game when the person is asked to put an X\ninto one of the empty cells. Figure 3.25 shows three states. The transition from the\nstate of Figure 3.25a to the state of Figure 3.25b is caused by the person putting\nan X in the top left cell, followed by the computer putting an O in the center cell.\nThe transition from the state of Figure 3.25b to the state of Figure 3.25c is caused\nby the person putting an X in the top right cell, followed by the computer putting\nan O in the top middle cell.\nSince there are nine cells, and each state has an X, an O, or nothing in each\ncell, there must be fewer than 39 states in the tic-tac-toe machine. Clearly there\nare far fewer than that, due to various constraints of the game.\nThere are nine inputs, corresponding to the nine cells a person can put an\nX in. There are three outputs: (a) game still in progress, (b) person wins, and\n(c) computer wins.\n3.6.4 The Synchronous Finite State Machine\nUp to now a transition from a current state to a next state in our nite state machine\nhappened when it happened. For example, a person could insert a nickel into the\nsoft drink machine and then wait 10 seconds or 10 minutes before inserting the\nnext coin into the machine. And the soft drink machine would not complain. It\nwould not dispense the soft drink until 15 cents was inserted, but it would wait\npatiently as long as necessary for the 15 cents to be inserted. That is, there is no\nxed amount of time between successive inputs to the nite state machine. This\nis true in the case of all four systems we have discussed. We say these systems are\nasynchronous because there is nothing synchronizing when each state transition\nmust occur.\nHowever, almost no computers work that way. On the contrary, we say that\ncomputers are synchronous because the state transitions take place, one after the\nother, at identical xed units of time. They are controlled by a synchronous nite\nstate machine. We will save for Chapter 4 and beyond the state transitions that\noccur at identical, xed units of time that control a computer. In this chapter, we\n",
    "page_number": 79,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "will take on a simpler task, the design of a trac controller, an admittedly simpler\nstructure, but one that is also controlled by a synchronous nite state machine.\nIt is worth pointing out that both the four asynchronous nite state machines\ndiscussed above and the synchronous nite state machine that controls a digital\ncomputer share an important characteristic: They carry out work, one state tran-\nsition at a time, moving closer to a goal. In the case of the combination lock, as\nlong as you make the correct moves, each state transition takes us closer to the\nlock opening. In the case of the soft drink machine, each state transition takes us\ncloser to enjoying the taste of the soft drink. In the case of a computer, each state\ntransition takes us closer to solving a problem by processing a computer program\nthat someone has written.\n3.6.5 The Clock\nA synchronous nite state machine transitions from its current state to its next\nstate after an identical xed interval of time. Control of that synchronous behavior\nis in part the responsibility of the clock circuit.\nA clock circuit produces a signal, commonly referred to as THE clock, whose\nvalue alternates between 0 volts and some specied xed voltage. In digital logic\nterms, the clock is a signal whose value alternates between 0 and 1. Figure 3.28\nshows the value of the clock signal as a function of time. Each of the repeated\nsequence of identical intervals is referred to as a clock cycle. A clock cycle starts\nwhen the clock signal transitions from 0 to 1 and ends the next time the clock\nsignal transitions from 0 to 1.\nWe will see in Chapter 5 and beyond that in each clock cycle, a computer\ncan perform a piece of useful work. When people say their laptop computers run\nat a frequency of 2 gigahertz, they are saying their laptop computers perform\ntwo billion pieces of work each second since 2 gigahertz means two billion clock\ncycles each second, each clock cycle lasting for just one-half of a nanosecond. The\nsynchronous nite state machine makes one state transition each clock cycle.\nWe will show by means of a trac signal controller how the clock signal\ncontrols the transition, xed clock cycle after xed clock cycle, from one state to\nthe next.\n1\n0\nFigure 3.28\nA clock signal.\n",
    "page_number": 80,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "3.6.6 Example: A Danger Sign\nMany electrical, mechanical, and aeronautical systems are controlled by a syn-\nchronous nite state machine. In this section, we will design the complete logic\nneeded for a synchronous nite state machine to control a trac danger sign.\nFigure 3.29 shows the danger sign as it will be placed on the highway. Note the\nsign says, Danger, Move Right. The sign contains ve lights (labeled 1 through\n5 in the gure).\nThe purpose of our synchronous nite state machine (a.k.a. a controller) is to\ndirect the behavior of our system. In our case, the system is the set of lights on the\ntrac danger sign. The controllers job is to have the ve lights ash on and o\nto warn automobile drivers to move to the right. The controller is equipped with\na switch. When the switch is in the ON position, the controller directs the lights\nas follows: During one unit of time, all lights will be o. In the next unit of time,\nlights 1 and 2 will be on. The next unit of time, lights 1, 2, 3, and 4 will be on.\nThen all ve lights will be on. Then the sequence repeats: no lights on, followed\nby 1 and 2 on, followed by 1, 2, 3, and 4 on, and so forth. Each unit of time lasts\none\ndir\nthe\nlig\nFigure 3.29\nA trafc danger sign.\nThe State Diagram for the Danger Sign Controller\nFigure 3.30 is a state dia-\ngram for the synchronous nite state machine that controls the lights. There are\nfour states, one for each of the four conditions corresponding to which lights are\non. Note that the outputs (whether each light is on or o) are determined by the\ncurrent state of the system.\nIf the switch is on (input = 1), the transition from each state to the next\nstate happens at one-second intervals, causing the lights to ash in the sequence\n",
    "page_number": 81,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "88\n1\nFigure 3.30\nState diagram for the danger sign controller.\ndescribed. If the switch is turned o (input = 0), the state always transitions to\nstate A, the all o state.\nThe Sequential Logic Circuit for the Danger Sign Controller\nRecall that\nFig\nFig\nto\nClock\nFigure 3.31\nSequential logic circuit for the danger sign controller.\n",
    "page_number": 82,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "First, the two external inputs: the switch and the clock. The switch determines\nwhether the nite state machine will transition through the four states or whether\nit will transition to state A, where all lights are o. The other input (the clock)\ncontrols the transition from state A to B, B to C, C to D, and D to A by controlling\nthe state of the storage elements. We will see how, momentarily.\nSecond, there are two storage elements for storing state information. Since\nthere are four states, and since each storage element can store one bit of informa-\ntion, the four states are identied by the contents of the two storage elements: A\n(00), B (01), C (10), and D (11). Storage element 2 contains the high bit; storage\nelement 1 contains the low bit. For example, the danger sign controller is in state\nB when storage element 2 is 0 and storage element 1 is 1.\nThird, combinational logic circuit 1 shows that the on/o behavior of the\nlights is controlled by the storage elements. That is, the input to the combinational\nlogic circuit is from the two storage elements, that is, the current state of the nite\nstate machine.\nFinally, combinational logic circuit 2 shows that the transition from the cur-\nrent state to the next state depends on the two storage elements and the switch. If\nthe switch is on, the output of combinational logic circuit 2 depends on the state\nof the two storage elements.\nThe Combinational Logic\nFigure 3.32 shows the logic that implements com-\nbinational lo ic circuits 1 and 2.\nexter\ntwo s\nFigure 3.32\nCombinational logic circuits 1 and 2.\n",
    "page_number": 83,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "First, let us look at the outputs that control the lights. As we have said, there\nare only three outputs necessary to control the lights. Light 5 is controlled by\nthe output of the AND gate labeled V, since the only time light 5 is on is when\nthe controller is in state 11. Lights 3 and 4 are controlled by the output of the\nOR gate labeled X, since there are two states in which those lights are on, those\nlabeled 10 and 11. Why are lights 1 and 2 controlled by the output of the OR gate\nlabeled W? See Exercise 3.42.\nNext, let us look at the internal outputs that control the storage elements,\nwhich specify the next state of the controller. Storage element 2 should be set\nto 1 for the next clock cycle if the next state is 10 or 11. This is true only if the\nswitch is on and the current state is either 01 or 10. Therefore, the output signal\nthat will make storage element 2 be 1 in the next clock cycle is the output of the\nOR gate labeled Y. Why is the next state of storage element 1 controlled by the\noutput of the OR gate labeled Z? See Exercise 3.42.\nThe Two Storage Elements\nIn order for the danger sign controller to work, the\nstate transitions must occur once per second when the switch is on.\nA Problem with Gated Latches as Storage Elements\nWhat would happen if the\nstorage elements were gated D latches? If the two storage elements were gated\nD latches, when the write enable signal (the clock) is 1, the output of OR gates\nY and Z would immediately change the bits stored in the two gated D latches.\nThis would produce new input values to the three AND gates that are input to\nOR gates Y and Z, producing new outputs that would be applied to the inputs of\nthe gated latches, which would in turn change the bits stored in the gated latches,\nwhich would in turn mean new inputs to the three AND gates and new outputs\nof OR gates Y and Z. This would happen again and again, continually changing\nthe bits stored in the two storage elements as long as the Write Enable signal to\nthe gated D latches was asserted. The result: We have no idea what the state of the\nnite state machine would be for the next clock cycle. And, even in the current\nclock cycle, the state of the storage elements would change so fast that the ve\nlights would behave erratically.\nThe problem is the gated D latch. We want the output of OR gates Y and Z\nto transition to the next state at the end of the current clock cycle and allow the\ncurrent state to remain unchanged until then. That is, we do not want the input\nto the storage elements to take eect until the end of the current clock cycle.\nSince the output of a gated D latch changes immediately in response to its input\nif the Write Enable signal is asserted, it cannot be the storage element for our\nsynchronous nite state machine. We need storage elements that allow us to read\nthe current state throughout the current clock cycle, and not write the next state\nvalues into the storage elements until the beginning of the next clock cycle.\nThe Flip-Flop to the Rescue\nIt is worth repeating: To prevent the above from\nhappening, we need storage elements that allow us to read the current state\nthroughout the current clock cycle, and not write the next state values into the\nstorage elements until the beginning of the next clock cycle. That is, the function\n",
    "page_number": 84,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Clock\nFigure 3.33\nA master/slave ip-op.\nto be performed during a single clock cycle involves reading and writing a partic-\nular variable. Reading must be allowed throughout the clock cycle, and writing\nmust occur at the end of the clock cycle.\nA ip-op can accomplish that. One example of a ip-op is the master/slave\nip-op shown in Figure 3.33. The master/slave ip-op can be constructed out\nof two gated D latches, one referred to as the master, the other referred to as the\nslave. Note that the write enable signal of the master is 1 when the clock is 0, and\nthe write enable signal of the slave is 1 when the clock is 1.\nFigure 3.34 is a timing diagram for the master/slave ip-op, which shows\nhow and why the master/slave ip-op solves the problem. A timing diagram\nshows time passing from left to right. Note that clock cycle n starts at the time\nlabeled 1 and ends at the time labeled 4. Clock cycle n+1 starts at the time\nlabeled 4.\nConsider clock cycle n, which we will discuss in terms of its rst half A, its\nsecond half B, and the four time points labeled 1, 2, 3, and 4.\nAt the start of each clock cycle, the outputs of the storage elements are the\noutputs of the two slave latches. These outputs (starting at time 1) are input to\nthe AND gates, resulting in OR gates Y and Z producing the next state values for\nthe storage elements (at time 2). The timing diagram shows the propagation delay\nof the combinational logic, that is, the time it takes for the combinational logic\nto produce outputs of OR gates Y and Z. Although OR gates Y and Z produce\nthe Next State value sometime during half-cycle A, the write enable signal to the\nmaster latches is 0, so the next state cannot be written into the master latches.\nAt the start of half-cycle B (at time 3), the clock signal is 0, which means\nthe write enable signal to the master latches is 1, and the master latches can be\nwritten. However, during the half-cycle B, the write enable to the slave latches is\n0, so the slave latches cannot write the new information now stored in the master\nlatches.\nAt the start of clock cycle n+1 (at time 4), the write enable signal to the slave\nlatches is 1, so the slave latches can store the next state value that was created by\nthe combinational logic during clock cycle n. This becomes the current state for\nclock cycle n+1.\n",
    "page_number": 85,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "nal to the master latches is now 0, the state of the\nThus, although the write enable signal to the slave\nt change because the master latches cannot change.\nslave latches contains the current state of the system\ncle and produces the inputs to the six AND gates in\nts. Their state changes at the start of the clock cycle\nby storing the next state information created by the combinational logic during\nthe previous cycle but does not change again during the clock cycle. The reason\nthey do not change again during the clock cycle is as follows: During half-cycle\nA, the master latches cannot change, so the slave latches continue to see the state\ninformation that is the current state for the new clock cycle. During half-cycle B,\nthe slave latches cannot change because the clock signal is 0.\nMeanwhile, during half-cycle B, the master latches can store the next state\ninformation produced by the combinational logic, but they cannot write it into\nthe slave latches until the start of the next clock cycle, when it becomes the state\ninformation for the next clock cycle.\n3.7 Preview of Coming Attractions:\nThe Data Path of the LC-3\nIn Chapter 5, we will specify a computer, which we call the LC-3, and you will\nhave the opportunity to write computer programs to execute on it. We close out\nChapter 3 with a discussion of Figure 3.35, the data path of the LC-3 computer.\nThe data path consists of all the logic structures that combine to process\ninformation in the core of the computer. Right now, Figure 3.35 is undoubtedly\nmore than a little intimidating, but you should not be concerned by that. You are\nnot ready to analyze it yet. That will come in Chapter 5. We have included it\nhere, however, to show you that you are already familiar with many of the basic\nstructures that make up a computer. For example, you see ve MUXes in the data\npath, and you already know how they work. Also, an adder (shown as the ALU\nsymbol with a + sign inside) and an ALU. You know how those elements are\nconstructed from gates.\nOne element that we have not identified explicitly yet is a register. A register\nis simply a set of n flip-flops that collectively are used to store one n-bit value. In\nFigure 3.35, PC, IR, MAR, and MDR are all 16-bit registers that store 16 bits of\ninformation each. The block labeled REG FILE consists of eight registers that each\nstore 16 bits of information. As you know, one bit of information can be stored in\none flip-flop. Therefore, each of these registers consists of 16 flip-flops. The data\npath also shows three 1-bit registers, N, Z, and P. Those registers require only one\nflip-flop each. In fact, a register can be any size that we need. The size depends only\non the number of bits we need to represent the value we wish to store.\nOne way to implement registers is with master/slave ip-ops. Figure 3.36\nshows a four-bit register made up of four master/slave ip-ops. We usually need\nip-ops, rather than latches, because it is usually important to be able to both\nread the contents of a register throughout a clock cycle and also store a new value\n",
    "page_number": 86,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "von Neumann\nel\nW\ne are now ready to raise our level of abstraction another notch. We will\nbuild on the logic structures that we studied in Chapter 3, both decision\nelements and storage elements, to construct the basic computer model rst pro-\nposed in the 1940s, usually referred to as the von Neumann machine. ...and, we\nwill write our rst computer program in the ISA of the LC-3.\n4.1 Basic Components\nTo get a task done by a computer, we need two things: (a) a computer pro-\ngram that species what the computer must do to perform the task, and (b) the\ncomputer that is to carry out the task.\nA computer program consists of a set of instructions, each specifying a well-\ndened piece of work for the computer to carry out. The instruction is the smallest\npiece of work specied in a computer program. That is, the computer either car-\nries out the work specied by an instruction or it does not. The computer does\nnot have the luxury of carrying out only a piece of an instruction.\nJohn von Neumann proposed a fundamental model of a computer for process-\ning computer programs in 1946. Figure 4.1 shows its basic components. We have\ntaken a little poetic license and added a few of our own minor embellishments\nto von Neumanns original diagram. The von Neumann model consists of ve\nparts: memory, a processing unit, input, output, and a control unit. The computer\nprogram is contained in the computers memory. The data the program needs to\ncarry out the work of the program is either contained in the programs memory\nor is obtained from the input devices. The results of the programs execution are\nprovided by the output devices. The order in which the instructions are carried\nout is performed by the control unit.\nWe will describe each of the ve parts of the von Neumann model in greater\ndetail.\n",
    "page_number": 87,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "122\nPC\nFigure 4.1\nThe von Neumann model, overall block diagram.\n4.1.1 Memory\nRecall that in Chapter 3 we examined a simple 22-by-3-bit memory that was con-\nstructed out of gates and latches. A more realistic memory for one of todays\ncomputer systems is 234 by 8 bits. That is, a typical memory in todays world of\ncomputers consists of 234 distinct memory locations, each of which is capable\nof storing eight bits of information. We say that such a memory has an address\nspace of 234 uniquely identiable locations, and an addressability of eight bits.\nWe refer to such a memory as a 16-gigabyte memory (abbreviated, 16 GB). The\n16 giga refers to the 234 locations, and the byte refers to the eight bits stored\nin each location. The term is 16 giga because 16 is 24 and giga is the term we use\nto represent 230, which is approximately one billion; 24 times 230 = 234. A byte is\nthe word we use to describe eight bits, much the way we use the word gallon to\ndescribe four quarts.\nWe note (as we will note again and again) that with k bits, we can represent\nuniquely 2k items. Thus, to uniquely identify 234 memory locations, each loca-\ntion must have its own 34-bit address. In Chapter 5, we will begin the complete\ndenition of the LC-3 computer. We will see that the memory address space of\nthe LC-3 is 216, and the addressability is 16 bits.\nRecall from Chapter 3 that we access memory by providing the address from\nwhich we wish to read, or to which we wish to write. To read the contents of a\n",
    "page_number": 88,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "111\nFigure 4.2\nLocation 6 contains the value 4; location 4 contains the value 6.\nmemory location, we rst place the address of that location in the memorys\naddress register (MAR) and then interrogate the computers memory. The\ninformation stored in the location having that address will be placed in the\nmemorys data register (MDR). To write (or store) a value in a memory location,\nwe rst write the address of the memory location in the MAR, and the value to be\nstored in the MDR. We then interrogate the computers memory with the write\nenable signal asserted. The information contained in the MDR will be written\ninto the memory location whose address is in the MAR.\nBefore we leave the notion of memory for the moment, let us again emphasize\nthe two characteristics of a memory location: its address and what is stored there.\nFigure 4.2 shows a representation of a memory consisting of eight locations. Its\naddresses are shown at the left, numbered in binary from 0 to 7. Each location\ncontains eight bits of information. Note that the value 6 is stored in the memory\nlocation whose address is 4, and the value 4 is stored in the memory location\nwhose address is 6. These represent two very dierent situations.\nFinally, an analogy: the post oce boxes in your local post oce. The box\nnumber is like the memory locations address. Each box number is unique. The\ninformation stored in the memory location is like the letters contained in the post\noce box. As time goes by, what is contained in the post oce box at any par-\nticular moment can change. But the box number remains the same. So, too, with\neach memory location. The value stored in that location can be changed, but the\nlocations memory address remains unchanged.\n4.1.2 Processing Unit\nThe actual processing of information in the computer is carried out by the\nprocessing unit. The processing unit in a modern computer can consist of many\nsophisticated complex functional units, each performing one particular operation\n(divide, square root, etc.). The simplest processing unit, and the one normally\nthought of when discussing the basic von Neumann model, is the ALU. ALU is the\nabbreviation for Arithmetic and Logic Unit, so called because it is usually capa-\nble of performing basic arithmetic functions (like ADD and SUBTRACT) and\nbasic logic operations (like bit-wise AND, OR, and NOT) that we have already\nstudied in Chapter 2. We will see in Chapter 5 that the LC-3 has an ALU, which\n",
    "page_number": 89,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "can perform ADD, AND, and NOT operations. Two of these (ADD and AND)\nwe will discuss in this chapter.\nThe ALU normally processes data elements of a xed size referred to as the\nword length of the computer. The data elements are called words. For example,\nto perform ADD, the ALU receives two words as inputs and produces a single\nword (the sum) as output. Each ISA has its own word length, depending on the\nintended use of the computer.\nMost microprocessors today that are used in PCs or workstations have a word\nlength of 64 bits (as is the case with Intels Core processors) or 32 bits (as is\nthe case with Intels Atom processors). Even most microprocessors now used in\ncell phones have 64-bit word lengths, such as Apples A7 through A11 processors,\nand Qualcomms SnapDragon processors. However, the microprocessors used in\nvery inexpensive applications often have word lengths of as little as 16 or even\n8 bits.\nIn the LC-3, the ALU processes 16-bit words. We say the LC-3 has a word\nlength of 16 bits.\nIt is almost always the case that a computer provides some small amount of\nstorage very close to the ALU to allow results to be temporarily stored if they\nwill be needed to produce additional results in the near future. For example, if a\ncomputer is to calculate (A+B)C, it could store the result of A+B in memory, and\nthen subsequently read it in order to multiply that result by C. However, the time\nit takes to access memory is long compared to the time it takes to perform the\nADD or MULTIPLY. Almost all computers, therefore, have temporary storage\nfor storing the result of A + B in order to avoid the much longer access time that\nwould be necessary when it came time to multiply. The most common form of\ntemporary storage is a set of registers, like the register described in Section 3.7.\nTypically, the size of each register is identical to the size of values processed\nby the ALU; that is, they each contain one word. The LC-3 has eight registers\n(R0, R1,  R7), each containing 16 bits.\nCurrent microprocessors typically contain 32 registers, each consisting of 32\nor 64 bits, depending on the architecture. These serve the same purpose as the\neight 16-bit registers in the LC-3. However, the importance of temporary storage\nfor values that most modern computers will need shortly means many computers\ntoday have an additional set of special-purpose registers consisting of 128 bits of\ninformation to handle special needs. Those special needs we will have to save for\nlater in your studies.\n4.1.3 Input and Output\nIn order for a computer to process information, the information must get into\nthe computer. In order to use the results of that processing, those results must\nbe displayed in some fashion outside the computer. Many devices exist for the\npurposes of input and output. They are generically referred to in computer jar-\ngon as peripherals because they are in some sense accessories to the processing\nfunction. Nonetheless, they are no less important.\nIn the LC-3 we will have the two most basic input and output devices. For\ninput, we will use the keyboard; for output, we will use the monitor.\n",
    "page_number": 90,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "There are, of course, many other input and output devices in computer sys-\ntems today. For input we have among other things the mouse, digital scanners,\nand shopping mall kiosks to help you navigate the shopping mall. For output we\nhave among other things printers, LED displays, disks, and shopping mall kiosks\nto help you navigate the shopping mall. :-) In the old days, a lot of input and out-\nput was carried out by punched cards. Fortunately, for those who would have to\nlug around boxes of cards, the use of punched cards has largely disappeared.\n4.1.4 Control Unit\nThe control unit is like the conductor of an orchestra; it is in charge of making\nall the other parts of the computer play together. As we will see when we describe\nthe step-by-step process of executing a computer program, it is the control unit\nthat keeps track of both where we are within the process of executing the program\nand where we are in the process of executing each instruction.\nTo keep track of which instruction is being executed, the control unit has an\ninstruction register to contain that instruction. To keep track of which instruc-\ntion is to be processed next, the control unit has a register that contains the next\ninstructions address. For historical reasons, that register is called the program\ncounter (abbreviated PC), although a better name for it would be the instruction\npointer, since the contents of this register is, in some sense, pointing to the\nnext instruction to be processed. Curiously, Intel does in fact call that register the\ninstruction pointer, but the simple elegance of that name has not caught on.\n4.2 The LC-3: An Example\nvon Neumann Machine\nIn Chapter 5, we will specify in detail the LC-3, a simple computer that we\nwill study extensively. We have already shown you its data path in Chapter 3\n(Figure 3.35) and identied several of its structures in Section 4.1. In this sec-\ntion, we will pull together all the parts of the LC-3 we need to describe it as a von\nNeumann computer (see Figure 4.3).\nWe constructed Figure 4.3 by starting with the LC-3s full data path\n(Figure 3.35) and removing all elements that are not essential to pointing out\nthe ve basic components of the von Neumann model.\nNote that there are two kinds of arrowheads in Figure 4.3: lled-in and\nnot-lled-in. Filled-in arrowheads denote data elements that ow along the cor-\nresponding paths. Not-lled-in arrowheads denote control signals that control the\nprocessing of the data elements. For example, the box labeled ALU in the pro-\ncessing unit processes two 16-bit values and produces a 16-bit result. The two\nsources and the result are all data, and are designated by lled-in arrowheads.\nThe operation performed on those two 16-bit data elements (it is labeled ALUK)\nis part of the controltherefore, a not-lled-in arrowhead.\nMEMORY consists of the storage elements, along with the Memory\nAddress Register (MAR) for addressing individual locations and the\n",
    "page_number": 91,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "126\nOUTPUT\nINPUT\nMEMORY\nFigure 4.3\nThe LC-3 as an example of the von Neumann model.\nMemory Data Register (MDR) for holding the contents of a memory\nlocation on its way to/from the storage. Note that the MAR contains 16 bits,\nreecting the fact that the memory address space of the LC-3 is 216\nmemory locations. The MDR contains 16 bits, reecting the fact that each\nmemory location contains 16 bitsthat is, the LC-3 is 16-bit addressable.\nINPUT/OUTPUT consists of a keyboard and a monitor. The simplest\nkeyboard requires two registers: a keyboard data register (KBDR) for\nholding the ASCII codes of keys struck and a keyboard status register\n(KBSR) for maintaining status information about the keys struck. The\nsimplest monitor also requires two registers: a display data register (DDR)\nfor holding the ASCII code of something to be displayed on the screen and\n",
    "page_number": 92,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "a display status register (DSR) for maintaining associated status\ninformation. These input and output registers will be discussed in detail\nin Chapter 9.\nTHE PROCESSING UNIT consists of a functional unit (ALU) that\nperforms arithmetic and logic operations and eight registers (R0,  R7) for\nstoring temporary values that will be needed in the near future as operands\nfor subsequent instructions. The LC-3 ALU can perform one arithmetic\noperation (addition) and two logical operations (bitwise AND and bitwise\nNOT).\nTHE CONTROL UNIT consists of all the structures needed to manage\nthe processing that is carried out by the computer. Its most important\nstructure is the nite state machine, which directs all the activity. Recall the\nnite state machines in Section 3.6. Processing is carried out step by step,\nor rather, clock cycle by clock cycle. Note the CLK input to the nite state\nmachine in Figure 4.3. It species how long each clock cycle lasts. The\ninstruction register (IR) is also an input to the nite state machine since\nthe LC-3 instruction being processed determines what activities must be\ncarried out. The program counter (PC) is also a part of the control unit;\nit keeps track of the next instruction to be executed after the current\ninstruction nishes.\nNote that all the external outputs of the nite state machine in Figure 4.3 have\narrowheads that are not lled in. These outputs control the processing through-\nout the computer. For example, one of these outputs (two bits) is ALUK, which\ncontrols the operation performed in the ALU (ADD, AND, or NOT) during the\ncurrent clock cycle. Another output is GateALU, which determines whether or\nnot the output of the ALU is provided to the processor bus during the current\nclock cycle.\nThe complete description of the data path, control, and nite state machine\nfor one implementation of the LC-3 is the subject of Appendix C.\n4.3 Instruction Processing\nThe central idea in the von Neumann model of computer processing is that the\nprogram and data are both stored as sequences of bits in the computers memory,\nand the program is executed one instruction at a time under the direction of the\ncontrol unit.\n4.3.1 The Instruction\nThe most basic unit of computer processing is the instruction. It is made up of two\nparts, the opcode (what the instruction does) and the operands (who it does it to!).\nThere are fundamentally three kinds of instructions: operates, data move-\nment, and control, although many ISAs have some special instructions that are\nnecessary for those ISAs. Operate instructions operate on data. The LC-3 has\nthree operate instructions: one arithmetic (ADD) and two logicals (AND and\n",
    "page_number": 93,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "NOT). Data movement instructions move information from the processing unit\nto and from memory and to and from input/output devices. The LC-3 has six data\nmovement instructions.\nControl instructions are necessary for altering the sequential processing of\ninstructions. That is, normally the next instruction executed is the instruction\ncontained in the next memory location. If a program consists of instructions\n1,2,3,4...10 located in memory locations A, A+1, A+2, ...A+9, normally the\ninstructions would be executed in the sequence 1,2,3...10. We will see before we\nleave Chapter 4, however, that sometimes we will want to change the sequence.\nControl instructions enable us to do that.\nAn LC-3 instruction consists of 16 bits (one word), numbered from left to\nright, bit [15] to bit [0]. Bits [15:12] contain the opcode. This means there are at\nmost 24 distinct opcodes. Actually, we use only 15 of the possible four-bit codes.\nOne is reserved for some future use. Bits [11:0] are used to gure out where the\noperands are.\nExa\ns a\nor a\nas e\ner\ne con en s o reg s er\no\ne con en s o\n",
    "page_number": 94,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "4.3.2 The Instruction Cycle (NOT the Clock Cycle!)\nInstructions are processed under the direction of the control unit in a very sys-\ntematic, step-by-step manner. The entire sequence of steps needed to process an\ninstruction is called the instruction cycle. The instruction cycle consists of six\nsequential phases, each phase requiring zero or more steps. We say zero steps\nto indicate that most computers have been designed such that not all instructions\nrequire all six phases. We will discuss this momentarily. But rst, we will examine\nthe six phases of the instruction cycle:\nFETCH\nDECODE\nEVALUATE ADDRESS\nFETCH OPERANDS\nEXECUTE\nSTORE RESULT\nThe process is as follows (again refer to Figure 4.3, our simplied version of\nthe LC-3 data path):\n4.3.2.1 FETCH\nThe FETCH phase obtains the next instruction from memory and loads it into\nthe instruction register (IR) of the control unit. Recall that a computer program\nconsists of a number of instructions, that each instruction is represented by a\nsequence of bits, and that the entire program (in the von Neumann model) is stored\nin the computers memory. In order to carry out the work of an instruction, we\nmust rst identify where it is. The program counter (PC) contains the address of\nthe next instruction to be processed. Thus, the FETCH phase takes the following\nsteps:\nFirst the MAR is loaded with the contents of the PC.\nNext, the memory is interrogated, which results\nin the next instruction being placed by the memory\ninto the MDR.\nFinally, the IR is loaded with the contents\nof the MDR.\nWe are now ready for the next phase, decoding the instruction. However, when\nthe instruction nishes execution, and we wish to fetch the next instruction, we\nwould like the PC to contain the address of the next instruction. This is accom-\nplished by having the FETCH phase perform one more task: incrementing the\nPC. In that way, after the current instruction nishes, the FETCH phase of the\nnext instruction will load into the IR the contents of the next memory location,\nprovided the execution of the current instruction does not involve changing the\nvalue in the PC.\nThe complete description of the FETCH phase is as follows:\nStep 1:\nLoad the MAR with the contents of the PC, and\nsimultaneously increment the PC.\n",
    "page_number": 95,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Step 2:\nInterrogate memory, resulting in the instruction\nbeing placed in the MDR.\nStep 3:\nLoad the IR with the contents of the MDR.\nEach of these steps is under the direction of the control unit, much like, as we said\npreviously, the instruments in an orchestra are under the control of a conductors\nbaton. Each stroke of the conductors baton corresponds to one machine cycle.\nWe will see in Section 4.3.5 that the amount of time taken by each machine cycle\nis one clock cycle. In fact, we often use the two terms interchangeably. Step 1\ntakes one clock cycle. Step 2 could take one clock cycle or many clock cycles,\ndepending on how long it takes to access the computers memory. Step 3 takes\none clock cycle. In a modern digital computer, a clock cycle takes a very small\nfraction of a second.\nIndeed, a 3.1 GHz Intel Core i7 completes 3.1 billion clock cycles in one\nsecond. Said another way, one clock cycle takes 0.322 billionths of a second\n(0.322 nanoseconds). Recall that the light bulb that is helping you read this text\nis switching on and o at the rate of 60 times a second. Thus, in the time it takes\na light bulb to switch on and o once, todays computers can complete more than\n51 million clock cycles!\n4.3.2.2 DECODE\nThe DECODE phase examines the instruction in order to gure out what\nthe microarchitecture is being asked to do. Recall the decoders we studied in\nChapter 3. In the LC-3, a 4-to-16 decoder identies which of the 16 opcodes is to\nbe processed (even though one of the 16 is not used!). Input is the four-bit opcode\nIR [15:12]. The output line asserted is the one corresponding to the opcode at\nthe input. Depending on which output of the decoder is asserted, the remaining\n12 bits identify what else is needed to process that instruction.\n4.3.2.3 EVALUATE ADDRESS\nThis phase computes the address of the memory location that is needed to pro-\ncess the instruction. Recall the example of the LD instruction: The LD instruction\ncauses a value stored in memory to be loaded into a register. In that exam-\nple, the address was obtained by sign-extending bits [8:0] of the instruction to\n16 bits and adding that value to the current contents of the PC. This calculation\nwas performed during the EVALUATE ADDRESS phase. It is worth noting that\nnot all instructions access memory to load or store data. For example, we have\nalready seen that the ADD and AND instructions in the LC-3 obtain their source\noperands from registers or from the instruction itself and store the result of the\nADD or AND instruction in a register. For those instructions, the EVALUATE\nADDRESS phase is not needed.\n4.3.2.4 FETCH OPERANDS\nThis phase obtains the source operands needed to process the instruction. In the\nLD example, this phase took two steps: loading MAR with the address calculated\nin the EVALUATE ADDRESS phase and reading memory that resulted in the\nsource operand being placed in MDR.\n",
    "page_number": 96,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "In the ADD example, this phase consisted of obtaining the source operands\nfrom R2 and R6. In most current microprocessors, this phase (for the ADD\ninstruction) can be done at the same time the instruction is being executed (the\nfth phase of the instruction cycle). Exactly how we can speed up the processing\nof an instruction in this way is a fascinating subject, but it is one we are forced to\nleave for later in your education.\n4.3.2.5 EXECUTE\nThis phase carries out the execution of the instruction. In the ADD example, this\nphase consisted of the step of performing the addition in the ALU.\n4.3.2.6 STORE RESULT\nThe nal phase of an instructions execution. The result is written to its designated\ndestination. In the case of the ADD instruction, in many computers this action is\nperformed during the EXECUTE phase. That is, in many computers, including\nthe LC-3, an ADD instruction can fetch its source operands, perform the ADD in\nthe ALU, and store the result in the destination register all in a single clock cycle.\nA separate STORE RESULT phase is not needed.\nOnce the instruction cycle has been completed, the control unit begins the\ninstruction cycle for the next instruction, starting from the top with the FETCH\nphase. Since the PC was updated during the previous instruction cycle, it contains\nat this point the address of the instruction stored in the next sequential memory\nlocation. Thus, the next sequential instruction is fetched next. Processing con-\ntinues in this way until something breaks this sequential ow, or the program\nnishes execution.\nIt is worth noting again that although the instruction cycle consists of six\nphases, not all instructions require all six phases. As already pointed out, the LC-\n3 ADD instruction does not require a separate EVALUATE ADDRESS phase or\na separate STORE RESULT phase. The LC-3 LD instruction does not require an\nEXECUTE phase. On the other hand, there are instructions in other ISAs that\nrequire all six phases.\nExample 4.4\nADD [eax], edx\nThis is an example of an Intel x86 instruction that requires\nall six phases of the instruction cycle. All instructions require the rst two phases,\nFETCH and DECODE. This instruction uses the eax register to calculate the address\nof a memory location (EVALUATE ADDRESS). The contents of that memory\nlocation is then read (FETCH OPERAND), added to the contents of the edx reg-\nister (EXECUTE), and the result written into the memory location that originally\ncontained the rst source operand (STORE RESULT).\n4.3.3 Changing the Sequence of Execution\nEverything we have said thus far happens when a computer program is executed\nin sequence. That is, the rst instruction is executed, then the second instruction\nis executed, followed by the third instruction, and so on.\n",
    "page_number": 97,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "We have identied two types of instructions, the ADD and AND, which are\nexamples of operate instructions in that they operate on data, and the LD, which\nis an example of a data movement instruction in that it moves data from one\nplace to another. There are other examples of both operate instructions and data\nmovement instructions, as we will discover in Chapter 5 when we study the LC-3\nin greater detail.\nThere is a third type of instruction, the control instruction, whose purpose is\nto change the sequence of instruction execution. For example, there are times, as\nwe shall see very soon, when it is desirable to rst execute the rst instruction,\nthen the second, then the third, then the rst again, the second again, then the third\nagain, then the rst for the third time, the second for the third time, and so on. As\nwe know, each instruction cycle starts with loading the MAR with the PC. Thus,\nif we wish to change the sequence of instructions executed, we must change the\ncontents of the PC between the time it is incremented (during the FETCH phase\nof one instruction) and the start of the FETCH phase of the next instruction.\nControl instructions perform that function by loading the PC during the\nEXECUTE phase, which wipes out the incremented PC that was loaded during\nthe FETCH phase. The result is that, at the start of the next instruction cycle,\nwhen the computer accesses the PC to obtain the address of an instruction to\nfetch, it will get the address loaded during the previous instructions EXECUTE\nphase, rather than the next sequential instruction in the computers program.\nThe most common control instruction is the conditional branch (BR), which\neither changes the contents of the PC or does not change the contents of the PC,\ndepending on the result of a previous instruction (usually the instruction that is\nexecuted immediately before the conditional branch instruction).\nExample 4.5\nThe BR Instruction\nThe BR instruction consists of three parts, the opcode (bits\n[15:12] = 0000), the condition to be tested (bits [11:9]), and the addressing mode bits\n(bits [8:0]) that are used to form the address to be loaded into the PC if the result of\nthe previous instruction agrees with the test specied by bits [11:9]. The addressing\nmode, i.e., the mechanism used to determine the actual address, is the same one we\nused in the LD instruction. Bits [8:0] are sign-extended to 16 bits and then added to\nthe current contents of the PC.\nSuppose the BR instruction shown below is located in memory location x36C9.\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n0\n0\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n0\n1\n0\nBR\ncondition\n6\nThe opcode 0000 identies the instruction as a conditional branch. Bits [11:9] = 101\nspecies that the test to be performed on the most recent result is whether or not that\nresult is something other than 0. In Chapter 5 we will describe in detail all the tests\nthat can be performed on the most recent result. For now, we will just use one test:\nIs the result not zero? Bits [8:0] is the value 6.\nAssume the previous instruction executed (in memory location x36C8) was an\nADD instruction and the result of the ADD was 0. Since the test not-zero failed,\nthe BR instruction would do nothing during its EXECUTE phase, and so the next\n(continued on next page)\n",
    "page_number": 98,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "instruction executed would be the instruction at M[x36CA], the address formed by\nincrementing the PC during the FETCH phase of the BR instructions instruction\ncycle.\nOn the other hand, if the result of the ADD instruction is not 0, then the test\nsucceeds, causing the BR instruction to load PC with x36C4, the address formed by\nsign-extending bits [8:0] to 16 bits and adding that value (-6) to the incremented PC\n(x36CA).\nThus, the next instruction executed after the BR instruction at x36C9 is either\nthe instruction at x36CA or the one at x36C4, depending on whether the result of the\nADD instruction was zero or not zero.\n4.3.4 Control of the Instruction Cycle\nThe instruction cycle is controlled by a synchronous nite state machine. An\nabbreviated version of its state diagram, highlighting a few of the LC-3 instruc-\ntions discussed in this chapter, is shown in Figure 4.4. As is the case with the\nnite state machines studied in Section 3.6, each state corresponds to one machine\ncycle of activity that takes one clock cycle to perform. The processing controlled\nby each state is described within the node representing that state. The arcs show\nthe next state transitions.\nProcessing starts with State 1. The FETCH phase takes three clock cycles,\ncorresponding to the three steps described earlier. In the rst clock cycle, the\nMAR is loaded with the contents of the PC, and the PC is incremented. In order\nfor the contents of the PC to be loaded into the MAR (see Figure 4.3), the nite\nstate machine must assert GatePC and LD.MAR. GatePC connects the PC to the\nprocessor bus. LD.MAR, the write enable signal of the MAR register, loads the\ncontents of the bus into the MAR at the end of the current clock cycle. (Registers\nare loaded at the end of the clock cycle if the corresponding control signal is\nasserted.)\nIn order for the PC to be incremented (again, see Figure 4.3), the nite\nstate machine must assert the PCMUX select lines to choose the output of the\nbox labeled +1 and must also assert the LD.PC signal to load the output of the\nPCMUX into the PC at the end of the current cycle.\nThe nite state machine then goes to State 2. Here, the MDR is loaded with\nthe instruction, which is read from memory.\nIn State 3, the instruction is transferred from the MDR to the instruction\nregister (IR). This requires the nite state machine to assert GateMDR and LD.IR,\nwhich causes the IR to be loaded at the end of the clock cycle, concluding the\nFETCH phase of the instruction cycle.\nThe DECODE phase takes one clock cycle. In State 4, using the external\ninput IR, and in particular the opcode bits of the instruction, the nite state\nmachine can go to the appropriate next state for processing instructions depend-\ning on the particular opcode in IR [15:12]. Three of the 15 paths out of State 4\nare shown. Processing continues clock cycle by clock cycle until the instruction\ncompletes execution, and the next state logic returns the nite state machine to\nState 1.\n",
    "page_number": 99,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "As has already been discussed, it is sometimes necessary not to execute\nthe next sequential instruction but rather to access another location to nd the\nnext instruction to execute. As we have said, instructions that change the ow of\ninstruction processing in this way are called control instructions. In the case of\nthe conditional branch instruction (BR), at the end of its instruction cycle, the\nPC contains one of two addresses: either the incremented PC that was loaded in\nState 1 or the new address computed from sign-extending bits [8:0] of the BR\ninstruction and adding it to the PC, which was loaded in State 63. Which address\ngets loaded into the PC depends on the test of the most recent result.\nA\nendix C contains a full descri tion of the im lementation of the\nL\nof\nm\no a e\no a e\no a e\nFigure 4.4\nAn abbreviated state diagram of the LC-3.\n",
    "page_number": 100,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "state diagram would be able to control, clock cycle by clock cycle, all the steps\nrequired to execute all the phases of every instruction cycle. Since each instruc-\ntion cycle ends by returning to State 1, the nite state machine can process, clock\ncycle by clock cycle, a complete computer program.\n4.3.5 Halting the Computer (the TRAP Instruction)\nFrom everything we have said, it appears that the computer will continue\nprocessing instructions, carrying out the instruction cycle again and again,\nad nauseum. Since the computer does not have the capacity to be bored, must this\ncontinue until someone pulls the plug and disconnects power to the computer?\nUsually, user programs execute under the control of an operating system.\nLinux, DOS, MacOS, and Windows are all examples of operating systems.\nOperating systems are just computer programs themselves. As far as the com-\nputer is concerned, the instruction cycle continues whether a user program is\nbeing processed or the operating system is being processed. This is ne as\nfar as user programs are concerned since each user program terminates with a\ncontrol instruction that changes the PC to again start processing the operating\nsystemoften to initiate the execution of another user program.\nBut what if we actually want to stop this potentially innite sequence of\ninstruction cycles? Recall our analogy to the conductors baton, beating at the\nrate of billions of clock cycles per second. Stopping the instruction sequencing\nrequires stopping the conductors baton. We have pointed out many times that\nthere is inside the computer a component that corresponds very closely to the\nconductors baton. It is called the clock, and it denes the amount of time each\nmachine cycle takes. We saw in Chapter 3 that the clock enables the synchronous\nnite state machine to continue on to the next clock cycle. In Chapter 3 the next\nclock cycle corresponded to the next state of the danger sign we designed. Here\nthe next clock cycle corresponds to the next state of the instruction cycle, which\nis either the next state of the current phase of the instruction cycle or the rst state\nof the next phase of the instruction cycle. Stopping the instruction cycle requires\nstopping the clock.\nFigure 4.5a shows a block diagram of the clock circuit, consisting primarily\nof a clock generator and a RUN latch. The clock generator is a crystal oscillator,\na piezoelectric device that you may have studied in your physics or chemistry\nclass. F\ntion of\nFigure\nclock c\nCr\no\nCl\ngen\nFigure 4.5\nThe clock circuit and its control.\n",
    "page_number": 101,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "If the RUN latch is in the 1 state (i.e., Q = 1), the output of the clock circuit\nis the same as the output of the clock generator. If the RUN latch is in the 0 state\n(i.e., Q = 0), the output of the clock circuit is 0.\nThus, stopping the instruction cycle requires only clearing the RUN latch.\nEvery computer has some mechanism for doing that. In some older machines, it\nis done by executing a HALT instruction. In the LC-3, as in many other machines,\nit is done under control of the operating system, as we will see in Chapter 9. For\nnow it is enough to know that if a user program requires help from the operating\nsystem, it requests that help with the TRAP instruction (opcode = 1111) and an\neight-bit code called a trap vector, which identies the help that the user program\nneeds. The eight-bit code x25 tells the operating system that the program has\nnished executing and the computer can stop processing instructions.\nQuestion: If a HALT instruction can clear the RUN latch, thereby stopping\nthe instruction cycle, what instruction is needed to set the RUN latch, thereby\nreinitiating the instruction cycle? Hint: This is a trick question!\n4.4 Our First Program:\nA Multiplication Algorithm\nWe now have all that we need to write our rst program. We have a data movement\ninstruction LD to load data from memory into a register, and we have two operate\ninstructions, ADD for performing arithmetic and AND for performing a bit-wise\nlogical operation. We have a control instruction BR for loading the PC with an\naddress dierent from the incremented PC so the instruction to be executed next\nwill NOT be the instruction in the next sequential location in memory. And we\nhave the TRAP instruction (a.k.a. system call) that allows us to ask the operating\nsystem for help, in this case to stop the computer. With all that under our belt, we\ncan write our rst program.\nSuppose the computer does not know how to multiply two positive integers.\nIn the old days, that was true for a lot of computers! They had ADD instructions,\nbut they did not have multiply instructions. What to do? Suppose we wanted to\nmultiply 5 times 4. Even if we do not know how to multiply, if we know that 5\ntimes 4 is 5+5+5+5, and the computer has an ADD instruction, we can write a\nprogram that can multiply. All we have to do is add 5 to itself four times.\nFigure 4.6 illustrates the process.\nLet us assume that memory location x3007, abbreviated M[x3007], contains\nthe value 5, and M[x3008] contains the value 4. We start by copying the two\nvalues from memory to the two registers R1 and R2. We are going to accumulate\nthe results of the additions in R3, so we initialize R3 to 0. Then we add 5 to R3,\nand subtract 1 from R2 so we will know how many more times we will need to\nadd 5 to R3. We keep doing this (adding 5 to R3 and subtracting 1 from R2) until\nR2 contains the value 0. That tells us that we have added 5 to R3 four times and\nwe are done, so we HALT the computer. R3 contains the value 20, the result of\nour multiplication.\nFigure 4.7 shows the actual LC-3 program, stored in memory locations x3000\nto x3008.\n",
    "page_number": 102,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "138\nStop\nFigure 4.6\nFlowchart for an algorithm that multiplies two positive integers.\nThe program counter, which keeps track of the next instruction to be\nexecuted, initially contains the address x3000.\nTo move the data from memory locations M[x3007] and M[x3008] to R1 and\nR2, we use the data movement instruction LD. The LC-3 computer executes the\nLD instruction in M[x3000] by sign-extending the oset (in this case 6) to 16 bits,\nadding it to the incremented PC (in this case x3001 since we incremented the PC\nF gure\n.\nA program that multiplies without a multiply instruction.\n",
    "page_number": 103,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "LC-3\nI\nn Chapter 4, we discussed the basic components of a computerits mem-\nory, its processing unit, including the associated temporary storage (usually\na set of registers), input and output devices, and the control unit that directs the\nactivity of all the units (including itself!). We also studied the six phases of the\ninstruction cycleFETCH, DECODE, ADDRESS EVALUATION, OPERAND\nFETCH, EXECUTE, and STORE RESULT. We used elements of the LC-3 to\nillustrate some of the concepts. In fact, we introduced ve opcodes: two operate\ninstructions (ADD and AND), one data movement instruction (LD), and two con-\ntrol instructions (BR and TRAP). We are now ready to study the LC-3 in much\ngreater detail.\nRecall from Chapter 1 that the ISA is the interface between what the soft-\nware commands and what the hardware actually carries out. In this chapter, we\nwill point out most of the important features of the ISA of the LC-3. (A few ele-\nments we will leave for Chapter 8 and Chapter 9.) You will need these features\nto write programs in the LC-3s own language, that is, in the LC-3s machine\nlanguage.\nA complete description of the ISA of the LC-3 is contained in Appendix A.\n5.1 The ISA: Overview\nThe ISA species all the information about the computer that the software has\nto be aware of. In other words, the ISA species everything in the computer\nthat is available to a programmer when he/she writes programs in the com-\nputers own machine language. Most people, however, do not write programs\nin the computers own machine language, but rather opt for writing programs in\na high-level language like C++ or Python (or Fortran or COBOL, which have\nbeen around for more than 50 years). Thus, the ISA also species everything\nin the computer that is needed by someone (a compiler writer) who wishes to\ntranslate programs written in a high-level language into the machine language of\nthe computer.\n",
    "page_number": 104,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "The ISA species the memory organization, register set, and instruction set,\nincluding the opcodes, data types, and addressing modes of the instructions in\nthe instruction set.\n5.1.1 Memory Organization\nThe LC-3 memory has an address space of 216 (i.e., 65,536) locations, and an\naddressability of 16 bits. Not all 65,536 addresses are actually used for memory\nlocations, but we will leave that discussion for Chapter 9. Since the normal unit\nof data that is processed in the LC-3 is 16 bits, we refer to 16 bits as one word,\nand we say the LC-3 is word-addressable.\n5.1.2 Registers\nSince it usually takes far more than one clock cycle to obtain data from mem-\nory, the LC-3 provides (like almost all computers) additional temporary storage\nlocations that can be accessed in a single clock cycle.\nThe most common type of temporary storage locations, and the one used in\nthe LC-3, is a set of registers. Each register in the set is called a general purpose\nregister (GPR). Like memory locations, registers store information that can be\noperated on later. The number of bits stored in each register is usually one word.\nIn the LC-3, this means 16 bits.\nRegisters must be uniquely identiable. The LC-3 species eight GPRs, each\nidentied by a three-bit register number. They are referred to as R0, R1,  R7.\nFigure 5.1 shows a snapshot of the LC-3s register set, sometimes called a register\nle, with the eight values 1, 3, 5, 7, 2, 4, 6, and 8 stored in R0,  R7,\nrespectively.\nFigure 5.1\nA snapshot of the LC-3s register le.\n",
    "page_number": 105,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Fi\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\nADD\nR2\nR0\nR1\nwhere the two sources of the ADD instruction are specied in bits [8:6] and\nbits [2:0]. The destination of the ADD result is specied in bits [11:9]. Figure 5.2\nshows the contents of the register le of Figure 5.1 AFTER the instruction\nADD R2, R1, R0.\nis executed.\n5.1.3 The Instruction Set\nRecall from Chapter 4 that an instruction is made up of two things, its opcode\n(what the instruction is asking the computer to do) and its operands (who the\ncomputer is expected to do it to!). The instruction set is dened by its set of\nopcodes, data types, and addressing modes. The addressing modes determine\nwhere the operands are located. The data type is the representation of the operands\nin 0s and 1s.\nThe instruction ADD R2, R0, R1 has an opcode ADD, one addressing mode\n(register mode), and one data type (2s complement integer). The instruction\ndirects the computer to perform a 2s complement integer addition and speci-\nes the locations (GPRs) where the computer is expected to nd the operands\nand the location (a GPR) where the computer is to write the result.\nWe saw in Chapter 4 that the ADD instruction can also have two addressing\nmodes (register mode and immediate mode), where one of the two operands is\nliterally contained in bits [4:0] of the instruction.\nFigure 5.3 lists all the instructions of the LC-3, the bit encoding [15:12] for\neach opcode, and the format of each instruction. Some of them you will recognize\nfrom Chapter 4. Many others will be explained in Sections 5.2, 5.3, and 5.4.\n",
    "page_number": 106,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "5.1.4 Opcodes\nSome ISAs have a very large number of opcodes, one for each of a very large\nnumber of tasks that a program may wish to carry out. The x86 ISA has more than\n200 opcodes. Other ISAs have a very small set of opcodes. Some ISAs have specific\nopcodes to help with processing scientific calculations. For example, the Hewlett\nPackard Precision Architecture can specify the compound operation (A  B) + C\nwith one opcode; that is, a multiply, followed by an add on three source operands\nA, B, and C. Other ISAs have instructions that process video images obtained from\nthe World Wide Web. The Intel x86 ISA added a number of instructions which they\noriginally called MMX instructions because they eXtended the ISA to assist with\nMultiMedia applications that use the web. Still other ISAs have specific opcodes to\nhelp with handling the tasks of the operating system. For example, the VAX ISA,\npopular in the 1980s, used a single opcode instead of a long sequence of instructions\nthat other computers used to save the information associated with a program that\nwas in the middle of executing prior to switching to another program. The decision\nas to which instructions to include or leave out of an ISA is usually a hotly debated\ntopic in a company when a new ISA is being specified.\nThe LC-3 ISA has 15 instructions, each identied by its unique opcode. The\nopcode is specied in bits [15:12] of the instruction. Since four bits are used\nto specify the opcode, 16 distinct opcodes are possible. However, the LC-3 ISA\nspecies only 15 opcodes. The code 1101 has been left unspecied, reserved for\nsome future need that we are not able to anticipate today.\nAs we already discussed briey in Chapter 4, there are three dierent types\nof instructions, which means three dierent types of opcodes: operates, data\nmovement, and control. Operate instructions process information. Data move-\nment instructions move information between memory and the registers and\nbetween registers/memory and input/output devices. Control instructions change\nthe sequence of instructions that will be executed. That is, they enable the exe-\ncution of an instruction other than the one that is stored in the next sequential\nlocation in memory.\n5.1.5 Data Types\nAs we rst pointed out in Section 2.1.2, a data type is a representation of infor-\nmation such that the ISA has opcodes that operate on that representation. There\nare many ways to represent the same information in a computer. That should not\nsurprise us, since in our daily lives, we regularly represent the same information\nin many dierent ways. For example, a child, when asked how old he is, might\nhold up three ngers, signifying that he is 3 years old. If the child is particularly\nprecocious, he might write the decimal digit 3 to indicate his age. Or, if the child\nis a CS or CE major at the university, he might write 0000000000000011, the\n16-bit binary representation for 3. If he is a chemistry major, he might write\n3.0  100. All four represent the same value: 3.\nIn addition to the representation of a single number by dierent bit patterns\nin dierent data types, it is also the case that the same bit pattern can corre-\nspond to dierent numbers, depending on the data type. For example, the 16\n",
    "page_number": 107,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "bits 0011000100110000 represent the 2s complement integer 12,592, the ASCII\ncode for 10, and a bit vector such that b13, b12, b7, b4, and b3 have the relevant\nproperty of the bit vector.\nThat should also not surprise us, since in our daily lives, the same represen-\ntation can correspond to multiple interpretations, as is the case with a red light.\nWhen you see it on the roadway while you are driving, it means you should stop.\nWhen you see it at Centre Bell where the Montreal Canadiens play hockey, it\nmeans someone has just scored a goal.\nEvery opcode will interpret the bit patterns of its operands according to the\ndata type it is designed to support. In the case of the ADD opcode, for example,\nthe hardware will interpret the bit patterns of its operands as 2s complement\nintegers. Therefore, if a programmer stored the bit pattern 0011000100110000 in\nR3, thinking that the bit pattern represented the integer 10, the instruction ADD\nR4, R3, #10 would write the integer 12,602 into R4, and not the ASCII code for\nthe integer 20. Why? Because the opcode ADD interprets the bit patterns of its\noperands as 2s complement integers, and not ASCII codes, regardless what the\nperson creating those numbers intended.\n5.1.6 Addressing Modes\nAn addressing mode is a mechanism for specifying where the operand is located.\nAn operand can generally be found in one of three places: in memory, in a register,\nor as a part of the instruction. If the operand is a part of the instruction, we refer to\nit as a literal or as an immediate operand. The term literal comes from the fact that\nthe bits of the instruction literally form the operand. The term immediate comes\nfrom the fact that we can obtain the operand immediately from the instruction,\nthat is, we dont have to look elsewhere for it.\nThe LC-3 supports ve addressing modes: immediate (or literal), register,\nand three memory addressing modes: PC-relative, indirect, and Base+oset. We\nwill see in Section 5.2 that operate instructions use two addressing modes: register\nand immediate. We will see in Section 5.3 that data movement instructions use\nfour of the ve addressing modes.\n5.1.7 Condition Codes\nOne nal item will complete our overview of the ISA of the LC-3: condition\ncodes. The LC-3 has three single-bit registers that are individually set (set to\n1) or cleared (set to 0) each time one of the eight general purpose registers is\nwritten into as a result of execution of one of the operate instructions or one of\nthe load instructions. Each operate instruction performs a computation and writes\nthe result into a general purpose register. Each load instruction reads the contents\nof a memory location and writes the value found there into a general purpose\nregister. We will discuss all the operate instructions in Section 5.2 and all the\nload instructions in Section 5.3.\nThe three single-bit registers are called N, Z, and P, corresponding to their\nmeaning: negative, zero, and positive. Each time a GPR is written by an operate\nor a load instruction, the N, Z, and P one-bit registers are individually set to 0\n",
    "page_number": 108,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "or 1, corresponding to whether the result written to the GPR is negative, zero,\nor positive. That is, if the result is negative, the N register is set, and Z and P\nare cleared. If the result is zero, Z is set and N and P are cleared. If the result is\npositive, P is set and N and Z are cleared.\nThe set of three single-bit registers are referred to as condition codes because\nthe condition of those bits are used to change the sequence of execution of the\ninstructions in a computer program. Many ISAs use condition codes to change\nthe execution sequence. SPARC and x86 are two examples. We will show how\nthe LC-3 does it in Section 5.4.\n5.2 Operate Instructions\n5.2.1 ADD, AND, and NOT\nOperate instructions process data. Arithmetic operations (like ADD, SUB, MUL,\nand DIV) and logical operations (like AND, OR, NOT, XOR) are common\nexamples. The LC-3 has three operate instructions: ADD, AND, and NOT.\nThe NOT (opcode = 1001) instruction is the only operate instruction that\nperforms a unary operation, that is, the operation requires one source operand.\nThe\nresul\nits s\nspeci\nI\ninstr\nNOT\nR3\nR5\nR3 will contain 1010111100001111.\nFigure 5.4 shows the key parts of the data path that are used to perform the\nNOT instruction shown here. Since NOT is a unary operation, only the A input\nof the ALU is relevant. It is sourced from R5. The control signal to the ALU\ndirects the ALU to perform the bit-wise complement operation. The output of\nthe ALU (the result of the operation) is stored in R3 and the condition codes are\nset, completing the execution of the NOT instruction.\nRecall from Chapter 4 that the ADD (opcode = 0001) and AND (opcode =\n0101) instructions both perform binary operations; they require two 16-bit source\noperands. The ADD instruction performs a 2s complement addition of its two\nsource operands. The AND instruction performs a bit-wise AND of each pair\nof bits of its two 16-bit operands. Like the NOT, the ADD and AND use the\nregister addressing mode for one of the source operands and for the destina-\ntion operand. Bits [8:6] specify the source register, and bits [11:9] specify the\ndestination register (where the result will be written).\n",
    "page_number": 109,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "152\nFigure 5.4\nData path relevant to the execution of NOT R3, R5.\n5.2.2 Immediates\nThe second source operand for both ADD and AND instructions (as also dis-\ncussed in Chapter 4) can be specied by either register mode or as an immediate\noperand. Bit [5] determines which. If bit [5] is 0, then the second source operand\nuses a register, and bits [2:0] specify which register. In that case, bits [4:3] are set\n0\n0\n0\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n1\nADD\nR1\nR4\nR5\nIf bit [5] is 1, the second source operand is contained within the instruction.\nIn that case the second source operand is obtained by sign-extending bits [4:0] to\n16 bits before performing the ADD or AND. The result of the ADD (or AND)\ninstruction is written to the destination register and the condition codes are set,\n",
    "page_number": 110,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure 5.5\nData path relevant to the execution of ADD R1, R4, #2.\ncompleting the execution of the ADD (or AND) instruction. Figure 5.5 shows the\nkey parts of the data path that are used to perform the instruction\nADD R1, R4, #-2.\nSince the immediate operand in an ADD or AND instruction must t in bits\n[4:0]\noperan\nimme\nWha\nANSWER:\nRegister 2 is cleared (i.e., set to all 0s).\n",
    "page_number": 111,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "154\nE\nE\nstion: What distasteful result is also produced by this sequence? How can it\neasily be avoided?\n5.2.3 The LEA Instruction (Although Not Really an Operate)\nWhere to put the LEA instruction is a matter for debate (when you have nothing\nmore important to do!). It does not really operate on data, it simply loads a register\nwith an address. It clearly does not move data from memory to a register, nor is it\na control instruction. We had to put it somewhere, so we chose to discuss it here!\nLEA (opcode = 1110) loads the register specied by bits [11:9] of the\ninstruction with the value formed by adding the incremented program counter\nto the sign-extended bits [8:0] of the instruction. We saw this method of con-\nstructing an address in Chapter 4 with the LD instruction. However, in this\ncase, the instruction does not access memory, it simply loads the computed\naddress into a register. Perhaps a better name for this opcode would be CEA (for\nCompute Eective Address). However, since many microprocessors in industry\nthat have this instruction in their ISAs call it LEA (for Load Eective Address),\nwe have chosen to use the same acronym.\n",
    "page_number": 112,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure 5.6\nData path relevant to the execution of LEA R5, #3.\nWe shall see shortly that the LEA instruction is useful to initialize a regis-\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n1\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\nLEA\nR5\n3\nR5 will contain x4016 after the instruction at x4018 is executed. Question: Why\nwill R5 not contain the address x4015?\nFigure 5.6 shows the relevant parts of the data path required to execute the\nLEA instruction. Note that the value to be loaded into the register does not involve\nany access to memory. ...nor does it have any eect on the condition codes.\n5.3 Data Movement Instructions\nData movement instructions move information between the general purpose reg-\nisters and memory and between the registers and the input/output devices. We will\nignore for now the business of moving information from input devices to registers\nand from registers to output devices. This will be an important part of Chapter 9.\nIn this chapter, we will conne ourselves to moving information between memory\nand the general purpose registers.\n",
    "page_number": 113,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "The process of moving information from memory to a register is called a\nload, and the process of moving information from a register to memory is called a\nstore. In both cases, the information in the location containing the source operand\nremains unchanged. In both cases, the location of the destination operand is over-\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nopcode\nDR or SR\nAddr Gen bits\nData movement instructions require two operands, a source and a destination.\nThe source is the data to be moved; the destination is the location where it is\nmoved to. One of these locations is a register, the other is a memory location or\nan input/output device. In this chapter we will assume the second operand is in\nmemory. In Chapter 9 we will study the cases where the second operand is an\ninput or output device.\nBits [11:9] specify one of these operands, the register. If the instruction is a\nload, DR refers to the destination general purpose register that will contain the\nvalue after it is read from memory (at the completion of the instruction cycle). If\nthe instruction is a store, SR refers to the register that contains the value that will\nbe written to memory.\nBits [8:0] contain the address generation bits. That is, bits [8:0] contain infor-\nmation that is used to compute the 16-bit address of the second operand. In the\ncase of the LC-3s data movement instructions, there are three ways to interpret\nbits [8:0]. They are collectively called addressing modes. The opcode species\nhow to interpret bits [8:0]. That is, the LC-3s opcode species which of the three\naddressing modes should be used to obtain the address of the operand from bits\n[8:0] of the instruction.\n5.3.1 PC-Relative Mode\nLD (opcode = 0010) and ST (opcode = 0011) specify the PC-relative address-\ning mode. We have already discussed this addressing mode in Chapter 4. It is\nso named because bits [8:0] of the instruction specify an oset relative to the\nPC. The memory address is computed by sign-extending bits [8:0] to 16 bits and\nadding the result to the incremented PC. The incremented PC is the contents of\nthe program counter after the FETCH phase, that is, after the PC has been incre-\nmented. If the instruction is LD, the computed address (PC + oset) species the\nmemory location to be accessed. Its contents is loaded into the register specied\nby bits [11:9] of the instruction. If the instruction is ST, the contents of the regis-\nter specied by bits [11:9] of the instruction is written into the memory location\nwhose address is PC + oset. ...and the N, Z, and P one-bit condition codes are\nset depending on whether the value loaded is negative, positive, or zero.\n",
    "page_number": 114,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n0\n1\n0\n1\n1\n1\n1\nLD\nR2\nx1AF\nFigure 5.7 shows the relevant parts of the data path required to execute this\ninstruction. The three steps of the LD instruction are identied. In step 1, the\nincremented PC (x4019) is added to the sign-extended value contained in IR [8:0]\n(xFFAF), and the result (x3FC8) is loaded into the MAR. In step 2, memory is\nread and the contents of x3FC8 is loaded into the MDR. Suppose the value stored\nin x3FC8 is 5. In step 3, the value 5 is loaded into R2, and the NZP condition codes\nare set, completing the instruction cycle.\nNote that the address of the memory operand is limited to a small range of\nthe total memory. That is, the address can only be within +256 or 255 locations\nof the LD or ST instruction. This is the range provided by the sign-extended value\nFigure 5.7\nData path relevant to execution of LD R2, x1AF.\n",
    "page_number": 115,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "5.3.2 Indirect Mode\nLDI (opcode = 1010) and STI (opcode = 1011) specify the indirect address-\ning mode. An address is rst formed exactly the same way as with LD and ST.\nHowever, instead of this address being the address of the operand to be loaded\nor stored, it is the address of the address of the operand to be loaded or stored.\nHence the name indirect. Note that the address of the operand can be anywhere\n\nLDI\nR3\nx1CC\nis in x4A1B, and the contents of x49E8 is x2110, execution of this instruction\nresults in the contents of x2110 being loaded into R3.\nFigure 5.8 shows the relevant parts of the data path required to execute this\ninst\nof a\nIR [\nis i\nFigure 5.8\nData path relevant to the execution of LDI R3, x1CC.\n",
    "page_number": 116,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "contents of x2110 being loaded into R3. In step 3, since x2110 is not the operand,\nbut the address of the operand, it is loaded into the MAR. In step 4, memory is\nagain read, and the MDR again loaded. This time the MDR is loaded with the\ncontents of x2110. Suppose the value 1 is stored in memory location x2110. In\nstep 5, the contents of the MDR (i.e., 1) is loaded into R3 and the NZP condition\ncodes are set, completing the instruction cycle.\n5.3.3 Base+ofset Mode\nLDR (opcode = 0110) and STR (opcode = 0111) specify the Base+oset\naddr\noper\nThe s\nspeci\nI\nwith\ninstr\nIR\nFigure 5.9\nData path relevant to the execution of LDR R1, R2, x1D.\n",
    "page_number": 117,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "contained in IR [5:0] (x001D), and the result (x2362) is loaded into the MAR.\nSecond, memory is read, and the contents of x2362 is loaded into the MDR.\nSuppose the value stored in memory location x2362 is x0F0F. Third, and nally,\nthe contents of the MDR (in this case, x0F0F) is loaded into R1 and the NZP\ncondition codes are set, completing the execution of the LDR instruction.\nNote that the Base+oset addressing mode also allows the address of the\noperand to be anywhere in the computers memory.\n5.3.4 An Example\nWe conclude our study of addressing modes with a comprehensive example.\nAssume the contents of memory locations x30F6 through x30FC are as shown in\nFigure 5.10, and the PC contains x30F6. We will examine the eects of carrying\nout the seven instructions starting at location x30FC.\nSince the PC points initially to location x30F6, the rst instruction to be\nexecuted is the one stored in location x30F6. The opcode of that instruction is\n1110, load eective address (LEA). LEA loads the register specied by bits [11:9]\nwith the address formed by sign-extending bits [8:0] of the instruction and adding\nthe result to the incremented PC. The 16-bit value obtained by sign-extending\nbits [8:0] of the instruction is xFFFD. The incremented PC is x30F7. Therefore,\nat the end of execution of the LEA instruction, R1 contains x30F4, and the PC\ncontains x30F7.\nNext, the instruction stored in location x30F7 is executed. Since the opcode\n0001 species ADD, the sign-extended immediate in bits [4:0] (since bit [5] is\n1) is added to the contents of the register specied in bits [8:6], and the result\nis written to the register specied by bits [11:9]. Since the previous instruction\nwrote x30F4 into R1, and the sign-extended immediate value is x000E, the sum\nis x3102. At the end of execution of this instruction, R2 contains x3102, and the\nPC contains x30F8. R1 still contains x30F4.\nNext, the instruction stored in x30F8. The opcode 0011 species the ST\ninstruction, which stores the contents of the register specied by bits [11:9]\n(R2)\naddr\n(x3\nFigure 5.10\nA code fragment illustrating the three addressing modes.\n",
    "page_number": 118,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "(xFFFB). Therefore, at the end of execution of the ST instruction, memory loca-\ntion x30F4 (i.e., x30F9 + xFFFB) contains the value stored in R2 (x3102) and\nthe PC contains x30F9.\nNext the instruction at x30F9. The AND instruction, with an immediate\noperand x0000. At the end of execution, R2 contains the value 0, and the PC\ncontains x30FA.\nAt x30FA, the opcode 0001 species the ADD instruction. After execution,\nR2 contains the value 5, and the PC contains x30FB.\nAt x30FB, the opcode 0111 signies the STR instruction. STR (like LDR)\nuses the Base+oset addressing mode. The memory address is obtained by\nadding the contents of the BASE register (specied by bits [8:6]) to the sign-\nextended oset contained in bits [5:0]. In this case, bits [8:6] specify R1, which\ncontains x30F4. The 16-bit sign-extended oset is x000E. Since x30F4 + x000E\nis x3102, the memory address is x3102. The STR instruction stores into x3102\nthe contents of the register specied by bits [11:9], in this case R2. Since R2 con-\ntains the value 5, at the end of execution of this instruction, M[x3102] contains\nthe value 5, and the PC contains x30FC.\nFinally the instruction at x30FC. The opcode 1010 species LDI. LDI (like\nSTI) uses the indirect addressing mode. The memory address is obtained by rst\nforming an address as is done in the PC-relative addressing mode. Bits [8:0] are\nsign-extended to 16 bits (xFFF7) and added to the incremented PC (x30FD).\nTheir sum (x30F4) is the address of the operand address. Since M[x30F4] con-\ntains x3102, x3102 is the operand address. The LDI instruction loads the value\nfound at this address (in this case 5) into the register identied by bits [11:9] of\nthe instruction (in this case R3). At the end of execution of this instruction, R3\ncontains the value 5 and the PC contains x30FD.\n5.4 Control Instructions\nControl instructions change the sequence of instructions to be executed. If there\nwere no control instructions, the next instruction fetched after the current instruc-\ntion nishes would always be the instruction located in the next sequential\nmemory location. As you know, this is because the PC is incremented in the\nFETCH phase of each instruction cycle. We have already seen in the program of\nSection 4.4 that it is often useful to be able to break that sequence.\nThe LC-3 has ve opcodes that enable the sequential execution ow to\nbe broken: conditional branch, unconditional jump, subroutine call (sometimes\ncalled function), TRAP, and RTI (Return from Trap or Interrupt). In this sec-\ntion, we will deal almost entirely with the most common control instruction, the\nconditional branch. We will also discuss the unconditional jump and the TRAP\ninstruction. The TRAP instruction, often called service call, is useful because\nit allows a programmer to get help from the operating system to do things that\nthe typical programmer does not fully understand how to do. Typical examples:\ngetting information into the computer from input devices, displaying information\nto output devices, and stopping the computer. The TRAP instruction breaks the\n",
    "page_number": 119,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "sequential execution of a user program to start a sequence of instructions in the\noperating system. How the TRAP instruction does this, and in fact, most of the\ndiscussion of the TRAP instruction and all of the discussion of the subroutine\ncall and the return from interrupt we will leave for Chapters 8 and 9.\n5.4.1 Conditional Branches\nOf the ve instructions which change the execution ow from the next sequential\ninstruction to an instruction located someplace else in the program, only one of\nthem decides each time it is executed whether to execute the next instruction in\nsequence or whether to execute an instruction from outside that sequence. The\ninstruction that makes that decision each time it is executed is the conditional\nbranch instruction BR (opcode = 0000).\nLike all instructions in the LC-3, the PC is incremented during the FETCH\nphase of its instruction cycle. Based on the execution of previous instructions in\nthe program, the conditional branchs EXECUTE phase either does nothing or it\nloads the PC with the address of the instruction it wishes to execute next. If the\nconditional branch instruction does nothing during the EXECUTE phase, then\nthe incremented PC will remain unchanged, and the next instruction executed\nwill be\nTh\nchange\nreect\nTh\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n0\n0\n0\n0\nn\nz\np\nPCoset\nBits [11], [10], and [9] are associated with the three condition codes, N, Z, and P.\nAs you know, the three operate instructions (ADD, AND, and NOT) and\nthe three load instructions (LD, LDI, and LDR) in the LC-3 write values into\ngeneral purpose registers, and also set the three condition codes in accordance\nwith whether the value written is negative, zero, or positive.\nThe conditional branch instruction uses that information to determine\nwhether or not to depart from the usual sequential execution of instructions that\nwe get as a result of incrementing the PC during the FETCH phase of each\ninstruction.\nWe said (without explanation) in the computer program we studied in\nSection 4.4 that if bits [11:9] of the conditional branch instruction are 101, we\nwill depart from the usual sequential execution if the last value written into a reg-\nister by one of the six instructions listed above is not 0. We are now ready to see\nexactly what causes that to happen.\nDuring the EXECUTE phase of the BR instruction cycle, the processor\nexamines the condition codes whose associated bits in the instruction, bits [11:9],\nare 1. Note the lower case n, z, and p in bits [11:9] of the BR instruction for-\nmat shown above. If bit [11] is 1, condition code N is examined. If bit [10] is 1,\n",
    "page_number": 120,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "condition code Z is examined. If bit [9] is 1, condition code P is examined. If any\nof bits [11:9] are 0, the associated condition codes are not examined. If any of the\ncondition codes that are examined are set (i.e., equal to 1), then the PC is loaded\nwith the address obtained in the EVALUATE ADDRESS phase. If none of the\ncondition codes that are examined are set, the incremented PC is left unchanged,\nand the next sequential instruction will be fetched at the start of the next\ninstruction cycle.\nThe address obtained during the EVALUATE ADDRESS phase of the\ninstruction cycle is generated using the PC-relative addressing mode.\nIn our example in Section 4.4, the ADD instruction in memory location\nx3004 subtracted 1 from R2, wrote the result to R2, and set the condition codes.\nThe BR instruction in memory location x3005 shows bits [11:9] = 101. Since bit\n[11] is 1, if the N bit is set, the result of the ADD must have been negative. Since\nbit [9] is also 1, if the P bit is set, the result must have been positive. Since bit\n[10] is 0, we do not examine the Z bit. Thus if the previous result is positive or\nnegative (i.e., not 0), the PC is loaded with x3003, the address calculated in the\nEVALUATE ADDRESS phase of the branch instruction.\nRecall that the program of Figure 4.7 used R2 to keep track of the number\nof times the number 5 was added to R3. As long as we were not done with all\nour additions, the result of subtracting 1 from R2 was not zero. When we were\ndone with our additions, subtracting 1 from R2 produced the result 0, so Z was\nset to 1, N and P were set to 0. At that point, bits [11:9] checked the N and P\ncondition codes which were 0, so the incremented PC was not changed, and the\ninstruction at location x3006, a trap to the operating system to halt the computer,\nwas executed next.\nLets Lo\nat x4027,\n15\n0\n0\n0\n0\n0\n1\n0\n0\n1\n1\n0\n1\n1\n0\n0\n1\nBR\nn\nz\np\nx0D9\nFigure 5.11 shows the data path elements that are required to execute this\ninstruction. Note the logic required to determine whether the sequential instruc-\ntion ow should be broken. Each of the three AND gates corresponds to one of\nthe three condition codes. The output of the AND gate is 1 if the corresponding\ncondition code is 1 and if the associated bit in the instruction directs the hardware\nto check that condition code. If any of the three AND gates have an output 1, the\nOR gate has an output 1, indicating that the sequential instruction ow should\nbe broken, and the PC should be loaded with the address evaluated during the\nEVALUATE ADDRESS phase of the instruction cycle.\nIn the case of the conditional branch instruction at x4027, the answer is yes,\nand the PC is loaded with x4101, replacing x4028, which had been loaded into\nthe PC during the FETCH phase of the BR instruction.\n",
    "page_number": 121,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "164\nYes!\nFigure 5.11\nData path relevant to the execution of BRz x0D9.\nAnother Example.\nIf all three bits [11:9] are 1, then all three condition codes\nare examined. In this case, since the last result stored into a register had to be\neither negative, zero, or positive (there are no other choices!), one of the three\nQuestion: What happens if all three bits [11:9] in the BR instruction are 0?\n",
    "page_number": 122,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "5.4.2 Two Methods of Loop Control\nWe saw in Section 4.4 in our multiplication program that we repeatedly executed\na sequence of instructions until the value in a register was zero. We call that\nsequence a loop body, and each time the loop body is executed we call it one\niteration of the loop body. The BR instruction at the end of the sequence controls\nthe number of times the loop body is executed. There are two common ways to\ncontrol the number of iterations.\nFigure 5.12\nAn algorithm for adding integers using a counter for loop control.\nFirst, as in all algorithms, we must initialize our variables. That is, we must\nset up the initial values of the variables that the computer will use in executing the\nprogram that solves the problem. There are three such variables: the address of\nthe next integer to be added (assigned to R1), the running sum (assigned to R3),\nand the number of integers left to be added (assigned to R2). The three variables\nare initialized as follows: The address of the rst integer to be added is put in R1.\nR3, which will keep track of the running sum, is initialized to 0. R2, which will\nkeep track of the number of integers left to be added, is initialized to 12. Then\nthe process of adding begins.\nThe program repeats the process of loading into R4 one of the 12 integers\nand adding it to R3. Each time we perform the ADD, we increment R1 so it will\npoint to (i.e., contain the address of) the next number to be added and decrement\nR2 so we will know how many numbers still need to be added. When R2 becomes\nzero, the Z condition code is set, and we can detect that we are done.\nThe 10-instruction program shown in Figure 5.13 accomplishes the task.\nThe details of the program execution are as follows: The program starts\nwith PC = x3000. The rst instruction (at location x3000) initializes R1 with\n",
    "page_number": 123,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "166\nx3009\n0\n0\n0\n0\n1\n1\n1\n1 1 1 1\n1 1 0 1 0\nBRnzp x3004\nFigure 5.13\nA program that implements the algorithm of Figure 5.12.\nthe address x3100. (The incremented PC is x3001; the sign-extended PCoset is\nx00FF.)\nThe instruction at x3001 clears R3. R3 will keep track of the running sum,\nso it must start with the value 0. As we said previously, this is called initializing\nthe SUM to zero.\nThe instructions at x3002 and x3003 initialize R2 to 12, the number of inte-\ngers to be added. R2 will keep track of how many numbers have already been\nadded. This will be done (by the instruction in x3008) by decrementing R2 after\neach addition takes place.\nThe instruction at x3004 is a conditional branch instruction. Note that bit\n[10] is a 1. That means that the Z condition code will be examined. If it is set, we\nknow R2 must have just been decremented to 0. That means there are no more\nnumbers to be added, and we are done. If it is clear, we know we still have work\nto do, and we continue with another iteration of the loop body.\nThe instruction at x3005 loads the next integer into R4, and the instruction\nat x3006 adds it to R3.\nThe instructions at x3007 and x3008 perform the necessary bookkeeping.\nThe instruction at x3007 increments R1, so R1 will point to the next location in\nmemory containing an integer to be added. The instruction at x3008 decrements\nR2, which is keeping track of the number of integers still to be added, and sets\nthe condition codes.\nThe instruction at x3009 is an unconditional branch, since bits [11:9] are all 1.\nIt loads the PC with x3004. It also does not aect the condition codes, so the next\ninstruction to be executed (the conditional branch at x3004) will be based on the\ninstruction executed at x3008.\nThis is worth saying again. The conditional branch instruction at x3004 fol-\nlows the instruction at x3009, which does not aect condition codes, which in\nturn follows the instruction at x3008. Thus, the conditional branch instruction\nat x3004 will be based on the condition codes set by the instruction at x3008.\nThe instruction at x3008 sets the condition codes based on the value produced\nby decrementing R2. As long as there are still integers to be added, the ADD\ninstruction at x3008 will produce a value greater than zero and therefore clear\nthe Z condition code. The conditional branch instruction at x3004 examines the\n",
    "page_number": 124,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Z condition code. As long as Z is clear, the PC will not be aected, and the next\niteration of the loop body will begin. That is, the next instruction cycle will start\nwith an instruction fetch from x3005.\nThe conditional branch instruction causes the execution sequence to follow:\nx3000, x3001, x3002, x3003, x3004, x3005, x3006, x3007, x3008, x3009, x3004,\nx3005,x3006,x3007,x3008,x3009,x3004,x3005,andsoon.Theloopbodyconsists\nof the instructions at x3005 to x3009. When the value in R2 becomes 0, the PC is\nloaded with x300A, and the program continues at x300A with its next activity.\nYou may have noticed that we can remove the branch instruction at x3004\nif we replace the unconditional branch instruction at x3009 with a conditional\nbranch that tests for not 0 (i.e., bits [11:9]=101), and branches to the instruc-\ntion currently located in x3005. It is tempting to do that since it decreases the\nloop body by one instruction. BUT, we admonish you not to do that! The pro-\ngram as shown obeys the rules of structured programming that we will discuss\nin Chapter 6. The shortcut does work for this simple example, but it breaks the\nmethodology of structured programming. You do not want to get in the habit of\ntaking such shortcuts, since for larger programs it is a clear invitation to disaster.\nMore on this in Chapter 6.\nFinally, it is worth noting that we could have written a program to add these\n12 integers without any control instructions. We still would have needed the LEA\ninstruction in x3000 to initialize R1. We would not have needed the instruction\nat x3001 to initialize the running sum, nor the instructions at x3002 and x3003\nto initialize the number of integers left to be added. We could have loaded the\ncontents of x3100 directly into R3, and then repeatedly (by incrementing R1),\nloaded subsequent integers into R4 and adding R4 to the running sum in R3 11\nmore times! After the addition of the twelfth integer, we would go on to the next\ntask, as does the example of Figure 5.13 with the branch instruction in x3004.\nUnfortunately, instead of a 10-instruction program, we would have a 35-\ninstruction program. Moreover, if we had wished to add 100 integers without any\ncontrol instructions instead of 12, we would have had a 299-instruction program\ninstead of 10. The control instructions in the example of Figure 5.13 permit the\nreuse of sequences of code (the loop body) by breaking the sequential instruction\nexecution ow.\nLoop Control with a Sentinel\nThe example above controls the number of times\nthe loop body executes by means of a counter. We knew we wanted to execute the\nloop 12 times, so we simply set a counter to 12, and then after each execution of\nthe loop, we decremented the counter and checked to see if it was zero. If it was\nnot zero, we set the PC to the start of the loop and continued with another iteration.\nA second method for controlling the number of executions of a loop is to use\na sentinel. This method is particularly eective if we do not know ahead of time\nhow many iterations we will want to perform. Each iteration is usually based on\nprocessing a value. We append to our sequence of values to be processed a value\nthat we know ahead of time can never occur (i.e., the sentinel). For example, if\nwe are adding a sequence of numbers, a sentinel could be a letter A or a *, that is,\nsomething that is not a number. Our loop test is simply a test for the occurrence\nof the sentinel. When we nd it, we know we are done.\n",
    "page_number": 125,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "168\nx\nnzp x\nFigure 5.15\nA program that implements the algorithm of Figure 5.14.\nSuppose we know the values stored in locations x3100 to x310B are all pos-\nitive. Then we could use any negative number as a sentinel. Lets say the sentinel\nstored at memory address x310C is 1. The resulting owchart for this solution\nis shown in Figure 5.14, and the resulting program is shown in Figure 5.15.\nAs before, the instruction at x3000 loads R1 with the address of the rst value\nto be added, and the instruction at x3001 initializes R3 (which keeps track of the\nsum) to 0.\nAt x3002, we load the contents of the next memory location into R4. If the\nsentinel is loaded, the N condition code is set.\nThe conditional branch at x3003 examines the N condition code. If N=1, PC\nis loaded with x3008 and onto the next task. If N=0, R4 must contain a valid\nnumber to be added. In this case, the number is added to R3 (x3004), R1 is\nincremented to point to the next memory location (x3005), R4 is loaded with\nthe contents of the next memory location (x3006), and the PC is loaded with\nx3003 to begin the next iteration (x3007).\n",
    "page_number": 126,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "5.4.3 The JMP Instruction\nThe conditional branch instruction, for all its capability, does have one unfor-\ntunate limitation. The next instruction executed must be within the range of\naddresses that can be computed by adding the incremented PC to the sign-\nextended oset obtained from bits [8:0] of the instruction. Since bits [8:0] specify\na 2s complement integer, the next instruction executed after the conditional\nbranch can be at most +256 or 255 locations from the branch instruction itself.\nWhat if we would like to execute next an instruction that is 2000 locations\nfrom the current instruction? We cannot t the value 2000 into the nine-bit eld;\nergo, the conditional branch instruction does not work.\nThe LC-3 ISA does provide an instruction JMP (opcode = 1100) that can do\nthe job.\ned\naddr\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\nJMP\nBaseR\nR2 contains the value x6600, and the PC contains x4000, then the instruction at\nx4000 (the JMP instruction) will be executed, followed by the instruction located\nat x6600. Since registers contain 16 bits (the full address space of memory), the\nJMP instruction has no limitation on where the next instruction to be executed\nmust reside.\n5.4.4 The TRAP Instruction\nWe will discuss the details of how the TRAP instruction works in Chapter 9.\nHowever, because it will be useful long before that to get data into and out of the\ncomputer, we discuss the TRAP instruction here. The TRAP (opcode = 1111)\ninstruction changes the PC to a memory address that is part of the operating\nsystem so that the operating system will perform some task on behalf of the\nprogram that is executing. In the language of operating system jargon, we say\nthe TRAP instruction invokes an operating system service call. Bits [7:0] of the\nTRAP instruction form the tra vector an ei ht-bit code that identies the ser-\nvice cal\nTable\nthe LC-\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n1\n1\n1\n1\n0\n0\n0\n0\ntrapvector\nOnce the operating system is nished performing the service call, the pro-\ngram counter is set to the address of the instruction following the TRAP instruc-\ntion, and the program continues. In this way, a program can, during its execution,\n",
    "page_number": 127,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "request services from the operating system and continue processing after each\nsuch service is performed. The services we will require for now are\n* Input a character from the keyboard (trapvector = x23).\n* Output a character to the monitor (trapvector = x21).\n* Halt the program (trapvector = x25).\n5.5 Another Example: Counting\nOccurrences of a Character\nWe will nish our introduction to the ISA of the LC-3 with another example\nprogram. Suppose we would like to be able to input a character from the keyboard,\nthen count the number of occurrences of that character in a le, and nally display\nthat count on the monitor. We will simplify the problem by assuming that the\nnumber of occurrences of any character that we would be interested in is small\nenough that it can be expressed with a single decimal digit. That is, there will be\nat most nine occurrences. This simplication allows us to not have to worry about\ncomplex conversion routines between the binary count and the ASCII display on\nthe monitora subject we will get into in Chapter 10, but not today.\nFigure 5.16 is a owchart of the algorithm that solves this problem. Note\nthat each step is expressed both in English and also (in parentheses) in terms of\nan LC-3 implementation.\nThe rst step is (as always) to initialize all the variables. This means pro-\nviding starting values (called initial values) for R0, R1, R2, and R3, the four\nregisters the computer will use to execute the program that will solve the prob-\nlem. R2 will keep track of the number of occurrences; in Figure 5.16, it is referred\nto as Count. It is initialized to zero. R3 will point to the next character in the le\nthat is being examined. We refer to it as a pointer since it points to (i.e., contains\nthe address of) the location where the next character of the le that we wish to\nexamine resides. The pointer is initialized with the address of the rst character\nin the le. R0 will hold the character that is being counted; we will input that\ncharacter from the keyboard and put it in R0. R1 will hold, in turn, each character\nthat we get from the le being examined.\nWe should also note that there is no requirement that the le we are examining\nbe close to or far away from the program we are developing. For example, it is\nperfectly reasonable for the program we are developing to start at x3000 and the\nle we are examining to start at x9000. If that were the case, in the initialization\nprocess, R3 would be initialized to x9000.\nThe next step is to count the number of occurrences of the input character.\nThis is done by processing, in turn, each character in the le being examined, until\nthe le is exhausted. Processing each character requires one iteration of a loop.\nRecall from Section 5.4.3 that there are two common methods for keeping track\nof iterations of a loop. We will use the sentinel method, using the ASCII code for\nEOT (End of Transmission) (00000100) as the sentinel. A table of ASCII codes\nis in Appendix E.\n",
    "page_number": 128,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "172\nx3013\n0\n0\n0\n0\n0\n0\n0 0 0 0 1 1 0 0 0 0 ASCII TEMPLATE\nFigure 5.17\nA machine language program that implements the algorithm of Figure 5.16.\nIn each iteration of the loop, the contents of R1 is rst compared to the ASCII\ncode for EOT. If they are equal, the loop is exited, and the program moves on to\nthe nal step, displaying on the screen the number of occurrences. If not, there is\nwork to do. R1 (the current character under examination) is compared to R0 (the\ncharacter input from the keyboard). If they match, R2 is incremented. In either\ncase, we move on to getting the next character. The pointer R3 is incremented, the\nnext character is loaded into R1, and the program returns to the test that checks\nfor the sentinel at the end of the le.\nWhen the end of the file is reached, all the characters have been examined, and\nthe count is contained as a binary number in R2. In order to display the count on the\nmonitor, it is first converted to an ASCII code. Since we have assumed the count\nis less than 10, we can do this by putting a leading 0011 in front of the four-bit\nbinary representation of the count. Note in Figure E.2 the relationship between the\nbinaryvalueofeachdecimaldigitbetween0and9anditscorrespondingASCIIcode.\nFinally, the count is output to the monitor, and the program terminates.\nFigure 5.17 is a machine language program that implements the owchart of\nFigure 5.16.\nFirst the initialization steps. The instruction at x3000 clears R2 by ANDing\nit with x0000. The instruction at x3001 loads the starting address of the le to be\nexamined into R3. Again, we note that this le can be anywhere in memory. Prior\nto starting execution at x3000, some sequence of instructions must have stored the\nrst address of this le in x3012. Location x3002 contains the TRAP instruction,\n",
    "page_number": 129,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "which requests the operating system to perform a service call on behalf of this pro-\ngram. The function requested, as identied by the eight-bit trapvector 00100011\n(i.e., x23), is to load into R0 the ASCII code of the next character typed on the\nkeyboard. Table A.2 lists trapvectors for all operating system service calls that\ncan be performed on behalf of a user program. The instruction at x3003 loads the\ncharacter pointed to by R3 into R1.\nThen the process of examining characters begins. We start (x3004) by sub-\ntracting 4 (the ASCII code for EOT) from R1 and storing it in R4. If the result\nis zero, the end of the le has been reached, and it is time to output the count.\nThe instruction at x3005 conditionally branches to x300E, where the process of\noutputting the count begins.\nIf R4 is not equal to zero, the character in R1 is legitimate and must be\nexamined. The sequence of instructions at locations x3006, x3007, and x3008\ndetermines whether the contents of R1 and R0 are identical. Taken together, the\nthree instructions compute\nR0  R1\nThis produces all zeros only if the bit patterns of R1 and R0 are identical. If the\nbit patterns are not identical, the conditional branch at x3009 branches to x300B;\nthat is, it skips the instruction at x300A, which increments the counter (R2).\nThe instruction at x300B increments R3, so it will point to the next charac-\nter in the le being examined, the instruction at x300C loads that character into\nR1, and the instruction at x300D unconditionally takes us back to x3004 to start\nprocessing that character.\nWhen the sentinel (EOT) is nally detected, the process of outputting the\ncount begins (at x300E). The instruction at x300E loads 00110000 into R0, and\nthe instruction at x300F adds the count to R0. This converts the binary represen-\ntation of the count (in R2) to the ASCII representation of the count (in R0). The\ninstruction at x3010 invokes a TRAP to the operating system to output the con-\ntents of R0 to the monitor. When that is done and the program resumes execution,\nthe instruction at x3011 invokes a TRAP instruction to terminate the program.\nQuestion: Can you improve the execution of the above program? Hint: How\nmany times are the instructions at x3006 and x3007 executed. What small change\nwill decrease the total number of instructions that have to be executed.\n5.6 The Data Path Revisited\nBefore we leave Chapter 5, let us revisit the data path diagram that we rst\nencountered in Chapter 3 (Figure 3.35). Many of the structures we have seen\nearlier in this chapter in Figures 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, and 5.11. We repro-\nduce the data path diagram as Figure 5.18. Note at the outset that there are two\nkinds of arrows in the data path, those with arrowheads lled in and those with\narrowheads not lled in. Filled-in arrowheads designate information that is pro-\ncessed. Unlled-in arrowheads designate control signals. Control signals emanate\nfrom the block labeled Finite State Machine. The connections from the nite\nstate machine to most control signals have been left o Figure 5.18 to reduce\nunnecessary clutter in the diagram.\n",
    "page_number": 130,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "5.6.1 Basic Components of the Data Path\n5.6.1.1 The Global Bus\nThe most obvious item on the data path diagram is the heavy black structure\nwith arrowheads at both ends. This represents the data paths global bus. The\nLC-3 global bus consists of 16 wires and associated electronics. It allows one\nstructure to transfer up to 16 bits of information to another structure by making the\nnecessary electronic connections on the bus. Exactly one value can be transferred\non the bus at one time. Note that each structure that supplies values to the bus\nhas a triangle just behind its input arrow to the bus. This triangle (called a tri-\nstate device) allows the computers control logic to enable exactly one supplier to\nprovide information to the bus at any one time. The structure wishing to obtain the\nvalue being supplied can do so by asserting its LD.x (load enable) signal (recall\nour discussion of gated latches in Section 3.4.2). Not all computers have a single\nglobal bus. The pros and cons of a single global bus is yet another topic that will\nhave to wait for later in your education.\n5.6.1.2 Memory\nOne of the most important parts of any computer is the memory that contains\nboth instructions and data. Memory is accessed by loading the memory address\nregister (MAR) with the address of the location to be accessed. To perform a load,\ncontrol signals then read the contents of that memory location, and the result of\nthat read is delivered by the memory to the memory data register (MDR). On the\nother hand, to perform a store, what is to be stored is loaded into the MDR. Then\nthe control signals assert a write enable (WE) signal in order to store the value\ncontained in MDR in the memory location specied by MAR.\n5.6.1.3 The ALU and the Register File\nThe ALU is the processing element. It has two inputs, source 1 from a register and\nsource 2 from either a register or the sign-extended immediate value provided by\nthe instruction. The registers (R0 through R7) can provide two values: source 1,\nwhich is controlled by the three-bit register number SR1, and source 2, which is\ncontrolled by the three-bit register number SR2. SR1 and SR2 are elds in the\nLC-3 operate instructions. The selection of a second register operand or a sign-\nextended immediate operand is determined by bit [5] of the LC-3 instruction.\nNote the mux that provides source 2 to the ALU. The select line of that mux is\nbit [5] of the LC-3 operate instruction.\nThe results of an ALU operation are (a) a result that is stored in one of the\nregisters, and (b) the three single-bit condition codes. Note that the ALU can supply\n16 bits to the bus, and that value can then be written into the register specified by the\nthree-bit register number DR. Also, note that the 16 bits supplied to the bus are also\ninput to logic that determines whether that 16-bit value is negative, zero, or positive.\nThe three one-bit condition code registers N, Z, and P are set accordingly.\n5.6.1.4 The PC and the PCMUX\nAt the start of each instruction cycle, the PC supplies to the MAR over the\nglobal bus the address of the instruction to be fetched. In addition, the PC, in\nturn, is supplied via the three-to-one PCMUX. During the FETCH phase of the\n",
    "page_number": 131,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "instruction cycle, the PC is incremented and written into the PC. That is shown\nas the rightmost input to the PCMUX.\nIf the current instruction is a control instruction, then the relevant source of\nthe PCMUX depends on which control instruction is currently being processed.\nIf the current instruction is a conditional branch and the branch is taken, then the\nPC is loaded with the incremented PC + PCoset (the 16-bit value obtained by\nsign-extending IR [8:0]). Note that this addition takes place in the special adder\nand not in the ALU. The output of the adder is the middle input to PCMUX. The\nthird input to PCMUX is obtained from the global bus. Its use will become clear\nafter we discuss other control instructions in Chapters 9.\n5.6.1.5 The MARMUX\nAs you know, memory is accessed by supplying the address to the MAR. The\nMARMUX controls which of two sources will supply the MAR with the appro-\npriate address during the execution of a load, a store, or a TRAP instruction. The\nright input to the MARMUX is obtained by adding either the incremented PC or\na base register to zero or a literal value supplied by the IR. Whether the PC or a\nbase register and what literal value depends on which opcode is being processed.\nThe control signal ADDR1MUX species the PC or base register. The control\nsignal ADDR2MUX species which of four values is to be added. The left input\nto MARMUX provides the zero-extended trapvector, which is needed to invoke\nservice calls, and will be discussed in detail in Chapter 9.\n5.6.2 The Instruction Cycle Specic to the LC-3\nWe co\ninstruct\nlocatio\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n0\n1\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\nLDR\nR3\nR2\n4\nSuppose the LC-3 has just completed processing the instruction at x3455, which\nhappened to be an ADD instruction.\n5.6.2.1 FETCH\nAs you know, the instruction cycle starts with the FETCH phase. That is, the\ninstruction is obtained by accessing memory with the address contained in the PC.\nIn the rst cycle, the contents of the PC is loaded via the global bus into the MAR,\nand the PC is incremented and loaded into the PC. At the end of this cycle, the\nPC contains x3457. In the next cycle (if memory can provide information in one\ncycle), the memory is read, and the instruction 0110011010000100 is loaded into\nthe MDR. In the next cycle, the contents of the MDR is loaded into the instruction\nregister (IR), completing the FETCH phase.\n5.6.2.2 DECODE\nIn the next cycle, the contents of the IR is decoded, resulting in the control\nlogic providing the correct control signals (unlled arrowheads) to control the\n",
    "page_number": 132,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "5.6.2.4 OPERAND FETCH\nIn the next cycle (or more than one, if memory access takes more than one cycle),\nthe value at that address is loaded into the MDR.\n5.6.2.5 EXECUTE\nThe LDR instruction does not require an EXECUTE phase, so this phase takes\nzero cycles.\n5.\n5.\n5.\n5.\nsame way as in the LC-3.\n",
    "page_number": 133,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "ramming\nW\ne are now ready to develop programs to solve problems with the com-\nputer. In this chapter we attempt to do two things: rst, we develop a\nmethodology for constructing programs to solve problems (Section 6.1, Problem\nSolving), and second, we develop a methodology for xing those programs (Sec-\ntion 6.2, Debugging) under the likely condition that we did not get everything\nright the rst time.\nThere is a long tradition that the errors present in programs are referred to as\nbugs, and the process of removing those errors is called debugging. The opportu-\nnities for introducing bugs into a complicated program are so great that it usually\ntakes much more time to get the program to work correctly (debugging) than it\ndoes to create the program in the rst place.\n6.1 Problem Solving\n6.1.1 Systematic Decomposition\nRecall from Chapter 1 that in order for electrons to solve a problem, we need to go\nthrough several levels of transformation to get from a natural language description\nof the problem (in our case English, although many of you might prefer Italian,\nMandarin, Hindi, or something else) to something electrons can deal with. Once\nwe have a natural language description of the problem, the next step is to trans-\nform the problem statement into an algorithm. That is, the next step is to transform\nthe problem statement into a step-by-step procedure that has the properties of def-\niniteness (each step is precisely stated), eective computability (each step can be\ncarried out by a computer), and niteness (the procedure terminates).\nIn the late 1960s, the concept of structured programming emerged as a way\nto dramatically improve the ability of average programmers to take a complex\ndescription of a problem and systematically decompose it into smaller and smaller\nmanageable units so that they could ultimately write a program that executed cor-\nrectly. The methodology has also been called systematic decomposition because\nthe larger tasks are systematically broken down into smaller ones.\n",
    "page_number": 134,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "We will nd the systematic decomposition model a useful technique for\ndesigning computer programs to carry out complex tasks.\n6.1.2 The Three Constructs: Sequential, Conditional, Iterative\nSystematic decomposition is the process of taking a task, that is, a unit of work\n(see Figure 6.1a), and breaking it into smaller units of work such that the collec-\ntion of smaller units carries out the same task as the one larger unit. The idea is\nthat if one starts with a large, complex task and applies this process again and\nagain, one will end up with very small units of work and consequently be able to\neasily write a program to carry out each of these small units of work. The process\nis also referred to as stepwise renement, because the process is applied one step\nat a time, and each step renes one of the tasks that is still too complex into a\ncollection of simpler subtasks.\nThe idea is to replace each larger unit of work with a construct that correctly\ndecomposes it. There are basically three constructs for doing this: sequential,\nconditional, and iterative.\nThe sequential construct (Figure 6.1b) is the one to use if the designated\ntask can be broken down into two subtasks, one following the other. That is, the\ncomputer is to carry out the rst subtask completely, then go on and carry out the\nsecond subtask completely never going back to the rst subtask after starting\nFigure 6.1\nThe basic constructs of structured programming.\n",
    "page_number": 135,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "condition is true, the computer is to carry out one subtask. If the condition is not true,\nthe computer is to carry out a different subtask. Either subtask may be vacuous; that\nis, it may do nothing. Regardless, after the correct subtask is completed, the program\nmoves onward. The program never goes back and retests the condition.\nThe iterative construct (Figure 6.1d) is the one to use if the task consists of\ndoing a subtask a number of times, but only as long as some condition is true. If\nthe condition is true, do the subtask. After the subtask is nished, go back and\ntest the condition again. As long as the result of the condition tested is true, the\nprogram continues to carry out the same subtask again and again. The rst time\nthe test is not true, the program proceeds onward.\nNote in Figure 6.1 that whatever the task of Figure 6.1a, work starts with the\narrow into the top of the box representing the task and finishes with the arrow out\nof the bottom of the box. There is no mention of what goes oninside the box. In each\nof the three possible decompositions of Figure 6.1a (i.e., Figure 6.1b, c, and d), there\nis exactly one entrance into the construct and exactly one exit out of the construct.\nThus, it is easy to replace any task of the form of Figure 6.1a with whichever of its\nthree decompositions apply. We will see how with several examples.\n6.1.3 LC-3 Control Instructions to Implement\nFigure 6.2\nUse of LC-3 control instructions to implement structured programming.\n",
    "page_number": 136,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "decomposition constructs. That is, Figure 6.2b, c, and d corresponds respectively\nto the three constructs shown in Figure 6.1b, c, and d.\nWe use the letters A, B, C, and D to represent addresses in memory containing\nLC-3 instructions. The letter A, for example, represents the address of the rst\nLC-3 instruction to be executed in all three cases, since it is the starting address\nof the task to be decomposed (shown in Figure 6.2a).\nFigure 6.2b illustrates the control ow of the sequential decomposition. Note\nthat no control instructions are needed since the PC is incremented from Address\nB1 to Address B1+1. The program continues to execute instructions through\naddress D1. It does not return to the rst subtask.\nFigure 6.2c illustrates the control ow of the conditional decomposition.\nFirst, a condition is generated, resulting in the setting of one of the condition\ncodes. This condition is tested by the conditional branch instruction at Address\nB2. If the condition is true, the PC is set to Address C2+1, and subtask 1 is\nexecuted. (Note: x equals 1 + the number of instructions in subtask 2.) If the\ncondition is false, the PC (which had been incremented during the FETCH phase\nof the branch instruction) fetches the instruction at Address B2+1, and subtask\n2 is executed. Subtask 2 terminates in a branch instruction that at address C2\nunconditionally branches to D2+1. (Note: y equals the number of instructions in\nsubtask 1.)\nFigure 6.2d illustrates the control ow of the iterative decomposition. As\nin the case of the conditional construct, rst a condition is generated, a condi-\ntion code is set, and a conditional branch instruction is executed. In this case,\nthe condition bits of the instruction at address B3 are set to cause a conditional\nbranch if the condition generated is false. If the condition is false, the PC is set\nto address D3+1. (Note: z equals 1 + the number of instructions in the subtask\nin Figure 6.2d.) On the other hand, as long as the condition is true, the PC will\nbe incremented to B3+1, and the subtask will be executed. The subtask termi-\nnates in an unconditional branch instruction at address D3, which sets the PC to\nA to again generate and test the condition. (Note: w equals the total number of\ninstructions in the decomposition shown as Figure 6.2d.)\nNow, we are ready to move on to an example.\n6.1.4 The Character Count Example from Chapter 5, Revisited\nRecall the example of Section 5.5. The statement of the problem is as follows:\nWe wish to input a character from the keyboard, count the number of occurrences\nof that character in a le, and display that count on the monitor.\nThe systematic decomposition of this English language statement of the\nproblem to the nal LC-3 implementation is shown in Figure 6.3. Figure 6.3a\nis a brief statement of the problem.\nIn order to solve the problem, it is always a good idea rst to examine exactly\nwhat is being asked for, and what is available to help solve the problem. In this\ncase, the statement of the problem says that we will get the character of inter-\nest from the keyboard, and that we must examine all the characters in a le and\ndetermine how many are identical to the character obtained from the keyboard.\nFinally, we must output the result.\n",
    "page_number": 137,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure 6.3\nStepwise renement of the character count program (Fig. 6.3 continued on\nnext page.)\nTo do this, we will need to examine in turn all the characters in a le, we will\nneed to compare each to the character we input from the keyboard, and we will\nneed a counter to increment each time we get a match.\nWe will need registers to hold all these pieces of information:\n1. The character input from the keyboard.\n2. Where we are (a pointer) in our scan of the le.\n3. The character in the le that is currently being examined.\n4. The count of the number of occurrences.\nWe will also need to know when we have reached the end of the le.\n",
    "page_number": 138,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "208\nFigure 6.3\nStepwise renement of the character count program (Fig. 6.3 continued on\nnext page.)\nThe problem decomposes naturally (using the sequential construct) into three\nparts as shown in Figure 6.3b: (A) initialization, which includes keyboard input\nof the character to be counted, (B) the process of determining how many occur-\nrences of the character are present in the le, and (C) displaying the count on the\nmonitor.\nWe have seen the importance of proper initialization in several examples\nalready. Before a computer program can get to the crux of the problem, it must\nhave the correct initial values. These initial values do not just show up in the GPRs\nby magic. They get there as a result of the rst set of steps in every algorithm: the\ninitialization of its variables.\nIn this particular algorithm, initialization (as we said in Chapter 5) consists\nof starting the counter at 0, setting the pointer to the address of the rst character\nin the le to be examined, getting an input character from the keyboard, and get-\nting the rst character from the le. Collectively, these four steps comprise the\ninitialization of the algorithm shown in Figure 6.3b as A.\n",
    "page_number": 139,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure 6.3\nStepwise renement of the character count program (continued Fig. 6.3\nfrom previous page.)\nFigure 6.3c decomposes B into an iteration construct, such that as long as\nthere are characters in the le to examine, the loop iterates. B1 shows what gets\naccomplished in each iteration. The character is tested and the count incremented\nif there is a match. Then the next character is prepared for examination. Recall\nfrom Chapter 5 that there are two basic techniques for controlling the number\n",
    "page_number": 140,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "of iterations of a loop: the sentinel method and the use of a counter. Since we\nare unlikely to know how many characters there are in a random le, and since\neach le ends with an end of text (EOT) character, our choice is obvious. We use\nthe sentinel method, that is, testing each character to see if we are examining a\ncharacter in the le or the EOT character.\nFigure 6.3c also shows the initialization step in greater detail. Four LC-3\nregisters (R0, R1, R2, and R3) have been specied to handle the four requirements\nof the algorithm: the input character from the keyboard, the current character\nbeing tested, the counter, and the pointer to the next character to be tested.\nFigure 6.3d decomposes both B1 and C using the sequential construct in both\ncases. In the case of B1, rst the current character is tested (B2), and the counter\nincremented if we have a match, and then the next character is fetched (B3). In\nthe case of C, rst the count is prepared for display by converting it from a 2s\ncomplement integer to an ASCII code (C1), and then the actual character output\nis performed (C2).\n6.2 Debugging\nDebugging a program is pretty much applied common sense. A simple example\ncomes to mind: You are driving to a place you have never visited, and somewhere\nalong the way you made a wrong turn. What do you do now? One common driv-\ning debugging technique is to wander aimlessly, hoping to nd your way back.\nWhen that does not work, and you are nally willing to listen to the person sitting\nnext to you, you turn around and return to some known position on the route.\nThen, using a map (very dicult for some people), you follow the directions pro-\nvided, periodically comparing where you are (from landmarks you see out the\nwindow) with where the map says you should be, until you reach your desired\ndestination.\nDebugging is somewhat like that. A logical error in a program can make\nyou take a wrong turn. The simplest way to keep track of where you are as\n",
    "page_number": 141,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "compared to where you want to be is to trace the program. This consists of\nkeeping track of the sequence of instructions that have been executed and the\nresults produced by each instruction executed. When you examine the sequence\nof instructions executed, you can detect errors in the ow of the program. When\nyou compare what each instruction has done to what it is supposed to do, you\ncan detect logical errors in the program. In short, when the behavior of the pro-\ngram as it is executing is dierent from what it should be doing, you know there\nis a bug.\nA useful technique is to partition the program into parts, often referred to as\nmodules, and examine the results that have been computed at the end of execu-\ntion of each module. In fact, the structured programming approach discussed in\nSection 6.1 can help you determine where in the programs execution you should\nexamine results. This allows you to systematically get to the point where you\nare focusing your attention on the instruction or instructions that are causing the\nproblem.\n6.2.1 Debugging Operations\nMany sophisticated debugging tools are oered in the marketplace, and\nundoubtedly you will use many of them in the years ahead. In Chapter 15, for\nexample, we will examine debugging techniques using a source-level debugger\nfor C.\nRight now, however, we wish to stay at the level of the machine architecture,\nso we will see what we can accomplish with a few very elementary interactive\ndebugging operations. We will set breakpoints, single-step, and examine the state\nof a program written in the LC-3 ISA.\nIn Chapter 15, we will see these same concepts again: breakpoints, single-\nstepping, and examining program state that we are introducing here, but applied\nto a C program, instead of the 0s and 1s of a program written in the LC-3 ISA.\nWhen debugging interactively, the user sits in front of the keyboard and mon-\nitor and issues commands to the computer. In our case, this means operating an\nLC-3 simulator, using the menu available with the simulator. It is important to be\nable to:\n1. Write values into memory locations and into registers.\n2. Execute instruction sequences in a program.\n3. Stop execution when desired.\n4. Examine what is in memory and registers at any point in the program.\nThese few simple operations will go a long way toward debugging programs.\n6.2.1.1 Set Values\nIn order to test the execution of a part of a program in isolation without having\nto worry about parts of the program that come before it, it is useful to rst write\nvalues in memory and in registers that would have been written by earlier parts of\nthe program. For example, suppose one module in your program supplies input\nfrom a keyboard, and a subsequent module operates on that input. Suppose you\n",
    "page_number": 142,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "want to test the second module before you have nished debugging the rst mod-\nule. If you know that the keyboard input module ends up with an ASCII code in\nR0, you can test the module that operates on that input by rst writing an ASCII\ncode into R0.\n6.2.1.2 Execute Sequences\nIt is important to be able to execute a sequence of instructions and then stop\nexecution in order to examine the values that the program has computed as a\nresult of executing that sequence. Three simple mechanisms are usually available\nfor doing this: run, step, and set breakpoints.\nThe Run command causes the program to execute until something makes it\nstop. This can be either a HALT instruction or a breakpoint.\nThe Step command causes the program to execute a xed number of instruc-\ntions and then stop. The interactive user enters the number of instructions he/she\nwishes the simulator to execute before it stops. When that number is 1, the com-\nputer executes one instruction, then stops. Executing one instruction and then\nstopping is called single-stepping. It allows the person debugging the program to\nexamine the individual results of each instruction executed.\nThe Set Breakpoint command causes the program to stop execution at a\nspecic instruction in a program. Executing the debugging command Set Break-\npoint consists of adding an address to a list maintained by the simulator. During\nthe FETCH phase of each instruction, the simulator compares the PC with the\naddresses in that list. If there is a match, execution stops. Thus, the eect of setting\na breakpoint is to allow execution to proceed until the PC contains an address that\nhas been set as a breakpoint. This is useful if one wishes to know what has been\ncomputed up to a particular point in the program. One sets a breakpoint at that\naddress in the program and executes the Run command. The program executes\nuntil that point and then stops so the user can examine what has been computed\nup to that point. (When one no longer wishes to have the program stop execution\nat that point, the breakpoint can be removed by executing the Clear Breakpoint\ncommand.)\n6.2.1.3 Display Values\nFinally, it is useful to examine the results of execution when the simulator has\nstopped execution. The Display command allows the user to examine the contents\nof any memory location or any register.\n6.2.2 Use of an Interactive Debugger\nWe conclude this chapter with four examples, showing how the use of interactive\ndebugging operations can help us nd errors in a program. We have chosen the\nfollowing four errors: (1) incorrectly setting the loop control so that the loop exe-\ncutes an incorrect number of times, (2) confusing the load instruction 0010, which\nloads a register with the contents of a memory location, with the load eective\naddress instruction 1110, which loads a register with the address of a memory\nlocation, (3) forgetting which instructions set the condition codes, resulting in\n",
    "page_number": 143,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "a branch instruction testing the wrong condition, and (4) not covering all possible\ncases of input values.\n6.2.2.1 Example 1: Multiplying Without a Multiply Instruction\nx\nx\nx\nx\nx\nFigure 6.4\nDebugging Example 1. An LC-3 program to multiply (without a Multiply\ninstruction).\nIf we examine the program instruction by instruction, we note that the pro-\ngram rst clears R2 (i.e., initializes R2 to 0) and then attempts to perform the\nmultiplication by adding R4 to itself a number of times equal to the initial value\nin R5. Each time an add is performed, R5 is decremented. When R5 = 0, the\nprogram terminates.\nIt looks like the program should work! Upon execution, however, we nd that\nif R4 initially contains the integer 10 and R5 initially contains the integer 3, the\nprogram produces the result 40. What went wrong?\nOur rst thought is to trace the program. Before we do that, we note that the\nprogram assumes positive integers in R4 and R5. Using the Set Values command,\nwe put the value 10 in R4 and the value 3 in R5.\nIt is also useful to annotate each instruction with some algorithmic descrip-\ntion of exactly what each instruction is doing. While this can be very tedious\nand not very helpful in a 10,000-instruction program, it often can be very helpful\nafter one has isolated a bug to within a few instructions. There is a big dierence\nbetween quickly eyeballing a sequence of instructions and stating precisely what\neach instruction is doing. Quickly eyeballing often results in mistaking what one\neyeballs! Stating precisely usually does not. We have included in Figure 6.4, next\nto each instruction, such an annotation.\nFigure 6.5a shows a trace of the program, which we can obtain by single-\nstepping. The column labeled PC shows the contents of the PC at the start of\neach instruction. R2, R4, and R5 show the values in those three registers at the\nstart of each instruction.\nA quick look at the trace shows that the loop body was executed four times,\nrather than three. That suggests that the condition codes for our branch instruction\ncould have been set incorrectly. From there it is a short step to noting that the\nbranch should have been taken only when R5 was positive, and not when R5 is 0.\nThat is, bit [10]=1 in the branch instruction caused the extra iteration of the loop.\n",
    "page_number": 144,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "214\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\nBR\nn\nz\np\n3\nWe should also note that we could have saved a lot of the work of tracing the\nprogram by using a breakpoint. That is, instead of examining the results of each\ninstruction, if we set a breakpoint at x3203, we would examine the results of\neach iteration of the loop. Setting a breakpoint to stop the program after each\niteration of the loop is often enough to have us see the problem (and debug the pro-\ngram) without the tedium of single-stepping each iteration of the loop. Figure 6.5b\nshows the results of tracing the program, where each step is one iteration of the\nloop. We see that the loop executed four times instead of three, immediately\nidentifying the bug.\n",
    "page_number": 145,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "216\nFigure 6.8\nDebugging Example 2. A trace of the rst four instructions of the Add\nprogram.\nx3002, the loop control (R4), which counts the number of values added to R1, is\ninitialized to #10. The program subtracts 1 each time through the loop and repeats\nuntil R4 contains 0. In x3003, the base register (R2) is initialized to the starting\nlocation of the values to be added: x3100.\nFrom there, each time through the loop, one value is loaded into R3 (in\nx3004), the base register is incremented to get ready for the next iteration (x3005),\nthe value in R3 is added to R1, which contains the running sum (x3006), the\ncounter is decremented (x3007), the P bit is tested, and if true, the PC is set to\nx3004 to begin the next iteration of the loop body (x3008). After ten times through\nthe loop, R4 contains 0, the P bit is 0, the branch is not taken, and the program\nterminates (x3009).\nIt looks like the program should work. However, when we execute the pro-\ngram and then check the value in R1, we nd the number x0024, which is not\nx8135, the sum of the numbers stored in locations x3100 to x3109. What went\nwrong?\nWe turn to the debugger and trace the program. Figure 6.8 shows a trace of\nthe rst four instructions executed. Note that after the instruction at x3003 has\nexecuted, R2 contains x3107, not x3100 as we had expected. The problem is\nthat the opcode 0010 loaded the contents of M[x3100] (i.e., x3107) into R2, not\nthe address x3100. The result was to add the ten numbers starting at M[x3107]\ninstead of the ten numbers starting at M[x3100].\nOur mistake: We used the wrong opcode. We should have used the opcode\n1110, which would have loaded R2 with the address x3100. We correct the bug\nby replacing the opcode 0010 with 1110, and the program runs correctly.\n6.2.2.3 Example 3: Does a Sequence of Memory Locations Contain a 5?\nThe program of Figure 6.9 has been written to examine the contents of the ten\nmemory locations starting at address x3100 and to store a 1 in R0 if any of them\ncontains a 5 and a 0 in R0 if none of them contains a 5.\nThe program is supposed to do the following: The rst six instructions (at\nx3000 to x3005) initialize R0 to 1, R1 to 5, and R3 to 10. The instruction at\nx3006 initializes R4 to the address (x3100) of the rst location to be tested, and\nx3007 loads the contents of x3100 into R2.\nThe instructions at x3008 and x3009 determine if R2 contains the value 5 by\nadding 5 to R2 and branching to x300F if the result is 0. Since R0 is initialized\nto 1, the program terminates with R0 reporting the presence of a 5 among the\nlocations tested.\n",
    "page_number": 146,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "x3010\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\nx3100\nFigure 6.9\nDebugging Example 3. An LC-3 program to detect the presence of a 5.\nx300A increments R4, preparing to load the next value. x300B decrements\nR3, indicating the number of values remaining to be tested. x300C loads the next\nvalue into R2. x300D branches back to x3008 to repeat the process if R3 still\nindicates more values to be tested. If R3 = 0, we have exhausted our tests, so R0\nis set to 0 (x300E), and the program terminates (x300F).\nWhen we run the program for some sample data that contains a 5 in one of\nthe memory locations, the program terminates with R0 = 0, indicating there were\nno 5s in locations x3100 to x310A.\nWhat went wrong? We examine a trace of the program, with a breakpoint set\nat x300D. The results are shown in Figure 6.10.\nThe rst time the PC is at x300D, we have already tested the value stored in\nx3100, we have loaded 7 (the contents of x3101) into R2, and R3 indicates there\nare still nine values to be tested. R4 contains the address from which we most\nrecently loaded R2.\nThe second time the PC is at x300D, we have loaded 32 (the contents of\nx3102) into R2, and R3 indicates there are eight values still to be tested. The\nthird time the PC is at x300D, we have loaded 0 (the contents of x3103) into R2,\nFigure 6.10\nDebugging Example 3. Tracing Example 3 with a breakpoint at x300D.\n",
    "page_number": 147,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "The error in the program occurred because the branch instruction immedi-\nately followed the load instruction that set the condition codes based on what was\nloaded. That wiped out the condition codes set by the iteration control instruction\nat x300B, which was keeping track of the number of iterations left to do. Since\nthe branch instruction should branch if there are still more memory locations to\nexamine, the branch instruction should have immediately followed the iteration\ncontrol instruction and NOT the load instruction which also sets condition codes.\nA conditional branch instruction should be considered the second instruction\nin a pair of instructions.\nInstruction A\n; sets the condition codes\nBR instruction ; branches based on the condition codes\nThe first instruction in the pair (Instruction A) sets the condition codes. The\nsecond instruction (BR) branches or not, depending on the condition codes set by\ninstruction A. It is important to never insert any instruction that sets condition codes\nbetween instruction A and the branch instruction, since doing so will wipe out the\ncondition codes set by instruction A that are needed by the branch instruction.\nSince the branch at x300D was based on the value loaded into R2, instead\nof how many values remained to be tested, the third time the branch instruction\nwas executed, it was not taken when it should have been. If we interchange the\ninstructions at x300B and x300C, the branch instruction at x300D immediately\nfollows the iteration control instruction, and the program executes correctly.\nIt is also worth noting that the branch at x300D coincidentally behaved cor-\nrectly the rst two times it executed because the load instruction at x300C loaded\npositive values into R2. The bug did not produce incorrect behavior until the third\niteration. It would be nice if bugs would manifest themselves the rst time they\nare encountered, but that is often not the case. Coincidences do occur, which adds\nto the challenges of debugging.\n6.2.2.4 Example 4: Finding the First 1 in a Word\nOur last example contains an error that is usually one of the hardest to nd, as we\nAddr\nx300\nx300\nx300\nx300\nx300\nx300\nx300\nx300\nx300\nx300\nFigure 6.11\nDebugging Example 4. An LC-3 program to nd the rst 1 in a word.\n",
    "page_number": 148,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "example, if the location examined contained 0010000110000000, the program\nwould terminate with R1 = 13. If the location contained 0000000000000110,\nthe program would terminate with R1 = 2.\nThe program Figure 6.11 is supposed to work as follows (and it usually does):\nx3000 and x3001 initialize R1 to 15, the bit number of the leftmost bit.\nx3002 loads R2 with the contents of x3400, the bit pattern to be exam-\nined. Since x3400 is too far from x3000 for a LD instruction, the load indirect\ninstruction is used, obtaining the location of the bit pattern in x3009.\nx3003 tests the most signicant bit of the bit pattern (bit [15]), and if it is\na 1, branches to x3008, where the program terminates with R1=15. If the most\nsignicant bit is 0, the branch is not taken, and processing continues at x3004.\nThe loop body, locations x3004 to x3007, does two things. First (x3004), it\nsubtracts 1 from R1, yielding the bit number of the next bit to the right. Second\n(x3005), it adds R2 to itself, resulting in the contents of R2 shifting left one bit,\nresulting in the next bit to the right being shifted into the bit [15] position. Third\n(x3006), the BR instruction tests the new bit [15], and if it is a 1, branches to\nx3008, where the program halts with R1 containing the actual bit number of the\ncurrent leftmost bit. If the new bit [15] is 0, x3007 is an unconditional branch to\nx3004 for the next iteration of the loop body.\nThe process continues until the rst 1 is found. The program works correctly\nalmost all the time. However, when we ran the program on our data, the program\nFigure 6.12\nDebugging Example 4. A Trace of Debugging Example 4 with a breakpoint\nat x3007.\n",
    "page_number": 149,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Each time the PC contained the address x3007, R1 contained a value smaller\nby 1 than the previous time. The reason is as follows: After R1 was decremented\nand the value in R2 shifted left, the bit tested was a 0, and so the program did not\nterminate. This continued for values in R1 equal to 14, 13, 12, 11, 10, 9, 8, 7, 6,\n5, 4, 3, 2, 1, 0, 1, 2, 3, 4, and so forth.\nThe problem was that the initial value in x3400 was x0000. The program\nworked ne as long as there was at least one 1 present. For the case where x3400\ncontained all zeros, the conditional branch at x3006 was never taken, and so the\nprogram continued with execution of x3007, then x3004, x3005, x3006, x3007,\nand then back again to x3004. There was no way to break out of the sequence\nx3004, x3005, x3006, x3007, and back again to x3004. We call the sequence\nx3004 to x3007 a loop. Because there is no way for the program execution to break\nout of this loop, we call it an innite loop. Thus, the program never terminates,\nand so we can never get the correct answer.\nAgain, we emphasize that this is often the hardest error to detect because it\nis as we said earlier a corner case. The programmer assumed that at least one bit\n.\n.\n.\neciently multiplies two integers and places the result in R3. Show the\n",
    "page_number": 150,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "embly Language\nB\ny now, you are probably a little tired of 1s and 0s and keeping track of 0001\nmeaning ADD and 1001 meaning NOT. Also, wouldnt it be nice if we\ncould refer to a memory location by some meaningful symbolic name instead of\nmemorizing its 16-bit address? And wouldnt it be nice if we could represent each\ninstruction in some more easily comprehensible way, instead of having to keep\ntrack of which bit of an instruction conveys which individual piece of information\nabout that instruction? It turns out that help is on the way.\nIn this chapter, we introduce assembly language, a mechanism that does all\nof the above, and more.\n7.1 Assembly Language\nProgramming\nMoving Up a Level\nRecall the levels of transformation identied in Figure 1.9 of Chapter 1. Algo-\nrithms are transformed into programs described in some mechanical language.\nThis mechanical language can be, as it is in Chapter 5, the machine language of a\nparticular computer. Recall that a program is in a computers machine language\nif every instruction in the program is from the ISA of that computer.\nOn the other hand, the mechanical language can be more user-friendly. We\ngenerally partition mechanical languages into two classes, high-level and low-\nlevel. Of the two, high-level languages are much more user-friendly. Examples\nare C, C++, Java, Fortran, COBOL, Python, plus more than a thousand others.\nInstructions in a high-level language almost (but not quite) resemble statements\nin a natural language such as English. High-level languages tend to be ISA inde-\npendent. That is, once you learn how to program in C (or Fortran or Python)\nfor one ISA, it is a small step to write programs in C (or Fortran or Python) for\nanother ISA.\n",
    "page_number": 151,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Before a program written in a high-level language can be executed, it must\nbe translated into a program in the ISA of the computer on which it is expected to\nexecute. It is often the case that each statement in the high-level language species\nseveral instructions in the ISA of the computer. In Chapter 11, we will introduce\nthe high-level language C, and in Chapters 12 through 19, we will show the rela-\ntionship between various statements in C and their corresponding translations to\nLC-3 code. In this chapter, however, we will only move up a small step from the\nISA we dealt with in Chapter 5.\nA small step up from the ISA of a machine is that ISAs assembly language.\nAssembly language is a low-level language. There is no confusing an instruc-\ntion in a low-level language with a statement in English. Each assembly language\ninstruction usually species a single instruction in the ISA. Unlike high-level lan-\nguages, which are usually ISA independent, low-level languages are very much\nISA dependent. In fact, it is usually the case that each ISA has only one assembly\nlanguage.\nThe purpose of assembly language is to make the programming process more\nuser-friendly than programming in machine language (i.e., in the ISA of the com-\nputer with which we are dealing), while still providing the programmer with\ndetailed control over the instructions that the computer can execute. So, for exam-\nple, while still retaining control over the detailed instructions the computer is to\ncarry out, we are freed from having to remember what opcode is 0001 and what\nopcode is 1001, or what is being stored in memory location 0011111100001010\nand what is being stored in location 0011111100000101. Assembly languages\nlet us use mnemonic devices for opcodes, such as ADD for 0001 and NOT for\n1001, and they let us give meaningful symbolic names to memory locations, such\nas SUM or PRODUCT, rather than use the memory locations 16-bit addresses.\nThis makes it easier to dierentiate which memory location is keeping track of a\nSUM and which memory location is keeping track of a PRODUCT. We call these\nnames symbolic addresses.\nWe will see, starting in Chapter 11, that when we take the larger step of\nmoving up to a higher-level language (such as C), programming will be even more\nuser-friendly, but in doing so, we will relinquish some control over exactly which\ndetailed ISA instructions are to be carried out to accomplish the work specied\nby a high-level language statement.\n7.2 An Assembly Language Program\nWe will begin our study of the LC-3 assembly language by means of an example.\nThe program in Figure 7.1 multiplies the integer initially stored in NUMBER\nby 6 by adding the integer to itself six times. For example, if the integer is 123,\nthe program computes the product by adding 123+123+123+123+123+123.\nWhere have you seen that before? :-)\nThe program consists of 21 lines of code. We have added a line number to\neach line of the program in order to be able to refer to individual lines easily.\nThis is a common practice. These line numbers are not part of the program. Ten\nlines start with a semicolon, designating that they are strictly for the benet of\n",
    "page_number": 152,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "15\n.END\nFigure 7.1\nAn assembly language program.\nthe human reader. More on this momentarily. Seven lines (06, 07, 08, 0C, 0D, 0E,\nand 10) specify assembly language instructions to be translated into machine lan-\nguage instructions of the LC-3, which will be executed when the program runs.\nThe remaining four lines (05, 12, 13, and 15) contain pseudo-ops, which are mes-\nsages from the programmer to the translation program to help in the translation\nprocess. The translation program is called an assembler (in this case the LC-3\nassembler), and the translation process is called assembly.\n7.2.1 Instructions\nInstead of an instruction being 16 0s and 1s, as is the case in the LC-3 ISA, an\ninstruction in assembly language consists of four parts, as follows:\nLabel\nOpcode\nOperands\n; Comment\nTwo of the parts (Label and Comment) are optional. More on that momentarily.\n7.2.1.1 Opcodes and Operands\nTwo of the parts (Opcode and Operands) are mandatory. For an assembly lan-\nguage instruction to correspond to an instruction in the LC-3 ISA, it must have\nan Opcode (the thing the instruction is to do), and the appropriate number of\nOperands (the things it is supposed to do it to). Not surprisingly, this was exactly\nwhat we encountered in Chapter 5 when we studied the LC-3 ISA.\nThe Opcode is a symbolic name for the opcode of the corresponding LC-3\ninstruction. The idea is that it is easier to remember an operation by the symbolic\n",
    "page_number": 153,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "name ADD, AND, or LDR than by the four-bit quantity 0001, 0101, or 0110.\nFigure 5.3 (also Figure A.2) lists the Opcodes of the 15 LC-3 instructions.\nPages 658 through 673 show the assembly language representations for the 15\nLC-3 instructions.\nThe number of operands depends on the operation being performed. For\nexample, the ADD instruction (line 0C in the program of Figure 7.1) requires\nthree operands (two sources to obtain the numbers to be added, and one desti-\nnation to designate where the result is to be stored). All three operands must be\nexplicitly identied in the instruction.\nAGAIN\nADD\nR3,R3,R2\nIn this case the operands to be added are obtained from register 2 and from register\n3. The result is to be placed in register 3. We represent each of the registers 0\nthrough 7 as R0, R1, R2,  , R7, rather than 000, 001, 010,  , 111.\nThe LD instruction (line 07 of the program in Figure 7.1) requires two\noperands (the memory location from which the value is to be read and the destina-\ntion register that is to contain the value after the instruction nishes execution). In\nLC-3 assembly language, we assign symbolic names called labels to the memory\nlocations so we will not have to remember their explicit 16-bit addresses. In this\ncase, the location from which the value is to be read is given the label NUMBER.\nThe destination (i.e., where the value is to be loaded) is register 2.\nLD\nR2, NUMBER\nAs we discussed in Section 5.1.6, operands can be obtained from registers, from\nmemory, or they may be literal (i.e., immediate) values in the instruction. In the\ncase of register operands, the registers are explicitly represented (such as R2 and\nR3 in line 0C). In the case of memory operands, the symbolic name of the mem-\nory location is explicitly represented (such as NUMBER in line 07 and SIX in line\n06). In the case of immediate operands, the actual value is explicitly represented\n(such as the value 0 in line 08).\nAND\nR3, R3, #0 ; Clear R3. It will contain the product.\nA literal value must contain a symbol identifying the representation base of the\nnumber. We use # for decimal, x for hexadecimal, and b for binary. Sometimes\nthere is no ambiguity, such as in the case 3F0A, which is a hex number. Nonethe-\nless, we write it as x3F0A. Sometimes there is ambiguity, such as in the case\n1000. x1000 represents the decimal number 4096, b1000 represents the decimal\nnumber 8, and #1000 represents the decimal number 1000.\n7.2.1.2 Labels\nLabels are symbolic names that are used to identify memory locations that are\nreferred to explicitly in the program. In LC-3 assembly language, a label consists\nof from 1 to 20 alphanumeric characters (i.e., each character is a capital or lower-\ncase letter of the English alphabet, or a decimal digit), starting with a letter of the\nalphabet.\nHowever, not all sequences of characters that follow these rules can be used\nas labels. You know that computer programs cannot tolerate ambiguity. So ADD,\n",
    "page_number": 154,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "NOT, x1000, R4, and other character strings that have specic meanings in an\nLC-3 program cannot be used as labels. They could confuse the LC-3 assembler\nas it tries to translate the LC-3 assembly language program into a program in the\nLC-3 ISA. Such not-allowed character strings are often referred to as reserved\nwords.\nNOW, Under21, R2D2, R785, and C3PO are all examples of legitimate LC-3\nassembly language labels.\nWe said we give a label (i.e., a symbolic name) to a memory location if we\nexplicitly refer to it in the program. There are two reasons for explicitly referring\nto a memory location.\n1. The location is the target of a branch instruction (e.g., AGAIN in line 0C).\nThat is, the label AGAIN identies the location of the instruction that will\nbe executed next if the branch is taken.\n2. The location contains a value that is loaded or stored (e.g., NUMBER in\nline 12, and SIX in line 13).\nNote the location AGAIN (identied in line 0C) is specically referenced by\nthe branch instruction in line 0E.\nBRp\nAGAIN\nIf the result of ADD R1,R1,#1 is positive (which results in the P bit being\nset), then the program branches to the location explicitly referenced as AGAIN\nto perform another iteration.\nThe location NUMBER is specically referenced by the load instruction\nin line 07. The value stored in the memory location explicitly referenced as\nNUMBER is loaded into R2.\nIf a location in the program is not explicitly referenced, then there is no need\nto give it a label.\n7.2.1.3 Comments\nComments are messages intended only for human consumption. They have no\neect on the translation process and indeed are not acted on by the LC-3 assem-\nbler. They are identied in the program by semicolons. A semicolon signies\nthat the rest of the line is a comment and is to be ignored by the assembler. If the\nsemicolon is the rst nonblank character on the line, the entire line is ignored. If\nthe semicolon follows the operands of an instruction, then only the comment is\nignored by the assembler.\nThe purpose of comments is to make the program more comprehensible to\nthe human reader. Comments help explain a nonintuitive aspect of an instruction\nor a set of instructions. In lines 08 and 09, the comment Clear R3; it will contain\nthe product lets the reader know that the instruction on line 08 is initializing\nR3 prior to accumulating the product of the two numbers. While the purpose of\nline 08 may be obvious to the programmer today, it may not be the case two years\nfrom now, after the programmer has written an additional 30,000 instructions and\ncannot remember why he/she wrote AND R3,R3,#0. It may also be the case that\ntwo years from now, the programmer no longer works for the company, and the\n",
    "page_number": 155,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Another purpose of comments is to make the visual presentation of a program\neasier to understand. That is, comments are used to separate pieces of a program\nfrom each other to make the program more readable. Lines of code that work\ntogether to compute a single result are placed on successive lines, but they are\nseparated from the rest of the program by blank lines. For example, note that\nlines 0C through 0E, which together form the loop body that is the crux of this\ncomputer program, are separated from the rest of the code by lines 0B and 0F.\nThere is nothing on lines 0B and 0F other than the semicolons in the rst column.\nIncidentally, another opportunity to make a program easier to read is the judi-\ncious use of white space, accomplished by adding extra spaces to a line that are\nignored by the assemblerfor example, having all the opcodes start in the same\ncolumn on the page, whether or not the instruction has a label.\n7.2.2 Pseudo-Ops (Assembler Directives)\nThe LC-3 assembler is a program that takes as input a string of characters repre-\nsenting a computer program written in LC-3 assembly language and translates it\ninto a program in the ISA of the LC-3. Pseudo-ops help the assembler perform\nthat task.\nThe more formal name for a pseudo-op is assembler directive. It is called a\npseudo-op because, like its Greek root pseudes (which means false), it does\nnot refer to an operation that will be performed by the program during execution.\nRather, the pseudo-op is strictly a message from the assembly language program-\nmer to the assembler to help the assembler in the assembly process. Once the\nassembler handles the message, the pseudo-op is discarded. The LC-3 assem-\nbly language contains ve pseudo-ops that we will nd useful in our assembly\nlanguage programming: .ORIG, .FILL, .BLKW, .STRINGZ, and .END. All are\neasily recognizable by the dot as their rst character.\n7.2.2.1 .ORIG\n.ORIG tells the assembler where in memory to place the LC-3 program. In line 05,\n.ORIG x3050 says, place the rst LC-3 ISA instruction in location x3050. As a\nresult, 0010001000001100 (the translated LD R1,SIX instruction) is put in loca-\ntion x3050, and the rest of the translated LC-3 program is placed in the subsequent\nsequential locations in memory. For example, if the program consists of x100 LC-\n3 instructions, and .ORIG says to put the rst instruction in x3050, the remaining\nxFF instructions are placed in locations x3051 to x314F.\n",
    "page_number": 156,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "7.2\nAn Assembly Language Program\n237\n7.2.2.2 .FILL\n.FILL tells the assembler to set aside the next location in the program and initial-\nize it with the value of the operand. The value can be either a number or a label.\nIn line 13, the ninth location in the resulting LC-3 program is initialized to the\nvalue x0006.\n7.2.2.3 .BLKW\n.BLKW tells the assembler to set aside some number of sequential memory loca-\ntions (i.e., a BLocK of Words) in the program. The actual number is the operand\nof the .BLKW pseudo-op. In line 12, the pseudo-op instructs the assembler to set\naside one location in memory (and, incidentally, to label it NUMBER).\nThe pseudo-op .BLKW is particularly useful when the actual value of the\noperand is not yet known. In our example we assumed the number in location\nNUMBER was 123. How did it get there? A common use of .BLKW is to set\naside a location in the program, as we did here, and have another section of code\nproduce the number, perhaps from input from a keyboard (which we cannot know\nat the time we write the program), and store that value into NUMBER before we\nexecute the code in Figure 7.1.\n7.2.2.4 .STRINGZ\n.STRINGZ tells the assembler to initialize a sequence of n+1 memory locations.\nThe argument is a sequence of n characters inside double quotation marks. The\nx\n: x\nx301C: x0021\nx301D: x0000\n",
    "page_number": 157,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "7.2.2.5 .END\n.END tells the assembler it has reached the end of the program and need not\neven look at anything after it. That is, any characters that come after .END will\nnot be processed by the assembler. Note: .END does not stop the program during\nexecution. In fact, .END does not even exist at the time of execution. It is simply a\ndelimiterit marks the end of the program. It is a message from the programmer,\ntelling the assembler where the assembly language program ends.\n7.2.3 Example: The Character Count Example of Section 5.5,\nRevisited Again!\nNow we are ready for a complete example. Lets consider again the problem of\nSection 5.5. We wish to write a program that will take a character that is input\nfrom the keyboard and count the number of occurrences of that character in a\nle. As before, we rst develop the algorithm by constructing the owchart.\nRecall that in Section 6.1, we showed how to decompose the problem system-\natically so as to generate the owchart of Figure 5.16. In fact, the nal step of\nthat process in Chapter 6 is the owchart of Figure 6.3e, which is essentially\nidentical to Figure 5.16. Next, we use the owchart to write the actual program.\nThis time, however, we enjoy the luxury of not worrying about 0s and 1s and\ninstead write the program in LC-3 assembly language. The program is shown in\nFigure 7.2.\nA few comments about this program: Three times during this program, assis-\ntance in the form of a service call is required of the operating system. In each case,\na TRAP instruction is used. TRAP x23 causes a character to be input from the\nkeyboard and placed in R0 (line 0D). TRAP x21 causes the ASCII code in R0\nto be displayed on the monitor (line 28). TRAP x25 causes the machine to be\nhalted (line 29). As we said before, we will leave the details of how the TRAP\ninstruction is carried out until Chapter 9.\nThe ASCII codes for the decimal digits 0 to 9 (0000 to 1001) are x30 to x39.\nThe conversion from binary to ASCII is done simply by adding x30 to the binary\nvalue of the decimal digit. Line 2D shows the label ASCII used to identify the\nmemory location containing x0030. The LD instruction in line 26 uses it to load\nx30 into R0, so it can convert the count that is in R2 from a binary value to an\nASCII code. That is done by the ADD instruction in line 27. TRAP x21 in line\n28 prints the ASCII code to the monitor.\nThe le that is to be examined starts at address x4000 (see line 2E). Usually,\nthis starting address would not be known to the programmer who is writing this\nprogram since we would want the program to work on many les, not just the one\nstarting at x4000. To accomplish that, line 2E would be replaced with .BLKW\n1 and be lled in by some other piece of code that knew the starting address of\nthe desired le before executing the program of Figure 7.2. That situation will be\ndiscussed in Section 7.4.\n",
    "page_number": 158,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "7.3 The Assembly Process\n7.3.1 Introduction\nBefore an LC-3 assembly language program can be executed, it must rst be\ntranslated into a machine language program, that is, one in which each instruction\nis in the LC-3 ISA. It is the job of the LC-3 assembler to perform that translation.\nIf you have available an LC-3 assembler, you can cause it to translate your\nassembly language program into a machine language program by executing an\nappropriate command. In the LC-3 assembler that is generally available via the\nweb, that command is assemble, and it requires as an argument the lename of\nyour assembly language program. For example, if the lename is solution1.asm,\nthen\nassemble solution1.asm outle\nproduces the le outle, which is in the ISA of the LC-3. It is necessary to check\nwith your instructor for the correct command line to cause the LC-3 assembler to\nproduce a le of 0s and 1s in the ISA of the LC-3.\n7.3.2 A Two-Pass Process\nIn this section, we will see how the assembler goes through the process of trans-\nlating an assembly language program into a machine language program. We will\nuse as our input to the process the assembly language program of Figure 7.2.\nYou remember that there is in general a one-to-one correspondence between\ninstructions in an assembly language program and instructions in the nal\nmachine language program. We could try to perform this translation in one pass\nthrough the assembly language program. Starting from the top of Figure 7.2, the\nassembler discards lines 01 to 09, since they contain only comments. Comments\nare strictly for human consumption; they have no bearing on the translation pro-\ncess. The assembler then moves on to line 0A. Line 0A is a pseudo-op; it tells\nthe assembler that the machine language program is to start at location x3000.\nThe assembler then moves on to line 0B, which it can easily translate into LC-3\nmachine code. At this point, we have\nx3000:\n0101010010100000\nThe LC-3 assembler moves on to translate the next instruction (line 0C). Unfor-\ntunately, it is unable to do so since it does not know the meaning of the symbolic\naddress PTR. At this point the assembler is stuck, and the assembly process fails.\nTo prevent this from occurring, the assembly process is done in two com-\nplete passes (from beginning to .END) through the entire assembly language\nprogram. The objective of the rst pass is to identify the actual binary addresses\ncorresponding to the symbolic names (or labels). This set of correspondences is\nknown as the symbol table. In pass 1, we construct the symbol table. In pass 2, we\ntranslate the individual assembly language instructions into their corresponding\nmachine language instructions.\nThus, when the assembler examines line 0C for the purpose of translating\nLD R3,PTR\n",
    "page_number": 159,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "during the second pass, it already knows that PTR is the symbolic address of\nmemory location x3013 (from the rst pass). Thus, it can easily translate line\n0C to\nx3001:\n0010011000010001\nThe problem of not knowing the 16-bit address corresponding to PTR no longer\nexists.\n7.3.3 The First Pass: Creating the Symbol Table\nFor our purposes, the symbol table is simply a correspondence of symbolic names\nwith their 16-bit memory addresses. We obtain these correspondences by pass-\ning through the assembly language program once, noting which instruction is\nassigned to which memory location, and identifying each label with the memory\naddress of its assigned entry.\nRecall that we provide labels in those cases where we have to refer to a loca-\ntion, either because it is the target of a branch instruction or because it contains\ndata that must be loaded or stored. Consequently, if we have not made any pro-\ngramming mistakes, and if we identify all the labels, we will have identied all\nthe symbolic addresses used in the program.\nThe preceding paragraph assumes that our entire program exists between our\n.ORIG and .END pseudo-ops. This is true for the assembly language program\nof Figure 7.2. In Section 7.4, we will consider programs that consist of multi-\nple parts, each with its own .ORIG and .END, wherein each part is assembled\nseparately.\nThe rst pass starts, after discarding the comments on lines 01 to 09, by\nnoting (line 0A) that the rst instruction will be assigned to address x3000. We\nkeep track of the location assigned to each instruction by means of a location\ncounter (LC). The LC is initialized to the address specied in .ORIG, that is,\nx3000.\nThe assembler examines each instruction in sequence and increments the LC\nonce for each assembly language instruction. If the instruction examined contains\na label, a symbol table entry is made for that label, specifying the current con-\ntents of LC as its address. The rst pass terminates when the .END pseudo-op is\nreached.\nThe rst instruction that has a label is at line 13. Since it is the fth instruction\nin the program and since the LC at that point contains x3004, a symbol table entry\nis constructed thus:\nSymbol\nAddress\nTEST\nx3004\nThe second instruction that has a label is at line 20. At this point, the LC has been\nincremented to x300B. Thus, a symbol table entry is constructed, as follows:\nSymbol\nAddress\nGETCHAR\nx300B\n",
    "page_number": 160,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "At the conclusion of the rst pass, the symbol table has the following entries:\nSymbol\nAddress\nTEST\nx3004\nGETCHAR\nx300B\nOUTPUT\nx300E\nASCII\nx3012\nPTR\nx3013\n7.3.4 The Second Pass: Generating the\nMachine Language Program\nThe second pass consists of going through the assembly language program a sec-\nond time, line by line, this time with the help of the symbol table. At each line,\nthe assembly language instruction is translated into an LC-3 machine language\ninstruction.\nStarting again at the top, the assembler again discards lines 01 through 09\nbecause they contain only comments. Line 0A is the .ORIG pseudo-op, which\nthe assembler uses to initialize LC to x3000. The assembler moves on to line 0B\nand produces the machine language instruction 0101010010100000. Then the\nassembler moves on to line 0C.\nThis time, when the assembler gets to line 0C, it can completely assemble\nthe instruction since it knows that PTR corresponds to x3013. The instruction is\nLD, which has an opcode encoding of 0010. The destination register (DR) is R3,\nthat is, 011.\nThe only part of the LD instruction left to do is the PCoset. It is computed as\nfollows: The assembler knows that PTR is the label for address x3013 and that the\nincremented PC is LC+1, in this case x3002. Since PTR (x3013) must be the sum\nof the incremented PC (x3002) and the sign-extended PCoset, PCoset must be\nx0011. Putting this all together, the assembler sets x3001 to 0010011000010001\nand increments the LC to x3002.\nNote: In order to use the LD instruction, it is necessary that the source of\nthe load, in this case the address whose label is PTR, is not more than +256 or\n255 memory locations from the LD instruction itself. If the address of PTR\nhad been greater than LC+1+255 or less than LC+1256, then the oset would\nnot t in bits [8:0] of the instruction. In such a case, an assembly error would\nhave occurred, preventing the assembly process from nishing successfully. For-\ntunately, PTR is close enough to the LD instruction, so the instruction assembled\ncorrectly.\nThe second pass continues. At each step, the LC is incremented and the\nlocation specied by LC is assigned the translated LC-3 instruction or, in the\ncase of .FILL, the value specied. When the second pass encounters the .END\npseudo-op, assembly terminates.\nThe resulting translated program is shown in Figure 7.3.\nThat process was, on a good day, merely tedious. Fortunately, you do not have\nto do it for a livingthe LC-3 assembler does that. And, since you now know the\n",
    "page_number": 161,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure 7.3\nThe machine language program for the assembly language program of\nFigure 7.2.\nLC-3 assembly language, there is no need to program in machine language. Now\nwe can write our programs symbolically in LC-3 assembly language and invoke\nthe LC-3 assembler to create the machine language versions that can execute on\nan LC-3 computer.\n7.4 Beyond the Assembly of a Single\nAssembly Language Program\nOur purpose in this chapter has been to take you up one more step from the ISA\nof the computer and introduce assembly language. Although it is still quite a\nlarge step from C or C++, assembly language does, in fact, save us a good deal\nof pain. We have also shown how a rudimentary two-pass assembler actually\nworks to translate an assembly language program into the machine language of\nthe LC-3 ISA.\nThere are many more aspects to sophisticated assembly language program-\nming that go well beyond an introductory course. However, our reason for\nteaching assembly language is not to deal with its sophistication, but rather to\nshow its innate simplicity. Before we leave this chapter, however, there are a few\nadditional highlights we should explore.\n",
    "page_number": 162,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "7.4.1 The Executable Image\nWhen a computer begins execution of a program, the entity being executed is\ncalled an executable image. The executable image is created from modules often\ncreated independently by several dierent programmers. Each module is trans-\nlated separately into an object le. We have just gone through the process of\nperforming that translation ourselves by mimicking the LC-3 assembler. Other\nmodules, some written in C perhaps, are translated by the C compiler. Some mod-\nules are written by users, and some modules are supplied as library routines by\nthe operating system. Each object le consists of instructions in the ISA of the\ncomputer being used, along with its associated data. The nal step is to combine\n(i.e., link) all the object modules together into one executable image. During exe-\ncution of the program, the FETCH, DECODE,  instruction cycle is applied to\ninstructions in the executable image.\n7.4.2 More than One Object File\nIt is very common to form an executable image from more than one object le.\nIn fact, in the real world, where most programs invoke libraries provided by the\noperating system as well as modules generated by other programmers, it is much\nmore common to have multiple object les than a single one.\nA case in point is our example character count program. The program counts\nthe number of occurrences of a character in a le. A typical application could\neasily have the program as one module and the input data le as another. If this\nwere the case, then the starting address of the le, shown as x4000 in line 2E of\nFigure 7.2, would not be known when the program was written. If we replace line\n2E with\nPTR\n.FILL\nSTARTofFILE\nthen the program of Figure 7.2 will not assemble because there will be no symbol\ntable entry for STARTofFILE. What can we do?\nIf the LC-3 assembly language, on the other hand, contained the pseudo-op\n.EXTERNAL, we could identify STARTofFILE as the symbolic name of an\naddress that is not known at the time the program of Figure 7.2 is assembled.\nThis would be done by the following line\n.EXTERNAL\nSTARTofFILE,\nwhich would send a message to the LC-3 assembler that the absence of label\nSTARTofFILE is not an error in the program. Rather, STARTofFILE is a label\nin some other module that will be translated independently. In fact, in our case,\nit will be the label of the location of the rst character in the le to be examined\nby our character count program.\nIf the LC-3 assembly language had the pseudo-op .EXTERNAL, and if we\nhad designated STARTofFILE as .EXTERNAL, the LC-3 assembler would be\nable to create a symbol table entry for STARTofFILE, and instead of assigning\nit an address, it would mark the symbol as belonging to another module. At link\n",
    "page_number": 163,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "U\np to now we have completely ignored the details of input and output, that is,\nhow the computer actually gets information from the keyboard (input), and\nhow the computer actually delivers information to the monitor (output). Instead\nwe have relied on the TRAP instruction (e.g., TRAP x23 for input and TRAP x21\nfor output) to accomplish these tasks. The TRAP instruction enables us to tell the\noperating system what we need done by means of a trap vector, and we trust the\noperating system to do it for us.\nThe more generic term for our TRAP instruction is system call because the\nTRAP instruction is calling on the operating system to do something for us while\nallowing us to remain completely clueless as to how it gets done. Now we are\nready to examine how input and output actually work in the LC-3, what happens\nwhen the user program makes a system call by invoking the TRAP instruction,\nand how it all works under the control of the operating system.\nWe will start with the actual physical structures that are required to cause\ninput and output to occur. But before we do that, it is useful to say a few words\nabout the operating system and understand a few basic concepts that have not been\nimportant so far but become very important when considering what the operating\nsystem needs to do its job.\nYou may be familiar with Microsofts various avors of Windows, Apples\nMacOS, and Linux. These are all examples of operating systems. They all have\nthe same goal: to optimize the use of all the resources of the computer system\nwhile making sure that no software does harmful things to any program or data\nthat it has no right to mess with. To better understand their job, we need to under-\nstand the notions of privilege and priority and the layout of the memory address\nspace (i.e., the regions of memory and the purpose of each).\n",
    "page_number": 164,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "9.1 Privilege, Priority, and the\nMemory Address Space\n9.1.1 Privilege and Priority\nTwo very dierent (we often say orthogonal) concepts associated with computer\nprocessing are privilege and priority.\n9.1.1.1 Privilege\nPrivilege is all about the right to do something, such as execute a particular\ninstruction or access a particular memory location. Not all computer programs\nhave the right to execute all instructions. For example, if a computer system is\nshared among many users and the ISA contains a HALT instruction, we would\nnot want any random program to execute that HALT instruction and stop the\ncomputer. If we did, we would have some pretty disgruntled users on our hands.\nSimilarly, some memory locations are only available to the operating system. We\nwould not want some random program to interfere with the data structures or\ncode that is part of the operating system, which would in all likelihood cause the\nentire system to crash. In order to make sure neither of these two things happens, we\ndesignate every computer program as either privileged or unprivileged. We often\nsay supervisor privilege to indicate privileged. We say a program is executing in\nSupervisor mode to indicate privileged, or User mode to indicate unprivileged. If a\nprogram is executing in Supervisor mode, it can execute all instructions and access\nall of memory. If a program is executing in User mode, it cannot. If a program\nexecuting in User mode tries to execute an instruction or access a memory location\nthat requires being in Supervisor mode, the computer will not allow it.\n9.1.1.2 Priority\nPriority is all about the urgency of a program to execute. Every program is\nassigned a priority, specifying its urgency as compared to all other programs.\nThis allows programs of greater urgency to interrupt programs of lesser urgency.\nFor example, programs written by random users may be assigned a priority of 0.\nThe keyboard may be asigned a priority of 4, and the fact that the computer is\nplugged into a source of energy like a wall outlet may be assigned a priority of 6.\nIf that is the case, a random user program would be interrupted if someone sitting\nat a keyboard wanted to execute a program that caused data to be input into the\ncomputer. And that program would be interrupted if someone pulled the power\ncord out of the wall outlet, causing the computer to quickly lose its source of\nenergy. In such an event, we would want the computer to execute some operating\nsystem program that is provided specically to handle that situation.\n9.1.1.3 Two Orthogonal Notions\nWe said privilege and priority are two orthogonal notions, meaning they have\nnothing to do with each other. We humans sometimes have a problem with that\nas we think of re trucks that have the privilege of ignoring trac lights because\n",
    "page_number": 165,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "9.1\nPrivilege, Priority, and the Memory Address Space\n315\nthey must quickly reach the re. In our daily lives, we often are given privileges\nbecause of our greater sense of urgency. Not the case with computer systems.\nFor example, we can have a user program that is tied to a physics experiment\nthat needs to interrupt the computer at a specic instance of time to record infor-\nmation being generated by the physics experiment. If the user program does not\npre-empt the program running at that instant of time, the data generated by the\nexperiment may be lost. This is a user program, so it does not have supervisor\nprivilege. But it does have a greater urgency, so it does have a higher priority.\nAnother example: The system administrator wants to execute diagnostic pro-\ngrams that access all memory locations and execute all instructions as part of\nsome standard preventive maintenance. The diagnostic program needs supervi-\nsor privilege to execute all instructions and access all memory locations. But it\nhas no sense of urgency. Whether this happens at 1 a.m. or 2 a.m. is irrelevant,\ncompared to the urgency of other programs that need access to the computer\nsystem exactly when they need it. The diagnostic program has privilege but no\npriority.\nFinally, an example showing that even in human activity one can have priority\nbut not privilege. Our friend Bob works in the basement of one of those New\nYork City skyscrapers. He is about to go to the mens room when his manager\ntells him to take a message immediately to the vice president on the 88th oor,\nand bring back a response. So Bob delays his visit to the mens room and takes\nthe elevator to the 88th oor. The vice president keeps him waiting, causing Bob\nto be concerned he might have an accident. Finally, the vice president gives his\nresponse, and Bob pushes the button to summon the elevator to take him back to\nthe basement, in pain because he needs to go to the mens room. While waiting for\nthe elevator, another vice president appears, unlocks the executive mens room,\nand enters. Bob is in pain, but he cannot enter the executive mens room. Although\nhe certainly has the priority, he does not have the privilege!\n9.1.1.4 The Processor Status Register (PSR)\nEach program executing on the computer has associated with it two very impor-\ntant registers. The Program Counter (PC) you are very familiar with. The other\nregister, the Processor Status Register (PSR), is shown in Figure 9.1. It contains\nthe privilege and priority assigned to that program.\nBit [15] species the privilege, where PSR[15]=0 means supervisor privi-\nlege, and PSR[15]=1 means unprivileged. Bits [10:8] specify the priority level\n(PL) of the program. The highest priority level is 7 (PL7), the lowest is PL0.\nFigure 9.1\nProcessor status register (PSR).\n",
    "page_number": 166,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "9.1.2 Organization of Memory\nFigure 9.2 shows the layout of the LC-3 memory.\nYou know that the LC-3 has a 16-bit address space; ergo, memory locations\nfrom x0000 to xFFFF. Locations x0000 to x2FFF are privileged memory loca-\ntions. They contain the various data structures and code of the operating system.\nThey require supervisor privilege to access. They are referred to as system space.\nLocations x3000 to xFDFF are unprivileged memory locations. Supervisor\nprivilege is not required to access these memory locations. All user programs and\ndata use this region of memory. The region is often referred to as user space.\nAddresses xFE00 to xFFFF do not correspond to memory locations at all.\nThat is, the last address of a memory location is xFDFF. Addresses xFE00 to\nxFFFF are used to identify registers that take part in input and output functions\nand some special registers associated with the processor. For example, the PSR\nis assigned address xFFFC, and the processors Master Control Register (MCR)\nis assigned address xFFFE. The benet of assigning addresses from the memory\naddress space will be discussed in Section 9.2.1.2. The set of addresses from\nxFE00 to xFFFF is usually referred to as the I/O page since most of the addresses\nare used for identif in re isters that take art in in ut or out ut functions. Access\nFigure 9.2\nRegions of memory.\n",
    "page_number": 167,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "operating system and requires supervisor privilege to access. The user stack is\ncontrolled by the user program and does not require privilege to access.\nEach has a stack pointer, Supervisor Stack Pointer (SSP) and User Stack\nPointer (USP), to indicate the top of the stack. Since a program can only execute\nin Supervisor mode or User mode at any one time, only one of the two stacks is\nactive at any one time. Register 6 is generally used as the stack pointer (SP) for\nthe active stack. Two registers, Saved SSP and Saved USP, are provided to save\nthe SP not in use. When privilege changes, for example, from Supervisor mode to\nUser mode, the SP is stored in Saved SSP, and the SP is loaded from Saved USP.\n9.2 Input/Output\nInput and output devices (keyboards, monitors, disks, or kiosks at the shopping\nmall) all handle input or output data using registers that are tailored to the needs\nof each particular input or output device. Even the simplest I/O devices usually\nneed at least two registers: one to hold the data being transferred between the\ndevice and the computer, and one to indicate status information about the device.\nAn example of status information is whether the device is available or is it still\nbusy processing the most recent I/O task.\n9.2.1 Some Basic Characteristics of I/O\nAll I/O activity is controlled by instructions in the computers ISA. Does the ISA\nneed special instructions for dealing with I/O? Does the I/O device execute at the\nsame speed as the computer, and if not, what manages the dierence in speeds? Is\nthe transfer of information between the computer and the I/O device initiated by\na program executing in the computer, or is it initiated by the I/O device? Answers\nto these questions form some of the basic characteristics of I/O activity.\n9.2.1.1 Memory-Mapped I/O vs. Special I/O Instructions\nAn instruction that interacts with an input or output device register must identify\nthe particular input or output device register with which it is interacting. Two\nschemes have been used in the past. Some computers use special input and output\ninstructions. Most computers prefer to use the same data movement instructions\nthat are used to move data in and out of memory.\nThe very old PDP-8 (from Digital Equipment Corporation, more than 50\nyears ago1965) is an example of a computer that used special input and output\ninstructions. The 12-bit PDP-8 instruction contained a three-bit opcode. If the\nopcode was 110, an I/O instruction was indicated. The remaining nine bits of the\nPDP-8 instruction identied which I/O device register and what operation was to\nbe performed.\nMost computer designers prefer not to specify an additional set of instructions\nfor dealing with input and output. They use the same data movement instructions\nthat are used for loading and storing data between memory and the general pur-\npose registers. For example, a load instruction (LD, LDI, or LDR), in which the\n",
    "page_number": 168,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "source address is that of an input device register, is an input instruction. Similarly,\na store instruction (ST, STI, or STR) in which the destination address is that of\nan output device register is an output instruction.\nSince programmers use the same data movement instructions that are used\nfor memory, every input device register and every output device register must be\nuniquely identied in the same way that memory locations are uniquely identied.\nTherefore, each device register is assigned an address from the memory address\nspace of the ISA. That is, the I/O device registers are mapped to a set of addresses\nthat are allocated to I/O device registers rather than to memory locations. Hence\nthe name memory-mapped I/O.\nThe original PDP-11 ISA had a 16-bit address space. All addresses wherein\nbits [15:13] = 111 were allocated to I/O device registers. That is, of the 216\naddresses, only 57,344 corresponded to memory locations. The remaining 213\nwere memory-mapped I/O addresses.\nThe LC-3 uses memory-mapped I/O. As we discussed in Section 9.1.2,\naddresses x0000 to xFDFF refer to actual memory locations. Addresses xFE00 to\nxFFFF are reserved for input/output device registers. Table A.3 lists the memory-\nmapped addresses of the LC-3 device registers that have been assigned so far.\nFuture uses and future sales of LC-3 microprocessors may require the expansion\nof device register address assignments as new and exciting applications emerge!\n9.2.1.2 Asynchronous vs. Synchronous\nMost I/O is carried out at speeds very much slower than the speed of the processor.\nA typist, typing on a keyboard, loads an input device register with one ASCII\ncode every time he/she types a character. A computer can read the contents of\nthat device register every time it executes a load instruction, where the operand\naddress is the memory-mapped address of that input device register.\nMany of todays microprocessors execute instructions under the control of a\nclock that operates well in excess of 2 GHz. Even for a microprocessor operating\nat only 2 GHz, a clock cycle lasts only 0.5 nanoseconds. Suppose a processor\nexecuted one instruction at a time, and it took the processor ten clock cycles to\nexecute the instruction that reads the input device register and stores its contents.\nAt that rate, the processor could read the contents of the input device register once\nevery 5 nanoseconds. Unfortunately, people do not type fast enough to keep this\nprocessor busy full-time reading characters. Question: How fast would a person\nhave to type to supply input characters to the processor at the maximum rate the\nprocessor can receive them?\nWe could mitigate this speed disparity by designing hardware that would\naccept typed characters at some slower xed rate. For example, we could design\na piece of hardware that accepts one character every 200 million cycles. This\nwould require a typing speed of 100 words/minute, assuming words on average\nconsisted of ve letters, which is certainly doable. Unfortunately, it would also\nrequire that the typist work in lockstep with the computers clock. That is not\nacceptable since the typing speed (even of the same typist) varies from moment\nto moment.\nWhats the point? The point is that I/O devices usually operate at speeds\nvery dierent from that of a microprocessor, and not in lockstep. We call this\n",
    "page_number": 169,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "latter characteristic asynchronous. Most interaction between a processor and I/O\nis asynchronous. To control processing in an asynchronous world requires some\nprotocol or handshaking mechanism. So it is with our keyboard and monitor. In\nthe case of the keyboard, we will need a one-bit status register, called a ag, to\nindicate if someone has or has not typed a character. In the case of the monitor,\nwe will need a one-bit status register to indicate whether or not the most recent\ncharacter sent to the monitor has been displayed, and so the monitor can be given\nanother character to display.\nThese ags are the simplest form of synchronization. A single ag, called\nthe ready bit, is enough to synchronize the output of the typist who can type\ncharacters at the rate of 100 words/minute with the input to a processor that can\naccept these characters at the rate of 200 million characters/second. Each time\nthe typist types a character, the ready bit is set to 1. Each time the computer reads\na character, it clears the ready bit. By examining the ready bit before reading\na character, the computer can tell whether it has already read the last character\ntyped. If the ready bit is clear, no characters have been typed since the last time\nthe computer read a character, and so no additional read would take place. When\nthe computer detects that the ready bit is set, it could only have been caused by a\nnew character being typed, so the computer would know to again read a character.\nThe single ready bit provides enough handshaking to ensure that the asyn-\nchronous transfer of information between the typist and the microprocessor can\nbe carried out accurately.\nIf the typist could type at a constant speed, and we did have a piece of hard-\nware that would accept typed characters at precise intervals (e.g., one character\nevery 200 million cycles), then we would not need the ready bit. The computer\nwould simply know, after 200 million cycles of doing other stu, that the typist\nhad typed exactly one more character, and the computer would read that charac-\nter. In this hypothetical situation, the typist would be typing in lockstep with the\nprocessor, and no additional synchronization would be needed. We would say the\ncomputer and typist were operating synchronously. That is, the input activity was\nsynchronous.\n9.2.1.3 Interrupt-Driven vs. Polling\nThe processor, which is computing, and the typist, who is typing, are two separate\nentities. Each is doing its own thing. Still, they need to interact; that is, the data that\nis typed has to get into the computer. The issue of interrupt-driven vs. polling is\nthe issue of who controls the interaction. Does the processor do its own thing until\nbeing interrupted by an announcement from the keyboard, Hey, a key has been\nstruck. The ASCII code is in the input device register. You need to read it. This is\ncalled interrupt-driven I/O, where the keyboard controls the interaction. Or, does\nthe processor control the interaction, specifically by interrogating (usually, again\nand again) the ready bit until it (the processor) detects that the ready bit is set. At\nthat point, the processor knows it is time to read the device register. This second\ntype of interaction when the processor is in charge is called polling, since the ready\nbit is polled by the processor, asking if any key has been struck.\nSection 9.2.2.2 describes how polling works. Section9.4 explains interrupt-\ndriven I/O.\n",
    "page_number": 170,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "9.2.2 Input from the Keyboard\n9.2.2.1 Basic Input Registers (KBDR and KBSR)\nWe have already noted that in order to handle character input from the keyboard,\nwe need two things: a data register that contains the character to be input and\na synchronization mechanism to let the processor know that input has occurred.\nThe synchronization mechanism is contained in the status register associated with\nthe keyboard.\nThese two registers are called the keyboard data register (KBDR) and the\nKBSR\nFigure 9.3\nKeyboard device registers.\nEven though a character needs only 8 bits and the synchronization mecha-\nnism needs only 1 bit, it is easier to assign 16 bits (like all memory addresses\nin the LC-3) to each. In the case of KBDR, bits [7:0] are used for the data, and\nbits [15:8] contain x00. In the case of KBSR, bit [15] contains the synchroniza-\ntion mechanism, that is, the ready bit. Figure 9.3 shows the two device registers\nneeded by the keyboard.\n9.2.2.2 The Basic Input Service Routine\nKBSR[15] controls the synchronization of the slow keyboard and the fast pro-\ncessor. When a key on the keyboard is struck, the ASCII code for that key is\nloaded into KBDR[7:0], and the electronic circuits associated with the keyboard\nautomatically set KBSR[15] to 1. When the LC-3 reads KBDR, the electronic\ncircuits associated with the keyboard automatically clear KBSR[15], allowing\nanother key to be struck. If KBSR[15] = 1, the ASCII code corresponding to the\nlast key struck has not yet been read, and so the keyboard is disabled; that is, no\nkey can be struck until the last key is read.\nIf input/output is controlled by the processor (i.e., via polling), then a pro-\ngram can repeatedly test KBSR[15] until it notes that the bit is set. At that point,\nthe processor can load the ASCII code contained in KBDR into one of the LC-3\nregisters. Since the processor only loads the ASCII code if KBSR[15] is 1, there is\nno danger of reading a single typed character multiple times. Furthermore, since\nthe keyboard is disabled until the previous code is read, there is no danger of the\nprocessor missing characters that were typed. In this way, KBSR[15] provides\nthe mechanism to guarantee that each key typed will be loaded exactly once.\n",
    "page_number": 171,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": ",\n04\nBRnzp\nNEXT_TASK\n; Go to the next task\n05\nA\n.FILL\nxFE00\n; Address of KBSR\n06\nB\n.FILL\nxFE02\n; Address of KBDR\nAs long as KBSR[15] is 0, no key has been struck since the last time the processor\nread the data register. Lines 01 and 02 comprise a loop that tests bit [15] of KBSR.\nNote the use of the LDI instruction, which loads R1 with the contents of xFE00,\nthe memory-mapped address of KBSR. If the ready bit, bit [15], is clear, BRzp\nwill branch to START and another iteration of the loop. When someone strikes a\nkey, KBDR will be loaded with the ASCII code of that key, and the ready bit of\nKBSR will be set. This will cause the branch to fall through, and the instruction\nat line 03 will be executed. Again, note the use of the LDI instruction, which\nthis time loads R0 with the contents of xFE02, the memory-mapped address of\nKBDR. The input routine is now done, so the program branches unconditionally\nto its NEXT TASK.\nFigure 9.4\nMemory-mapped input.\n",
    "page_number": 172,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "carry out the EXECUTE phase of the load instructions. Essentially three steps\nare required:\n1. The MAR is loaded with the address of the memory location to be read.\n2. Memory is read, resulting in MDR being loaded with the contents at the\nspecied memory location.\n3. The destination register (DR) is loaded with the contents of MDR.\nIn the case of memory-mapped input, the same steps are carried out, except\ninstead of MAR being loaded with the address of a memory location, MAR is\nloaded with the address of a device register. Instead of the address control logic\nenabling memory to read, the address control logic selects the corresponding\ndevice register to provide input to the MDR.\n9.2.3 Output to the Monitor\n9.2.3.1 Basic Output Registers (DDR and DSR)\nOut ut works in a wa ver\nsimilar to in ut with DDR and DSR re lacin the\nDSR\nFigure 9.5\nMonitor device registers.\nAs is the case with input, even though an output character needs only 8 bits\nand the synchronization mechanism needs only one bit, it is easier to assign\n16 bits (like all memory addresses in the LC-3) to each output device register.\nIn the case of DDR, bits [7:0] are used for data, and bits [15:8] contain x00. In\nthe case of DSR, bit [15] contains the synchronization mechanism, that is, the\nready bit. Figure 9.5 shows the two device registers needed by the monitor.\n9.2.3.2 The Basic Output Service Routine\nDSR[15] controls the synchronization of the fast processor and the slow monitor\ndisplay. When the LC-3 transfers an ASCII code to DDR[7:0] for outputting, the\nelectronics of the monitor automatically clear DSR[15] as the processing of the\ncontents of DDR[7:0] begins. When the monitor nishes processing the character\non the screen, it (the monitor) automatically sets DSR[15]. This is a signal to\nthe processor that it (the processor) can transfer another ASCII code to DDR\nfor outputting. As long as DSR[15] is clear, the monitor is still processing the\nprevious character, so the monitor is disabled as far as additional output from the\nprocessor is concerned.\n",
    "page_number": 173,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "If input/output is controlled by the processor (i.e., via polling), a program can\n,\n04\nBRnzp\nNEXT_TASK\n05\nA\n.FILL\nxFE04\n; Address of DSR\n06\nB\n.FILL\nxFE06\n; Address of DDR\nLike the routine for KBDR and KBSR in Section 9.2.2.2, lines 01 and 02 repeat-\nedly poll DSR[15] to see if the monitor electronics is nished with the last\ncharacter shipped by the processor. Note the use of LDI and the indirect access\nto xFE04, the memory-mapped address of DSR. As long as DSR[15] is clear,\nthe monitor electronics is still processing this character, and BRzp branches to\nSTART for another iteration of the loop. When the monitor electronics nishes\nwith the last character shipped by the processor, it automatically sets DSR[15]\nto 1, which causes the branch to fall through and the instruction at line 03 to be\nexecuted. Note the use of the STI instruction, which stores R0 into xFE06, the\nmemory-mapped address of DDR. The write to DDR also clears DSR[15], dis-\nabling for the moment DDR from further output. The monitor electronics takes\nover and writes the character to the screen. Since the output routine is now done,\nthe program unconditionally branches (line 04) to its NEXT TASK.\n9.2.3.3 Implementation of Memory-Mapped Output\nFigure 9.6 shows the additional data path required to implement memory-mapped\noutput. As we discussed previously with respect to memory-mapped input,\nthe mechanisms for handling the device registers provide very little additional\ncomplexity to what already exists for handling memory accesses.\nIn Chapter 5, you became familiar with the process of carrying out the\nEXECUTE phase of the store instructions.\n1. The MAR is loaded with the address of the memory location to be written.\n2. The MDR is loaded with the data to be written to memory.\n3. Memory is written, resulting in the contents of MDR being stored in the\nspecied memory location.\nIn the case of memory-mapped output, the same steps are carried out, except\ninstead of MAR being loaded with the address of a memory location, MAR is\nloaded with the address of a device register. Instead of the address control logic\nenabling memory to write, the address control logic asserts the load enable signal\nof DDR.\nMemory-mapped output also requires the ability to read output device reg-\nisters. You saw in Section 9.2.3.2 that before the DDR could be loaded, the ready\n",
    "page_number": 174,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "324\nINMUX\nFigure 9.6\nMemory-mapped output.\nbit had to be in state 1, indicating that the previous character had already n-\nished being written to the screen. The LDI and BRzp instructions on lines 01\nand 02 perform that test. To do this, the LDI reads the output device register\nDSR, and BRzp tests bit [15]. If the MAR is loaded with xFE04 (the memory-\nmapped address of the DSR), the address control logic selects DSR as the input\nto the MDR, where it is subsequently loaded into R1, and the condition codes\nare set.\n9.2.3.4 Example: Keyboard Echo\nWhen we type at the keyboard, it is helpful to know exactly what characters we\n0B\nDDR\n.FILL\nxFE06\n; Address of DDR\n",
    "page_number": 175,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "9.2.4 A More Sophisticated Input Routine\nIn the example of Section 9.2.2.2, the input routine would be a part of a program\nbeing executed by the computer. Presumably, the program requires character input\nfrom the keyboard. But how does the person sitting at the keyboard know when to\ntype a character? Sitting there, the person may wonder whether or not the program\nis actually running, or if perhaps the computer is busy doing something else.\nTo let the person sitting at the keyboard know that the program is waiting for\ninput from the keyboard, the computer typically prints a message on the monitor.\nSuch a message is often referred to as a prompt. The symbols that are displayed\nby your operating system (e.g., % or C:) or by your editor (e.g., :) are examples\nof prompts.\nThe program fragment shown in Figure 9.7 obtains keyboard input via\npolling as we have shown in Section 9.2.2.2. It also includes a prompt to let the\nperson sitting at the keyboard know when it is time to type a key. Lets examine\nthis program fragment.\nYou are already familiar with lines 13 through 19 and lines 25 through 28,\nwhich correspond to the code in Section 9.2.3.4 for inputting a character via the\nkeyboard and echoing it on the monitor.\nYou are also familiar with the need to save and restore registers if those reg-\nisters are needed by instructions in the input routine. Lines 01 through 03 save\nR1, R2, and R3, lines 1D through 1F restore R1, R2, and R3, and lines 22 through\n24 set aside memory locations for those register values.\nThis leaves lines 05 through 08, 0A through 11, 1A through 1C, 29 and 2A.\nThese lines serve to alert the person sitting at the keyboard that it is time to type\na character.\nLines 05 through 08 write the ASCII code x0A to the monitor. This is the\nASCII code for a new line. Most ASCII codes correspond to characters that are\nvisible on the screen. A few, like x0A, are control characters. They cause an action\nto occur. Specically, the ASCII code x0A causes the cursor to move to the far\nleft of the next line on the screen. Thus, the name Newline. Before attempting\nto write x0A, however, as is always the case, DSR[15] is tested (line 6) to see\nif DDR can accept a character. If DSR[15] is clear, the monitor is busy, and the\nloop (lines 06 and 07) is repeated. When DSR[15] is 1, the conditional branch\n(line 7) is not taken, and (line 8) x0A is written to DDR for outputting.\nLines 0A through 11 cause the prompt Input a character> to be written\nto the screen. The prompt is specified by the .STRINGZ pseudo-op on line 2A\nand is stored in 19 memory locations18 ASCII codes, one per memory location,\ncorresponding to the 18 characters in the prompt, and the terminating sentinel\nx0000.\nLine 0C iteratively tests to see if the end of the string has been reached (by\ndetecting x0000), and if not, once DDR is free, line 0F writes the next character\nin the input prompt into DDR. When x0000 is detected, the entire input prompt\nhas been written to the screen, and the program branches to the code that handles\nthe actual keyboard input (starting at line 13).\nAfter the person at the keyboard types a character and it has been echoed\n(lines 13 to 19), the program writes one more new line (lines 1A through 1C)\nbefore branching to its NEXT TASK.\n",
    "page_number": 176,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "326\new\nne .\nx\n;\nco e\nor new\nne\n2A\nPrompt\n.STRINGZ ''Input a character>''\nFigure 9.7\nThe more sophisticated input routine.\n9.2.5 Implementation of Memory-Mapped I/O, Revisited\nWe showed in Figures 9.4 and 9.6 partial implementations of the data path to\nhandle (separately) memory-mapped input and memory-mapped output. We have\nalso learned that in order to support interrupt-driven I/O, the two status registers\nmust be writeable as well as readable.\n",
    "page_number": 177,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "INMUX\nFigure 9.8\nRelevant data path implementation of memory-mapped I/O.\nFigure 9.8 (also shown as Figure C.3 of Appendix C) shows the data path\nnecessary to support the full range of features we have discussed for the I/O device\nregisters. The Address Control Logic Block controls the input or output operation.\nNote that there are three inputs to this block. MIO.EN indicates whether a data\nmovement from/to memory or I/O is to take place this clock cycle. MAR contains\nthe address of the memory location or the memory-mapped address of an I/O device\nregister. R.W indicates whether a load or a store is to take place. Depending on the\nvalues of these three inputs, the address control logic does nothing (MIO.EN = 0),\nor it provides the control signals to direct the transfer of data between the MDR\nand the memory or between the MDR and one of the I/O registers.\nIf R.W indicates a load, the transfer is from memory or I/O device to the\nMDR. The Address Control Logic Block provides the select lines to INMUX to\nsource the appropriate I/O device register or memory (depending on MAR) and\nalso enables the memory if MAR contains the address of a memory location.\nIf R.W indicates a store, the contents of the MDR is written either to memory\nor to one of the device registers. The address control logic either enables a write\nto memory or asserts the load enable line of the device register specied by the\ncontents of the MAR.\n9.3 Operating System Service\nRoutines (LC-3 Trap Routines)\n9.3.1 Introduction\nRecall Figure 9.7 of the previous section. In order for the program to successfully\nobtain input from the keyboard, it was necessary for the programmer to know\nseveral things:\n1. The hardware data registers for both the monitor and the keyboard: the\nmonitor so a prompt could be displayed, and the keyboard so the program\nwould know where to get the input character.\n",
    "page_number": 178,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "2. The hardware status registers for both the monitor and the keyboard: the\nmonitor so the program would know when it was OK to display the next\ncharacter in the input prompt, and the keyboard so the program would know\nwhen someone had struck a key.\n3. The asynchronous nature of keyboard input relative to the executing program.\nThis is beyond the knowledge of most application programmers. In fact, in the\nreal world, if application programmers (or user programmers, as they are some-\ntimes called) had to understand I/O at this level, there would be much less I/O\nand far fewer programmers in the business.\nThere is another problem with allowing user programs to perform I/O activity\nby directly accessing KBDR and KBSR. I/O activity involves the use of device\nregisters that are shared by many programs. This means that if a user programmer\nwere allowed to access the hardware registers, and he/she messed up, it could\ncreate havoc for other user programs. Thus, in general it is ill-advised to give\nuser programmers access to these registers. That is why the addresses of hardware\nregisters are part of the privileged memory address space and accessible only to\nprograms that have supervisor privilege.\nThe simpler solution, as well as the safer solution to the problem of user\nprograms requiring I/O, involves the TRAP instruction and the operating system,\nwhich of course has supervisor privilege.\nWe were rst introduced to the TRAP instruction in Chapter 4 as a way to\nget the operating system to halt the computer. In Chapter 5 we saw that a user\nprogram could use the TRAP instruction to get the operating system to do I/O\ntasks for it (the user program). In fact a great benet of the TRAP instruction,\nwhich we have already pointed out, is that it allows the user programmer to not\nhave to know the gory details of I/O discussed earlier in this chapter. In addition,\nFigure 9.9\nInvoking an OS service routine using the TRAP instruction.\n",
    "page_number": 179,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "operating system to perform the task on behalf of the user program. The operating\nsystem takes control of the computer, handles the request specied by the TRAP\ninstruction, and then returns control back to the user program at location x4001.\nAs we said at the start of this chapter, we usually refer to the request made by the\nuser program as a system call or a service call.\n9.3.2 The Trap Mechanism\nThe trap mechanism involves several elements:\n1. A set of service routines executed on behalf of user programs by the\noperating system. These are part of the operating system and start at\narbitrary addresses in system space. The LC-3 was designed so that up to\n256 service routines can be specied. Table A.2 in Appendix A contains the\nLC-3s current complete list of operating system service routines.\n2. A table of the starting addresses of these 256 service routines. This table\nis stored in memory locations x0000 to x00FF. The table is referred to by\nvarious names by various companies. One company calls this table the\nSystem Control Block. Another company calls it the Trap Vector Table.\nFigure 9.10 shows the Trap Vector Table of the LC-3, with specic starting\naddresses highlighted. Among the starting addresses are the one for the\ncharacter output service routine (memory location x0420), which is stored\nFigure 9.10\nThe Trap Vector Table.\n",
    "page_number": 180,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "3. The TRAP instruction. When a user program wishes to have the operating\nsystem execute a specic service routine on behalf of the user program, and\nthen return control to the user program, the user program uses the TRAP\ninstruction (as we have been doing since Chapter 4).\n4. A linkage back to the user program. The service routine must have a\nmechanism for returning control to the user program.\n9.3.3 The TRAP Instruction\nThe TRAP instruction causes the service routine to execute by (1) changing the\nPC to the starting address of the relevant service routine on the basis of its trap\nvector, and (2) providing a way to get back to the program that executed the TRAP\ninstruction. The way back is referred to as a linkage.\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\nTRAP\ntrap vector\nThe EXECUTE phase of the TRAP instructions instruction cycle does\nthree things:\n1. The PSR and PC are both pushed onto the system stack. Since the PC was\nincremented during the FETCH phase of the TRAP instructions instruction\ncycle, the return linkage is automatically saved in the PC. When control\nreturns to the user program, the PC will automatically be pointing to the\ninstruction following the TRAP instruction.\nNote that the program requesting the trap service routine can be running\neither in Supervisor mode or in User mode. If in User mode, R6, the stack\npointer, is pointing to the user stack. Before the PSR and PC can be\npushed onto the system stack, the current contents of R6 must be stored\nin Saved USP, and the contents of Saved SSP loaded into R6.\n2. PSR[15] is set to 0, since the service routine is going to require supervisor\nprivilege to execute. PSR[10:8] are left unchanged since the priority of the\nTRAP routine is the same as the priority of the program that requested it.\n3. The 8-bit trap vector is zero-extended to 16 bits to form an address that\ncorresponds to a location in the Trap Vector Table. For the trap vector x23,\nthat address is x0023. Memory location x0023 contains x04A0, the starting\naddress of the TRAP x23 service routine. The PC is loaded with x04A0,\ncompleting the instruction cycle.\nSince the PC contains x04A0, processing continues at memory address\nx04A0.\n",
    "page_number": 181,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Location x04A0 is the starting address of the operating system service rou-\ntine to input a character from the keyboard. We say the trap vector points to\nthe starting address of the TRAP routine. Thus, TRAP x23 causes the operating\nsystem to start executing the keyboard input service routine.\n9.3.4 The RTI Instruction: To Return Control\nto the Calling Program\n15\n14\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nRTI\nThe RTI instruction (opcode = 1000, with no operands) pops the top two\nvalues on the system stack into the PC and PSR. Since the PC contains the address\nfollowing the address of the TRAP instruction, control returns to the user program\nat the correct address.\nFinally, once the PSR has been popped o the system stack, PSR[15] must\nbe examined to see whether the processor was running in User mode or Super-\nvisor mode when the TRAP instruction was executed. If in User mode, the stack\npointers need to be adjusted to reect that now back in User mode, the relevant\nstack in use is the user stack. This is done by loading the Saved SSP with the\ncurrent contents of R6, and loading R6 with the contents of Saved USP.\n9.3.5 A Summary of the Trap Service Routine Process\nFigure 9.11 shows the LC-3 using the TRAP instruction and the RTI instruction to\nimplement the example of Figure 9.9. The ow of control goes from (A) within a\nuser program that needs a character input from the keyboard, to (B) the operating\nsystem service routine that performs that task on behalf of the user program,\nback to the user program (C) that presumably uses the information contained in\nthe input character.\nAs we know, the computer continually executes its instruction cycle (FETCH,\nDECODE, etc.) on sequentially located instructions until the flow of control is\nchanged by changing the contents of the PC during the EXECUTE phase of the\ncurrent instruction. In that way, the next FETCH will be at a redirected address.\nThe TRAP instruction with trap vector x23 in our user program does exactly\nthat. Execution of TRAP x23 causes the PSR and incremented PC to be pushed\nonto the system stack and the contents of memory location x0023 (which, in this\ncase, contains x04A0) to be loaded into the PC. The dashed line on Figure 9.11\nshows the use of the trap vector x23 to obtain the starting address of the trap\nservice routine from the Trap Vector Table.\nThe next instruction cycle starts with the FETCH of the contents of x04A0,\nwhich is the rst instruction of the relevant operating system service routine.\nThe trap service routine executes to completion, ending with the RTI instruction,\n",
    "page_number": 182,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "332\nFigure 9.11\nFlow of control from a user program to an OS service routine and back.\nwhich loads the PC and PSR with the top two elements on the system stack, that is,\nthe PSR and incremented PC that were pushed during execution of the TRAP\ninstruction. Since the PC was incremented prior to being pushed onto the system\nstack, it contains the address of the instruction following the TRAP instruction\nin the calling program, and the user program resumes execution by fetching the\ninstruction following the TRAP instruction.\nThe following program is provided to illustrate the use of the TRAP instruc-\ntion. It can also be used to amuse the average four-year-old!\nExample 9.1\nWrite a game program to do the following: A person is sitting at a keyboard. Each\ntime the person types a capital letter, the program outputs the lowercase version of\nthat letter. If the person types a 7, the program terminates.\nThe following LC-3 assembly language program will do the job.\n01\n.ORIG x3000\n02\nLD\nR2,TERM\n; Load -7\n03\nLD\nR3,ASCII ; Load ASCII difference\n04\nAGAIN\nTRAP\nx23\n; Request keyboard input\n05\nADD\nR1,R2,R0 ; Test for terminating\n06\nBRz\nEXIT\n; character\n07\nADD\nR0,R0,R3 ; Change to lowercase\n08\nTRAP\nx21\n; Output to the monitor\n09\nBRnzp AGAIN\n; ... and do it again!\n0A\nTERM\n.FILL xFFC9\n; FFC9 is negative of ASCII 7\n0B\nASCII\n.FILL x0020\n0C\nEXIT\nTRAP\nx25\n; Halt\n0D\n.END\n",
    "page_number": 183,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "The program executes as follows: The program rst loads constants xFFC9 and\nx0020 into R2 and R3. The constant xFFC9, which is the negative of the ASCII code\nfor 7, is used to test the character typed at the keyboard to see if the four-year-old wants\nto continue playing. The constant x0020 is the zero-extended dierence between the\nASCII code for a capital letter and the ASCII code for that same letters lowercase\nrepresentation. For example, the ASCII code for A is x41; the ASCII code for a is\nx61. The ASCII codes for Z and z are x5A and x7A, respectively.\nThen TRAP x23 is executed, which invokes the keyboard input service routine.\nWhen the service routine is nished, control returns to the application program (at\nline 05), and R0 contains the ASCII code of the character typed. The ADD and\nBRz instructions test for the terminating character 7. If the character typed is not a\n7, the ASCII uppercase/lowercase dierence (x0020) is added to the input ASCII\ncode, storing the result in R0. Then a TRAP to the monitor output service routine is\ncalled. This causes the lowercase representation of the same letter to be displayed on\nthe monitor. When control returns to the application program (this time at line 09),\nan unconditional BR to AGAIN is executed, and another request for keyboard input\nappears.\nThe correct operation of the program in this example assumes that the per-\nson sitting at the keyboard only types capital letters and the value 7. What if the\nperson types a $? A better solution to Example 9.1 would be a program that tests\nthe character typed to be sure it really is a capital letter from among the 26 cap-\nital letters in the alphabet or the single digit 7, and if it is not, takes corrective\naction.\nQuestion: Augment this program to add the test for bad data. That is, write a\nprogram that will type the lowercase representation of any capital letter typed and\nwill terminate if anything other than a capital letter is typed. See Exercise 9.20.\n9.3.6 Trap Routines for Handling I/O\nWith the constructs just provided, the input routine described in Figure 9.7 can\nbe slightly modied to be the input service routine shown in Figure 9.12. Two\nchanges are needed: (1) We add the appropriate .ORIG and .END pseudo-ops.\n.ORIG species the starting address of the input service routinethe address\nfound at location x0023 in the Trap Vector Table. And (2) we terminate the\ninput service routine with the RTI instruction rather than the BR NEXT TASK,\nas is done on line 20 in Figure 9.7. We use RTI because the service routine is\ninvoked by TRAP x23. It is not part of the user program, as was the case in\nFigure 9.7.\nThe output routine of Section 9.2.3.2 can be modied in a similar way, as\nshown in Figure 9.13. The results are input (Figure 9.12) and output (Figure 9.13)\nservice routines that can be invoked simply and safely by the TRAP instruction\nwith the appropriate trap vector. In the case of input, upon completion of TRAP\nx23, R0 contains the ASCII code of the keyboard character typed. In the case of\noutput, the initiating program must load R0 with the ASCII code of the character\nit wishes displayed on the monitor and then invoke TRAP x21.\n",
    "page_number": 184,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "ave\n.\n0F\n.END\nFigure 9.13\nCharacter output service routine.\n9.3.7 A Trap Routine for Halting the Computer\nRecall from Section 4.5 that the RUN latch is ANDed with the crystal oscillator\nto produce the clock that controls the operation of the computer. We noted that if\nthat one-bit latch was cleared, the output of the AND gate would be 0, stopping\nthe clock.\nYears ago, most ISAs had a HALT instruction for stopping the clock. Given\nhow infrequently that instruction is executed, it seems wasteful to devote an\no code to it. In man modern com uters, the RUN latch is cleared b a TRAP\n,\nFigure 9.14\nHALT service routine for the LC-3 (Fig. 9.14 continued on next page.)\n",
    "page_number": 185,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "336\n.\nFigure 9.14\nHALT service routine for the LC-3 (continued Fig. 9.14 from\nprevious page.)\nrst (lines 02 and 03), registers R1 and R0 are saved. R1 and R0 are saved\nse they are needed by the service routine. Then (lines 07 through 0C),\nnner Halting the machine is displayed on the monitor. Finally (lines 10\nh 13), the RUN latch (MCR[15]) is cleared by ANDing the MCR with\n0111111111111111. That is, MCR[14:0] remains unchanged, but MCR[15] is\ncleared. Question: What instruction (or trap service routine) can be used to start\nthe clock? Hint: This is a trick question! :-)\n9.3.8 The Trap Routine for Character Input (One Last Time)\nLets look again at the keyboard input service routine of Figure 9.12. In particular,\nlets look at the three-line sequence that occurs at symbolic addresses L1, L2, L3,\nand L4:\nLABEL\nLDI\nR3,DSR\nBRzp\nLABEL\nSTI\nReg,DDR\nCan the JSR/RET mechanism enable us to replace these four occurrences of the\nsame sequence with a single subroutine? Answer: Yes, almost.\nFigure 9.15, our improved keyboard input service routine, contains\nJSR\nWriteChar\nat lines 04, 0A, 10, and 13, and the four-instruction subroutine\nWriteChar\nLDI\nR3,DSR\nBRzp\nWriteChar\nSTI\nR2,DDR\nRET\nat lines 1A through 1D. Note the RET instruction (a.k.a. JMP R7) that is needed\nto terminate the subroutine.\n",
    "page_number": 186,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Note the hedging: almost. In the original sequences starting at L2 and L3,\nthe STI instruction forwards the contents of R0 (not R2) to the DDR. We can x\nthat easily enough, as follows: In line 08 of Figure 9.15, we use\nLDR\nR2,R1,#0\ninstead of\nLDR\nR0,R1,#0\nThis causes each character in the prompt to be loaded into R2. The subroutine\nWritechar forwards each character from R2 to the DDR.\nIn line 0F of Figure 9.15, we insert the instruction\nADD\nR2,R0,#0\nin order to move the keyboard input (which is in R0) into R2. The subroutine\nWritechar forwards it from R2 to the DDR. Note that R0 still contains the key-\nboard input. Furthermore, since no subsequent instruction in the service routine\nloads R0, R0 still contains the keyboard input after control returns to the user\nprogram.\nIn line 12 of Figure 9.15, we insert the instruction\nLD\nR2,Newline\nin order to move the newline character into R2. The subroutine Writechar\nforwards it from R2 to the DDR.\nFigure 9.15 is the actual LC-3 trap service routine provided for keyboard\ninput.\n9.3.9 PUTS: Writing a Character String to the Monitor\nBefore we leave the example of Figure 9.15, note the code on lines 08 through 0C.\nThis fragment of the service routine is used to write the sequence of characters\nInput a character to the monitor. A sequence of characters is often referred to\nas a string of characters or a character string. This fragment is also present in\nFigure 9.14, with the result that Halting the machine is written to the monitor. In\nfact, it is so often the case that a user program needs to write a string of characters\nto the monitor that this function is given its own trap service routine in the LC-3\noperating system. Thus, if a user program requires a character string to be written\nto the monitor, it need only provide (in R0) the starting address of the character\nstring, and then invoke TRAP x22. In LC-3 assembly language this TRAP is\ncalled PUTS.\nPUTS (or TRAP x22) causes control to be passed to the operating system,\nand the trap routine shown in Figure 9.16 is executed. Note that PUTS is the code\nof lines 08 through 0C of Figure 9.15, with a few minor adjustments.\n",
    "page_number": 187,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1F\nSaveR3\n.FILL\nx0000\n20\n.END\nFigure 9.16\nThe LC-3 PUTS service routine.\n9.4 Interrupts and Interrupt-\nDriven I/O\nIn Section 9.2.1.3, we noted that interaction between the processor and an I/O\ndevice can be controlled by the processor (i.e., polling) or it can be controlled by\nthe I/O device (i.e., interrupt driven). In Sections 9.2.2, 9.2.3, and 9.2.4, we have\nstudied several examples of polling. In each case, the processor tested the ready\nbit of the status register again and again, and when the ready bit was nally 1, the\nprocessor branched to the instruction that did the input or output operation.\nWe are now ready to study the case where the interaction is controlled by the\nI/O device.\n9.4.1 What Is Interrupt-Driven I/O?\nThe essence of interrupt-driven I/O is the notion that an I/O device that may or\nmay not have anything to do with the program that is running can (1) force the\n",
    "page_number": 188,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "340\n.\n.\nFigure 9.17\nInstruction execution ow for interrupt-driven I/O.\nrunning program to stop, (2) have the processor execute a program that carries out\nthe needs of the I/O device, and then (3) have the stopped program resume exe-\ncution as if nothing had happened. These three stages of the instruction execution\now are shown in Figure 9.17.\nAs far as Program A is concerned, the work carried out and the results\ncomputed are no dierent from what would have been the case if the interrupt\nhad never happened; that is, as if the instruction execution ow had been the\nfollowing:\n.\n.\n.\nProgram A is executing instruction n\nProgram A is executing instruction n+1\nProgram A is executing instruction n+2\nProgram A is executing instruction n+3\nProgram A is executing instruction n+4\n.\n.\n.\n9.4.2 Why Have Interrupt-Driven I/O?\nAs is undoubtedly clear, polling requires the processor to waste a lot of time spin-\nning its wheels, re-executing again and again the LDI and BR instructions until\nthe ready bit is set. With interrupt-driven I/O, none of that testing and branching\nhas to go on. Interrupt-driven I/O allows the processor to spend its time doing\n",
    "page_number": 189,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "what is hopefully useful work, executing some other program perhaps, until it is\nnotied that some I/O device needs attention.\nExample 9.2\nSuppose we are asked to write a program that takes a sequence of 100 characters\ntyped on a keyboard and processes the information contained in those 100 characters.\nAssume the characters are typed at the rate of 80 words/minute, which corresponds\nto one character every 0.125 seconds. Assume the processing of the 100-character\nsequence takes 12.49999 seconds, and that our program is to perform this process on\n1000 consecutive sequences. How long will it take our program to complete the task?\n(Why did we pick 12.49999? To make the numbers come out nice, of course.) :-)\nWe could obtain each character input by polling, as in Section 9.2.2. If we did,\nwe would waste a lot of time waiting for the next character to be typed. It would\ntake 100  0.125 or 12.5 seconds to get a 100-character sequence.\nOn the other hand, if we use interrupt-driven I/O, the processor does not waste\nany time re-executing the LDI and BR instructions while waiting for a character to\nbe typed. Rather, the processor can be busy working on the previous 100-character\nsequence that was typed, except for those very small fractions of time when it is inter-\nrupted by the I/O device to read the next character typed. Lets say that to read the next\ncharacter typed requires executing a ten-instruction program that takes on the aver-\nage 0.00000001 seconds to execute each instruction. That means 0.0000001 seconds\nfor each character typed, or 0.00001 seconds for the entire 100-character sequence.\nThat is, with interrupt-driven I/O, since the processor is only needed when characters\nare actually being read, the time required for each 100-character sequence is 0.00001\nseconds, instead of 12.50000 seconds. The remaining 12.49999 of every 12.50000\nseconds, the processor is available to do useful work. For example, it can process the\nprevious 100-character sequence.\nThe bottom line: With polling, the time to complete the entire task for each\nsequence is 24.9999 seconds, 12.5 seconds to obtain the 100 characters + 12.49999\nseconds to process them. With interrupt-driven I/O, the time to complete the entire\ntask for each sequence after the rst is 12.5 seconds, 0.00001 seconds to obtain\nthe characters + 12.49999 seconds to process them. For 1000 sequences, that is the\ndierence between 7 hours and 3 1\n2 hours.\n9.4.3 Two Parts to the Process\nThere are two parts to interrupt-driven I/O:\n1. the mechanism that enables an I/O device to interrupt the processor, and\n2. the mechanism that handles the interrupt request.\n9.4.4 Part I: Causing the Interrupt to Occur\nSeveral things must be true for an I/O device to actually interrupt the program\nthat is running:\n1. The I/O device must want service.\n2. The device must have the right to request the service.\n3. The device request must be more urgent than what the processor is\ncurrently doing.\n",
    "page_number": 190,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "If all three elements are present, the processor stops executing the program\nthat is running and takes care of the interrupt.\n9.4.4.1 The Interrupt Signal from the Device\nFor an I/O device to generate an interrupt request, the device must want service,\nand it must have the right to request that service.\nThe Device Must Want Service\nWe have discussed that already in the study of\npolling. It is the ready bit of the KBSR or the DSR. That is, if the I/O device is\nthe keyboard, it wants service if someone has typed a character. If the I/O device\nis the monitor, it wants service (i.e., the next character to output) if the associated\nelectronic circuits have successfully completed the display of the last character.\nIn both cases, the I/O device wants service when the corresponding ready bit is\nset.\nThe Device Must Have the Right to Request That Service\nThis is the interrupt\nenable bit, which can be set or cleared by the processor (usually by the operating\nsystem), depending on whether or not the processor wants to give the I/O device\nthe ri ht to re uest service. In most I/O devices, this interru t enable IE bit is\nInterrupt signal to the processor\nFigure 9.18\nInterrupt enable bits and their use.\nIf the interrupt enable bit (bit [14]) is clear, it does not matter whether the\nready bit is set; the I/O device will not be able to interrupt the processor because\nit (the I/O device) has not been given the right to interrupt the processor. In that\ncase, the program will have to poll the I/O device to determine if it is ready.\nIf bit [14] is set, then interrupt-driven I/O is enabled. In that case, as soon as\nsomeone types a key (or as soon as the monitor has nished processing the last\ncharacter), bit [15] is set. In this case, the device wants service, and it has been\ngiven the right to request service. The AND gate is asserted, causing an interrupt\nrequest to be generated from the I/O device.\n9.4.4.2 The Urgency of the Request\nThe third element in the list of things that must be true for an I/O device to actually\ninterrupt the processor is that the request must be more urgent than the program\n",
    "page_number": 191,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "that is currently executing. Recall from Section 9.1.1.2 that each program runs\nat a specied level of urgency called its priority level. To interrupt the running\nprogram, the device must have a higher priority than the program that is currently\nrunning. Actually, there may be many devices that want to interrupt the processor\nat a specic time. To succeed, the device must have a higher priority level than\nall other demands for use of the processor.\nAlmost all computers have a set of priority levels that programs can run at.\nAs we have already noted, the LC-3 has eight priority levels, PL0 to PL7. The\nhigher the number, the more urgent the program. The PL of a program is usually\nthe same as the PL (i.e., urgency) of the request to run that program. If a program\nis running at one PL, and a higher-level PL request wants the computer, the lower-\npriority program suspends processing until the higher-PL program executes and\nsatises its more urgent request. For example, a computers payroll program may\nrun overnight, and at PL0. It has all night to nishnot terribly urgent. A program\nthat corrects for a nuclear plant current surge may run at PL6. We are perfectly\nhappy to let the payroll wait while the nuclear power correction keeps us from\nbeing blown to bits.\nFor our I/O device to successfully stop the processor and start an interrupt-\ndriven I/O request, the priority of the request must be higher than the priority\nof the program it wishes to interrupt. For example, we would not normally want\nto allow a keyboard interrupt from a professor checking e-mail to interrupt the\nnuclear power correction program.\n9.4.4.3 The INT Signal\nTo stop the processor from continuing execution of its currently running program\nand service an interrupt request, the INT signal must be asserted. Figure 9.19 shows\nwhat is required to assert the INT signal. Figure 9.19 shows the status registers of\nseveral devices operating at various priority levels (PL). Any device that has bits [14]\nand [15] both set asserts its interrupt request signal. The interrupt request signals are\ninput to a priority encoder, a combinational logic structure that selects the highest\npriority request from all those asserted. If the PL of that request is higher than the PL\nof the currently executing program, the INT signal is asserted.\n9.4.4.4 The Test for INT\nFinally, the test to enable the processor to stop and handle the interrupt. Recall\nfrom Chapter 4 that the instruction cycle continually sequences through the\nphases of the instruction cycle (FETCH, DECODE, EVALUATE ADDRESS,\nFETCH OPERAND, EXECUTE, and STORE RESULT). Each instruction\nchanges the state of the computer, and that change is completed at the end of\nthe instruction cycle for that instruction. That is, in the last clock cycle before the\ncomputer returns to the FETCH phase for the next instruction, the computer is\nput in the state caused by the complete execution of the current instruction.\nInterrupts can happen at any time. They are asynchronous to the synchronous\nnite state machine controlling the computer. For example, the interrupt signal\ncould occur when the instruction cycle is in its FETCH OPERAND phase. If\nwe stopped the currently executing program when the instruction cycle was in\n",
    "page_number": 192,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "344\nINT\nFigure 9.19\nGeneration of the INT signal.\nits FETCH OPERAND phase, we would have to keep track of what part of the\ncurrent instruction has executed and what part of the current instruction still has\nwork to do. It makes much more sense to ignore interrupt signals except when\nwe are at an instruction boundary; that is, the current instruction has completed,\nand the next instruction has not yet started. Doing that means we do not have to\nworry about partially executed instructions, since the state of the computer is the\nstate created by the completion of the current instruction, period!\nThe additional logic to test for the interrupt signal is to augment the last\nstate of the instruction cycle for each instruction with a test. Instead of always\ngoing from the last state of one instruction cycle to the rst state of the FETCH\nphase of the next instruction, the next state depends on the INT signal. If INT\nis not asserted, then it is business as usual, with the control unit returning to the\nFETCH phase to start processing the next instruction. If INT is asserted, then the\nnext state is the rst state of Part II, handling the interrupt request.\n9.4.5 Part II: Handling the Interrupt Request\nHandling the interrupt request goes through three stages, as shown in Figure 9.17:\n1. Initiate the interrupt (three lines numbered 1 in Figure 9.17).\n2. Service the interrupt (four lines numbered 2 in Figure 9.17).\n3. Return from the interrupt (one line numbered 3 in Figure 9.17).\nWe will discuss each.\n",
    "page_number": 193,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "9.4.5.1 Initiate the Interrupt\nSince the INT signal was asserted, the processor does not return to the rst state\nof the FETCH phase of the next instruction cycle, but rather begins a sequence of\nactions to initiate the interrupt. The processor must do two things: (1) save the\nstate of the interrupted program so it can pick up where it left o after the require-\nments of the interrupt have been completed, and (2) load the state of the higher\npriority interrupting program so it can start satisfying its request.\nSave the State of the Interrupted Program\nThe state of a program is a snap-\nshot of the contents of all the programs resources. It includes the contents of the\nmemory locations that are part of the program and the contents of all the general\npurpose registers. It also includes the PC and PSR.\nRecall from Figure 9.1 in Section 9.1.1.4 that a programs PSR species\nthe privilege level and priority level of that program. PSR[15] indicates whether\nthe program is running in privileged (Supervisor) or unprivileged (User) mode.\nPSR[10:8] species the programs priority level (PL), from PL0 (lowest) to PL7\n(highest). Also, PSR[2:0] is used to store the condition codes. PSR[2] is the N\nbit, PSR[1] is the Z bit, and PSR[0] is the P bit.\nThe rst step in initiating the interrupt is to save enough of the state of the\nprogram that is running so that it can continue where it left o after the I/O device\nrequest has been satised. That means, in the case of the LC-3, saving the PC and\nthe PSR. The PC must be saved since it knows which instruction should be exe-\ncuted next when the interrupted program resumes execution. The condition codes\n(the N, Z, and P ags) must be saved since they may be needed by a subsequent\nconditional branch instruction after the program resumes execution. The priority\nlevel of the interrupted program must be saved because it species the urgency of\nthe interrupted program with respect to all other programs. When the interrupted\nprogram resumes execution, it is important to know what priority level programs\ncan interrupt it and which ones cannot. Finally, the privilege level of the program\nmust be saved since it species what processor resources the interrupted program\ncan and cannot access.\nAlthough many computers save the contents of the general purpose registers,\nwe will not since we will assume that the service routine will always save the\ncontents of any general purpose register that it needs before using it, and then\nrestore it before returning to the interrupted program. The only state information\nthe LC-3 saves are the PC and PSR.\nThe LC-3 saves this state information on the supervisor stack in the same\nway the PC and PSR are saved when a TRAP instruction is executed. That is,\nbefore the interrupt service routine starts, if the interrupted program is in User\nmode, the User Stack Pointer (USP) is stored in Saved USP, and R6 is loaded with\nthe Supervisor Stack Pointer (SSP) from Saved SSP. Then the PSR and PC of\nthe interrupted program are pushed onto the supervisor stack, where they remain\nunmolested while the service routine executes.\nLoad the State of the Interrupt Service Routine\nOnce the state of the inter-\nrupted program has been safely saved on the supervisor stack, the second step\n",
    "page_number": 194,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "is to load the PC and PSR of the interrupt service routine. Interrupt service rou-\ntines are similar to the trap service routines we have already discussed. They are\nprogram fragments stored in system space. They service interrupt requests.\nMost processors use the mechanism of vectored interrupts. You are famil-\niar with this notion from your study of the trap vector contained in the TRAP\ninstruction. In the case of interrupts, the eight-bit vector is provided by the device\nthat is requesting the processor be interrupted. That is, the I/O device trans-\nmits to the processor an eight-bit interrupt vector along with its interrupt request\nsignal and its priority level. The interrupt vector corresponding to the highest\npriority interrupt request is the one supplied to the processor. It is designated\nINTV.\nIf the interrupt is taken, the processor expands the 8-bit interrupt vector\n(INTV) to form a 16-bit address, which is an entry into the Interrupt Vector\nTable. You know that the Trap Vector Table consists of memory locations x0000\nto x00FF, each containing the starting address of a trap service routine. The Inter-\nrupt Vector Table consists of memory locations x0100 to x01FF, each containing\nthe starting address of an interrupt service routine. The processor loads the PC\nwith the contents of the location in the Interrupt Vector Table corresponding to\nthe address formed by expanding the interrupt vector INTV.\nFor example, the LC-3 keyboard could interrupt the processor every time a\nkey is pressed by someone sitting at the keyboard. The keyboard interrupt vector\nwould indicate the location in the interrupt vector table that contains the starting\naddress of the keyboard interrupt service routine.\nThe PSR is loaded as follows: Since no instructions in the service routine\nhave yet executed, PSR[2:0] contains no meaningful information. We arbitrarily\nload it initially with 010. Since the interrupt service routine runs in privileged\nmode, PSR[15] is set to 0. PSR[10:8] is set to the priority level associated with\nthe interrupt request.\nThis completes the initiation phase, and the interrupt service routine is ready\nto execute.\n9.4.5.2 Service the Interrupt\nSince the PC contains the starting address of the interrupt service routine, the\nservice routine will execute, and the requirements of the I/O device will be\nserviced.\n9.4.5.3 Return from the Interrupt\nThe last instruction in every interrupt service routine is RTI, return from trap\nor interrupt. When the processor nally accesses the RTI instruction, all the\nrequirements of the I/O device have been taken care of.\nLike the return from a trap routine discussed in Section 9.3.4, execution of\nthe RTI instruction (opcode = 1000) for an interrupt service routine consists\nsimply of popping the PC and the PSR from the supervisor stack (where they\nhave been resting peacefully) and restoring them to their rightful places in the\n",
    "page_number": 195,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "processor. The condition codes are now restored to what they were when the\nprogram was interrupted, in case they are needed by a subsequent BR instruction\nin the interrupted program. PSR[15] and PSR[10:8] now reect the privilege level\nand priority level of the about-to-be-resumed program. If the privilege level of\nthe interrupted program is unprivileged, the stack pointers must be adjusted, that\nis, the Supervisor Stack Pointer saved, and the User Stack Pointer loaded into R6.\nThe PC is restored to the address of the instruction that would have been executed\nnext if the program had not been interrupted.\nWith all these things as they were before the interrupt occurred, the program\ncan resume as if nothing had happened.\n9.4.6 An Example\nWe complete the discussion of interrupt-driven I/O with an example.\nSuppose program A is executing when I/O device B, having a PL higher\nFigure 9.20\nExecution ow for interrupt-driven I/O.\nProgram A consists of instructions in locations x3000 to x3010 and was in\nthe middle of executing the ADD instruction at x3006 when device B sent its\ninterrupt request signal and accompanying interrupt vector xF1, causing INT to\nbe asserted.\nNote that the interrupt service routine for device B is stored in locations\nx6200 to x6210; x6210 contains the RTI instruction. Note that the service routine\n",
    "page_number": 196,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "for B was in the middle of executing the AND instruction at x6202 when device\nC sent its interrupt request signal and accompanying interrupt vector xF2. Since\nthe request associated with device C is of a higher priority than that of device B,\nINT is again asserted.\nNote that the interrupt service routine for device C is stored in locations\nx6300 to x6315; x6315 contains the RTI instruction.\nLet us examine the order of execution by the processor. Figure 9.21 shows\nseveral snapshots of the contents of the supervisor stack and the PC during the\nexecution of this example.\nThe processor executes as follows: Figure 9.21a shows the supervisor stack\nand the PC before program A fetches the instruction at x3006. Note that the stack\npointer is shown as Saved SSP, not R6. Since the interrupt has not yet occurred,\nFigure 9.21\nSnapshots of the contents of the supervisor stack and the PC during\ninterrupt-driven I/O.\n",
    "page_number": 197,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "saved on the supervisor stack, the rst step is to start using the supervisor stack.\nThis is done by saving R6 in the Saved.UPC register and loading R6 with the\ncontents of the Saved SSP register. The PSR of program A, which includes the\ncondition codes produced by the ADD instruction, is pushed on the supervisor\nstack. Then the address x3007, the PC for the next instruction to be executed in\nprogram A is pushed on the stack. The interrupt vector associated with device B\nis expanded to 16 bits x01F1, and the contents of x01F1 (x6200) is loaded into\nthe PC. Figure 9.21b shows the stack and PC at this point.\nThe service routine for device B executes until a higher priority interrupt\nis detected at the end of execution of the instruction at x6202. The PSR of the\nservice routine for B, which includes the condition codes produced by the AND\ninstruction at x6202, and the address x6203 are pushed on the stack. The interrupt\nvector associated with device C is expanded to 16 bits (x01F2), and the contents\nof x01F2 (x6300) is loaded into the PC. Figure 9.21c shows the supervisor stack\nand PC at this point.\nAssume the interrupt service routine for device C executes to completion, n-\nishing with the RTI instruction in x6315. The supervisor stack is popped twice,\nrestoring the PC to x6203 and the PSR of the service routine for device B, includ-\ning the condition codes produced by the AND instruction in x6202. Figure 9.21d\nshows the stack and PC at this point.\nThe interrupt service routine for device B resumes execution at x6203 and\nruns to completion, nishing with the RTI instruction in x6210. The supervisor\nstack is popped twice, restoring the PC to x3007 and the PSR of program A,\nincluding the condition codes produced by the ADD instruction in x3006. Finally,\nsince program A is in User mode, the contents of R6 is stored in Saved SSP and\nR6 is loaded with the contents of Saved USP. Figure 9.21e shows the supervisor\nstack and PC at this point.\nProgram A resumes execution with the instruction at x3007.\n9.4.7 Not Just I/O Devices\nWe have discussed the processing of interrupts in the context of I/O devices that\nhave higher priority than the program that is running and therefore can stop that\nprogram to enable its interrupt service routine to execute.\nWe must point out that not all interrupts deal with I/O devices. Any event that\nhas a higher priority and is external to the program that is running can interrupt\nthe computer. It does so by supplying its INT signal, its INTV vector, and its pri-\nority level. If it is the highest priority event that wishes to interrupt the computer,\nit does so in the same way that I/O devices do as described above.\nThere are many examples of such events that have nothing to do with I/O\ndevices. For example, a timer interrupt interrupts the program that is running in\norder to note the passage of a unit of time. The machine check interrupt calls atten-\ntion to the fact that some part of the computer system is not functioning properly.\nThe power failure interrupt noties the computer that, for example, someone has\nyanked the power cord out of its receptacle. Unfortunately, we will have to put\no dealing with all of these until later in your coursework.\n",
    "page_number": 198,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "9.5 Polling Revisited, Now That\nWe Know About Interrupts\n9.5.1 The Problem\nRecall our discussion of polling: We continually test the ready bit in the relevant\nstatus register, and if it is not set, we branch back to again test the ready bit. For\nexample, suppose we are writing a character string to the monitor, and we are\nusing polling to determine when the monitor has successfully written the current\ncharacter so we can dispatch the next character. We take it for granted that the\nthree-instruction sequence LDI (to load the ready bit of the DSR), BRzp (to test\nit and fall through if the device is ready), and STI (to store the next character in the\nDDR) acts as an atomic unit. But what if we had interrupts enabled at the same\ntime? That is, if an interrupt occurred within that LDI, BRzp, STI sequence (say,\njust before the STI instruction), it could easily be the case that the LDI instruction\nindicated the DDR was ready, the BRzp instruction did not branch back, but by\nthe time the interrupt service routine completed so the STI could write to the\nDDR, the DDR may no longer be ready. The computer would execute the STI,\nbut the write would not happen.\nA simple, but somewhat contrived example :-), will illustrate the problem.\nSuppose you are executing a for loop ten times, where each time the loop body\nprints to the monitor a particular character. Polling is used to determine that the\nmonitor is ready before writing the next character to the DDR. Since the loop\nbody executes ten times, this should result in the character being printed on the\nmonitor ten times. Suppose you also have keyboard interrupts enabled, and the\nkeyboard service routine echoes the character typed.\nSuppose the loop body executes as follows: LDI loads the ready bit, BRzp\nfalls through since the monitor is ready, and STI stores the character in DDR. In\nthe middle of this sequence, before the STI can execute, someone types a key.\nThe keyboard interrupt occurs, the character typed is echoed, i.e., written to the\nDDR, and the keyboard interrupt service routine completes.\nThe interrupted loop body then takes over and knows the monitor is ready,\nso it executes the STI. ... except the monitor is not ready because it has not com-\npleted the write of the keyboard service routine! The STI of the loop body writes,\nbut since DDR is not ready, the write does not occur. The nal result: Only nine\ncharacters get written, not ten.\nThe problem becomes more serious if the string written is in code, and the\nmissing write prevents the code from being deciphered.\nA simple way to handle this would be to disable all interrupts while polling\nwas going on. But consider the consequences. Suppose the polling was required\nfor a long time. If we disable interrupts while polling is going on, interrupts would\nbe disabled for that very long time, unacceptable in an environment where one is\nconcerned about the time between a higher priority interrupt occurring and the\ninterrupt getting service.\n",
    "page_number": 199,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "LC-3 ISA\nA.1 Overview\nThe instruction set architecture (ISA) of the LC-3 is dened as follows:\nMemory address space 16 bits, corresponding to 216 locations, each\ncontaining one word (16 bits). Addresses are numbered from 0 (i.e., x0000)\nto 65,535 (i.e., xFFFF). Addresses are used to identify memory locations\nand memory-mapped I/O device registers. Certain regions of memory are\nreserved for special uses, as described in Figure A.1.\nxFFFF\nFigure A.1\nMemory map of the LC-3\n",
    "page_number": 200,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Table A.1\nDevice Register Assignments\nAddress\nI/O Register Name\nI/O Register Function\nxFE00\nKeyboard status register (KBSR)\nThe ready bit (bit [15]) indicates if the keyboard has received a\nnew character.\nxFE02\nKeyboard data register (KBDR)\nBits [7:0] contain the last character typed on the keyboard.\nxFE04\nDisplay status register (DSR)\nThe ready bit (bit [15]) indicates if the display device is ready to\nreceive another character to print on the screen.\nxFE06\nDisplay data register (DDR)\nA character written in bits [7:0] will be displayed on the screen.\nxFFFC\nProcessor Status Register (PSR)\nContains privilege mode, priority level and condition codes of\nthe currently executing process.\nxFFFE\nMachine control register (MCR)\nBit [15] is the clock enable bit. When cleared, instruction\nprocessing stops.\ndata. Addresses xFE00 to xFFFF specify input and output device registers\nand special internal processor registers that are also only accessible if the\nprocess is executing in Supervisor mode (PSR[15]=0). For purposes of\ncontrolling access to these device registers, their addresses are also\nconsidered part of privileged memory.\nMemory-mapped I/O Input and output are handled by load/store (LD/ST,\nLDI/STI, LDR/STR) instructions using memory addresses from xFE00 to\nxFFFF to designate each device register. Table A.1 lists the input and\noutput device registers and internal processor registers that have been\nspecied for the LC-3 thus far, along with their corresponding assigned\naddresses from the memory address space.\nBit numbering Bits of all quantities are numbered, from right to left,\nstarting with bit 0. The leftmost bit of the contents of a memory location is\nbit 15.\nInstructions Instructions are 16 bits wide. Bits [15:12] specify the opcode\n(operation to be performed); bits [11:0] provide further information that is\nneeded to execute the instruction. The specic operation of each LC-3\ninstruction is described in Section A.2.\nIllegal opcode exception Bits [15:12] = 1101 has not been specied. If\nan instruction contains 1101 in bits [15:12], an illegal opcode exception\noccurs. Section A.3 explains what happens.\nProgram counter A 16-bit register containing the address of the next\ninstruction to be processed.\nGeneral purpose registers Eight 16-bit registers, numbered from 000 to\n111 (R0 to R7).\nCondition codes Three 1-bit registers: N (negative), Z (zero), and P\n(positive). Load instructions (LD, LDI, and LDR) and operate instructions\n(ADD, AND, and NOT) each load a result into one of the eight general\npurpose registers. The condition codes are set, based on whether that result,\ntaken as a 16-bit 2s complement integer, is negative (N = 1; Z, P = 0), zero\n(Z = 1; N, P = 0), or positive (P = 1; N, Z = 0). All other LC-3 instructions\nleave the condition codes unchanged.\n",
    "page_number": 201,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Interrupt processing I/O devices have the capability of interrupting the\nprocessor. Section A.3 describes the mechanism.\nPriority level The LC-3 supports eight levels of priority. Priority level 7\n(PL7) is the highest, PL0 is the lowest. The priority level of the currently\nexecuting process is specied in bits PSR[10:8].\nProcessor status register (PSR) A 16-bit register, containing status\ninformation about the currently executing process. Seven bits of the PSR\nhave been dened thus far. PSR[15] species the privilege mode of\nthe executing process. PSR[10:8] species the priority level of the currently\nexecuting process. PSR[2:0] contains the condition codes. PSR[2] is N,\nPSR[1] is Z, and PSR[0] is P.\nSupervisor mode The LC-3 species two modes of operation, Supervisor\nmode (privileged) and User mode (unprivileged). Interrupt service routines\nand trap service routines (i.e., system calls) execute in Supervisor mode.\nThe privilege mode is specied by PSR[15]. PSR[15]=0 indicates\nSupervisor mode; PSR[15]=1 indicates User mode.\nPrivilege mode exception The RTI instruction executes in Supervisor mode.\nIf the processor attempts to execute the RTI instruction while in User mode, a\nprivilege mode exception occurs. Section A.3 explains what happens.\nAccess Control Violation (ACV) exception An ACV exception occurs if a\nprocess attempts to access a location in privileged memory (either a location in\nsystem space or a device register having an address from xFE00 to xFFFF)\nwhile operating in User mode. Section A.3 explains what happens.\nSupervisor stack A region of memory in system space accessible via the\nSupervisor Stack Pointer (SSP). When PSR[15]=0, the stack pointer (R6) is\nSSP. When the processor is operating in User mode (PSR[15]=1), the SSP\nis stored in Saved SSP.\nUser stack A region of memory in user space accessible via the User Stack\nPointer (USP). When PSR[15]=1, the stack pointer (R6) is USP. When the\nprocessor is operating in Supervisor mode (PSR[15]=0), the USP is stored\nin Saved USP.\nA.2 The Instruction Set\nThe LC-3 supports a rich, but lean, instruction set. Each 16-bit instruction consists\nof an opcode (bits[15:12]) plus 12 additional bits to specify the other informa-\ntion that is needed to carry out that instruction. Figure A.2 summarizes the 15\ndierent opcodes in the LC-3 and the specication of the remaining bits of each\ninstruction. The 16th four-bit opcode is not specied but is reserved for future use.\nIn the following pages, the instructions will be described in greater detail.\nTable A.2 is provided to help you to understand those descriptions. For each\ninstruction, we show the assembly language representation, the format of the\n16-bit instruction, the operation of the instruction, an English-language descrip-\ntion of its operation, and one or more examples of the instruction. Where relevant,\nadditional notes about the instruction are also provided.\n",
    "page_number": 202,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Table A.2\nNotational Conventions\nNotation\nMeaning\nxNumber\nThe number in hexadecimal notation. Example: xF2A1\n#Number\nThe number in decimal notation. Example #793\nbNumber\nThe number in binary. Example b10011\nA[l:r]\nThe eld delimited by bit [l] on the left and bit [r] on the right, of the datum A. For example, if PC\ncontains 0011001100111111, then PC[15:9] is 0011001. PC[2:2] is 1. If l and r are the same bit\nnumber, we generally write PC[2].\nBaseR\nBase Register; one of R0..R7, specied by bits [8:6] of the instruction, used in conjunction with a six-bit\nofset to compute Base+ofset addresses (LDR and STR), or alone to identify the target address of a\ncontrol instruction (JMP and JSRR).\nDR\nDestination Register; one of R0..R7, which species the register a result should be written to.\nimm5\nA ve-bit immediate value (bits [4:0] of an instruction), when used as a literal (immediate) value. Taken\nas a ve-bit, 2s complement integer, it is sign-extended to 16 bits before it is used. Range: 16..15.\nINTV\nAn eight-bit value, supplied along with an interrupting event; used to determine the starting address\nof an interrupt service routine. The eight bits form an ofset from the starting address of the interrupt\nvector table. The corresponding location in the interrupt vector table contains the starting address\nof the corresponding interrupt service routine. Range 0..255.\nLABEL\nAn assembly language construct that identies a location symbolically (i.e., by means of a name,\nrather than its 16-bit address).\nmem[address]\nDenotes the contents of memory at the given address.\nofset6\nA six-bit signed 2s complement integer (bits [5:0] of an instruction), used with the Base+ofset\naddressing mode. Bits [5:0] are sign-extended to 16 bits and then added to the Base Register to\nform an address. Range: 32..31.\nPC\nProgram Counter; 16-bit register that contains the memory address of the next instruction to be\nfetched. For example, if the instruction at address A is not a control instruction, during its execution,\nthe PC contains the address A + 1, indicating that the next instruction to be executed is contained in\nmemory location A + 1.\nPCofset9\nA nine-bit signed 2s complement integer (bits [8:0] of an instruction), used with the PC+ofset\naddressing mode. Bits [8:0] are sign-extended to 16 bits and then added to the incremented PC to\nform an address. Range 256..255.\nPCofset11\nAn eleven-bit signed 2s complement integer (bits [10:0] of an instruction), used with the JSR opcode\nto compute the target address of a subroutine call. Bits [10:0] are sign-extended to 16 bits and then\nadded to the incremented PC to form the target address. Range 1024..1023.\nPSR\nProcessor Status Register. A 16-bit register that contains status information of the process that is\nexecuting. Seven bits of the PSR have been specied. PSR[15] = privilege mode. PSR[10:8] =\nPriority Level. PSR[2:0] contains the condition codes. PSR[2] = N, PSR[1] = Z, PSR[0] = P.\nSaved SSP\nSaved Supervisor Stack Pointer. The processor is executing in either Supervisor mode or User mode.\nIf in User mode, R6, the stack pointer, is the User Stack Pointer (USP). The Supervisor Stack Pointer\n(SSP) is stored in Saved SSP. When the privilege mode changes from User mode to Supervisor\nmode, Saved USP is loaded with R6 and R6 is loaded with Saved SSP.\nSaved USP\nSaved User Stack Pointer. The User Stack Pointer is stored in Saved USP when the processor is\nexecuting in Supervisor mode. See Saved SSP.\nsetcc()\nIndicates that condition codes N, Z, and P are set based on the value of the result written to DR.\nSEXT(A)\nSign-extend A. The most signicant bit of A is replicated as many times as necessary to extend A to\n16 bits. For example, if A = 110000, then SEXT(A) = 1111 1111 1111 0000.\nSP\nThe current stack pointer. R6 is the current stack pointer. There are two stacks, one for each privilege\nmode. SP is SSP if PSR[15] = 0; SP is USP if PSR[15] = 1.\nSR, SR1, SR2\nSource register; one of R0..R7 that species the register from which a source operand is obtained.\nSSP\nThe Supervisor Stack Pointer.\ntrapvect8\nAn eight-bit value (bits [7:0] of an instruction), used with the TRAP opcode to determine the starting\naddress of a trap service routine. Bits [7:0] are taken as an unsigned integer and zero-extended to\n16 bits. This is the address of the memory location containing the starting address of the\ncorresponding service routine. Range 0..255.\nUSP\nThe User Stack Pointer.\nZEXT(A)\nZero-extend A. Zeros are appended to the leftmost bit of A to extend it to 16 bits. For example, if\nA = 110000, then ZEXT(A) = 0000 0000 0011 0000.\n",
    "page_number": 203,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "ADD\nAddition\n15\n12\n11\n9\n8\n6\n5\n4\n0\n0001\nDR\nSR1\n1\nimm5\nOperation\nif (bit[5] == 0)\nDR = SR1 + SR2;\nelse\nDR = SR1 + SEXT(imm5);\nsetcc();\nDescription\nIf bit [5] is 0, the second source operand is obtained from SR2. If bit [5] is 1, the\nsecond source operand is obtained by sign-extending the imm5 eld to 16 bits.\nIn both cases, the second source operand is added to the contents of SR1 and the\nresult stored in DR. The condition codes are set, based on whether the result is\nnegative, zero, or positive.\nExamples\nADD\nR2, R3, R4\n; R2  R3 + R4\nADD\nR2, R3, #7\n; R2  R3 + 7\n",
    "page_number": 204,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "AND\nBit-wise Logical AND\n12\n11\n9\n8\n6\n5\n4\n0\n15\n0101\nDR\nSR1\n1\nimm5\nOperation\nif (bit[5] == 0)\nDR = SR1 AND SR2;\nelse\nDR = SR1 AND SEXT(imm5);\nsetcc();\nDescription\nIf bit [5] is 0, the second source operand is obtained from SR2. If bit [5] is 1,\nthe second source operand is obtained by sign-extending the imm5 eld to 16\nbits. In either case, the second source operand and the contents of SR1 are bit-\nwise ANDed and the result stored in DR. The condition codes are set, based on\nwhether the binary value produced, taken as a 2s complement integer, is negative,\nzero, or positive.\nExamples\nAND\nR2, R3, R4\n;R2  R3 AND R4\nAND\nR2, R3, #7\n;R2  R3 AND 7\n",
    "page_number": 205,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "BR\nConditional Branch\nAssembler Formats\nBRn\nLABEL\nBRzp\nLABEL\nz\nn\np\n0000\nPCofset9\nOperation\nif ((n AND N) OR (z AND Z) OR (p AND P))\nPC = PC + SEXT(PCoffset9);\nDescription\nThe condition codes specied by bits [11:9] are tested. If bit [11] is 1, N is tested;\nif bit [11] is 0, N is not tested. If bit [10] is 1, Z is tested, etc. If any of the condi-\ntion codes tested is 1, the program branches to the memory location specied by\nadding the sign-extended PCoset9 eld to the incremented PC.\nExamples\nBRzp\nLOOP\n; Branch to LOOP if the last result was zero or positive.\nBR\nNEXT\n; Unconditionally branch to NEXT.\nThe assembly language opcode BR is interpreted the same as BRnzp; that is, always branch to the target\naddress.\nThis is the incremented PC.\n",
    "page_number": 206,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "JMP\nRET\nJump\nReturn from Subroutine\nAssembler Formats\n000\n111\n000000\n0\n5\n6\n8\n9\n11\n12\n15\nRET\n1100\nOperation\nPC = BaseR;\nDescription\nThe program unconditionally jumps to the location specied by the contents of\nthe base register. Bits [8:6] identify the base register.\nExamples\nJMP\nR2\n; PC  R2\nRET\n; PC  R7\nNote\nThe RET instruction is a special case of the JMP instruction, normally used in the\nreturn from a subroutine. The PC is loaded with the contents of R7, which con-\ntains the linkage back to the instruction following the subroutine call instruction.\n",
    "page_number": 207,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "JSR\nJSRR\nJump to Subroutine\nAssembler Formats\n00\n0\nBaseR\n000000\n0\n5\n6\n8\n9\n10\n11\n12\n15\nJSRR\n0100\nOperation\nTEMP = PC;\nif (bit[11] == 0)\nPC = BaseR;\nelse\nPC = PC + SEXT(PCoffset11);\nR7 = TEMP;\nDescription\nFirst, the incremented PC is saved in a temporary location. Then the PC is loaded\nwith the address of the rst instruction of the subroutine, which will cause an\nunconditional jump to that address after the current instruction completes execu-\ntion. The address of the subroutine is obtained from the base register (if bit [11]\nis 0), or the address is computed by sign-extending bits [10:0] and adding this\nvalue to the incremented PC (if bit [11] is 1). Finally, R7 is loaded with the value\nstored in the temporary location. This is the linkage back to the calling routine.\nExamples\nJSR\nQUEUE ; Put the address of the instruction following JSR into R7;\n; Jump to QUEUE.\nJSRR R3\n; Put the address of the instruction following JSRR into R7;\n; Jump to the address contained in R3.\nThis is the incremented PC.\n",
    "page_number": 208,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "LD\nLoad\nPCofset9\n0010\nDR\n15\n12\n11\n9\n8\n0\nOperation\nif (computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nDR = mem[PC + SEXT(PCoffset9)];\nsetcc();\nDescription\nAn address is computed by sign-extending bits [8:0] to 16 bits and adding\nthis value to the incremented PC. If the address is to privileged memory and\nPSR[15]=1, initiate ACV exception. If not, the contents of memory at this address\nis loaded into DR. The condition codes are set, based on whether the value loaded\nis negative, zero, or positive.\nExample\nLD\nR4, VALUE\n; R4  mem[VALUE]\nThis is the incremented PC.\n",
    "page_number": 209,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "LDI\nLoad Indirect\nPCofset9\n1010\nDR\n15\n12\n11\n9\n8\n0\nOperation\nif (either computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nDR = mem[mem[PC + SEXT(PCoffset9)]];\nsetcc();\nDescription\nAn address is computed by sign-extending bits [8:0] to 16 bits and adding this\nvalue to the incremented PC. What is stored in memory at this address is the\naddress of the data to be loaded into DR. If either address is to privileged mem-\nory and PSR[15]=1, initiate ACV exception. If not, the data is loaded and the\ncondition codes are set, based on whether the value loaded is negative, zero, or\npositive.\nExample\nLDI\nR4, ONEMORE\n; R4  mem[mem[ONEMORE]]\nThis is the incremented PC.\n",
    "page_number": 210,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "LDR\nLoad Base+oset\n15\n12\n11\n9\n8\n6\n5\n0\nBaseR\nDR\n0110\nofset6\nOperation\nIf (computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nDR = mem[BaseR + SEXT(offset6)];\nsetcc();\nDescription\nAn address is computed by sign-extending bits [5:0] to 16 bits and adding this\nvalue to the contents of the register specied by bits [8:6]. If the computed address\nis to privileged memory and PSR[15]=1, initiate ACV exception. If not, the con-\ntents of memory at this address is loaded into DR. The condition codes are set,\nbased on whether the value loaded is negative, zero, or positive.\nExample\nLDR\nR4, R2, #5\n; R4  mem[R2  5]\n",
    "page_number": 211,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "LEA\nLoad Eective Address\n15\n12\n11\n9\n8\n0\nDR\n1110\nPCofset9\nOperation\nDR = PC + SEXT(PCoffset9);\nDescription\nAn address is computed by sign-extending bits [8:0] to 16 bits and adding this\nvalue to the incremented PC. This address is loaded into DR.\nExample\nLEA\nR4, TARGET\n; R4  address of TARGET.\nThis is the incremented PC.\nThe LEA instruction computes an address but does NOT read memory. Instead, the address itself is\nloaded into DR.\n",
    "page_number": 212,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "NOT\nBit-Wise Complement\n11111\n15\n12\n11\n9\n8\n6\n5\n4\n3\n2\n0\nDR\n1001\nSR\n1\nOperation\nDR = NOT(SR);\nsetcc();\nDescription\nThe bit-wise complement of the contents of SR is stored in DR. The condi-\ntion codes are set, based on whether the binary value produced, taken as a 2s\ncomplement integer, is negative, zero, or positive.\nExample\nNOT\nR4, R2\n; R4  NOT(R2)\n",
    "page_number": 213,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "RET\nReturn from Subroutine\n000\n111\n000000\n5\n11\n1\n15\n1100\nOperation\nPC = R7;\nDescription\nThe PC is loaded with the value in R7. Its normal use is to cause a return from a\nprevious JSR(R) instruction.\nExample\nRET\n; PC  R7\nThe RET instruction is a specic encoding of the JMP instruction. See also JMP.\n",
    "page_number": 214,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "RTI\nterrupt\n15\n12\n11\n0\n000000000000\n1000\nOperation\nif (PSR[15] == 1)\nInitiate a privilege mode exception;\nelse\nPC = mem[R6]; R6 is the SSP, PC is restored\nR6 = R6+1;\nTEMP = mem[R6];\nR6 = R6+1; system stack completes POP before saved PSR is restored\nPSR = TEMP; PSR is restored\nif (PSR[15] == 1)\nSaved SSP=R6 and R6=Saved USP;\nDescription\nIf the processor is running in User mode, a privilege mode exception occurs. If\nin Supervisor mode, the top two elements on the system stack are popped and\nloaded into PC, PSR. After PSR is restored, if the processor is running in User\nmode, the SSP is saved in Saved SSP, and R6 is loaded with Saved USP.\nExample\nRTI\n; PC, PSR  top two values popped o stack.\nNote\nRTI is the last instruction in both interrupt and trap service routines and returns\ncontrol to the program that was running. In both cases, the relevant service routine\nis initiated by first pushing the PSR and PC of the program that is running onto the\nsystem stack. Then the starting address of the appropriate service routine is loaded\ninto the PC, and the service routine executes with supervisor privilege. The last\ninstruction in the service routine is RTI, which returns control to the interrupted\nprogram by popping two values off the supervisor stack to restore the PC and PSR.\nIn the case of an interrupt, the PC is restored to the address of the instruction that was\nabout to be processed when the interrupt was initiated. In the case of an exception,\nthe PC is restored to either the address of the instruction that caused the exception or\nthe address of the following instruction, depending on whether the instruction that\ncaused the exception is to be re-executed. In the case of a TRAP service routine,\nthe PC is restored to the instruction following the TRAP instruction in the calling\nroutine. In the case of an interrupt or TRAP, the PSR is restored to the value it had\nwhen the interrupt was initiated. In the case of an exception, the PSR is restored to\nthe value it had when the exception occurred or to some modified value, depending\non the exception. See also Section A.3.\n",
    "page_number": 215,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "ST\nStore\nPCofset9\n0011\nSR\n15\n12\n11\n9\n8\n0\nOperation\nif (computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nmem[PC + SEXT(PCoffset9)] = SR;\nDescription\nIf the computed address is to privileged memory and PSR[15]=1, initiate ACV\nexception. If not, the contents of the register specied by SR is stored in the\nmemory location whose address is computed by sign-extending bits [8:0] to 16\nbits and adding this value to the incremented PC.\nExample\nST\nR4, HERE\n; mem[HERE]  R4\nThis is the incremented PC.\n",
    "page_number": 216,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "STI\nStore Indirect\nPCofset9\n1011\nSR\n15\n12\n11\n9\n8\n0\nOperation\nif (either computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nmem[mem[PC + SEXT(PCoffset9)]] = SR;\nDescription\nIf either computed address is to privileged memory and PSR[15]=1, initiate\nACV exception. If not, the contents of the register specied by SR is stored\nin the memory location whose address is obtained as follows: Bits [8:0] are sign-\nextended to 16 bits and added to the incremented PC. What is in memory at this\naddress is the address of the location to which the data in SR is stored.\nExample\nSTI\nR4, NOT HERE\n; mem[mem[NOT HERE]]  R4\nThis is the incremented PC.\n",
    "page_number": 217,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "STR\nStore Base+oset\n15\n12\n11\n9\n8\n6\n5\n0\nBaseR\nSR\n0111\nofset6\nOperation\nif (computed address is in privileged memory AND PSR[15] == 1)\nInitiate ACV exception;\nelse\nmem[BaseR + SEXT(offset6)] = SR;\nDescription\nIf the computed address is to privileged memory and PSR[15]=1, initiate ACV\nexception. If not, the contents of the register specied by SR is stored in the\nmemory location whose address is computed by sign-extending bits [5:0] to 16\nbits and adding this value to the contents of the register specied by bits [8:6].\nExample\nSTR\nR4, R2, #5\n; mem[R2+5]  R4\n",
    "page_number": 218,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "TRAP\nSystem Call\nAs\nEn\n0\n7\n8\n11\n12\n15\n1111\n0000\ntrapvect8\nOperation\nTEMP=PSR;\nif (PSR[15] == 1)\nSaved USP=R6 and R6=Saved SSP;\nPSR[15]=0;\nPush TEMP,PC on the system stack\nPC = mem[ZEXT(trapvect8)];\nDescription\nIf the the program is executing in User mode, the User Stack Pointer must be\nsaved and the System Stack Pointer loaded. Then the PSR and PC are pushed\non the system stack. (This enables a return to the instruction physically follow-\ning the TRAP instruction in the original program after the last instruction in the\nservice routine (RTI) has completed execution.) Then the PC is loaded with the\nstarting address of the system call specied by trapvector8. The starting address\nis contained in the memory location whose address is obtained by zero-extending\ntrapvector8 to 16 bits.\nExample\nTRAP\nx23\n; Directs the operating system to execute the IN system call.\n; The starting address of this system call is contained in\n; memory location x0023.\nNote:\nMemory locations x0000 through x00FF, 256 in all, are available to contain\nstarting addresses for system calls specied by their corresponding trap vectors.\nThis region of memory is called the Trap Vector Table. Table A.3 describes the\nfunctions performed by the service routines corresponding to trap vectors x20\nto x25.\nThis is the incremented PC.\n",
    "page_number": 219,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Unused Opcode\n1101\nOperation\nInitiate an illegal opcode exception.\nDescription\nIf an illegal opcode is encountered, an illegal opcode exception occurs.\nNote:\nThe opcode 1101 has been reserved for future use. It is currently not dened. If\nthe instruction currently executing has bits [15:12] = 1101, an illegal opcode\nexception occurs. Section A.3 describes what happens.\n",
    "page_number": 220,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Table A.3\nTrap Service Routines\nTrap Vector\nAssembler Name\nDescription\nx20\nGETC\nRead a single character from the keyboard. The character is not echoed onto the\nconsole. Its ASCII code is copied into R0. The high eight bits of R0 are cleared.\nx21\nOUT\nWrite a character in R0[7:0] to the console display.\nx22\nPUTS\nWrite a string of ASCII characters to the console display. The characters are\ncontained in consecutive memory locations, one character per memory location,\nstarting with the address specied in R0. Writing terminates with the occurrence of\nx0000 in a memory location.\nx23\nIN\nPrint a prompt on the screen and read a single character from the keyboard. The\ncharacter is echoed onto the console monitor, and its ASCII code is copied into\nR0. The high eight bits of R0 are cleared.\nx24\nPUTSP\nWrite a string of ASCII characters to the console. The characters are contained in\nconsecutive memory locations, two characters per memory location, starting with\nthe address specied in R0. The ASCII code contained in bits [7:0] of a memory\nlocation is written to the console rst. Then the ASCII code contained in bits [15:8]\nof that memory location is written to the console. (A character string consisting of\nan odd number of characters to be written will have x00 in bits [15:8] of the\nmemory location containing the last character to be written.) Writing terminates\nwith the occurrence of x0000 in a memory location.\nx25\nHALT\nHalt execution and print a message on the console.\nA.3 Interrupt and Exception\nProcessing\nAs has been discussed in detail in Chapter 9, events external to the program that\nis running can interrupt the processor. A common example of an external event\nis interrupt-driven I/O. It is also the case that the processor can be interrupted\nby exceptional events that occur while the program is running that are caused by\nthe program itself. An example of such an internal event is the presence of an\nunused opcode in the computer program that is running.\nAssociated with each event that can interrupt the processor is an eight-bit\nvector that provides an entry point into a 256-entry interrupt vector table. The\nstarting address of the interrupt vector table is x0100. That is, the interrupt vector\ntable occupies memory locations x0100 to x01FF. Each entry in the interrupt\nvector table contains the starting address of the service routine that handles the\nneeds of the corresponding event. These service routines execute in Supervisor\nmode.\nHalf (128) of these entries, locations x0100 to x017F, provide the starting\naddresses of routines that service events caused by the running program itself.\nThese routines are called exception service routines because they handle excep-\ntional events, that is, events that prevent the program from executing normally.\nThe other half of the entries, locations x0180 to x01FF, provide the starting\naddresses of routines that service events that are external to the program that\nis running, such as requests from I/O devices. These routines are called interrupt\nservice routines.\n",
    "page_number": 221,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "A.3.1 Interrupts\nAt this time, an LC-3 computer system provides only one I/O device that can\ninterrupt the processor. That device is the keyboard. It interrupts at priority level\nPL4 and supplies the interrupt vector x80.\nAn I/O device can interrupt the processor if it wants service, if its interrupt\nenable (IE) bit is set, and if the priority of its request is greater than the priority of\nany other event that wants to interrupt and greater than the priority of the program\nthat is running.\nAssume a program is running at a priority level less than 4, and someone\nstrikes a key on the keyboard. If the IE bit of the KBSR is 1, the currently execut-\ning program is interrupted at the end of the current instruction cycle. The interrupt\nservice routine is initiated as follows:\n1. The PSR of the interrupted process is saved in TEMP.\n2. The processor sets the privilege mode to Supervisor mode (PSR[15]=0).\n3. The processor sets the priority level to PL4, the priority level of the\ninterrupting device (PSR[10:8]=100).\n4. If the interrupted process is in User mode, R6 is saved in Saved USP and\nR6 is loaded with the Supervisor Stack Pointer (SSP).\n5. TEMP and the PC of the interrupted process are pushed onto the supervisor\nstack.\n6. The keyboard supplies its eight-bit interrupt vector, in this case x80.\n7. The processor expands that vector to x0180, the corresponding 16-bit\naddress in the interrupt vector table.\n8. The PC is loaded with the contents of memory location x0180, the address\nof the rst instruction in the keyboard interrupt service routine.\nThe processor then begins execution of the interrupt service routine.\nThe last instruction executed in an interrupt service routine is RTI. The top\ntwo elements of the supervisor stack are popped and loaded into the PC and PSR\nregisters. R6 is loaded with the appropriate stack pointer, depending on the new value\nof PSR[15]. Processing then continues where the interrupted program left off.\nA.3.2 Exceptions\nAt this time, the LC-3 ISA species three exception conditions: privilege mode\nviolation, illegal opcode, and access control violation (ACV). The privilege mode\nviolation occurs if the processor attempts to execute the RTI instruction while\nrunning in User mode. The illegal opcode exception occurs if the processor\nattempts to execute an instruction having the unused opcode (bits [15:12] =\n1101). The ACV exception occurs if the processor attempts to access privileged\nmemory (i.e., a memory location in system space or a device register having an\naddress from xFE00 to xFFFF while running in User mode).\nExceptions are handled as soon as they are detected. They are initiated very\nmuch like interrupts are initiated, that is:\n1. The PSR of the process causing the exception is saved in TEMP.\n",
    "page_number": 222,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "2. The processor sets the privilege mode to Supervisor mode (PSR[15]=0).\n3. If the process causing the exception is in User mode, R6 is saved in\nSaved USP and R6 is loaded with the SSP.\n4. TEMP and the PC of the process causing the exception are pushed onto the\nsupervisor stack.\n5. The exception supplies its eight-bit vector. In the case of the privilege mode\nviolation, that vector is x00. In the case of the illegal opcode, that vector is\nx01. In the case of the ACV exception, that vector is x02.\n6. The processor expands that vector to x0100, x0101, or x0102, the\ncorresponding 16-bit address in the interrupt vector table.\n7. The PC is loaded with the contents of memory location x0100, x0101, or\nx0102, the address of the rst instruction in the corresponding exception\nservice routine.\nThe processor then begins execution of the exception service routine.\nThe details of the exception service routine depend on the exception and the\nway in which the operating system wishes to handle that exception.\nIn many cases, the exception service routine can correct any problem caused\nby the exceptional event and then continue processing the original program. In\nthose cases, the last instruction in the exception service routine is RTI, which pops\nthe top two elements from the supervisor stack and loads them into the PC and\nPSR registers. The program then resumes execution with the problem corrected.\nIn some cases, the cause of the exceptional event is suciently catastrophic\nthat the exception service routine removes the program from further processing.\nAnother dierence between the handling of interrupts and the handling of\nexceptions is the priority level of the processor during the execution of the service\nroutine. In the case of exceptions, we normally do not change the priority level\nwhen we service the exception. The priority level of a program is the urgency\nwith which it needs to be executed. In the case of the exceptions specied by the\nLC-3 ISA, the urgency of a program is not changed by the fact that a privilege\nmode violation occurred or there was an illegal opcode in the program or the\nprogram attempted to access privileged memory while it was in User mode.\n",
    "page_number": 223,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "LC-3 to x86\nA\ns you know, the ISA of the LC-3 explicitly species the interface between\nwhat the LC-3 machine language programmer or LC-3 compilers produce\nand what a microarchitecture of the LC-3 can accept and process. Among those\nthings specied are the address space and addressability of memory, the number\nand size of the registers, the format of the instructions, the opcodes, the data types\nthat are the encodings used to represent information, and the addressing modes\nthat are available for determining the location of an operand.\nThe ISA of the microprocessor in your PC also species an interface between\nthe compilers and the microarchitecture. However, in the case of the PC, the ISA\nis not the LC-3. Rather it is the x86. Intel introduced the rst member of this ISA\nin 1979. It was called the 8086, and the normal size of the addresses and data\nelements it processed was 16 bits, the same size as the LC-3. Today, the typical\nsize of addresses and data is 64 bits. With special vector extensions, instructions\ncan operate on vectors that can be of size 128, 256, and 512 bits. Because there\nare a lot of old programs and data expressed in 32 bits, the x86 is able to process\ninstructions in what we call 64-bit mode or 32-bit mode. That is, in 32-bit mode,\nthe x86 restricts itself to a 32-bit address space and 32-bit elements.\nFrom the 8086 to the present time, Intel has continued implementations\nof the x86 ISA, among them the 386 (in 1985), 486 (in 1989), Pentium (in\n1992), Pentium Pro (in 1995), Pentium II (in 1997), Pentium IV (in 2001), 1st\nGeneration Core i7-9xx Series, codename Nehalem (in 2008), 4th Generation\nCore i7-4xxx Series, codename Haswell (in 2013), and 8th Generation Core\ni7-8086K, codename: Coee Lake (in 2018).\nThe ISA of the x86 is much more complicated than that of the LC-3. There\nare more opcodes, more data types, more addressing modes, a more complicated\nmemory structure, and a more complicated encoding of instructions into 0s and\n1s. However, fundamentally, they have the same basic ingredients.\nYou have spent a good deal of time understanding computing within the con-\ntext of the LC-3. Some may feel that it would be good to learn about a real ISA.\nOne way to do that would be to have some company such as Intel mass-produce\nLC-3 microprocessors, some other company like Dell put them in their PCs, and a\nthird company such as Microsoft compile Windows NT into the ISA of the LC-3.\nAn easier way to introduce you to a real ISA is by way of this appendix.\n",
    "page_number": 224,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "We present here elements of the x86, a very complicated ISA. We do so in\nspite of its complexity because it is one of the most pervasive of all ISAs available\nin the marketplace.\nWe make no attempt to provide a complete specication of the x86 ISA.\nThat would require a whole book by itself, and to appreciate it, a deeper under-\nstanding of operating systems, compilers, and computer systems than we think\nis reasonable at this point in your education. If one wants a complete treat-\nment, we recommend the Intel Architecture Software Developers Manual. In this\nappendix, we restrict ourselves to some of the characteristics that are relevant to\napplication programs. Our intent is to give you a sense of the richness of the x86\nISA. We introduce these characteristics within the context of the LC-3 ISA, which\nat this point you are very familiar with.\nB.1 LC-3 Features and\nCorresponding x86 Features\nB.1.1 Instruction Set\nAn instruction set is made up of instructions, each of which has an opcode and\nzero or more operands. The number of operands depends on how many are needed\nby the corresponding opcode. Each operand is a data element and is encoded\naccording to its data type. The location of an operand is determined by evaluating\nits addressing mode.\nThe LC-3 instruction set contains one data type, 15 opcodes, and three\naddressing modes: PC-relative (LD, ST), indirect (LDI, STI), and register-plus-\noset (LDR, STR). The x86 instruction set has more than a dozen data types,\nmore than a thousand opcodes, and more than two dozen addressing modes\n(depending on how you count).\nB.1.1.1 Data Types\nRecall that a data type is a representation of information such that the ISA pro-\nvides opcodes that operate on information that is encoded in that representation.\nThe LC-3 supports only one data type, 16-bit 2s-complement integers. This\nis not enough for ecient processing in the real world. Scientic applications\nneed numbers that are represented by the oating point data type. Multimedia\napplications require information that is represented by a dierent data type. Com-\nmercial applications written years ago, but still active today, require an additional\ndata type, referred to as packed decimal. Some applications require a greater\nrange of values and a greater precision of each value than other applications.\nAs a result of all these requirements, the x86 is designed with instructions\nthat operate on (for example) 8-bit integers, 16-bit integers, and 32-bit integers,\n32-bit and 64-bit oating point numbers, 64-bit, 128-bit, 256-bit, and 512-bit-\nmultimedia values. Figure B.1 shows some of the data types present in the\nx86 ISA.\n",
    "page_number": 225,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "B.1.1.2 Opcodes\nThe LC-3 comprises 15 opcodes; the x86 instruction set comprises more than\na thousand. Recall that the three basic instruction types are operates, data\nmovement, and control. Operates process information, data movement opcodes\nmove information from one place to another (including input and output), and\ncontrol opcodes change the ow of the instruction stream.\nIn addition, we should add a fourth category to handle functions that must\nbe performed in the real world because a user program runs in the context of an\noperating system that is controlling a computer system, rather than in isolation.\nThese instructions deal with computer security, system management, hardware\nperformance monitoring, and various other issues that are beyond what the typical\napplication program pays attention to. We will ignore those instructions in this\nappendix, but please realize that they do exist, and you will see them as your\nstudies progress. Here we will concentrate on the three basic instruction types:\noperates, data movement, and control.\nOperates\nThe LC-3 has three operate instructions: ADD, AND, and NOT. The\nADD opcode is the only LC-3 opcode that performs arithmetic. If one wants to\nsubtract, one obtains the negative of an operand and then adds. If one wants\nto multiply, one can write a program with a loop to ADD a number some specied\nnumber of times. However, this is too time-consuming for a real microprocessor.\nSo the x86 has separate SUB and MUL, as well as DIV, INC (increment), DEC\n(decrement), and ADC (add with carry), to name a few.\nA useful feature of an ISA is to extend the size of the integers on which it can\noperate. To do this, one writes a program to operate on such long integers. The\nADC opcode, which adds two operands plus the carry from the previous add, is\na very useful opcode for extending the size of integers.\nIn addition, the x86 has, for each data type, its own set of opcodes to operate\non that data type. For example, multimedia instructions (collectively called the\nMMX instructions) often require saturating arithmetic, which is very dierent\nfrom the arithmetic we are used to. PADDS is an opcode that adds two operands\nwith saturating arithmetic.\nSaturating arithmetic can be explained as follows: Suppose we represent the\ndegree of grayness of an element in a gure with a digit from 0 to 9, where 0 is\nwhite and 9 is black. Suppose we want to add some darkness to an existing value\nof grayness of that gure. An element could start out with a grayness value of\n7, and we might wish to add a 5 worth of darkness to it. In normal arithmetic,\n7 + 5 is 2 (with a carry), which is lighter than either 7 or 5. Something is wrong!\nWith saturating arithmetic, when we reach 9, we stay therewe do not generate\na carry. So, for example, 7 + 5 = 9 and 9 + n = 9. Saturating arithmetic is a\ndierent kind of arithmetic, and the x86 has opcodes (MMX instructions) that\nperform this type of arithmetic.\nScientic applications require opcodes that operate on values represented\nin the oating point data type. FADD, FMUL, FSIN, FSQRT are examples of\noating point opcodes in the x86 ISA.\nThe AND and NOT opcodes are the only LC-3 opcodes that perform logical\nfunctions. One can construct any logical expression using these two opcodes.\n",
    "page_number": 226,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Table B.1\nOperate Instructions, x86 ISA\nInstruction\nExplanation\nADC x, y\nx, y, and the carry retained from the last relevant operation (in CF) are added and\nthe result stored in x.\nMUL x\nThe value in EAX is multiplied by x, and the result is stored in the 64-bit register\nformed by EDX, EAX.\nSAR x\nx is right shifted (arithmetic shift) n bits, and the result is stored in x. The value of n\ncan be 1, an immediate operand, or the count in the CL register.\nXOR x, y\nA bit-wise exclusive-OR is performed on x, y and the result is stored in x.\nDAA\nAfter adding two packed decimal numbers, AL contains two BCD values, which\nmay be incorrect due to propagation of the carry bit after 15, rather than after 9.\nDAA corrects the two BCD digits in AL.\nFSIN\nThe top of the stack (call it x) is popped. The sin(x) is computed and pushed onto\nthe stack.\nFADD\nThe top two elements on the stack are popped, added, and their result pushed\nonto the stack.\nPANDN x, y\nA bit-wise AND-NOT operation is performed on MMX values x, y, and the result is\nstored in x.\nPADDS x, y\nSaturating addition is performed on packed MMX values x, y, and the result is\nstored in x.\nHowever, as is the case with arithmetic, this also is too time-consuming. The x86\nhas in addition separate OR, XOR, AND-NOT, and separate logical operators for\ndierent data types.\nFurthermore, the x86 has a number of other operate instructions that set and\nclear registers, convert a value from one data type to another, shift or rotate the\nbits of a data element, and so on. Table B.1 lists some of the operate opcodes in\nthe x86 instruction set.\nData Movement\nThe LC-3 has six data movement opcodes: LD, LDI, ST,\nSTI, LDR, and STR. They all copy information between memory (and memory-\nmapped device registers) and the eight general purpose registers, R0 to R7.\nAlthough the x86 does not have LDI or STI opcodes, it does have the other\nfour, and in addition to these, many other data movement opcodes. XCHG can\nswap the contents of two locations. PUSHA pushes all eight general purpose\nregisters onto the stack. IN and OUT move data between input and output ports\nand the processor. CMOVcc copies a value from one location to another only if a\npreviously computed condition is true. Table B.2 lists some of the data movement\nopcodes in the x86 instruction set.\nControl\nThe LC-3 has ve control opcodes: BR, JSR/JSRR, JMP, RTI, and\nTRAP. x86 has all these and more. Table B.3 lists some of the control opcodes in\nthe x86 instruction set.\nB.1.1.3 Two Address vs. Three Address\nThe LC-3 is a three-address ISA. This description reects the number of operands\nexplicitly specied by the ADD instruction. An add operation requires two\nsource operands (the numbers to be added) and one destination operand to store\n",
    "page_number": 227,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Table B.2\nData Movement Instructions, x86 ISA\nInstruction\nExplanation\nMOV x, y\nThe value stored in y is copied into x.\nXCHG x, y\nThe values stored in x and y are swapped.\nPUSHA\nAll the registers are pushed onto the top of the stack.\nPUSH\nPush a register onto the top of the stack.\nPOP\nPop a register from the top of the stack.\nMOVS\nThe element in the DS segment pointed to by ESI is copied into the location in\nthe ES segment pointed to by EDI. After the copy has been performed, ESI and\nEDI are both incremented.\nREP MOVS\nPerform the MOVS. Then decrement ECX. Repeat this instruction until ECX = 0.\n(This allows a string to be copied in a single instruction, after initializing ECX.)\nLODS\nThe element in the DS segment pointed to by ESI is loaded into EAX, and ESI is\nincremented or decremented, according to the value of the DF ag.\nINS\nData from the I/O port specied by the DX register is loaded into the EAX\nregister (or AX or AL, if the size of the data is 16 bits or 8 bits, respectively).\nCMOVZ x, y\nIf ZF = 1, the value stored in y is copied into x. If ZF = 0, the instruction acts like\na no-op.\nLEA x, y\nThe address y is stored in x. This is very much like the LC-3 instruction of the\nsame name.\nTable B.3\nControl Instructions, x86 ISA\nInstruction\nExplanation\nJcond x\nBranch based on the condition specied by cond. If cond is true, the IP is loaded\nwith x.\nJMP x\nIP is loaded with the address x. This is very much like the LC-3 instruction of the\nsame name.\nCALL x\nThe IP is pushed onto the stack, and a new IP is loaded with x.\nRET\nThe stack is popped, and the value popped is loaded into IP.\nLOOP x\nECX is decremented. If ECX is not 0 and ZF = 1, the IP is loaded with x.\nINT n\nThe value n is an index into a table of descriptors that specify operating system\nservice routines. The end result of this instruction is that IP is loaded with the\nstarting result of the corresponding service routine. This is very much like the\nTRAP instruction in the LC-3.\nthe result. In the LC-3, all three must be specied explicitly, hence the name\nthree-address ISA.\nEven if the same location is to be used both for one of the sources and for the\ndestination, the three addresses are all specied. For example, the LC-3\nADD R1,R1,R2\nidenties R1 as both a source and the destination.\nThe x86 is mostly (except for special instructions dened as SSE or AVX\ninstructions) a two-address ISA. Since the add operation needs three operands, the\nlocation of one of the sources must also be used to store the result. For example,\nthe corresponding 16-bit ADD instruction in the x86 ISA would be\nADD AX,BX\nwhere AX and BX are names of two of the x86s eight 16-bit general purpose\nregisters. AX and BX are the sources, and AX is the destination.\n",
    "page_number": 228,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Since the result of the operate is stored in the location that originally con-\ntained one of the sources, that source operand is no longer available after that\ninstruction is executed. If that source operand is needed later, it must be saved\nbefore the operate instruction is executed.\nB.1.1.4 Memory Operands\nA major dierence between the LC-3 instruction set and the x86 instruction set\nis the restriction on where operate instructions can get their operands. An LC-3\noperate instruction must obtain its source operands from registers and write the\nresult to a destination register. An x86 instruction, on the other hand, can obtain\none of its sources from memory and/or write its result to memory. In other words,\nthe x86 can read a value from memory, operate on that value, and store the result\nin memory all in a single instruction. The LC-3 cannot.\nThe LC-3 program requires a separate load instruction to read the value from\nmemory before operating on it, and a separate store instruction to write the result\nin memory after the operate instruction. An ISA, like the LC-3, that has this\nrestriction is called a load-store ISA. The x86 is not a load-store ISA.\nB.1.2 Memory\nThe LC-3 memory consists of 216 locations, each containing 16 bits of informa-\ntion. We say the LC-3 has a 16-bit address space, since one can uniquely address\nits 216 locations with 16 bits of address. We say the LC-3 has an addressability\nof 16 bits, since each memory location contains 16 bits of information.\nThe x86 memory has a 64-bit address space and an addressability of eight\nbits. Since one byte contains eight bits, we say the x86 memory is byte address-\nable. Since each location contains only eight bits, four contiguous locations in\nmemory are needed to store a 32-bit data element, say locations X, X+1, X+2,\nand X+3. We designate X as the address of the 32-bit data element. In actuality,\nX only contains bits [7:0], X+1 contains bits [15:8], X+2 contains bits [23:16],\nand X+3 contains bits [31:24] of the 32-bit value.\nOne can determine an LC-3 memory location by simply obtaining its address\nfrom the instruction, using one of the three addressing modes available in the\ninstruction set. An x86 instruction has available to it more than two dozen\naddressing modes that it can use to specify the memory address of an operand.\nWe examine the addressing modes of an x86 instruction in Section B.2.\nIn addition to the larger number of addressing modes, the x86 contains a\nmechanism called segmentation that provides a measure of protection against\nunwanted accesses to particular memory addresses. The address produced by an\ninstructions addressing mode, rather than being an address in its own right, is\nused as an address within a segment of memory. Access to that memory location\nmust take into account the segment register that controls access to that segment.\nThe details of how the protection mechanism works will have to wait for later in\nyour studies.\nHowever, Figure B.2 does show how an address is calculated for the\nregister+oset addressing mode, both for the LC-3 and for the x86, with\n",
    "page_number": 229,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "segmentation. In both cases, the opcode is to move data from memory to a gen-\neral purpose register. The LC-3 uses the LDR instruction. The x86 uses the\nMOV instruction. In the case of the x86, the address calculated is in the DS\nsegment, which is accessed via the DS register. That access is done through a 16-\nbit selector, which indexes into a segment descriptor table, yielding the segment\ndescriptor for that segment. The segment descriptor contains a segment base reg-\nister, a segment limit register, and protection information. The memory address\nobtained from the addressing mode of the instruction is added to the segment\nbase register to provide the actual memory address, as shown in Figure B.2.\nB.1.3 Internal State\nThe internal state of the LC-3 consists of eight 16-bit general purpose registers,\nR0 to R7, a 16-bit PC, and a 16-bit PSR that species the privilege mode, priority,\nand three 1-bit condition codes (N, Z, and P). The user-visible internal state of\nthe x86 consists of 64-bit application-visible registers, a 64-bit Instruction pointer\n(RIP), a 64-bit RFLAGS register, and the 16-bit segment registers.\nB.1.3.1 Application-Visible Registers\nFigure B.3 shows some of the application-visible registers in the x86 ISA.\nIn 64-bit mode, the x86 has 16 general purpose registers: RAX, RBX, RCX,\nRDX, RSP, RBP, RCI, RDI, and R8 through R15. Each register contains 64 bits\nreecting the normal size of operands. In 32-bit mode, there are eight general\npurpose registers: EAX, EBX, ECX, EDX, ESP, EBP, ECI, and EDI, which use\nbits [31:0] of the corresponding 64-bit registers. Also, since some x86 opcodes\nprocess 16-bit and 8-bit operands, x86 also species 16-bit registers AX, BX,\n...DI by using bits [15:0] of the 64-bit registers, and 8 bit registers AL, BL,\nCL, and DL using bits [7:0] and AH, BH, CH, and DH, using bits [15:8] of\nthe corresponding 64-bit registers. The x86 also provides 128-bit, 256-bit, and\n512-bit SIMD registers for operands needed by SSE and AVX operations. They\nare, respectively, XMM0 to XMM31 for 128 bits, YMM0 to YMM31 for 256\nbits, and ZMM0 to ZMM31 for 512 bits.\nB.1.3.2 System Registers\nThe LC-3 has two system-level registersthe PC and the PSR. The user-visible\nx86 has these and more. Figure B.4 shows some of the user-visible system\nregisters in the x86 ISA.\nInstruction Pointer (RIP)\nThe x86 has the equivalent of the LC-3s 16-bit pro-\ngram counter. The x86 calls it an instruction pointer (RIP). Since the address\nspace of the x86 is 64 bits, the RIP is a 64-bit register. In 32-bit mode, since\nthe address space is only 32 bits, the instruction pointer (EIP) uses bits [31:0] of\nthe RIP.\nRFLAGS Register\nCorresponding to the LC-3s N, Z, and P condition codes,\nthe x86 has a one-bit SF (sign ag) register and a one-bit ZF (zero ag) register.\n",
    "page_number": 230,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Figure B.4\nx86 system registers.\nThe CF ag stores the carry produced by the last relevant operation that\ngenerated a carry. As we said earlier, together with the ADC instruction, CF facil-\nitates the generation of procedures, which allows the software to deal with larger\nintegers than the ISA supports.\nThe OF ag stores an overow condition if the last relevant operate generated\na value too large to store in the available number of bits. Recall the discussion of\noverow in Section 2.5.3.\nThe DF ag indicates the direction in which string operations are to process\nstrings. If DF = 0, the string is processed from the high-address byte down (i.e.,\nthe pointer keeping track of the element in the string to be processed next is decre-\nmented). If DF = 1, the string is processed from the low-address byte up (i.e., the\nstring pointer is incremented).\nTwo ags not usually considered as part of the application state are the IF\n(interrupt) ag and the TF (trap) ag. Both correspond to functions with which\nyou are familiar.\nIF is very similar to the IE (interrupt enable) bit in the KBSR and DSR,\ndiscussed in Section 9.4.4.1. If IF = 1, the processor can recognize external\n",
    "page_number": 231,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "interrupts (like keyboard input, for example). If IF = 0, these external inter-\nrupts have no eect on the process that is executing. We say the interrupts are\ndisabled.\nTF is very similar to single-step mode in the LC-3 simulator, only in this case\nit is part of the ISA. If TF = 1, the processor halts after every instruction so the\nstate of the system can be examined. If TF = 0, the processor ignores the trap and\nprocesses the next instruction.\nSegment Registers\nWhen operating in its preferred operating mode (called pro-\ntected mode), the address calculated by the instruction is really an oset from the\nstarting address of a segment, which is specied by some segment base regis-\nter. These segment base registers are part of their corresponding data segment\ndescriptors, which are contained in the segment descriptor table. At each instant\nof time, six of these segments are active. They are called, respectively, the code\nsegment (CS), stack segment (SS), and four data segments (DS, ES, FS, and\nGS). The six active segments are accessed via their corresponding segment reg-\nisters shown in Figure B.4, which contain pointers to their respective segment\ndescriptors.\nB.2 The Format and Specication of\nx86 Instructions\nThe LC-3 instruction is a 16-bit instruction. Bits [15:12] always contain the\nopcode; the remaining 12 bits of each instruction are used to support the needs\nof that opcode.\nThe length of an x86 instruction is not xed. It consists of from 1 to\n16 bytes, depending on the needs of that instruction. A lot of information\ncan be packed into one x86 instruction. Figure B.5 shows the format of an\nx86 instruction.\nThe two key parts of an x86 instruction are the opcode and, where neces-\nFigure B.5\nFormat of the x86 instruction.\n",
    "page_number": 232,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "the use of registers, a one-, two-, or four-byte displacement, and additional register\ninformation contained in an optional SIB byte.\nSome opcodes specify an immediate operand and also specify the number\nof bytes of the instruction that is used to store that immediate information. The\nimmediate value (when one is specied) is the last element of the instruction.\nFinally, instructions assume certain default information with respect to the\nsemantics of an instruction, such as address size, operand size, segment to be\nused, and so forth. The instruction can change this default information by means\nof one or more prexes, which are located at the beginning of the instruction.\nEach part of an x86 instruction is discussed in more detail in Sections B.2.1\nthrough B.2.6.\nB.2.1 Prex\nPrexes provide additional information that is used to process the instruction.\nThere are four classes of prex information, and each instruction can have from\nzero to four prexes, depending on its needs. Fundamentally, a prex overrides\nthe usual interpretation of the instruction.\nThe four classes of prexes are lock and repeat, segment override, operand\noverride, and address override. Table B.4 describes the four types of prexes.\nB.2.2 Opcode\nThe opcode byte (or bytessome opcodes are represented by two bytes) spec-\nies a large amount of information about the needs of that instruction. The\nTable B.4\nPrexes, x86 ISA\nRepeat/Lock\nxF0 (LOCK)\nThis prex guarantees that the instruction will have exclusive use\nof all shared memory until the instruction completes execution.\nxF2, xF3\n(REP/REPE/REPNE)\nThis prex allows the instruction (a string instruction) to be\nrepeated some specied number of times. The iteration count\nis specied by ECX. The instruction is also terminated on the\noccurrence of a specied value of ZF.\nSegment override\nx2E(CS), x36(SS),\nx3E(DS), x26(ES),\nx64(FS), x65(GS)\nThis prex causes the memory access to use the specied\nsegment, instead of the default segment expected for that\ninstruction.\nOperand size override\nx66\nThis prex changes the size of data expected for this instruction.\nThat is, instructions expecting 32-bit data elements use 16-bit\ndata elements. And instructions expecting 16-bit data elements\nuse 32-bit data elements.\nAddress size override\nx67\nThis prex changes the size of operand addresses expected for\nthis instruction. That is, instructions expecting a 32-bit address\nuse 16-bit addresses. And instructions expecting 16-bit\naddresses use 32-bit addresses.\n",
    "page_number": 233,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "opcode byte (or bytes) species, among other things, the operation to be per-\nformed, whether the operands are to be obtained from memory or from reg-\nisters, the size of the operands, whether or not one of the source operands is\nan immediate value in the instruction, and if so, the size of that immediate\noperand.\nSome opcodes are formed by combining the opcode byte with bits [5:3]\nof the ModR/M byte, if those bits are not needed to provide addressing mode\ninformation. The ModR/M byte is described in Section B.2.3.\nB.2.3 ModR/M Byte\nThe ModR/M byte, shown in Figure B.5, provides addressing mode information\nfor two operands, when necessary, or for one operand, if that is all that is needed.\nIf two operands are needed, one may be in memory, the other in a register, or both\nmay be in registers. If one operand is needed, it can be either in a register or in\nmemory. The ModR/M byte supports all cases.\nThe ModR/M byte is essentially partitioned into two parts. The rst part\nconsists of bits [7:6] and bits [2:0]. The second part consists of bits [5:3].\nIf bits [7:6] = 00, 01, or 10, the rst part species the addressing mode\nof a memory operand, and the combined ve bits ([7:6],[2:0]) identify which\naddressing mode. If bits [7:6] = 11, there is no memory operand, and bits [2:0]\nspecify a register operand.\nBits [5:3] specify the register number of the other operand, if the opcode\nrequires two operands. If the opcode only requires one operand, bits [5:3] are\navailable as a subopcode to dierentiate among eight opcodes that have the same\n,\n.\n,\n[5:3]) are part of the opcode.\n",
    "page_number": 234,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": ".\nthen added to whatever is specied by the\nModR/M byte.\nB.2.4 SIB Byte\nIf the opcode species that an operand is to be obtained from memory, the Mod-\nR/M byte species the addressing mode, that is, the information that is needed\nto calculate the address of that operand. Some addressing modes require more\ninformation than can be specied by the ModR/M byte alone. Those operand\nspeciers (see the third entry in Table B.5) specify the inclusion of an SIB byte\nin the instruction. The SIB byte (for scaled-index-base), shown in Figure B.5, pro-\nvides scaling information and identies which register is to be used as an index\nregister and/or which register is to be used as a base register. Taken together, the\nSIB byte computes scale  index + base, where base and/or index can be zero,\nand scale can be 1. Table B.6 lists some of the interpretations of the SIB byte.\nB.2.5 Displacement\nIf the ModR/M byte species that the address calculation requires a displacement,\nthe displacement (one, two, or four bytes) is contained in the instruction. The\nopcode and/or ModR/M byte species the size of the displacement.\nFigure B.6 shows the addressing mode calculation for the source operand if\nthe instruction is as shown. The prex x26 overrides the segment register and\nspecies using the ES segment. The ModR/M and SIB bytes specify that a four-\nbyte displacement is to be added to the base register ECX + the index register\nEBX after its contents is multiplied by 4.\nB.2.6 Immediate\nRecall that the LC-3 allowed small immediate values to be present in the instruc-\ntion, by setting inst[5:5] to 1. The x86 also permits immediate values in the\ninstruction. As stated previously, if the opcode species that a source operand\nis an immediate value in the instruction, it also species the number of bytes\nof the instruction used to represent the operand. That is, an immediate can be\n",
    "page_number": 235,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "694\nAddress\nFigure B.6\nAddressing mode calculation for Base+ScaledIndes+disp32.\nrepresented in the instruction with one, two, or four bytes. Since the opcode\nalso species the size of the operand, immediate values that can be stored in\nfewer bytes than the operand size are rst sign-extended to their full size before\nFigure B.7\nExample x86 instruction in 32-bit mode: ADD EAX, $5.\n",
    "page_number": 236,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "B.3 An Example\nWe conclude this appendix with an example. The problem is one we have dealt\nwith extensively in Chapter 14. Given an input character string consisting of text,\nnumbers, and punctuation, write a C program to convert all the lowercase letters\nto uppercase. Figure B.8 shows a C program that solves this problem. Figure B.9\nshows the annotated LC-3 assembly language code that a C compiler would gen-\nerate. Figure B.10 shows the corresponding annotated x86 assembly language\ncode, assuming we are operating the x86 in 32-bit mode. For readability, we show\nassembly language representations of the LC-3 and x86 programs rather than the\nmachine code.\n#include <stdio.h>\nvoid UpcaseString(char inputString[]);\nint main (void)\n{\nchar string[8];\nscanf(\"%s\", string);\nUpcaseString(string);\n}\nvoid UpcaseString(char inputString[])\n{\nint i = 0;\nwhile(inputString[i]) {\nif (('a' <= inputString[i]) && (inputString[i] <= 'z'))\ninputString[i] = inputString[i] - ('a' - 'A');\ni++;\n}\n}\nFigure B.8\nC source code for the upper-/lowercase program.\n",
    "page_number": 237,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Microarchitecture\ne LC-3\nW\ne have seen in Chapters 4 and 5 the several stages of the instruction cycle\nthat must occur in order for the computer to process each instruction. If a\nmicroarchitecture is to implement an ISA, it must be able to carry out this instruc-\ntion cycle for every instruction in the ISA. This appendix illustrates one example\nof a microarchitecture that can do that for the LC-3 ISA. Many of the details of\nthe microarchitecture and the reasons for each design decision are well beyond\nthe scope of an introductory course. However, for those who want to understand\nhow a microarchitecture can carry out the requirements of each instruction of the\nLC-3 ISA, this appendix is provided.\nC.1 Overview\nFigure C.1 shows the two main components of a microarchitecture: the data path,\nwhich contains all the components that actually process the instructions, and the\ncontrol, which contains all the components that generate the set of control signals\nthat are needed to control the processing at each instant of time.\nWe say, at each instant of time, but we really mean during each clock cycle.\nThat is, time is divided into clock cycles. The cycle time of a microprocessor is the\nduration of a clock cycle. A common cycle time for a microprocessor today is 0.33\nnanoseconds, which corresponds to 3 billion clock cycles each second. We say\nthat such a microprocessor is operating at a frequency of 3 gigahertz, or 3 GHz.\nAt each instant of timeor, rather, during each clock cyclethe 52 control\nsignals (as shown in Figure C.1) control both the processing in the data path and\nthe generation of the control signals for the next clock cycle. Processing in the\ndata path is controlled by 42 bits, and the generation of the control signals for the\nnext clock cycle is controlled by 10 bits.\nNote that the hardware that determines which control signals are needed each\nclock cycle does not operate in a vacuum. On the contrary, the control signals\nneeded in the next clock cycle depend on the following:\n1. The control signals that are present during the current clock cycle.\n2. The LC-3 instruction that is being executed.\n",
    "page_number": 238,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "42\n40\nControl\n10\nControl Signals\n(J, COND, IRD)\n52\nMemory, I/O\nAddr\n16\nInst.\nData,\n16\n16\nData\nData Path\n2\nR\nINT\nIR[15:11]\nBEN\nPSR[15]\nACV\nFigure C.1\nMicroarchitecture of the LC-3, major components.\n3. The privilege mode of the program that is executing, and whether the\nprocessor has the right to access a particular memory location.\n4. If that LC-3 instruction is a BR, whether the conditions for the branch have\nbeen met (i.e., the state of the relevant condition codes).\n5. Whether or not an external device is requesting that the processor be\ninterrupted.\n6. If a memory operation is in progress, whether it is completing during this cycle.\nFigure C.1 identies the specic information in our implementation of the\nLC-3 that corresponds to these six items. They are, respectively:\n1. J[5:0], COND[2:0], and IRDten bits of control signals provided by the\ncurrent clock cycle.\n2. inst[15:12], which identies the opcode, and inst[11:11], which\ndierentiates JSR from JSRR (i.e., the addressing mode for the target of the\nsubroutine call).\n3. PSR[15], bit [15] of the Processor Status Register, which indicates whether\nthe current program is executing with supervisor or user privileges, and\nACV, a signal that informs the processor that a process operating in User\n",
    "page_number": 239,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "mode is trying to access a location in privileged memory. ACV stands for\nAccess Control Violation. When asserted, it denies the process access to the\nprivileged memory location.\n4. BEN to indicate whether or not a BR should be taken.\n5. INT to indicate that some external device of higher priority than the\nexecuting process requests service.\n6. R to indicate the end of a memory operation.\nC.2 The State Machine\nThe behavior of the LC-3 microarchitecture during a given clock cycle is com-\npletely determined by the 52 control signals, combined with ten bits of addi-\ntional information (inst[15:11], PSR[15], ACV, BEN, INT, and R), as shown in\nFigure C.1. We have said that during each clock cycle, 42 of these control signals\ndetermine the processing of information in the data path and the other ten control\nsignals combine with the ten bits of additional information to determine which\nset of control signals will be required in the next clock cycle.\nWe say that these 52 control signals specify the state of the control struc-\nture of the LC-3 microarchitecture. We can completely describe the behavior of\nthe LC-3 microarchitecture by means of a directed graph that consists of nodes\n(one corresponding to each state) and arcs (showing the ow from each state to\nthe one[s] it goes to next). We call such a graph a state machine.\nFigure C.2, combined with Figure C.7, is the state machine for our implemen-\ntation of the LC-3. The state machine describes what happens during each clock\ncycle in which the computer is running. Each state is active for exactly one clock\ncycle before control passes to the next state. The state machine shows the step-\nby-step (clock cyclebyclock cycle) process that each instruction goes through\nfrom the start of its FETCH phase to the end of its instruction cycle, as described\nin Section 4.3.2. Each node in the state machine corresponds to the activity that\nthe processor carries out during a single clock cycle. The actual processing that\nis performed in the data path is contained inside the node. The step-by-step ow\nis conveyed by the arcs that take the processor from one state to the next.\nLets start our study of Figure C.2 by examining the FETCH phase of the\ninstruction cycle. As you know, every instruction goes through the same FETCH\nphase in its instruction cycle. Recall from Chapter 4 that the FETCH phase starts\nwith a memory access to read the instruction at the address specied by the PC.\nNote that in the state numbered 18, the MAR is loaded with the address contained\nin PC, and the PC is incremented in preparation for the FETCH of the next LC-3\ninstruction after the current instruction nishes its instruction cycle. If the content\nof MAR species privileged memory, and PSR[15] = 1, indicating User mode,\nthe access of the instruction will not be allowed. That would be an access control\nviolation, so ACV is set. Finally, if there is no interrupt request present (INT = 0),\nthe ow passes to the state numbered 33. We will describe in Section C.7 the ow\nof control if INT = 1, that is, if an external device is requesting an interrupt.\n",
    "page_number": 240,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Before we get into what happens during the clock cycle when the proces-\nsor is in the state numbered 33, we should explain the numbering systemthat\nis, why are states numbered 18 and 33. Recall, from our discussion of nite\nstate machines in Chapter 3, that each state must be uniquely specied and that\nthis unique specication is accomplished by means of state variables. Our state\nmachine that implements the LC-3 ISA requires 59 distinct states to implement\nthe entire behavior of the LC-3. Figure C.2 shows 31 of them plus pointers to\nseven others (states 8, 13, 15, 48, 49, 57, and 60). Figure C.7 shows the other\n28 states (including the seven that are pointed to in Figure C.2). We will visit all\nof them as we go through this appendix. Since k logical variables can uniquely\nidentify 2k items, six state variables are needed to uniquely specify 59 states. The\nnumber next to each node in Figure C.2 and Figure C.7 is the decimal equivalent\nof the values (0 or 1) of the six state variables for the corresponding state. Thus,\nfor example, the state numbered 18 has state variable values 010010.\nNow, back to what happens after the clock cycle in which the activity of state\n18 has nished. As we said, if no external device is requesting an interrupt, the\now passes to state 33 (i.e., 100001). From state 33, control passes to state 60\nif the processor is trying to access privileged memory while in User mode, or to\nstate 28, if the memory access is allowed, that is, if there is no ACV violation.\nWe will discuss what happens if there is an ACV violation in Section C.7.\nInstate28,sincetheMARcontainstheaddressoftheinstructiontobeprocessed,\nthis instruction is read from memory and loaded into the MDR. Since this memory\naccess can take multiple cycles, this state continues to execute until a ready signal\nfrom the memory (R) is asserted, indicating that the memory access has completed.\nThus, the MDR contains the valid contents of the memory location specified by\nMAR. The state machine then moves on to state 30, where the instruction is loaded\ninto the instruction register (IR), completing the fetch phase of the instruction cycle.\nThe state machine then moves to state 32, where DECODE takes place. Note\nthat there are 13 arcs emanating from state 32, each one corresponding to bits [15:12]\nof the LC-3 instruction. These are the opcode bits that direct the state machine to\none of 16 paths to carry out the instruction cycle of the particular instruction that has\njust been fetched. Note that the arc from the last state of each instruction cycle (i.e.,\nthe state that completes the processing of that LC-3 instruction) takes us to state 18\n(to begin the instruction cycle of the next LC-3 instruction).\nC.3 The Data Path\nThe data path consists of all components that actually process the information\nduring each clock cyclethe functional units that operate on the information, the\nregisters that store information at the end of one cycle so it will be available for\nfurther use in subsequent cycles, and the buses and wires that carry information\nfrom one point to another in the data path. Figure C.3, an expanded version of\nwhat you have already encountered in Figure 5.18, illustrates the data path of our\nmicroarchitecture of the LC-3.\nNote the control signals that are associated with each component in the data\npath. For example, ALUK, consisting of two control signals, is associated with\n",
    "page_number": 241,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "the ALU. These control signals determine how that component (the ALU) will\nbe used each cycle. Table C.1 lists the set of 42 control signals that control the\nelements of the data path and the set of values that each control signal can have.\n(Actually, for readability, we provide a symbolic name for each value, rather than\nthe binary value.) For example, since ALUK consists of two bits, it can have one\nof four values. Which value it has during any particular clock cycle depends on\nwhether the ALU is required to ADD, AND, NOT, or simply pass one of its inputs\nto the output during that clock cycle. PCMUX also consists of two control signals\nand species which input to the MUX is required during a given clock cycle.\nLD.PC is a single-bit control signal and is a 0 (NO) or a 1 (YES), depending on\nwhether or not the PC is to be loaded during the given clock cycle.\nDuring each clock cycle, corresponding to the current state in the state\nmachine, the 42 bits of control direct the processing of all components in the data\npath that are required during that clock cycle. As we have said, the processing\nthat takes place in the data path during that clock cycle is specied inside the\nnode representing the state.\nC.4 The Control Structure\nThe control structure of a microarchitecture is specied by its state machine. As\ndescribed earlier, the state machine (Figure C.2 and Figure C.7) determines which\ncontrol signals are needed each clock cycle to process information in the data path\nand which control signals are needed each clock cycle to direct the ow of control\nfrom the currently active state to its successor state.\nFigure C.4 shows a block diagram of the control structure of our implemen-\ntation of the LC-3. Many implementations are possible, and the design consider-\nations that must be studied to determine which of many possible implementations\nshould be used is the subject of a full course in computer architecture.\nWe have chosen here a straightforward microprogrammed implementation.\nEach state of the control structure requires 42 bits to control the processing in the\ndata path and 10 bits to help determine which state comes next. These 52 bits are\ncollectively known as a microinstruction. Each microinstruction (i.e., each state\nof the state machine) is stored in one 52-bit location of a special memory called\nthe control store. There are 59 distinct states. Since each state corresponds to one\nmicroinstruction in the control store, the control store for our microprogrammed\nimplementation requires six bits to specify the address of each microinstruction.\nThose six bits correspond to the state number associated with each state in the\nstate machine. For example, the microinstruction associated with state 18 is the\nset of 52 control signals stored in address 18 of the control store.\nTable C.2 lists the function of the ten bits of control information that help\ndetermine which state comes next. Figure C.5 shows the logic of the micro-\nsequencer. The purpose of the microsequencer is to determine the address in the\ncontrol store that corresponds to the next state, that is, the location where the\n52 bits of control information for the next state are stored.\n",
    "page_number": 242,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "708\nAddress of Next State\nFigure C.5\nThe microsequencer of the LC-3.\nAs we said, state 32 of the state machine (Figure C.2) performs the DECODE\nphase of the instruction cycle. It has 16 next states, depending on the LC-3\ninstruction being executed during the current instruction cycle. If the IRD con-\ntrol signal in the microinstruction corresponding to state 32 is 1, the output MUX\nof the microsequencer (Figure C.5) will take its source from the six bits formed\nby 00 concatenated with the four opcode bits IR[15:12]. Since IR[15:12] speci-\nes the opcode of the current LC-3 instruction being processed, the next address\nof the control store will be one of 16 addresses, corresponding to the 15 opcodes\nplus the one unused opcode, IR[15:12] = 1101. That is, each of the 16 next states\nafter state 32 is the rst state to be carried out after the instruction has been\ndecoded in state 32. For example, if the instruction being processed is ADD, the\naddress of the next state is state 1, whose microinstruction is stored at location\n000001. Recall that IR[15:12] for ADD is 0001.\nIf, somehow, the instruction inadvertently contained IR[15:12] = 1101,\nthe unused opcode, the microarchitecture would execute a sequence of\nmicroinstructions, starting at state 13. These microinstructions would respond to\nthe fact that an instruction with an illegal opcode had been fetched. Section C.7.3\ndescribes what happens in that case.\n",
    "page_number": 243,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "(d)\nFigure C.6\nAdditional logic required to provide control signals.\nSeveral signals necessary to control the data path and the microsequencer\nare not among those listed in Tables C.1 and C.2. They are DR, SR1, BEN, INT,\nACV, and R. Figure C.6 shows the additional logic needed to generate DR, SR1,\nBEN, and ACV.\nThe INT signal is supplied by some event external to the normal instruction\nprocessing, indicating that normal instruction processing should be interrupted\nand this external event dealt with. The interrupt mechanism was described in\nChapter 9. The corresponding ow of control within the microarchitecture is\ndescribed in Section C.7.\nThe remaining signal, R, is a signal generated by the memory in order to\nallow the LC-3 to operate correctly with a memory that takes multiple clock\ncycles to read or store a value.\nSuppose it takes memory ve cycles to read a value. That is, once MAR\ncontains the address to be read and the microinstruction asserts READ, it will take\nve cycles before the contents of the specied location in memory is available to\nbe loaded into MDR. (Note that the microinstruction asserts READ by means of\ntwo control signals: MIO.EN/YES and R.W/RD; see Figure C.3.)\n",
    "page_number": 244,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "Recall our discussion in Section C.2 of the function of state 28, which\naccesses an instruction from memory during the FETCH phase of each instruc-\ntion cycle. If the memory takes ve cycles to read a value, for the LC-3 to operate\ncorrectly, state 28 must execute ve times before moving on to state 30. That is,\nuntil MDR contains valid data from the memory location specied by the con-\ntents of MAR, we want state 28 to continue to re-execute. After ve clock cycles,\nthe memory has completed the read, resulting in valid data in MDR, so the pro-\ncessor can move on to state 30. What if the microarchitecture did not wait for the\nmemory to complete the read operation before moving on to state 30? Since the\ncontents of MDR would still be garbage, the microarchitecture would put garbage\ninto the IR in state 30.\nThe ready signal (R) enables the memory read to execute correctly. Since the\nmemory knows it needs ve clock cycles to complete the read, it asserts a ready\nsignal (R) throughout the fth clock cycle. Figure C.2 shows that the next state\nis 28 (i.e., 011100) if the memory read will not complete in the current clock\ncycle and state 30 (i.e., 011110) if it will. As we have seen, it is the job of the\nmicrosequencer (Figure C.5) to produce the next state address.\nThe ten microsequencer control signals for state 28 are:\nIRD/0\n; NO\nCOND/001\n; Memory Ready\nJ/011100\nWith these control signals, what next state address is generated by the microse-\nquencer? For each of the rst four executions of state 28, since R = 0, the next\nstate address is 011100. This causes state 28 to be executed again in the next clock\ncycle. In the fth clock cycle, since R = 1, the next state address is 011110, and\nthe LC-3 moves on to state 30. Note that in order for the ready signal (R) from\nmemory to be part of the next state address, COND had to be set to 001, which\nallowed R to pass through its four-input AND gate.\nC.5 The TRAP Instruction\nAs we have said, each LC-3 instruction follows its own path from state 32 to\nits nal state in its instruction cycle, after which it returns to state 18 to start\nprocessing the next instruction. As an example, we will follow the instruction\ncycle of the TRAP instruction, shown in Figure C.7.\nRecall that the TRAP instruction pushes the PSR and PC onto the system\nstack, loads the PC with the starting address of the trap service routine, and\nexecutes the service routine from privileged memory.\nFrom state 32, the next state after DECODE is state 15, consistent with the\nTRAP instruction opcode 1111. In state 15, the Table register, which will be\nused to form MAR[15:8] of the trap vector table entry, is loaded with x00, the\nPC is incremented (we will see why momentarily), and the MDR is loaded with\nthe PSR in preparation for pushing it onto the system stack. Control passes to\nstate 47.\n",
    "page_number": 245,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "In state 47, the trap vector (IR[7:0]) is loaded into the eight-bit register\nVector, PSR[15] is set to Supervisor mode since the trap service routine exe-\ncutes in privileged memory, and the state machine branches to state 37 or 45,\ndepending on whether the program that executed the TRAP instruction was in\nUser mode or Supervisor mode. If in User mode, state 45 saves the User Stack\nPointer in Saved USP, loads the stack pointer from Saved SSP, and continues on\nto state 37, where the processor starts pushing PSR and PC onto the stack. If the\nprogram executing the TRAP instruction is already in Privileged mode, state 45\nis not necessary.\nIn states 37 and 41, the PSR is pushed onto the system stack. In states 43, 46,\nand 52, the PC is pushed onto the system stack. Note that in state 43, the PC is\ndecremented before being pushed onto the stack. This is necessary in the case of\ndealing with interrupts and exceptions, which will be explained in Section C.7.\nThis is not necessary for processing the TRAP instruction, which is why PC is\nincremented in state 15.\nThe only thing remaining is to load PC with the starting address of the trap\nservice routine. This is done by loading MAR with the address of the proper entry\nin the trap vector table, obtained by concatenating Table and Vector (in state 54),\nloading the starting address from memory into MDR (in state 53), and loading\nthe PC (in state 55). This completes the execution of the TRAP instruction, and\ncontrol returns to state 18 to begin processing the next instruction  in this case,\nthe rst instruction of the trap service routine.\nThe last instruction in every trap service routine is RTI (return from trap or\ninterrupt). From DECODE in state 32, the next state of RTI is state 8, consistent\nwith its eight-bit opcode 1000. In states 8, 36, and 38, the PC is popped o the\nsystem stack and loaded into PC. In states 39, 40, 42, and 34, the PSR is popped\no the system stack and loaded into PSR. This returns the PC and PSR to the\nvalues it had before the trap service routine was executed. Finally, if the program\nthat invoked the TRAP instruction was in User mode, PSR[15] must be returned\nto 1, the Supervisor Stack Pointer saved, and the User Stack Pointer loaded into\nSP. This is done in state 59, completing the instruction cycle for RTI.\nC.6 Memory-Mapped I/O\nAs you know from Chapter 9, the LC-3 ISA performs input and output via\nmemory-mapped I/O, that is, with the same data movement instructions that it\nuses to read from and write to memory. The LC-3 does this by assigning an\naddress to each device register. Input is accomplished by a load instruction whose\neective address is the address of an input device register. Output is accomplished\nby a store instruction whose eective address is the address of an output device\nregister. For example, in state 25 of Figure C.2, if the address in MAR is xFE02,\nMDR is supplied by the KBDR, and the data input will be the last keyboard\ncharacter typed. On the other hand, if the address in MAR is a legitimate memory\naddress, MDR is supplied by the memory.\n",
    "page_number": 246,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "712\nother\n0\nW\n0\nx\n0\n0\n0\nother\n1\nR\n1\nmem\n0\n0\n0\nother\n1\nW\n1\nx\n0\n0\n0\nThe state machine of Figure C.2 does not have to be altered to accommo-\ndate memory-mapped I/O. However, something has to determine when memory\nshould be accessed and when I/O device registers should be accessed. This is the\njob of the address control logic (ADDR.CTL.LOGIC) shown in Figure C.3.\nTable C.3 is a truth table for the address control logic, showing what con-\ntrol signals are generated, based on (1) the contents of MAR, (2) whether or not\nmemory or I/O is accessed this cycle (MIO.EN/NO, YES), and (3) whether a load\n(R.W/Read) or store (R.W/Write) is requested. Note that, for a memory-mapped\nload, data can be supplied to MDR from one of four sources: memory, KBDR,\nKBSR, or DSR. The address control logic provides the appropriate select signals\nto the INMUX. For a memory-mapped store, the data supplied by MDR can be\nwritten to memory, KBSR, DDR, or DSR. The address control logic supplies the\nappropriate enable signal to the corresponding structure.\nC.7 Interrupt and Exception Control\nThe nal piece of the state machine needed to complete the LC-3 story are those\nstates that control the initiation of an interrupt, those states that control the return\nfrom an interrupt (the RTI instruction), and those states that control the initiation\nof one of the three exceptions specied by the ISA.\nInterrupts and exceptions are very similar. Both stop the program that is cur-\nrently executing. Both push the PSR and PC of the interrupted program onto the\nsystem stack, obtain the starting address of the interrupt or exception service rou-\ntine from the interrupt vector table, and load that starting address into the Program\nCounter. The main dierence between interrupts and exceptions is the nature of\n",
    "page_number": 247,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "an interrupt by asserting its interrupt request signal. Recall from Chapter 9 that\nif the priority level of the device asserting its interrupt request signal is higher than\nboth the priority level of the currently executing program and any other external\ninterrupt request asserted at the same time, INT is asserted and INTV is loaded\nwith the interrupt vector corresponding to that external event. The microproces-\nsor responds to INT by initiating the interrupt. That is, the processor puts itself\ninto Supervisor mode if it isnt in Supervisor mode, pushes the PSR and PC of\nthe interrupted process onto the supervisor stack, and loads the PC with the start-\ning address of the interrupt service routine. The PSR contains the privilege mode\nPSR[15], priority level PSR[10:8], and condition codes PSR[2:0] of a program.\nIt is important that when the processor resumes execution of the interrupted pro-\ngram, the privilege mode, priority level, and condition codes are restored to what\nthey were when the interrupt occurred.\nThe microarchitecture of the LC-3 initiates an interrupt as follows: Recall\nfrom Figure C.2 that in state 18, while MAR is loaded with the contents of PC\nand PC is incremented, INT is tested.\nState 18 is the only state in which the processor checks for interrupts. The\nreason for only testing in state 18 is straightforward: Once an LC-3 instruction\nstarts processing, it is easier to let it nish its complete instruction cycle (FETCH,\nDECODE, etc.) than to interrupt it in the middle and have to keep track of how far\nalong it was when the external device requested an interrupt (i.e., asserted INT).\nIf INT is only tested in state 18, the current instruction cycle can be aborted early\n(even before the instruction has been fetched), and control directed to initiating\nthe interrupt.\nThe test is enabled by the control signals that make up COND5, which are\n101 only in state 18, allowing the value of INT to pass through its four-input AND\ngate, shown in Figure C.5, to contribute to the address of the next state. Since the\nCOND signals are not 101 in any other state, INT has no eect in any other state.\nIn state 18, the ten microsequencer control bits are as follows:\nIRD/0\n; NO\nCOND/101\n; Test for interrupts\nJ/100001\nIf INT = 1, a 1 is produced at the output of the AND gate, which in turn\nmakes the next state address not 100001, corresponding to state 33, but rather\n110001, corresponding to state 49. This starts the initiation of the interrupt (see\nFigure C.7).\nSeveral functions are performed in state 49. The PSR, which contains the\nprivilege mode, priority level, and condition codes of the interrupted program,\nare loaded into MDR, in preparation for pushing it onto the supervisor stack.\nPSR[15] is cleared, reecting the change to Supervisor mode, since all inter-\nrupt service routines execute in Supervisor mode. The three-bit priority level\nand eight-bit interrupt vector (INTV) provided by the interrupting device are\nrecorded. PSR[10:8] is loaded with the priority level of the interrupting device.\nThe internal register Vector is loaded with INTV and the eight-bit register Table\nis loaded with x01 in preparation for accessing the interrupt vector table to obtain\nthe starting address of the interrupt service routine. Finally, the processor tests\n",
    "page_number": 248,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "the old PSR[15] to determine whether the stack pointers must be adjusted before\npushing PSR and PC.\nIf the old PSR[15] = 0, the processor is already operating in Supervisor mode.\nR6 is the Supervisor Stack Pointer (SSP), so the processor proceeds immediately\nto states 37 and 41 to push the PSR of the interrupted program onto the super-\nvisor stack. If PSR[15] = 1, the interrupted program was in User mode. In that\ncase, the User Stack Pointer (USP) must be saved in Saved USP and R6 must be\nloaded with the contents of Saved SSP before moving to state 37. This is done in\nstate 45.\nThe control ow from state 49 to either 37 or 45 is enabled by the ten\nmicrosequencer control bits, as follows:\nIRD/0\n; NO\nCOND/100\n; Test PSR[15], privilege mode\nJ/100101\nIf PSR[15] = 0, control goes to state 37 (100101); if PSR[15] = 1, control\ngoes to state 45 (101101).\nIn state 37, R6 (the SSP) is decremented (preparing for the push), and MAR\nis loaded with the address of the new top of the stack.\nIn state 41, the memory is enabled to WRITE (MIO.EN/YES, R.W/WR).\nWhen the write completes, signaled by R = 1, PSR has been pushed onto the\nsupervisor stack, and the ow moves on to state 43.\nIn state 43, the PC is loaded into MDR. Note that state 43 says MDR is loaded\nwith PC-1. Recall that in state 18, at the beginning of the instruction cycle for the\ninterrupted instruction, PC was incremented. Loading MDR with PC-1 adjusts\nPC to the correct address of the interrupted program.\nIn states 46 and 52, the same sequence as in states 37 and 41 occurs, only\nthis time the PC of the interrupted program is pushed onto the supervisor stack.\nThe nal task to complete the initiation of the interrupt is to load the PC\nwith the starting address of the interrupt service routine. This is carried out by\nstates 54, 53, and 55. It is accomplished in a manner similar to the loading of\nthe PC with the starting address of a TRAP service routine. The event causing\nthe INT request supplies the eight-bit interrupt vector INTV associated with the\ninterrupt, similar to the eight-bit trap vector contained in the TRAP instruction.\nThis interrupt vector is stored in the eight-bit register INTV, shown on the data\npath in Figure C.8.\nThe interrupt vector table occupies memory locations x0100 to x01FF. In\nstate 54, the interrupt vector that was loaded into Vector in state 49 is combined\nwith the base address of the interrupt vector table (x0100) and loaded into MAR.\nIn state 53, memory is READ. When R = 1, the read has completed, and MDR\ncontains the starting address of the interrupt service routine. In state 55, the PC\nis loaded with that starting address, completing the initiation of the interrupt.\nIt is important to emphasize that the LC-3 supports two stacks, one for each\nprivilege mode, and two stack pointers (USP and SSP), one for each stack. R6 is\nthe stack pointer and is loaded from the Saved SSP when privilege changes from\nUser mode to Supervisor mode, and from Saved USP when privilege changes\n",
    "page_number": 249,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "from Supervisor mode to User mode. When the privilege mode changes, the cur-\nrent value in R6 must be stored in the appropriate Saved stack pointer in order\nto be available the next time the privilege mode changes back.\nC.7.2 Returning from an Interrupt or Trap Service Routine, RTI\nInterrupt service routines, like trap service routines already described, end with\nthe execution of the RTI instruction. The job of the RTI instruction is to restore\nthe computer to the state it was in before the interrupt or trap service routine was\nexecuted. This means restoring the PSR (i.e., the privilege mode, priority level,\nand the values of the condition codes N, Z, P) and restoring the PC. These values\nwere pushed onto the stack during the initiation of the interrupt or execution of\nthe TRAP instruction. They must, therefore, be popped o the stack in the reverse\norder.\nThe rst state after DECODE is state 8. Here we load the MAR with the\naddress of the top of the supervisor stack, which contains the last thing pushed\n(that has not been subsequently popped)the state of the PC when the interrupt\nwas initiated. At the same time, we test PSR[15] since RTI is a privileged instruc-\ntion and can only execute in Supervisor mode. If PSR[15] = 0, we can continue\nto carry out the requirements of RTI.\nStates 36 and 38 restore PC to the value it had when the interrupt was initi-\nated. In state 36, the memory is read. When the read is completed, MDR contains\nthe address of the instruction that was to be processed next when the interrupt\noccurred. State 38 loads that address into the PC.\nStates 39, 40, 42, and 34 restore the privilege mode, priority level, and con-\ndition codes (N, Z, P) to their original values. In state 39, the Supervisor Stack\nPointer is incremented so that it points to the top of the stack after the PC was\npopped. The MAR is loaded with the address of the new top of the stack. State\n40 initiates the memory READ; when the READ is completed, MDR contains\nthe interrupted PSR. State 42 loads the PSR from MDR, and state 34 increments\nthe stack pointer.\nThe only thing left is to check the privilege mode of the interrupted pro-\ngram to see whether the stack pointers have to be switched. In state 34, the\nmicrosequencer control bits are as follows:\nIRD/0\n; NO\nCOND/100\n; Test PSR[15], privilege mode\nJ/110011\nIf PSR[15] = 0, control ows to state 51 (110011) to do nothing for one cycle.\nIf PSR[15] = 1, control ows to state 59, where R6 is saved in Saved SSP and\nR6 is loaded from Saved USP. In both cases, control returns to state 18 to begin\nprocessing the next instruction.\nC.7.3 Initiating an Exception\nThe LC-3 identies three cases where processing is not allowed to continue nor-\nmally due to something going awry in the executing program. We refer to these\ncases as exceptions. They are initiated in the same way interrupts are initiated,\n",
    "page_number": 250,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "by pushing the PSR and PC onto the system stack, obtaining the starting address\nof the exception service routine from the interrupt vector table, and loading that\naddress into the PC to initiate the exception service routine.\nThe three exceptions identied in the LC-3 are (1) a privileged mode excep-\ntion caused by the program attempting to execute the RTI instruction while in\nUser mode, (2) the illegal opcode exception caused by the program trying to exe-\ncute an instruction whose opcode is 1101, and (3) an access control violation\n(ACV) exception caused by the program trying to access a privileged memory\nlocation while in User mode.\nC.7.3.1 Privilege Mode Exception\nIf the processor is in User mode (PSR[15] = 1) and is attempting to execute\nRTI, a privilege mode exception occurs. The processor pushes the PSR and the\naddress of the RTI instruction onto the supervisor stack and loads the PC with\nthe starting address of the service routine that handles privilege mode violations.\nFigure C.7 shows the ow, starting with a branch from state 8 to state 44 if\nPSR[15] = 1.\nIn state 44, the eight-bit Table register is loaded with x01, indicating the\naddress of an entry in the interrupt vector table, and the eight-bit Vector register\nis loaded with x00, indicating the rst entry in the interrupt vector table. The con-\ntents of x0100 is the starting address of the service routine that handles privilege\nmode exceptions. The MDR is loaded with the PSR of the program that caused\nthe exception in preparation for pushing it onto the system stack. Finally, PSR[15]\nis set to 0, since the service routine will execute with supervisor privileges. Then\nthe processor moves to state 45, where it follows the same ow as the initiation\nof interrupts.\nThe main dierence between this ow and that for the initiation of interrupts\nis in state 54, where MAR is loaded with x01Vector. In the case of interrupts,\nVector is loaded in state 49 with INTV, which is supplied by the interrupting\ndevice. In the case of the privilege mode violation, Vector is loaded in state\n44 with x00.\nThere are two additional functions performed in state 49 that are not per-\nformed in state 44. First, the priority level is changed, based on the priority of\nthe interrupting device. We do not change the priority in handling a privilege\nmode violation. The service routine executes at the same priority as the program\nthat caused the violation. Second, a test to determine the privilege mode is per-\nformed for an interrupt. This is unnecessary for a privilege mode violation since\nthe processor already knows it is executing in User mode.\nC.7.3.2 Illegal Opcode Exception\nAlthough it would be a rare situation, it is possible, we suppose, that a pro-\ngrammer writing a program in machine language could mistakenly include an\ninstruction having opcode = 1101. Since there is no such opcode in the LC-3 ISA,\nthe computer cannot process that instruction. State 32 performs the DECODE,\nand the next state is state 13.\n",
    "page_number": 251,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  },
  {
    "text": "The action the processor takes is very similar to that of a privilege mode\nexception. The PSR and PC of the program are pushed onto the supervisor stack,\nand the PC is loaded with the starting address of the Illegal Opcode exception\nservice routine.\nState 13 is very similar to state 44, which starts the initiation of a privilege\nmode exception. There are two dierences: (1) Vector is loaded with x01, since\nthe starting address of the service routine for the illegal opcode exception is in\nx0101. (2) In the case of the privilege mode exception, we know the program is\nin User mode when the processor attempts to execute the RTI instruction. In the\ncase of an illegal opcode, the processor can be in either mode, so from state 13\nthe processor goes to state 37 or state 45, depending on whether the program is\nexecuting in Supervisor mode or User mode when the illegal opcode instruction\nis encountered.\nLike state 44, the priority of the running program is not changed, since the\nurgency of handling the exception is the same as the urgency of executing the\nprogram that contains it. Like state 49, state 13 tests the privilege mode of the\nprogram that contains the illegal opcode, since if the currently executing pro-\ngram is in User mode, the stack pointers need to be switched as described in\nSection C.7.1. Like state 49, the processor then microbranches either to state 37\nif the stack pointer is already pointing to the supervisor stack, or to state 45 if the\nstack pointers have to be switched. From there, the initiating sequence continues\nin states 37, 41, 43, etc., identical to what happens when an interrupt is initiated\n(Section C.7.1) or a privilege mode exception is initiated (Section C.7.3.1). The\nPSR and PC are pushed onto the supervisor stack and the starting address of the\nservice routine is loaded into the PC, completing the initiation of the exception.\nC.7.3.3 Access Control Violation (ACV) Exception\nAn Access Control Violation (ACV) exception occurs if the processor attempts\nto access privileged memory while operating in User mode. The state machine\nchecks for this in every case where the processor accesses memory, that is, in\nstates 17, 19, 23, 33, and 35. If an ACV violation occurs, the next state is respec-\ntively states 56, 61, 48, 60, or 57 (see Figure C.2). In all ve states, the processor\nloads Table with x01, Vector with x02, MDR with the PSR, sets PSR[15] to 0,\nexactly like state 44, with one exception. Vector is set to x02 since the starting\naddress of the ACV exception service routine is in memory location x0102. Pro-\ncessing continues exactly like in state 44, moving rst to state 45 to switch to the\nsystem stack, and then pushing PSR and PC onto the stack and loading the PC\nwith the starting address of the service routine.\nC.8 Control Store\nFigure C.9 completes our microprogrammed implementation of the LC-3. It\nshows the contents of each location of the control store, corresponding to the\n52 control signals required by each state of the state machine. We have left the\nexact entries blank to allow you, the reader, the joy of lling in the required signals\nyourself. The solution is available from your instructor.\n",
    "page_number": 252,
    "textbook_name": "Yale-Patt_Sanjay-Patel--Intro_to_Computing_Systems"
  }
]